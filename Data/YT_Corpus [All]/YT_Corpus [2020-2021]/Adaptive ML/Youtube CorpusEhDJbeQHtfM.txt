 Shyam Gollakota: Alright, thank you everyone for coming and welcome to the talk. It's a pleasure to introduce Ryo Suzuki. He is a fifth-year PhD student at University of Colorado, and he works at the intersection of HCI, Robotics, and UbiComp, and today he'll be talking to us about programmable environments with distributed swarm reports. Please feel free to ask questions. Just give him the first ten minutes to set the context for the talk before you ask your questions. Ryo Suzuki: Great. Thank you for the introduction. Hi, my name is Ryo Suzuki. I'm currently a fifth-year PhD candidate at the University of Colorado department of computer science. So, today I want to talk about the programmable environments with distributed swarm robots. So my main research interests is situated at the intersection between human-computer interaction and robotics. During my PhD, I've been working on how we can leverage the interactive swarm and soft robots human-computer interfaces and how we can make- how we can leverage these robots to make our everyday environment more adaptive, interactive, and programmable. So, this talk is about, to answer that research question. How can we make static, physical environments more programmable through distributed swarm robots? And how can we computational control the physical elements of our everyday environment with distributed swarm robots? So, I'm interested in this research question because ever since I started working on a human-computer interaction research, I'm motivated by how we can leverage the physical objects and environments as the means of human-computer interaction because, today's the computer interfaces are mostly dominated by the graphical user interface and a tiny computer screen. So what we can change, what we can program is what we can change this tiny computer screen, right? But, we are surrounded by our rich physical environments and we have an innate capability of manipulating objects, interacting with the physical environment, which we have developed since prehistoric time. But, the graphical user interface is a so powerful so they can, you know, simulate anything like maps, the camera, books, or the typewriter or, you know, the the musical instrument (inaudible). So the the, the graphical user interfaces are dominating how we can interact with the digital world. So, as we can see, is the two worlds are currently divided each other. So the computer cannot really control the physical object and we cannot also (inaudible) the physical object environments to interact with the digital world. So, I'm interested in how we can create the environments, object, and the user experiences which can- where the digital world is seamlessly blended into the physical world. Then the question is how we can make this static, non-programmable environment programmable and how we can completely control the elements of this environment. So, if you look at the history of the human-computer interaction research, that- we can also- it can be also seen as a history of the other (inaudible) research question For example, Mark Weiser, ubiquitous computing, to then Pierre Wellner, a digital desk or spatial augmented reality, and Hiroshi Ishii is tangible bits and tangible user interfaces. So all of this research (inaudible) have tried to address how we can seamlessly blend the digital world into the physical world. So, thanks to the decades of this research, we are gradually able to program the environment. Right? So, for example, we are now living in an environment where we can computationally control the lighting, the heating, or air-conditioning, the sound, or the visual property of the environment. However, the physical elements of our living environment are still static and non-programmable because the computation- in contrast to computationary control, lighting, or air conditioning, you know, controlling these physical elements, physical property of the elements, are still difficult to manipulate. So again, my research question is, then, how we can make the non-programmable physical environments more programmable, and how we can computationally control the physical elements of these living environments in a more dynamic and interactive and programmable way. So to answer that research question, I have been working on the using distributed swarm robots at various scales as the means of making the physical environment more programmable. So these custom-built swarm robots, from the finger-tip size to the room-scale size, can, you know, dynamically- dynamically construct the shape and collect debris, collectively actuate objects, and, you know, the dynamic or interactively reconfigure the environment in a more programmable way. So, this robot can be easily deployed into the existing environment so that we can make environments more adaptive, interactive, and programmable. So, in the literature of HCI, there is an emerging research topic about programmable shape-changing interfaces. So, again, today's user interfaces has been dominated by the graphical user interfaces. The shape-changing interfaces have- has been working on how we can leverage the dynamic physical object at the computer interfaces. So, this kind of the research is started from the special-purpose shape-changing interfaces then gradually shifting to more like a general-purpose shape-changing interfaces by leveraging the so many actuated elements. So for example, this one is inFORM, developed by MIT Media Lab. So, this system are trying to program not only the pixel of the computer screen, but more like program the physical surfaces to embrace a digital computation so that it can actuate the object, construct the shape, or represent the human hand in a remote way. By the way, he is my adviser, Daniel Leithinger. Um. So this system are pretty cool, but this system often require a large, mechanically complex devices, something like that, and that which can be often difficult to deploy into the existing environment. So in general, if you plot the existing work, that the system are going to general-purpose, then it's gonna be required a much more complex, gigantic system. So, I'm interested in, then, how we can increase the deployability of this general-purpose (inaudible)- general-purpose dynamic interfaces, but while maintaining- maintain the general-purpose programmability. So, I believe distributed swarm robots can be a pretty, you know, the- one of the promising approach to achieve this score because the distributive-ness of the swarm robot can be easily deployed into the existing environment. There are also the swarm-ness or modularity of this robot can, you know, can be used for the many applications. So, so that you have, for example, this robot can correctly and autonomously deploy and move on a transform in existing environment. We can probably achieve this goal. So that's kind of- that's one of the motivations that I'm taking this approach. And also, I believe we are standing at the very unique and exciting moment in the history of robotics. So, in the history of computers, the computer used to be a very gigantic, you know a mainframe computer. But, the computation becomes cheaper the smaller and more powerful. You know, we are now living in an environment where the hundreds of computer surrounding and this within a single room. So, I believe the same thing has been happening in the history of robotics. So the robots are so now, you know, they're pretty gigantic and only work within a factory or warehouses. But, they are gradually migrating from the factory into our everyday environment. And I believe The robots are also ubiquitous in the next 20 to 30 years. Then, the question is how we can interact with these robots and what kind of application can be possible? So for example, imagine, think about the future where robots are almost equivalent to today's IoT devices and which can- that we are living in an environment where hundreds of robots can support our everyday life. So then the question is, how we can interact with it and how these robots can support our life? So, this is how I contributed as a HCI researcher. So as an HCI researcher, I have contributed to illustrating the potential future, or the potential application enabled by this research, by these swarm robots, as well as develop the interaction technique with the swarm robots, and also identify the challenges from the both technical and the user interaction perspective to which we need to solve today. So in this talk, I want to introduce my work to illustrate this potential future. So during my PhD, I have explored the application that ranged from the using the swarm robots as an everyday assistant to the using the swarm robots to the tangible interfaces. So for example, one of my work is using the swarm robots to provide physical affordances to augment our everyday environment, and that the swarm robots can also reconfigure environments by rearranging furniture, or the swarm robots can make data or information accessible for blind users. And these robots can also construct a shape to make an object which we can interact with (inaudible). This work made my contribution in my domain, and the CHI and the UIST, it's my you know, two major fields in HCI. So, okay now I wanna talk about the first project called the ShapeBots. So the ShapeBots is shape-changing swarm robots that can move around on tabletop surfaces so that they can collectively collect debris and individually transform its shape to represent the data, or data visualization, or actuating the objects, or construct a shape for the everyday purposes. So, the ShapeBots is inspired by the recent research topic about swarm user interfaces. So the swarm user interfaces is trying to leverage swarm robots as a computer interfaces. The most of the swarm robot- swarm user interfaces, and the swarm robots in general, have a limited capability about the constructing a shape or the functionality because they can't- they cannot individually transform its shape. So in this project, we are interested in what if this robot can also change its shape, also collectively change the shape, so the idea for provide- constructing the shape, or actuating an object, or representing data. So, we are interested in, you know, the- how this kind of individual and a collective shape transmission capability can enrich the functionality as well as expressiveness of these interfaces. But, to enable that, we have technical challenges because the swarm robots are so tiny. So the technical challenges how we can make an ACME into an actuator that can fit into this small form factor, but also have knowledge transmission capability. So to do that, we have the little reel-based linear actuator, so, which is kind of inspired by, it's a tape measure. So the tape- as you can see the tape measure, you know, it's pretty compact but it also has a kind of large transformation capability by rotating and extending this kind of, you know, (inaudible). And then they're putting this kind of, you know, the modular actuator on top of the each swarm robots so that each swarm robot can not only changing the shape in a horizontal direction, but also the vertical direction, or the using as a curved line, or attaching an origami to expand in the surface- 2D surfaces or volumetric change. So, then the question is: what kind of application we can use it for? So, we have identified the four different categories of the application. So, for example, the first one is object actuation. So like, the swarm robots can actuate the object like, for example, the tasks in the desk so that they can clean up before the user is starting using it. Or they can, you know, the- you know, the providing a tool whenever you use. Also, this robot can be used for- to provide a physical affordances. For example, you know, if the coffee cup is too hot, then the swarm robots can gather together to- to make a, you know, vertical (inaudible). So that- to indicate, you know, you shouldn't touch because it's too hot. But, once it's ready, you know, the robots can disperse and disappear into the environment. So, these robots can kind of organize the existing environment to provide, you know, the physical affordances. And also, this robot can be used for the displaying information like, for example, the sin curve or the- this one is showing the population data for each state so that we can see and touch and interact with it. Or construct a shape. Or showing the character. So we submitted to UIST, so why not? Yeah. And also, we can use it for physical interactive, you know, tool. So, to synchronize between the digital world and the physical world. Like, for example, if you work- if you're working on a CAD software, then these robots can not only show the output but also uses- use as an input tool. But also this robot can, themselves, become a tool like a ruler, for example in this case. So the robot can, you know, become a provider tool, you know, on demand. So these are kind of the four different categories how we can use these swarm robots for everyday assistance. So the swarm robots can deploy and blend into the environment so that they can naturally augment existing environment. But, we are also interested in how we can, you know, not only move on the tabletop size, but also in the scale up to the largest scale. Because we are interested in, you know, the how we can scale up to- not only to reconfigure the object within tabletop size, but we are surrounded in a, you know, large object in like a space vector, so the how we can actuate or, you know, reconfigure the environment at a larger scale. So to do that, we have developed shape-changing swarm robots that can, you know, rearrange the furniture. So these swarm robots can go underneath the table, or the chair, or the furniture, and pick it up and then, you know, they lift up and move to rearrange the furniture in a more programmable way. So, particularly, this approach it's interesting because when we try to make the environment more programmable, or interactive, the typical approach is to hack existing- existing table by attaching the motor or actuator. But, this robot can, you know, adjust the height and lift up any kind of common, standard furniture so that they can actually deploy into the environment and then they can, you know, naturally adapt to the environment. So that, for example, this robot can move this kind of standard, common furniture we have, we tested. And then, there are so many application domains, I think. Like, for example, using the spatial reconfiguration for adaptive environment. Like, for example, if you press a button then this robot can immediately reconfigures a space by rearranging furniture from the meeting space to the kind of interview space, or you know, the discussion space based on the calendar event for example. There are also, you know, we can use it for- to provide haptic interfaces for like augmented reality or virtual reality to match between the virtual world and the physical world. So, in this project, we specifically explore the second application. So here is, you know, in a model reality we are in seeing, (inaudible) seeing, like, you know, the new room, a new office you're designing. But, this kind of virtual object we cannot- we cannot see, but we cannot touch, interact- interact with, or sit on it. So, the idea is what if we can be reconfigure- reconfigure the physical environment to synchronize the virtual world and the physical world so that we can also, you know, not only see the virtual object, but also can touch the object. So, in this way- so, these swarm robots can continuously, you know, they are rearranging the spatial environment, so whenever you work or whenever you teleport, then these robots can construct, you know, the physical space based on the virtual, you know, virtual reality so that whatever you see, there is a actually kind of physical object, physical chair, or physical desk you can touch. So, there are several, you know, interaction techniques. Like, for example, if you teleport in a virtual scene, then these robots can construct- reconfigure the physical environment around you to match to the virtual scene. As well as, you can use it for if you virtually edit the scene, then they can also physically change the scene. As well as, using the locomotion capability of these robots to mimic, like, a larger scale, you know, of the desk to- like a- to provide a video (inaudible) for that. So, this actually how- how we build, and the good thing of that is it's pretty low cost and easy to make. So, it's actually a Roomba and we attach this laundry rack we bought in Target. And then, yeah, we attach on with a linear actuator to, you know, to extend it. So, the nice thing about it is the research project- research prototype is- usually kind of become useless after the project. But, we can actually use it for- as a laundry rack at home. (Audience Laughs) It's also programmable. So yeah, we can actually program the laundry rack to continuously change the sunlight, you know, at home or something like that. And then, we are using retro-reflective marker and the motion capture, you know, to track the robots as well. So these are- these, you know, the scissor structure can extend from 20 centimeter to 100 centimeter. And then, we also attach to the (inaudible) marker for the furniture as well, so here we can continuously track position of both the robots and furniture. So, again, this one is one specific application domain about how we can use swarm robots to reconfigure the environment to match to the virtual scene. But we could also- we are also interested in how we can deploy into the environment to reconfigure the environment. So these two are more like how we can use it as a everyday system. So there is no- but we also- we are also interested in using swarm robots as tangible interfaces, or the tangible medium that we can interact with it. So, the next project that I want to show you is to- the using these robots to make the data and the information accessible for- particularly for blind user. So this project started from the 3D printed picture book project led by Professor Tom Yeh, who is one of my advisors. So this project we are trying to make the picture book tangible for blind children. Because the blind children cannot view the picture book, but they need to touch the information to read it. So, the idea is how we can make the picture book more tangible so they can touch the information- touch the book to read it. So, typically to do that, you know, we often use tactile graphics, like- like this- like using embossed paper to make a bump to- to draw, you know, graphics on top of the, you know, the paper. But, these tactile graphics are static, so we- it cannot embrace dynamic content on top of it. So, for example, if we are interacting with tactile maps then the- if we want to, kind of, ask the question like where is the nearest coffee shop around here? So, if you interact with graphical user interfaces, it's so easy to program the dynamic content on top of it. But, in the physical, you know, the static graphics, it doesn't have any capability to dynamically, you know, change the content. So the idea is, what if the using this kind of tiny swarm elements to point out the location when you ask the question, like oh, where is the coffee shop? So this object can- these swarm elements can point out the location so that they can embrace the dynamic content and they can also touch the information through it. So to enable that, we have investigated several different approaches. So one of the which is obviously kind of pin-based approach, or kind of off-the-shelf kind of swarm robots like that. But, for blind users, these kind of objects are pretty big. Like, the swarm robot cannot be smaller than 3 or 5 centimeter, but they need to touch, like, a more precise location, like a millimeter scale. So one of the promising approaches is using electro-magnetic coil, or electro-magnetic force. So, because using a plastic magnet so we can scale it down to the millimeter scale, but one of the question is cost and the scalability, because this one is from the (inaudible) of the patent on it he or she issued, that they need to have- they need to construct a crazy number with the core to actually, you know, construct the electromagnetic force. So one of the contribution we had is the using a PC- developing a PCB-based electromagnetic actuation so that the having a coil or the (inaudible) with the PCB, then the (inaudible) current the coil- through the coil for each coil and generate an electromagnetic force to attract the magnets nearby location. So that by sequentially turning on the coil, then we can actuate in any arbitrary x and y position, or something like that. And, it's also pretty thin and scalable so, I already have here, but, you know, it's- it's pretty thin and we can also scale it up to the larger- to cover the larger display. And it's also pretty, you know, cheap compared to the manually constructed (inaudible). And it also can- you can also actually, kind of, order to (inaudible) and get this board in two weeks, for example. So it's pretty, you know, accessible way to do it. So, there are also several different applications, like spatial navigation, to data analysis, or guided drawing, and we have the- the- we have an user interview with six blind people to identify the workability and potential benefit of that. So, they are pretty appreciated by the low-cost and the large scale of size of the display because the refreshable Braille display often becomes a small size so it cannot, you know, embrace a larger display. But also, the mobility can increase independence, so they can bring it to whenever they go. Also particularly, they are excited about, you know, they can bring it to the classroom so that making the learning more- more, you know, accessible or adaptive based on the- like, synchronized between what the teacher said and what they can share. So this is the kind of, you know, the sort of category about how we can use a swarm of the actuated elements to make the data and the information more accessible. But, all of these kind of swarm elements are only capable to move on 2D surfaces. But how we can go beyond the 2D? Like, how we can construct and assemble the 3D shape (inaudible) to make objects that we can interact with. So, when I think about this idea, I made up some kind of concept video with stop-motion animation. So, for example, you can grab the object out of the computer screen and then you can also- so these are kind of physical objects, so you can actually interact with it and and then these objects are also digital, you know, digital things so that you can, you know, seamlessly integrate between the digital and physical world. So, this is a kind of idea what I want to make. The- think about it and then we have an idea about how we can make it. So, this is a kind of idea. So, we have- we have a kind of pin-based display and we're gonna stack the blocks on top of it. Then, the actuating pins to push up this blocks. By the way, here is a kind of actual blocks, like that. Then they construct layer by layer to- to create the kind of arbitrary 3D shape. And this 3d shape are also made of the blocks, so if you pushed- press down to the ground then yeah, it's gonna be deconstruct- be usable for the next iteration. So, to make it that, we have first tried to make the, kind of, pin-based linear actuator. So, these are kind of the the motor for each column. And then, we have- we made a kind of 24 by 16 to actuate the pins, and then stacked the blocks on top of it so that this pin can push up from the bottom to construct a shape. For the block, we used a 3D printed- 3D printed blocks and embed three magnets inside it so the edges- the blocks can, you know, connect and disconnect in horizontal ways. And then, by pushing up selectively, computationally, these blocks can come up to- from the- from the surfaces to make a object layer by layer. So to- for the trick for the disconnection and connection, we use the spacer between- between each blocks. So, this spacer can prevent the magnets- connect magnets- from connecting each other, but once we push then this spacer is gonna be gone so the magnets can connect together. And then, using weaker magnets for the vertical connection so that we can push up and then, yeah, we can construct a kind of over- overhanging structure. So in this way, we can construct- dynamically construct and assemble arbitrary 3D shape just like a 3D printer. So, for example, here is a actual real time, you know, video and you can see that this object, you know, assemble by pushing up the pin, and it can also create an overhanging structure, like a miniature table like that. So, for example, the (inaudible) can dynamically construct this shape without waiting for an hour for the 3D printing to finish. This object can also, you know, be reconstructible. So if you push it down, then yeah, you can make a new object out of it. Again, it's also, you know, dynamically programmable, so- and a layer-by-layer board can make arbitrary 3D shapes, so, yeah, we can also make the kind of low resolution stuff, like the bunny, like that. So, this is a kind of- the how we can use the swarm elements to not only move on our tabletop- not only move on a tabletop, but also construct a shape out of it. So, we can use the swarm robots to provide physical affordances. The swarm robots can also reconfigure the environment by the arranging furniture, but- but swarm robots can also make data and information accessible for blind users, and the swarm elements can also construct (inaudible) to make a shape. So these are kind of four major projects for the, you know, the main contribution, how we can use- leverage the swarm robots for everyday environment assistance, to tangible interfaces. The- the robots- the definition of the robots are also kind of expanding recently. So, from the mechanical robots to recently, kind of, soft and the pneumatic-based robots are kind of emerging in the field of the robotics and as well the HCI. So, in other research highlights, I've been also developing a novel modular soft robot and inflatable actuator for the shape-changing interfaces. So for example, the first one is about the MorphIO. So in this project, we developed the novel entirely soft sensor- sensing and actuation module for the soft robot that can sense the deformation as well as the actuator. So in this module, we have a conductive sponge sensor in it so that they can sense the deformation. So, if you deform it, then they can memorize the motion and in the same way, they can actuate the same motion. Like, for example, the programming by demonstration of soft robots. So again, this module can, you know, the dynamical sense of deformation so we can propagate the change. So this is how we can measure the deformation. So the using a conductive sponge to measure the resistance between bottom and top to, you know, to see the deformation - to sense the deformation. So, interesting application is these are kind of soft and adaptable to the environment or object so that we can attach to the stuffed animal to, you know, dynamically- remotely kind of control the prototypes motion of these these robots. These are also kind of modular, so we can quickly prototype the soft robots to construct the new geometry and we can also, kind of, prototype these by coding- not coding on the screen, but programming through the tangible interactions. And also, this one is- we published to the TEI (inaudible) that the using a pneumatic actuator to make a modular, you know, the room scale shape-changing interfaces. So, for example, the using the plastic tube to inflate the object- inflate, you know, the object, but also using constant force springs to naturally, you know, (inaudible) the actuator. So, it has, kind of, 20 centimeters by 20 centimeters, and using the air supply and the air release hole to- by- by supplying air to extend it, and also they are interacting with deflating, Again, it's also the modular, so that we can easily combine to the horizontal direction or in a vertical direction so that we can combine to the kind of- to 1D surfaces, or we can also a dynamically, you know, extend in horizontal surfaces. So for example, one of the application could be, like, using this- the object for- to provide haptic interfaces for virtual realities. And also, the interesting thing is that this is a compact, so we can make it in a horizontal direction to provide a kind of shape-changing wall. So, for example, these are, kind of, showing a character. We submitted to TEI, so why not? And then, we are showing like an arrow shape to construct this kind of room scale size. So, we are proposing these molecular prototyping environment- prototyping tool for the room scale shape-changing interfaces. But, again, these are, kind of, pretty kind of low-cost, you know, scalable approaches. We are also interested in how we can combine the material size of this actuation to make it more environment- more dynamic and interactive. Alright, so here is the summary. Again, so my research question is how we can make non-programable physical environments more programmable. So how we can computationally control physical elements of this, you know, the everyday environments more programmable. So to answer that research question, I've been working on- to using distributed swarm robots at various scales, from the fingertip size to the room scale size, to make a everyday environment programmable. So, the swarm robot can collectively, you know, actuate the object, construct a shape, as well as reconfigure the environment for human-computer interaction. And, I'm also taking this approach because, again, I think this is a pretty promising approach to make- make the interfaces more deployable to our everyday environment while maintaining- maintaining a general-purpose programmability of the swarm robots. So, here are the main four categories of that- what we can do- what we can use it. So, we design the application and interaction technique through the providing affordances, to rearranging furniture, to- for accessible purposes, and provide dynamic, you know, shape construction. Alright, so let me briefly talk about the future- future work and future research during directions. So again, my research future is to- in the next 20 to 30 years- swarm robots are almost becoming more like IoT devices today. So, the swarm robot can embed it and distribute it into the everyday environment. Then, the- this robot can, you know, seamlessly blend into the environment. So in my previous work, I have tried to develop the proof of concept prototype to envision this kind of future, but if we really want to deploy into the real world environment, we have so many aspects of the program we need to solve today. So for example, like a software problem- a software aspect, and hardware aspect, and a programming aspect, and the user interaction aspect. So for example for the software aspect, we need to have better, robust, and easier to integrate the tracking system as we have the scene understanding system- scene understanding capability because compared to the factory, our everyday environment more- more, like, messy, so we need to have a better understanding of the scene and how the user interacts with it. And talking about the hardware, you know, these robots are, you know, running a battery in an hour, so it's very, you know, frustrating for the maintenance. So, how we can- how we can solve that. As well as, these robots can only move around on top of surfaces how we can make it more, you know, the robots can climb up on the wall or moving on the ceiling. And the programming aspect is about, you know, these robots are mostly central- central control. How we- how we can program these robots in a more distributed way? As well as the end-user programming aspect of the swarm robots are also interesting. And finally, the user interaction aspect, you know, we need to better understand about how the people react with the swarm robots and how they, you know, interact with it. So, particularly about the first topic, so we have been using the kind of easier way to deploy, because our main goal is to make a proof of concept. For example, we use- using a fiducial marker on the bottom of the- on the bottom of the robots or using a motion capture system or sometimes using computer vision, but these are more like, you know, the not very robust, as well as, you know, the cursory tracking system, so how we can, you know, solve this kind of tracking mechanism problem. And also, the human-robot interaction aspect is how we can better communicate with the swarm robots. So for example, sometimes like Roomba robots is somehow very, you know, dangerous in the sense, you know, this robot can immediately kind of, relocate a chair if you want to sit down, so that we may probably need to better- have better channel to communicate with the swarm robots. So one of the projects I've been working- I have been working on is with a colleague at my university which is, for example, using augmented reality to see the intention or the feel the path of the drone. So for example, in this case the drones are also pretty dangerous, but by showing the path with augmented reality we can see the how- where the robot is going. So yeah, it's kinda interesting to how we can leverage the augmented reality to better support- better communicate and better support the understanding between the robots and the human. And also, how we can make the programming a little but more distributed in the sense of the user interaction perspective, so each swarm element can have a state or the program and then we can combine it to- to make a new state, for example. So this is kind of in a point wise future research direction. But, if we think about the design space of the existing swarm robots that I showed you, is probably the design decision between whether we- whether we off-load the capability to the environment, or we can embed into each robot. So for example, like a computation and communication, or sensing or tracking capability, or actuation, or the power supply, or connection/disconnection capability. So for example, think about the Roomba robots, like reconfigure robots, so we can see it's computation and communication in the central, and using a tracking- using environment based tracking. But if we think about, for example, using simultaneous- simultaneous localization approach, like a SLAM-based approach, to- for- to attach the camera for each robot to make more, you know, increase the deployability. As well as, it's also somehow interesting- interesting to see how we can also (inaudible) for the central control, but also make the program distributed to each robot so that they can work more kind of scalable way as well autonomous way. And for the Shiftbots, you know, we are currently, you know, each- each robot has a battery, but what if we can use wire transferring- a wireless power transferring to reduce the size as well as maintenance for these robots. So we actually, you know, we're collaborating with the researcher at the University of Tokyo to- trying to, you know, achieve it. As well as, you know, what if these robots can also not only move around, but also connect together to make graspable robots out of it. For this one, so most of these capability- so these blocks are only having, you know, capabilities to connect and disconnect, but what if, you know, we can embed a tiny, you know, a tiny sensor into each robot so that each element, each block, so that the block can sense how the user is currently constructing a shape. Also, you know, for this form we can only actuate- actuate the magnetic object because using electromagnetic force, but we are interested in what if using vibration-based actuation, like a piezoelectric actuation, to actuate any arbitrary shape on top to the table so that we can not only actuate a magnetic object, but also any kind of objects. So these are kind of, you know, the research- the direction I'm interested in and this is also kind of research, you know, the design (inaudible) how we can off-load the computation or communication or actuation power supply into the environment or to each swarm robot. So there are kind of each space how we can augment this kind of work. Again, so this is the kind of research future. So in the next decades, we are trying to make the robots almost equivalent to the IoT devices today. So we can surround it by the robots and then the question is how we can interact with the swarm robots. So through my work, I try to make the proof of concept prototype, but making- actually making the swarm robots truly ubiquitous and autonomous we have so many problems we need to address, which I'm very excited about working on the next five to ten years. For the long term, so the swarm robots can only interact- can interact with the existing environment to make our everyday environment more programmable. Another way of the direction is- there is also the concept about, you know, the making all the tech out of this robot so the robot itself could be a kind of material. Like, so the programmable material- programmable matter approaches has been working like 50 years now. But I think the swarm robots approach is somehow, they can cover the same- both sides because they can also actuate- reconfigure the existing environment, but they can also correct every move to make a shape like for example, in Disney's Big Hero 6. So yeah, these are kind of dynamic objects out of the many (inaudible) and, you know, the swarm elements. So, I do think the swarm robots approach is not trying to solve the problem today, but also towards 50 years from now to the Ivan Sutherland "Ultimate Display." So Ivan Sutherland, who is a founder of- founder of augmented and virtual reality, once wrote the Ultimate Display would be, of course, be a room to- to which- to computationally control the existence of matter. So, I think we are also working toward this vision that, you know, yeah, not only for the solving the problem today, but also this project could be going towards programmable reality to make- to make the environment truly dynamic and interactive and programmable. So finally, I want to thank my collaborators, and advisor, and thesis committee. So, without the continuous support with them and continuous interaction with them it was not possible. So I really thank particularly Mark Gross and Tom Yeh, and Hiroshi Ishii and Takeo Igarashi for the continuous support to make it happen. So, this is my work. So thank you for listening and I'm happy to take any questions. (Audience Applause) Shyam Gollakota: You have time for a few questions. Magdalena Balazinska: So you mention maybe using AR for- So I have a question. You mention maybe, like using AR to try to follow, you know, show to the users the path of the drone. More generally, what's your take on how can we design this environment in a safe way such that, you know, if people mistake how to program the swarm, or, you know, interact with the world in some way that kind of the environment remains safe? Ryo Suzuki: Yeah, so no, actually safety is one of the biggest challenges we have and then we are- So your question is not only leveraging augmented reality, but making a robot itself safe for many people. Yeah that's a important question, so maybe, it's definitely a combination with (inaudible) like a software, you know, the software augmentation. Like for example, using the camera- using camera- attaching cameras onto the robots or attaching the camera in the environment to continuously track the position of the robot- position of the robot and the people. But one of the challenges is it's becoming a little bit difficult to continuously survey- continuously monitor with the camera for the everyday environment because of privacy, so maybe, it would be very interesting to think about a different approach. Like for example, like, I don't know, the thermal sensing or something like that. So this is, I think, the kind of the research challenge we need to, yeah, we need to solve. But it's definitely the combination with sensing and (inaudible) to make it smarter. Audience: So, usually when I use applications, I only need to know a handful of metaphors that work across many applications so that I can effectively get them to do what I want. Across these projects, were there- was there some sort of small set of common metaphors a user might learn to control in all these different applications? Sort of a language of interaction that's emerging as you have more and more applications. Ryo Suzuki: Oh, I see. So... I see. That's... Yeah, that's an interesting question. We have like, we- we didn't develop, you know, the umbrella, you know, the interaction- the vocabulary across the project, so the- So we- we can identify the kind of technical challenges across, this project. But, no, the short answer is we haven't- we haven't investigated so much, but yeah. That's a really good question. Jon Froehlich: So, thank you for the talk, super creative. The key question I have, and I think you articulate this vision of twenty years out, thirty years out, we have swarm robots all around us like we have IoT devices all around us, so what is the value proposition? Can you help me understand like, two or three key societal level problems that might be addressed through swarm robotics? Ryo Suzuki: That's... So one obvious way to use swarm robots, is probably- I think I starting from the automation aspect, like home automation, so that we have- we could probably have, you know, there are so many kind of the labor intensive- How do you say? Labor intensive tasks that we need to actually use a human to move or actuate it, so I do think the- that would be somehow very- the important aspect. Like, if the people (inaudible) like immediately from the social aspect. So yeah, I do think starting from like home or maybe liberating for like a factory automation would be the- would be the very first step. But another thing is to, let's say, social aspect, yeah. So... So, yeah. So one aspect- so one is home and automation aspect but another is we also gradually need to have the physical objects we need interact, because we are gradually- the computation is becoming gradually out of the computer screen, now we are having like a virtual reality or augmented glasses that we need to have a gradually better input devices to interact with it. So yeah, I do think it's also, yeah, the- kind of the growing needs, how the society can leverage these swarm robots for, yeah. (Silence) Shyam Gollakota: Alright, let's thank the speaker again. (Audience Applause) 