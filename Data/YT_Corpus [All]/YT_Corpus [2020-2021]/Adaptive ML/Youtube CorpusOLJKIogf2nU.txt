 [Applause] [Music] you [Music] [Music] [Applause] [Music] [Music] on behalf of data bricks welcome to spark in AI summit 2020 my name is Alec Otzi I'm the CEO of the Eric's and I'm looking forward to being your host over the next couple days we're excited to bring together 60,000 people from around the globe for what has quickly become one of the biggest data in AI events in the world now more than ever spark an AI summit is truly a global community event we've got an incredible lineup of speakers content and experience this plan for you this week we have over 250 trainings tutorials lots of different sessions both live and on-demand and I'm especially excited about our guest keynote lineup featuring people like nate silver from 538 professor jennifer chase from UC berkeley kim Hazelwood from Facebook and dr. Philip atiba golf co-founder of the Center for policing equity also please explore everything else that this virtual summit has to offer including our developer hub advisory lounge and much more you can find it all in the left nav Weaver chill summit dashboard also I want to take a moment to thank all the organizations who are helping put this event on we wouldn't be here and we couldn't pull this off without these important sponsors please make sure to pay them a visit in the dev hub of the expo with that I want to spend some time talking about why this event is so important to us at theta Brix our mission has always been to help data teams solve the world's toughest problems this is a mission that we share with the entire data community nobody understands the power and vast potential of data and AI like this group and this event is a great manifestation of that it's an incredible opportunity to learn from each other share new ideas and move the industry forward and today I'm sure you all agree that solving the world's toughest problems has never felt more urgent as an example racial injustice continues to be a systemic issue that is impacting the lives of so many recent events and protests around the world remind us that individuals organizations and governments must continue to do more to bring awareness to racial injustice and drive meaningful change well it's of course so much more than a data problem some organizations are using their science to tackle some of its systemic routes a great example is the Center for policing equity the Center for policing equity uses data science to reduce the cause of racial disparities in law enforcement they use advanced analytics to diagnose gaps in policing and help police build healthier relationships with the communities they serve I hope you all attend the afternoon keynote sessions especially the one where CPE co-founder dr. Phillip atiba golf will talk about the important role that data community can play in addressing racial and social challenges we are inspired by the work that the Center for policing equity is doing and have decided to sponsor them here at the SPARC in AI summit in addition to CPE we also decided to sponsor the n-double-a-cp Legal Defense and Educational Fund the n-double-a-cp was formed over 100 years ago and its Legal Defense and Educational Fund has emerged as one of the country's best legal organizations that fights for justice on behalf of the african-american community through litigation advocacy in public education LDF seeks structural changes to expand democracy eliminate disparities and achieve racial justice so in order to support both of these organizations we're setting an ambitious fundraising goal for this event with your help we want to raise $100,000 by the end of this event we made it super easy for you to donate at any given time by clicking donate now button on your navigation panel but I'm also really happy to share the data books will match all donations up to $100,000 so I sincerely hope you'll consider making a contribution it would be amazing if we together could raise $200,000 over the next few days when we talk about solving the world's toughest problems it's also hard not to think about the disruptive impact of kovat 19 the virus itself is of course a massive problem but so are the many issues associated with mitigating its impact not to mention the lasting impact it will have on entire industries on where and how we live and the global economy we are living in an unprecedented times what inspires me personally and hopefully it inspires all of you is the role that we can all play in solving problems like the one we face today more than ever before this is our time it's the data community's time to unlock the full potential of data and AI but why now well one because the world understands and acknowledges that potential more than ever before too because technology has evolved and things are possible now that were unimaginable only a couple of years ago and third because as a data community we're more than ready willing and capable than we've ever been before as it relates specifically to the kovat crisis it has been inspiring to see how data teams are answering the call as an example Rush University Medical Center is one of the top-ranked hospitals in the United States they are doing data analytics services for capacity tracking across multiple hospitals in Chicago what stands out about their story is how they were able to build this so quickly across multiple hospitals with multiple data teams and multiple data sets this probably wouldn't have been possible a couple years ago and while technology advancements have a lot to do with this it also has a lot to do with how projects like this are tackled and at theta Brix what we've learned from working with organizations like Rush is that data and AI is a team sport every company wants to be a data company if you think about it what that actually means it requires a new way of empowering people working with data enabling them to organize around the data they need collaborate and get to the answers they need more quickly it's about the right people coming together with the right data as quickly as possible unfortunately most organizations aren't able to operate this way today most data teams aren't set up for success because they're stuck in silos defined by closed and proprietary systems they're working with fractions of the data that they need often incomplete or inconsistent they spend more time chasing data looking for latest versions figuring out who can give them access and not nearly enough time exploring analyzing or deriving value from it so in order to succeed data teams need to evolve they need to evolve from being separated by data to being unified by data agile dynamic connected to each other so that they can move fast and do their best work that's why our theme for this year's conference is data teams unite it's a call to unite around the data unite to do our best work and help others do theirs unite to innovate faster and to do so at a time when the world needs it the most in fact we technically launched this theme a couple months ago with our data teams unite hackathon in it we asked data teams to come together and innovate for social good by focusing on solutions to help address the covert 19 crisis climate change and social issues in their communities as prizes data bricks will be making donations to the Charities of the winners choice $5,000 to go to the third place 10,000 to the second and 20,000 to the first we've received a number of great submissions and I will announce the winners in tomorrow morning's opening keynote please don't miss it the other teams unite also captures the spirit of this event and of the open source data community bringing their teams together has long been the vision of Apache sport and it's been really exciting to watch the evolution of this community in fact this year Apache spark turned 10 years old there's nobody better suited to talk about it than the guy who created it Matea haria it's a happy - 10 years old Oh thankfully this looks great so today is a very special year for Apache spark spark people I know is out but it's also ten years since the first open source release of spark so I want to talk to you today about what got us here with the open source project and what did we learn in the process that were using to contribute to Apache spark development in the future and to do that I'll start by telling you my story with big data how I got into this space when I started my PhD at UC Berkeley so I started my PhD in 2007 and I was very interested in distributed systems I had actually been working on peer-to-peer networks before this but I wanted to try doing something different and as I looked around at what's happening in the industry I got very excited about data center scale computing these large web companies were starting to do computations on thousands of machines and store petabytes of data and do interesting things with them and I wanted to learn more about these and so I actually worked with some of the big data teams at Yahoo and Facebook early on and in working with these I realized that there's a lot of potential for this technology beyond web companies I thought this would be very useful for saying different data sets industrial data sets many other things but it was also very clear that working with them these technologies were too hard to use for you know anyone who has not at a large web company in particular all the data pipelines to you know to use these technologies have to be written by professional software engineers and also all the technologies were really focused on batch processing there was no support for interactive quays which is very important in data analysis no support for machine learning and advanced algorithms and when I came back to Berkeley after spending some time working on these I also found that there were local research groups who wanted a more scalable engine for machine learning in particular so I thought this was a great opportunity to try building something in this space and in particular I worked early on with Lester Mackey who was one of the top machine learning students at Berkeley in here and who was on I was on a team for the Netflix price competition this million dollar competition to improve Netflix's algorithm and so by seeing the kind of applications that he wanted to earn I started to design a programming model that you know would make it possible for people like Lester to develop these applications and I started working on the spark engine in 2009 now once I started the work the feedback on it was great these machine learning researchers at Berkeley were actually using that and so we decided to open source the project in 2010 and this is what the initial version of it looked like so it was very small open source project really just focused on you know MapReduce tile computing with a cleaner and faster API but the exciting thing is that within two months of us open sourcing had we started getting pull requests from other community members so there were really people picking this up trying to use this very you know early project I'm doing interesting things with it and so we spent the next two years working with early users in the Bay I spent a lot of time actually visiting these companies and organizing meetups and trying to you know to make this a great platform for a large-scale data processing and in working with these early users we were blown away by some of the early use cases people were doing things that we as researchers had never anticipated and we thought that there's the potential to do a lot more here so for example some of the early users were powering interactive operational apps like a user interface using spark on the back end for example there was a group of neuroscientists that janelia farm that were monitoring data from like you know the brains of animals and other sensors and real-time while they're running neuroscience experiments and and built building that up using spark that was something we had never anticipated it's obviously very cool to be able to build these operational apps for exploring large data sets another interesting group was from this startup company iguana finds that have built you know product for its customers to analyze social media and other data sources and they actually were using spark to update data sets and memory and implement streaming computation this was before we added a streaming engine to spark so they had just built streaming computation on this platform and finally there were groups in industry for example Yahoo's data warehousing group that that were running sequel queries over spark or our data science were closed and are there workloads and they were opening up the spark engines to tens or even hundreds of more users then than we initially you know had had been targeted so based on these use cases and what spark was was able to do for these companies we spent the next few years really working on expanding access to spark making sure that anyone who works with data can somehow connect to this engine and be able to run computations at scale and there were three major efforts there or the first one was around programming languages so we developed Python R and sequel interfaces to spur emulate today I by are by far the most popular ways of using the engine the second was around libraries we wanted to have great built in libraries for machine learning graphs and stream processing and these provide a lot of the value of the platform today and finally we also got a lot of feedback on the API what users found difficult you know for example how to set up map and reduce and other operators to do a distributed computation and we decided to make this big change to the high-level API called data frames which owns most of the API is in spark on top of the spark sequel engine so you get the same kind of optimizations and quite planning that a sequel engine will do but in these easy to use you know programming language interfaces and that quickly became the dominant way to use a patchy spark so if you look at apache spark today these changes have all had a huge impact on the project let's start with support for python among the users of interactive notebooks have data base so we can measure this 68 of the commands coming into the platform this is more than six times the amount of Scala it's by far the most widely used programming language on database today and it means a wide range of software developers are able to connect their code to spark and execute an at scale interestingly the next most common language and notebooks is sequel there's also a similar amount of commands to the total that we have in notebooks is just coming in or through sequel connectors directly so that's also very widely used now next thing is sequel so even when developers are using Python or Scala about 90% of the SPARC API calls are actually running on spark sequels through these data frame api's and other api's which means they benefit from all the optimizations in the sequel engine and just I'm data breaks alone we're seeing exabytes of data equate per day using spark sequel as a result the community has invested a lot in the spark sequel engine and making it you know one of the best open source sequel engines on the planet so for example this blog here shows the improvement and performance on the TPC Dias benchmark the leading benchmark for data warehousing workloads and you can see what sparks a point no spark is now running two times faster than the previous version and also about 1.5 times faster than presto very highly regarded sequel engine for large-scale data sets and another exciting piece of news that happened with sequel this year is that earlier this year Alibaba has set a new benchmark for the TP CD as a new record for the TPC das benchmark using the spark sequel engine to exceed you know the cost performance of all the other engines that have been submitted to this benchmark so it's really the basis of a very efficient and powerful sequel engines today and the final one I want to talk about is streaming just on database alone we see about five trillion records per day processed with structured streaming which makes it super easy to just take a data frame or sequel computation and turn it into a streaming one and this number has been going by close to a factor of four year over year so we're seeing very fast the option of this high level streaming API okay so given these changes of the project what are the major lessons that we learned for me at least there were two big lessons the first of all the first one is to focus on ease-of-use pirate eyes dad for both data exploration and production we found that a lot of the applications that people built quickly became these operational apps or streaming apps or repeated you know reports and we wanted to add a lot of features into into the engine to make it easy for these to keep going and to tell you you know if something changes or something breaks in in a way that's easy to fix so a lot of the work in spark now is to make that super easy the second big lessons we had was to really design the system around API is that enable software development best practices and integrate with the bad ecosystem so we designed all the API since spark so that they fit in the standard programming environments like Python and Java and so that you can use best practices like composition of libraries into an application testability and modularity and you can build packages that a user in your company can can safely use to do a computation or have this wide open source community around them and this is something where we've done a lot of improvements over time as well okay so given these lessons what's what's happening in Apache spark Sepoy know this is our largest release yet with over 3,000 patches to the community and it's actually designed to be easy to switch to from spark 2 so we definitely encourage you to check it check it out and you know switch to ant 1 you can um this chart here shows where the patches have gone and you can actually see almost half the patches are in the spark sequel engine both for sequel support itself and because it's an underlying engine for all these data frame API calls so this is you know the most active piece of development but there's of course a lot else going on as well so I just want to highlight a few of the features that I'm excited about focusing specifically on sequel and Python but of course you know there's there's quite a bit of other stuff going on in 3.0 as well and I'm gonna start with a major change to the spark sequel engine the largest change in recent which is called adaptive query execution so this is a change where the engine can actually update the execution plan for a computation at one time based on observed properties of the data for example it can automatically tune the number of reducers when doing an aggregation or the join algorithms or it can even adapt to skew in the data to plan the computation as it's going and this makes it much easier to run SPARC because you don't need to configure these things in advance it will actually adapt and optimize based on your data and also leads to better performance in many cases so to give you a sense of how it works let's look at a simple example of setting the number of reducers we found that today about 60 percent of clusters you know the users tune the number of registers in them so that's a lot of manual configuration that you want to eliminate and automate and so with a QE what happens is after SPARC owns the initial phase of an aggregation it actually observes the result size you can have some number of partitions there and it can set a different number of partitions for the reduce for example coalesce everything down to five partitions and optimize it for the best performance based on what kind of data came out of that aggregation now even more interesting things happen with joins and and more complicated operators for example when you're joining two tables even if you have high quality statistics about the data it's hard to know how many records will end up before the joint and using a QE SPARC can actually observe this after the initial stages of the joint have happened and then choose the joint algorithm that best optimizes performance downstream and in fact it can adapt both to the size of the data and to the skewer on different keys so you don't have to worry about tweeting skewed keys in a special way anymore and these results in really big speed ups on sequel workloads so for example on TPC des queries we've seen up to a factor of 8 speed up with with a QE and even better it means you can actually run SPARC on a lot of data sets even without pay computing statistics and get great performance because it discovers the statistics as you go along ok that's just one of the changes that affects both sequel usability and performance or there's quite a bit more so on the performance side we have dynamic partition pooling quakin compile time speed ups and optimizer hints and as I said earlier these have led to a 2x reduction in an execution time for DP CDs and a lot of improvements are real workloads and finally there's been a big effort in 3.0 on sequel compatibility in particular and an SI sequel dialect that follows the standard sequel convention in May many areas of the language and that makes it very easy to point in workloads from other sequel systems and this is one of the areas that were continuing to invest in by we've made it we've made significant strides in 3.0 ok so that's a bit about sequel or the other thing that I wanted to highlight is around Python so we've done quite a bit of work on both Python usability and performance for usability we've made it much easier to define panda's user-defined functions using type hints in Python which let you specify the format of data you expect so in SPARC it's easy to make a function that they send batches of data spanned a series or data frames for example or even as an innovator of series and you can just specify these with type heads for comparison the previous API required you to write a lot of boilerplate code in order to say what kind of input your function expects and that was variable so now that can go away and it's it's super straightforward to define and use these functions there's also been a lot of work on performance using Apache Aero we're seeing about 20 to 25% speed ups in Python UDF performance using the latest enhancements in Apache hello and also up to a 40 times speed up in spark or by using our to exchange data between our and spark and these are all transparent to the end user you just stop great and you have these speed ups and we have quite a few new api's for combining pandas with spark as well and finally there features throughout the engine including our new structured streaming UI for monitoring computations a way to define custom observable metrics about your data that you want to check in streaming jobs sequel reference guide and a powerful new API for data sources so if you want to learn more about Apache spark 3.0 I invite you to check out charlie's stock and the summit and many of the talks on these individual features and the spark project itself is not the only place where things are happening there's actually a bot community around it and I wanted to highlight some of the changes so our data breaks last year we released koalas which is a panda's API that can run directly over spark to make it very easy to port workloads and that's evolved a lot in this year I'll talk about it of course there's Delta Lake for reliable tables storage and we've also done quite a bit of work to add spark as a scale out back-end in popular libraries including scikit-learn high part and job lips so if you use these for machine learning you can just scale out your jobs on a spark cluster we also collaborating with region on to develop glow a widely used library for large-scale genomics and NVIDIA has been developing Rapids which provides a wide range of data science and machine learning algorithms that can you can call from spark that is GPU acceleration and really speeds up these workloads and finally we've also done a lot of work on data bits on improving connectivity with both commercial and open source visualization tools so that users can build interactive dashboards using spark as the backend so I'll just dive into some of the changes in koalas specifically if you're not familiar with koalas it's an implementation of the pandas API over spark to make it very easy to port data science code in this really popular library we launched it actually a year ago at spark AI summit and it's already up to 850 thousand downloads per month which is about a fifth the total downloads of Pi spark so we're really excited with how the community has adoption adopted this library and we're investing quite a bit more into colas and at this event were actually excited to announce koalas version 1.0 this new release has close to 80% of the API coverage of Banda's it's also quite a bit faster thanks to the improvements in spark 3.0 and it supports a lot of features that were missing before including missing values and haze and in-place updates and it's also got you know faster distributed index type it's very easy to install koalas and pie pie and get started and if you're a panda's user we believe this is the easier way the easiest way to migrate your workloads to spark so that's enough of me talking we also like to show you demos of all these new things and I'm really excited to invite book wenig the machine learning practice leader data books to give you a demos of the new features on koalas and spark 3.0 hi everyone these are some crazy times and while we're all still social distancing many of us are staying at home trying to figure out what are we going to cook for dinner tonight I know my whole SF office has gone through the banana bread and sourdough craze but now we need some new recipes to impress our co-workers with on our food sock channel in particular we need to find a recipe for matang the tag is a very busy person who loves to optimized everything given the increase of people contributing recipes we now have millions of recipes that we need to analyze in a scalable manner so that metate can make a data-driven decision on what he's going to prepare tonight along the way we'll explore some new features in spark 302 and koalas let's get going like any data scientist I'm going to start off with some exploratory data analysis with pandas pandas is a great way to get started with exploring your data because it's simple to use has a great documentation and a fantastic community let's go ahead and visualize the subset of our data you can see here that our recipes data contains the name of the recipe when has contributed nutrition ingredients etc however we don't just have one file make a whole directory of park' files we need to read in so let's go ahead and load in our entire data set with parking with pandas unfortunately though pandas wasn't prepared to handle the rising recipe count so if we allow this query to continue on it'll crash trying to load in 30 gigabytes of data onto a single machine butts instead cancel it now let's use koalas to load in our data set instead koalas provides the pandas like syntax and features that you love combined with the scalability of Apache spark to load in our data set with koalas we simply replace any of the PD logic with KS for koalas and now you can see just how quickly we can load in our entire data set with koalas but let's same entities pretty busy tonight and he wants to find a recipe that takes less than 30 minutes to prepare you can see I've already written the panda's code to filter out for recipes that take less than 30 minutes and then visualize the distribution of the number of steps these recipes take to convert it to koalas I simply replace my pandas dataframe with my qualitative frame and voila I no longer need to downsample my data in order to visualize it I can finally visualize the big data at scale we can see here the distribution of the number of steps while most recipes take less than 10 steps you can see the x-axis extends all the way out to 100 there's a universal muffin mix that takes 97 steps to prepare but I won't be cooking that one tonight in addition to our recipes data we're also interested in the ratings for those recipes because koalas runs on a patchy spark under the hood we can take advantage of the spark sequel engine and issue sequel queries against our koalas data frame you'll see here that we have our ratings table now we want to join our ratings table with our recipes table and you'll notice I can simply pass in the koalas data frame with string substitution let's go ahead and run this query Wow we can see that this query took over a minute to run let's see how we can speed it up I'm going to copy this query and move it down below after enable adaptive query execution with adaptive query execution it can optimize our query plan based on runtime statistics let's take a look at the query plan that's generated once it's enabled I'm going to dig into the spark UI and take a look at the Associated sequel query here we can see that it passed in Eskew handling hint to the sort merge join based off of these runtime statistics and as a result this query now takes only 16 seconds for about a 4x feedom with no code changes in addition to using koalas to scale your panda's code you can also leverage spark in other ways to scale your panda's code such as using the pen dysfunction api's we're going to use the panda's function api's to apply arbitrary python code to our data frame and in this example we're going to apply a machine learning model after seeing that universal muffin mix take 97 steps in 30 minutes I'm a little bit skeptical of the time estimates for these recipes and I want to better understand the relationship between the number of steps and ingredients with the number of minutes as a data scientist I built a linear regression model in our data set using scikit-learn to predict the time that a recipe will take now I want to apply that model in parallel to all records of our data frame you'll notice here just how easily I can convert our koalas data frame to a spark data frame to apply our model I'm going to use the function called map and pandas map and pandas accepts a function you want to apply with the return schema of that data frame the function I want to apply is called predict predict accepts an iterator of pandas dataframes and returns an iterator of pandas dataframes I'm then going to load in our model if your model is very large then there's high overhead to repeatedly load in the model over and over again for every batch in the same Python worker process by using iterators we can load the model only once and just apply that to batches of our data so now let's go ahead and see how well our model performs we can see here that the predicted minutes is pretty close to the number of minutes for some recipes this case the true number of minutes is 45 we predict 46 are the true value is 35 we predict 39 but in some cases our time estimates are a bit off and we can actually see in the description don't be dissuaded by the long cooking time so Mateus decided he doesn't want to cook anything tonight he just wants to make a smoothie with the shortest preparation time so matei which recipe will you be preparing looks like matei will be preparing a berry berry smoothie with only four ingredients and a grand total of zero minutes wow he's really optimizing for speed stay tuned for a berry delicious surprise Wow thanks Brooke that was an amazing demo and the smoothie turned out delicious it's great one other announcement I wanted to make that also ties into book is that we've been working to publish a new edition of learning spark on the the popular book from O'Reilly book is one of the co-authors actually and we're giving away a free copy of the e-book to every attendee of the summit so we invite you to check this out if you want to learn about the new features in Sparks a Porno okay and then the final thing I want to end on today is what's happening next in the Apache spark ecosystem if we step back and look at the you know the state of data and AI software today it's obviously made great strides over the past ten years but we still think that data and applications are just more complex to develop them they should be and we think that we can make that quite a bit easier and we have a lot of ongoing efforts in open source Apache spark to do this building on these two lessons ease of use in exploration and production an API is that connect with the standard baud software ecosystem so I'm just going to briefly talk about the big initiatives that we are working on a database for the next few versions of spark so the first one is what we're calling projects and the goal is the great improve Python usability in Apache spark because it is the most widely used language now we want to make sure that Python developers have a fantastic experience that's familiar with everything else that that they do in Python and there are quite a lot of areas where we think we can improve the experience we've called this project and by the way after the Zen of Python the set of principles for designing Python that have led to it being such an amazing environment today so some of the things we're working on are better error reporting porting some of the API changes from koalas we got to experiment with them there and we think they're useful and after that we want them to just be part of spark improve performance and pythonic api design for new api's I'll give a few examples of what we're doing with this next we also have a continued initiative on adaptive query execution we've been really pleased with what is doing so far and we think we can cover more and more of the sequel optimizer ease decisions adaptively and really dramatically reduce the amount of configuration or preparation of the data needed to get great performance with spark and finally we're also continuing to push on an C sequel the goal is to run unmodified cuase from all the major sequel engines by having dialects that match these and we think we've been working a lot with the community to build these and we think it will provide a lot of benefit and spark just to give you a couple of examples of the projects and features one of them is around error messages so if you run a Python computation today and you have an error on your work up artists such as dividing by zero you get this very scary looking error trace in your terminal you know lots of stuff going on and if you look at it closely there's a lot of Java stuff and maybe you know you can see part of the era traces actually about the problem that happened in your Python worker which in this case was division by zero but it's pretty unfriendly especially if you're just a Python developer it's hard to see exactly what was going on so as part of project said were simplifying the behavior and python of a lot of common error types so that if it's a Python only error or sequel planning error you see a very small message that lets you directly fix the follow-up so you can see this has just the Python relevant bits of that ever and you can see that there is a division by zero of course this might be a bad example because if you really accept projects then you will come to a much better understanding of emptiness and you will probably never get divisions by zero again but it's just one example and then the second change I want to show is a new documentation site for Vice Park that's designed you know to make it really easy to browse to find Python examples and it's built on kind of the latest best practices for Python documentation and we think this will make it easier to get started for many of our users as well so these are some of the changes we're really excited about what's next for a badge a spark and we look forward to seeing what you do with it back to you Ali thanks but a hmm so which spark 3.0 we've now added broad support for sequel we've also integrated it with Python and in some sense it's really unifying now sequel analytics or data warehousing and data science and that's really exciting because that's what we think the future is going in fact our customers were unifying these two things for a while and actually a pattern has emerged in which they call this the lake house paradigm so I want to talk a lot about that today but before I dive into the details of the lake house I want to provide you some context of why we need it so data warehouses were built 40 years ago and there are purpose-built for bi reporting and they work really great for that however they don't have any support for video data audio data and text data which is a lot of the kind of data that we have today and as a result of that it's very hard to do data science or machine learning on these data sets in the data warehouses also they have limited support for real-time streaming which is becoming increasingly important and finally they're closed and proprietary formats so the data is either locked in in a data warehouse or you have to take it out if you want to operate on it with other tools therefore today most of the data is actually first stored in a data Lake or a blob store and a subset of it is moved into a data warehouse so let's look at these data links so the data leaks they're great in that they can handle all your data now and you can do data science and machine learning on it however they're bad at exactly the things that the data warehouses were good at so they have really poor bi support they're very complex to set up and configure you need advanced data engineers to do that and then once you start using them if you try to do bi ever on a data Lake you're going to find out that the performance is really really bad hence as a result what we're seeing is that a lot of these data lakes have turned into unreliable data swamps and actually most organizations unfortunately have to have both of these things side by side so that is why do believe in the lake house paradigm the lake house paradigm brings the best of both these worlds you get the structure you get the BI and reporting advantages of data warehouses and you also get the data science machine learning AI and real-time capabilities of data lakes on all of your data in fact the lake house is distinct from data lakes it actually starts out like a data Lake so it looks at the bottom layer like you didn't Lake but the crucial difference is that it has something that we call a structure transactional layer this layer brings quality structure governance and performance to data lakes annotator bricks we believe that open source Delta Lake project is a great way to get structural transactional layer on top of your data link so I'm going to talk a little bit about this next so before I go into the details of the Delta Lake open source project I want to tell you a little bit about its history and how it came about so Delta lakes are built on top of data mix and as we know data lakes they're great they're really cheap in fact they have ten nines of durability that can scale infinitely they also can store now all kinds of data a video audio text you can also store structured semi-structured and unstructured data in these and finally they're based on open standards formats in recent years we've seen in particular the park a format becoming the standard in this ecosystem and lots of different tools can all directly operate these parka phones so that's awesome and as a result of this organizations have now been promised that if they just build a data leak they can get all these benefits they can get real-time analytics they can get data science taking it machine learning looking at data warehouse saying reporting bi everything so they've been very excited they built up these data lakes unfortunately though most of these projects are actually failing and at theta Brix we started to actually see lots of lots of demand for professional services we had to put our own Solutions Architect on these customers to look at what's the problem that they're facing and what we did is we rank ordered the problems that we have to address with these customers and I'm gonna walk you through each of these problems so there's nine of them in the order of sort of severity that we saw and then I'm going to talk about how we address them with the Delta Lake project okay so the first problem that we actually notice that is happening over and over again is actually very trivial simple problem but it's actually that we were getting pulled in and customers were saying we have a hard time appending new data into the data lake while at the same time correctly reading it turned out some of the new data comes in if you try to read it at the same time because of eventual consistency you might not see the results so our solutions architects were creating copies of the data in different directories and switching it over when it was ready needless to say this was really really complex and costly second modification of existing data is really difficult with the italics and this is exacerbated when companies are trying to satisfy gdpr or CCPA so in GDP RSC CPA if a customer says I don't want you to store any records in your data sets about me you have to go and scrub those and delete those well that's really hard with traditional data mix because they're basically batch oriented systems so you have to run a big spark job all over your data set once a week to scrub it to make it compliant this was again extremely costly third some of our customers were having problems that were hard to diagnose after we looked into the logs and did diagnosis it turns out that many years back maybe some spark job failed halfway through some of the data that was in thing had made it in the rest was missing and as a result the whole project was failing for real-time operations are really really hard on data leaks this is a special case of problem number one but mixing real-time streaming with batch led to inconsistencies in particular if you're appending new data in batch operation but at the same time in real-time you're trying to read it you're gonna have problems and this one is actually hard to solve it's not just to copy files or copy directories five organizations want to keep historical versions of their data specially regulated environments you have to reproduce your datasets because there's auditing and governance that happens so here again the poor man's version was keep copies of all your data sets and different directories with different date names very very costly six difficult to handle the metadata itself now as these data lakes was were growing the data itself was growing into petabytes the metadata for these systems was itself now becoming terabytes in size and as a result of this the catalogs they're storing the metadata were falling over they were not scalable so we were pulled in to do professional services again to fix that seven the too many files problems because data leaks were fired oriented you could end up in a situation where you had millions of tiny files or a few gigantic files again leading to lots of problems when we were trying to consume it eight it was hard to get great performance because you're just dumping your data into the data lake without thinking about how it's gonna be used later how do you know if the data layout is sort of optimized for the performance that you need later and finally the most important problem in my opinion but a hard one to actually know was data quality issues it was constant headaches where customers would pull us in and it turns out that the data was changing in a subtle way or the semantics was changing over time or all of it wasn't there so this was leading to all kinds of problems with these projects so at theta Brix we wanted to take an opinion it's approach we said we want to build a system that from the get-go tries to get all these things right so that we don't have to ever configure them or ever have to address these issues ever again so the Delta Lake open-source project addresses this by adding massive reliability quality and performance to your data Lake and the way it does it is by bringing the best of data warehousing and data lakes to one place together and finally it's based on open-source park' format so you can keep all your data on your data Lake in this open source format with all these benefits so how does it do this so these were the nine problems that I mentioned earlier and I'm gonna go through each of these and mention how we actually address this with a Delta project well luckily it actually turns out the first five problems can all be addressed with the same technique by having a transaction log that stores all the operations that you're doing on your data Lake you can actually now make it fully atomic that means that every operation that you do on your data Lake either fully completes or it aborts and cleans up any residue you can try it later the way that actually happens is that there is a delta log and it stores every operation so for instance here we have an example where one operation is adding two files to park' files they will atomically be added then after that maybe that first file is deleted and a third file is added the key thing here is that Delta always makes sure that each of these are happening as if it was sequentially executed okay so that's awesome now we can make sure that one if we're appending new data into the system it's always either gonna be there or it's gonna be canceled and you can read it at the same time you won't have any issues to modifying existing data if you want to get gdpr or CCPA compliance actually is really easy now because transactions enable you to do fine-grained observes so you can go in and modify small data in your data set in a transactional manner jobs failing Midway that also goes away because the job ID are fully succeeds or fully is aborted and then real-time operations it's the same thing all operations where the real-time or batch are consistent with the transaction log and now finally because we have all the deltas of all the operations that happened on your data set we can actually implement something called time travel time travel lets you submit queries and then say I want the answer to this as of the time two years ago and it gives you the response as if the query was submitted two years ago so this is great now for compliance and governance okay so that's awesome now we solved those first five problems how do we deal with the metadata problem the fact that metadata is getting large and the catalogs are falling over well luckily it turns out that there is a project called Apache spark and it's really really good at large-scale parallel operations so under the hood we use spark and all our metadata is stored in port K format right next to the rest of your part K files that way you can actually move your data from one place to another by just copying the data you don't have to worry about catalogs that are out of sync with the data that you have so that's awesome so okay now we can scale the system with the Delta Lake project to really large metadata what about the too many files problems well the too many file problems and the performance problems that we're seeing with data leaks we now automatically fix those because we automatically optimize the layout of your data how do we do that well a bunch of indexing techniques one we partition of course your data but two we also do data skipping data skipping enables you to prune the files based on statistics that you have on numerals so you know exactly what's the max and the minimum of every data that you have in the files so if you can actually avoid reading a lot of the files when you get a query and then finally we have something called Z ordering the ordering actually enables you to optimize multiple columns at the same time so if you have a year month date index you now can actually search on just date and it can be as fast as if you searched on year which is hard with partitioning okay so this is awesome now we get all these advantages that we had in data warehouses as it comes to performance what about the last problem the most important and the hardest problem the quality issues well it turns out that we can actually solve that with schema validation and evolution so all data in Delta tables have to adhere to a strict schema can be our schemas snowflake schema but as soon as you have the schema we make sure that all your data 100% adheres to this schema if any data appears that's not satisfying the schema that you have we actually put it on the side in a quarantine that you can then look at you can clean it up and when you cleaned up the quarantine the data makes it back into Delta but now you know that all your data in a Delta table is always pristine ok and finally we've actually added something called Delta expectations Delta expectations is a novel way in which you can actually express quality expressions using sequel or user-defined functions UDF's where you can specify pretty much any business logic for quality that you like and now we guarantee that your table satisfies all those quality metrics always and with expectations and schema validations the pattern that we're seeing our customers use in data Lakes is that they're actually now creating a curated they don't make the curated data Lake now has bronze tables where you might have the raw data coming in and then that gets refined into filtered cleaned and augmented tables that we call silver tables and then finally the most optimized business level aggregated data sets are in so-called gold tables and those are the ones that you really sort of serving into your bi needs so zooming out the Delta Lake project solves the problems we had with the data lakes using four techniques first it uses asset transactions second uses spark under the hood to get the scale for the metadata third it uses indexing and lots of different indexing techniques to get speed ups that we traditionally used to have in data warehouses and then finally it uses schema validation and expectations to get the quality that we expect to have if we're using a data warehouse we really want this to become the standard for how people create their lake houses so as a result of this we've made this project work well with all these other systems so you can now actually read and write both from Delta and hive and spark presto Amazon redshift Amazon Athena and snowflake and creating data with Delta is really easy in sequel instead of saying using park' you just now in sequel say using Delta and you're just using Delta so with Delta Lake who are able to fill in critical layer of the modern lake house architecture bringing structure and reliability to your data lakes in support of any downstream data use case but from a performance perspective indexing alone isn't enough for many analytic workloads that involve small queries that need to be executed very fast you need more you want a high performance query engine that can deliver the performance required for traditional analytic workloads i'm pleased to announce that at data breaks we've been working on a solution that we're calling delta engine so I want to welcome on stage Reynolds chin co-founder and chief architect of the lyrics and he's gonna tell us what Delta engine looks like under the hood thank you honey I'm really excited to be talking to you today about Delta engine a high-performance correa engine for Delta Lakes delta engine builds on apache spark 3.0 and fully compatible with sparks api's this includes spark CoAP ice sparks data frame AP ice and by extension the koalas data frames API it delivers massive performance for sequel and data frame workloads with three important components the first component is improved query optimizer the second component is a native vectorize execution engine this return from scratch in C++ for maximum performance in the third component is a caching layer that sits between the execution engine and can object stores to faster i/o to put dealt hinges improve Cree optimizer extend sparks cost-based optimizer as well as sparks the adaptive Curie execution optimizer was more advanced statistics and this technique can actually deliver up to 18 times performance increase for star schema workloads the other engines caching layer again sits between the execution engine object storage where we build our data lakes and you can automatically choose this what input data to cache for the user and it's not just a dump bytes cache it caches the raw data it transcodes data into a more CPU efficient format that is faster to decode and for query processing later and with this we can fully leverage the throughput of the local nvme SSDs in a lot of the cloud virtual machines in space and this delivers up to five times scan performs increase for all maturity all workloads for the rest of the talk I want to focus on the native execution engine and walk you through some of our thought exercises in designing implementing this engine every time when we design a piece of software for for performance it's important to recive look at two aspects one is the first aspect is what are the hardware trends how are we changing and where will how we're going the future and that's important because ultimately the piece of software we write will run on some piece of hardware the second aspect are the workloads one other characteristics of the workloads that we need to run for or on and this is extremely important because without knowing that we don't know what we're optimizing for so let's first take a look at hardware for some of you that were here five years ago and sparks on mid 2015 you might remember a keynote site in which I compared the hardware spec from 2010 compared with the 2015 and the long I did that along three dimensions storage network and CPU and we found that along with dementia of storage and network which were basically Iowa throughput I also would have gone up by an order of magnitude in those five years so it's gone up by 10x whereas if you clock frequency had largely remained the same at around 3 gigahertz so as I also becoming faster and faster more bottlenecks are happening on the CPUs so we launched in response to that we launched project tungsten in apache spark which is aimed at substantially speeding up execution by optimizing for CPU efficiency it's been five years since 2015 let's take a look at what has changed with hardware in the last five years interestingly IO third booth has continued to go up as a matter of fact they went up by another 10x this days is really not difficult for any of us with a swipe of a credit card to launch a new version machine in any of the public cloud providers infrastructure they can give you 16 gigabytes of an nvme SSD and with a hundred gigabit network but CPU clock frequency remain the same around three gigahertz so as every other dimension is becoming faster and faster the CPU become more of the bottleneck even after five more years and the first important question we ask ourselves in designing the execution engine for delta engine is how do we achieve the next level performance given CPUs continue to be the bottleneck now the second aspect workloads in the past few years is becoming more and more obvious that businesses are moving faster and faster as a result data teams are given less and less time in properly modeling and preparing their data if your business context is changing every six months or every year and you given some new business problems to solve there's really no point spending six months to a year modeling your data like how you would do it back in the 90s over 2000 an era of data warehouses it's a matter of fact this days most columns don't have so constraints defined strings are used everywhere because strings are so easy to manipulate and many of the columns for example such as day or times them are just or strings these days and unfortunately a lot of this characteristics of the new workloads and new lack of data modeling are not really benefiting the performance of the Korea execution because many of the kree engines are design in era in which data very well modeled and we've all learned hey in order to achieve better performance this model the data better so the second important question we ask ourselves in designing the new execution engine is can we actually get both agility and performance can we get great performance for very well model data but also steal pretty good or great performance for not so well model data and fourth - sorry our answer to those two questions if there's a new execution engine for Delta engine in design - accelerates Park sequel workflows it is beautiful in simplest bus so we can get maximum control of the how underlying however would behave in order to extract the maximum performance and we really leverages choose they're very important that principles the first is called vectorization and the idea he doing idea here is to exploit data level parallelism and instruction level parallelism for performance and the second is we didn't want to design something that will work very well for model data and we spend a lot of time thinking about and making it happen to optimize for not-so-well model data before I explain to you how so photon works it's important for me to give you a refresher of how modern CPUs work so we consider it a CPU 101 while we found out from the earlier slide the CPU clock frequencies are not getting higher there are some dimensions of the CPU say improving and the most seizures are around the degree of parallelism and the first is data level parallelism in a second instruction level parallelism for data level parallelism you might have heard that hims the term Cindy and what seemly does is through a single instruction now a CP was capable of processing multiple data points at once and that degree of parallelism is measured by what we call a simply register with when seen the concept same we first came out with MMX and SSE instructions the 90s it seemed the register with 128 bits and 128 bits register with me is that a single install trans capable for example processing for 32-bit integers and when the avx2 came out the same dear Esther ban was actually doubled and avx-512 came out which is the most recent seeing the instructions at the register was double again so a degree of parallelism from data level parallelism to receive the instructions being doubling and the second part instruction level parallelism what that means is when the CPU received instructions to execute it doesn't actually the extra deck secure them in the order when they come in in any sequential order as a matter of fact the CPU will look at had in this what we call out the water window which is a number of instruction the CPU can look at head for and he will figure out for example can the CP automatically reorder the instructions and sometimes run multiple instructions in parallel if they don't have dependency and sometimes even running it in parallel with dependencies speculatively in order to actually increase the throughput and performance of software and as you can see on the slides in the past few years with the server newer generation of Intel CPUs like Sandy Bridge Hospital Skynet the other board of windows also been increasing and about 15 years ago there's such a seminal paper published in side there conference which one of the most prestigious database conferences common a dbx hundred is written by Professor Peter bonds out of this institution called CWI in the Netherlands and this paper really is the classic paper that detail how we can exploit the maximum performance with the technique of vectorization that focuses on getting data level parallelism and instruction level parallelism so for the past two and half years the data bricks engineering team have been working very closely with Professor Peter bonds in designing what a new vector wise execution engine would look like in the 2020s with all this new workable characteristics so first on to excel walk you through some of the techniques web use in photon in order to exploit performance and start with data level parallelism the first step in designing the vector is execution engine to exploit performance is to think about the in memory data format most naturally humans think about data in a row oriented format that is the first row and then there's the values in them but in order to vectorization to work really well we would transpose a row oriented format into a column oriented format in which case all the values for the column for the same column are laid out together cuz and as shown on the screen here and one of the nice benefit of the this sort of column-oriented memory format is when we actually do any type of compute for example in this case I'm just showing you how to sum up the value of two columns we could write a very simple tight loop here just loop over the data and they generate a new column and if you look at this very simple snippet of code which actually mirrors what the real code would look like in a query engine it's very compact and it accesses the memory in exactly a sequential order because it's a column oriented format if the bed of the data is laid out in memory using rule-oriented format then every time when we go from one row to another and when I sum increments we would have to skip a column two which is the list of strengths and that will actually lead to worse cache behavior and West prefetching and the other benefit is with such a simple code snippet but we call it a kernel it's very easier to for the modern compilers to be optimizing such simple loops and you apply techniques such a loop unrolling it's also in most cases the compositing recognize this pattern and generally seen the instructions automatically was to demonstrate you to you what that means with seeing the instructions I compiled this snippet of code using sort of two different powers one without seeing DM one with simply the version without Cindy you can actually see it's pretty sure and that kind of the body of the Lubell color coded it to show you the body of the loop the and you don't have to understand all the actual instructions being generated by the compiler but you can probably tell and receive the instruction while it's significantly longer mostly because of loop unrolling the instructions started mostly with the verb V write V and V here basically means vectorize it means for each one of those instructions they are effective processing for or sometimes a or even 16 values at once rather than compare with the left side they're just processing one value at once and the performance difference of the Cindy to let kind of data level parallelism is actually massive with a fully Cynthia curve measure even and to answer with a lot of overhead of the system set up in all that this is a simple ad arithmetics could generate a four-time speed-up so now we can process almost eight billion rolls per second just on a single core whereas the non cindy version can only process even though it's also native C++ code can only process two billion now data level parallelism cindy i want to move on now to instruction level parallelism in order to explain that i need to bring up a slightly more complicated example hash tables hash tables are one of the most important data structures in memory for data processing it is used very frequently in aggregates is used in varying joints here i'm just showing you a very simple example as well as i already simplified that computes the s some for some values grouping by a group by certain key and the way this will work so very naturally if i were to ask you to just write this code you probably write something similar to what i have on the screen what you do is you create a hash table and then you would dope over the suit of your input data and for each of the row you would come through a hash bucket and you would do a probe to don't care where the hash bucket key is the same as the careless key and they are the same you would sort of aggregate the data value into the hash table itself so the hash table storing a bunch of partial salt and to simplify the cost type in i don't even deal with kanaky collision here now if we look at this code it turned out most of the time are actually spent in the highlighted green part HT bucket and the reason is the the nature of the hash table means whenever use of scan through some data you're going to trigger a lot of random memory nodes that is the buckets are just random in different random locations and read the memory rows are actually very expensive from all the CPUs to because modern CPUs are really optimized for social sequential adventures even for your memory data because CPU runs so fast if you have let's see if you have to wait for some data to arrive in a random location memory that wait is significantly longer than just doing for example arithmetics where profile this we actually found that about two-thirds to two-thirds of the time for this code snippet the cpus are just waiting for memory for data to arrive from memory and the CBO's really spending only 1/3 of the time doing useful work so the question now is how do we optimize this code in order to optimize the Academy to know what's wrong with it from a performance point of view and I've color-coded it so the different parts of the inner loop for you so and then we're soon there realize it's very obvious that there's a lot of different constraints done in a single loop we are computing hash code we're doing key comparison we're doing some memory lookups to load data from memory and we're also doing the additions or aggregates and when there's such a very large loop body it makes it this actually becomes a very unfriendly behavior for the modern CPUs and the reason is remember why I brought up earlier this CPU says instruction level parallelism as a concept out of order window which is a roughly around 200 instructions see we can look ahead to reorder or run in parallel if the body of the loop is very very long the CPU could probably only be looking at either one or two loop iterations which means at the maximum the CPU can actually only reorder one or two loop iterations and so run two of those the most expensive parts of the computation which is the access load to a memory road at once it's only one or two so then understanding that now we might have a server a pretty simple solution to this problem how do you make this any bit of go fast even though it was a little bit unintuitive the way to make this loop fast is to break one single Fablab into multiple small rooms aren't you intuitive because we are now with multiple smaller loops we are actually running through the loop multiple times it seems like more overhead for example just incrementing I but the reason this is actually significantly better is the most extensive part right now on the green part is its own loop and the body of the loop is extremely simple and with such a simple loop the CPU can actually in this out of order window can predict and look that many many loop bodies and as a result the CPU can see hey how we need to fetch from for example this 12 different memory locations and you can launch all 12 through memory edges in parallel instead of waiting for one to complete and launch another one it can actually launch all the in parallel and do all the ventures at once and this actually substantially speed up the actual computation time of this cost a bit as a matter of fact with this techniques plus a number of other things such as minimizing TLB misses with huge pages we managed to significant speed up this core to the aggregation functions performance in the photon engine and the amount of ash you wasted memory stalls have also a significantly reduced so we talked about data level parallelism instruction level parallelism but how this although you would put it all together to run through some traditional sequel worries and we're running experiment just with photon on and photon off and the rest is just of Delta edges or it already includes a lot of the great career optimization before improvements and that provides offensive mode or manual speed ups and we can see we actually achieve a 3.3 times speed up just at a physical execution in the engine there with the photon engine so so far we have talked about the techniques of vectorization is not actually a new technique as a matter of fact a lot of other databases on leveraging now one thing is very different and is we spent probably more time optimizing for the modern hardware but the other aspect is we are spend a lot of time thinking about how we can actually optimize for the more modern more clothes in which people want to move faster and don't have to always properly well model data I won't have time to actually go into all of it all of the tricks and techniques that we have leverage to make string go faster I want to give you one examples you can understand the favor when we first rewrote set of string functions from some code that runs on the JVM to native C++ code we observed quite a bit of sweet for example the helper function god we achieve almost a 50% speed up and the substring function was almost three times speed up but we're gonna stop there we asked ourselves how can we do better um now I have to introduce you to how strings encoded in order to understand to one important technique we abused and that UK FA is actually the most ubiquitous string encoding different from a lot of other encodings UDF is a variable length encoding what it means is a single character could take anywhere from one byte all the way to four bytes I'm showing you some examples here for example the character a is won by the copyright symbol is two bytes the Chinese character Shing is three bytes and the pile Pooh which is a legitimate utf-8 character is four bytes the reason you TF a so became popular and the reason they serve in his design with their variable length encoding it's it's great for memory a lot of the data as Malevich probably most of the data especially in the Western world are just ASCII characters they will all fit in one bite so when using utf-8 you would only wasting one by program so string per character whereas with a lot of other fixed land schema you might have to use two bytes or three bytes always but the unfortunate this variable lancing coding well is great for memory saving because very compact actually computationally expensive it just takes to the sub stream functions example in the fixed lines encoding the substring function simply returns the number of plates if we know for example every character is three bytes we'll just return so three times however many characters won't return but in the variable length encoding we have to look at each of the character or each of the bites one by one to determine what the character boundaries or the sometimes some databases actually give the user as the option of defining what your character set is because often some users will only have ASCII character but we feel that's not very realistic for two reasons one this users don't often actually do that because they sometimes don't even know this optimization they can do by themselves manually the second is often data it's actually may be same vast majority with data as ASCII but then every once in a while was a one row come in it has a special symbol and as a result now you're going to curve up your data if you don't declare properly the character set so really the real question is given the fact that most data especially in the Western world are just ASCII character set but with the locations of utf-8 beyond asti can we get both the savings of duty of a in-memory compact compactness and the performance of ASCII or put it differently can we only pay for the performance penalty of utf-8 when necessary when we actually have that data and we actually found a pretty interesting trick that would leverage in this case and the idea is who I should stab us separate any string processing into two steps the first step is the fully seam the ASCII detection algorithm that's run on the string and this serve detection algorithm kernel is optimized so well they let produce if it's handwritten with AVX instructions it can run a 60 gigabyte per second per court so this is almost running a memory bandwidth this is so fast as negligible overhead in virtually all cases and the second step is depending on the result of the ASCII action if we realize the string is really just can fit in all the entirety of the string and fit in ASCII characters that we can run a special fixed length version of our string function otherwise you run the variable last version and with this technique we achieve an order knack of speed up for most of our string functions compared with the original JVM variant and in many cases even compared with the same diversion the c++ period when the data czar for example the spit in ASCII character set and this is done without the users have to manually do anything it's just an automatic to automatic algorithm for example upper and sub strings are signifying faster but this is just one technique I want to show you this is one more example which is that regular expressions it is used pretty frequently when people are dealing with or massaging data it's very rare you see regular expression showing up in the actual benchmark but we have found the use of it in the world so much they've spent a lot of time optimizing it it's a matter of fact regular expression in photon is more than four times faster than regular expressions house were just to wrap up photon is a purposeful execution engine for maximum performance as built in C++ elaborate is vectorization so we can take advantage of the data level parallelism instruction level parallelism presented by modern CPUs it's also really designed to optimize for modern were close in which is a lot of strings everywhere and I've shown you some examples of that and to conclude Delta engine delivers best-in-class performance with all the expressiveness of spark it has three important components a create optimized ER an execution engine and as stated they are caching layer both the Cree optimized and the caching layer is available now on data Brooks and you can actually contact us to get access to the new execution engine this coffee photon and thank you all right capacity talk back to Ally Thank You Reynold so this is great this is assume if you have your data in Delta lake how do you actually get it into Delta lake in the first place I'd like to spend some time to talk about that so theta bricks we have something called the data ingestion Network data ingestion network is tightly integrated partnership with things like ADF and companies like v Tran and stitch we go inside data bricks there's something called data ingestion network gallery when you click on that you can now see an icon of all the different operational databases that an organization might have their data in you click on it and it can now automatically transfer that data into Delta Lake we also want to make it easier for you to get data that is in a blob store into Delta Lake before all of this was manual you would have to manually connect to a notification system register that whenever files appear please kick off a job scheduler that then runs maybe a spark job that takes that data and converts it into Delta Lake but what we've done is that we have now something called the auto loader the auto loader automatically connects with the underlying notification systems and figures out when new files appear and automatically convert them into Delta Lake so that's awesome now you have all your data in Delta Lake but how do we actually keep this secure so in data breaks we provide fine-grained access control on top of Delta Lake this is standard sequel based GCLs you can now submit revoke and grant queries in sequel you can create groups and add individuals to them and it tightly integrates with the cloud vendors Active Directory that means you don't need any passwords or tokens or certificates the identity goes from the Active Directory straight to data bricks and then you access the files seamlessly so let's put all this together so we talked a lot about how Delta Lake solves a lot of this complexity around managing data and how Delta engine improves the performances of all your analytics workloads but once you have all the data there's only a handful of analysts who have the tools to make sense of it in an organization zooming out data teams are small part of the equation in most organizations we have organizations like marketing HR finance they don't want to use Python or sequel or Scala today to consume data the rest of the organization has to really jump through lots of Hoops if you want data teams to truly unite we have to make it easier to consume that data for the rest of the folks in these organizations recently a really fascinating open-source project has emerged that's starting to solve these problems we - is an easy-to-use open-source dashboarding and visualization service for data scientists and sequel analysts with it data teams are better able to democratize sharing data across them within organizations the community behind this project is absolutely amazing this project started in 2013 just like data bricks they have over 300 open source contributors and they're really passionate about democratizing access to data across organisations and it's working in a really really big way every day millions of users and thousands of organizations around the world are using we - to get insights and make data-driven decisions they've seen a ton of success over 7,000 deployments around the world and the engineering team standing behind this great project is absolutely fantastic and they've built something great that aligns completely with the data box values so it's my great pleasure to announce today that reed ash has joined dataworks and we're super excited to align these two open-source communities the SPARC community with the read ash community to tell us more about how this will work it is my great pleasure to introduce Eric frame which the founder of reed ash Eric thank you Lee I'm really excited to be joining both data breach and the larger open source community of spark seven years ago I started reading as a hackathon project and I didn't expect to be here today sharing the story with such a large community back then we just moved our data into Data Lake and we needed some how to use the data so we tried some sequel workbench tool and this was really missing collaboration we realized that what we need is somewhere application where we can type in the query get the results visualize and share them and I decided to give it a try that hackathon and that what started Reedus working with data bricks earlier this year I realize how much alignment we have on a lot of these core ideas that I had when I started the project make it easy to collaborate with others around the data and democratize data for all the teams but beyond the product and vision alignment it's the amazing culture in data bricks that really makes me and the team excited about joining data bricks but now let's talk a bit about what we - actually is so read ash gives you a sequel interface to cover your database in its natural syntax but we give you some tools to make it easier like the schema browser autocomplete and query snippets now once you have the data that you queried you can visualize it with wide variety of visualizations and then group these visualizations into dashboards you can also set up refresh schedule so that you get the most fresh data without waiting and when it comes to databases research has you covered so we have supports about 40 to over 40 types of databases data warehouses data lakes and different types of api's so it's likely that whatever data source your organization uses you can connect it to read ash now let's check out how you can use readers with data bricks let's say that I'm a data analyst in a SAS company and I'm looking to help the business understand revenues usage and how to drive conversions better and reduce churn we have at least three types of data account data payments data and usage data but this data is coming from three different sources account data is in the operational database payments is with the payments processor and only usage data is in our data Lake form any meaningful analysis of the data we will need to join the data from all three sources we can do this join in readers but this has limits and performance issues what we really want is a single source with all the data and this is exactly what we can achieve with the Delta Lake now over data is in a single location and we can easily use the data together create more meaningful insights and have the most fresh data and now with that intro let's check out the actual demo right so here we have the rediscovery screen and let me type in a query to just show you a bit how it works so I have the autocomplete helping me a bit here although it doesn't save me from tip typos now here on the Left we have the schema browser that shows us the tables and their columns now we got the data back we can look at it but it's much nicer to look at visualization it's easier to understand this way so let's create one real quick so here we see the trend of our monthly recurring revenue over time and we can see that it's growing but we don't know why or how it's growing so it probably makes sense to see a breakdown of what's contributing for it so let's create a bit more complex query this time so this one is a bit long so I'm using query snippet for it now correct snippet is a very simple construct you have this keyword that triggers it and you have a definition of what the snippet will be and then when you trigger it it gets inserted in it to your query I got the query back and this time I have much more information but again a visualization can really help understand it so here's a breakdown of our monthly recurring revenue and we can see that most of the time it's growing due to new monthly recurring giving revenue but we had some successful campaigns around expansion in a few months and we have some regular expansion over time and unfortunately we also have some cheering over time now from the same set of data I can create some other visualizations like again the growth of monthly recurring revenue over time some counters and the average revenue per user over time next once we have we sometimes would like to look at this data at a daily level so here we have another visualization that showing us that a rolling window of 30 days of the monthly recurring revenue at that day and that's useful when we want to track it in more real-time fashion for example when we're running some campaign or we are aware of some issue that we want to track more closely once we have a few of these visualizations we would like to create a dashboard in these dashboards we see different facets of our monthly recurring revenue like this breakdown that we saw before the growth over time break down by different plant types and break down by geography now looking at the revenues is great but we also want to help our customers its success them to help drive this revenue forward so here we have a dashboard for our customer success team that shows them two types of accounts one type is promising trial accounts that basically accounts that are currently in their current period didn't convert yet but showing good indication of usage so we might want to reach out to them and help them switch over to a paid account on the other hand we have the paid account that showing a decline in usage where we would like to reach out to them and see if we can help them get back on track to help the customer success agents each row here is a link to this other customer insights dashboard and this dashboard is using a parameter to filter out and show us only this account here we can see some details about the account like their usage over time type of account the number of widgets they created and we can definitely see the decline and we can drill deeper to understand why it might be or just reach out to the customer now this was a really quick taste to read it we have a more in-depth today that I recommend you checking out thank you very much and back to you Ellie thanks Eric I'm super excited about this innovation so to zoom out in summary we have Delta Lake as a structured transaction layer on top of your data Lake it allows you to overcome a lot of the challenges that data lakes have historically presented by using asset transactions indexing fine-grained control and schema validation and expectations so that's Delta Lake project then to ensure that you have all your analytic workloads and that they can be executed really fast at low latency we have on top of this Delta engine we think Delta engine would its vectorize query engine query optimizations custom connectors combined with all the enhancements that substantially improve performance and price for analytic workloads make data bricks the ideal platform to deploy Delta lakes and finally all of this comes together in the ways that data is actually made useful Reed ash is going to simplify dashboarding for many analysts and data scientists and tomorrow we're going to deep dive into how enhancements to ml flow the open source end-to-end machine learning platform will make data bricks workspace even better and I'm really looking forward to what we have to share there the data workloads and the data platform working together end-to-end is the heart of data bricks unified data analytics and our vision for data teams to share how this vision is coming to life within Starbucks to drive collaboration innovation it is my great pleasure to introduce fish subrahmanyam director of data analytics and engineering fish what's brewing at Starbucks Thank You Allie hello all welcome to spark in a SME 2020 my name is vishwanadh Subramanian I'm the director of data analytics engineering at Starbucks as Holly mentioned earlier has been all navigating through these challenging times we strive to make fact-based decisions with data I'm here to talk to you about our data journey a Starbucks our mission is to inspire and nurture the human spirit one person one cup one a board at a time you may be familiar with some of our 30,000 stores worldwide that may be a part of your experience we have over 30,000 employee who we call as partners who ensure our standing as one of the world's most admired companies in order to adhere to that quality of service and product the key focus areas that we focused on are making sure we have elevated partner and customer connections breakthrough in beverage innovation and also accelerating our customer engagement across channels including digital however this story is driven by our ability to harness data to power these customer experiences amazing teams across Starbucks continue working on the next breakthrough innovation to ensure we build the right customer experience and an enduring brand appeal these extend across our multiple domains partners products store portfolios and digital there are tons of awesome teams working on big ideas and enabling them to be brewed at scale as much as it's about the product it's about elevating customer connections and experience that is driven by the convergence of data for example on mobile we use rapid a/b testing to improve recommendations using reinforcement learning models are the Brocade platform on drive through we cross cell modifications using engines like D brew on store applications we serve near real-time transaction data with billions of data points to internal reporting applications so that actual store personnel can monitor transactions near real time more importantly data is extremely crucial as we extend channels for delivery and new norms in today's new era the ability to serve these use cases is only possible through a sound data strategy our data strategy and guiding principles are built on three pillars single version of the truth providing trusted store of information across customer product stores data analytics enablement as well as trusted data to ensure data quality privacy security and the right data definitions to access this data how do execute our strategy we are to be with several modernization challenges on the architecture side we were dealing with petabyte scale of data and fragmentation across systems or the ingestion and processing the ability to process near data products and a huge variety of sources the inability to implement updates merges on fast-changing data in an optimal fashion also non optimal engineering experiences with services that did not perform at scale for example taking a long time to provision or scale also on the consumption side there was no real single source of truth a lack of a unified user experience and an impedance mismatch between data and model development and operations blocking experimentations and reproducibility to illustrate how we dealt with all these challenges ruk it was our zero friction on eddies framework this is built on a strong foundation of Asscher data breaks and delta brouk it would be would be a unified analytics platform to reduce our impedance mismatch between access to data ability to perform data science and operationalization we wanted to make sure the smallest of teams at starbucks have the ability to do at scale data science and data engineering with this framework brew kit our massive scale data framework has all the necessary capabilities in a box to perform at scale data science and data engineering we leverage multiple larger services and homegrown Starbucks engineering to light up a functioning environment multiple SKUs translate into data engineering and data science templates all this generated declaratively in a matter of minutes we have services like ml flow and Azure ml that help power our model management life cycles all this back down storage services as such as Delta Lake view's Hydra service such as a droid keyboard to help us secure key management across different environments from an infrastructure essential standpoint we have data breaks as the core engine for our demand compute and the notebook collaborative experience helps teams collaborate an experiment at a rapid fashion the air so we clear is powered by services such as cosmos DB that help us enable internet scale low latency serving we also provide automation and start a notebooks and templates across a variety of use cases so that users can strap quickly on to the environment all activities are audited for governance reason as well as return on investment moving on to our second challenge data and ingestion and huge focus areas for teams has been to ingest data at scale using custom Spock utilities with massive policy with inherent benefits of Delta such as asset transactions metadata handling and schema enforcement we can use services now like our human hubs and spark structured streaming that help us process millions of transaction at scale per second for long running pipelines fault tolerance is trivial now with structured streaming water bodily Delta has now helped us build out our historical data and library locations together to make sure we are now giving our store partners real-time insights on data based on history and on current time consider pipelines at whole transaction data there are cleansed audited and processed to be sent right back to the stores within minutes to ensure that our partners have this latest data to make decisions with the ability to perform stateful and stateless aggregations helps us tailor trade-offs based on use cases features like compaction or optimization help us now have the right partition keys in our partitioned data like to get us 5200 X performance gains as well as storage optimization it's crucial for us to support time travel and version data for data and model provenance operations our Delta such as merges updates and deletes help us build out our own anonymization frameworks we can also have logical separation on the Delta layer with discovery and integration zone where we can apply schema enforcement based on the source of the data as well as apply retention policies this translates to a highly industrialized process where bi Brigade data across different zones and load data from a variety of sources using the workspaces clusters and job API is an orchestration tools like air flow we have predefined patterns for these ingestion processes that help us define retention policies as well as partition according to the use case features Delta helps us with features like I reported see with merge operations and with fast changing change data capture use cases time travel provides snapshot isolation for fast changing tables we also govern we make sure we're now governing an enterprise data lake with the right permissioning and access control over all the strategic view has been here to commoditize data ingestion to such an extent so that the teams can focus on business problems of the up the value chain rather than focusing on moving data from point A to point B on the consumption side which was the third challenge we had we wanted to make sure your experience is as good as you walk into one of our premiere experiences we saw over 50 plus workspaces on the enterprise with different business units with logical separations between them this gives us flexibility to monitor secure and segregate these environments based on the use case we also run our own version of our metadata sync process across these 50-plus environments to sync across these workspaces at scale so all users are looking at one government version or publish data with a single source of truth the successful data enablement and rookie journey has now led us to focus on our yeah ml strategy we have enabled convergence of data and enable Dru kit for the creation of models in the past our goals were to deploy environments in a matter of minutes to deploy data pipelines in a matter of minutes which we have largely achieved now our focus is now to democratize machine learning and how do we enable our data sign this to enable machine learning models in a matter of minutes we have our framework AI reserved that we are in the process of deploying with a goal to achieve exactly that powered by ml flow and ashur services ai reserve will be guiding larger constructs in our enterprise including a model marketplace this will be used in many use cases across the board such as store operations quality of service analysis personalization experience and much more the ML stack comes together with analytics out of the box with our brocade environments users have a plethora of environments that they develop their code and we can use version control to deploy bills based on integration with Azure services like additive ops based on use cases like on schedule or drift scenarios users can use Azure ml or ml flow for their model lifecycle management and all this back to on a persistent data store on the Delta Lake we also make sure I air is of integration allows our data scientists to containerize their solutions and use Azure Community Services to essentially serve our REST API which leads to the larger construct of the air is a model marketplace today's real time data along with today third-party data is driving our decisions across the board including store reopening strategies all this is being supported by a unified data na our solution we now have petabytes of data located on Delta Lake at massive scale hundreds of data products built on spark an average deployment time or pipelines and environments and minutes 4,000 plus pipelines across engineering and user workspaces and more importantly these are all highly reliable and fault tolerant from a dare a tabular collaboration and productivity standpoint this has been huge the tooling is collaborative we also now foster a culture of experimentation and self service and maintain shared responsibility across our environments and the focus is now on outcomes and our technology or infrastructure I would encourage you if you want to check out additional talks by Starbucks at the summit including how we operationalize our big data pipelines as well as our machine learning please be sure to visit stored star Starbucks comm for latest news and initiatives I hope you have a great sparkling eye summit 2020 thank you very much back to you ollie thanks wish and I also want to thank all the speakers and the demos today's and before you go a few things I want to remind you of please attend as many of today's breakout sessions as you can today tons of interesting content and speakers can be found visit the dev hub and the Expo for live demos and engage with our technology partners network this is a great opportunity to meet and learn from thousands of people like you you can head to the Advisory lounge birds of a feather networking experiences theater sessions and my personal favorite ask me anything sessions with matei and other industry leaders share your thoughts lessons learned and selfies on social media be sure to use our hashtag Sparky I summit and data teams and finally please join us to support the Center for policing equity and the n-double-a-cp legal defense and education fund click donate now in your dashboard and database will match all donations up to hundred thousand dollars you can hear directly from the founder of CPE dr. Phillip a tea bag off along with professor Jennifer chase from UC Berkeley and nate silver from 538 in this afternoon's keynote session you don't want to miss this so be sure to join us at 1 p.m. Pacific time I hope you found today's session informative and hope you enjoyed the rest of spark an AI summit 2020 thank you [Music] [Music] [Music] [Applause] [Music] [Music] [Applause] [Music] [Music] 