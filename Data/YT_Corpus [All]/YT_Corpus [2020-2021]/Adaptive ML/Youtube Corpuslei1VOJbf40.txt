 We introduce Transform and Tell, an end-to-end model that can generate linguistically rich image captions in news articles by attending to the visual and textual domains. The model consists of four encoders that can provide a representation of the article text, the image, along with any faces and objects in that image. We use a Transformer architecture to attend over the previous tokens in the caption and over the context embeddings. The decoder then generates byte-pair tokens which are then combined to form words in the caption, including named entities. On the existing GoodNews dataset, our full model was able to achieve a CIDEr score that is four times higher than the previous state of the art. We also introduce the NYTimes800k dataset which is now the largest news image capturing dataset. Check our code on GitHub and go to the demo at transform-and-tell.ml to generate your own caption from any New York Times article. 