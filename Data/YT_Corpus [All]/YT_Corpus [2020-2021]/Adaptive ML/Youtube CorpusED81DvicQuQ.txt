 hello everybody my name is william and i'm the data science platform lead at gojek today i'll be talking to you about scaling data and ml with feast and apache spark so the agenda for the day i'll give you a bit of background on some of the data challenges we faced at gojek why we decided to create feasts a high-level functionality of east and we'll talk a bit about i can get data into these how you can use trainer model how you can use it for model serving statistics generation validation and a bit about the project and the the road ahead for us so what is gojek kojic is an indonesian uh super application today we're classified as a techacorn we are focused on many different products and services more we have more than 15 products and services we're most famous for ride-hailing hailing cars motorcycles we have food delivery one of the biggest food living networks in southeast asia digital payments logistics lifestyle services so the company has more than 500 000 merchants restaurants more than a million drivers so this scale of our operations is very big in southeast asia so given the product diversity and the scaled organization ml and data is critical to what we do the most classic example is matchmaking matching customers and drivers but not just matchmaking dynamic pricing so search pricing having machine learning models calculate prices across the whole so all these service areas that we cover so those are just two of the bigger examples we also have routing recommendation systems for example for our food product and food delivery incentive optimization we have supply positioning so how do you make sure that the drivers are located in the right positions for trips to be started and to balance the supply and demand and then of course fraud prevention so given all of these use cases and these are basically just the tip of the iceberg ml and data is critical to what we do and we originally started a typical flow for a data scientist was something like they're given a business problem they think in terms of notebooks and data and basically hacking something together so most of our original use cases when the cs team in go check started follow this approach you know teams had an idea of going into production would require some kind of model surveying blueprint some features and be some integration with the production system but for the most part the focus was on what can you extract out of data in order to deliver a value so teams typically evolve these notebooks they'll hack together a notebook they'll iterate on that and this is a very common practice not just at gojek but across the industry these notebooks involved into airflow pipelines or any kind of ml pipelines that are holistic into inflows you start with your data lake or your data warehouse you're transforming your data you can use spark for that pandas whatever you require you use some framework like extra boost or tensorflow training model and you somehow deploy that model into your serving environment and in that serving environment production system can integrate with you and you know you can make predictions based on feature values that you're also persisting and serving and this is kind of the approach that gojek took originally and it worked you'd need to manage this whole end-to-end pipeline you'd need to have some kind of system that can transfer the data that you've created in the batch pipeline into your production environment eventually you'd also start you know hooking up streams and transforming those streams and then populating your online stores as well as your historical stores with data so for a holistic system this works this is fine um an mvp can easily be created that does this but at gojek this approach didn't really scale for us and i think that most organizations this is great for a v1 but it doesn't really allow you to reach higher efficiencies and scalability so some of the problems we experienced with this approach this into a monolithic approach is one of the biggest ones was a lack of ability to trade independently so we had teams that would want to iterate on features and we had members that want to trade on the modeling aspects and then others that want to iterate on the models in production so they want to either add new features they want to experiment with different models and using a b testing or they want to just train new models but all of these life cycles are independent yet they're coupled extremely tightly if you evolve a notebook into an end-to-end production system so that was one of the bigger brains we faced another pain we faced is that code needed to be ported over from the training environment because that's what was being used for for example using python to manipulate data and then train a model it needed to be imported into the streaming stream processing engines where it needed to be ported over into the serving environment in order to create features so this duplication of work was one of the pains that builds experience and the duplication of code led to inconsistencies because now the data that you're receiving and serving is different from the data that you're using to train your model one and that inconsistency can lead to data quality loss and more performance loss and another problem we had was because we were building these systems from the ground up yes you've got you know production grade infrastructure but they're not built with ml in mind they're generic data infrastructure and so the quality and monitoring tools that are available are it needs to be needs to be configured for your specific use case and we needed to configure that in a way that made sense and one of the problems with this approach is that the production engineers that are typically on call for managing these models do not really have the context of data that is being used in the model and they do not know what is the right properties or shapes shapes of data and so this is disconnect between the production side and the data creator side is one of the bigger problems that we also faced and then finally when you build these into in monolithic systems there's a deep lack of reuse and sharing of especially features so you'll find that firstly there's no knowledge of what is being used in these pipelines so another team doesn't know which features are being created here and if they do know which features are created here they don't have they don't have a way to extract that data out of this pipeline out of this whole flow so so that was one of our bigger challenges is how can we avoid teams recreating these pipelines from scratch every time a new project is started because clearly spending two-thirds of the time creating features is not an efficient way to go about things so feast is kind of one of the approaches to solving these key data challenges with productionizing machine learning give you a bit of background about the project feast was developed in collaboration with google cloud and gojek it was developed at the end of 2018 and it was open sourced early 2019. sagojic is the primary contributor there and it's running in production using for most of our large-scale ml systems so our pricing engines our matchmaking systems many of the large systems within gojek runs on feast today feast is a community driven effort it has adoption at multiple tech companies and some of these are listed here and these companies either are adoptees or they are contributors to the project so if we go back to this original diagram i think the question really is how can we improve this monolithic approach to you know ml if you introduce feast the stages of this pipeline are decoupled so each of these stages creation of features training of models serving of models have a touch point with features and they were all done independently so the feast allows you to decouple them and iterate independently so if you're a data creator and this might still be the same data scientist you're creating features either through processing a stream either through processing in your batch data and data lake or your bachelorette data warehouse or even just a csv but that data ends up in feast and that's where that specific iteration cycle ends when you're training a model you're not processing your data from scratch you're not re-transforming raw data you're selecting data from the feature store and so that's where kind of this decoupling means that there's no strong relationship between the person training the model and the person creating the features although normally in projects those were the same person but this decoupling allows the person creating the model for training to select any amount of features from any amount of teams throughout the organization so this is very powerful for us to make it possible for teams to iterate independently and so when you're training a model it's mostly a matter of selecting the features that apply to your use case and then creating your model binary and processing that in a model store like ml mlflow so what teams typically also do is they store a list of features these are the lists of features or feature references as we call them that were used to train the model with that model binary and then they ship that into model serving environment so model serving you're loading your data from this registry like in author you're serving it into your production environment and all you need to do to get the same data to that model is send another request to feast for online values and peace will return those to you so here you have three stages that are not decoupled and you can iterate on them independently and what you'll find is at scale the you'll build up a base of features that are reusable across projects and you'll automate your production serving environment so all of the focus goes into the modeling environment and so that's where the data scientist spends most of his time instead of you know crafting features from scratch over so what is feast so in summary fees is an ml-specific data system and it attempts to solve these ml-specific problems so i highlight that aspect because if you're only focused on data storage or stream processing or any one of those aspects none of those are ml specific but there are specific constraints to running ml and production like the consistency between training and serving that need to be upheld and so fees can be seen as an opinionated configuration of production data technologies in order to serve these ml needs so for example feast allows you to consume from multiple sources so either batch sources or streaming sources and it stores those process those in a way that it can be served in a point-in-time correct way later for training your data for sorry for training your models and for serving for model serving so when you're ingesting data it stores the data in online stores it stores it into historical stores this allows you to standardize the definitions of features so as a data creator you define entities you define features and you map this into the data that you create then as an organization you have a standardized definition of those features that you can reuse across projects and this encourages reuse and sharing because now you also have with feast a single canonical reference to each one of those features and so you're not talking about look at the csv look at this column or look at this key in this database you know look at this query you're talking about a name of a feature this reference and that's all your model sees it doesn't see the implementation details of the infrastructure feast allows you to ensure consistency between your training and serving environments through that abstraction and feast ensures that the data through and partly through the ingestion layer and the way that it stores data it ensures that the training and serving layers always receive the same data so the model is trained on the same data that'll get in serving and that ensures that the performance of the model is upheld feast is also able to do a time to time travel or point in time correct query of your features for uh producing a model training data set this is a critical aspect of creating training data sets that a lot of data scientists do today but it's quite error-prone if you do this incorrectly you're either going to have stale data that you're serving your training model for training or you're going to leak data from the future to your model and in both cases the model will perform badly and it will be very hard to debug and know that that is a problem feast is also able to allow data creators to define schemas and these schemas can be used based on statistics that feast generates on the data to validate the quality of your data so if there's a data shift these can let you know that something is wrong and it can prevent you from shipping a bad model or it can prevent you from you know continuing to serve a bad model or a good model and bad data in production so what is fees not so this is not a workflow scheduler it's not a pipelining solution it isn't a warehouse though it is a storage um system for data it abstracts the warehouse lake or online stores but it's not just a database visa is not a transformational system so this is a very key point some of the other feature stores have transformation capabilities as the key value-add for us we consider existing tools to be a better solution to these problems so we try and focus on the other aspects of other challenges to productionizing ml so we encourage the use of spark or pandas in upstream systems and then we become productionization there this is not a data discovery or cataloging system for your whole organization an aspect of what we do is feature discovery and reuse and cataloging but that's a subset of the data that's available in your organization this is not yet for data or data versioning or linux the new system there are tools and products that do that we might integrate with those but they're they're solving a different kind of problem to what we're trying to solve piece is not a model serving or metadata tracking system so we produce artifacts and we produce matrix that can be tracked in those systems we do not serve models when you serve a model because the model is ultimately a transformation you can produce data as an option to the model that gives back into feast so you can use fees to track statistics about the models and to actually use the schema validation to see if your models are drifting from their traditional outputs so in some ways you can integrate with model serving but it doesn't certain models itself so let's talk a little bit about how you can get data into feast how you can define features and entities and schemas when you're working with on ml systems typically what you're trying to do is you're trying to predict some kind of event some kind of phenomenon on an entity so the entity can be a customer it can be a driver it can be a booking an order you want to make some kind of prediction and you do that through features these features or attributes or properties about that entity or about any entity in the environmental world that you're operating so a key concept in face is definition of these features the definition of these entities so the feature set is how you do that on the screen we have an example of a feature set driver weekly feature set this feature set maps onto one table that we've shown there in green and yellow and this is a table that represents drivers so on the left you can see driver ids you can see there's a conversion rate an acceptance rate and an average daily trips and then so this is back these are batch features they're calculated on a daily basis so you can think of the features set as a kind of bulk way to define sets of features that occur together and importantly these all occur on the same time stamp so they're computed as part of the same aggregations and um these features i think it's a very important thing to notice that a feature set is an ingestion concept it's a means of defining a schema on how data will be loaded into the system or data will be sourced by feast into the system it's not a means of selecting features for training a model it's only a definitions grouping so here's an example of ingesting data into feast so what we're going to do is we're going to take that existing data frame that we showed in the previous slide load it in as a csv driver weekly data csv we're going to define a feature set we're going to infer the entities and features from that data frame and then we have an object that a feast object a feature set object that we can apply and the apply step essentially registers that feature set with feast so now fisa has an idea of that entity the type of entity and the features that are associated with that entity so the final step there is ingestion suggestion step loads the data into feast the important thing to know here is that once you ingest that data depending on the subscription of the stores that are inside of east the serving layer those stores will all immediately be updated with this data so when you do an ingestion all of online stores immediately get access to this data and they do it they have the data available to them in a point in time and consistent way but fees is also capable of ingesting from streams so in this case there's a subtle difference we're defining a driver stream feature set on the right you can see an example um data frame of the speed this event stream so you'll see there's a trips today feature in the one the column in green and these are just events that are coming in on different kind of driver ids on some kind of stream and the topic is the driver stream topic so what happens here is that you're defining a feature set you're registering with feast using the apply method as soon as you do that this is going to provision a ingestion job this suggestion job is going to start streaming in the data and it will make sure to populate all the stores that are subscribed to this feature data and this is kind of the high level architecture of what happens to that data so if you look at the the five stages here you've got your data you can think of that first layer as kind of the raw layer it's yours your existing streams your existing warehouse deadline or just your notebook where you're doing iteration that's the area where you're working with your data then you have the ingestion layer the suggestion layer is going to take data from wherever you've created it and populate stores and these stores can be either historical stores used for creating training data sets or they can be online stores an aspect of what actually happens that's not really shown here is that there's a streaming layer that populates online stores and so that streaming layer allows you to have as many online environments as you want so if you have a large organization they can have their own independent serving environments that just tap over the stream and their stores automatically get populated when anybody loads new data sets or streams into feast your serving layer is the fourth layer and that's the way that teams will interact with the data that's persistent within feast so a model training pipeline or a model serving system could send a request to the feast certificate layer either for a point in time correct training data set or for online features and this will export that or return that at low latency all of this is managed through fist core vscore is you can think of that as having a dual purpose of being a registry it's the central place where you define features defined into these defined feature sets allows you to search and discover these features it allows you to track metadata and attributes and properties of those features and v-score also allows you to generate statistics an important job that feast core does is it manages the ingestion there so when you register feature sets peace corps is the one spinning up the jobs and ingesting the data into these stores and so this is a kind of high level view of how feast takes data stores it in a way that is consistent and the reason it's consistent is because the ingestion layer writes to all of the storage locations in one go so that ensures that at no point in time do these stores become inconsistent with each other so that unified storage is the key aspect of what fees does so once you've stored your data inside a feast feature serving is the next important thing to look at so here we have two loops if you look at the right there's the model training loop and the model serving loop for training what you want to have is a training data set and for serving you want to have you know just the features that you need to do a prediction but there's one thing that the model always needs and that is the data should be in this the correct format a consistent format in both cases and feast helps with this by abstracting away kind of the infrastructural aspects the implementation details and so the only api contract that the user needs is the feature reference list so feature references are canonical references to features stored within feast an important thing to note here and if you look at this feature list is that they're based on the two feature sets that we registered the reason that's important is because you can reference features from any amount of feature sense as long as a feature is referenced in feast you can reference it so you can send the request to a fee serving either for training that is set or for online serving and feast will build a training data set it will do a point in time correct join and stitch together the data to produce that training data set the only requirement is that you have the correct entities to join that featured data onto so in training you're going to send fees to a list of entities so driver ids and timestamps and then fees will join the data onto the feature data onto that and in serving you will send just for example the driver ids list of features and then fees will send back the feature values attached to those drive varieties so an online serving you're only getting back the latest data button for historical serving it's giving you a point on correct view of the data for training so one of the key value adds here is that is the consistency between these environments and the fact that the historical data sets represent the point in time correct view of the data an important thing to talk about is this point in time correctness and kind of the events that happen throughout the life cycle of the project so if you look at this timeline the green diamonds are essentially feature values that are being computed that can be during you know batch transformations or streams it doesn't really matter but essentially at some point in time a new value is populated and it's ingested into fees where it's streamed into a store and that becomes available the red square is a event in the production system so that's some kind of event that mandates a prediction so that can be like a booking is made or some kind of transaction is made and you need to make a prediction and the final purple square is the outcome so that's the final event that happens or the final data point that arrives that tells you whether it was a success or a failure so these three colors represent the three types of data that you deal with when you're building an ml system and you need all three of them to be able to train a model and to be able to know the outcome of a model and to have you know the correct labels also to train to train that model so if you look at this view um if you're if you're doing a prediction online um so that let's let's assume that at that rider booking event you want to make a prediction you have your model already what you want to do is you want to provide the correct feature values so for each of those um four green rows you're going to send the latest values to your model that's very easy to do in online serving all you need to do is make sure that the latest values are available and that they're not too stale so you always have a point in time correct view of the data in online serving but how do you do that for historical survey this is very tricky to do because these timestamps don't line up perfectly so if you have many different booking events and you have many different labeling events or outcomes and all of your features are calculated at different time steps producing this training data set is non-trivial and this is something that is requirement for data scientists to do and it is an extremely error prone and can be often a complex task for them to undertake this is something that feast does natively it allows you to stitch together the data based on its source based on its update rate and frequency all of that is abstracted away from you it will produce a point in time correct time traveled training data set for you regardless of at what time the the data points were updated so a quick example here is using our existing data frame of driver features so on the right you have a new data frame with some labeled events um so these are basically just has just been completed successfully or not successfully those are the target values that you want to join onto and on the left you have features and then driver ids and so if if these were two data frames that have been loaded into feast and you query this out you could produce this final training data set and it doesn't matter whether the timestamps don't line up feast is able to feast is you can indicate your feast that the one table is your basis table the table on the right and the one on the left is the features that you want to join onto it and fees can ensure that it's joined in a point in time correct manner so here's this code snippet using our python sdk and how you can retrieve feature values from fees for trading model so on the left you have your list of features those are the feature references so those are the canonical references that i spoke of earlier so this is all you need to pause to visit this list of features and then you want to pass the entity data frame to fees as well this data frame contains driver ids and timestamps and those typically are mapped straight on to the timestamps that the events occur for prediction and so you send that as a request to feast these returns a data set that can be materialized into a data frame a pandas data frame or it can also just keep the data frame persisted on disk or an object store but essentially what it's going to do is produce this final training data set based on feature sets multiple feature sets so if you look on the right you have two different categories of feature data you have the batch data that we ingested originally and we have the trips today streaming features and so it's irrelevant where the data came from at least now as a feast because it can just join these in a point in time correct way but it doesn't matter that the ones coming from a stream and the ones coming from batch so these can join all of those together for you and prevent the typical errors that occur like leaking feature data or joining data together in a way that propagates stale data to your model training so a team will typically produce the data frame it will change they will change their model and then they would press their model binary with list of features so here we have an example of going to production so this is the python sdk and you'll notice that the list of features is identical so you don't have you know a spark query or a sql query in training and then you have some kind of different database used in serving the only difference here is um the method that you call in online serving but the list of features is consistent and that's kind of one of the key value adds that fee springs is that the list of features and the feature references unify both environments so in production you'd have driver ids these are your entities you would not have time stamps because you're always requesting the latest feature values so you'd send the list of features with driver ids and the fees would attach the latest feature values that you then return so in this case we're only asking for one driver how these features but in reality you'd ask for let's say 100 driver ids or more and the piece would give you these values at low latency so our current stores operate in the single digit millisecond latencies for for this and operating at scale is really one of the requirements that we have we also have jvm and guildland clients so this python client is basically just shown for illustrated purposes feast is also able to allow you to generate statistics and use those statistics to generate schemas and then subsequently use those schemas across training runs or as well as in production to validate data and prevent data shifts from occurring so here's an ex yeah so feast integrates with tfx feast visas interoperability with tfx through its features specifications so tfx and tftv are tools developed by google that allow you to find schemas on data sets and these schemas can be used for validating data we felt that these were great tools and we didn't want to reinvent something that really existed but we wanted to complement them and so that's why we integrated those schemas into our feature specifications so fees is able to generate statistics that are compatible with tftv so once you load your data into feast you can export these statistics in a tfdb compatible format you're all you've also been able to use those tftv schemas to do validation of the statistics and you can do this during training runs and you can do this during at the ingestion term as well when you're streaming data into your stores and fees is also able to integrate with monitoring and alerting systems so currently we have integration with prometheus so our injection pipelines can produce metrics and statistics and these statistics can trigger alerts based on schemas that are defined upstream so these schemas are defined based on the properties of data by the data scientist i think this is one of the key values this is why we wanted to integrate tftv and tfx into feast is because the data creators know the most about the data that they're publishing or authoring for consumers they have the domain knowledge for it's important for them to be the ones defining the schemas that ultimately will be used for validation and if we only limited this to tfx or tfdv it would be a batch it could be a siloed batch process that happens in a single pipeline by incorporating this into our feature specifications we can apply those schemas not only for training but also for serving and ingestion so throughout all the touch points that features occur in your organization here's an example of inferring schemas from existing features and then registering that with feast so the first step there we're generating statistics based on an existing iris feature set that is within feast that statistics object that is returned and calculated is the computation is all done by a feast behind the scenes so it abstracts away the infrastructural and computational aspects but the schema of that the statistics is returned is 100 compatible in fact it is a tfd schema that's returned um excuse me it's it's tv statistics that's returned those statistics can then be used to infer a schema so that's all happening client side outside of feast you can then use the normal tftv approach of you know tweaking the schema and setting the the values and then you can so what we want to do is once you've defined that schema you want to update the feature set that you're working with in order to enrich it with the schema so we're going to retrieve using the get feature set method the schema sorry the feature set we're going to import the schema into that feature set and we're going to re-register it using the apply method so the blind method is an important update method for that feature set so we're re-registering that iris feature set with feast and if you look to the right you'll see that now the irs feature set has not just the name and the value type of the feature but also crazensburn fraction when count so these are tfx tftv properties that can ultimately be used to validate that those features the schema can then be used for not just for validating training data sets not just for validating ingestion or serving but it can also be used to indicate the intent and the the properties of that feature and an important thing to know here is that when you're registering these properties about these features you're doing it on a feature by feature basis so even though they're grouped together here when they're used ultimately they can be used across feature sets so here's an example of creating a training data set and then returning statistics and schema from the training data set so this is going back to our previous example of the driver based features so you're sending your driver entities to feast you're sending a list of features and this is going to return the data set but for statistics and validation purposes these will also return pre-computed statistics based on the data set that has been produced so for that exported data set it will run a tftv compatible operation that produces statistics and it will also provide the schemas that have already been registered on those features so remember now you're picking features that are crossing feature sets it's any kind of driver features an organization you can pick from and it will propagate the schemas that those feature authors created and it will return that with your data set so you can use tfd client side to validate the data that you're going to use for training your model prior to training and you can just use tfvdb to detect those anomalies because we are using tftp you also benefit from being able to use facets so facets illustrated on the right using that ui so you can just generate the statistics and visualize it for for both basically for eda or for debugging purposes so what are the the key takeaways from incorporating fees into your lifecycle with email so the key value adds a feast are with sharing teams can now start with selection instead of creation they can quickly get up to speed focusing on ml aspects on iteration they can independently attract in different aspects of the different stages of the life cycle without having to be stuck on a monolithic approach you know they can focus on independent aspects feast provides consistency between training and serving and ensures point time views of your data feast loss or centralized definitions and it allows you to reuse features throughout different projects fees also ensures the quality of the data that you are producing and allows users to encode more data science to encode their knowledge into those schemas and to ensure the quality of the data that ultimately reaches your british remodels so finally the road ahead so feast 0.6 is landing soon it's landing in june of 2020 with that we'll have statistics validation improved discovery data functionality the community is currently developing data break support zero support aws support we have bigquery sql sources landing soon jdbc connectors we're looking at developing a drive features system for online serving as well as a user interface and allowing for automated training certain skew detection so we encourage you to get involved there are some links to our open source project we're on the kubeflow slack channel you can have a look at our mailing list and join that if you want to find out more there's a link to the slide deck and yeah that's it from my side thank you you 