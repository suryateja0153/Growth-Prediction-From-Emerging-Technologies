 hi everyone victor o'lachlan and sartre pattineau here representing bank of new york mellon's clearance and collateral management business excited to present what we've learned from our journey in helping solve a key financial services challenge that has the potential to bring big time benefits to our clients and the us treasury market as a whole all realized by leveraging google cloud's powerful data science platform as the global digital business leader for clearance and collateral management leveraging data through analytics and machine learning is a key focus area for our business in order to enhance our ability to improve the client experience whether it's predicting a failed transaction or offering a prescriptive analytics solution to resolve it our future is firmly planted and operating as a data first company bny melon has a long history of serving our clients as a cio of clearance and collateral my responsibility is to focus on growing this business but it is important for us to know how we got here and how the business grew to where it is today it will share the importance of this exciting new service that we are building it actually starts with the average investor who for example saves into a 401k if the investor rebalances her 401k account shifting from equities in favor of bonds then her 401k administrator may get those bonds by contacting their trading desk who in turn goes to the bond market and says i'd like to buy treasury bond a b c for 95 of the face value buyer and seller named eb make their offers and a trade is arranged the buyer and seller agree to swap the bond for cash and they notify a third-party service provider of the agreement because they want that service provider to manage the actual exchange of bond for cash at the end of the day the fund admin will agree to many such trades throughout the day in most cases that exchange is successfully completed but sometimes it's not a counterparty may not be able to deliver the bond due to some unforeseen event occurring after the agreement but the investor still needs their bond the fund admin keeps some extra on hand to make the investor whole in case of a settlement fail how many extra should the admin keep well that depends on how often a settlement of failure occurs if the admin knew that they could right-size their stockpile and free up valuable cash this cash could improve market liquidity by buying and selling more bonds or fund the development of new services for the investor so predicting settlement fails is key but that would require a total view of the market the fund admin doesn't have that but the clearing and settlement service does b y melon is that clearing and settlement service because we're in the center of the market we have a total market view and with that view plus google cloud's powerful analytics we can predict which trades will fail to settle allowing the admin to predict how many spare treasuries to keep on hand of course being why malin didn't begin in the cloud era we had a 236-year head start in 1784 we were founded by then u.s treasurer alexander hamilton some of you may have seen the musical in 1789 the first loan but to the u.s government was provided by bnym in the form of a treasury security fast forward to today treasuries are the largest and most liquid fixed income market in the world with b y melon still deeply involved in this market clearing and settling over 9 trillion in fed eligible securities a day because of our important role in the financial markets a core mission of our business is to invest in resiliency and value-added services that help create efficiency and stability and helping solve the fed eligible securities fails challenge is a prime example of that so we kicked off of an investigation or what we call formed a hypothesis into how we might help predict these settlement fails the fails vary from day to day but generally they amount to one to two percent of approximately 4.5 trillion in security deliveries this percentage seems like a small number and it is but due to the sheer size of this market it amounts to approximately 70 billion of settlement fails a day transactions that do not successfully settle by 3 pm eastern standard time each day are considered fails so we aim to predict the fails at 1 15 pm each day providing approximately two hours notice sartok can you tell us how we went about this there are four stages to our machine learning process stage one is focused on data ingestion where we built a data pipeline to move data from our on-prem transactional system to our on-prem data lake all the way to google cloud storage in the second stage we have focused on data preparation the first part of it is ensuring good data quality controls and the second part of it is feature engineering in the feature engineering step we start off with transactional information things like account security information and transactional information and then we iterate across a multi functional group try to get more information from the clients from the business from products from developers as well as from the data engineers at the end of multiple iterations we landed up with 51 variables 44 of them were something that we identified as part of those iterations and most of these variables were economic in nature things like demand supply variables liquidity variables and transaction velocity variables the third stage is around model building and when we are building the model we were targeting three objective functions one accuracy second performance and third interpretability and we evaluated a whole plethora of models from simple regression to tree based models all the way to neural net as we iterated over the models we did finally settle with the light gbm model and the reason for that is the balance between accuracy and interpretability our clients were very interested to use a model which provides them directionality as to why a transaction has a higher chance of failure than another transaction the fourth stage is building the production pipeline once we built a model which which we train and test having an automation to take the output of the model the output coefficients into a production system and ensuring that that is seamless was critical for our success also important was the frequency as we retrained the model every three months and also going through the model validation to ensure that the model did what is supposed to do what did we learn from this entire process three things first is data culture we are trying to transform ourselves from a transaction based organization to a data-driven organization and if you think about data we think about the five v's which is volume velocity variety veracity and value veracity was the most important concept for us because we wanted to build a solid data control process and solid quality controls up front the second thing we learned was around our transactional systems and storing everything traditional transaction systems we focus a lot on storing our inputs and then we have our algorithms and that we store the outputs which is used for client reporting our end-of-day regulatory reporting but as a part of this process we tried to store information at every point in time during the algorithm for every decision point the algorithm took as a result we captured inordinate amount of risk credit usage inventory position and cash information because we do not know which of this data is going to be useful for our next experiment thirdly we realized very quickly that the success of this project has to be a mix of expertise from the data engineers as well as the subject matter experts as we were iterating there were a couple of sprints where we had 95 plus accuracy and when we went back and looked at it we actually found that we are predicting the past so that was an interesting thing that when your model is significantly good go back and check your model the second thing in this entire process that we learned was the concept of agility this is a multifunctional team and the goal of the team was to have small experiments in a piecemeal fashion and have an hypothesis for each of them and as we addressed as we hit those hypotheses we move forward or we go back and restart with our new set of sets of hypotheses so for that to happen it is important that infrastructure is not your bottom leg and infrastructure is an enabler for that and the third thing that we learned is digital transformation this is not about technology it's about people process and technology together when we build machine learning projects those are not deterministic these are exploratory these do not have a requirements document a design a development and a testing process like a waterfall you have to think about small experiments you have to look at hypothesis and you have to iterate so fast iteration is key to our success now let's talk about results our target was to predict 40 percent of the fails with 80 precision and happy to say that we are able to hit those objectives now we'll hand it over to victor to talk about what's next in the service thanks sarthik so next is a client trial after approximately a year of development and testing we are now currently in the process of trialing this service with our key clients this includes uh interacting with the service in a production like environment and evaluating the the benefits of the service so that they can trust that the service is accurate and consistent as the client provides feedback to us we can seek out opportunities to make client specific model optimizations are there specific operational nuances about that particular client that are required in order for us in from a modeling perspective in order for us to refine the precision or recall in addition we will continue iterating on improving the service through various market cycles for example in our line of business month ends quarter ends year ends are important times for our clients and the dynamics of trading change therefore making it more difficult for us to predict settlement fails but by identifying these different cycles adjusting for the different training periods and looking at different model variables we can find opportunities to improve the model during those time periods and finally as we predict outcomes we can then identify opportunities to have prescriptive analytics integrated into those predictions to help mitigate the fails which will then allow for our clients to get more value out of this service now finally we'll hand it over to sarthik to just talk about why google cloud why google cloud so first i want to talk about why cloud if you think about this project the two focus areas were around scalability and agility the ability for us to run complex machine learning models requiring high infrastructure requirements and if we had to do it on-prem it would have taken us significant time as well as it would not have been cost efficient so cloud was a natural solution for this project why google cloud there are couple of reasons here one is google bigquery bigquery is highly scalable and extremely performant our developers and business analysts used to run queries on our on-prem databases of two to three days of transaction data on google cloud bigquery they were able to run the same set of queries over two to three years at a transaction level and the performance was very similar to two to three days on-prem the second thing was about collaboration this was a perfect opportunity for the google engineers and the bny melon engineers to work together both of us focused around data how do we build strong data practices in terms of data ingestion data storage data indexing as well as delivering data to our clients it was fascinating to watch both the teams come together and solve a business problem thank you for watching this and hope this was helpful for yourselves you 