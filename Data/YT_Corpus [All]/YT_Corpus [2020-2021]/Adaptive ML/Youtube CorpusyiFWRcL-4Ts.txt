 you [Music] thank you for the kind introduction so it's a real pleasure to be here for the next hopefully a bit less than an hour I'm going to talk about a couple of topics the first half roughly being about optimized complex metrics in the second half thinking about how to pick good metric and both of these are ongoing work I'm hoping both of these sparking conversations or quality of I'd be interested in thinking about some of these questions maybe putting some of these ideas board okay so everything I'm gonna talk about is with X and collaborators the first to Brian and Cheyenne okay so I'm gonna just go right ahead and try to set the stage I'm gonna set up a cartoon problem where you're interested in building some kind of medical decision-making engine so it was a medical classifier this is very clearly going to be a cartoon so lots of holes many of I'll try to hopefully flesh yourself as that goes along okay so I'm interested in say predicting whether a patient has Alzheimer's or not and I'm interested in this you'll need a treatment decision so I'm gonna decide what to do based on my diagnosis okay rising young learner they did some awesome model and they get some pretty high accuracy so this looks good and one of the men trying to use this directly I use this action a decision of course most of you should the many reasons for this but at least one of them is that there is a ton of imbalance we know in population and so it's easy to get high accuracy but not have learned anything beyond this when I'm gonna make a decision based on my classifier so they're asymmetric coughs and treatment okay and so an interesting question as well as of thinking about how one makes decisions in a way that manages a symmetry so let's just go back to the sort of issue with rates so these are semi real numbers it's sort of easy to come up with a cartoon and work this out but based on the population prevalence and based on accuracy you can sort of show that the false positive rate of classifier like this can be as high as 90% so this is when the model predicts a healthy when the patient has Alzheimer's and the opposites when model predicts Alzheimer's and a patient is healthy can be quite low so number of errors if fraction of errors that the model makes in this setting okay so you sort of learned a little bit more about modeling you look at these asymmetric and he's bad rates on sort of different chunks of predictions and so because of this you decide that you don't like neural networks and so you want to use a different model so say you use a sort of nearest neighborhood kind of classifier which is it's my best approximation of the nearest neighbor classifier and so again you look at the results afterwards a little bit worse but maybe that's fine and then the false positive and false negative rates change quite a bit so you're a little bit better in false negative rates even though you've lost quite a bit and false positive and so there's a different trade off different decision that you end up making here okay so I'm gonna just keep this going because why not 50 of these so so now I have a support vector machine again different prediction accuracy different trade-offs in terms of false positive false negative rates there's a fun one where I always predict healthy for everyone if anyone remembers the numbers at the beginning or maybe someone can guess what might happen in this setting okay so yeah so you end up with you know some like you know real high accuracy in this case ninety nine percent and again the obvious reason is that there's a very low prevalence of this disorder in population if you build a decision making an engine not accounting for sort of all the steps that go in you end up making a prediction that seems reasonable from all the point of view but maybe not a good one and so the question that comes up from all this is of all of these models that all have different kinds of errors and different trade-offs in there are making profile which of these models is sort of the right model to pick for a certain decision-making task assuming again I'm caveat in a bunch of things I'm assuming that sort of the ML model is plugged in right into a decision making process which you probably shouldn't do right now for most things but anyways okay so this gets tied into I think of slightly subtler question but a very important one which is thought about in machine learning broadly but mostly by sub-communities and I think not so much by the broad ml community we're just thinking about how one should measure how good the model you're building is how do you measure what good performance is and like all good questions I have a great answer so the answer is it depends so you don't have a clearly there isn't the simple clean answer unfortunately for this because it depends on what you're hoping to use the model for depends on the relative cost and benefits of sort of whatever the downstream tasks that you're interested in and so one way to try to quantify measurement of performance of a model is generically what is known as a metric and I would just dig a little bit more into this metric idea and talk a bit about some work we've been doing thing about metrics for primarily classification problems in machine learning okay so a very pretty important object when thinking about classification in particular is the confusion matrix this should be familiar to most of you and I alluded to it at the beginning so just a quick summary so what the confusion matrix tries to capture the statistics of different kinds of errors when you're making predictions so on the sort of the top here I have the ground truth which in the binary setting is either 1 or 0 and here I have the prediction which is going to be the 1 a 0 for every instance and the 4 numbers tell me sort of how the fraction of times either the first box drew positive I predict positive when the ground truth is positive or a predict negative when the Grantham's negative and sort of the two errors that I can make so I apologize true positive to negative and in two hours that can make false positive and false negative and if I'm interested in accuracy then the thing that I'm measuring is roughly trying to average the diagonal so you can show fairly easily our accuracy is the average of a diagonal of the confusion matrix is the average of the true positive and true negatives or error sort of is just the average of the off diagonals so an average of the false negatives and false positives but beyond basic accuracy if I'm interested in sort of other ways of quantifying or measuring performance you can write essentially all you have to work pretty hard to come up with a metric they can be written as some function applied to the confusion matrix for classification problems and again I'm happy to talk about Fline but it balls down to this discrete nature of the decisions and the discrete nature of the set of sample space and because of that sort of mostly in the binary setting the sort of four ways things can happen and so everything ends up being summarized even fairly complex metrics and roughly been summarized by some function of the confusion matrix okay so as outline metrics used to compare classifiers we're going to talk a bit about how tried optimize them when they're potentially complex and I guess a bit of an aside but I'll come back is that these are thought of US population properties so thought of his expectations respect to whatever your data generating distribution is but clearly you can approximate these from data so you can count the fraction of times your classifier predicted one and the ground truth was one and this gives you an estimate of the true positives for instance as opposed to whatever the population quantity is okay hopefully so far so good okay so as you might imagine a lot of metrics so so each field often spends a bit of time coming up with what they think is a good metric for the area and this is only showing here only such classification metrics and as you might imagine this is a tiny tiny subset I'm aware of at least one paper that's entirely a metric paper is just essentially a list of metrics and some in you know thoughts on when they might be good or bad so it's it's a it's a fun process to come up with measures potentially for some task that you're interested in and this begs the question if I come up with some exotic complex metric how do I come up with a good procedure for optimizing it so the talk today roughly has two parts to it as I outlined at the beginning before the mic came on so this is good to repeat it so the first part is thinking about how to optimize complex metrics so I give you some metric that happens to be not as straightforward as an accuracy metric are they're good strategies for trying to come up with a good optimizer so a classification procedure that has good statistical properties for that complex metric and the second part I'll reverse the question a bit and talk about a bit of a new problem for us but one that we think is quite important which is given some new problem have to think about how to measure what the performance is and we're going to reason about one strategy which we think is a reasonable first step for thinking about how to come up with new measures a good measure for estimating performance of a classifier but one that is tied to the task that you're interested in as opposed to something generic that you pull off the shelf okay mmm so I'm gonna channel a bit of web spam you might not remember this but this is to be your thing it might still be a thing one simple trick on many websites so the first part of this is gonna roughly ball down to a fairly straightforward procedure that works pretty well as a generic approach for trying to construct classifiers optimized complex metrics in order to set this up I'm gonna take a half step back and instead of thinking just about binary classification gonna expand the space a little bit to think about multi-class classification from binary to multi class is fairly straightforward in terms of here I'm looking at the confusion matrix so again I'm measuring statistics of different decisions versus ground truth in the binary case it was two choices and the multi-class case is some k number of choices so the ground truth is you know 1 through K I'm using one numbering here and the prediction is 1 through K and every pair gives me some number that tells me the fraction of times the ground truth is a certain prediction and sorry the ground suit is a certain value and the prediction is a certain value and so you you can again get this measure of what the model is doing based on these statistics and as you might imagine accuracy takes the same form so again accuracy is just the average over the diagonal of this sort of more complicated thing but it has the same flavor I write this because this is going to come back I'm gonna think about accuracy and friends as essentially linear in terms of confusion matrix so linear being broadly trace a C transpose C is usually symmetric if I write this hopefully you can see this this is what I mean so sort of just think about this as a linear function of the confusion matrix so element wise product sum it out essentially what comes out sorry I didn't hear the voice Oh so trace is the sum of the diagonal after the product yes actually you okay yeah you don't need to trace here so here what you want to do I apologize I made a mistake here so here what you want to do is multiply the two you can do it a couple of ways so you can do a lot in matrix multiplication and then computer trace or you can do L my element wise multiplication and then sum of all the entries and both of those will get you answer so either way it's fine the trace is sum of the diagonals of whatever the product in the inside is okay other questions yeah so I mentioned symmetry because I guess it it you it may be many times nothing changes if it's not so it's the same definition mostly what you want to be able to do is reweighed every error in some way and so the entry of a she just matched over waiting that you're doing so if you do transpose then you need to match so maybe I should take out the transpose the details here not important what's important and thanks for pointing out I guess my sloppy notation is that you want to be able to do this sum over IJ a IJ CIJ so that that's the key thing so you wanna be able to reweighed every kind of sort of arrow statistic that you're getting and this is thought of as the simplest case and we'll spend a bit more complex cases does it answer okay the beginning will start with linear and then I expand to more complex settings okay other questions okay hmm no trace okay good question though okay so what do people do in practice so in practice people mostly ignore the particular sort of so if you read a set of standard machine learning paper what people do is construct some kind of model for solving a certain task then evaluate it on sort of ten different metrics and not necessarily tying the test that they're interested in with or the model data building with the measure that they're using and this may be a good strategy but I'll argue that you can do something maybe a little bit better and so the standard strategy say for multi-class classification is to first construct some kind of score function almost always so logistic regression random forests some kind of neural network and then your classification just becomes the maximum score out of the list of scores and again this is sort of standard approach that you would take who's trying to build a multi class classifier and what argue it seems it feels trivial if you followed me so far because it's the obvious thing to do but I'm arguing this because it's actually not done very often and again this feels like a sort of cheap easy thing to do to to help you predict sort of more or get better predictions if your goal is sum so for now just think about the linear case so I just have a linear weighting of the confusion matrix as my metric of performance a better strategy is again the first step is the same so just estimate some kind of score function but instead of just picking the max what I want to pick is some kind of weighted max where the max is weighted by this rewedding essentially and you can show that the sort of Bayes optimal classifier for this metric when I have a real waiting of the confusion matrix balls down to rebating the scores so instead of again instead of just picking the max what I'm gonna do is for every choice I get a list of weights irie weights I multiply them out and I pick the prediction that is maximum over all these choices okay so fairly straightforward procedure but I think something that is cheap to do and can lead to much better performance in practice so so to drive this point home I will do a sort of quick simulation experiment so suppose I just sample data from in this case is just Gaussian normal data push through a logistic function and the metric that I'm using is based on just again just to make analysis simple there gonna be two parameters that matter the most one parameter is this d2 which is tied to the skewness of the of the a matrix of the relating so I'm waiting class-by-class there's a matrix is of the form of a 1 1 a K K and 0 elsewhere so I'm just some classes are more important than others essentially is what's going on here and the d2 is capturing how skewed your waiting is of d2 depending how I choose d-28 a halves of almost uniform across classes or I can make it so that one class is hugely more important than the other one again there's a cartoon example just to illustrate what's going on here and then the other parameters its importance is wk which ends up controlling sort of the skew of the score function so the again depending on how I tuned this I can you have a score function that roughly has equal probability predictions for all the classes or a score function that sort of predict one class much much more than the other classes modulated by the Gaussian underneath it ok so I can set up this simple experiment random data fit some logistic regression model it's sort of the right model class here so that's not gonna be an issue and then I can try to look at what happens if I either use the generic strategy of not bothering about the relating or using this strategy that uses the rear weighting in the decision-making procedure and the results are mostly predictable so the way I'm gonna measure results is the performance ratio so it's ratio yes this is the generative model and I fit a logistic to it so the score function yes the main thing I want to control again this is a cartoon yeah the main thing I want to be able to control anything I'm trying to get at here you can replace this with other models the thing that I want to control is P of y equals K given X and I want to control how uniform this is over the X's are all skewed this is across different sorry across K so across the different classes whether this is sort of roughly equivalent for every class so they get predicted at fairly equal fractions or it's very skewed so one class gets predicted much more than the other one that's the important point here everything else is yes but again I want to emphasize that the point is to play with the skewness so from the point of view of the from the population point of view the sort of specifics of the generative model are not critical the thing again that I'm trying to make sure I capture is what's going on at the conditional distribution level so if you like you can use some more complex generative model as long as you can I want a simple experiment allows me to control the skewness of the conditional probabilities and the skewness of my weighting of different classes and that's sort of the key idea here everything else is interesting and is worth exploring what I want to think about that is maybe less important is the main story good question other questions okay so I'm controlling two things and I want to see what the effects is mm-hmm and again this is a made-up problem but the points made here is if you know how you're awaiting classes use that information in your decision-making procedure that's on saying essentially but saying it in a convoluted messy way but that's the key point here so I'm gonna measure how good the model is by the ratio of what happens if I actually apply this way to be processing knowing the measure or waiting for the classes versus if I just pick the most likely class without taking into account they were waiting up the different classes and the main takeaway that I want to sort of show here is that in terms of the metric of interest or in terms of the final task in this setting again this very toy cartoon model setting I see up to sort of five times better performance by incorporating their weighting it's my classification procedure then if I just pick the most likely class without incorporating this sort of the weight how important different classes are in terms of how I pick my model yes so the performance here is the related accuracy because again the goal is that the target problem is the related problem and so the question is do I bother awaiting in my classifier if I'm interested in the related problem or not again it's it feels obvious in this room but I will bet you that most papers even if they have a sort of relating previously specified don't bother to sort of build that we're waiting into how they tune their classifier it's mostly what I'm trying to say here okay so so the point of this slide is that again it's a simple strategy but it can have a big effect so if you know how important or you have a weight for different classes use it in the way you're building your predictor it's a cheap thing to do and it can sort of improve your prediction by a lot okay so you can do similar things and I will I won't go into a ton of detail because it can get a little bit technical but I'll give sort of the highlights of of this approach but you can do a similar thing for general metrics and not just for re waiting sublinear measures you can do this we're sort of nonlinear measures as well so f measure like a ratio type function many other kinds of function classes you can do a similar kind of strategy the setup is very similar sort of two steps the first step is you estimate some score function at a high level theory says as long as you have a sort of calibrated score function you're fine the real conditions are a little bit weaker than that and then you're going to need to find every weighting matrix and then re weight to your prediction based on their rating matrix and I'll try to give some examples and then some sums of high-level ideas of what this Roommate weighting matrix might look like for more complex measures and a few few slides okay so again if I give you some arbitrary measure there exists a strategy for sort of most measures that looks like estimated a score function and every weight the score function are to find a good decision maker so for sort of probably maybe bayesians in the room this I'm just this is effectively the Bayes optimal a plug in Bayes optimal predictor so the based off of a predictor looks like score function ree weighted somehow i am estimating the score function and an estimated are awaiting combined and together and i can show that this has sort of good statistical properties okay so you can do this in lots of settings us and others so we've looked at this for binary classification forgive my animation i needed a slight animation this was the only one I could think of classic old-school animation so we've done this for binary classification so we talked about that already seen confusion matrix multi-class classification we just talked about as well so K classes K predictions I'm looking at the trade off and statistics you can do this for multi-label classification so this is the case where I'm making multiple binary classification decisions simultaneously and then you can also do this for what we call multi output classification I'm making multiple multi-class classification at the same time and roughly the strategy doesn't differ that much they're details that are different for different settings but it's a very similar kind of idea okay so I also touch on a bit of verbal application so here's an application to recommender systems so I have users and have items users rate some subset of items I'm interested in predicting sort of the rest of the matrix I want to predict the ratings of the users and other items essentially I'm doing this in a discreet setting so it's a setting where Jesus give actual ratings one through five one through ten some of us may have been doing machine learning where this was when this wasn't a problem that lots of people thought about research but anyways so this might be a good flash back for some people here anyways so I predict you know 1 through 5 1 through 10 and the what I'm interested in is trying to find a good predictor simultaneously over items for for all the users and I'm gonna do this by predicting the score for every user and then sort of finding a procedure to decide what to predict across our different items in terms of did they rate this item a 1 2 3 4 or 5 and in terms of how I'm going to measure performance a good magic here is an ordinal style measure and roughly what it's trying to do is the cost of predicting to when the grounds worth is 1 is less than the cost of predicting 500 grams with this one so you want your measure of error to sort of scale with how far away you are from the user prediction right and so this is how you construct your awaiting matrix so it sort of penalized is you making bad or predictions that are farther away from the ground truth okay and so this is a problem setting what we did here is that we looked at an existing model a probabilistic model for sort of ratings prediction it's called or dreck I think it won the best paper award at rexis that year it's an excellent model and does something interesting the main thing that it does is that it tries to build ordering into how it builds the sort of probabilistic prediction and so the scores have sort of ordering prior knowledge built into them and so it's a good model for predicting ordering what I'm going to show is again to drive home this point is I can consider two strategies I can take the or direct predictor which knows that I care about ordering and just use that as my final prediction or I can take the or direct predictor pre post process it using this sort of linear weighting and then use the linear weighted predictions as my final predictions what I'm showing here is again even if you model knows about the structure using it in the sort of final prediction procedure ends up gaining you something in terms of performance so we show some non-trivial performance gains again I'm dating myself a little bit but in this world those numbers are quite meaningful in terms of a gap showing improved performance just by adding this sort of final pre-processing post-processing that relates based on how I'm scoring the final model again I want to build a score such that it penalize is far away decisions much more than close by decisions and that's sort of all that I'm building into this model and again most papers would not bother to do this that we just use because the model has all this structure built in you just use that model to make up and when I'm advocating for is well if you know the score and you have some kind of score function that you expect to have sort of good good a calibration properties then you should actually build that into the way that you build your your final prediction okay so I think the point here is trying to tie your procedures to the tasks that the classifier is meant to be doing so again we chose the tasks to be making predictions such that you minimize this ordinal score you could replace that as instead of using that measure you could choose something else that you think is more appropriate for your task and the point of this whole set up is whatever meant measure you pick you should pick a sort of prediction procedure that is tied to that measure that you're interested in as opposed to some generic one so I don't know if that answers your question but okay maybe you can expand a yeah so for folks familiar with this literature this will be considered a non-trivial improvement because sort of at least for the standard data sets it's hard to squeeze a bunch more performance I think my takeaway is you're leaving something on the table it may not be a ton but it's sort of non trivial amounts of sort of the performance gain for the task if you don't do this tie in between the task and the prediction model but I think that's a fair question yes it's giving you a rating for every item distribution over so Furby use item pair or distribution over the different scores so one through five in this case so it's being treated there's a bit of nuance in your question which is possibly vest on offline but multi output in the sense that I am predicting multiple items at the same time I think you're alluding to correlation maybe yeah I will so you can define multiple problems so there is correlation in the set of ground truth this model is capturing it conditionally based on the sort of underlying structure in particular I didn't mention this but does lower ranks structure underneath the model that captures correlations between users and items so the model is capturing correlation to some large extent I will die I can spend an hour talking a bit more on multi output so I will defer to maybe if you don't mind asking offline I'm gonna expand on this a bit but I all just broadly say that there is correlation here so it's sort of a standard multi output you can consider this as a multi output problem or I'll say by Fiat that you can consider doesn't know well if you have a problem and then if you don't believe me we can chat offline okay other yes yes so this metric was related because or in the way I could actually be more specific the a matrix will look like so if the grounders is one of you predict two will be sort of one round there's one you predict three so two grand for the coin critic for three and so forth and fill out the rest of the matrix based on the gap so it's it's AI J is I minus J I did one - I apologize yeah and yeah I should have clarified so I turned it from a error to a utility tell me what you mean by I'm missing what you're saying let's go back to the previous slide if you don't we check the details it's a really good question it's normalized to slip switch it from a utility to a score it won't be one because of the point that you just made so I made a mistake in the description of the metric the normalization should be the maximum difference roughly instead of rescaled so there's been zero one I will double-check the specific metric but you know I wrote it this way in fact I remember what it is now it's 1 over I applied is 1 over K so it's normalized based on scale and then because I want to focus on the utility I do one - I think this yeah thanks for jogging my memory so the nura's ation makes sure that big things and small things roughly 0 to 1 and then to make because i like to work with utility instead of losses maybe one - and it's common in this setting as well ok good question other questions cool all right so let's move on a bit I'll talk a bit about why are we waiting works and for thinking about we're waiting I think it's useful in the way that we came up with this strategy and others that have looked at this problem is instead of thinking about it in terms of classifiers sort of take a step back and think about it in terms of confusion matrices and that seems to be the right abstraction for classification problems again because for most problems you can abstract them out into just re weighting in some way these kinds of errors so to sticks ok so specifically what's going on is that I can actually map out if I take a particular problem assume some generative distribution I can map out all possible classifiers it's hard to do but you can characterize the space of sort of all possible confusion matrices for a certain problem setting if you're familiar with the ROC curve it's effectively the ROC curve for this top portion so this part is the ROC curve for that problem and then for completeness you can also sort of flip it around and I intentionally make bad decisions so flip every classifier to make more negative and positive and you can map out this sort of the other side the negatives have the ROC curve it has some nice symmetry properties which are actually kind of fun geometrically and happy to chat offline about those but anyways if you mapped out the space of all possible confusion matrices you get a lot of actually really interesting geometry in particular the space of all possible confusion matrices ends up being a convex space and because of this it ends up in terms of characterizing what good classifier ends up giving us a little bit of insight which I'll try to briefly outline here so to make the discussion easy I'm gonna focus on metrics which we've roughly called monotonic metrics which are metrics that reward you for making good decisions you can construct metrics that don't it's probably not a good idea but if you're flexible to do whatever you want you could do it if you use a metric there was for making the decisions roughly the match is going to be monotonic in the space in particular the gradient field is sort of it's gonna be sort of up and to the right somehow angle might be different it might be a bit of curvature but roughly up into the right is a good thing and so just by analyzing what's going on between the metric and the space of confusion matrices matrices it might be obvious to you already that if my metric is monotonic then the optimal must be somewhere on the boundary so the best confusion matrix out of all the possible confusion matrices for this particular metric must be somewhere sort of along this boundary of confusion matrices and boundaries are nice because boundaries have nice properties mmm so boundaries a convex X in particular you can construct a support function which is just a linear the tangent plane at that boundary points and so what this means is that you can show that if a metric is monotonic then it must be optimizable by this sort of plug in relating strategy that targets a boundary points and the question now becomes to which boundary point did I target or sort of what this line is and for linear problems it ends up being easy it ends up being essentially just this relating matrix for other kinds of problems that ends up being a bit more complicated but you can show that the optimum is given by the gradient of the loss at the at the best possible confusion matrix this is a weird problem because in order to find this tangent plane I need to know C star the best confusion matrix in order to find the best confusion matrix I need to know the tangent plane so it's a bit circle in practice there are sort of good empirical procedures for trying to estimate what the good boundary point is using plug-in procedures now I'll talk about the binary case and I'll defer a fly into sort of other cases one nice thing I should also mention is that this characterization is exhaustive so this rough property is sort of being optimized a lot the boundary characterizes sort of it goes two ways so I want from one point of view tells us all the classifiers that can be optimized by this route waiting strategy optimized in a certain sense meaning the strategy that I come up with will have will be consistent so in large sample limits will converge to the best possible classifier the Bayes classifier in this setting and it also says sort of roughly the other as well so if a classifier doesn't end up on one of these boundary points then there were waiting strategy will not work it's possible to come up with other strategies for doing it for 99.99% of the cases it's not necessary because again we almost always use monotonic functions because we like metrics that reward good predictions so this is a good enough characterization for almost every case that you'll come up with sometimes choosing a specific fraction where the cost just want to make sure that those two could be different yes so I guess to expand a bit on your question I'm assuming that I think it's a fair point that you bring it up but I'm assuming that for every problem that you interested in you can you're focused on characterizing things in terms of some function of isometric decision costs what's the actual so the question of like picking particular points along the former surface at which to predict I think is an interesting question there are ways to do it I'm not gonna I'm not talking about them here I probably made things worse by trying to describe this sort of metric selection problem in terms of picking an operating point what else that what I was trying to express was that their metric trade-offs and potentially complex trade-offs in terms of different kinds of errors and that's what you want to capture is any metric okay so as I mentioned so you can show that this overall strategy is consistent again it's a bit abstract right now I'll talk about the concrete case of binary classification in a second but roughly the idea is estimated score function figure out this magical B matrix and then based on the score function and B matrix use that to come up with a good classifier and so in particular with binary classification you can use a similar procedure one nice thing about binary classification is that you can show that the degrees of freedom in the a matrix actually balls down to just a single number essentially which is effectively where I'm going to I am I going to put my threshold in this binary classifier and so the strategy now becomes estimate the score function estimate a good threshold based on validation data and then make a prediction based on that thresholded score function so the main difference is that instead of thresholding for folks familiar with this let for accuracy always threshold at a half for more complex settings your threshold somewhere else and the threshold is tied to the sort of metric that you insert in optimizing and you can show rates for this so roughly under weak conditions I think the main thing to keep in mind is that you pay a log on cost versus if I need a threshold ahead of time so if I was interested in accuracy I'd get you can get to the order 1 over N and here I'm paying this extra log n factor because I have to find a good threshold in addition to sort of finding a good score function okay so that's that for the first part in the next ten minutes I'll very quickly talk about how to pick metrics so again a quick reminder there are lots of metrics in a binary classification case there are lots of metrics in other cases as well I'm showing just motivating for recommender systems the point that metrics are not always exchangeable in the sense changeable has technical meaning but what I mean here by exchangeable is that optimizing one metric doesn't necessarily do well on a different metric and here's a sort of reward example from a paper thinking about optimizing top and accuracy based on a procedure that was very common there which is if you're anyone remembers the Netflix problem everyone was thinking about maximizing minimizing are messy and sort of main statement newspaper is minimizing our MSE doesn't necessarily give you good top end performance again two different metrics up maizing for different things the performance of the sort of different metrics are not necessarily the same regardless of this as anyone that is play with particularly apply problems will know is that you really can't easily escape thinking about how to measure performance so in addition to sort of picking model classes optimization thinking about how to get your data an important part of the ml pipeline is thinking about how to measure performance and that ends up affecting or should affect sort of the rest of the whole process so you might want to ask how experts pick metrics lots of experts in this room I'm going to argue for 99% of the cases and I could eat me anyway people just use whatever is fault in Deerfield and I just throw that on to the problem in industry it's maybe a bit more important to get things right and so you might run some maybe tests or user surveys and maybe hire people to try to come up with the quantification of what good performance is for a certain problem and I have one controversial slide the the main important part is this bottom thing is a suggestion by a fun colleague our metrics are just almost always completely made up so so we've come up with a general framework that we're expanding on and I'm happy to again talk a bunch more here on thinking about this question of how can i elicits good metrics by interacting with either an expert or a population of users the first problem that comes up is I need to quantify how good a certain model is people are quite bad at sort of giving scores to models if I try to ask them usually they're a little bit better at doing pairwise comparisons and so if I ask people you know is a certain model better than another and asserting setting or if I do a B test and I sort of measure a whole population I can usually get a much better quantification is characterized by the false positive or negative but in more complicated situations to be harder to say even so the setup we have requires that you roughly decide the set of things that you're comparing ahead of time and what we're assuming is that there is sort of an internal trade-off function but it's hard to make turned out into math and this is roughly trying to turn sort of internal trade-off function into math there is a problem in the middle that we haven't fully addressed this doesn't address we're thinking about how to address which is what you're loading to that we've pre decided the set of things that we're going to trade off and there might be a much richer space of potential thanks including things that are not sort of easily captured here we are still working on it I don't have an easy answer for you but we have an answer when I've decided ahead of time the set of things that I'm trading off okay hmm so the next point is that you want to minimize number of queries so remember we're asking real people be looking at tired if you ask them too many queries and so though the goal is to focus on so query complexity so you want the fewest number of queries in order to get the answer that you're interested in okay so I'm gonna rest you a couple of things here but it's maybe easiest to describe for binary classification saying binary classification again as we talked about weights are effectively linear functions and so eliciting metrics turns or is equivalent to sort of finding the right tangent plane that recovers what the Oracle is interested in okay so what I can do in order to solve this task is and what we actually do is unroll this function so it in the binary setting I can solve this as a the following problem I have a one-dimensional function I don't know what the function is but I'm able to ask pairwise comparisons so is the function is if this is B and a I can ask is fee of B greater than V of a okay a quick question and half a minutes if I have this kind of problem and I'm interested in fact finding a maximum what strategy could I use to find the maximum cs101 you can use bandits gives you things so it turns out for this setting it's enough to do binary search so binary search is good enough and actually probably sort of optimal in this setting in order to find a maximum just doing if all I'm restricted to do these comparison queries okay so I'm rushing a bit to make sure I get done in time I apologize sorry over budget at at the beginning here it's my metric is a bit off but um so you can show some nice properties of this in the linear setting you can show that you can recover the article metric with a sort of low error in particular you can guarantee epsilon error after sort of log 1 over epsilon queries and you can actually show that you can't do any better without making much stronger assumptions that are saying that I choose the low gun and we've expanded this to more complex settings it's I think four here it's sufficient to talk about the binary classification setting with linear metrics and we can also show that the general procedure is quite stable to noise so Oracle's not giving us the exact answer or no users of other various sources of noise at this problem okay so to summarize hopefully I've convinced you or at least in trade you that selecting good metrics is important in coming up with good machine learning models roughly talked about two ideas so one sort of a plugin procedure given some complex metric to come up with a good classifier that sort of is good for that metric statistically and seems to work quite well in practice and the second part I talked a bit about how do I come up with good metrics for a certain problem by interacting with users with experts and sort of other kinds of interaction procedures so we're doing a bunch more work on this we're thinking I think just I guess at the end I'll briefly I'll just mention that one area where we think this might have a fairly big impact is thing about ml fairness we're quantifying exactly what your incident in terms of the trade-offs is it can be quite challenging and we think particularly the other station strategy might be a good approach there and some other things were actually pursuing and with that I'm about on time which is great I'm done thank you very much [Applause] and we do have time for one or two quick questions what kind of difference this might make to changing the rewedding if possible during training I'm just worried about a model which may essentially learn not to distinguish between what should genuinely be different classes and that information will be thrown away so that your score will reflect that maybe some of the cases on the boundary will be reclassified correctly but you could be missing out on a whole bunch of cases I think that's a that's a important point I would say everything we talked about here is plug-in style meaning we're assuming that you first estimate a sort of prediction problem succour score function model that captures sort of roughly captures everything that you think that is going on into data and then we apply this rating instead of a second stuff so the particular issue that you brought up will not happen using the particular approach that we're using now if you build in this relating into the learner then it's completely possible that we will for instance ignore some classes if the score function says it's not important I think again if the metric at the end doesn't care about a pair of classes then I think it's arguable whether your model should be spending a lot of effort most rural metrics would not have this but if your particular problem says cats and dogs are equivalent and I don't care about making errors between them and the metric reflects the fact that there's sort of no error from no no you don't get penalized for making mistakes being cats and dogs then it's completely reasonable for the model to not be able to disambiguate cats and dogs because and the setting day decided and the evaluation that you decided that's not a meaningful difference for you if does that answer your question the other direction so suppose I'm training my model my my negative class is really really rare example yeah and I'm training them on with accuracy it's a metric that model itself so so models are rarely trained in fact you cannot train a model of my zacchara see what you can do is some surrogate logistic laws squared loss whatever instead of all the hard metrics hardness terms of discrete metrics are MP for most function classes anyway so we always use surrogates I can cut off line but roughly most score functions do you actually use and I mean they do a better or worse job about rare classes I think that's a fair point but they're almost all smooth surrogates I'm sitting not directly optimizing for accuracy the accuracy part is often at the end where you then make a decision based on the score for your threshold so that problem does happen but a slightly different version from from Holly what [Applause] 