 hello everyone and thank you for joining me today my name is franco patano and i'm a senior solutions architect with databricks for enterprises and today i'm going to be showing you some common strategies to improve performance on your delta lake house so what we're going to be covering today i'm going to be showing you some strategies with table properties like how to structure your tables at each layer of the process and how to use the ordering to your advantage i'm going to be showing you uh configurations with spark and delta that will help improve the performance of your execution time for etl and for queries against delta lake tables and then i'm going to be showing you some query optimizations for hints and join strategies to kind of help move the query execution along all right so table structure here's the thing with delta tables uh stats are collected on the first 32 fields this is configurable there is a config called data skipping num index columns that essentially uh spark and delta will attempt to collect statistics on the first of 32 columns or whatever you have that config set to and essentially uh what we want to do is take advantage of this operation and how do we do that essentially we're going to restructure our data accordingly uh stats collection is really good on numerical fields so we want to move numbers or key fields so anything that's used in a join or in a predicate or high cardinality columns numericals that are used in query predicates we want to move those fields over to the left into the first 32 positions or if you've adjusted that that config within the the the columns of that config and what we also want to do is we want to move long strings that really don't differ that much to the after the 30 seconds position or that that setting wherever you have that column to and the reason here is that long strings are kryptonites the stats collection and we really don't want to waste time if it's not going to help us with query execution so knowing these things if we restructure our tables accordingly it'll actually help boost performance on our delta lake tables table properties all right so here's the thing there's this awesome new feature called auto optimize optimize right and essentially what it does when you break it all down there's an adaptive shuffle that actually happens before the write operation that organizes the data accordingly so that it's doing a more efficient write to the partitions in the delta tables and this works for all of the different types of write operations so if you're writing back to persisted store you want to turn this feature on you can turn it on with tables setting the table parameter that you see there or you can set it as a default on the cluster you know the way that this feature works and how i see it in the wild i highly advise just turning this on on the cluster level unless you find performance to be you have an edge case or something where it's really not taking advantage of it and where i really see the benefit of this is in all of the right operations but also if you're not running if so if you have a stream that's coming in constantly adding data to your delta tables um and you're and you're maybe you're running optimize every every day or it's on a schedule with optimize rights when it does that uh adaptive shuffle before the right actually organizes the data so on streaming queries we actually get better performance on selects uh in between optimize runs so in general uh i in the wild i'm seeing really good performance with this setting so i'm advising uh to give it a shot unless you have a really weird edge case you're gonna see uh increased performance on your inserts merge and updates all right so if you are one of those uh individuals or organizations uh the few the proud that have a a really really high velocity use case where you have data that's coming in at the thousands of requests per second against s3 we have some tricks to make this a little bit more performant what we do is we randomize prefixes on s3 so we can avoid the hotspots in the metadata and what we do is we we dedicate an s3 bucket to a delta table so you have to put it at the root and then you turn on this table property randomize file prefixes and this what this does is it helps even out uh the the requests so that it's not putting hot spots on s3 and you're not getting errors uh from s3 during production runs so i highly advise if you you do have use cases where you're in the thousands of requests per second reader writes you should definitely look into this to speed up your your high velocity tables all right optimizing z order i often get questions about this in the field you know what when should i be doing this how should i be doing this so what you want to be able to do is is essentially what optimizes the order is is optimized will bin pack our file files for better read performance and z order will organize our data for better data skipping so data skipping is already included with the stats collection that we talked about a little bit earlier but what z order does is it actually organizes that data so that you range selections are more effective so data skipping becomes more effective uh when we organize that data so what how should you be doing this how should you be organizing that data so commonly uh so if you're in like the data warehousing realm or if you have those types of etl jobs what you want to do is the order by the keys so the primary foreign keys in your demon fax tables so come and join operations and remember on numericals so keys are very common to be numbers between tables so we want to we want to z order by these keys so on very large fact tables uh we want to kind of take a look at at those uh at those as prime candidates and then if you have like id fields so if you don't have traditional dim and fact tables but if you have keys that are being joined on by different tables right we want to and the maybe you want to increase in performance on those joins you want to use that you want to z order by that key that's going to make uh the operation the execution morph efficient and then very commonly if you have query predicates that have high cardinality you want to be you want to z order by those because you're going to get excellent performance improvements uh but by leveraging the order all right so we talked about high cardinality what what exactly does that mean uh how should i be thinking about this where does partitioning come into to play like how how should i balance these things um so essentially this is how i i see the world and i provide a little bit examples here to be able to visualize and understand what's going on here so essentially with partitioning what you most commonly people use date that is a very uh very good candidate because if you think about how data comes in usually there's a cadence to date right you'll get n number of equal records per per period of time and so data is a very common thing to do but let's say you have an instance where date isn't an option or something like that essentially partitioning is good when we create about a gigabyte partitions a piece per partition so we want regular cardinality we want something in the middle for partitioning effectiveness um and you could see some examples there of common repeatable data but essentially what you're aiming for is to create nice even chunks of data of about a gigabyte in size and that's where you're going to get the most effectiveness for partitioning z order on the other hand is is a kind of a different case what you want to do to optimize for z order effectiveness is high cardinality columns uh and and in particular uh you know items that uh will increase or decrease they have ranges uh you can order them uh that's where you're gonna get the most effect because it's you're essentially with data skipping which is the key to performance on these types of queries uh takes advantage of min max's on on numericals so z order will help with the high cardinality columns and then partitioning we want regular cardinality and in general this is how how things work and how you can think about those two constructs all right so now we're going into some some parameters uh there are some new things that came out in spark 3. one of my favorite is adaptive query execution and i'm going to go through a couple a few of the optimizations and how they work in order to explain a little bit about what's going behind on behind the scenes and to kind of let you know how why you should be turning these things on if you have those types of use cases so adaptive query execution you should turn it on first off right so you want to enable that uh this is default on databricks runtime 7.3 plus awesome uh but just in case you're using a different runtime uh you know if it's previous to 7.3 it has to be on 7.0 plus because this is spark 3. uh what we want to do next is there's a new uh feature in adaptive query execution called coreless partitions if you've ever had to mess with the sql partitions in order to tune performance on your job you know that you kind of have to query the cluster to figure out how many cores you have and then you want a multiple of that there's a lot of math that goes on but this can be made significantly simpler essentially what happens here is that uh you know the optimizer will attempt to coalesce the sql partitions down in order for it to make sense for the execution of that query so you know i i just set this parameter to turn it on because then i just let adaptive query execution managed my sql partitions which is excellent um so if you know you might have skewey data where there's more data on one side of a join than the other when you want to do is play around with the adaptive sku join configuration i have found that turning this on uh really helps with queries that would have skew in the data and it's less for me to manage i can just turn it on and if it's there aqe will just kind of manage it if not it kind of really doesn't hurt very much for the testing that i found so i suggest turn it on and uh and take advantage of that it's also another feature that i really like especially if you if you use different types of of cluster instance types and um i'm going to be explaining a little bit of this in the demo section but essentially the local shuffle reader instead of communicating over the network on shuffle if the cores are within the same box it'll attempt to use the local shuffle reader instead of going over the network but actually improves performance uh when you do have shuffle in your jobs so i recommend turning that on and then the join threshold so very commonly uh the broadcast hash join is one of the it turns out to be very very performant uh in in a lot of our queries so the key to that is we have to broadcast the smaller side right there's a big table think of like dim fact tables right fact table large dim table small a lot of times that dimension table could be larger than the default default here is 10 megabytes but the cluster actually has enough memory to broadcast more so if we increase this value we can actually increase the likelihood that the uh that the plan will broadcast that smaller table so i recommend bumping this up to 100 megabytes uh you know you could play around with different configurations but what i found in most use cases in general uh 100 megabytes is a good place to start and then finally uh not to prefer sort merge join start merge join was great it kind of for big data it provides a an excellent execution mechanism uh to kind of manage that at execution time unfortunately most often than not with a lot of the most common queries what we find is that we don't actually need sort merge join we actually need like a broadcast hash join or something like that and we don't want it to use that so here we're going to say hey you know what don't prefer sort merge join and see how that works and i'll kind of show you a little bit about how these parameters can help out a little bit later all right now the delta configs first things first turn the cache on especially if you're doing etl or if you're doing queries um this this is enabled by default on cluster types and data bricks that say delta cache enabled um but some cluster instant types what i'm going to talk about a little bit when we get to the demo don't have this turned on by default but they actually do have locally attached fast disks which delta cache really uh the proponent of it is the locally uh attached fast disks so if you're not using if you are using a cluster instance that does have that but it's not in the delta cache accelerated turn this on you will get much better performance and then you know the key here is the faster the disk the better the performance you can enable it on uh clusters that don't have fast disks so if for some reason as a hard drive or something it still is a little bit more performant but you know on nvmes you're gonna get much better performance all right delta cash staleness this one's interesting a lot of sometimes because the delta cache is loaded right um when you query the the when you do a subsequent query or your query is taking advantage of the delta cache oftentimes at the beginning of the query it will do a check to see if the cache is still valid uh meaning that there hasn't been any new data coming in this can actually hurt execution time and oftentimes if you think about etl orchestration you will have um you won't be getting your data refreshed very frequently so if you have like a batch job where maybe your data is getting refreshed every day or every six hours or maybe every hour you don't need to check the cash that frequency that frequently you don't need to call back to the files so you can actually increase performance on bi use cases or analytics use cases by increasing what's called the staleness limit i i generally set this to an hour uh if you think about the workflow of a common analyst they are going to be querying successive queries oftentimes trying to find data or trying to understand a problem and so a lot of within an hour is like when that work stream kind of holds so that's kind of how i i do it by default but if your etl orchestration only updates the delta table once a day you can turn this on to once a day and you would get pretty pretty good performance throughout the day for your analysts finally use del use databricks runtime 7.3 lts plus uh there was an excellent new feature that was released for enhanced checkpoints um you can turn it on on 7.2 with the delta config checkpoint write stats as json essentially it eliminates a deserialization step in the checkpoint process which inc d speeding up the latency on on short queries so definitely turn this on in bi use cases and make sure when you're writing delta tables you're using 7.3 plus or you're going to have to mess with different levels of support for this i just recommend using 7.3 plus okay so now what happens if adaptive query execution isn't getting the hint right uh there might be i want to use i wanted it to prefer broadcast hash join but for some odd reason in execution time it's doing it's doing sort merge drawing um i'm gonna give it a hint this time so we're gonna strongly suggest hey take a look at this and you could do that by these uh you know putting these query hints inside your sql statements um broadcast hash join or nested loop join uh one side has to be to be small right so this doesn't work if both tables are really large uh shuffle hash join um this is when you have to to shuffle data but you don't really want to sort it uh it can do large tables but you have some risk about booming uh about running into out of memory errors if your data is skewed so kind of be careful there uh and then sort merge join right it's the most robust one uh but it kind of you have the the shuffle and the sort and it can be uh slower when you know you have small data or it's not both sides aren't large data and then finally cartesian uh i don't recommend this unless you have a use case that requires it um you know i can think of a few but but generally you want to stay away from that uh you know when you're joining data all right so we've talked about a lot right well let's think about the delta architecture pattern that we have in front of us and kind of align this these table structures and these tips that we've kind of been been talking about now how do i structure my layers of my delta architecture and what i can suggest is is these tips for your bronze tables and this is raw data this is kind of data that's coming in from the source system you want to land it raw don't change anything and turn off stats collection because at this point in time it's kind of just extra overhead on on data processing and we're not really querying bronze very often we're actually just using it as a staging area so let's kind of diminish what we have to do there um but when we get to silver or the next layer filtered or clean layer this is when we want to start moving data around like i made the suggestions earlier restructure your columns right move the the keys and the numericals to the left of the the data in data skipping index columns or 32 columns and then move your long strings to the right of that property and that's going to help out with stats collection and then when you want to do optimize and z order on those fields they're going to be within those columns so that z order is effective you want to make sure turn uh turn on optimized rights right at the cluster level that way all of our insert update and merge operations are are efficient um and then we want to optimize and z order by the keys between the bronze layer and the silver layer and on our silver tables and this is going to help with our etl processing right our merge operation is going to get boosted if those keys are organized and then when we get to the gold layer our business level aggregates or kind of our consumable layer again here we want to optimize and z order by common join keys or common high cardinality query predicates and what that's going to give is our analysts really good performance on these delta tables again turn on optimize writes because we want the merge from silver to gold to be efficient as possible we want to enable delta cache and we want as fast as disk as we have available to us and then on these on these clusters let's turn that staleness limit up to align with our our etl orchestration so if it's every hour you could do one uh one hour it's every day you could do one day this is going to help manage that delta cache and make that experience more uh performant for our analysts at runtime all right finally some pro tips always use the latest databricks runtime we're constantly improving performance and adding features you want to make sure you're taking advantage of all that engineering brilliance and use the latest runtime the key to fast update merge operations is to rewrite the least number of files optimize rights help here you can also uh configure the max file size for optimize which kind of uh helps with rewriting the least amount of data so we recommend 32 you can go between 16 and 128 depending upon how your operations are working the key to fast select queries is delta cache optimize and z order and adaptive query execution so you want to make sure you take advantage of those configuration items and settings finally the hilbert curve for optimize this is something that's new so it's it's kind of a little bleeding edge i don't you might not want to put this in production but you might want to test it out in your pre-production systems to see what kind of performance you're getting uh it's the the the multi-dimensional curves that we use uh right now is the z order curve and if you look at it in in dimensional space it kind of looks like a z that's why it's called the order hillberg curve is actually like a u uh it's a different curve different math essentially uh what we're finding is that z order curve is really effective for like one two or three columns that you're z ordering by and then the effectiveness kind of falls off so it's only this bell curve to how many columns you're you're optimizing z ordering hilbert's curve we actually see a more even uh uh distribution with performance for doing up to five or six columns that you're that you're z ordering by so you know take this as you will it's something that we're continuing to test and work on uh more to come as as we kind of figure out how to best guide everyone okay so now let's take a look at uh some code and actually see this stuff if it really works do these tips actually work let's check it out all right thank you everyone so now we're going to take a look at actually in practice how are these configurations really going to help me with my etl performance and how can i nbi performance and what can i be doing can i see this in in real time absolutely so let's take a look at a common use case that i i see in the field i'm going to be using uh tpc-h data it's it's kind of just uh benchmarking test data it's really just you know synthetic data um it's not really meant to be you know real uh but what i'm gonna be doing here is i'm gonna be loading that data into a database in delta lake tables and i'm gonna be performing etl operations so insert operations and then we'll also be performing some queries so here i'm going to be creating that database uh in the in the meta store and then i'm going to be creating my tables so here are all the tables uh metadata and i'm going to be using a delta on those tables and then i'm what i'm doing here is just looking at all of my files um i'm going to be collecting a list of all the tables i have a simple function here to help me insert the data and what i'm doing is just kind of reading the file in and then creating a temp view on it and then inserting that data into the table and i also have a function for optimizing all of my tables i'll show you a little bit about that um in the in in the following steps so what we're doing here is we're just importing the data uh right we're running through all the tables and we're creating a data frame uh from csv files and then we're inserting them into the delta table that we created in the previous step and you can see here that this etl operation the loading took about 19 minutes right so that's just taking the files from where they are in object store and loading them into delta tables so now we want to run some queries on it right and uh before optimize this query took about 2.7 seconds which hey is not that bad uh but what i want to do now is i want to actually take a look at what optimizing z order really provide me in execution time right so let's take a look and what i'm going to do here is i'm going to use that optimize function that i created and i'm just going to create a table of um i call this like a configuration table but essentially if i need to add new optimization keys i can just add it to this table instead of having to rewrite code which is very key for when you think about you know how to manage your operations from day to day so i'm just going to create a simple table um and it each each uh column here is the table name and then these are the keys of that table the join keys i'm just going to run through those and i'm going to optimize them you can see here it took about 17 minutes to perform on this cluster and then after my optimizing z order my query went down to about 1.4 seconds so if you think about before the optimizing z order i was about double the time than it was to perform it after the optimizing z order so that's the type of effect you can have on just leveraging optimizing z order now you might be thinking to yourself is that the really the bus that it can get absolutely not we need to optimize all those configurations i haven't done any of those here this is just a simple default configuration for for data breaks right so now what happens if i apply some of those those tweaks to my configurations so here i'm creating a new database same step as before so i'll kind of just skip over a lot of the intro material but essentially this one is going to be optimized so what are we doing to optimize it i'm using an instance type called c5d9xl so these are compute optimized but they actually include locally attached fast disks and instead of doing auto scale i'm going to do static i'm going to set 8 workers to this cluster and then i'm going to be activating a few of the optimizations that i talked about today as well as some of the optimizations around optimize itself so let's take a look i created all my tables uh i i created my functions and i processed all of my data this time it took 1.03 minutes let's just go back how long did that take on the previous cluster 18 minutes it's like significant performance improvement but what does this mean for performance is one thing i i might have had better nodes uh you know is this really accounting for cost performance let's take a look at something the instance types i used for the previous step was i3 xl and that's kind of the information about it what you want to know is it's about 3.312 dollars per hour right if i were the same job that took nine twenty we're just running up to twenty minutes and two minutes at this point even though it really wasn't two um if i look at cost performance right running it on the better cluster type actually is cheaper if you consider execution time because remember folks in the cloud it's all you pay it's everything is metered so you pay infrastructure over time so if i can finish faster it could be cheaper and in this instance it was about half as cheap so you want to make sure that you solve for the right things here you know not only did we solve for time but we also solved for cost so that so let's take a look at what the execution time was for for doing this is this this query uh we just inserted the data right and we turned auto optimize on and here we see before we even run optimize yet we're getting performance of about 1.4 seconds which is actually pretty decent um so let's actually optimize and z order those those by those keys and see if we can improve performance a little bit and what we get is uh performance going down to about uh you know these aren't these are nanoseconds so it's less than a second but 332 nanoseconds so we're talking a significant performance improvement so this is great so we covered kind of etl performance cost perform cost and time performance now let's take a look at some sql analytics use cases like uh actually querying the data itself and some of those optimizations so here what i'm doing is i'm using those optimized tables and i'm saying uh select the data so this is before doing any of those configuration items that we talked about and first run on this it took about six seconds so 6.6279 on on this query so this is just a cold boot it's not using delta cash i kind of remove removed the cash beforehand and that's how long it takes not not bad can we get any better so this is an optimized cluster so using those optimized delta lake files that i generated in the etl optimize step now i want to analyze that data right so here i'm using i'm not using the c5ds i'm using the i3xl it's a very important thing to keep in mind and then i'm doing all those other configurations so before the delta cache is even loaded i have about a second this query took about one second to execute which is pretty good if you're thinking about a cold boot right so that's what those those configurations really gain uh it's really good instantaneous query execution and then now what happens if i leverage some of the delta cache wow so i'm getting down to the through you know the 300s again like i did before so the key here is that the delta cache is really really important and even if you're not using a a bigger instance type even if you have locally attached fast disks which the delta cache accelerated types have that you're still going to get good performance so that's why i always recommend to use the right instance type for each type of operation when we're talking etl and optimize compute optimized instances with locally attached disk are really good thank you everyone for joining me today and i hope you have a great summer you 