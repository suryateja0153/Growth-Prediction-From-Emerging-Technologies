 [Music] however i will be presenting today is about enabling distributed themes for the mobile web over cloud agent and device so and's contents include the overview of executing things on the web two ways for accelerating things for the web with edge computing and some thinking discussion for slide 3 so we as we all know deep neural networks as a representative way of achieving artificial intelligence in numerous applications also show great promise in providing more intelligence to the web applications so i'll show you in the figure there are two typical things executive schemes on the web the first approach is exacting the whole things on the web via javascript and web assembly such as tensorflow.js pedals just and keras yes web dealing etc so however this approach requires a high transmission delay for loading heavily models for example tensorflow.js russ net 50 deep learning model whose size up can be up to 97.8 megabytes so besides an image the computing results of the wave also performs a slow influence even we can accelerate computation by web summary or web gpu and more commonly the cloud only approach overloads the whole task by executing the holdings on remote cloud thus large amounts of data such as images audios and videos are sent to the remote cloud and it increases the computing pressure especially for high concurrent requests in addition transmitting the complete task to the remote cloud also addresses new privacy concerns for users such as home security cameras uh slide four as mobile ad computing is becoming an important uh competing infrastructure in 5g euro it's promising to consider the use of the ad cloud that has the benefit of no communication costs compared to uh the uh to uploading uh computations to their remote cloud and release the burdens of the car network to sell so to accelerate or distribute the ins for the web it's natural to consider the use of competition of loading approach to leverage the computing resources of the end devices in the edge server so notice that we can deploy the web applications on the edge server and remote cloud is responsible for 20b models with gpus this approach partitions the dean's computation by layers and dynamically distributes the computations between the web and the ad server since the mobile level loads just a small portion of the models and access a part of dm computations so we can protect that privacy by transmitting there in transmitting the intermediate results to the ad server for examining the rest of the inference instead of uh transmitting their whole tasks so however the challenge is how to provide dynamic information to cope with various tasks unstable network conditions when uh and various devices with different computer companies so how can the web or mobile web perceive mirrors the computing capability of the device and monitors the normal condition when employing partition of loading approach into the web uh as for slider 5 with partition of loading approach in edge companies infrastructure also a small portion of dn computations can be offloaded to the web and the server spear has to undertake the majority of the incompetence so we introduced adding an efficient branch to the traditional dns for example influence on the web independently we add a binary neural network range as the first convolutional layer of the traditional neural network and it has the same strategy to the rest of the traditional neural network that it's easy to design a night with dn for the web especially for the mobile web without reach expert experience announcement so which which also can provide accurate accurate compensation for the lightweight bridge if we are collaborating with canyon for giving some point if the binary branch is confident to predict the result and satisfy the users the sample can exist from binary graded directory otherwise it has to transfer the output of the first convolutional error to the ad server for precise results and measuring the confidence of inference results can use the normalized entropy which is always employed in collaborative beings and then we compare the normalized entropy against a threshold to determine whether or not exited from the tiny bridge we pick the appropriate body by zoom exit from a threshold as a hyper parameter during the training phase so uh in summary i think adding lightweight bridge can reduce the model size and accelerate influence on the web which provides a collaborative mechanism with the ad server for accuracy composition so okay slide six furthermore we we also consider that in actual scenarios the user requirements uh for delay network conditions and the computing capabilities of device may change dynamically so as a constant lightweight branch or traditional the income compression network cannot meet the requirements so that this requires providing a context of where pruning algorithms that incorporates the latency of the network condition and the computing capability of the device and in this figure we show a deep adapter framework across mobile web the edge server in the cloud cloud server which contains offline and online phase and the online offline phase consists of network pooling a model cache updating and div adapter employs a context of where run-time adapter providing a product model that will be optimal for the mobile user by monitoring the network condition and incorporating the cpu frequency of the mobile device so when this promote model is on is is unsatisfactory self-satisfactory for the mobile user so deep adapt then receives output from the first convolutional layer for the mobile web from uh from the mobile web and receives output from the first convolution uh and and executes the rest of the influence of the improved model on the edge server so for each new request result uh merged through the model where first level register is the ad server to process the mobile user's request and then we send the pruning requirement to a address the queue of network company modula on the cloud server and the network pruning module module obtains the requirements from the requested queue as improved network and addresses approve the model to the multicast of the ad server for the nexus for the next similar request okay and a slide seven although we have discussed some ideas for accelerating the exemption of teens on the web to implement air service the uh that's the office problems in actual deployment the first thing is what rules should the answer are playing playing in providing processing support for and talented web applications requiring heavy computation so i mean is there a better computing collaboration or deployment model for accelerating the second is how to develop developers who use the ad server more easily uh for accelerating and collaborating with web apps and the third thing is how can the ad server deploy and overload the computations more easily uh including how web apps can monitor changes in network conditions and responses to the answer and how the ad server can perceive the computing capability of the device and distribute appropriate competition to uh web apps for execution in real time okay this is what i want to share and discuss about how uh enabling distributed begins uh for the mobile web over cloud age and device uh in thanks for your lesson [Music] 