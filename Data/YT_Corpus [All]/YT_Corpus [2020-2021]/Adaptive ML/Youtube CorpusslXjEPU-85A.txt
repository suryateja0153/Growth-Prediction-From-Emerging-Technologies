 Welcome to another edition of TechBytes. I am Youko Watari, a Product Marketing Manager for Teradata Vantage and related products. In this recording, I and my colleague Sri Raghavan will review new analytic functions available in Teradata Vantage's latest release. With our new release of Vantage, we tackled 2 areas: First is to provide greater efficiency in data preparation aspect of data science process. Studies show that great majority, some say up to 80%, of data science process are spent in preparing data. This may include collecting and analyzing raw data for quality, deciding which data to include, combining or shredding data, etc. With the latest version of Vantage, we are adding 5 new analytic functions that will help simplifying data preparation process that will accelerate the data preparation time. In addition, we are adding 2 new analytic functions: StringSimilarity, which is a data exploration function and 6 variations of MovingAverage function. Next, my colleague Sri Raghavan will provide high-level overview of each of the functions. Hello. My name is Sri Raghavan. In general, there are three new categories of functions: data exploration, data preparation / transmission, and statistical analysis. In data exploration, we’re going to be talking a little bit about StringSimilarity. In data preparation and transmission, we’re going to be talking to you about Antiselect Pack, Unpack, and nGram splitter. And, finally, within statistical analysis we’re going to be looking at a bunch of MovingAverage functions which are being included as part of Feature Update 2.  First one is string similarity. Well, think about the number of times where you’ve had strings that have been part of the data analytics. So, strings can be, basically, a sequence of characters. And these characters can be alphabetical, they can be numeric, they can be alpha-numeric. And realistically the reason why you want to compare strings is because you want to see if there is differences between the data. So, let’s take the example of a healthcare company, which has all these various diagnostic notes. Now, a doctor may have prescribed two similar kinds of treatments for two different patients. Now, instead of you having to manually ferret through all of that to determine if that medication has been prescribed elsewhere - instead of doing it manually, you can actually do a string similarity, which actually searches through all the records and comes up with the records that are pretty similar to the one that you started searching for. That’s what string similarity is. It measures the distance between two strings to figure out what the level of match is. So, as you can see, for instance, in the output example, the word on the side is A-C-Q-I-E-S-E and the target word that you’re comparing this against is “acquiesce,” the right spelling. The similarity between the two is 0.92. Well, if you took this example and if you blew it up to real life use-cases in healthcare and things like retail, where you’re looking at customer sentiments that have been expressed by different customers, and if you want to actually figure out what the similar sentiments that have been expressed are, you can use string similarity to extract all these various strings that you pick up from, for example, social media to determine what the similarity is based on which can make the assessment. So, that’s what string similarity is all about. Next up is antiselect. This one is really important when you are faced with working with very large datasets. Now think about it. Let’s say you have a database or rather a dataset which has 2,000 columns. Let's assume that you want to actually select 1,900 columns out of the 2,000 for your analytics purposes. Now, typically, when you write a select statement, it’ll be “Select A, B, C, D, E, F, G”-- all the different variables you have selected. You cannot simply write a statement which brings in 1,900 different column names into your statement. What you can do instead is you do something called Antiselect, which is -- Antiselect allows you to specify which variables you don’t need as part of the table, the dataset, that you want to analyze. It is basically saying, “Give me the entire dataset, but not these variables.” That’s what Antiselect does. So in this case, on the right you see the original dataset - ID source, age, gender, race, NumBuys, NumSells - when you Antiselect on source age and race, those fields get dropped and this is the result. That’s what Antiselect is about. Pack, Unpack: another big data prep function. Now the reasons sometimes you pack data into your tables is because you want to actually free up disk space. So, Pack, effectively, is exactly that that word says: It compresses all the information and it stores it into the database. Now, when it compresses all that information and stores it into a database and merges just like you see here - it merges all of these things into this - what looks like a stream-of-consciousness line. Right? Source, side; age, 62; gender, male; race, white, NumBuys, and so on. Now, this looks like unstructured text in some ways, and it is. If you apply the Pack function, it simply takes all of the existing unpacked information, which is in columns, and simply packs it and puts it in this kind of line; which is good if you want to actually store all the information and free up some space on your infrastructure. Now, on the flipside, if it is Unpack. Now let’s say you have packed data sitting in front of you. Obviously, you can’t really do anything with the packed data unless you deconstruct it into specific columns. And that’s exactly what Unpack does: It takes your existing packed data and it structures it into the constituent columns from which the Pack has originally generated. So, on the one hand, Pack is used for storing information in a highly efficient manner and Unpack is used for quickly deconstructing all of that information so that you can do downstream analytics. So, that’s what Pack and Unpack do. nGramSplitter. This, to me, is perhaps one of the more important functions that has a predominance as far as some of the analytics that’s done particularly in the realm of things like natural language processing. But it’s a prep function. Here’s what it does: nGramSplitter, essentially, takes a corpus of text, which is basically a paragraph or an entire page or sometimes large numbers of documents - whatever the size. It doesn’t really matter. And it splits that text based on certain options you can provide in the code. So, for instance, let’s say the text has got a lot of periods or punctuations or specific characters in it. You can tell the nGramSplitter to split out the text based on these various options, such as punctuation, spaces, and characters. So, what happens is your big text actually now becomes much smaller chunks. And much smaller chunks can be read by downstream analytics, like Sentiment Analytics, for you to identify prevailing sentiments. Or if you want to do things like topic identification, it becomes very easy. So, effectively, what it does is it takes this large piece of text - as you can see here, my favorite band is the Beatles. So, “No band has come close to capturing and collecting individual imaginations as much as the Beatles have.” So, you have this entire bit of text here, which really is one big paragraph. When I apply nGramSplitter to these, it then splits out. So, the first one I’ve said is going to be based on, let’s say, certain keywords or certain punctuation marks that, let’s say, I put into it. So, here it splits out the words “No band has come close,” and then it provides you a frequency of one. Similarly, based on the same criteria that I specify, it picks a different part of the text and puts it elsewhere. It just, effectively, splits out the text into smaller chunks so that these smaller chunks can be fed into downstream analytics for further analytics and inquiry. That’s what nGramSplitter does.  Now come the moving average functions that I use in statistical analysis. There are quite a few functions in this and they all have one purpose, which is to compute a certain number from the time series data or any other sequential data that’s provided. The most often seen variety of moving averages is simple moving average, which is effectively putting a time window on your data. So, let’s say, for instance, you have 12 months’ worth of salary and you want to compute the average salary. It’s pretty straightforward, right? You add up the salary for each month and you total that and divide it by the total time period from which you’re trying to get the average. In this case, it’s 12 months. Let’s say you’re trying to compute that from January to December. Very simple: You take all the salary information for each month from January to December, add them, divide it by 12. But a simple moving average moves by definition, which is next time a new salary comes - let’s say for the next January. Meaning, you start it off at January 2018 and you computed the simple moving average for the year 2018 and now you have a new salary indicator for the month of January 2019. What simple moving average does - it moves the window. It requires you to give it only 12 time periods. Whatever would be the number of time periods. So, it discards January 2018, adds January 2019 to the total, and divides it by 12. So, the divisor, which is the total number that you divide it by will always be constant. That’s why it’s a simple moving average. It moves. The window moves. And you compute. So, this is a visual which shows the trend over time. In general, all these moving average functions are used to look at trends to compare trends with other variables over time, to look at outliers, to look at data analysis and so on. So, these use cases don’t particularly differ from between the different types of moving average functions we have. So, this is simple moving average.  The next one is cumulative moving average, which is slightly different from the simple moving average in that nothing gets dropped as far as the data is concerned. So, let’s say in the previous example I said Jan to December; you took all the salaries and then computed an average. Fine. Let’s say you add the January 2019. Now, cumulative moving average basically says, “I’m not going to drop anything, I will now compute the average on 13 months instead of 12.” That’s the only difference. You look at short-term fluctuations to look at trends. I’ve put different use cases on some of these moving averages - only because I wanted to cover a vast number of use-cases on a moving average, but all of these use-cases are applicable to pretty much all of the moving average functions that we have here. It’s just a choice depending on the kinds of computation you want to have to determine moving average numbers. So, this is cumulative moving average.  Exponential moving average is slightly different. Now, it still computes an average, but in this case there is a what we call a decaying factor. So, let’s say, for instance - let’s stick to the January 2018 to December 2018 average example. If you’re computing an exponential moving average, you will compute it on this 12-month period, but you will say that the most recent information, meaning that if it’s month #12, which is December, that will be given the greatest weight in your calculation. There is a certain multiplier that you’re actually going to put for it and that multiplier will come down as the chronology increases. So which means that the farther back you go, the smaller the multiplier. So which means most recent information is given greater weight. And the reason why this is kind of important is because you want to actually - let’s say you look at your stock broker - you want to actually look at certain price movements. Now, sometimes price movements are mandated by certain events that have occurred in the recent past as opposed to price movements from more distant past, where if you’re a person who’s observing the market, market typically reacts to things which have happened recently. The market has already settled on things which have happened very much in the past. So, really, you don’t have a lot of weight for things in the pasts. That’s how exponential moving averages function as well. It allows you to be able to look at certain indicators in your data which has a more recency aspect to it. So, that’s exponential moving average.  Modified moving average is another one there is a certain lag that you will see in the average. So, again, it’s a computation of average. But there is something called a sloping factor. Let me explain that, and the best way to explain this is by looking at the visual. So, look at the blue line. This is, let’s say, the price of a prominent security in the technology space. Now, what happens is you can see that this price shoots up sometime between 1998 and 2002, perhaps around 2000. Now, the red line is your modified moving average, MMA. But, as you notice, the MMA trails: It’s got a similar pattern in terms of the fact that it rises up and comes down, but it trails. And the reason why it trails is because of the MMA. The MMA, basically, says, “If you are increasing in value, I’m going to then mirror that increase with the sloping factor to show how it actually changes over time.” So, you will see the modified moving average moving in tandem in terms of the path, but it doesn’t exactly reflect the same magnitude of movement. And the reason why you do MMA with sloping factors is because you want to actually see what the movements of the data is over any particular period of time. It’s a little bit of a hard concept to understand. Suffice to say that it basically is the trend which allows you to smooth out the effects of data changes. So, in this case, you know the stock price is increasing, but what the sloping factor allows you to do is it doesn’t allow you to overstate the fact that it’s going up or coming down. It provides an even trend and this way, when you look at things like the modified moving average over a long period of time, you are better able to look at future predictions. You’re better able to actually make future predictions, which are a lot more accurate with how the long-term patterns trend as far as the data is concerned. So, as I said before, it’s a slightly complicated construct to both convey and understand, but the only thing I want you to take out of this is that the modified moving average essentially is a responsive average indicator; it allows you to look at existing data, put a trailing slope value to it, and smooth out the trends to make certain long-term predictions about your data. That’s what modified moving average is about. Triangular moving average. This is another statistical analytics function. The triangular moving average, just like other moving averages, is a trending function. Basically the way the calculation is done for the TMA is different: it takes the average of averages. So as you, simple moving average is a simply the addition of values across a certain time period and dividing it by the number of units in the time period. So let’s say in the for instance you calculate a simple moving average for monthly salaries for 12 months. It basically takes the salaries for every month - in this case P1 for the first month, P2, P3, P4, all the way to P12 - divides it by 12. That would be the simple moving average.  The triangular moving average takes the average of simple moving averages. So how does that work? Let’s take the same example as I have here, which is 4 months. The simple moving average for the first month, is P1 – the salary for the 1st month. The simple moving average for the second month would be P1 + P2 divided by 2. The simple moving average for time period period 3 would be P1 + P2 + P3 divided by 3. And for the 4th period would be P1 + P2 + P3 + P4. Now each one of these simple moving averages for each period - period 1, which is here; period 2, which is here; period 3, which is here; and period 4, which is here - is taken, and the average for all of the four is computed. So, as you can see, it’s taking the average of the averages. That’s the triangular moving average. So mathematically if you imagine this, right? P1 + P2 divided by 2 - the average would be somewhere in between P1 and P2. So when you put that calculation into the TMA, you can see how the curve will be a little bit more smooth. It won’t be as sharp as it would be in the case of simple moving average. I encourage you to take a number of examples and plot out curve yourself. But basically, an average of averages. Why is it important? As a use case, it’s used to identify a trend. The triangular moving average is particularly sensitive to long-term changes in data. So therefore, if you do see – even if the curve is smooth compared to simple moving average – if you see a change in the TMA, you will know that the trend is moving to a different direction. That’s why lots of financial analysts use TMA to understand market movements. If the price of shares move above the TMA lines then it’s considered to be a bullish market; vice versa if it's below the TMA, it's considered to be a bear market. That is triangular moving average. And then weighted moving average. Weighted moving average you will recognize is very similar to the exponential moving average. The concept is the same. That is, it’s a weighted average. Things which are recent get a higher weighting. Things that are more in the distant past get a lower weighting. The only difference being purely how you apply this weighting factor. The way you calculate the weighted moving average is very different from the way you calculate the exponential moving average. It’s simply a mathematical difference and it’s just another option. Again, these are used quite extensively across different verticals; in particular, finance, banking, insurance use quite a lot of it. Why you would choose one or the other? It depends on what it is you’re using for it [sic]. A lot of the time, things like weighted moving average is used to understand stock market price movements, to actually look at how investment securities trend. It’s used by a lot of central banks to understand the impact of interest rate cuts, for instance, on certain kinds of activity. A lot of the times, these kinds of moving averages are used. But, most importantly, one of the things that we need to mention is that these moving average, in and of itself alone, is not enough for you to understand the underlying insights. Right? You need to actually combine these with other pieces of analytics to understand what’s going on with your data. Thank you so much Sri. For more information about Teradata Vantage, please visit our website at teradata.com/vantage. Thank you for much for watching. 