 welcome to the spark summit i'm vinícius while working as a customer success engineer at the brakes I worked closely with data engineers data scientists and executives across various industry including finance geospatial gaming video healthcare I helped their cloud and baiting the implementation and strategizing AI and machine learning as well as analytics use cases so I'm here to share some of the lessons that I learned with our customers and best practices being delighted I want to take you through the data engineering journey as 80% of the business problems are associated with that we will cover data strategies building cost-effective and performant data pipelines securing it for production ization and I will concluded with a summary of customer reference architecture at data breaks we think that data has the potential to solve the world's toughest problems how do you do that you have data and you have business problems to solve all you need to do is run data science machine learning alright if you have traditional stack runs on bi reporting on it if you can just do that our jobs will be much more easier unfortunately the reality is complicated we no longer are talking about business intelligence or business analytics for Tina's ations are realizing the value of having all the data and now they want to do much more than that they want to be data driven to leverage machine learning and AI effectively you need a unified faster and scalable solution to support growing volume and diverse data including open for that data Lake is more like a body of water in its natural state data flows from source systems to the lake it collects everything eliminates data silos and facilitates a central location dalek serves as a single source of truth where you can source all the data decoupler compute and storage and use it to drive insights analytics data science and machine learning but this is how the usual data link looks like it's garbage in and garbage out this is what the data scientists dream of let's summarize these problems no admitting means fail production jobs living data in corrupt state there's a huge data quality and reliability issue with this architecture for each copy it's hard to keep it consistent so it's hard to do anything with it data scientists spend 80 percent of their time debate emerging and 20 percent people how do you want to solve it you need a unified service to remove all the data silos and ensure that you have reliability and the great thing is you already have a lot of data in your data leak all we need to do is make sure it's reliable SPARC is one component of that and most people have already standardized on spherical Zetas processing engine second really important component that a lot of users find value is in theta linked data link it creates a transactional layer on top of your existing theta leak and once you have acid transactions on top of it you can make sure that you have reliability and high quality data and you can all do you can do all kinds of computation on it so in fact if you have patents streaming both big Delta it's okay to have streaming in data while someone in patch is reading it that's the unified service now because a verdict on fixed whenever you start starting a data pipeline think about what is the use case or business outcomes you want to achieve usually like to start a machine learning a use case or even a project if I don't even the dicta right it's always a good practice to ask yourselves if the data that you are provisioning helps a use case and if your data is ready for analytics or machine learning so think about your audience for whom you are building the data for create tables and purpose towards use cases and audience think about file format does lend this file format lend itself to a faster application whilst yes we have some challenges like why transformation etc Part A is more optimized for that you need to think about table segmentation as well so instead of one size fits all approach my recommendation is having different tiers based on the user persona and use cases in dirtily we use the terminology of bronze silver and gold bronze meaning in that year you can try to preserve all your row rate or raw data keep it in bronze layer and use life cycle policies with minimum transformation this is mainly used for data scientists second is Silver's here combine all your logic partitions to keep it in optimized format with some transformation so this serves some sophisticated advanced users like product managers goal tables so this is where you want to keep roll-ups and aggregations for business users and specific to you streams this serves bi domain level tools now you are all set with data our strategy to consume data downstream you need to think about efficient ways to run queries and use compute today there are two major factors which drive the need of cost optimization the evolving global financial conditions in response to forbid 19 pandemic and the accelerated adoption of cloud since I've worked with customers directly I was in a well I was in a position well 3 is to assist with effective use of existing tools and best practices on how to optimize fast there is no silver bullet or drop-down algorithm where you can plug in parameters and get a perfectly architected cluster if your objective is to allocate the right amount of clustered resources for a job then we will need to understand how different types of jobs demand different types of clustered resources if our goal is to allocate just the right amount of clustered size for the application it's important to know how your data is going to be used each of the use cases require different thinking for example if you have use case which requires memory optimized instances and these are good for applications where you want to put on machine learning workload so most everything is iterative meaning you will need to work on the same data again so caching becomes a really important feature to run machine learning algorithms in more performant manner compute optimized instances are great for streaming and ETL use cases traditionally when I am doing streaming on the whole I am performing operations on a row basis as opposed to a large data set and I might be adding timestamp or removing null value but I'm not trying to join these elements together so I don't need that much memory with ETL you need to do full file scans but there is no data read and most of the workloads with CPU bound so you can use C class for absorption for storage Optimus virtuals this is this is an option which has a locally mounted disk as well if you are going to be handling images repeatedly that will be a good one here for analytics use case or ad hoc queries we recommend IT or storage optimized webster's we also have delta io catch option which caches data locally and is food for repeated read of data so we don't need to read and read from s3 ok next question you should ask how fast the job needs to be run we can throw in more coals at the job but it may not be for the consumption cost you should always remember that experimentation is cheap so getting a rough order of magnitude is important but spending time trying to perfect your architect or cluster might be the name your development time as you move along you should use the proper size of node I usually try to keep a good happy medium so when you have a very large workload for example we choose on node size of 256 gigabytes of memory that means you could end up with a large tinium hip and as a result garbage collection is going to take a very long time when you have a small node then you need to ski it out as more nodes have less memory it means more spills so I choose 64 or 120 it gets memory keyboard shake using that range so what memory size you should choose really depends on popular the ultimate decision over there is a benchmark so you should start with that secondly if you need to think about your cluster size you should think about how many workers you need to complete the job and it depends on data volume complexity of transformation and eventually your application requirement you can start with how many tasks you can create initially which is for related to the input data size once you start running some workload observe the behaviors from the spike UI for example storage tab inspired qi gives you information on caching to indicate if you have enough memory or not how much of data set is cached in memory and how much of data set is cached on this this is the information that can help you as well for example if your data set is freely cached with some room to spare you can find that from the storage tab of your application there is a chance for you to reduce a number of workers if you don't need that much memory let's see if you are almost completely cached then you might need to increase the cluster size so that you have enough memory to run the jobs third one is not even close to cash so if your data set is super big then instead of cashing in memory you may want to choose cash on disk but not on the EBS you should choose the instance type that support large number of SLG so we can catch it to the disk and when we retrieve data it is much faster next is our observing Kenya metrics and tweaking the workloads ganglia matrix UI is the integrated matrix tool that comes with data breaks so with it you can track important raster metrics such as CPU memory and network utilization over time for analytical workload you want to make sure that you have enough parallelism and partitions in your data frame the more pores you give to workouts the better if your spark application is well written then there is no bottleneck and you should be able to linearly so if you double your cluster size then your runtime should be cut in half CPU usage is aggregated over all machines in a cluster for this reason you never want to want your usage too near hundred percent this would mean that extra work is being put on your driver and you are really stressing the driver in this case for example the good good amount of maximum usage should be 80% on a cluster with five machines for being workers and one being covered let's say if you are about workloads network bound then you need to think about where are the big sites so you can use ganglia UI for big spikes green indicates that data is being red and blue indicates data being written out for this use case you can use big machines with better network throughput and i3 type with SSD backed instances so that you don't have to do repeated remote reads the memory pane on the top right describes the object stored in memory for a given SPARC application so if you spill a ton you can use more memory and I three instances now we have data storage and compute all figured out next thing which comes up is can I make my application run faster so look for this for symptoms when running your workloads shuffle spills queue small files and spark UI is where you can do this first shuffle so let's look at shuffle I have two tables here flights and carriers where my flights is a large data set and carriers and small data set and this is how I am joining my tables when I run query it took 28 minutes to complete let's look at this sequel logical plan to see what happened during query execution so I am going to locate my job and observe the query execution I am going to scroll down to the sort merge-join you can see that it outputs a very large number of rows and this might be because the way we are joining a large table to small table and if I scroll little more down I can also observe that there is a spill size this is some huge spill size so to improve that you can take advantage of join optimization so how you do that is optimizing broadcasting the smaller number to all the executives that we have shuffle doesn't have to incur it during the joining process and then you want to review your join order it's ideal to join the small in staples first and work up to the big one now once we factor this element loop the query took one point eight minutes and looking at sequel plan in Sparky y we can see that broadcast improved the shuffle and now it was completed in much more faster time now we are going to look at what is the result of sort merge join it's around thousand rows output which is a significant improvement from the previous value I Nexus spill so when the data cannot fit in memory disk still may dominate overall performance since the default configuration allocates one CPU core for SPARC tasks this can lead to situations where spam jobs are not using all the CPU resources allocated to them and as a result the overall cluster CPU utilization can remain low even under big scheduling conditions similarly for memory depending on the input data and shuffle sizes the job might not fully utilize its out memory this is not obvious to fine but you can leverage sequel plan to in the most informative manner here it shows a large amount of spill when my shovel partitions were set to 16 so approach I took is increase the partitions to 48 and reduce the number of course the executor can use you can see on the right my spell size decreased from these kind of tweaking and it was almost to zero so the more spill you can remove larger the impact now you you might think for your application if you find a skew now what so data skew is a condition in which our tables data is unevenly distributed among partitions in the cluster data skew can severely down the performance of queries especially those the choice so if you're joining big tables that requires shuffling of data and you can lead to an extreme imbalance of what imbalanced cluster so what you can do is observe your CPU usage using ganglia UI so in my graph you can see that usage becomes low after an initial spike another metric that can be helpful is looking at the stages in this file Qi which I am showing to the right side so look at the duration of task there is a significant difference in the value of max from the 25 percentile and 75 percentile values another indicator is input size and records are also significantly different to help with this you can use broadcast joins spark sekolah accepts cue joins in queries so with the information from these hints spark can construct a better query plan one that does not suffer from data skew you can always play around with other settings which are repartition according sauce salt cheese so over the years there has been an extensive and continuous effort to improve spark sequence query optimizer and one of the various improvement is the post optimization framework adaptive query execution looks to tackle such issues by optimizing and adjusting query plans based on runtime statistics it reduces manual effort of tuning spark sagas spark shuffle partitions and it dynamically changes sort merge join in to this broadcast has joined and also it is able to handle few joys this this spark query execution is available in our newest version which is seven dot X and spunky dot o there is also a different talk in this box amid that you might want to watch if you are interested in learning so to resolve small files problem let's look at scenario and the solution we took so I'm going to talk about an example of our media customer I was working closely with they were getting close to 500,000 wheels and bitter requests per second on their cluster so getting a lot of URLs every second which is dumped into s3 for certain time interval creates a large number of small files problem so we took two step approach to solve it we managed to process these large number of small files instead of a cluster of bidders writing directly to and three they send URLs to kinases stream instead which eliminated the generation of small files all the data is in the streams now which would lead to utilizing spark resources more efficiently instead of caching or persisting data on the cluster they were writing directly to the spark park 84 however this resulted in another issue that is with the park a tables they ended up writing so many files to the tables the problem with it is continuous a pain on the table is too slow and if one job is updating the table another one cannot vary the table so ii stock was solving this challenge using data table Delta supports asset transactions which basically means they were able to confidently read and write this table it is also very efficient with continuous effect and the table and data like serve both as a batch table as well as streaming source and sink while we are at it let me cover few more performance options with data first is compaction so if you continuously write data to a data table over a period of time it accumulates a large number of files especially if you add data in small batches this can have an adverse effect on the efficiency of table reads and it can also affect the performance of your file system ideally a large number of small files should be rewritten into a smaller number of larger files on a regular Texas this is known as compaction you can compact a table by the partitioning into a smaller number of files second is auto optimize this consists of two complementary features optimized writes and auto compaction it is available in Delta if you run data on tied up rigs with optimized writes data bits dynamically optimizes spark partition sizes based on the actual data and it maximizes the throughput of the digna being returned so in terms of Auto pop action after an individual right Peter bricks checks if files can be further compacted and it will run a quick optimized job to further compact files for partitions where small files are still existing z ordering is a technique to co-locate related information in the same set of files this cool locality is automatically used Phoenix Delta data skipping and marathons so this time a dramatically reduces the amount of data that needs to be read to see order data you need to specify the columns to order in the C order by query one of the use case of this is time-sensitive requests emerging from GDP RNC CEA laws hace ordering can be helpful because the data on the keys that need to be deleted helps with speeding up locality of the requested records and efficiently rewriting their Patrick files so you got data storage compute and operational efficiency figured out for every company and especially the data companies it's really important to have governance to scale the solutions and operate in a secure secure and reliable manner one of the use cases which has emerged recently for organizations is the GDP RNC CPA compliance let's talk about how data helps with it with Delta and performance features we talked about in the previous slide you can dramatically simplify and speed up your ability to locate and remove personal information in response to consumer Chile here and CeeCee Munich West by default Delta Lake retains table history for 30 days and makes it available for other options like time travel and rollbacks but if you determine that GDP are and compliance requires stale records to be made unavailable before the default retention period is up you can perform deletes and use vacuum function to remove the files once you have removed the table history using the vacuum cover all users will lose the ability to view that history and roll back another best practice in general is especially for gdpr is if your original table contains personal identifiable information about the customers separate out the columns into PII and pompeii papers you can also see you Donna Mize the records by using sha algorithms for linking up to a lookup table so restricted PII table access and used non PII tables across your downstream and other upward stream of tables apart from applying these best practices to dazzle 8 it is recommended to set a retention policy with your block storage for 30 days or less so that you don't have to end up looking for data in the last minute so if you want to learn about this use case and more detail we also have our technical variable and the link is in the reference section other best practices for auditing and monitoring are using cluster tags so you can work towards chargebacks for different users and departments you can use audit logs generated by it interprets monitor you use the GPU usage regularly for awareness and spikes and just to be aware of how much you are using what does s transaction logs should be leveraged since it captures all the information about actions performed timestamp person who perform the transaction etc so you can leverage that for audit as well data pipeline is consumed by different layers in your organization and managing access roles and responsibilities as well as managing usage is a must you should implement fine-grain and course gain access and controls for your users think about the controls to give on the storage layer as well you can manage inscription up like I am role policies to the data being stored as well always it's a good practice to have minimum level permission required for each user bringing it together so this is a reference pipeline from one of my customers they started from very small step and efficiently utilized each step in Delta Kajaani to build a scalable pipeline this is how each component fits together in their in their full data architecture having our good data strategy in place leveraging design patterns that we discussed earlier to automate optimize and build performant execution pipelines you can then top it off with security and governance cut roads and tie it all together for business value hopefully the lessons and best practices that I shared can be helpful for your data peace and dignity knee thank you and want to mention that your feedback is really important so please rate our session now I can open the forum for questions 