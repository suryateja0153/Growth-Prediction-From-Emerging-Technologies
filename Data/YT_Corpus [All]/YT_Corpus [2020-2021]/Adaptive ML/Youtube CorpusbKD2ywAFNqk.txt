 all right hi everybody thank you so much for joining us for the fourth talk in the series um today it's my pleasure to introduce david alvarez melis david is a postdoctoral researcher at microsoft research new england and a research affiliate at mit he obtained a phd in computer science from mit computer science and artificial intelligence lab in 2019 2019 where his thesis works span various topics in machine learning and natural language processing he also is a bachelor of science in master of science degrees in mathematics from item and current institute at nyu he previous previously spent time at ibm research and is the recipient of kona sit hewlett packard and ai two awards just as a reminder this talk is around 45 minutes long and will be followed by a live q a you can ask questions throughout the talk via interface david will be keeping an eye on them and we'll answer clarifying questions real time or close to real time any questions that benefit from a more advanced longer answer will be discussed in our q a at the end i will be collecting them and with that we can start the presentation thank you so much hi my name is david alvarez-melis and i'm very happy to be here i'm very grateful to the organizers for inviting me to give this talk i've been following the previous editions of this uh seminar series and i i'm very happy to be among such a great company in terms of speakers so yeah let's dive right in the talk i've prepared today is a tale about data sets so a little bit unusually for a machine learning talk uh the focus is going to be on the data sets rather than the models and before i move forward i want to acknowledge that this is based on work with a stellar set of collaborators including nicola fussy from msr tommy jacqueline and stephanie igelka from mit and yousef mario from ibm okay to set the context for the talk i'm contractually obligated to recall that machine learning has witnessed a staggering progress over the past decade we're all familiar with these very visible success stories and across many domains many of which show superhuman performance and nowhere is this progress as impressive as for benchmark data sets which have truly driven the field forward in many regards so after looking at all these results from image classification to machine translation to speech recognition to a newcomer in the field it would seem as if superhuman performance was just around the corner for any task or data set that they had in mind however if you ask any machine learning practitioner of which i'm sure there's many here in the audience they will tell you that the story is not quite as simple of course there's many reasons for this but i want to focus in one in particular and it's perhaps the most obvious one and it's the issue of data scarcity so despite being continuously told that big data is everywhere this is again not the case for many applications in practice and here i'm using the word scarcity in a very broad way so i'm using it to refer to settings where there's no data but perhaps there's data from a similar domain available it could be that we have multiple data sources each with a tiny fraction of data or it could be that we do have late data but it's mostly unlabeled so labeled data is scarce or finally it could be that the data is just not usable or accessible in its uh in its natural way because of restrictions in terms of privacy or gdpr for example and i want to hone in on a set of common scenarios in machine learning in which this data scarcity plays a role so for example we have the typical scenario of transfer learning where we have a small usually a scarce target data set and then we want to pre-train our model in a potentially much larger data set and we have many data sets to choose from there's also the issue of distribution shift or drift where we train a model on a distribution on a data set arising from distribution and then as time progresses that distribution shifts so the data on which we want to test the model is not exactly the same as the one we trained it on there's the issue of data augmentation in which we're trying to modify our data in such a way that we get better performance in the future in a target target data set and there's also the context of federate to learn where we have potentially many small data sets for example from devices and then we have a large in-server data set that we sort of want to aggregate for all those data sets and as i went through this i'm sure that many of you started thinking of approaches to solve these problems and i bet most of them involve some form of model modification so you take a model you fine-tune it adapt it transform it something like that but i want to kind of raise a question of what happens in the case where we cannot do this so in the cases where the model is either fixed or cannot be easily modified and this could arise in many scenarios for example it could be that it's just too expensive to adapt so as we know the adaptation cost is proportional to the model size so if we have a very large model adapting it might not be feasible it could also be that the model is an api or a black box so we really don't have any way to to interact with it except querying it for answers or finally uh it could be that in the setting where we're trying to get multiple uh model adaptation settings we just don't have the memory capacity to store copies of the model for each of those instances and given that in mind i want to come back to these four scenarios that i described and i want to kind of brainstorm what would it look like to tackle these problems without modifying models so just operating on the data so for the first setting for pre-training transfer learning we could intuitively think that we want to find the most similar data set so compare all the potential pre-training data sets and find the most similar to the target domain of interest similarly for data set augmentation we want to find the augmentation that brings our data set closest to the target data set of interest for distribution shift we're sort of trying to reverse the process of drift so we want to take the test data set and kind of transform it back to what it looked like when we train the model and finally for federated learning we're trying to somehow aggregate the data from all these multiple sources in a consistent way where we can apply a unified model so despite the differences i claim that hiding behind all these operations on datasets are three fundamental and deeply intertwined mathematical concepts so these are sort of building blocks of data set operations if you will and the three notions that i'm talking about are the notion of correspondence between instances of the two data sets the notion of distance between the data sets as a whole themselves and a notion of transformation between data sets so these these three notions are essential to quantifying anchoring and maneuvering these operations that i described before and what if i told you that there's a mathematical framework that ties together these three things in a unified view and which in addition has very elegant and deep theory and practical feasible algorithms so you would probably think that it's too good to be true i would if i was in your position but luckily for us it's not the name of that framework is optimal transport it's a mathematical gem in the intersection of statistics optimization and analysis and i'm going to describe what it is in a minute but before moving forward i want to use this context to set the the layout for the rest of the talk so in the first part of the talk i'm going to talk i'm going to describe how to use optimal transport to align data sets then we'll move on to the problem of quantifying similarity or distances between data sets and finally we'll extend this notion and use it to define a framework to transform data sets in a principle way okay so let's start with optimal transport so i'm sure many people in the audience are familiar with optimal transport or at least with vasos and distance which are a particular case but in any case i'll try to provide a brief and self-contained overview of optimal transport so the most high-level way to describe it is through collections and just to set it in historical context let's think about coal mines and coal factories which is the setting that one of the forefathers of optimal transport gas pack mons had in mind when he proposed this formulation so we have the mines and the factories each one has a geographical location x and y and there's supply and demand constraints in terms of how much call there's in the minds and how much each factor it can receive since these are geographical locations we can compute pairwise distances between the source and target points and then the problem is to find an assignment of mines to factories such that we minimize the total distance traveled by the coal so we sum over the distances and then we try to minimize that objective so this is precisely the formulation that mons proposed almost three centuries ago but soon it was obvious that this formulation was too strict and didn't always have a solution so along came uh kantorovich two centuries later and he said why don't we allow mass splitting which essentially means why don't we allow the call from one uh mine to be split and sent to multiple factories so now instead of having hard assignments we're looking for a a soft assignment so a a coupling that tells us essentially how much mass is moved from each point in the source to each point in the target domain and again the total cost is how much mass is transported times how that how far the distance that is transported we sum over all these things this gives us the total transportation cost which again we're trying to minimize and now we have these supply and demand constraints to make sure that no call is created or destroyed in the process so this is optimal transport in a nutshell of course it doesn't always doesn't is not restricted to mines and factories this naturally can be phrased in terms of probability distributions so for discrete distributions uh as we've been talking about so far we could think of having multiple points each with a probability weight associated with it so we have two collections of of uh histograms the problem as i said before is minimizing the transportation cost by finding a coupling which in this case because it's finite dimensional space is a matrix so we're trying to find this uh allocation matrix that minimizes the total cost of course this can be naturally extended to continuous distributions and this is one of the the beauties of optima transport that it's equally flexible to talk about discrete distributions or continuous distributions i won't go into too many details of the continuous setting i'll just say that we can replace these things by their continuous counterpart so continue continuous distributions uh the cost is now an integral instead of a sum and then the coupling is essentially a joint distribution uh in in the cross product space so this is the the formal setting and i want to kind of point out that in the case where the cost function is the distance then the optimal value so the value that we get out of the problem is called the vassastine distance which is i'm sure many people know ot through that name in particular so that's the case when we take the call to be the distance which will be most of the applications okay so now moving closer to our domain of interest data sets how can we use optimal transport to compare them well we would do exactly what i described before the first step would be to compute pairwise distances between all the points in the two domains then we would essentially look for different ways to couple them different different ways to to match them and we want the least cost way of matching them and the value of that that's the distance and then we can take that to be a notion of distance or equivalently similarity between the two data sets so that's how ot would be used for data sets and before moving forward i should point out that machine optimum transport is of course not new it has three centuries of history but also in machine learning it's not new it's been used for the past two decades extensively especially in the computer vision community to compare shapes uh and more recently in in proper machine learning for domain adaptation and other classification settings and even more recently it's been applied uh as a loss function for generative models so if you're familiar with the vasa stand gan that's a particular instance of ot in the generative model setting so ot is appealing because it provides a principle and theoretically sound approach to all these problems which in addition has witnessed a a vast amount of work in making it more scalable in in fast and feasible so there's been a lot of work in fast approximate algorithms that solve the problem up to epsilon accuracy and they also have a compelling theory so there's been it's been a very active area of research in the past five to ten years okay so now that we have ot we know what it is let's put it to practice first for aligning data sets and the motivation application here is going to be the problem of unsupervised word translation so as you all know machine translation is usually solved by appealing to large amounts of parallel data so what this means is that we have two collections of documents and then we have correspondences let's say at the sentence level between one collection of documents and the other one in english and spanish in this example and then we take this parallel data we run it through a supervised machine translation setting and then we get our empty model which we can use to translate so this is all nice and beautiful but what happens in the case where we don't have any parallel data so this could be the case for example if we're trying to translate between languages catalan and finnish that just don't occur together in practice too much so we don't have any parallel documents so in that case it's it seems obvious that we need to do something unsupervised so without relying on known correspondences so this is the setting so we assume we have documents on two different languages just monolingual documents and in order to kind of build towards full-blown machine translation we'll start with the simple setting of finding correspondences between the words and intuitively what we could do is to leverage the fact that there's some similarities across languages that should ideally should probably be observed in their co-occurrence count so if we look at similar words across languages they probably co-occur with words that are similar across the languages as in the examples in english and spanish that i'm showing here so if we take this co-current statistics then hopefully we can use that somehow to find the correspondences across the two languages and i'm sure many of you are already moving ahead and saying why don't we do that with word embeddings which are a way to represent this coherence information in a very compact and computationally friendly way so we put this information together in word embeddings so we can take our favorite word embedding uh method like vertovec we run the algorithms on the two languages independently and then we have a collection of word embeddings across the two languages so now we put our optimal transport hats on and we look at this problem as having two discrete distributions that we're trying to match and we're trying to find correspondences across so we can compute distances between these two sets of points uh which is uh what previous work has done and there is one issue here and the issue is that by doing this we're implicitly assuming that the overall spaces are aligned the register and what this means is that the we're assuming that the axes across the spaces are indirect correspondence but this is not true in general in particular across different languages what could be happening is that one of the spaces is rotated or translated or reflected in a way that if we just try to compute knife distances across the two spaces then this is going to fail because these distances are meaningless unless we know what's the global orientation uh correspondence between the two spaces so embedded data has this problem of rotational invariance but this is an instance of a much more general class of problems in which we have two embedded data sets uh we assume that there's no prior correspondence now so we don't know anything about how they relate and even further the spaces themselves are unregistered so they're not globally aligned and of course the goal is to find correspondences between those two data sets so i'm going to briefly describe two approaches that we've proposed to tackle this problem in the first one we extend optimal transport to deal with invariances of a certain class for example rotations so what we want to do is we want to find in addition to the best matching that optimal transport finds we want to find the best registration between the space so the best global alignment of the spaces so as opposed to classical t where we just try to find the optimal gamma matrix here we're also trying to find the best orthogonal matrix p for example that rotates one space and aligns it to the other so we have this double minimization problem and this kind of formulation is not new it's been proposed before for example in the computer vision community but it's usually tailored to 2d or 3d shapes and it doesn't scale very well to large high dimensional settings like the word embedding that we're trying to tackle here and i won't go into too many details i'll just say that the formulation we propose which uses shatter normal variances is fairly general but at the same time is efficiently optimizable so it has a good trade-off between these two things so this is a formulation i think in the interest of time i'm gonna skip this part but i'm happy to discuss it either in the q a or i encourage you to check out the paper for the details on how it's optimized instead i'm just gonna jump right to the results and i'll show you how this new formulation compares to the classic optimal transfer formulation so here i have two point clouds one is a rotation of the other but they're otherwise identical and what i'm showing here are the correspondences found by optimal transport and they're called red if they're incorrect and green if they're correct so if they're the points are in good correspondence so for with classical d they're all wrong and it's not surprising because the classic formulation doesn't know about this rotation so it's just trying to kind of really match closest points whereas when we use our orthogonally invariant ot then it finds the right correspondences and this happens across many synthetic datasets and i'm going to move on to real experiments in a minute but before i do so i want to quickly describe the other approach that we propose to this problem which instead relies on the gram of vases time distance the gram of system distance is an extension of optimal transport for the non-registered case so the case where the distances between the space are non-existent or not meaningful which is exactly what we're dealing with here and the main idea behind the gram of asset and distance is to compare the relations between the points instead of the absolute positions of the points what this means in practice is for example for the collections of word embeddings that we have here we would compute pairwise distances in the two spaces and then we would try to align the distance matrices themselves instead of the points so for this we define a loss between uh entries of these matrices which essentially can be interpreted as the cost of transporting one unit mass from point i and k to points j and l on the other side so this is now a quadratic assignment problem and we're looking at pairs of assignments but otherwise the problem is similar we're trying to find a coupling a matrix that minimizes this cost so this is the formulation of gram of acetylene and again without going into too many details i'll just stately i state briefly a few interesting things about this distance first it's an actual proper distance in the mathematical sense so that's nice uh then the formulation is non-convex so that's not good news in general but it still can be solved uh efficiently in an approximate way as has been shown by uh solomon and colleagues and for this particular instance of word embeddings for very large problems we proposed a way to first solve a reduced version of the problem and then fit an orthogonal mapping that kind of lets you extend and uh align the rest of the the the points in the two domains okay so let's uh focus on our application at hand more translation so just as a recap we have monolith uh documents we embedded them and then we'll try these two approaches the invariance ot approach and then the gram of assets line approach so that setting is a benchmark that is widely used for unsupervised work translation it's the muse data set which contains fast text embeddings of dimensionality 300 and here we have five pairs of languages and the evaluation metric gives the translation precision at k so for example i'm going to show you a run of our algorithm on this particular pair english to spanish so here i'm showing in the red dashed line is the objective of the optimization problem and then the colored lines are the accuracy of translation so this is for the invariant of the problem and this is for the grammar of acetone problem so the first thing to note here is that the objective the optimization objective the red dashed line is strongly predictive of the metric of interest uh that is the accuracy and this is very good because in a real application we wouldn't have access to this accuracy as we're training so it's good to know that the objective that we have at hand during optimization the red line is a close predictor of the final accuracy that we'll have okay so now looking at the pairs of languages this the precursors method is a very strong baseline it's supervised so it provides a kind of very high baseline starting point and then this adversarial network method is the one that is proposed in that newspaper and at the time it was state of the art so now we look at what we get with the invariant ot and gram of ascetic settings so we see that overall we either match or outperform the state-of-the-art at the time method the other cereal network but the really nice thing about this is that when you look at the runtimes this is the adversary network approach and these are two approaches so they're orders of magnitude faster than the neural network approach so they give comparable performance sometimes even better at the fraction of the computational cost so that's very good news and without going into too many details i'll just mention that there's we have follow-up work that shows how to extend this to hierarchical data in which case we now embed the data sets in hyperbolic space we define an extension of ot in hyperbolic space and then we apply this framework and we also get pretty nice results so if you're interested in that i'll have a reference to that paper at the end okay so for the second part we're gonna switch gears and now talk about distances between data sets so the motivation for this part of the talk is going to be transfer learning and classification so in that setting we have a target data set that we assume has you know scarce availability of data and then we want to pre-train our model on some data set so there's many publicly available data sets out there so how do we choose which one to use intuitively we want to choose the one that is most similar to the target data set of interest but similar in what way so that's the key question that we're trying to answer here given two data sets how do we define a notion of densest distance or equivalently similarity between them and the notion of distance that we get should deal with many challenges that arise in this setting so first the data sets could have different size different number of examples they could potentially have completely different label sets so digits on one side and fashion clothing items in the other side like for these two data sets and finally we want something that is sufficient and is ideally modern agnostic so it doesn't depend on any particular model or architecture so our approach to this problem can uh as you guessed uses optimal transport and can be conceptually broken into two steps the first step is to define a notion of distance between the labels so the discrete labels and to be able to do this even when the labels are completely unrelated what we do is we represent the labels as distributions so as a distribution of the the features that have that label so in this case the label 2 would be represented as a collection of images which have the label too so all those images of twos and the same on the other side with this we can define a notion of distance between the labels as the vast line distance between their distributions and in turn this allows us to define a notion of distance between pairs of feature labels so given for example two with a label two and then an image of a bag with a label bag we can define a distance between those two instances first by just using something like the euclidean distance to compare the the features the images and then using what i just described to compare the labels so with this we have a notion of distance between z z prime which are pairs of feature labels then we can do this for all the the points in the dataset so we compute all pairwise distances in the data sets and then using optimal transport yet again we can define a notion of distance between the collections of points the distribution of future level pairs also known as data sets so again we're supplying this metric and then we get this optimal transfer problem that now gives us a notion of actual distance between data set a and data set b so i won't go into too many details i'll just describe four key properties of this distance one is that as i hinted at before it's a true actual metric in the mathematical sense b is that it's computationally scalable especially if you use this guts approximation that we propose in the paper that lets us scale to very large dimensional data sets using the fact that devices and distance between custom distributions has a close form solution so no optimization is needed in that case we also can provide upper and lower bounds that let us estimate the confidence interval around the distance and finally we have some experiments showing that using the label information so not just the feature information it's crucial to get good performance so all of this is in the paper and you can check it out for for details but here i'm going to move on to the experiments so what can we do with this novel notion of distance so thinking about the setting of transfer learning which motivated this approach let's consider a setting where we have these five simple pairs uh simple data sets all uh mnist and friends which have different types of labels for images of digits and then letters and clothing items so they have uh some of them have non-overlapping and completely different label sets so the first thing we do is we compute pairwise distances between all of them so these are shown in this uh heat plot here and you can right away see for example that according to this metric mnist and fashion mnist are the most different among these five pairs of data sets now in order to see how this could be helpful for transfer learning uh independently on the side we train models on one data set and then adapt it to the fine tune and compute test accuracy on the other one and we do this for all pairs of of data sets and this is what i'm showing in the plot here so in the x axis i'm showing our ot there's a distance and on the y axis the notion of transferability so the drop in test error when we train on one and then fine-tune on the other and right away you see that these two quantities are highly negatively correlated which means that data sets that are close in otd distance are also lead to the best transferability on average across the data sets so this suggests that we could use otd to to predict and to select the data set that we want to use for pre-training and fine tuning and here i just quickly want to show some of the inner workings of the distance for a particular case from emis to usps so here i'm showing the optimal coupling of the optimal transfer problem which in this case given that we have digits on both sides is what we would expect a block diagonal matrix that is essentially telling us zeros are matched to zeros and nines to nines etc another interesting thing to look at is the pairwise labeled distances so these vases and distances between the label distributions which again shows is a subtle diagonal structure as would be expected and by playing with the regularization of the optimal transfer problem we can make this coupling the correspondence as sharp as we want in order to get actual uh assignments from source to target okay so we can also use resistance to guide augmentation protocols so in this case let's suppose we have a data set mnlist and then a target data set usps and then we want to apply different transformations or augmentations on the source data set and we want to select the one that is best for transferring into the target data set we repeat a similar experimental protocol we first compute pairwise distances between the augmented versions of advised and then usps and then transferability across the data sets and again we see that the otd is highly predictive of the final transfer accuracy in particular you can see in the left top part of the plot that cropping the images gives us both the the small resistance so the closest data sets and also the best transferability which once you look at the the digits in mnist and usps that makes sense we have additional experiments on larger data sets like tiny imagenet c410 and also on nlp data sets which for which we embed the sentences using birth and then we repeat this experimental framework in all cases we still observe this very high correlation between our notion of distance and then transfer uh accuracy in in the source and target data set okay so for the third part of the talk i'm gonna sweet gears yet again and we'll now talk about uh transforming data so i'm not just comparing them but actually modifying one of them [Music] and for this i want to motivate it from the perspective of what i call data set shaping so by this i mean that we have a an initial data set and then we'll try to modify it to transform it using different kinds of constraints so one possible constraint would be to for example uh obfuscate certain protected attributes that the dataset has uh we might want to make it easier to learn on this data set by for example increasing the class separation between the the clusters of the classes or we could uh want to modify it in such a way that for a pre-trained model we are maximizing the performance of that model which is frozen on this data set so there's many kinds of things that you can think of that you would want to impose on a data set and what we argue here is that all of these can be approached and formulated in a unified framework that essentially consists of two components we have a an initial data set represented as a joint distribution and then we have an optimization objective on data sets such as the ones that i described here and then by minimizing this objective we want to get a solution that is an optimal distribution that minimizes this objective that we proposed so this is now the the question how do we formulate and optimize this in a computationally feasible way so just to kind of develop our intuition for this i want to emphasize the difference of what we're trying to achieve here and the typical optimization paradigm that we see in machine learning which revolves around parameters of models so usually we have the optimization variable is a vector or a matrix but in any case it's finite dimensional uh the objective is a function defined again in this finite dimensional space and then the way we solve it is usually by some form of gradient descent so we take gradient steps until we find a good local solution in contrast what we're trying to do here with the data sets is we're trying to optimize in the space of joint distributions joined between features and labels and therefore we need an objective that is also defined in this space so it's a functional that takes a joint distribution and spits out a real value and then the optimization method well that's what we're trying to figure out so it's not exactly clear how this uh translates especially because this crucial aspect of moving from finite dimensional optimization to infinite dimension optimization so the approach we use there's many ways to approach this and here we do it through the language of gradient flows which are a linchpin of applied mathematics and to kind of show the intuition of what uh this framework is about uh let's go back to the usual optimization paradigm where we take discrete gradient steps and you can think of taking the limit of the step size in that scheme as the step size goes to zero you make smaller and smaller steps and eventually you end up with something that is a continuous path so in this graph instead of this dotted line we would have a continuous line that takes it from initial to end points so now the optimization path instead of a sequence of iterates is a curve in this space and then the scheme that guides the progression of that curve which is a geodesic is a differential equation and there's been a lot of recent work uh exploring the properties of gradient descent methods based on this limit uh taking the limit of the step size approach but here what we're gonna do is we're gonna first map the gradient flow to probability space so that's that that has been done in the past so there's a lot of interesting theory uh on how that extension works i'm not gonna delve into details i'll just briefly mention that gradient flows can be defined in these probability spaces they have a more complicated form but here again there's a differential equation now it's a partial differential equation that describes this curve of maximal descent and an important aspect of the the usual optimization setting is that implicitly it uses the euclidean metric it might not be visible from these equations but it's there it's implicitly using the fact that there's a metric in the space and it's usually the euclidean metric so in order to define these things we need a metric on probability space and not just any probability space but the the space of joint distributions um x y and so here what we're doing is we're implicitly using the ot dd distance that we described in the previous part of the talk to endow this space with a metric so now that we have this metric we can apply all these uh all this theory for flows in this space and finally the last step is how do we get something computable from this so there's two steps here first is a finite particle approximation so we represent this continuous distributions as finite samples and then there's a time discretization step which finally takes us to something that looks a lot like the grain step scheme but now we're doing it for multiple particles so we have multiple samples each one is being updated through this step scheme which as you'll notice still has a function defined on a data set on a distribution so that's kind of coupling all the particles together but we make updates independently on them okay so two technical details that i want to highlight the first one is let's recall that we have our points z here our pairs of features and labels and the labels are discrete so how exactly is this update happening and there's different way different ways to do this we propose three ways to do it the first one is to take gradient steps on the features and then recompute the label distributions based on those updated points and then we continue doing this an alternative approach is to do gradient steps on both the features and the distributions the label distributions through their parameters so we make parameterized updates on the distributions but we keep the label assignments fixed which means that if we start with a point that is labeled two uh as we progress the flow it's always going to be a two even though the representation of the distribution of twos might change that point is always a two and the final and most flexible approach and that's closer what we're trying to achieve is one that allows us to have uh flexible label assignments so that point that was two at the beginning as the flow progresses might become something else so it doesn't have to always have the same label and i'm going to compare these methods in the next slide so this gives us three types of feature label dynamics and then the other elephant in the room is how feasible is this and whether they're nice objectives that we can formulate so akin to how convexity gives us nice objectives in the usual euclidean space there's a an equivalent notion a similar notion is not fully equivalent of displacement convexity in probability space and so what we do is we restrict our function somewhat so that we can guarantee displacement convexity for the types of objectives that we choose so we without going into details um i just mentioned that we have a general family of functionals that combine uh first a the otd is a notion of distance between data sets and then three other types of objectives which can be interpreted as a potential energy an interaction energy and an internal energy so by combinations of these objectives we can encode many data set objectives like the ones i used to motivate this of future affiliation class separation and empirical risk minimization etc so this is a fairly flexible class of functionals which still has a nice convergence properties okay so now that we know what optimal transport is about let me tell you why i care about it so here in this case uh using this notion of dynamic optimal transport we can transport a source uh initial data set the red one into a reference data set the blue one and here these are five class data sets so each cluster corresponds to a class so this will this uses sgd and is the first type of dynamics that are described they feature driven dynamics so as we move the flow we can see that one distribution adapts and matches the other distribution and in the paper we have a comparison of different types of optimization and the three types of dynamics that are described before which give us qualitatively very different flows so very different dynamics between the two data sets so this is all for sgd we can also do it for atom and we get also qualitatively qualitatively different uh dynamics between the data sets so this is all nice and good this is in two dimensions [Music] so let's do something a little bit more interesting probability constraint and code through an objective of the type that i described before so i'm going to compare using three types of objectives and the kind of flow that we get between the data sets so the first one is just using the distance to set a baseline of whether we can recover the data set faithfully so as this progresses you'll see that indeed we can recover the data set so now what happens if we just use the constraint functional we run it and indeed we get classic probability that's nice and then if we combine the two which is what we're really after we see that we can recover the overall uh semantic uh form like the shape of this data set with the constraint that the classes are indeed separated so that's nice and we can do this for other types of constraints for example let's say we have a protected attribute that we're trying to collapse we want to kind of obfuscate it so again we can run the pure distance flow then the just constrained flow which is squishing the distribution and then those two combined gave us again a data set that now is a collapse along that dimension but it keeps the geometry of the dataset that we're trying to emulate okay so now moving into more interesting experiments here i'm showing an instance of a flow between usps and mnist so you'll see how the digits progress as the flow moves forward and this sort of begs the question of um efficiency so if we have to run this flow for every particle then you know we have to run this whole process so a natural question is whether we could do this by skipping that part by learning to imitate what the effect of the flow is and then just parameterizing it somehow and mapping additional particles without having to run the whole pipeline just taking this mapping so that's what this experiment is about so we take the initial particles and then the particles after t steps and then we try to learn a mapping between these two we parameterize it as a neural network and then we run usual training there and we map the initial particles with that so that the third paint shows what we get with this mapping which you can see it's pretty close to what the actual flow gives us with the uh added bonus that in a way it looks like it's a little bit more regularized but overall this tells us that indeed there's hope to use a few particles to flow the data sets and then try to learn this effect of the flow parametrically okay so finally moving to a more realistic application of transfer learning again i'm going back to now these four data sets of variances of mnist and then we have a setting where we have a small target data set just a hundred examples per class and then a much larger source data set and what i'm going to show you is the result of flowing those points so taking those points and using different versions of the flow and then training a model on the float samples so here the green is the baseline of just using the target the few samples that we have there and then the other ones incorporate the source information in different ways i just want to kind of highlight the the pink line here and the green line i would show that using the flow information indeed is very helpful for for adaptation so we get much better accuracy and interestingly using the full flow trajectory improves over just using the last step which conceptually and intuitively it makes sense but it was nice to see in practice so there's there's rich information in the whole flow that we can use for adaptation and here i'm just showing for each of these pairs of data sets uh how the flow the flow looks like between you know these two these pairs of data sets how the particles are transformed through the flow um and finally an interesting thought experiment and validation is an oracle setting where we have a very good large pre-trained model on the target data set and then we get samples from the source data set so in this case the source is usbs we have a very good high accuracy model pretend on mnist and then what we want to see is we want to a quantitative way to evaluate the quality of the flow as it progresses and we can use this this oracle model to gauge the quality by looking at the accuracy of that model on the flow data so that's the bottom plot the top plot is the value of the of the flow objective and you can see that uh as expected as we flow the particles they start looking more and more like mnist so the the oracle model on amnesia gives us good accuracy to a point that we can reach even 100 accuracy in some cases so these are different settings with different regularization parameters and we observe um similar results on different parts of data sets and interestingly on the third pane here we do that with a histopathology data set the chameleon data set which has a breast cancer image diagnosis images and then cipher 10. so it's interesting here to think about the accuracy that we're getting so we're using a model that was trained on c410 that has never seen any kind of histopathology images and it's still giving us a pretty remarkable high accuracy so it's it's interesting that we can kind of validate the quality of these flows through this oracle setting so okay i've talked about a lot of things in this talk so i want to wrap up by first summarizing and then discussing a few key takeaways and future directions of this work so as you recall in the first part of the talk we focus on the notion of correspondence between data sets and we talked about methods to align embedded data sets where we don't have any prior correspondence so the fully unsupervised setting and we show how to extend optimal transport to model these in variances and we show that for the word translation setting we can get very very good results at a fraction of the computation cost of much more complicated methods then in the second part of the talk we discussed how to propose and define notions of distance between data sets based on optimal transport and we showed that this distance is particularly useful because it can compare data sets even if their labels are completely incomparable [Music] and we show that this is essentially an extension of optimal transport for distributions that are joint feature label pairs and that it it gives us a good uh proxy notion for transferability so it can you it can be used to guide transfer learning and data documentation and finally on the third part of the talk i described a fairly general framework for a principled data set optimization which uses the optimal transport dataset distance that we proposed makes it differentiable and applies it to this transformation setting and as we showed with some proof of point experiments it can be used to shape data sets imposing different types of constraints okay so in terms of future directions there's of course many things from different uh the different parts of the talk uh one very interesting one that we're looking at right now is using the otd in the context of meta-learning which is one of those settings where we have multiple data sets that might be slightly different so having a notion of similarity between those data sets could be very useful and there's previous approaches that uh most most of them are model dependent and architectural dependent so it would be interesting to try this completely modern diagnostic framework in that setting then there's also the question of uh for the for the gradient flow setting we showed how adaptive uh grading descent method like adam can be used but there's a question of if we can import other kind of tricks from the deep learning toolkit so things like batch norm and those kind of things can we apply them there are they useful so those are very interesting experiments to try and finally one of the applications that i mentioned uh for the last part of the talk but i i didn't get to uh to show is using this for uh automatic automating and uh kind of optimizing the data set augmentation policies so given a data set i sort of want to transform it through augmentations in a continuous way in such a way that it helps me minimize a certain objective so that's also a useful and interesting application of this setting so as i mentioned through the talk there's many details that i had to skip but here's the references to the paper so these are links so you can click on the if you have the slides you can click on the links to to see the details i'm also happy to discuss the details during the q a session i also want to highlight that we had a blog post in the msr research blog on the otd so there's a lot of interest interesting interactive visualizations that i encourage you to check out and finally the code for the otd and the growing flow parts is here hosted in the microsoft github repo so please check it out and yeah with that i'm happy to move to the q a session and take questions from the audience thank you very much you 