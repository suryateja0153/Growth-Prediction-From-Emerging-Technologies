 *Maryam Fazel* Okay, Hi and good morning everyone I'm Maryam Fazel, a professor in Electrical & Computer Engineering, and today it's my great pleasure to introduce professor Tara Javidi from UC San Diego and actually to welcome her back to UW, well at least for a virtual visit Tara received her bachelor's in Electrical Engineering from Sharif University in Iran and she has two masters degrees from the University of Michigan one in Electrical Engineering, and one in Applied Math. And so that is her PhD from the University of Michigan in Electrical & Computer Science and this from 2002 to 2004 she was actually an assistant professor in our own department we were very happy to have her here for a bit at UW in 2005 she joined the University of California in San Diego where she is currently a professor of Electrical & Computer Engineering. And she's the co-director of the Center for machine integrated computing and security, Tara's research interests are in the theory of active learning statistical inference information theory very broad set of interests also including sarcastic control and communication networks. So with that we welcome back Tara to UW ECE and we are looking forward to your talk! *Tara Javidi* Thank you so much Maryam, and thank you for inviting me, as I was telling you I was really really hoping for a nice revisit to campus and get to see everybody all my old friends who are now very important people running the campus as I learned but oh well I'll come back. So thank you for hosting me and I am very happy to be giving this talk! so today I'm going to talk about black box optimization and this is most of the talk is going to be on on some theoretical results that my really brilliant student [inaudible] has obtained and then at the end of the talk and we'll also talk about applying some of these theory at least in theory and then sort of coming up with heuristics with joint work actually oh I'm sorry I'm missing the name of my collaborative final solution the computer engineering side has introduced the application to compression of deep neural networks on an acceleration and that work is joint work with Mohammad Samragh and Mohsen Javaherpei and at the end of the talk I'll show you some more practical implementations. So let's get started so let me just describe what the what I call black blackbox optimization so consider a simple function f on on some domain X that gives you a functional that gives you a value in R and we would like to optimize this function with the caveat that F is not known explicitly to us so I don't have a closed-form formula for F so that I can optimize it and in fact I might actually like quite a lot of knowledge about the structures of this problem and this function so the way I would like to optimize it the only way that's left for me to optimize it is to access it through evaluations that are expensive so I can submit X and get a noisy version of F of X and I would like to sort of repeat this process in order to find the optimizer so the the problem of blackbox optimization or some people call it zeroth order optimization is to design a sequential strategy of selecting m query points x1 to xn in this curly x to efficiently optimize F over a horizon and over these samples. And the performance metric that people look at are two different type they're closely related but you'll see that sort of distinct distinction between them the first one is called simple regret so these are the difference will be between what you will find at the end of the process of exploring this function f so you will submit X of M as the best optimizer and you will compare the performance of a Oracle that would know the function f and would have optimized it and find X star the optimizer you compare yourself to the performance of that Oracle so that's called simple regret because basically all the process in the middle x1 to xn minus 1 are just exploration you can do anything you like another maybe more complicated notion of performance is cumulative regret where you're actually penalized every point in time up to the horizon and relative to this Oracle response f of X star in this talk for simplicity let's assume X is some cube in Rd so just a compact subset of Rd will do will do even though that the results can often be generalized to more complicated spaces of x. The problem that I would argue comes up in engineering all the time so in my former life in fact when I started at UW I was focusing on wireless networking so what I'm showing you in fact I don't know if it's it's visible on the screen is the layout of a wireless network that we have set up in the build in my building consisting of a few nodes some of them programmable and some of them are just sniffing the the wireless medium and the job, my job back then was to actually optimize these programmable clients in terms of the setting of these wireless devices so you can imagine the setting of every box to be put together into a long vector of X often these networks are so complicated that we don't have any close form understanding of how these settings will affect. And network designers often are left with heuristicly searching in the in the space of the settings and play with this run the network for a while measure the performance let's say in terms of error probability or power consumption or whatnot or whatever measure of performance that you care and come back as how good this setting is and then you just do this for a while right so that's exactly the setup that I just posed and and of course it's a noisy version because you're not going to be able to run these networks for very long, right, so that like whatever you're measuring its averaged out over small window of time relative to the time that the network will be deployed another example and the example that I'm you know these days is very popular and has brought a lot of attention to this black box optimization is the problem of acceleration of a deep neural network on a mobile device I'm gonna talk about this a little bit in more details so the idea is that you have a neural net often these are incredibly expensive to run on smaller devices so you usually have a data scientist and an engineer working together to decide what parts of this neural net they can sort of throw away which hyper parameters they can sort of optimize over in order to get a good performance in terms of hardware cost and accuracy. So and actually in fact these two applications give a good sense of what this simple regret versus cumulative regret is so let me just let me just talk about this like I just described this so in the network optimization I have these set of possible settings my network performance is the unknown function f and the horizon of empirically Network tuning the network is this n so if you have time to train the network you can do a simple regret if you're using the network as you go of course you don't want to take crazy settings that you already know are not so good right so you don't want you want to minimize the interruption to the to the network operation so you would think about cumulative regret. So now of course this problem in general is very ill posed right because unless I tell you some things and give you some prior assumption on f of X it really makes nodes you know there's nothing there's nothing you can do except for search the whole space for the Maximizer so in most of the literature you have to have some assumptions about how f of X and how f of X prime are related to each other as a function of proximity between X and X bottom right so that will help you hopefully by testing f of X to give you some information about some X finds and having some prediction as what the value of the function will be and the first thing type of assumptions that I like to talk about in this talk is when we assume that F is an unknown function yet it's a sample drawn from a known gaussian process as zero mean Gaussian process with a given covariance function so I don't know the function itself but I assume that it's pulled out of a Gaussian process with zero mean and it and the observation model is also Gaussian so I'm observing lies as a function of f of X plus the noise that I again know the variance of this response so in this setup the problem becomes far simpler why because I can I have a Gaussian setting so I start with a Gaussian prior and now I can compute for every sample X I can come in that I have collected or maybe x1 to XM t that I have looked at t minus 1 that I have looked at I can describe the process still remains Gaussian it's a linear observation so the process remains to be Gaussian and all I have to keep track of is the posterior mean and posterior and posterior variances at all points and this can be computed as a linear estimator that that we have seen in your basics in our processing classes so you can compute the covariance matrices between the observations and and that makes it it gives you the best estimator of the posterior mean. *Maryam* I have a quick question. So f is a Gaussian function and it's R is also Gaussian noise? *Tara* R is Gaussian noise f is a sample drawn from a Gaussian process so so let's think about X being a discrete set so let's say X has needham and you know that's like one dimension and I look at x1 x2 and so all it says is that the process of F of X is a Gaussian process as a function of x1 x2 right so let's say right like so x1 let's say it's a 1 2 3 as I rank order them so that's the Gaussian process. Right so if it's a Gaussian process is described with the second moments so every time I have to get a guess I can calculate the remaining positive emotion and the mean and the pursuers the posterior mean and posterior variance can be confused. *Maryam* so some R can be gaussian? *Tara* you imagine at the beginning you sample from a Gaussian prior and now the function is fixed and forced to you the noise is Gaussian so now you are making a Gaussian observation of a Gaussian vector. Right so you have a guassian vector that you are basically estimate. *Maryam* Yep, thank you! *Tara* Okay so this figure on the right shows actually I should have actually in answering your question my own I should have shown this so this is showing my process was a sample process that the black function is the process itself I have sampled it at the red spots and I have observed f of x1 x2 and so on and now given those values I have shrunk my poster mean and variance which was around zero I was a zero mean prior whether known variants and I have shrunk it and made it sort of tight around the real sample sample that was drawn. So this actually is used a lot in this second class of problems that are telling you is very popular it's called hyper parameter tuning in machine learning so the idea as I said is that you have these models and you would like to map them on different Hardware so there are a lot of hyper parameters or maybe compassion techniques that you can use on these neural nets and normally people imagine a data scientist and an engineer working together to decide what the right hyper parameters are and this function optimization framework is really replacing that iterative process between the engineer and the and the data scientist with a sort of an engine of optimization engine basically and in this set up really the hyper parameters of the neural nets create a classifier you know define the classifier the best trained classifier and the performance on the test set is usually the function that you're trying to optimize and you're going to try this over time if you are again if you're only interested in training your network and then you can take crazy exploration decisions you can use some hyper parameters that are off just to learn something if you're actually using the model as you go you you would like to do a minimum cumulative rate that right so these are this is and this is very popular and and the Bayesian version of this is often used where people assume for these for these problems that a very popular covariance function that is assumed is known as a maturing family and basically I'm writing down the covariance function as describing the the correlation between the value of the function at point X and X prime as you can see it's a bit of an exponential component here and as you see my the exponent as a function of the distance between the two value and there's a polynomial term and the popular family of maternal that are used in in practice are parameterised by this factor new and this new you know I'm going to talk about you know these are the target family that we're really interested to perform well right as you can see so I have two points sort of go away from each other their dependents sort of decrease this line that's roughly the intuition gets me okay. So I'm going to take a bit of the time to explain what the what the setup and the prior work is and where we sort of started looking at this problem slightly in a difference on framework and I should say I should give credit to Galen Reeves I was giving a talk on active hypothesis testing and he introduced me to this problem said can you actually hypothesize that maximizers location and then they've thought and you know use your active hypothesis testing for solving this problem again remember I'm going back and forth between measurings thing or submitting a query and looking at noisy version which is very similar to active hypothesis testing so let me try to understand the Prior work too to explain the prior work and there are people at UW who are very very have done excellent work in the space so I apologize if more important to those of you who are familiar with the state of the art so often as I said people look at the posterior behavior the posterior mean and the variance and the query the next value the next sample to be queried X of T to be maximizing something that is called an acquisition function so they have a sense of how informative sample X would be and then they try to maximize that informative eg and this alpha of X this acquisition function is the utility of querying X and often is balances exploration and exploitation and the common ones the most popular ones is you try to estimate what is the probability that the neck this value is gonna improve on the values you have seen so what is the probability you know that this function is larger than some tau it could be an expected improvement what is you know you will look at the value of the function and and say ah what is the probability what is the expected improvements in the value that I will actually obtain a simpler notion is an upper confidence bound you can look at the posterior mean and look at the linear combination of the posterior mean and the posterior variance and I will give you the figure that I showed you that will give you an upper bound as well as the lower bound but the upper bound is often is you can be viewed as an acquisition function so and so these are the common ones there are other ones but this is this is good to get an intuition and of course where we came at this problem we were surprised because alpha of X itself is often nonconvex with a lot of local maxima so even optimizing and it was not clear how even to choose X to optimize alpha of X so that was the first problem that that becomes critical so what people practically do is practically discretized X and look at doing you know sort of a exhaustive search over that discretization which immediately meant that this complexity of these algorithms are going to be very poor web dimensions because if I discretize the dimensions will show up in terms of an exponential complexity of the search over to optimize alpha so that was the first thing that we thought can you do better can we actually come up with algorithms that don't explode with dimensions the second aspect of the prior work that I would like to talk about is the the kind of regret analysis that you would get from these so existing bounds on on cumulative regret and you can get similar bounds in terms of simpler regart of our the following form that they will always show that the regret is bounded by you know a function of n the number of samples and a function of what is known as gamma of n or the maximum information game so this is you you can define this gamma of n as the supreme um over the choice of samples that knowing Y at those points gives you the informative mutual information that you can get that gives information about the function itself so you can calculate this mutual information and this ends up being sort of the upper bound on you know on the redraft that you can control and and for specific kernels like mature and kernels you can often go and calculate this quantity and hence get specific bounds for what the regrets looks like for these families and - just the the second point that we were feeling like this is actually an overkill is that gamma of n gives you a maximum information about function f and if you think about it from the perspective of hypothesizing the position of the Maximizer you should really be looking for maximum mutual information about the position of X star I'm not necessarily as F could be a really complex but position of X star might be cheap so that's the second thing that that we wanted to actually address and we were hoping that this way of looking at hypothesis testing will solve the problem let me show you an example to crystallize this so this is a Gaussian process which looks very complicated it's a fractal structure so it's you can actually compute gamma of n for this so this is actually Shekhar has gotten very good at building this pathological example the gamma of n is very large actually grows with n is the linear function but the Maximizer is very very simple it's either this point or the flipped version often so in other words you with one sample I could have found the Maximizer but I it takes me a long time to learn the full function because of its practical fractal structure so we could come up with an example when we knew these regret bounds are too pessimistic and we were looking for algorithms that can do better in terms of using the geometry so I'm gonna give the overview of what what we did which is different and then I will go into describing the algorithm and so on so let me just give you the overview so the first thing that we did the first contribution that I would say shape our path is the algorithmic improvements so and if you remember when when we were to solve the optimization problem all prior work except for one which I will come back at the end and compare our results the two there was a discretization of the sample space X which meant that you have a exponent growth in terms of dimensions in the complexity of the algorithm you always have to do this auxilary optimization of acquisition function over this discretized space so algorithmically and I will show you the algorithm what we did we said we don't need to discretize the space offline we can discretize the space in an adaptive fashion and that will give us an improvement in terms of complexity so we I will show you that the complexity of our little competition for boxes of our algorithm grows only linearly with dimensions but also it allows this adaptive structure to adaptively and opportunistically you know adapt to the simpler structure of function f if such a simpler structure exists so you don't assume it but as you go on if the structure is simpler our algorithm will zoom in faster on the on the lower dimensional may be embedding that that might exist in your function and again I will show you some example with that the second set of contributions is in terms of analysis so there regret analysis and that we have on these problems uses the geometry and hence instead of being informational type using this gamma of n this information about the function it really is a dimensional type bound it basically gives you a sense of this structure the underlying structure of the function and so I will show you the regret bounds and what I can say in summary is that it's better than the regret bounds in Prior work it's sometimes strictly better in particular form a trunk kernels of relatively general dimensions the the results are strictly tighter and in particular it gives us the first sub linear bounds for exponential kernels these are my mature kernels when the new is as the smallest is a half so let me tell you what the algorithm does and and kind of shed light on why I was saying this is a you know this view of active hypothesis test is useful so the kind the construction of the algorithm is to take the function and replace it with piecewise constant upper bound confidence intervals so I so we were just gonna do get a really simple approximation of the original function with piecewise constant upper bounds okay so this is my function I have evaluated at these points and observed these values so I'm gonna use what I know about the uncertainties or the noise and as well as the Gaussian structure to get these upper bounds function the upper bound relies on two bounds one bound is simple it's basically mu the posterior mu T plus the you know linear plus the you know some sort of a constant weighted variance so this quantity is what is called upper confidence bound that is used in the literature so our bound is inclusive but it has the second component which is this quantity and it's really a function as an upper bound on how much variation this function will do in the cell that the function is constant over so I have use these piecewise constant functions the width of the constant and that constant piece is going to be determined by how much variation a Gaussian process can do and this is a geometric phenomenon that check out would show that if you cool as the Gaussian process sample from a Gaussian process with a given kernel you can actually bound how variable how much variation you have in any subset or in fact you can show it's a Lipschitz function and if you can get guarantees for what that lives just constant is so that's how he finds this second quantity and now you have this piece my function with upper bounds you look at the these and you maximally pick the set that that gives you the maximum and you decide okay this is the right place to further explore and you do this by either sampling again in which case that if you know basically you're smoothing the noise if you sample the stay in the same location in the middle of this cell you will just reduce the noise or if you decide the noise is small the problem is the variation is too large because the size of this cell is large then you actually take that subset and break it into pieces that's the adaptive discretization so instead of looking at one point in the middle now I look at two points that are maximally breaking this region and you do this algorithm you sort of do this over so to just formalize what I just described and you can do this again now if you look back now the Maximizer is another subset and I can do the same thing either sample again or if this uncertainty is due to the variations because the set is really large the set is really large what you do is you break it so you just actually build this sort of this adaptive discretization of this space so this is the formal formalized way so basically we define a six tree of the partition of the space where you know you can sort of think about this decision tree of middle points one-fourth points and so on and so forth and you create these cells and in the and the cells are such that they're sort of size are getting smaller as you go lower and lower in decision tree you can easily build it by having picking half point if you for example look at D dimensional Q so in in round T you maintain and sort of a set of leaf nodes and each leaf node represents the them a cell has the center as well as the width and variations so you keep track of these quantities and you build these upper bounds for your function refine you make a decision to refine as I said if the variance is small compared to variations in the cell or you evaluate by by saying okay happy noise in observation so let me so this is the sort of this structure the formal structure of the complexity you can show that grows only linearly with dimension because when I break my cell I only have to break it along the longest longest side of it so you're basically breaking one dimension at a time so and then under assumptions model technical conditions on the kernel and and this would be you know sort of you can sort of think about it that upper bound on smoothness of this function you can get high probability bounds on the cumulative as well as simple regret and the the results are basically minimum of two bounds one is the informational bound that is reminiscent of the prior works analysis and another one which is a geometric bound now what is interesting about this bound is that the complexity I'm sorry the regret is going to be a random variable with high probabilities that it actually grows according to a random variable we call it the dimensionality near the optimal point so it's it's a random variable as a function of F if F has a small structure D tilde will be small if F does not have a nice structure it potentially could be as large as D so so basically that's what I was saying it gives you an opportunistic bound if the function is nice you're gonna have a better regret if the function is not nice then you're out of luck you just have to accept the second and for all realization this random variable is founded by the ambient dimension D so for mature kernel as I was telling you this is gonna give you a strict improvement so between these two bounds the informational bound and the demand and the geometric our dimensional bounds and this this guy wins and this is where you can see this results so for Mature kernels with this parameter new and as I said they're useful ones is one thirty one half three halves and five halves you can see that with high probability you will get that you're bound in high probability is better for pretty much all dimensions larger than nu minus 1 this is dimensions greater than 2 you will get strictly better bounds so we also get we can also go back and do the analysis without the noise and there you can redo their evaluations again you get better performance compared to to a known algorithm in the literature which is called Bayesian multi scale optimization so this is the only algorithm in prior work that had this adaptive structure so the idea and it only works with the noiseless observation hence why I specialized our theorem for this noiseless study and it works with a similar partition tree but instead of using the variations that we had to account for the valuation it just works directly with the upper confidence interval so you can sort of think about it doesn't have that VH term that gives you the variational bound and and so we can compare our algorithm directly with this algorithm in terms of complexity it's comparable to our algorithm it grows only in linearly with dimension so you can actually apply it to similar problems and in terms of regret analysis it's very limited that's not a problem of algorithms you can apply the algorithm anywhere you want it's a very powerful algorithm in terms of regrets analysis and the regret analysis is very restrictive on which kernels they can give bound and it excludes the whole family of maternes a whole subsets of family of mature instructions op them and for simple regrets we can show that our our bounds are strictly better than basing multi scale optimization or bouncing so let me stop here and see if there is any question and then if there's not I'll just I'll just proceed with with the second part of this okay so now there's a lot of critique of the work I just described in the sense that I have this unknown function and instead of optimizing this unknown function what I did is I put a prior a Gaussian prior on the assumption and then said I know all the parameters of this Gaussian process right and I said oh if I don't know much I make the variance wider the difficult trick of Bayesian were and a lot of people complained that this is actually a very very bad idea you don't have a model for your function but you assume you have a probabilistic model where the function comes back so what I'm gonna talk about next is an agnostic studying where again remember if the function we don't have a prior on it and it's useless like we did the problem will be opposed there is nothing I could do except for exhaustive search so I need some assumption this in the agnostic studying the assumption is that f is in a reproducing kernel Hilbert space and with that is bounded in terms of its norm and the observation or the observation noise is sub Gaussian so I have made the the assumptions slightly less probabilistic except for you know an upper bound. So what does it mean for F to sit in this reproducing kernel Hilbert space is that I am gonna assume that my function can be represented by a linear sum of kernels some kernel functions that are known to me and this you know this is not to complex in the sense that the norm is small so I don't have too many C eyes in this representation so this kernel methods is very reminiscent of how we think about you know in in signal processing or statistical learning of functions so you you basically imagine the function can be well represented by some samples if you knew where to sample and what is the kernel that gives you a good reconstruction right so and so this this set up said I don't I don't have a probabilistic interpretation but I have a prior knowledge that the function itself has some nice structure in terms of it can be represented as a linear sum of kernels now you would say are you now you shoved all the complexity in knowing this kernel the nice thing about this view is that you can take some samples estimate a good kernel and then use that estimate to go ahead with your so you you can imagine a data-driven version of the of the solution and in fact when I show you the empirical result that's exactly what we do we we assume we don't know the kernel we take some samples within a class of family of kernels for example let's say Exponential's whatnot we estimate what is the good kernel to explain the data so far so you take some you know sort of uniform completely arbitrary samples you make an estimate of the kernel and then from that point on you use this so it's actually much more prone to to realistic implementation for real so now what is the what is very nice is that and you even though this problem has no probabilistic structure if you try to find the best function that that satisfies you know basically so what what your end up you're gonna end up doing is you have a bunch of XY's and you're gonna get the closest function in the space that approximates X&Y so you're actually doing a you know a sort of an estimation of this function now if you when you are trying to optimize and look for this function your measure of error is quadratic or or square error then the posterior mean of a Gaussian process with zero mean and the same kernel K is the same as the solution to the ridge regression estimator of that so now this is really telling me that even though this problem does not have a probabilistic structure the two solutions are identical to each other so I can just use the previous algorithms any algorithm that I have for Bayesian optimization to get a solution for this setup decide not except up where F is just lies in the space of linear sums of the kernels. Now the implication is very simple and this is known this is not our work in fact that there is a really nice paper that first exposed this base in optimization and in there they immediately saw this connection so basically if you come up with a solution for the Bayesian optimization and in fact at the time the you know one of the most popular one is Gaussian process with you see with the upper confidence found you see B you can just modify it and apply it to the studying by through this connection between the and regression estimator and the posterior mean and if you do that you get a similar regret analysis and can show that the resulting regret found is of the form square root of n now gamma n is outside we you can use the same kind of analysis because of this connection between the two two versions of the problem and get a a modified version of our algorithm so you can take the same algorithm we got and now apply it in the setting that is the agnostic study so the mod the modification grows is this you take the same setup of these piecewise constant functions over these cells that are basically adaptively refined you compute the you see be using the GP surrogate right so the basically you can sort of think about that I have this GP surrogate that is equivalent if I look at the posterior mean and the variance of it it's gonna give me the the Ridge regression solution now instead of the variations that we were using we're using the Gaussian process and the variability of the Gaussian process within a region to add to our function the variability now I don't have the Gaussian process assumption so I can't use my result but I can just use that upper bounding from the geometry of these functions in particular since F can be represented as a sum of my kernels all I have to do is upper bounded loosely with the distance of these functions so in other words and in these kernels if you know if the kernel has any property like a holder holder continuty or whatnot you get this for free so here when I write this I'm assuming this is the typical kernels when they're smooth okay so that's like immediately gives us an algorithm and but then we are now using the smoothness of the kernel the immediate question is why not use the smoothness of that so immediately we ask ourselves can I make an assumption that not only f is actually can be represented in terms of the sum of the kernels that are smooth and you know whether bounded Norm and such can actually make directly assumption on the smoothness of that so and if I do what do I gain from doing that so the simplest assumption is we said what if F also lies in some holder space so I actually, that means that the case derivatives are also hold are continuous with some exponents, if you do this what you can do is let me go to this figure if you know that the function itself is smooth and its derivatives are smooth now I can sample more than one point I can sample sort of in a distributed location and use the samples to locally ask them to make this function with my favorite polynomial estimator right so the more points I have the better effect of a polynomial estimator, I can make and if the smoother this function is this estimates are better and better so immediately this will give us a variant of the algorithm where instead of working with a constant upper bound coming from this function I can try to approximate the function in the star because of the smoothness properties and then build the upper bound based on that approximation so instead of this loose upper bound that I had originally we're gonna improve on an upper bound by first approximating the function then providing you know an upper bound on the function the simplest is if you don't have of much smoothness properties you just can take one sample and approximate it with a piecewise constant function and then use that to build an upper bound using that variations. So these local polynomials now give us a lot of a lot of nice structure that can tighten this bound quite a bit Is there a question? Okay, so um so they're the algorithm so we define we refer to this modified algorithm. *Maryam*I do see a question let me actually, Cameron Harris did you have a question? Go ahead you can unmute yourself. *Cameron Harris* Yeah, I'm just confused do we know the kernel in this setting? *Tara* Yes, yes the theoretical setting you assume you know the kernel again when I will show you the empirical result we will not that's what I was bragging about it that even though in the theoretical standing is not that different from the Bayesian setting the nice thing about the kernel methods is and you can estimate that. *Cemeron* Thanks yeah! *Tara* So with this polynomial estimators now you can get these tiny little bounds and you can improve on the results again I'm not gonna give the full results and just there's a lot of equations but instead try to give you a summary what is the benefit of working with this local polynomial again remembered I have the three partitions and then I have these local estimators so you can actually oh actually the interesting thing is we first started looking at this problem by saying okay F is in the kernel space and it's smooth it turns out for mature kernels you can prove that the mature kernels did the function do have smoothness properties and you can actually compute what they're you know holder parameters K and alpha are so if you do that then then basically the second assumption is not really an assumption it just follows from your original assumptions and basically you can show that this LPG PUC B algorithm achieves the tighter bound both on the simple regret and cumulative regret the more interesting fact is that there is a lower bound on on regret that can be obtained from an information theoretic argument this is due to Johnson Scarlett and his co-authors we're for the fan there's a huge gap between all other prior work in terms of regret both simple and cumulative and this lower bound and for us for a range of matern kernels than the ranges when new is less than D time T plus 1 you know you can say like you know if you put any reasonable dimension D this is really new less than 1 for these arranges maternal the the bound is tight so basically our regret bounds matches the lower bound up to a poly poly logarithmic term so in fact closes this gap that that was known in the literature another aspect that is useful is beside this mature kernels that have practical use practical interest by by the users we can actually also get for for the first time explicitly regret bounds and explicit I mean explicit and for other kernels rational quadratic and gamma exponential terms by the way are bound for this where exponential or maturing for new fill half the bounds matches the existing work in the literature which is the GPU see ok are there any questions on that before I sort of look at that is sort of more try to apply agency I am at 15 minutes left right Maryam? *Maryam* Yeah 15 Minutes *Tara* So I  really like by the way this coat because every time we take this theoretical idea that we learned quite a lot about how much we don't know how to scale them to practice but I'm gonna first show you the simplest example this is I call it empirical results but empirical only in the following sense so we have a synthetic function we know the function the function is basically a brannon function we combine them from various dimensions so even though this is the the ambient dimension is 8 it's on at the point on our H we actually play with it and create a function that effectively only two variables are really two dimensions are really important in your optimization so this is where we expect to for our than we would expect our algorithm to be good at finding this structure and basically zoom in on the Maximizer faster than other algorithms what I'm showing you here so this is just the sanity check of course we the algorithm doesn't know as we know have to be constructed to see how the performance behaves what I'm sure what I'm showing you in terms of performance I'm comparing the performance of this algorithm when just four just so that visually is clear with expected improvement and probability of improvement algorithms and a improved version of GPU CB which is this Purple Line GPU CB is known to be far more exploratory than expected improvement than expected sorry and probability improvements so this is this sort of not not surprising that it does worse than those now the two top brass are our algorithm so the the PS frag didn't work I'm sorry the name of this house should have been more informative so the red curve the performance of sorry the blue curve is the performance of our algorithm the the theoretical that I showed you its complexity is very high when I was doing my local estimation of the polynomials I ended up losing that benefit of linear demand growing linearly with the dimensions because if I want to fit a good polynomial in any dimensions I need to sample as a function of that dimension so I need my sample complexity to grow exponentially with the dimension the red is our heuristic is a heuristic algorithm we came up linearly grows with dimension competition is cheaper in terms of performance is not too far off on this algorithm if anybody is interested I can I can discuss that what the heuristic looks like this is just synthetic data this is what when we applied it to the first semi practical algorithm Here I am setting hyper parameters of a really simple neural network of to two convolutional layers and two fully connected layers and the hyper parameters that we are searching over are the back side learning rate kernel size for each of the convolution layers and hidden layers so this is it I think I believe it's at eight one two five dimensional optimization to make a problem harder this is the performance of that of the final neural net on a test set which is a 10,000 sample of the amnestied I said and this is well known to be too easy so we're actually shrinking the training quite a bit to make the problem harder so that the performance of the algorithms are not all so good and and what you are seeing is the performance of our algorithm with with with the here this is where we actually so this is where we really try to take this algorithm and apply it to a real study this is the collaboration with finals push on parse group we were looking at practical compression techniques that people do in accelerating a DNN on it on a specific hardware and the compression parameters are pruning you can throw away some of your connections in your neural network some percentage of them and the pruning parameter is that percentage you can look at the quantization what bit love with what bit precision your multipliers are gonna multiply numbers you can do an SP composition the parameter of that decomposition so these are the kind of a practical tricks that people do to come up with these contrast neural nets of course if you take any useful neural net out there like moojiji 16 or something this is growing exponentially with the number of layers so it's a really really really gonna test your algorithms performance and in fact the theoretical algorithms that we have had wouldn't work here so what we did we looked at the heuristic and this is the joint work with with finance group is that we took this kernel method but instead of working with these theoretically more complicated kernels that are and are nice to work with we just took a simple Gaussian kernel so now my kernels are Gaussian and we are using the data to estimate the covariance functions of these gaussians but in a local fashion so in that sense it's it's similar to the local polynomials but instead of approximated with a polynomial we approximate big number with Gaussian kernels in in those areas and we did a bad sample version so instead of sequentially pick every single sample that was too expensive we take 50 samples at any given time and run the run the sort of tests on those 50 hyper parameters or compassion values this is the performance I'm just going to show you because one of the things that we learned is that the best competitor to us with this genetic algorithm not basically use the sample sort of using some tricks and these are for or sort of more practical networks that are larger sizes and useful more useful for more complicated tasks and we the nice thing about the the framework we had looked at is that you can consider all compression tricks all in the same basket which is something that the people in the acceleration community had not looked into so in that sense it performs better and I am sorry I'm a little bit over time but almost almost done time to takefurther questions *Maryam* thank you very much we can use the yes there's a very nice talk thanks a lot okay there are any questions that's right so, Blake, please go ahead *Blake Hannaford* Thank you welcome back it's nice to have you visited Tara back only a few slides back like 26 or 25 your example yeah two active dimensions are these dimensions do they have to be aligned with the original eight or could we talk are you talking also about potentially hyperplane of two dimensions that's tilted in the space *Tara* Yeah no you're right I have obsessing over that problem now what is my my dimensions are not lined up here all of our algorithm the notion of dimensionality around an optimal which is what the algorithm does if then if the dimensions have rotated you will have to look at the rotation will fall in a higher dimension but hopefully not as high as the ambient dimension but you're right that potentially a really interesting question is what if I actually have rotational you know features feel like that's where this problem of the art of it is this assumption of writing as a function of the sum of kernels right you can sort of do in in in practice and that's what we did for the actual for this setup what we did it because exactly what you're saying what we try to do is to say if you have dimensions that are related to each other you would like to move along those dimensions and the Gaussian kernel allows us to compare the art using features that are elongated around some of the dimensions and maybe more compressed among others to create these linear dependencies but the design of that feature space is all that the art of it here we were late Gaussian I can generate also the linear dependencies from that data *Blake* Thank you *Tara* Sure, I'm okay are there any other questions I don't see I don't see raised hands let me see in the chat window oh yeah *Sreeram Kannan* Great Talk, my one question is you mentioned that in the kernel setting you can post facto estimate what are good kernels I guess I have two questions one is can you deal with kernels of kind of varying weights you know when you're in your setting? *Tara* Thats a great question, so the idea behind these local polynomials was to localize and and we are like you can sort of think about it maybe locally you will have some structure that is richer than other parts and you can just sort of change that local parameter and we haven't played with it but it what you're saying is absolutely the right way to think about it I think and this is what actually Maryam was asking me the connection to optimizations so if you take our algorithm just say let's say I would do a polynomial of degree one so I'm thinking about fitting planes to a locality now when I decide to break my area or expand myself what I inherit is the slope of this plane is going to give me the bounds in those regions so there's some nice connection should be there in the gradient estimate which we always bothering me because in reality when you do empirical often even when you do first-order optimization in reality you're gonna take some samples around your point and estimate your gradient so I was expecting our algorithm in some settings to do a mimic of estimating gradient and this is our local polynomial is allowing me to do now this is just like me you know my my students very precise person [inaudible] what I have asked him to look at scenarios when we know the gradients are helpful and see in those scenarios if our algorithm competes the other algorithms by a greater margin because of this notion of gradient and then the question is where is the limit because you can take now the polynomial can be a new turn like when you go to quadratic or like higher and higher so you can do these estimates better I think a good algorithm should try to figure out how this is a good setting for me to do cranium estimates and then just do a higher order movement along along the line but this is just really me conjecturing *Sreeram* No it's fascinating stuff I have one follow-up question the the thing is on the on the second part where the kernel you said you can estimate it from data but also even in the Bayesian part people do empirical base estimating it from there yes what kind of conceptual value *Tara* You know I am an information theorist I have no problem with Bayesian's view of the world and it teaches me a lot but if you want to be but I have also seen people who bash like - this estimating of the parameters of Bayesian as meta learning on a Bayesian learning in Bayesian so yes I agree with you you could do it and there are practitioners who do it very successfully philosophically the defendable I don't know. As a person who is often a Bayesianist, I am sympathetic to what you are saying but it's a big no-no in some philosophical terms. I don't know if you know the quote that I put up there epistemic ignorant cannot be approximated by a probability distribution it's actually these papers are really fun to read if you have any time this john north and others *Sreeram* Sounds like a great point *Maryam* Okay we have a few Thank You messages posted on chat [inaudible] is thanking you Krishna and there's one quote saying that the talk was very captivating so with that let's end the Colloquium and let's thank Tara again, Thank you very much! 