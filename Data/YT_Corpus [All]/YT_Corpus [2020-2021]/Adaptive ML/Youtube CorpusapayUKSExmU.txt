 you [Music] [Laughter] this afternoon I talked to you about synapse and this is joint work with my fellow students Jonathan Gordon James Requena and my PhD supervisors Sebastian doe is in and rich turnip so first a little bit of background on multi task view shot-- learning which is what this is all about humans are able to to learn new things and new concepts with just a few examples and machines should be able to do the same thing and our goal here is to quickly adapt and learn to make predictions from a small number of training examples or shots at test time and the strategy here is we're in a leveraged a large number of tasks and learn from those tasks in order to generalize to some new unseen tasks and some scenarios actually this overlaps with what Andrew was talking about that might come in handy is to do 3d reconstruction from a small number of views and you've probably used very geometry based approaches and these days you would use machine learning approaches to do something very similar so given some views learn the latent space that can generate these 3d models similarly you might want to have a recommendation algorithm or personalize some services you've got lots and lots of users but you don't have much signal from each user perhaps you have a few likes or a few ratings but you still want to make good recommendations based on that data oops now I've really broken it there we go same idea is if you wanted to do face recognition within groups maybe you've got lots of different groups and different number of people in the groups in different number of photos each person but you still want to do good face recognition so first a little bit of math and then we'll move on to some pictures and this is all work based on a so called conditional neural process and a conditional neural process is really just a predictive distribution of the form here where you want to predict some value Y star some output based on some input X star and some global parameters theta and the sort of context or the training data for the task detail and so just going through that tau it's the task this detail is the so-called context or the training data for the task the starred X's and Y's are the target or the things you want to make predictions for theta the global parameters that are shared across tasks and the side tau R parameters learnfree specific tasks so that gets all the notation out of the way and the idea is this synapse system specializes conditional neural processes to multi task classification and in particular one main difference is that the shared parameters the goal promise are trained offline so they're sort of pre trained and learned and fixed and what this does is it captures two high-level features of the datasets facilitating multi task and transfer learning the other difference is that the task specific parameters saw how our parameters of the model itself as opposed to some fixed dimensional inputs the model as they are in conditional neural processes and this gives us a lot more flexibility and support for a really wide range of distribution okay so now away from symbols into two pictures so this is a standard image recognition system you put some kind of image in like a cap and it goes through a feature extractor perhaps some convolutional layers or residual layers and then there's a linear classification stage and out you get a label cat hopefully now in multi task learning at test time we want a few changes we we want to support brand new tasks that are have maybe a different number of classes in entirely different classes so instead of cats and dogs and birds maybe you're asked to recognize characters or plants or something like that and so this implies that we have to change the linear classifier for each new task so we have to learn a new set of weights because there may be a different number of weights and and you get a need a different set of weights and so the leader stage has to adapt similarly we might want to recognize or interpret very diverse data everything has to say from handwritten characters to to natural images and we sort of want to be robust to very small number of training examples so we choose also to parameterize the feature extractor and adapt it as well so the big question of what we'll talk about going forward is how you do this quickly and economically and a key ingredient to this of how will parameterize the feature extraction stage is the so-called film layer and film is an acronym for future wise linear modulation and the idea here is that a film layer scales and shifts the output of a convolutional stage feature map so at the output of some convolutional layer might be some number of future maps say 64 or 128 of these maps and for each one of those maps there's an affine transform that that weights it and shifts it and so simple affine transformation on every map and so if there's 64 layers you're gonna have 64 pairs of these parameters and the idea is you add these film layers into convolutional network blocks here it shows a ResNet block that might be repeated many times in a mini layer ResNet and you stick in these little adapters and it's maybe not a great analogy but these are almost like eyeglasses little corrective lenses and that that fit in and adapt the learned set of features to to some other space and the idea is we will use these parameterizations and as is a very very expressive these sort of little adapters can can make adapt from you know very very very diverse data sets and they only add a very small number of parameters so our entire system these extra variables only add 1% of the number of weights so how do we do this well here's our feature extractor again and we first pre train it offline on something like image and add a big database and we learn the parameters data and fix them and then we add these little film layer parameters little adapters in there and we learn the parameters for that based on another network a hyper network that we call the feature extractor adaptation network and it takes in the training tasks or the training examples for each task and the idea is the system this doctor' network that learns how to generate the parameters is trained on many many tasks from many many data sets similarly we can adapt the cost of fire as well so again we take in the training examples for the class but this time we it's all cost conditional we take all the examples from each class and process them separately so if there's five different classes of things cats and dogs and other types of stuff we take all the examples that are cats all the examples that are dogs and and sort of split them and process them one at time and in particular we pool the features we average the features from each class and then we use another network parametrized with V sub W which generates the weights and P sub B which generates the biases and because we can do this quite efficiently because it really there's only one column of the weight matrix and one element of the bias matrix that will affect the logit in the softmax for that class so we can do it bit by bit so when the first class comes through caps or whatever it generates one column of weights that would affect the cat logit in the in the softmax similarly we can generate all the weights for all the classes and once we have all the weights for all the classes we can pass features through and and generate a proper label so here as I say we've got two mechanisms for parameterizing the whole system with with quite economically and not a lot of extra parameters so altogether this is how the system works you take a training task you pass it through this feature extractor adaptation Network you generate the film parameters now you can pass your input through the classifier because it's fully parameterize then we take the output of the feature extractor and combine it also with the training examples and it feeds into the classification adaptation Network and then you can generate the weights and biases and then you can classify so that's the processing flow and we tested this out on a very very challenging data set it's actually called metadata set which is appropriate I guess and it's now sort of the most difficult multi task view shot-- learning benchmark its consists of ten different data sets aided them training and two of them entirely hauled out so a test on you don't even see the held out data sets and the way they have done the splits is that the the training classes are all different from the test classes so generalization is very important to be able to do well here and the procedure for sampling each toss during training is we just choose a data set at random then you choose a number of classes from those data sets somewhere between 5 and 50 from the data set and then you choose a number random number of training examples or shots for each class and obviously you need at least one example per class and there's a maximum 500 across all the classes so if your random number of classes would say 20 and then the random number of shots in total would say 200 you'd have on average 10 examples per class to learn from but it could be less begin because the the numbers are assigned randomly to class some of the data sets have hierarchy and and then choosing the classes that hierarchy is respected and it takes about a hundred thousand tasks to train these networks here is the results on this benchmark and of course because it's method and we put in the paper the synapse right hand column is bold and best in pretty much all the categories and which is actually quite an accomplishment because the the leading technique this so called proto mammal which is a fusion of prototypical networks and modely agnostic meta learning was was was pretty strong benchmark but this pretty much beats it in in all the all the categories often quite comfortably but not only is it more accurate in terms of classification accuracy or better in terms of classification accuracy it's it's faster so given enough time and enough data you could just learn all these system parameters with gradient descent and so you know as I say time and data were not a problem you could just do that but the idea here is we want to be fast and we want it to work with a low number of examples and in these plots the blue dots are synapse our system and the red dots are stochastic gradient descent and the horizontal axis is time and the vertical axis is accuracy so you probably notice all the blue dots to the left means that's faster that's good and you know further up is is means higher accuracy and and so in general our system is at least five times faster at test on because instead of having to do a backward and forward pass that you need to do in gradient descent you just need one forward pass through the network also the accuracy at lower number of training example I glow or shot is generally higher like if you look at the eminence data set there at one shot where somewhere at 70 some-odd percent whereas the gradient descent is somewhere 40 some-odd percent and so we're sort of better in lower number of examples and certainly faster and one of the reasons why it would be better more accurate is say gradient descent with a few number of examples can overfit quite badly or is work quite resilient to that because the network has shared parameters learned over many many tasks the other sort of a surprising thing that we did is we used the same system for continual learning sometimes called lifelong learning and involves learning a sequence of tasks over time without forgetting the earlier ones and we use exactly the same model we did not train this for continual learning specifically we use the exactly the same model that we used for future classification and the only tweak that we made at test time was we put in a bit of memory we learned the the feature representations going into the adaptation networks and did a running average of them between the sequential tasks and a results here oh I keep hitting the wrong button there here we go are also excellent there's two benchmarks so called split em missed where you might know em this has it's a digit recognition thing zero to ten and we try and learn it in stages so you learn zeros and ones and then twos and threes and then fours and fives etc and you do these tasks sequentially and try not to forget same thing there's a same deal with split CFR is that you sort of do chunks of ten classes at a time in in sequence in all cases we're really really close to the best and a lot of these are very very strong base lines this Romanian Walker they are walk thing is is state of the art numbers on these tasks and we're either very very very close to what they do or in the case of CFR 100 better and this is not being trained on continual learning never being exposed to these data sets during training and observing orders of magnitude fewer samples at test time and that's it off to pizza [Applause] [Music] Thanks 