 A few preliminary comments. So one of the things that a man named Mike Besech [phonetic], biomedical informatics has taught me in a very poignant talk was that in order to make an impact in health-related research, we really need to come all the way back to the patient. And I think it's really wonderful that we're having this conversation about genomic data sharing, personal control, et cetera. But one of the things that I really wanted to make sure that we got to was to begin to open the discussion about how we get this back to the patient. How we get this back to the point of care so we can impact treatment with the most immediate and impactful, in the most immediate and impactful way possible. So this is going to be a little bit of an esoteric kind of set of talks, but I want us all to think about how each of these speakers is going to inform how we as a medical system, as patients and as clinicians take what we've learned over the last day or so and translate that into clinical practice. Without further ado then, I want to thank Amanda for inviting or suggesting that we invite Jace Ward and his mom, Lisa, to come and join us about their recent story and I'll leave it to Jace to tell the rest of that. [Applause] So hello. My name is Jace Ward. I'm a 20 year old kid from Kansas and I'm about as normal as they come. I'm the youngest in a very healthy family. My mom and dad, they both have good jobs, very good health insurance and I took all of that for granted until about May. In May after a car accident I was able to walk away from, I noticed some double vision in my left eye. I wasn't sure what it was. I thought it was just maybe something from the accident. And so I didn't think twice about it for a few months until finally I just decided to get it checked out. I went in and shortly after followed a CT scan and a MRI and I was told I had a mass in my brain. We immediately went to MD Anderson where I was told I could get the best imaging and talk to the best doctors about what I could do moving forward and what I was going to, what my life would look like, I guess. So after MD Anderson, we reached out to tumor boards with my images and all of them came back with telling me I need a biopsy. And I didn't even know what that meant. They told me, they said a biopsy would tell me what I have and at the time I didn't even know I had DIPG. And so I said yeah, I guess that looks like what I am going to do. I'm not too scared of it, let's just get it going. So let's refresh. I am normal as of May 16th. May 17th my entire life gets uplifted and sent to Houston, Texas. And then or then June 5th I'm sitting in a waiting room getting ready for a biopsy. And as a 20 year old with only the advice of my mom to kind of get me through, it's incredibly scary. I had to, when I was in the waiting room before my biopsy, I had about ten minutes to decide whether or not I was going to take additional samples from my tumor. And the way they explained that was the doctor doesn't feel like taking more than 6 samples is even safe for me and said there would be a lot more side effects if he did that. He's going to take at least four and I've got to decide if the extra two are worth the quoted 35,000 for genome sequencing that only works about 30 percent of the time. That's the reality of what you're told as a patient and you only have about ten minutes to make these decisions. Don't get me wrong, the doctor made a patient-centered decision not to encourage me to risk more samples without the likelihood of using the info. But the point is, that the decision was made in 10 minutes before, the decision was made in 10 minutes and it was about 15 minutes before I went in for a brain biopsy. It's not the best venue of making these decisions. We need to do better on allowing people to have time and understand what they are deciding. So now we fast forward to June 24th. I get a random email quoting that I have an update on My Chart. And My Chart is basically the way that as a patient I'm able to talk to my doctor and get updates. So I get an email and I opened it and it happened to be my genomic sequence or happened to be my genomic data. It had all of my mutations and I didn't have the slightest clue what I was looking at. After a few hours of Google research and my mom talking to her Facebook friends, I shortly realized that I had what my worst nightmare, had DIPG and I basically had to face that alone. I was scared. Finally two days later we were able to talk to our doctor from UCSF and they basically said that my options are radiation, chemotherapy and then to look at clinicaltrials.gov for future, see where I want to go, what looks good to me and basically didn't use my mutations on helping me decide that at all. So at that point I'm sitting there, 20 years old with a hole in the head wondering why even biopsy it? Why is it important for me to have this genomic data? Why do I, why would I want to do that? During my biopsy process, I had to relearn how to walk and relearn how to use my right arm. Like that was not worth going through if I'm just going to go through chemo and radiation just like if I didn't biopsy. So what happened... Let me catch myself back up - sorry. So when I was sitting there thinking through those questions, that's when I really felt alone. That's when I felt like there wasn't much help for a kid in my situation. My parents were looking at every end of the earth trying to figure out what might save their child. But the answer is, without understanding what my genomic data is, there is really not much of an option besides just go what looks best. And so luckily we were able to speak to a doctor who helped us understand what our genomic data was and understand why we got our biopsy. And understand that there are certain clinical trials that would work better than others. When it comes to DIPG, there are about 300 to 500 a year, but that's 300 to 500 new patients every single year. The clinicians aren't really on the top of what's going on in all of these clinical trials. They don't understand what other clinical trials are doing because they're not reported quick enough. And so we called the primary investigators for a few different clinical trials knowing that we may not get the success that we were looking for. But luckily, this is a tight knit DIPG community and luckily every single one of those investigators were able to stay up throughout all hours of the night and call and email on their spare time just to be able to talk to us and try to find us an answer. So finally after finding, after calling 15 to 20 of the top doctors, we had the idea of what our marker would look like after radiation and chemo. For some, a biopsy at their hospital was required. For others, radiation had to start 30 days after diagnosis. So we barely made that. You know, we barely understood that if I didn't start radiation within 30 days of May 17th, then I wouldn't be qualified for the clinical trial that I'm on now and I wouldn't be able to do the things that I'm able to do. I probably wouldn't look as good as I do as right now. So even with the best facilities that UCSF and MD Anderson and a few others, it's very tough on the patient to make these decisions without somebody kind of guiding us. These oncologist know what I'm getting, like they understand my disease, but they don't understand how to help yet and that's where we need to focus our efforts. So I had to make sure my information was shared between different hospitals and different oncologists. And to do that, I would have to literally go pick up a disc that would have my imaging and my mutations and everything on it and I would have to either send it in the mail or physically take it to my next location. They wouldn't share it for us. They wouldn't, they would just say you either have to come here and pick it up and send it in the mail yourself, or you have to come here, pick it up and drive it yourself. So luckily we were able to mail my data all the way to Europe and to also a few different doctors to help us kind of figure out what it would look like after I got done with radiation. Because at the time, I had no clue and I was very scared and very sick. But our research was through talking to these doctors in their spare time, and then talking to moms on Facebook. You know, that's where most of our, that's where most of the DIPG community can speak to each other simply because there's not a good way for us to find other DIPG patients and find other people who kind of know the situation other than Facebook talking to parents of children who have died. And so parents were very good to us. They send their UCSF 500s, they sent their genomic data to us just so we could match up and trying to figure out if there's any similarities. If I should try what they tried. If what they tried didn't work at all, and so I can't do that. It was - just to say, I would never do a research project researching on Facebook, but now I'm going to choose the decisions that will ultimately save me or kill me through what these parents are telling me on Facebook. And so, I mean you can imagine the stress of that is just unbearable. So I guess one point I really want to drive home is why do parents trust Facebook? Why would I make such a point to talk to all these parents if I'm just going to be learning it through Facebook, like social media and nothing that they say can really be backed up or proven. It's because DIPG won't wait. It is a very short... well, it's a very aggressive tumor and about five months ago I was given nine months to live. DIPG won't wait to take my mobility. It won't wait to take my voice. It won't wait to take my sight and it won't wait to take my life. So I don't care that twenty researchers have my data. I want a treatment. You know, if I try one and it disqualifies me from another, that's okay, at least I'm able to make decisions. At least I'm able to make some decisions rather than sitting here not doing anything and just chemo and radiation and then whatever looks good. I'm able to make a knowledgeable decision on moving forward that I can at least try to trust. Within one to two weeks of diagnosis, I have to take all of the decisions that really matter in my course after DIPG, I have to decide if I'm going to biopsy, if I'm going to do radiation and chemotherapy and when I'm going to do that. And I've got to decide what kind of clinical trial I'm going to be aiming for after radiation. And then I just have to pray that my labs come back with good. I mean, if I have a low blood count, than I won't be good for any type of clinical trial. So I have to pray that I do okay and luckily, I did. I just want to make sure that we all understand. I get risk. I work at a law firm. I understand you make exceptions to what I was signing as a first consent. I understand that not everybody is going to be using my information to try to cure me. But the way I see it is it's like a financial transaction. In the olden days we all used a bank and whenever we needed to make a purchase, we would go to the bank. We'd take our money out and we'd go make a purchase. But, I mean, since I've been alive we've always had debit cards which you take them around, they share your financial data at every single place you swipe them. And they are risky. But I think we assume that risk when we understand the convenience of being able to swipe that card and not have to carry cash everywhere. And so in my personal situation, I wish I could swipe my genomic data everywhere. It doesn't matter to me if China is going to use it to, or use it against me or discriminate against me in some way. I really don't care as long as somebody can use it to find a cure, either for me or for someone shortly after me. Because while we're sitting here deciding whether or not it's okay for people to share their own genomic data, 300 kids are dying every single year, 300 to 500 and they're going without a voice. So I guess I want to end my speech off with a few points that I really want to drive home. And the point, the main one is you know, 300 kids a year lose 70 years of life, on average, because we don't incentivize and monetize sharing our research with each other in real time. Two, we require too many kids in a trial to move forward into further phases of the trial. There's not enough kids that get DIPG to get the thousand kids needed in order for a trial to move forward. And three, our clinicians don't know where to send our patients. When a clinician receives a patient with a rare disease, they try to help them out, but like they really don't know what they're doing. And so Facebook is the only place we have to go. So we really have to do better for this group. Government funding must build a better comprehensive data depository that one, research can access and two, patients can access differently, but they'd still be able to access it. As the owner of my data, I can determine my risk and I can navigate the database and I can choose who gets my data. And finally, on my death I get to pick who authors further my information and my data. For me, we don't have time to work through this or for me, we don't have time to work through the risk. For me and my - sorry, I cannot read my own handwriting - for me, we don't have time to work through the multitude of risk. For me, I'm in the bypass lane and I need a fast track to cures. So to make it work better, we need clinicians to be trained and accurate, but empathetic delivery and we need data to be shared freely with some restrictions. Thank you. [Applause] There is nothing more meaningful than what we just heard. But I now want to introduce Andy Faucett who is a genetic counselor. I believe 32 years - yeah, so a while. And he's going to be talking to us about how a healthcare system thinks about genomic data and some of the challenges they face, they've faced, in making patient data available. Am I on? Okay. Okay, I will agree that's a tough act to follow, but I congratulate him for the effort that he's making on how we move the research forward and how we move treatments forward. So thank you. Okay, this is my information. It'll be in the slides. If you have questions later, feel free to reach out. If I can do something to help you or your organization, feel free to reach out. I don't have any conflicts. I do have some NIH funding. We also, the exome sequencing in our research project is funded by Regeneron Genetics. What I hope to do today are a couple of things. A kind of a mix of a talk because I want to talk about Geisinger. I also want to talk about some of the projects that I'm working on and then leave you with some personal thoughts about how we might move forward. Okay, just to orient folks, this is a map of Geisinger. We're a healthcare system that originally started in central Pennsylvania, so kind of the middle part. But we've progressed up into the Scranton area, down into the Harrisburg, the state capital of Pennsylvania area. And then we also are over in Atlantic City, New Jersey. So classically we've been described as a rural healthcare system, but that's an expanding description because Scranton is a fairly large middle sized city. Harrisburg is your typical state capital that has a lot of diversity and Atlantic City also has a lot of diversity. So we're rural plus, I would say, now. The first part I want to talk to you about is that Geisinger was one of the first institutions to integrate Epic almost 24 years ago, and we haven't changed systems. If you talk to a lot of healthcare systems, they've gone through a couple of EHR vendors. So they've lost data because it never transfers from one vendor to the next. But one of the parts of that was we always wanted to make this information as open and accessible to patients as possible, as easy for them to retrieve as possible. So this is one of the first studies that Geisinger did. We were involved with several other institutions around the country in something called OpenNotes. What OpenNotes means is you as a patient are able to see your doctor's notes, your surgery notes, your radiology reports, your images. Everything that a healthcare provider has access to, the goal was to make that available to the patient in the system. This is a national effort. This just shows you the slide from 2018 of where the institutions are that are doing this. Now it's a little deceiving because there's some in every state. But if you look at the graphic, the medium purple is two to three institutions in that state. The dark purple is four to five. The next is six to ten and then the green is over ten. So really the only state that's got huge involvement is Washington State. Pennsylvania is definitely in the darker; most of that is Geisinger. But again, the idea that patients would have access. Interestingly, there was some pushback from physicians initially. What we found though is it resulted in much better on charting. They're not embarrassed if another physician has trouble reading their note, but they are embarrassed if their patient doesn't understand it. There's also you'll find in electronic medical records, there's a lot of cutting and pasting and that cutting and pasting can lead to a lot of errors. So, patients actually identify things in their record that are incorrect, so their record gets a little bit better. It also primes them for their discussion on their next visit. Doctor, I thought you said three times a day. He says no, every three hours, or other things that were not understood very well. Now, step into this and you have Dr. Glenn Steele who is a former CEO of Geisinger, who realized that Geisinger was a perfect place to do genomic discovery and genomic research. Why did he feel it's a perfect place? Because that original central Pennsylvania is almost what you can call a genetic isolate. People don't move. It's an agricultural based economy so they tend to have very large families. There's some families I've worked with that they have five generations living. And that, also because Geisinger is the primary healthcare system in the region, there's a lot of trust. Because we were one of the first places to integrate Epic, we have that data on the individuals. We have their entire history. So all those things together led Geisinger to create the MyCode Community Health Initiative, which was launched in 2007. Now at that time genetic information was very scary, we were very worried about it. So the consent said that it's very unlikely we'll learn anything that we would ever share with you and if we did we would never put it in your medical record. So that was kind of where we started in 2007. But we had good enrollment. And this is kind of our goal, our vision statement when it comes to genomics at Geisinger. And I'll just read it: "Universal genomic sequencing will become a routine part of health and medicine to improve individual health and well-being while maintaining or reducing the cost of healthcare over the lifespan. To do this we need to demonstrate the clinical utility and value of genomic information at the individual and population level." So as an institution we need to show that this works and that it's valuable, because we have lots of places that we can spend our money. And then we again felt that Geisinger was the perfect place to try to do this. This is the announcement in The New York Times from January of 2014 where we sign the agreement with Regeneron Genetics to rapidly expand the bio-bank, and also to begin doing exome sequencing. Now this is research sequencing so it's not clinical, we can't use it for care. But we'll set up a process to deal with that, and I'll talk about it in a moment. Now my role, I'm just coming into Geisinger at this time, and that consent that said you might not get anything and it won't go in your chart, really wouldn't work. So I held focus groups around the system. What I heard is that people were comfortable with genetic information, they were comfortable even if we didn't understand it, because they knew we wouldn't understand a lot of it. And if it was medically important, they wanted it in their medical records. That's why they came to Geisinger. So we changed their consent. And this picture is with Dr. Dan Davis, the Head of BioEthics. And Dan and I wrote an article about as an institution, because we're primarily a medical institution, people are coming to us for medical care, that maybe our duty is different. That if we have genetic information we have to use that information to help patients where we can. So we can't just sequence the data and put it in there and just use it for research; we have to use that data to also help patients. So we did redo the consent form, and we re-consented folks. We got most of them, not all of them, and I'll show you some numbers in a moment. But if you want to understand our logic and ethical reasoning, this is the article to turn to. Okay, now this is a process for the MyCode genomic screening and counseling. So remember I mentioned that Regeneron was going to help us by funding the part of the biobank and part of the sequencing? They were not interested in being involved in the actual return of results. A lot of legal liability questions, but it wasn't their focus. But Geisinger itself made this commitment that if we had this data we were going to analyze it and return it. What we heard in the focus groups was that the individuals who are participating wanted Geisinger to make those decisions. So the first thing was that we created a group, an expert, to reach consensus on which genes to look at, and what we could share. We decided to only return pathogenic and likely pathogenic. Often in genetics they also return variants of uncertain significance. We do not touch those in this program - a whole two hour lecture if you want that discussion. But there, we wanted to minimize false positives because we were doing population screening. We were not diagnosing individuals who were sick. And that we wanted to make sure that we had all the supports in place to be able to handle this and help the individuals with it. Now this is our short card. These are posted monthly on the website so if you want to see how we're doing and I'll walk you through it very quickly. The top is, now this is of August 1st of this year. We had 251,000 individuals have consented to be in the biobank; 171,000 of them have given us a blood sample or a saliva sample that we could sequence for DNA; 92,000 of them we have sequences available right now. We actually have another 50,000 that we just got a big data dump. Regeneron likes to do the sequencing in large batches; for a quality control in research it's better to do it that way. We have 65,000 people in that first batch that we can return results to. The difference between the 65 and the 92 are individuals that we couldn't re-consent, or individuals who've passed away. So we're not looking at those for actionable results; but the 65,000. And out of those we found 1,068 individuals with medically actionable results that they can do something about today. And we have a process. All of them are kind of headed by one of our genetic counselors that we share those results. We insist, early data, we saw that it was very effective and was working, so we decided let's move it from research to clinic. So we started a population health DNA screening program where we - and this was launched just this year in certain of our internal medicine clinics - a Geisinger patient with Geisinger insurance can come in and have their exome sequence. That is done by a commercial laboratory. It's done by GeneDX. We set it up where it's minimal time by the provider, it's not a long consent process. All the results come back to their primary care physician and then they're tapped into genetics as needed to be able to follow through and again have that support. We did this with funding both from a philanthropic organization and from the healthcare plan with the idea that this again, answer that question: is this ready for prime time, are we ready to put that out? We're already planning on expanding it into additional clinics so it's something that we feel is working, but we don't have the data yet to publish this data out. Now with that program, one of the questions is very relevant to this meeting is can you get access to that data? Well, we're using the commercial vendor GeneDX and the answer is yes. These are three pages from their website. You can send a copy of your exome to any researcher that you want to and you can also send a copy to yourself. These are the forms you have to fill out, send it and they will send it for you. So yes, that exome data is available. Now the other thing that happens that hasn't come up so far is that most healthcare institutions today use electronic medical records and in most of those electronic medical records, genetic data is buried in a way that cannot be used. And what do I mean by that? Most of it is a scanned report from the laboratory, so it's not searchable, it's not findable, it's very hard to work with. It's often even hard to find in the medical record. So as part of the eMERGE project we were challenged to do something about that. So Dr. Nephi Walton and Dr. Mark Williams had been working with Epic to fix that and we've actually done it on a pilot basis. The go live was successful and we actually imported genetic discreet data into just under 1,200 patients from the E-merge. 142 of them were disease risk variants and about 1,200 of them were pharmacogenomic variants. And what this means, and I'm using pharmacogenomics as CYP2, C119. This affects the way that you process certain drugs. So it is in the patient's chart. It's a discreet data field. What that means is that then can fire all those things that electronic medical records are supposed to be useful for. So it can way the doctor that this individual probably shouldn't have a certain medication or they should have at a higher dose or a lower dose and give them alternatives. It doesn't change a patient's care. That's still left up to the physician, but it gives them the warning. And this just shows you what, the screen that pops up. And basically it says a medication alert. Based on the genotype result, this patient is predicted to be an ultra-rapid metabolizer. The patient is at risk for poor response to the medication that you have considered. You will want to do something different. And that's one of the strategies that Geisinger uses in a lot of their alerts is that we make it harder to do the wrong thing, but it's still possible because there might be some reason that physicians still wants to put the patient on this medication, but he or she should have to stop and think about it before they move forward. Okay, this is what it looks like in the patient's chart. Again, all of this information will be available. So this is in patient-friendly language as what does it mean to have this. And this is also important so if they happen to go somewhere else outside the Geisinger system, they'll be able to access this information and understand it. Now I want to switch gears again and talk about how do we help to do this on a national level. And the first is I've been very involved in patient registries over the last 15 years or so. And when the ClinVar Project was created, we wanted to create a way for individuals to get their data into ClinVar. That question came up yesterday. And one way to do that is the Genome Connect registry. What this is anybody who's had a genetic test can sign up. They can go in and fill out the survey, which is kind of a body system survey, and they can upload their genetic report. We'll curate that report and the data will end up in ClinVar. Now this just shows you the Genome Connect participation. Our original thoughts is we that we might get 1,000 or 2,000. We're up to about 2,500. We're lagging a bit on getting the reports, but it's moving forward. Is this making a difference? Well, the first is that we've submitted data to ClinVar several times and the blue part shows you that 43 percent of the variants that we submitted were not in ClinVar. They had never been submitted by a laboratory. Now these tests were all done by laboratories, but it hadn't gotten in the database yet. And in 57 percent of them, we added additional phenotypic information, medical information that wasn't previously in the chart. So again, that's what Genome Connect can help do and anybody who's had a genetic test can sign up. We're also committed to helping other patient registries to do this. Right now as far as we know, they're no other patient registries that are putting their data in ClinVar. So one of the other parts, one of the other arms of Genome Connect is to help other patient registries figure out how to get their data into ClinVar. How to curate it, make sure it's in the right format and then upload it. So as you heard yesterday, the process is not exactly straightforward. Now, I'm going to close with some of my personal thoughts. The first is you go back to that original ethics discussion about why we return results, and that is do medical institutions who are doing research have a different ethical duty than someone who's just an academic researcher? And I think they do. So the patients come to you for healthcare. So I think there is an additional ethical duty for those institutions to support you in getting your data where you want it to go, to support you in participating in research. And I think that's something that they really need to be called, they should do that. Now the next question I want to ask you is how do we make all of this safe? You heard yesterday about the interpretation tools. Well, it's a real problem because most of the news articles that you heard about how bad DTC was, was not the DCC lab, it was the DTC data uploaded into an online converter and that interpretation. So we really need to either fix these tools or close them down. Also we need efficient tools to be able to move the data around. It's not very easy to move data. All of the genomic data of Geisinger in the cloud, it's not so easy to download it and move it. And then the question of who bears the cost. It costs, it would cost us about $100 dollars to download someone's data and put it and then the cost of the drive if we wanted to send it somewhere, if we just want to ship it online. But who pays that cost? What we've heard from many of our research participants is we don't want, they don't want the research dollars to be spent doing that. They want research to do what it's in mind for. But again, how do we do that? The cost to store that - you heard yesterday in the Citizens Talk that they had the opportunity to collect genomic data, but they didn't because they didn't have anywhere to put it and they weren't sure they could use it yet. So that's an issue that also needs to be resolved. And then the other question is, is there a way, somewhere to store it where we don't have to move. That's the cost generally. Can we just store it in a central place and then make it accessible. And I'll stop there. [Applause] I gave Andy an impossible task of covering all of that in 15 minutes or less, so he did a great job. Next up we have Casey Green. Casey and I have bumped into each other in lots of different, our ven diagrams seem to overlap a lot. But Casey is one of the most sort of innovative thinkers that I've met with respect to data and how it can be used in very creative ways. So I wanted him to speak to, and I should say, he is probably one of the largest or strongest public data advocates, open data advocates that I know. So I'm going to pass it along to Casey to talk about how data can be used in the clinical setting. Thank you for that. Yeah, so I'm going to share, I'd like to share a quick research vignette of ways that we've been using public data in our group and what having large collections of public data, and in particular those data that are available without a restriction, allows us to do. And I have a lab at U Penn and then I also recently, a couple of years ago, started something with Alex's Lemonade Stand Foundation, which is called the Childhood Cancer Data Lab which is focusing on enhancing the data science ecosystem in the pediatric cancer research community. So first I have disappointing news for all of us who really hoped that machine learning was going to solve all of our problems out of the box. In our experience, unfortunately, machine learning isn't magic. Which means we can't just take these algorithms off the shelf and expect to apply them to every setting and end up with actionable biological insights. I know if you read sort of the popular press, The New YorkTimes sometimes it feels like these are going to be the answers to all of our problems. And in our experience, unfortunately, that isn't the case. And in particular in biology and in particular with rare diseases, that's not the case for one sort of relatively simple reason. It's just that machine learning models need many examples. And what I mean by that is they need lots of things they can learn from. So we're going to think about this box as our data and it's sort of got two dimensions. It's sort of some amount wide and some amount long. And with, for machine learning models what you really want is you want to know a modest number of things. So you could learn hundreds of thousands of things or you could learn 100 relevant things and you'd really rather learn 100 relevant things. And you're going to use those things to hopefully build a model that's predictive. And really you'd like to know those things about as many settings as possible, about as many cases as possible. And so this is sort of the ideal world for machine learning. When you think about how Google and Facebook can build these algorithms that are incredibly accurate at predicting what they see on an image, that's because many of us carry cell phones around and we take a lot of pictures and those pictures end up in databases. So they have very extensive lengthy data sets that have examples of pictures. In biology, we don't usually have that so we have often very few examples. So because we can profile the genome, we can measure many, many things about a person, but we often have very few people with exactly the same condition. And in this case, even in the largest sort of settings in biology, we might have thousands of examples. In rare diseases, and particularly in pediatric cancers, we often have 100 or so examples. So it's a much, much smaller space in terms of the number of people that we have been able to learn from. And so what we've decided to do is to try to get around this by making machine learning models that work for many different biological contexts. So instead of trying to build a model for each individual pediatric cancer, can you build a model that works across many pediatric cancers? And in fact, if we go broader since there's a relatively limited amount of pediatric cancer data that's publically available, can we build a machine learning model from essentially large amounts of public data and apply it to rare diseases? And so this is in sort of more scientific terms. This is a project that Jacqueline Taroni did. She was a post-doc in my lab and now she's the principal data scientist for the Childhood Cancer Data Lab. So her hypothesis was these patterns are directly applicable, even if they're not, they can be learned from large collections of public data and those patterns apply to the rare disease setting. It's just that in rare diseases, those patterns are being used in slightly different ways. So the same patterns are present; just with sort of different levels of expression or sort of used in different ways. The idea is let's learn from a large collection of public data. So we have this major existence of gene by samples, let's learn from a very large collection of that and then we'll end up with a major existence of samples by patterns. We can then take those patterns that we learned from the generic data and apply it to the dataset we really want to know about, which is the rare disease dataset. And so from that level, then we can start to say okay, this pattern is different in individuals with a more severe form of this disease. So what we have here, I know we've talked, I was at an Alex's event yesterday, so I wasn't able to make the symposium. But what I'm gathering from this morning and from conversations with people is that you talked a lot about how participant data gets used. So these are data that are often controlled access. What we're talking about here is very different. So we essentially download the things that are completely public access. So these are lots of cell line experiments or unrestricted data. And we use that data to build a model which we're then going to apply to the sort of more important, the data that is more important to the disease of interest, the targeted data from the rare disease. So what we used for this was data from a resource called recount2 which was built by some researchers at Johns Hopkins and part of Jeff Leek's group. These were 70,000 RNA-seq samples. If you just want to benchmark out what it would have cost to generate this in your own lab, you can say that would cost about $1,000 dollar a sample when you consider the person and time to acquire the sample, run the experiment and then profile it. So that's about a $70 million dollar dataset there that we're learning from. And it's really, like for us it would have been easy as just downloading this from the internet. And then what Jackie did was essentially apply that to a machine learning model called PLIER to create something that we called multi-data set PLIER. So PLIER was initially created to look at individual datasets. And what she's done is she's repurposed it across this large public context for learning from public data. And the just to give you an idea of sort of why we do this. So what she's done here, so she uses this, in this case, to study a rare autoimmune disease called ANCA-associated vasculitis. There was no ANCA- associated vasculitis data in the public compendium. But what we wanted was sort of a dataset we could use potentially to understand how the model works. And so on the right here, Jackie's created a compendium of SLE data. So SLE is a much less rare autoimmune disease. And it's one where we can sort of create a large collection of public data on the order of 1,000 or so samples. And then what she's done as we've looked at recount2 which is sort of just the public data that you would have downloaded from the internet that captures many different biological contexts. And so between these bars what we're looking at is datasets of the same size, but different composition. One is the disease specific data and the other one is data that's sort of this, what has come out of genome-wide biology over the last decade and a half. So these two bars are the same size datasets. And then within recount2, we have this box plot, which is a reduced collection to exactly the same size. But what we also have is the full recount2 datasets. So what if we could learn from all the public data, and that's the diamond. So I'm not going to go in detail into sort of exactly how these plots work because I don't think this is quite the right audience for it. I'm just going to give you the summary. So the first thing is, if you use the entire collection of public data, you learn sort of many more gene expression patterns. So you're getting a fine grained understanding of what is happening in biology when you use more data. The other thing that happens is, if you look at how much of what's coming back you already knew, you end up learning more things that you already knew. So we're getting more of the things we knew we should find back, which is an encouraging result. So essentially we're finding more of the stuff that should have been out there. And then finally, the last part which we think is more important is that actually many more of the things, a large fraction of the things we get back, and in fact a larger fraction than in the sub-sample data, are things we didn't know we should be looking for. So there are these recurrent patterns that show up often in the data, but we didn't sort of know in advance that we would find them. Essentially they don't align to biological pathways that we would have expected to find. So not only do we get more known knowns by using the public data, we also get more of these unknown unknowns. So the idea is now you can take these to a dataset of interest. In this case, Jackie applied it to this ANCA-associated vasculitis study and there's this really nice comparison in the paper if that level of detail is of more interest to you about how this can be used to look across multiple datasets. I just want to note that the paper itself is available, it just came out in CellSystems. It's an open access paper. But also there's a GitHub available. I know most of the folks in this room are not data scientists, but if you're a data scientist and you want to apply this in your own setting, you can go to that GitHub and follow a series of notebooks that will show you sort of how we did each of the things that we did and let you sort of take that and repurpose it to study a different disease. So because the machine learning model is general, it means you don't have to build a new model for each disease. You can take that model and apply it to specific datasets for diseases of interest. And we're talking now about using this to better understand pediatric brain tumors as part of an open pediatric brain tumor atlas that we're doing with the CBTCC and PNOC data in Adam Resnick's group. It's good to see Adam here. So yeah, so we're reusing this as well and we hope that others will. So I just want to close with a note that these large public datasets, because they haven't contained sort of often a large amount of participant data, essentially these are data that researchers, making completely available for downloads without restrictions. And so what that means is it's often a lot of cell line work or other work. These teach us a lot about biology. And so if we can learn from that, we can really apply it to new settings. And I think going after these large collections of public data is relatively unexplored. So Vannevar Bush wrote to FDR sort of advocating for this sort of national interest in enhancing our scientific enterprise, "the pioneer spirit is still vigorous within this nation. Science offers a largely unexplored hinterland for the pioneer who has the tools for his or her task." I think what we're seeing with these large scale datasets is that having this public data really creates this type of environment where people can go, learn these patterns and then apply them to target datasets. You know, I think this would be enhanced by having data that is broadly available from these rare diseases, but I mean we definitely have to balance what we're asking of people when we're asking them to share that data. And so I think learning from the public data, so the data that is available without restriction and then responsibly applying that to the private datasets I think is going to be really important to moving this forward. With that, I just want to wrap up and thank the folks who made this possible. In terms of projects, I really actually only got to talk about some work from Jacqueline Toroni, who is now a data scientist in the CCDL. This work was funded, in part, by the Gordon and Betty Moore Foundation, as well as Alex's Lemonade Stand Foundation. And I'll be happy to participate in the panel in a few minutes. Thanks. [Applause] Moving right along to introduce Carly Strasser from the Fred Hutchinson Cancer Research Center. She's director of data and partnership at Fred Hutch. And I think in Andy's slides, you remember seeing the country colored purple, all except one seat which was green. And there's a reason for that, and I think that the group that Carly works for is part of that reason. So hand it over to Carly. Okay, thanks everybody for having me here. I'm actually an oceanographer by training. I would not have expected to be here when I got my degree years and years ago. But I'm really excited to be here, because I think this is a really, it's been a great two days. I've learned quite a bit about this space and the role that patients can really play in moving cancer research forward. So if you're not familiar with Fred Hutch, we're located in Seattle. We're right on South Lake Union, so our campus is down below in the corner there. And it's a pretty great place to be. We have lots of good views of sea planes kind of landing on that lake right over our heads as we do our work. We are an NCI comprehensive cancer center. We're part of the Cancer Care Alliance in Seattle which - or sorry, Cancer Care Consortium, which includes the Seattle Cancer Care Alliance. That's our kind of more clinical arm - the University of Washington and Seattle Children's. We have around 2,000 employees and 238 faculty members and we're most famous for pioneering bone marrow transplants years ago. We have the research divided up into four basic divisions. When I came in, I thought that trying to make research work at the center was going to be easier because everybody there is a biologist and everybody there is working on something related to biomedical research. But it turns out these divisions are all very distinct, just like any other academic organization. So part of my role is to figure out how to create partnerships across these different divisions. And in fact, the leadership there also thought that was a good idea and created something called integrated research centers. And so these are areas that span all five divisions and look at things like immunotherapy, pathogen associated malignancies and then most recently the translational datascience integrated research center. There's also two arms that support the research that goes on and one of those there at the bottom is the Hutch Data Commonwealth, and that's the group that I work within at the Hutch. So the Hutch Data Commonwealth is really a substantiation of something that the leadership at the Hutch thought was important. So we know that scientific research has changed dramatically in the last 15 years, even in the last five years due to more data, more connectivity, internet things like that. And the Hutch really recognized this and established both the IRCs and the HDC, Hutch Data Commonwealth, to really try and meet those needs of the researchers that are working in this space. So HDC helps build data intensive research capabilities at the Hutch and then also within our community through software and data engineering, training and partnering. And so if we think about researchers, both here and everywhere else, you can think of them as being on a continuum of less data savvy researchers to more data savvy researchers. It would be easy to bin these into older generation researchers and newer generation researchers, but that's also not the case. Sometimes newer generation researchers also don't have the skills that they really need to make use of data. And sometimes the more senior folks can really recognize the value that data is bringing into the space that we work in. So HDC and other parts of the Hutch really think about pushing people into that more data savvy space. And we do that primarily through things like training and consulting, so bioinformatics consulting or data science consulting. And then also thinking a lot about collaborations and partnerships of how we can really partner our researchers that have those data skills and expertise with the clinicians who might not necessarily have those skills, but have access to patients or data or resources. So research itself also falls a bit on a spectrum of things that involve less data to things that involve more data. And we're going to see more and more of that research is pushing towards that end of more data. And what that means is when you have more data involved, it means more power and more relevance. And we've seen that in the last couple of days with discussions around having patient populations that are big enough to really do those research studies on. And so the way that we push research closer to the end with more data is through things like public databases, data sharing, and collaborations and partnerships again. So this is often talked about in terms of a data commons. And so that is these data commons can be thought of as a way to collocate data, storage, compute, and core services that are used for managing, analyzing and sharing data, to create an interoperable resource for the research community. And I would expand that so say for the community at large, not just researchers. And this is something Bob Grossman has said rather famously around the space associated with data commons. This is a pretty hot topic I think right now. Yesterday we had a speaker that referenced Eleanor Ostrom who received the Nobel Prize for doing some work around the data commons, or sorry, around commons in general. And a lot of that work has been taken to the data commons idea, particularly here at the NIH. So, most recently there was a data commons pilot based consortium that wrapped up. And this data common pilot based consortium was really intended to start exploring this idea of a data commons that could be used by the entire community not just researchers, but all folks that are interested in tapping into resources associated with data that's produced primarily out of NIH funded studies. So my boss, Matthew Trunnell, he's the Chief Data Officer at Fred Hutch, he was associated with the data commons pilot based consortium as an advisor of sorts. And so he went to a lot of meetings. He came here to D.C. a lot, he talked to people a lot. He spent a lot of time in working groups thinking through techniques and tools, infrastructure that's needed to make something like a data commons work. And in the course of all that, he thought, well, why don't we try and do something like this in our own region, in the Cascadia region? So, if you're not familiar with the phrase, Cascadia, it involves kind of the area around the Cascade Mountains. And so you can see that the yellow star there is Seattle; we also have Vancouver, British Columbia, and Portland, Oregon, and two of the other major cities. There's other cities in there. But thinking about how we can unite the groups across this general regional area, and do something that starts small but could potentially grow to something much larger. So, we instantiated about a year ago the Cascadia Data Alliance. And so this is intended to be a regional data sharing ecosystem. And part of the reason we want that is that we want to take advantage of the fact that there are a lot of things that are collocated in this area. So there's certainly some research organizations including University of British Columbia, BC cancer, Fred Hutch, the University of Washington. There's also Oregon's Health Sciences University, and a few other universities in the Oregon State area. There's also lots of tech companies. So, Seattle is famously home for Amazon and Microsoft. We also have a large presence from Google, Facebook, the Allen Institute is also there. There's lots of activity in this space happening very close, and within driving distance of Seattle. And so the hypotheses that we're going into this Cascadia Data Alliance, that proximity matters. So that is that we can actually take a day trip to Vancouver from Seattle. It's not the most pleasant experience, you have to wake up really early, but you can do it in one day. I've done it several times now. And you can have face to face meetings with folks. You can really make progress on things like governance and agreement and have really productive discussions about infrastructure and technology building. You can get the technology builders in the same space working closely together. And then the second hypothesis is that we're going to start with metadata. So, metadata is the data about the data and it's a little easier to share. That is, for instance if you have a patient population, the metadata might be the number of patients, the type of samples that they're providing, their ages, and things like that. So the higher level metadata is kind of the summary of all of that information. And what it means is that you don't have that personally identifiable information in that dataset. It makes it much easier to share, much easier to collaborate on and it's really more of a gateway. So we think of the metadata sharing as a way to create collaborations and partnerships. So the Cascadia Data Alliance currently has these organizations as members. The Microsoft logo down there is to indicate that they have financially supported this to the tune of millions of dollars. So we're really excited about working with them, both to develop the technology, but then also to support the work that we're trying to do to collect these different organizations into one kind of cohesive data commons. So there's three main areas of focus for that Cascadia Data Alliance, and I've touched on all three a little. The technology is really how do we build an infrastructure, a platform, some way to effectively share the metadata and the information across the group? There's also a lot of governance discussions happening. We've heard threads of this throughout the day yesterday and today on how hard it is to actually share data; what you can do to accelerate that process. So, it can take months or years for two organizations to decide to share data and as we talked about a lot yesterday, patients don't have that kind of time. So governance is definitely a huge part of this discussion. And then finally communities. So, how do we actually get the researchers to know that each other exists? How do we partner them with the right groups that are really going to accelerate the work? How do we make sure that they have access to the right technology? And how do we kind of knit all of these groups together to make it as effective as possible? So this is a great little graphic that our communications and marketing team pulled together. And you can see that it's basically a flow for how we think the Cascadia platform might work. So you have these folks at the end there that are saying I have interesting data that I want to share. And you might have someone else that says oh I'm looking for data to really grow the dataset that I'm currently using. So the person that has data to share would provide their metadata to this database; the person looking for data would of course mount search for that metadata. And we would assume that they would have found some interesting dataset or provided access to an interesting dataset. You can then connect the data owner and the data seeker and that's really just through something really easy, like email. They can informally agree to share data if they decide they want to work together, then we can connect them with a governance library, which is really a way to formalize the data sharing agreements that have to go into this type of work. They can then share the data with each other, and kind of carry on with the research process. And so really the idea here is to just really streamline and simplify how these researchers can connect with one another, and how they can find data that might be useful for the work that they're engaging in. So these are just some fun phrases that we've been thinking about in terms of the Cascadia platform and how to move forward. So to try and get around the idea that there's a lot of personally identifiable information in healthcare, we're thinking a lot about things like creating synthetic datasets. These are artificial datasets that have all of the probability characteristics of a real dataset. You can actually do the analysis on the synthetic data without actually exposing the real data. Differential privacy which is something that Casey Green didn't talk about, but knows a lot about. And this is another way to actually do analyses on the data without exposing any information that might otherwise cause harm to individuals. We're thinking about ways for researchers to subscribe to a data search. So if they are really interested in data of a particular type, they can ask for that type of data to be emailed to them or a notification to be emailed to them when it shows up on the platform. And then metadata game-ification. So one of the big problems with data sharing is that a lot of people don't provide enough information in the metadata so that people could actually say oh yeah that's a dataset I really want to use. And so how do we make it so that people are more likely to add that metadata information, add some more fields to that metadata, so that people could find the work effectively? So, I've been an open science and data sharing advocate for a long time and the challenges of sharing data, particularly in the healthcare space are really interesting to me. But I just want to remind everyone that research data sharing is hard. This doesn't even get into patient data sharing; this is just the individual researchers working every day at a lab. There are privacy and security concerns like we just talked about. Interoperability challenges, which are huge and very difficult to tackle. There's a huge lack of information and skills knowledge that the researchers need to have to be able to actually create data that can be shared effectively. Doing that data sharing is time-consuming. And there are not very many professional incentives to share data across different research programs. And I think that's - it's something worth noting because it is, it does affect the way that researchers interact with data sharing, and thinking about data sharing. And I think that the patient populations have a very unique ability to push researchers to work past these barriers and find ways to really accelerate research. So this is from yesterday. Sean put up this great graph, which has this idea of - here, I'll switch to the next one which I recreated. So there's this trigger, there's a peak of inflated expectations, there's a trough of disillusionment, and then a plateau of productivity. I thought this was a really great way to talk about data commons. So data commons and data sharing platforms have been around for years - 10, 15, maybe 100 years. They haven't always worked; we certainly don't have some sort of an amazing platform that everybody's using these days. But I think we're in a really good spot. I've seen a lot of really interesting things in the last two days. There's a lot of evidence that we are hitting that plateau of productivity, and that we might actually be able to start facilitating data sharing really effectively, not just among the researchers, but also among the patient populations. So thinking finally, you know, open science and data sharing really accelerates research. And in fact this is really important for Fred Hutch. So our president, Gary, is very famous for talking about his plan to defeat cancer by 2025, which is really not that far away. It's about 1,900 days. So, how do we get to that? We get to it by data sharing. We really accelerate the pace of research by pushing these agenda items forward to really make the research space as community driven as possible, and make sure that everybody is participating and contributing to the data. And with that I will end it. If you have any questions I would love to talk more about this with anybody here. [Applause] Carly was donating her phone to us. So, our last speaker is bringing this sort of full scope for full circle back to the clinic, is Stacy Gray. She's an oncologist at the City of Hope Comprehensive Cancer Center and she's going to be giving us the clinician's perspective on some of this stuff. Good morning everyone. I want to thank you very much, the organizers of the meeting for inviting me to participate. I'm thrilled to be here talking to genomic data sharing and giving this a clinical perspective. So a quick overview, I'm going to talk about how we use genomic data at the clinic. We'll review studies on data sharing. I want to focus on what I consider the very high cost of ineffective data sharing and then a little bit at the end talk about changing the culture. So how do we use genomic data in a clinical context? And as you've heard over the last couple of days, it's really important to remember that cancer deals with two genomes. So we have alterations in the somatic or the tumor DNA that has implications for large treatments. So as Janet so eloquently described this morning, if you identify an alteration in the tumor that's driving the tumor you can find a targeted therapy to turn that signal off. This kind of data is also sometimes used for a prognostication and determining a diagnosis. That's in contrast to alterations in germline DNA. This is the DNA that we're born with, and that is in every cell of our body, that we can also pass on through families. We use on information about germline DNA for cancer risk or risk of other diseases, but increasingly in cancer this information is also helping to inform treatments. Now when we think about germline DNA, we also know that there are lots of concerns that are raised in this space that aren't necessarily seen with tumor sequencing, such as relationships in the family, psychosocial concerns, privacy, and discrimination. The first thing that I want to stress, that I think you've heard both from some of our other advocates is that genomic testing can dramatically improve patient outcomes. So just one example in the germline space, if you've identified somebody who has a BRCA1 mutation, and offer that individual prophylactic surgery, the survival gains are on the order of about 24 percent. When we have tumor sequencing, and we can identify these driver mutations that can be targeted with therapies, we also see profound benefits. So this is an example of an individual who has metastatic melanoma with thousands of melanoma deposits under his skin. He had an alteration identified in a gene called BRAF, and was treated with a BRAF inhibitor. And you can see that the melanoma virtually disappears. Right? So we see in melanoma, just as one example, that we can achieve survival gains of upwards of 20 percent. So, given that genomic data is really important in a clinical context, what are patients' and sort of other individuals' preferences for receiving genomic information? So this was a study that I think was reviewed that spanned over attitudes about receiving genetic data from almost 7,000 individuals from 75 countries. And they've looked at attitudes among the general public, as well as among healthcare professionals both within genetics and outside of genetics, as well as genomics researchers. And one of the things that I think was very striking about this study is that when they asked these participants if research participants should be able to choose to receive pertinent findings if they want them, there were some really different attitudes among the different providers. So, it turned out that genetic health professionals, non-genetic health professionals, and genomic researchers were much more conservative. So just as one example, health professionals were 2.8 times less likely than individuals in the general public to endorse research participant receipt of genetic data. It would be interesting to talk a little bit about why that might be the case. What about cancer patients? It's a lot about sort of the urgency that cancer patients feel, because this information is important for them now. So this is a study that we did in the context to pull exome sequencing at the Dana Farber, where we were asking individuals whether they wanted their data back, either their somatic or their tumor results, as well as their germline results and we were returning that data based on their preferences. And what we find is, pretty much across the board, almost all of the patients want all of their data. We found some slight differences when it came to negative prognostic information, or when it came to information about non-cancer conditions that couldn't be treated, slightly lower rates of endorsement of wanting those data back. But in general, most people want all of their data. There was another study at MD Anderson where they were looking specifically at germline data, as well as at Memorial Sloane Kettering, and they found similar findings. The MST study was a qualitative study that showed endorsement rates of 57 percent, and we can talk a little bit more about why that might be the case. So how do patient participants react to the return of results? When we started this study, you know, many years ago now, in the exome setting, people said, well, these data, particularly for people who are sick or who have other conditions, it might be overwhelming and there might be psychological harm. So what we did in the context of a group of studies funded by the NHGRI under the CSER1 Consortium, was a meta-analysis of psychosocial outcomes related to the receipt of findings out their genomic test results. We had 1,300 participants who had all received information after the kind of disclosure, and filled out their surveys. Now one of the things to know about the CSER Consortium is that these are participants across many different healthcare contexts. So some of them were healthy individuals; other people like those people in our study had advanced cancer. And what we found is that there was essentially no clinically significant psychological harms from the return of genomic data. So on the right you'll see these are random sex meta-analyses, looking at anxiety on the top and depression on the bottom. What we found is that from pre-disclosure to post-disclosure, there was actually a reduction in anxiety and distress and depression. And that for the vast majority of individuals, all time points were under the level of clinically significant depression. When we were digging a little bit deeper, we found that some populations might experience very low levels of test-related distress, or also greater psychological benefits from the test result disclosure. But in general, patients responded well to the return of results. So how about attitudes about sharing their data? Well, there was a really nice study that was done showing patients' preferences for data sharing [unintelligible] in general. There were lots of benefits and some risk to decide overall, 98 percent of patients identified benefits to data sharing, including helping others with a similar condition, as well as helping themselves or their family members. The risks, 74 percent overall, were things that we would probably expect. Potentially revealing my identity; not knowing what happens with the data; and health insurance discrimination. But in general there was a trend to identifying more benefits to data sharing than identifying risk. Now one of the things that the study also did which I thought was very interesting which was they asked them about their preferences both sort of in a hypothetical way, but then also what they actually did when it came to determining their data sharing preferences in the context of the study. And they found participants were more restrictive in how they would report sharing than actually what they decided to do. So when it came to broad data sharing, for example, when they were asked hypothetically, 69 percent of patients said they would share their data. But when it came to actually participating, 100 percent of the patients actually shared their data. And trying to tease out why there might be differences here, the authors did a very interesting thing, which was they look at attitudes related to protecting privacy, versus attitudes to advancing research. And when asked those questions separately, as you can see in the far left part of the figure, there were high rates of endorsement for both wanting to protect privacy and also wanting to advance research. The advancing research is in sort of the more maroon and less purple color. So when they asked them on the far right if they had to waive privacy versus advancing research directly against one another, they found that more people endorsed advancing research than they did protecting privacy. So while patients in this study were concerned about privacy, they often were compelled to share their data by a desire to advance research. So how about how do people feel about genetic data as compared to other data? This is a study that recently came out in two academic centers, asking patients about these issues. 23 percent of patients wanted to share all types of information with researchers. But one of the things that's really neat about the study is they've looked at genetic data as compared to other kind of data. And in the first column you'll see, okay, this is really restrictive, I don't really want to share my data with anyone. If you look to the far side of the table, this is more liberal data sharing, I want to share this with my home institution, not for profit institutions, and for-profit institutions. Well, you can see from these numbers is that attitudes toward genomic data sharing were really similar to attitudes related to sharing information about allergies. Something that is very I think non-charged and not potentially very, you know, having a lot of sort of unintended consequences with it. The researchers also identified several areas where they did see sort of more heightened privacy concerns and that related to things like their sexual life, which is shown in the bottom row, as well as things like sharing their contact information. So it's interesting that a willingness to share genetic data among these participants was similar to their willingness to share other kinds of data. And I think this really speaks a lot to the history of genetic exceptionalism, and the fact that maybe genetic exceptionalism among participants is going away. And I'm going to transition a little and talk about the high cost of ineffective data sharing in a clinical context and I want to anchor this with a case example. So recently I saw a woman, she was 54 years old, she was adopted, and she had breast cancer. Now, her provider at another institution decided to do genetic testing both because of her diagnosis and because we didn't know anything about her family history. And she was told verbally that she had genetic test results that included a pathogenic mutation in the gene called CHEK2. Now, based on this result, she was told that she needed to have a bilateral mastectomy, that she needed to have bilateral removal of her tubes and her ovaries to prevent ovarian cancer, and that she needed to have high-risk screening for colorectal cancer, as well as engage in screening for thyroid cancer. So based on these recommendations, she scheduled two surgeries and planned to undergo all of the additional screening. Now one of the surgeons actually importantly said, I think maybe you should get a second opinion. And so she came to see us. When we actually got her genetic test results, it turned out there were two important things. One, that she did not have a pathogenic mutation in CHEK2. She had a variant of uncertain significance. And Andy today and Bob yesterday sort of referred to the fact that a lot of genetic data is seen in individual patients for the very first time, and we don't necessarily know what it means. And for the vast majority of patients, we treat these findings as though they're negative. We don't act on them, and we don't do anything with them clinically. And data has shown that over time, over 90 percent of these variants are [inaudible] variation. And so the decision not to act on them is the right decision. In fact though she did have a pathogenic mutation but it was in a different gene, a gene called NBN. And the findings of the test actually contradict both what she was told as well as what was documented throughout her entire medical record. So why does this matter and how did this change things? Well, she didn't need a bilateral mastectomy. She didn't need to have her tubes and her ovaries removed. She didn't need to undergo high-risk colorectal cancer screening and she didn't need to have thyroid cancer screening. So it dramatically changed the entire course of what she was going to do. In fact all she needed to do was to have an enhanced screening of her remaining breast tissue. So I think there's a potential for significant mismanagement. And that one of the ways that we can help to overcome this is by improved data sharing with our patients in a way that is, I think, much more substantive and that this might help us to avoid poor outcomes. And you might say, well, Stacy, this is just one case example. But I wish I could say that this were an exception, but it's not. It's something that we see all the time. And this is further justified by a study by Allison Kuriant at [inaudible] population based study of breast cancer patients. And what she found is that when she asked breast surgeons how they would manage patients who had these variants of uncertain significance, that low volume surgeons actually about half of the time would manage them exactly the same way. And even high volume surgeons, 24 percent of the time would give them the same recommendations. And again, this is exactly sort of inconsistent with care. And it's important because of the patients and their cohort they found that 43 percent to 51 percent of patients who had variants of uncertains actually got bilateral mastectomies. So physician misinterpretation of germline testing is associated with unnecessary treatments. How does this look in the tumor space, right, because the issues are somewhat different. And I think one of the biggest things that we see here is that there is an under-utilization of standard of care. So, as Janet so eloquently talked about this morning, lung cancer is a disease in which identifying genetic drivers and matching them to targeted therapies is absolutely essential. This is a study that was recently published on non-small cell lung cancer patients. There were over 4,000 individuals. And what they found is that 48 percent of patients who had a targetable alteration received genetically targeted therapy. Which means that over half of the people who had them did not receive targeted therapy. And the figures to the right show that this varied by gene. So it was a little bit higher for genes like EGFR and ALK and lower for the other genes. But this kind of under-treatment is absolutely unacceptable. So up to 30 to 50 percent of selected patients who have these genomic biomarkers are not receiving the standard of care. How does informing patients help to potentially ameliorate some of these problems? Well I think one of the places you need to start is that many patients either don't know their genetic test results or they misunderstand them. And this is something that goes back to things that are very, very basic. So, in a population of cancer patients that was done by Rachel Freeman out of Dana Farber, she found that when she asked breast cancer patients if they knew sort of the characteristics of their tumor, that strikingly high numbers of women didn't even know if their tumors were ER-positive or HER2 positive. And these are the kinds of data that have been used for a very long time to inform therapies. These disparities or knowledge gaps were even more significant in black and Hispanic women. It's also important because whether the women knew about whether their tumors were ER-positive was associated with their ability to get targeted therapy. So women who knew if their tumors were ER-positive were almost four times more likely to get the targeted hormonal therapy that they needed. So patients' poor understanding of cancer genomic biomarkers is associated with lower rates of targeted therapies. And I think this is an area where if we can really move the needle, we can potentially increase value to our patients. So obviously there are a lot of other barriers to effectively using genomics in the clinics. I mentioned this morning include things like limited samples. We also have low confidence among providers in their genomic knowledge. This is a summary of two studies, one that we did and one that was done in the pediatric oncology contact showing that even at a major academic medical center, between about 25 and 50 percent of oncologists actually don't feel confident in their ability to use genomic data. As Andy mentioned this morning, there's lack of structured data in the EHR, which means that they can't actually often find the results, let alone build decision support on them. Reimbursement is a huge problem. So actually getting tested in the first place is a challenge because there are lots of prior authorizations or their insurance restricts it. This happens both for tumor testing, as well as germline testing. And then trial accessibility and lack of drugs are another thing that are significantly problems. You can see in this table, the number of [inaudible] is between 39 and 94 percent. So the number of people who actually get treated with targeted therapies or are on genomic trials is much lower. And obviously there are many, many reasons that relate to this, but trial accessibility and sometimes we just don't have the right drugs to treat them, are two of the major issues. This complex set of barriers to using genomic data effectively in the clinic. So, just to spend just the last couple of minutes talking about how we need to change the culture. Yesterday we talked a lot about the ways in which we can change the research culture. Things like innovations and consent, sharing research grade data, and very significantly involving patients in genomics research. And I also think we need to change the clinical culture. So the first is we need to educate our providers. And there are lots of very, very novel ways to do this. But one of the most important things is to think about how we can educate providers at the point of care where they're making decisions. So if you have that patient who comes in with a rare mutation and you don't really know what to do with it, how can you learn about it in a very short order to be able to direct them to the right place? We need to get data in the EHR in a structured format. We need to [inaudible] for the clinical trial access. There's a really innovative example through ASCO which is the American Society of Clinical Oncology where they're taking genomically grounded clinical trials into the community to try to get them to patients where they are. And then we also need to return data directly to patients. And Geisinger has made a lot of great inroads in this area. And we're working on tools that will not only return the information to them, but embed it in a context of education so that they understand what the results mean, how they can be used, how they influence their family members, and what the next steps are. So I think all of these kinds of things are things that we need to really improve care. So thank you very much, I appreciate it. [Applause] 