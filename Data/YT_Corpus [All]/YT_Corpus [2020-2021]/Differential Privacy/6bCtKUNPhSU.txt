 [Music] thanks everyone for joining in and thank you for having me I look forward to the summit every year and this year is no exception I'm very excited to talk to you about analytics in AI and how you're reimagining the boundaries of what can achieve so many new AI capabilities have been talked about at the summit already and today I wanted to draw your attention to something that should be at the crux of all this innovation responsible AI practices looking at how the world has changed around us over the last few months and more so over the last few weeks has greatly amplified the need for us to ensure fair practices and ethical behavior across all sections of society to get a sense of the intensity of this topic Capgemini published late last year that nearly nine out of ten organizations have encountered ethical issues resulting from the use of AI these executives cited reasons including a lack of resources dedicated to ethical AI systems the failure to consider ethics when constructing AI systems and the pace at which AI is touching every part of our lives the statistic is alarming v as a community have a duty to address these issues head-on and empower organizations to use AI with a sense of responsibility at Microsoft we've also heard these concerns firsthand from our customers as a result we've invested in the advancement of AI driven by Responsible principles that put people first our approach to responsibility is anchored around the following focus areas keeping ethical principles at the core of everything we build enabling governance and operational transparency to drive write practices leading efforts for public policy at the highest levels and finally empowering teams with cutting-edge tools to encourage responsible AI practices now since the spark in AI community comprises majorly or developers and data scientists will focus on the tools in collaboration with Microsoft Research we are bringing the latest of research in responsible AI to Azure over the next few minutes I will talk about how the new responsible machine learning capabilities in Azure and our open source tool kits empower data scientists and developers to understand machine learning models protect people and their data and control the end to end machine learning process let's look at each of these three pillars in detail first let's look at the understand pillar it comprises driving an understanding of the models fairness index and its interpretability but what does that mean one challenge with building AI systems today is the inability to assess and mitigate unfairness in the models to address this challenge we recently open sourced a toolkit called failure there are two components to failure first focused on fairness min is a dashboard with both high-level and detailed views for assessing which groups are negatively impacted second focused on mitigation is a set of algorithms for mitigating the observed fairness issues these strategies are based on a variety of supported fairness definitions such as demographic parity or equalized odds for classification tasks the supported unfairness mitigation techniques can easily be incorporated into existing machine learning pipelines and assessed model fairness during both model training and deployment together these two components enable data scientists and business leaders to navigate any trade-offs between fairness and performance and to select the mitigation strategy that best fits their needs using fair learn developers and data scientists can leverage specialized algorithms to ensure fair outcomes for everyone another important aspect of understanding model is the ability to interpret or explain its results this is where interpret ml comes in as the name suggests it helps interpret models and their results interpret ml is another toolkit we've open sourced and can be used for both glass box and black box models glass box models are designed to be interpretable they provide lossless explained ability as they use interpretable algorithms like decision trees or explainable boosted machines interpreted ml can help visualize the various features affecting the outcome of a model in an interactive dashboard and even allow for what-if analysis on the other hand blackbox models are based on more complex techniques like neural networks and can be a little tricky to interpret you can still achieve approximate explained ability with blackbox models using explainers like lime or sharp in different ML uses these explainers to surface a dashboard with feature importance interpretability is needed to ensure that there is optimal transparency within models to assess and reason through the predictions it generates or the recommendations it creates with falen and interpret ml you can a very good understanding of your models let's now look at the protector protect reference to protecting data against any potential misuse used to train the models to understand this better let's envision a scenario where a Dean a scientist wants to use some sensitive data to create a model in a non protected scenario this would result in submitting a query to the private sensitive data set and provided the right credentials that receive sensitive data as a response to their query well this is problematic now the data scientists can look at the sensitive data and potentially reduce PII information about an individual this is clearly not at all compliant or ethical now let's introduce a differential privacy toolkit within the open DP initiator that has been developed by Microsoft in collaboration with the researchers at Howard's Institute for quantitative social science this toolkit ensures differential privacy by first managing exposure risk by tracking the information budget used by individual queries and limiting further queries as appropriate in our example the queries first validated against the information privacy budget and processes only if it isn't over on second it injects statistical noise and data without significant accuracy loss to help tremendous closure of private information again in our example because the query response may be sensitive before it reaches the user it is injected with noise to make it differentially private in addition to differential privacy you can also protect your data on either using Hardware back confidential machine learning capabilities and homomorphic encryption and ensure your data isn't compromised at any stage of the machine learning lifecycle the third pillar of a responsible ml is control it refers to tracing the lineage of models and supporting a standardized format of documenting it for cataloging and reusability means on the control side there are some features and capabilities that allow you to manage your audit trail and data sheets for the models with the Naja all the actions undertaken throughout the lifecycle of a machine learning model can be tracked for auditing purposes this audit trail enables us to trace the lineage of the training data the operations performed on it its drift and most importantly the model versions and the various deployment endpoints now this is important for compliance reasons organizations can leverage this audit trail to trace how and why a models predictions showed a certain behavior additionally once you have a model it is important to capture its properties and purpose alongside any other metadata that could be helpful from a reusability standpoint in Azure we enable this with custom tags you can define for the models and capture all its information so that it travels with the model all the trails and data sheets enable accountability for models and are an integral part an integral component of responsible ml I've been talking about quite a few capabilities so let's bring them to life with a demo let me invite Sarah Byrd to demo these toolkits and show how you can use them Sarah thanks Rohan hello everyone today I'm gonna be walking through a demo of some of our responsible AI tools on Azure as we progress through the ML lifecycle I'm gonna be demonstrating three key capabilities protecting sensitive information with differential privacy assessing and improving model fairness with fair learn and analyzing an explaining model behavior with interpret ml in this demo I'm gonna be building a model for loan applications to decide which to accept in which to reject let's dive in here I'm using Azure data bricks but you could also use a Jupiter notebook or your favorite Python environment like any good model building scenario the first thing I want to do is look at my data here I'm looking at aggregate information as well as individual features to help give me a feel for the data set and uncover any potential issues up front it looks like this data set contains a lot of great data to help me build my model it also contains highly sensitive information that must be kept private obviously I can limit direct access to the data with the data store however what many people don't realize is that aggregate values can still be used to reveal private information about individuals let's go to an example to see what I mean so here I'm going to demonstrate how we can use aggregate information to reconstruct the data set so in this case I'm going to assume that I know aggregates about individuals income as well as a little bit of individual information about two individuals in the data set and with that I can take it off the shelf Sat solver and use that to reconstruct a data set that's consistent with the information that we know and so if we run our sets over here it's working to reconstruct that data set and we see that it can actually find a result and now we're gonna compare that to the the real data set and see how well our attacker did so in this case it looks like the attacker did pretty well we're able to correctly reconstruct the incomes of individuals within $5,000 for more than 20 percent of the data set which could be a significant privacy violation so what do we do about a text like these how do we use data but still protect privacy and this is where differential privacy can help so differential privacy hides the contribution of individuals in the output computation by adding a small amount of statistical noise to the query the noise is significant enough to protect the privacy of individuals but often small enough to still allow us to use the data differential privacy also calculates the amount of information that is revealed in the query and subtracts that from an overall privacy loss budget let's look at how this works so here I'm going to use our open source differential privacy platform now I'm going to add that statistical noise to the income information and then I can retry my attack with my Sat solver and what you'll see here is that we get a very different result we aren't able to reconstruct the data set that's consistent with the differential private incomes and so we don't have any guesses at any private information however it's not enough to just protect privacy we still want to be able to use the data so let's look at how we're doing here in this case you can see that we're we're looking pretty good we're you know pretty close to the original information you can definitely see some noise added particularly this is a small data set however now I can take this and go and use it in my problem knowing that I don't have to be concerned about privacy attacks so one of the ways that we can use differential privacy in machine learning is to generate synthetic data sets so here I'm gonna use my open source toolkit and I'm gonna create a dataset that matches the patterns and trends of my original dataset but it doesn't reveal any private information and that way I can take that and give it to my data scientist without having to worry about privacy so I'm gonna generate my dataset here and now that I have my private dataset I can go and build my model so here I'm going to build a psychic learn model for my problem and now that I've built a model I want to actually analyze the performance and see is it good enough for my problem so of course I can look at traditional metrics like accuracy however since this is a problem where the wrong decision could have a significant impact on an individual's life it's important that I go a step farther and I also consider the fairness of my mom so in this case I'm going to use fair learn so that I can understand how well my model works for different groups of people and make sure that it's not using sensitive attributes to make decisions so in this case I'm going to use fair learn and Farren has an interactive dashboard to help me better understand the fairness assessment so I'm gonna click get started here and my dataset has two values a sex and race so let's click into sex and understand how well my model is doing but I also need to provide a performance metric which metrics important from my problem so in this case it's accuracy so I'm going to click through and the first thing I have is a graph here that helps me understand the difference in model performance across these two groups so I can see that my model is more accurate for women than men and it's about 85% to accurate overall however I'm really interested in the outcome of this model what's the difference in who I'm offering loans to so in this case I can see that I'm offering loans to 19% of people overall however I have a significant difference in the number of loans I'm offering to women in men I'm offering loans to about 7% of women that apply and about 25% of men that apply now I don't necessarily know if this is a problem it could be that these two groups have very different distributions however I do know that it's a sign that I want to dive in and investigate further so for that I can use interpret ml and it's black box explanations so I'm going to explain generate explanations for my model here and then I'm gonna use its interactive dashboard to explore those and understand how it's making its decisions so in this case I'm going to set up two different cohorts so let's set up one for women so we can understand what's going on there so I'm going to add a filter for sex and women are zero in this data set so we can add this filter and I'm gonna save then I'm gonna have a second cohort here for men and I'm going to select this and in this case I'm saying it for men and women since I want to look at this fairness problem but you can use lots of different cohorts like train and test to compare as well so in this case what we can see is the model performance just like we saw in a fair loan but in a lot more detail however what I want to do here is a jump over to the explanations to understand the future importance and how the models making decisions and what I can see right away is that the model is using sex directly as a feature to make predictions for women but not necessarily for men and if I click on this I can actually dive in and see that for for women for every single woman in the data set the value of sex being 0 is directly contributing to the prediction that they are rejected which is pretty concerning and so what I want to do at this point is move on and explore mitigation techniques so for that I can go back to fair learn and use some of the built in mitigation algorithms that allow me to reduce disparity either during training or after so in this case I'm going to use the grid search approach where I'm going to retrain many different models by rereading the data to try to reduce the disparity so I've run my model training here and now I can go back to the fair learned dashboard to look at the best models so in this case now I have a range of models that are along the the curve of difference in predictions and accuracy so here's my original model up here and now I'm actually going to click through and look at this model which is a little less accurate about 82% accurate but if I jump down to the disparity in predictions I can see that I have a much different outcome now I'm offering loans to 15% of men and 14% of women that apply so this might be a much more promising model for my problem and so what I'm going to do now is actually save all of my work so that I can share it with collaborators or show it to other stakeholders as we make a decision about whether or not we want to move forward with this model so I'm gonna register all of my models here I can upload my dashboards and my explanations everything to Azure machine learning then if I move over to my add your machine learning studio here then I can click on a particular run and so here's my different runs and I can click into one and now I can actually have my dashboard stored directly with my model as well as the fairness and so now I can show this other stakeholders are compared to previous models as I decide whether or not I want to take this into production back to you Rohan thanks Sarah it was so nice to see all of these machine learning tools come to life with the loan scenario just to add more color this demo is very similar to what eva is doing with these tool kits the using feather and extensively to mitigate unfairness in models they've been able to reduce their disparity by more than 90 percent which is amazing and many more customers are using these tool kits to build responsible machine learning practices I want to take this opportunity to invite the community to get involved contribute to the open source projects powering responsible machine learning capabilities create scalable impact with the objective of having ethically high practices and solutions all these tools are hosted on github and already have hundreds of downloads within the last few weeks if you want to try them in action please use the agile links to try them with an azure machine learning or agitator breaks with that I want to thank wait did I forget something well I must let you in on a little secret this one about my presentation environment I'm sure most of you think I'm in front of a green screen here and there are some graphics people doing their magic in the background we thought of experimenting with AI a little bit and putting it into the use just for the summer first let me show you where I am this is not a formal studio it is a normal living space when I just came in to record the stock now let me show you how it was converted to a presentation stage for me we use connect and the azured Connect def care to bring AI and AR together to simulate the stage here's an architecture of the solution that produced the video for the stock amazing isn't it we use normal camera images and depth-sensing connect camera images to replace my actual background with the stage and design of our choice I thought about a beach in Hawaii but opted for a bit more presentation friendly option this is just an example of how together we can stretch the boundaries of AI the solution has been published in a github repo linked here for you to use for your next meeting or video call again thank you for the opportunity to share I hope you have a great summer please stay safe [Music] 