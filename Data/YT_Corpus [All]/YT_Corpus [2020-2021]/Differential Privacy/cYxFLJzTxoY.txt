 happy to introduce our second speaker of the morning leilana kegel a principal research scientist in the computer science and artificial intelligence laboratory where she leads the decentralized information group leilana will be speaking about block flow an accountable and privacy preserving approach to federated learning please join me in welcoming leilana thank you eric for the introduction and thank you for having me good morning all today i'd like to discuss a project that deals with accountability in federated machine learning so accountability is built into a human society through legal and social rules but it is rarely reflected in our technologies the vast majority of these rules in society are not enforced perfectly uh yet most of us follow these rules because it's easier to uh to meet the norms than violate them risk getting caught and being punished so this research project tries to model how social accountability would work through punishments and rewards using blockchain technology we believe that this will be extremely useful for banks hospitals matters dealing with national security and other situations in which shared machine learning models need to be developed but where parties are untrusted and the data is highly sensitive block flow can coordinate such experiments by ensuring that individuals sensitive information is not leaked by detecting agents who are contributing poor models either inadvertently or maliciously and by being resilient to malicious attacks over the past decade machine learning has led to tremendous achievements in in all kinds of areas from self-driving cars to helping us understand the human genome it is so pervasive nowadays that it's being used in all kinds of things and we don't even realize that there's machine learning inside such as more effective web search or in our spam filters machine learning models basically build mathematical models based on some uh sample data that they're given you know this data is called trading data and then they use these models in order to make predictions or decisions without being explicitly programmed to do so these machine learning algorithms as you know are extremely powerful but require useful data insufficiently diverse data sets can lead to brittle models that suffer from inaccuracy or bias and it is usually very difficult for a single organization to collect sufficiently diverse information so one approach is to collect data from other organizations to build uh more robust more highly generalizable models the problem here is that we have data regulations such as hipaa and gdpr that restrict how data can be shared across organizational boundaries also the privacy of individuals within a data set is very hard to guarantee when this kind of sharing happens when when data is sent from one organization to another organization federated machine learning plays a very important role here it was first proposed by google just a few years ago i think maybe 2017 2018 something like that and it enables the development of a highly generalizable robust machine learning model across a collaborating organizations without requiring them to actually ship their data off anywhere so the data remains with the owner but the shared model can take advantage of the diversity of the combined data set of all these collaborating entities so this approach addresses the privacy concerns around sharing data but it also helps with the data diversity problem how does um you know federated learning work so let us assume that there are several hospitals that want to build a global shared federation a shared global machine learning model rather than send their raw data off to a centralized server each model trains its own each each hospital trains its own local model and at every iteration the hospital sends their local model off to the server the server uses these local models in order to compute the global model and then sends it to the individual hospitals this process repeats um until conversion occurs or some kind of stopping criteria is is achieved and this could be you know a certain number of rounds or a certain a certain limit on the expense or something like that though federated learning reduces some of the privacy risks associated with sharing data naive federated learning implementations are susceptible to several threats some of the most important ones are shown here by having a central data using a central trusted server to collect local models and compute the global model the system will have a weaker threat model and if the server is compromised then you have a single point of failure an adversary or a group of colluding adversaries can also affect the global model by model poisoning and lastly even though no data is shared it is possible to leak information from the local models output and parameters during the federated learning phase an adversary can run model poisoning attacks to uh um to to affect the the global model model poisoning attacks aim to to uh to modify local models before they're sent to the server so that the global model is affected or to insert hidden back doors into the global model random attacks mainly uh are aimed at reducing the accuracy of the global model but in targeted attacks the adversary's objective is to cause the global model to uh misclassify a set of cl clo chosen inputs with a high confidence in model inversion an adversary can use the local models output to infer values of the sorry um i'm sorry uh to infer our values of uh sensitive attributes used to train the model so for example given the model and some demographic information about a patient whose records are used for training an attacker could predict the sensitive attributes of the patient or the other example is provided on the slide some attackers were able to reconstruct images of faces used to train a facial recognition system in membership inference attack given a model and a specific data point the adversary infers whether this point was used to train the model or not did this point exist in the training data was it used for the training at all for example in if if health related records or maybe even images like mri mris are used to train a classifier discovering that a specific record was used for training inherently leaks information about that individual's health similarly if images from a database of criminals is used to train a model for predicting recidivism which is the probability that someone will reoffend a successful membership inference attack would in would expose an individual's criminal history in property inference the adversary tries to infer properties that are true of a subset of the training in parts so for example an adversary can learn the uh accent of speakers used to train a speech recognition model there are some defenses for these attacks and that's what i'm going to talk about next to help with the trusted server issue a serverless approach can be used so there is no single point of failure and it will become a trustless system in this case the collaborating parties will need to do what the server was was providing all this time which is computing and distributing the global model to address model poisoning local models need to be audited before they are used to compute the global model and any adversaries contributing poor models need to be identified and and held accountable lastly to address you know the privacy issue of information being leaked from local models we need to figure out a way of perturbing these local models so that the information doesn't leak this can be done using differential privacy which is a way of adding noise to results to prevent the identification of individuals in a data set we proposed an approach called block flow and block flow attempts to address these specific attacks it incorporates differential privacy into the local models to prevent adversaries from learning information about the local data sets secondly it supports decentralization and it does this by allowing learning and auditing to be done by the collaborating agents themselves instead of being done on any trusted central server and lastly it uses smart contracts to enforce penalties on malicious agents and to reward honest ones before i get into details about block flow i'd like to go over some of the technologies that we're using uh block flow uses differential privacy and it uses uh differential privacy to address model inversion and inference attacks so differential privacy is a mathematical framework that provides very strong privacy guarantees it allows adversaries to learn overall statistics of a data set without learning sensitive information about individuals that are in the data set so what it does is to add noise to the computations but the noise that is added is not completely random but based on the range of values in the data set so it's very carefully controlled noise that is added as the image shows differential privacy ensures that the presence or absence of an individual from a data set does not significantly modify the outcome the intuition here is that the output should not greatly depend on any single record and this ensures that an individual cannot be traced back from the computation differential privacy gives the database or a data set owner the ability to balance utility and privacy database owners can set the epsilon parameter for their for their data and the smaller this epsilon parameter the privacy last parameter the more privacy you can guarantee but this also means that more noise is added and because more noise is added the results are less accurate but by tuning this uh privacy loss parameter a data set owner could decide whether to provide greater privacy and less accuracy or greater accuracy with less privacy if this is a trade-off that they have to make there are three ways that differential privacy can be used in three main ways that differential privacy can be used in machine learning input perturbation which allows the data set to be modified before training uh objective perturbation in which the objective function is put up and output perturbation by modifying the model parameters in block flow the agents in a federation experiment can choose the perturbation they want while training but should ideally choose the same epsilon the reason for this is if all if all the collaborating agents choose the same epsilon the system can guarantee an overall epsilon for the global model if agents select differential different epsilons for their differentially private uh training the result will still be differentially private but the epsilon will be the max of all epsilons and the problem here is if even a single agent chooses a large epsilon the the the the the epsilon for the global model will also be this larger epsilon and the larger the epsilon the more accurate the results will be but the greater the privacy risks in order to incentivize good behavior block flow uses ethereum the ethereum blockchain to make monetary rewards and punishments uh ethereum blockchain is a decentralized open source blockchain featuring smart contract functionality so ether is the cryptocurrency that is used to uh reward uh computations that are performed to secure the blockchain uh so what is great about ethereum blockchain is that it provides an auditable immutable ledger as well as a scripting language and this turns out to be extremely powerful when we want to do things like accountability however it can be fairly expensive and slow to run computations on the public blockchain so in order to improve on the cost and performance of ethereum instead of asking or instead of expecting the local models to be shared on the blockchain block flow proposes the use of some hosting service some technology like either ipfs or some cloud provider to share local models even though ipfs can be substituted by another cloud provider i wanted to mention some of its details here because it was used as part of our initial implementation the main details of ipfs are that it is content addressable and that it is decentralized however all data is automatically public so this means if you want to if you want to share a private information or you want to keep something secure it needs to be encrypted secondly though redundancy is built into ipfs if all the nodes host hosting a certain piece of data go down the data effectively disappears from the network so these are the technologies we're using we're using differential privacy ethereum and some kind of uh storage some kind of hosting service now that we've seen what technologies are being used let's see how we put them together in in workflow here is a simplified workflow a block flow federation experiment consists of a set of untrusted agents the ethereum blockchain and a data hosting service and in this case we're using rpfs every federated learning round is made up of four steps we have training then we there's sharing auditing and averaging the block flow clients post a bond to actually start the federation experiment that's the first step in order to initiate a federation experiment all all the collaborating clients have to post a bond and this bond is posted in ether there are four parameters that need to be decided by these collaborating parties and epsilon value for the experiment which is the privacy loss parameter the per round refund fraction so this is how much uh how much of the bond is refunded at every round this is this is per round and not per party the reserve amount which is returned at the end of the experiment and the evaluation criteria training happens locally and encrypted models are shared via ipfs the models are audited by clients and the scores are submitted to the smart contract the the contract then computes the overall scores for each model and performs a refund of the bond proportional to the scores and this repeats for every round uh so this was this was how the system works at very high level and i'll go through the details of how each component works in the next few slides in the training step each block flow client trains a local model it applies differential privacy by adding laplacian noise it then shares the model shares its model local model with the other clients in the experiment and it does this by encrypting the model using diffie-hellman shared keys uh because as i said earlier uh ipfs is you know all data on ipfs is public by default in the auditing step each client retrieves and evaluates uh the models of all other clients uh it it calculates an evaluation score and then reports this to the block flow smart contract the block flow smart contract then uses this these scores in order to figure out the overall scores for each model in the averaging or the aggregating step each client retrieves the overall scores from the blockchain and then averages the clients model models based on those scores in the block flow architecture each client needs to evaluate every other client's model and clients are encouraged to use their entire data set for this evaluation by evaluating models in this manner block flow does not require public validation data sets and it also doesn't require a trusted server as mentioned earlier clients must also agree on which formula they're going to use to evaluate each other's models you can't have one agent using f1 scores and then and another agent using precision because then it's very hard to come up with the overall scores but you know what do we do what does block flow do about malicious agents how do how does block flow ensure that clients are honest with their evaluation that is they use the agreed upon formula and they actually provide underscores the next slide we'll talk about uh how we we kind of ensure that clients need to be honest with their evaluations so block flow ensures that clients are honest with their evaluation using this contribution scoring procedure this procedure is used to identify malicious agents who are either contributing poor models or are trying to get higher refunds by giving inaccurate scores to honest agents so there are two there are two kinds of uh issues that the contribution scoring procedure is trying to address uh i i want to talk about this procedure by using an example because this is very important part of the architecture so let us assume in this example that alice and bob are honest agents and chuck is malicious so chuck gives alice and bob a zero and scores his own model as a one the smart contract takes all these scores including the scores from alice and bob and computes the overall scores for each model using using this contribution scoring procedure so in in step one you know looking at the example for chuck in step one we get the median score and we do this so that we can guarantee a 50 malicious model in step two we we scale it so that we can compare it against other models step three we we try to figure out the least accurate evaluation for chuck and we do this in order to penalize agents for being inaccurate um and uh step four we check if the least accurate score is reasonable if it is not reasonable the the the client is again penalized for that and in step five step five is similar to step two but in this case we are comparing against the the best evaluator in this particular round and step six is a chuck score so the client's overall scores are the minimum of the the median score reported for their model as determined by all the other clients and the inverse of the maximum difference between their reported score and the median score for each model so the procedure accounts for both the clients poor model as determined by the scores of other clients as well as their inaccurate scores um so one one uh one thing to note here is when uh computing the global model at the end of each round agents are encouraged to weigh models in proportion to the overall scores received each block flow experiment must also deploy its own instance of the block flow smart contract on the ethereum blockchain at the beginning of an experiment as i mentioned earlier the smart contract requires that every client post a bond and this bond is done in ether which is the theory in cryptocurrency uh this bond is used to ensure that clients follow the protocol and to either reward or penalize clients for their contributions at the end of each round based on the overall score of the model the smart contract redistributes funds proportional to the client's overall scores malicious clients who don't meet the protocol are eliminated and lose their funds completely dishonest clients who follow the protocol but try to cheat on their scores receive lower scores as seen in in the earlier in in chuck's example and thereby they get lower partial refunds in short the smart contract uses the contribution scoring algorithm to provide accountability in blood flow we have in we have an initial implementation of block flow and for the block flow client we use scikit-learn to train logistic regression models we are using a library that we've developed of differentially private noise adding mechanisms in order to add laplacian noise and we share the model via ipfs so the ethereum smart contract is written in viper which is which then compiles down to solidity but also we've also tested this on a couple of data sets we ran several experiments to evaluate different aspects of block flow and i'll describe some of these next we evaluated our system on on two data sets the adult census income and the kdd cup data set two thirds of the data was split equally among the end clients and the clients reserved 20 of their data for training validation the remaining one-third was reserved for testing clients use their entire data sets when evaluating others models and all experiments used logistic regression models in the first experiment we validated that the median f1 score accurately estimates model quality and we we checked that the median of the agent's f1 score aligned with the one one-third test data f1 score we wanted to show that using the median is is a good indicator for scoring the quality of models in the second experiment we evaluated whether the contribution scoring procedure would identify and reward those agents with higher quality or larger data sets the assumption being that with higher quality or with more data you could you could get more robust models so in this experiment one agent had a disproportionate amount of data and that is indicated by the relative data set on the x-axis we compute the score of that agent by increasing the amount of data the agent had and as the data set size goes up the scores go up as shown in the figure the agent's model evaluation overall scores were strongly correlated with the data set size in the third experiment we evaluated collusion attacks uh specifically we we considered the case when less than half the clients award uh you know a score of one a perfect score to the other colluding clients and the figure illustrates how such collusion is reflected in the evaluation and scores as you can see in the overall scores the the honest agents have a higher percentile score so the honest agents are indicated so if you look at the at the the last figure the honest agents for in the kdd data set are are indicated by the red dots and the honest agents in the the adult income data set are indicated by the green i don't know what that is uh green green dots in the fourth experiment we explored model poisoning attacks uh we assumed a minority subset which is less than half of the agents submit models trained on either random or on inverted data so we independently sampled each figure with probability each feature with with probabilities from the real data set distribution to form data sets for random agents and for inverted agents data set we just flip the output labels like honest clients malicious clients split the data sets into 80 uh training and 20 validation uh this the the figure illustrates how the malicious agents scores were statistically lower than those of the honest agents for the smart contract we looked at both runtime and gas consumption if there are n clients in the experiment each client has n models to evaluate for each of the r rounds and the blockchain needs to record all scores for each round hence the complexity is of order and squared the gas costs are a little harder to estimate because it depends on market conditions however if we assume that clients are fine with slower processing time we estimate that it would take about twenty dollars per round per client to run a federation experiment on block flow blog flow was developed mainly to work in situations with untrusted collaborators who cannot agree on a central trusted server and who are extremely concerned about privacy uh i i i'd like to talk about its threat model next in terms of data privacy the system guarantees that n minus 1 colluding agents cannot infer information about the remaining agent because the local models are made differentially private when it comes to model sharing model sharing is done in ipfs which is public however ipfs is immutable so agents cannot change their models once they've submitted the location to the smart contract and lastly even though ipfs is public the uh block flow prevents them from being accessed by non-federation participants by requiring them to be encrypted before they are put on ivfs the the most interesting aspect of our threat analysis is related to the the contribution scoring algorithm the block flow contribution scoring procedure punishes those who submit malicious models as well as those that provide inaccurate model scores for others with collusion a group of dishonest agents could submit fabricated scores so we can we can consider the extreme case where a minority subset of malicious agents report perfect scores for their models and zero scores for all others you know for models from honest agents since there are strictly less than half malicious agents and only the median model scores i need to determine one's overall score the median score is guaranteed to be between the minimum and maximum scores reported by the honest agents and as any honest agent score is a fair evaluation it is impossible for colluding agents to affect the evaluation scores secondly the fabrication of scores will only punish those who attempt it and our contribution scoring a procedure enforces this the the contribution scoring procedure limits one's overall scores with the evaluation on which they were furthest away from the media so as long as the median scores are bounded by the range of the honest agent scores which are threat model guarantees by having a 50 uh malicious threat model fabrication is never optimal so it doesn't pay to be dishonest in this system let's move on to related work um blockchain platforms have been used uh to support federated learning in in some other systems some of them are serverless and guarantee a malicious threat model however several don't provide sufficient privacy so are susceptible to model inversion and inference attacks many of these systems require data sets to be publicly revealed on the blockchain and incur significant blockchain computational expenses for on-chain model evaluation these proposals are only applicable for small models with public data sets there are some other systems that propose a robust trustless and gradient-based validation schemes where only private gradient updates are revealed on the public blockchain these do support some form of accountability and are privacy preserving however the error increases proportional to the number of malicious agents so as the number of malicious agents goes up the accuracy of the model suffers it also doesn't scale well to large models because gradients are averaged on the public blockchain a blog flow offers many advantages compared to these other uh these existing works first it is both privacy preserving and trustless so it does not require any secret test data set it doesn't require uh you know trust set of agents or a trusted server and it doesn't require the revealing of data or weights on a public blockchain it also supports a robust threat model so long as there are a minority of malicious agents block flow filters out all malicious agents and that thus preserves the the the quality of the global model what i've described here is our initial implementation and evaluation there are several extensions we're interested in exploring namely we want to implement some smart contract optimizations to lower the gas costs we're also looking to see how to support other machine learning algorithms we are also interested in in improving the usability of block flow by providing easy to use interfaces and incorporating cloud-hosted ethereum and storage for local models so in conclusion federated machine learning enables the development of a shared machine learning model among collaborators without requiring them to actually share their data though this helps with privacy and data diversity problems naive federated systems are susceptible to several threats including model poisoning inversion and inference attacks the system i described today block flow is an accountable federated learning system that is fully decentralized and privacy preserving and is aimed at addressing these specific issues its uh primary goal is to reward agents proportional to the quality of their contribution to the global model while protecting the privacy of their data sets and it is also resilient to malicious adversaries initial experiments have been promising and we're looking to extend the system with some optimizations and support for cnns so you can find the paper it's on archive and if you have any questions or you want to see a demo or you want to have a discussion about a possible extension and improvements please feel free to send us email um thought we'd take a couple of minutes to go through some questions uh the first was um what what are the uh accuracy costs of um preserving privacy how much does this impact what the models are able to um achieve and and how much does um uh subdividing the models into local models also impact accuracy uh so that's a really great question uh so different as i mentioned earlier differential privacy is this thing that we're using for privacy and what it allows you to do is to kind of finely tune how much accuracy you are willing to sacrifice for uh for privacy and this is something so this this parameter the epsilon parameter is extremely context dependent so it depends on the data that you have so you know in terms of how how much accuracy you uh give up it's completely based on your data it's very hard to kind of say right now but there is definitely a trade-off you do in fact lose accuracy in order to obtain this um the privacy feature excellent and you had mentioned that you developed block flow to work with situations where the the collaborators don't trust each other how modular is the system could you move to a situation where uh maybe you have um you maintain the privacy preserving features but you drop away some of the the serverless technology in a slightly more trusted environment but where privacy is paramount uh so that is an interesting question and it's something that we haven't uh considered uh so yes so i think in short the answer is yes we could move to a trusted environment and use a central server but if we did that we would need all the additional functionality so if you're willing to give your data off to a server and trust it then you don't need to have uh you know you don't need to have the blockchain trying to enforce um uh malicious agents and stuff like that so it would it would it would it would make the model much simpler but the threat model would become much weaker excellent um the last question and it follows directly from that you mentioned the difference between a malicious um uh partner in this network and a dishonest partner um could you talk a little bit about how you determine the the difference between those are there situations where that boundary isn't entirely clear or where potentially um you know a collaborator that's just misconfigured their system is appearing dishonest where otherwise they might not be so uh when i when i talk about a malicious agent i mean some some adversary that is deliberately trying to affect the the you know either affect the global model or to kind of take advantage of other people's local model in order to do something it shouldn't be doing uh uh in terms of a dishonest uh uh client we are thinking more in terms of a client that is trying to game the system in order to take advantage of the ether to try to get more refunds back by modifying the scores it gives to honest agents or by you know a group of agents would come together and say we'll all give each other one and we'll get back more money than the honest agents so that is the difference a malicious agent is someone that is trying to deliberately affect the accuracy of the global model and someone who's dishonest is just trying to game the system excellent uh with that i uh i think we'll conclude the questions thank you very much for the presentation very interesting work and can't wait to see how it evolves thank you so much 