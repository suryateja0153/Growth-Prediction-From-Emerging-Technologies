 afternoon everyone i'm dave richardson  dean of liberal arts and sciences   and welcome to our program today today's workshop in artificial intelligence  or as we all call it these days ai   uh is on the topic of ethics and fairness and  artificial intelligence uh our presenter will   be introduced in just a moment but i'd  like to take just a couple of slides and   introduce you to uh a broader perspective of  ethics in a.i in the public eye these days   so uh my first slide uh is a summary chart from  a paper entitled ai in the headlines portrayal of   the ethical issues of artificial intelligence in  the media this uh paper was published this year to   2020 in march 2020 and it is a study of the  media references to ai ethics over the last   uh previous five years from 2013 to 2018.  as you can see there has been a huge climb   in media coverage of the issues of ethics  and ai and in this particular chart   the blue stripes section at the top of each  of the columns indicates the proportion of   the papers that are judged to be critical of the  the ai applications uh that uh were discussed in   the papers uh there are also many papers which are  highly complementary but a lot of the press is now   entering into a phase where it is balanced in its  view uh talking about both sides of the subject   we think that it's time for the ethics of  artificial intelligence to become more deeply   ingrained at the university of florida and in our  college because it is becoming an important social   issue i do want to point out that the journal  in which this paper was published ai and society   is a springer journal but it was first published  in 1987. so even in 1987 there was a recognition   that there were impacts of the use and potential  of artificial intelligence in our society   on the other hand what this shows what this chart  shows is that in recent years ethics and concepts   of fairness have become quite prevalent  in the literature or in the popular press   another data point that i'll show you is a  book which duncan purvis will mention here   later today but uh this is a book written by  kathy o'neil entitled weapons of math destruction   uh kathy o'neill's uh background was in  mathematics and technology and she actually   started in the finance industry making money using  the tools and techniques uh to understand and   create the data which would allow for positive  uh successful investments uh in the markets she   became disillusioned with many aspects of these  algorithms and the impact that they were having on   society back in the uh early 2010s and eventually  wrote this book weapons of math destruction   uh you can see on the bottom left there did it  that it won many many prizes uh from both the   technical press uh technical popular press and  uh the mass media markets like new york times   what's interesting is that here we are in 2020  this book is now four years old and you'll notice   on the amazon page today that is the number  one best-selling book in business statistics   it is the number one best-selling book in data  processing and it's the second highest selling   book in privacy and surveillance in society  so this this book is an illustration of how   pervasive this subject is becoming in both  industry and increasingly so in academics so with that i would like to welcome you once  again thank you for joining us and at this point   i will turn the zoom room over to associate dean  david ferries who will be our moderator today   david hi everyone it's a pleasure to be here today  with you and i would like to introduce our speaker   for today and then lay down a couple of the rules  we're going to be following um our speaker is   dr duncan purvis he has been assistant professor  in the department of philosophy since fall of 2017   his phd in philosophy is from  the university of colorado   and his areas of interest are ethics  overall ethical theory applied ethics   bioethics and what interests us today especially  ethics of technology and he has developed a   very very interesting sub-specialization  in ethics and artificial intelligence   one of the uh issues he has written about for  example is autonomous weapon systems that is   weapon systems that kill without human  oversight and he recently received an nsf   grant to investigate another very important  issue in ai which is the use of policing   algorithms so-called predictive policing which  can lead to discrimination against certain groups today i know many of you  will end up having questions   uh if you look at the bottom of  your screen you'll see there's a q a button you can register your questions at any  time during the talk i will be the moderator   of the question and answer session in the sense  that once the talk is over i will be choosing   questions from the among those that have been  submitted and i will ask dr purvis to answer them   given the amount of time that is available to  us so without further ado then i would like to   hand over the screen to dr purvis to for  his talk thank you very much dr purpose   thank you david uh hi everyone um  i'm just uh running to a last minute   zoom complication with my presentation but i  think we'll have it up in just a second here okay here we are and okay okay uh hopefully you all can  see that um david let me know if something   looks wrong on my screen um so  today i want to discuss some of the   emerging ethical issues facing artificial  intelligence and contemporary data science algorithms help determine who is admitted to college who's hired and fired where and when to police whom we date who gets  a loan what advertisements and discounts we see   and what stories enter our news feed designing  data science data science models responsibly   i want to suggest requires understanding the  ethical issues surrounding the data we obtain   and use the algorithms we employ  and their impact on people i'm going to begin the discussion  today with a very brief   sort of introduction or discussion of the subject  matter of ethics i'll then look at ethical issues   for the collection and use of a person's  data after this i will talk about ethical   issues that arise when algorithms are applied to  real-world decision-making that affects people's   lives and i'm going to try to illustrate each  point with examples throughout the presentation i hope that the presentation will make clear  just how challenging it can be to create and   use an algorithm that aligns with our values  indeed as we will see sometimes we must think   deeply about what those values really are i want  to close the discussion by discussing an emerging   and important critique of artificial intelligence  and data science which looks at the way that   existing power structures in society can be the  source of ethical challenges for ai and big data   so to begin what are what are ethics well that's  an entire uh course on its own um but one way to   think about ethics is as a set of principles  or rules that determine how we ought to act an   ability to think ethically is one of the features  of humans that distinguishes us uh from animals   um and uh you know as a philosopher i i like to  say that ethics are a cornerstone of civilization   now laws prohibit me from stealing  your car and this might motivate me   not to do it because i could get caught and  put in jail non-legal consequences might   also motivate me i might be unsuccessful  and you would catch me and beat me up   however in the end i think that a commitment  to ethical principles is what really stops me   from stealing your car ethics go beyond laws  or self-interest if you tell me a secret and i   promise not to tell it but then i do it's not a  crime it might damage my reputation with others   if they find out or i could lose your friendship  if you find out but these are not the only reasons   that i would give for keeping the secret i would  simply say because i promised i would keep it or   what i mean is given that i promised i would keep  the secret keeping it as the right thing to do   in this case a commitment to doing the right thing  motivates me to keep my promise people believe uh   people who believe that their only moral reasons  for acting are to promote their own interests are   what we might call egoists and people who deny  that there are any moral reasons for acting at all   are called nihilists now i don't think  very many people really are egoists   or nihilists and those who are  are not much fun to be around   most of us i want to suggest have ethical  commitments of some sort or another um ethical principles are often not encoded in  law there's no general legal prohibition against   telling your friend secrets however laws often  follow ethics insofar as laws are often created   to enforce our shared strong ethical commitments  and we might critique our laws if they no longer   align with our firm ethical commitments so  as we'll see in many of our examples the   ethically problematic features of data science  and ai applications often become apparent only   after the fact so the challenge is  how to anticipate and address those   uh those features those problematic features  before the application is deployed in the world um and i want to just flag that what i  plan to do today is just sort of give   a sampling of some of the the key issues um  nothing i say here is meant to be exhaustive   um and i'm going to raise more questions  than i give answers so uh but i still hope   that you find some of what i say useful in  your own thinking about about these issues so the first set of ethical issues i want  to discuss concerns the collection and use   of a person's information or data as that  information is called once it's quantified   and digitized um so data is constantly being  collected about us from our cell phone cameras   our phone's gps signal our social media  accounts and our web surfing activity   there's a lot of recent discussion around  the ethics of how data is collected   about whether we own it and should therefore  have control over how and whether it is used   for example the eu's general data protection  regulation gives individuals the right to ask   organizations to delete their personal data  so i want to discuss some cases in which   individuals data was collected or used  in ways that were ethically questionable before turning to those examples though i want  to describe one framework for thinking about   the ethical collection and use of data  principles of autonomy and informed consent   beneficence and non-maleficence govern ethical  conduct in human subjects research it is an   attractive idea to try to apply these principles  to data science insofar as data science deals with   human data subjects now these principles  that i have listed here are very general   it is therefore unclear whether they can guide  our actions without further clarification about   when and how exactly they apply in particular  instances but they might be helpful to keep in   mind when we're thinking about whether the conduct  in the following examples was ethically wrong   so according to the principle of autonomy  data subjects should have meaningful control   over how their data is used autonomy  also respect for autonomy also requires   uh informed consent that is data subjects  should also be given the opportunity to   explicitly approve uh use of their data  based on a full understanding of the facts   the principle of beneficence requires that data  be used in ways that subjects would recognize as   beneficial and the principle of non-maleficence  requires the data not be used in harmful ways   in the first example i want to discuss in 2012  researchers at facebook and cornell university   manipulated the news feed of selected facebook  users some users were shown more positive articles   others were shown more negative or sad articles  people who were shown more positive articles   posted more positive articles themselves  people were shown more negative articles   posted more negative articles uh in other words  they demonstrated uh a phenomenon that they called   that the researchers called emotional contagion now whether these findings  are particularly interesting   scientifically or socially is not really our  concern the interesting thing for our purposes   here is that facebook didn't ask anyone if  they wanted to be part of the experiment   this calls into question both the legality  and the ethics of of the conduct of the   experiment so according to facebook's terms of  service users relinquish the use of their data   when they sign in to facebook for data analysis  testing and research in defending their experiment   facebook said this research search was conducted  for a single week in 2012 and none of the data   used was associated with a specific person's  facebook account we do research to improve our   services and to make the content people see on  facebook as relevant and engaging as possible   a big part of this is understanding how  people respond to different types of content   etc we carefully consider what research we do  and have a strong internal review process there   is no unnecessary collection of people's data in  connection with these research initiatives okay so   based on facebook's terms of service what they did  does not appear to be illegal but was it ethical   um and stephanie are you able to bring up that uh  poll for me now um we'll just do a very simple one   question survey i'm curious about your intuitions  what do you think was facebook's experiment   ethically wrong or ethically uh okay after all  they do state in their terms of service that um   you are permitting the use of your data for these  types of purposes when you use their platform um oh it looks like i can take the poll too all  right let's do it i didn't know i could do that   um so facebook um so we can explore your reasons  the reasons you answered why you did in the q a if   you'd like to um so facebook social psychologist  adam kramer himself ultimately doubted whether   the experiment was the right thing to do he said  at the end of the day the actual impact on the   of people in the experiment are on people  in the experiment was the minimal amount   required to statistically detect it however  having written and designed this experiment   myself i can tell you that our goal was never  to upset anyone in hindsight the research   benefits of the paper may not have justified  all this anxiety all right there we go um   so so was the facebook experiment an instance  of human subjects research and uh what are the   implications for the ethics of that type of work  if it is an instance of human subjects research   what are the general implications ethical  implications for organizations such as facebook   and google um in human subjects research there is  a notion of informed consent which is required by   the principle of autonomy that is subjects  must understand what is being done to them   they must voluntarily consent to the experiment  and they must have the right to withdraw consent   at any time now it's far from clear that those  conditions were satisfied in the facebook case um   so if you do think that facebook that um informed  consent is required for ethical experimentation on   social media users there are several issues uh or  several challenges for acquiring informed consent   for the collection and use of a person's data in  experiments like the one that facebook conducted   first the terms of consent are frequently  difficult to understand and buried in fine print   most of us ignore the terms of usage and just  click through uh when it comes to websites like   facebook most of us ignore the term um oh a second  it's it's difficult um to control how data once   it's collected could be used in the future um  it's difficult to keep track of how it's copied   and reused and this calls into question whether  informed consent really was obtained by facebook   users when they signed the user agreement  could they have possibly anticipated that um that they might be manipulated in these ways   an important ethical question to think about in  this context then is whether the relationship   between data subjects and data scientists  or programmers and social media companies   is really akin to the relationship between  medical researchers and their subjects   that is do data scientists really have the  same sorts of obligations to data subjects   that medical researchers have to their subjects  and if not why not on to a second example um   during okcupid's uh love is blind day uh  okay cupid being the popular dating site   um uh okay cupid suppressed photographs  in user profiles so that customers could   not see profile photographs of  potential people potential matches   customers could still read what was written in  profiles for example their interests or their   their educational background or something like  that they couldn't see them and an effect of this   was that people had much longer conversations  with each other until the photos were restored   not surprisingly um in other experiments they  took people with low compatibility scores and   told them it was high and they took people with  high compatibility scores and told them it was low again we can ask was this was this legal  and was this ethical well okay cupid's ceo   was unapologetic for their social experiments  as he put it guess what everybody if you use   the internet you're the subject of hundreds of  experiments at uh at any given time on every site   that's how websites work and indeed google changes  rankings facebook a b tests uh user functionality   and recommendations and so on on the other  hand having a company intentionally lie to you   intentionally give you a wrong score particularly  if you're paying a paying customer is something   that many people consider ethically unacceptable  it's both deceptive and potentially harmful i want to raise actually to talk about  one more point here really quickly um   so it is true that on on websites you're  regularly manipulated in various ways to   find out uh for the sake of improving user  functionality so for example um back here   um here's an example of uh an  on the right side of the screen   an a b test uh simple a b test um so  it's two versions of a website one is um   one presents this sort of pre this discount  offer for pre-ordering the game simcity   the other does not and it's a very simple a b test  to find out if more people purchase the pre-order   uh with or without that offer on the screen right  so it's like a very very simple experiment but   nobody objects to that sort of experimentation so  we might ask what the difference is between that   and um and facebook and okay  cupid's social experiments i now want to talk about some challenges with data  and privacy protection privacy is a basic human   need we need a protected sphere of control over  certain information about ourselves and our lives   we need this control over our information in part  because controlling information about ourselves   helps us shape our relationships with other people   part of what makes my relationship to my wife  a special one is i choose to share information   about myself with her that i would not share  with my colleagues in a world where everyone   had perfect information about me this  selective sharing would not be possible privacy also protects us from bad actors who would  use our personal information to harm us it's not   hard to imagine the possible harm of being outed  as transgender on social media or having had an   elective abortion or having a history of substance  abuse or having contracted hiv and privacy is not   all or nothing we have more or less of it to the  extent that we control access to information about   ourselves when we lose control over access to  our personal data we lose some degree of privacy now as i said at the start of this uh of this  whole talk data about us is everywhere it's   constantly collected and this data is incredibly  useful both for researchers and for private firms   for example being able to use data about  individuals mental or physical health condition   current prescriptions past provision of  health care and demographic information   might be crucial for the development of novel  medicines or diagnostic medical tools the problem   is how to share the sensitive data without leading  to harmful consequences for the people whose data   it is a common thought is that de-identifying  the data is enough to protect privacy   by removing any information that would make the  data traceable to the person whose data it is   but what identifiable information must be removed  from data to protect the identity of individuals   in the massachusetts in a messages  re-identification incident the   general health insurance commission released  de-identified health records which included   information about individual zip code birth date  and sex and a woman by the name of latonya sweeney   used this information to locate the health  record of then governor william weld and she   was able to using only this information look up  his diagnoses and health prescriptions inferring   a great deal about his personal health history and  this event was ended up being pivotal in revisions   to the hipaa privacy rule to restrict the  disclosure of a person's full birth date and zip codes um now the question of how and whether data you  release is safe i.e whether or not you can be   identified using the release data is a topic that  has been recently well studied in the computer   science community one notion that has been  formalized is what's called differential privacy   which is a notion pioneered by researcher  cynthia dwork the aim of differential privacy   is to provide as much statistical information  as possible from data stored in a database while   introducing enough noise to the data to guarantee  that an individual cannot be identified by it so   differential privacy ensures for example that a  person can contribute their genetic information   to a medical database without fear that anyone  is anal anyone who's analyzing the database will   be able to figure out which genetic information  is is hers or even whether she's participated   in the database at all and it achieves a security  guarantee in a way that allows researchers to use   the database to make new discoveries and ideas  from this this research on differential privacy   are now finding their way into a lot of  different practices involving involving big data okay so i've discussed some issues related  to the collection and use of a person's   personal information personal data i  now want to turn to some ethical issues   arising for the use of algorithms in making  important decisions affecting people's lives so most algorithmic systems fundamentally  are classification systems that is   they're designed to classify subjects into groups  of interest when the algorithm is designed well   the classifications they make will  correspond to real world outcomes   therefore a simplistic approach to algorithm  development is that our only goal should be   to maximize classification accuracy right what  else could we want however i think things are   a bit more complicated than that the real  challenge i want to suggest is how to make   algorithmic systems promote or support human  social values including ethical values and   i think optimizing for this is much more  complex than optimizing for mere accuracy   one place where algorithms can go ethically  awry is when data used to train the algorithm   are not representative of the population we  want to classify um so one example of of this   is predictive policing um which is uh something  that i've i've been doing some uh some research   on recently predictive policing systems use  algorithms trained on historical crime data   to predict when and where crime is most likely  to occur this little uh um display on the right   side of your screen there with the red boxes  is what it is a display from the pred pole   uh predictive policing algorithm each of those  boxes is about a 500 by 500 square foot 500 square   foot box um identifying a high risk area of crime  a serious concern about some of these predictive   policing systems is that their forecasts are  based in part on arrest data and arrest data   are biased and hence unrepresentative and hence  an unrepresentative reflection of neighborhood   crime rates arrest data is susceptible to bias in  the sense that arrest statistics can be driven by   historical racial bias by police officers the  idea here is that police disproportionately   look for crime in black communities so  they find more crime in black communities   and so they arrest more people there we should  therefore expect algorithmic forecasts based on   arrest data simply to replicate any discriminatory  policing practices against communities of color   um a vicious what's what some people have  called a vicious feedback loop can occur when   algorithms trained on biased arrest data recommend  greater police presence in communities of color   once there are more officers in a community this  will yield more arrests which will be used as a   proxy for higher crime rates which are then  used to justify placing more police in those   communities and on and on the cycle goes so  here we have a case where unrepresentative   training data can lead to an inefficient  and discriminatory use of policing resources and so as we've just just seen algorithmic  classifications are increasingly used to shape   the life prospects of ordinary people it's therefore important that determinations  of these algorithms classify people in ways   that align with our shared commitment to  fairness for example we wouldn't think   it's ethically acceptable to offer one set  for a bank to offer one set of lending terms   to minority applicants and another set  of lending terms to white applicants   but as recent work has shown most notably  and popularly in the book weapons of math   destruction by kathy o'neil discrimination that  we reject in normal life can creep into algorithms fairness has been studied in philosophy social  choice theory game theory economics and law to   name only a few disciplines but measuring fairness  has become a hot topic in c and computer science now we can think we can distinguish between  two broad ways of thinking about fairness   in algorithms or trying to measure fairness and  algorithms so what we might call unfairness or   discrimination concerning an individual occurs  when an algorithm treats a member of a protected   group differently from an otherwise identical  individual not from the protected group um what we might call group unfairness occurs when  an algorithm generates a percentage of negative   outcomes for members of a protected group that  is greater than the percentage for individuals   not in the protected group and we will see an  example of this of group unfairness momentarily   but suppose you had a minority group in which  the smart students were seared towards science   and a dominant group in which the smart  students were steered towards finance now if somebody wanted to write a quick  and dirty classifier to find smart students   they might just look for students who  study finance because the majority is much   uh much bigger than the minority right  so this is a sort of easy way to find   uh more good students the problem is that not only  is this unfair to the minority because they will   be sort of excluded entirely but it's also less  useful compared to a classifier that understands   that if you're a member of the minority and  you study science you should be viewed as   similar to a member of the majority who studies  finance fairness in this context therefore might   require a kind of cross-cultural sensitivity and  unfortunately merely training individuals treating   individuals alike regardless of their protected  group membership does not ensure group fairness   understood as parity of positive and negative  outcomes for example suppose that you're looking   at college applications and you're thinking about  using test scores as your admission criteria   if you have two racial groups that  perform differently on standardized tests   then you won't get a representative  representative sample of one of those   groups in your incoming class if you only have  one threshold for the standardized test score   this will violate group fairness understood as  parity of positive and negative outcomes and   if you want to see a a lengthier discussion of  sort of these attempts to measure fairness and   and adjust algorithms to be more fair uh you can  read the book um it's called the ethical algorithm   the science of the socially aware of socially  aware algorithm design i meant to have a picture   of that in here but i don't um you can email  me for for the reference afterwards okay um so where algorithmic fairness is seriously called  into question is in the use of algorithms in   sentencing and parole these algorithms generate a  score predicting the likelihood of an individual   committing a future crime however these  risk scoring algorithms seem to lead okay more time okay um where algorithmic fairness is seriously  called in questions use of algorithms and   sentencing and parole these algorithms generate  a risk score predicting the likelihood of an   individual committing a future crime however  these risk-soaring algorithms seem to lead   to racial disparities in classification for  example propublica obtained the risk scores   assigned by one such algorithm to more than 7000  people um arrested in broward county florida   in 2013 and 2014 and checked to see how many were  charged with new crimes over the next two years   they discovered significant racial disparities  in the algorithms forecasting in forecasting   who would reoffend the algorithm made  mistakes with black and white defendants   at roughly the same rate but it made those  mistakes in very different ways the formula   was particularly likely to falsely flag  black defendants as future criminals   wrongly labeling them this uh this way at  almost twice the rate as white defendants   and white defendants by contrast were mislabeled  as low risk much more often than black defendants now this disparity is deeply troubling from the  perspective of fairness but i want to take a   couple minutes to describe how complicated it can  be to rectify this disparity in a way that aligns   with our other ethical values or commitments  for while the algorithm falsely flagged black   defendants as high risk at twice the rate as  whites it was also it was also racially neutral in   an important way first the algorithm did not rely  on race as a factor when generating risk scores   second and importantly the risk scores assigned  by the algorithm mean the same thing regardless   of a person's race for example both white and  black defendants who were assigned a score of   7 out of 10 on the risk scale reoffended 60 of  the time so when a judge sees the risk score they   need not consult the defendant's race in order to  understand what the risk score means in terms of   the defendant's probability of reoffending if the  algorithm did not remain race blind in this sense   judges would need to take the  defendant's race into account   when making decisions about pre-trial detention  but considering a defendant's race and making this   sort of life-altering decision seems to violate  our commitment to equal treatment under the law   regardless of race now the tricky thing is  this because defendants reoffend at a high   uh sorry because black defendants reoffend  at a higher rate than white defendants   52 percent compared with 39 and because black  defendants disproportionately possess features   that are predictive of future offending  some uh for example um criminal history   it's difficult to have a predictive system that  is race blind in terms of the predictive accuracy   of its risk scores but which also which  simultaneously avoids any racial disparity   in its false negative and false positive rates  so this result and this result this sort of um   unfortunate result or challenging result has been  proven in several places in the computer science   literature and i can send you those references  after the presentation if you're interested so   we face the following dilemma with respect to  fairness either our algorithm can mislabel black   defendants at the same rate as white defendants or  the algorithm's risk categories can mean the same   thing regardless of the defendant's race but not  both and this dilemma may not be resolvable simply   through further technical fixes resolving  the dilemma requires careful consideration   about how these two aspects of fairness fit  into our larger tapestry of social values moreover achieving equal error rates for white and  black defendants might mean raising the threshold   for being labeled high risk of reoffending  for black defendants but raising this   threshold could lead to mislabeling more  high-risk defendants as low-risk defendants   and releasing them into society where they  harm someone again so we must also balance   public safety against these sort of two competing  fairness concerns that we might be concerned about   and almost done here so i want to close  by considering an emerging movement   in data and ai ethics that goes beyond these  sort of technical and philosophical questions   that i've been raising so far proponents of  what we might call the structural approach   uh ask how new data technologies  interact with existing social structures   uh in order to enable unfair or unjust outcomes  so they're interested in sort of diagnosing the   root of some of these problems so on this approach  you cannot disentangle the technological systems   from the social systems in which they operate key  to addressing issues of fairness and justice is   addressing is addressing the distribution of power  in decision making about how data systems operate   for example it might not be a coincidence that an  overwhelming overwhelmingly white asian and male   tech workforce produced a racial facial  recognition technology that was remarkably bad at   identifying black faces or that disproportionately  labeled tweets by african americans as offensive   addressing structural problems often requires a  much more activist stance than the technical and   philosophical approaches i've been assuming  for example mutually nakandi now a fellow at   the berkman klein center for internet and society  last year authored and introduced to the us senate   the algorithmic accountability act which would  have the federal trade commission oversee and   enforce regulations about privacy and bias risk  assessment in artificial intelligence systems   the structural approach might also incur urge  greater effort to divert to diversify data   science and technology industries so that their  creations are more aligned with the interests   and values of all people so i hope that this brief  presentation has given you a sense of some of the   emerging technical and ethical challenges facing  contemporary work in artificial intelligence and   data science as well as some of the efforts being  made to address those challenges but addressing   the challenges will require collaboration it  will require collaboration between legal scholars   computer and data scientists sociologists  scholars and women's gender and race   studies and philosophy to name only a few  so uh thanks for your time i appreciate it all done david all right thank you very much um thank you very much i should have  my video started uh okay thank you uh   what we would like to do now is  ask uh is address your questions i   wanted to ask a question myself  to start with which is uh whether or not you think that people are highly  aware of these kinds of ethical issues do you   think that there's a kind of a void out there of  people knowing how to think about these questions   yeah um you know i i do so i i think that um in  the initial uh sort of uh wave and infatuation   with uh big data and artificial artificial  intelligence over the last couple of decades i   think that um i think that a lot of developers and  designers were kind of um uh shooting blindly or   or stabbing stabbing blindly when it came to  ethics and there was a kind of optimism that   sort of given that we're creating these  technologies to address important social   challenges like communication between people  um medical diagnostics and so on that they were   ethically good technologies but i think that what  we are coming to realize is that there are a lot   more challenges um associated with uh with data  with data science and artificial intelligence   than we appreciated but i think we're still um  i think we're still in the early days of sort   of figuring out how to think about these things um  and also about how to take those ethical insights   and then operationalize them in in the technology  itself so thanks all right well it seems that   people are really interested in your talking about  predictive policing one question says arrests   themselves are biased do you think that arrest  data can themselves be used for creating fair   algorithms or do we need external information i am  skeptical about our ability to use arrest data to   create fair predictive policing algorithms i i  um i don't think it's a good source of data um   i there are other options um that  researchers have floated um so for example   um some have suggested that what you want to do  is you want to use emergency calls for service   in in training those algorithms and um in  identifying high-risk places right the more   concentration you have of emergency calls  for service um the greater concentration of   crime you have in an area the nice thing about  emergency calls for service that is 9-1-1 calls   is that they're generated by the community right  community members generate those calls not police   out there looking for crime um and so uh  now interestingly um pred poll which is the   algorithm used by or it was used by the the lapd  until this past spring um that the lapd claimed   that that system did not use arrest data in  making its forecasts that it only relied on   emergency calls for service and crime reports now  there are interesting questions about how good   that data is how good any of that data is because  different communities show different levels of   trust in police right or different attitudes  towards policing in general and that might   affect on the extent to which they they call on uh  law enforcement to help them with their problems   thank you that's very complicated uh another  person is well um do the police really need   algorithms um are they less fair or more  fair than actual people it seems that people   transfer their their biases to algorithms  why not just stick with the people yeah good   um yeah no that's a that's a a very good question  um so when it comes to policing an interesting   feature of predictive policing is that it is  incredibly its efficacy is incredibly understudied   um so some of the one of the major critiques  of predictive policing systems is that   the the evidence base is just not there to confirm  that it's actually better at preventing crime   than human crime analysts um now the the general  idea uh so the attraction of predictive policing   for police departments um or a couple of them  are are that one um in times of budget cuts   uh having a kind of algorithmic system to  do a lot of the work for you allows you to   uh to do the work with a smaller group of  crime analysts and you might otherwise need   and also you know it's true that that that  algorithms or especially machine learning   algorithms uh which are sort of uh trained up to  recognize patterns in crime they can recognize   crime patterns that human analysts may not detect  and they can do it for a much they can do it more   quickly and for a much wider array of crimes  um and so that is the attraction but i would   i would say that as of right now the evidence  base is not there to suggest that predictive   policing is um certainly better than human crime  analysts thank you uh do you think that um just   just how conscious do you think that police are  of the problems that may exist in these ai systems well based on the fact that um so based on  the fact that many police departments are   uh discontinuing their subscriptions to a lot  of these systems um that suggests to me that   they are becoming aware at least of the public  critique um and are are responsive to that um   you know i was i i recently read um the lapd's uh  strategic plan for the years 2019-2021 and that   strategic plan uh you can find it online i can  also email it to you if you're interested includes   its primary activity of emphasis in the prevention  of victimization and crime is data-driven policing   so they stopped using predictive they stopped  using predpol but they're still very interested   in the use of data-driven policing what i did  not see in that document was a great deal of   discussion of the issues we've discussed here they  didn't talk about uh they didn't talk about bias   um or uh or transparency which is something i  didn't raise in this talk but um uh so i think   they're aware of it i don't know yet the extent  to which they're trying to address it but um   i hope in some of the work i'm doing  uh as a result of this nsf grant   we're we're hoping to reach police departments  um to try to sort of start a conversation with   them about these issues that's great boy these  are great answers uh here's someone who asked   something interesting uh could ai be used to  actually enhance our ethical practices it seems   like all the talk has been about how ai just makes  our prejudices worse is there any way you could   train ai to find prejudice prejudice in algorithms  for example and to root them out yeah i mean so the last question uh i want to say sort  of um i think maybe in principle yes   i'm not a computer scientist so the  technical features of that i don't   really want to speak to but i mean  um what i what i can say is that yes as i sort of briefly mentioned in the talk  um one advantage of using say predictive   or using algorithmic systems or machine learning  systems uh is that there is a sense in which   they're more transparent than human decision  makers now people often talk about sort of   the opacity or the lack of scrutibility of these  systems but you do know for instance in the case   of um the the risk-scoring algorithm compass uh  that we talked about uh that that scored the risk   of reoffending for um for for defendants um you  you the information about what factors go into   that risk scoring are publicly available so you  know what factors go into that system and you can   tweak the weight of those factors in order to  help fix or address some of the issues of bias   that is in some ways an improvement over human  decision makers because we don't know really   what's going on in say a judge's head when they're  deciding how risky a particular defendant is of   reoffending right are they are they more reliable  or are they less biased than the predictive system   we don't know we don't have access to their  mental states so that is a sense in which   we might hope that algorithmic systems  can improve on human decision making   that is a really important point i didn't  mean for this talk to seem so pessimistic uh   that i didn't think there were there were  ethically valuable uses of artificial intelligence   i asked you a minute ago if the police were aware  that there are problems with these algorithms   i'm assuming the writers of the the creators of  the algorithms by now they must be hyper sensitive   to the kinds of problems that they have created in  the past through their algorithms that actually uh made their own uh that included certain kinds of  discriminations in there do you think that people   who are creating algorithms now are using the kind  of research that's being done in ethics to improve   their products i do uh actually and um this  last one oop let me just move this back here um   so temnit gebru um uh is a research  scientist in at google um and uh she is on   google's ethical ai team so they actually  have an ethical ai team at google   um uh one of whose charges is to kind of address  these concerns that we've been raising here   so i think that some of the big the big tech  firms at the very least are highly aware of   this stuff and in some times and in many cases  they're leading the charge uh in in trying to   sort of incorporate these these ethical insights  into the design of the tech itself um so uh uh   microsoft has recently um released a suite of  tools um i think it's microsoft dave you guys   can correct me if i'm wrong um that is designed  specifically to help ibm ibm yep thanks ibm has   recently received a recent developed a suite of  tools um designed specifically to help sort of   uh ferret out and and correct um biases that exist  in in computer algorithms so there is definitely   growing awareness that's great so let's let's turn  to the thing we're all mostly interested in which   is education um here's a really good question what  do you think college students need to know about   ethics in order to be able to navigate the terrain  of um artificial intelligence both programmers and   students across many fields and majors yeah  um so i think one thing they need to i mean   i think one valuable thing that we can sort of  impart to students who are who are taking off   on a data science or or a computer science track  is just to sort of um make them aware that there   are ethical issues involved in the conduct of  their work um and and to sort of make it clear   where and how those issues arise um so i i've  been talking a lot about sort of the um the   sort of internal the the ethical issues that are  internal to the practice of data science right um   algorithms that give sort of racially disparate uh  results or something like that and i think that's   super important stuff i think it's also  important to sort of explain um or to sort   of get students to sort of understand uh what  the sort of why we ought to be concerned about   those types of disparities right what's so  troubling about those disparities right um uh   a set but a second thing that i think is  important that i didn't focus on in this talk   is to try to convey to students the idea that  with great power comes great responsibility um   and that firms like google and facebook  and ibm are they're really shaping um   society uh about as much as as any institution um  that we have and uh you know take um take sort of   facebook's uh ostensible impact on our elections  right um this is this is not sort of internal   to the workings of their of their algorithm but  it's sort of a larger question about sort of um   uh what are the larger social goals that we're  trying to achieve when we develop uh these   social media systems or this search engine or  whatever and and i think that's the kind of those   are the types of of of things that computer  science students and data science students   could spend more time reflecting on uh in the  classroom and outside of it that's great i   we're pretty much out of time i want to ask one  more question i don't want to wear you out so um   do you think there might be the chance that people  are using algorithms as a way of outsourcing human   responsibility um and that this could lead  to you know more authoritarian some kind of   an authoritarian regime in other words the  algorithm did it not us it's don't blame me   do you think that's a possibility absolutely i  think that's a very real concern um and i think   that you see evidence of that um so there was a  recent story by the tampa bay times um that was   a kind of expose on pasco county's sheriff's  office um using a an intelligent they called   it an intelligence-led policing program part of  which involved an algorithmic risk scoring system   um to in the story right the the expose  is that they appeared to be using this um   to harass um high-risk juvenile offenders so they  would spend a lot of time going to their homes   and and just showing up at their houses and citing  their family members for um minor offenses like   i don't know not picking up um you know fallen  limbs or something like that in their in their   yard and so the idea is that this this this  program was a kind of pretext for harassing   uh low-income high-risk kids um and in response  to the this expose you know the sheriff's office   said look we thought we were doing something you  know look we we're just relying on an unbiased   algorithm here right we're taking we tried  to take the bias out of it by just putting   the data into the system and then going where it  told us to go but what i think is happening here   is and i don't think it's intentional by this  by the sheriff's office at all and i don't i   you know um i don't mean to be overly cynical  but i do think there's a sort of phenomenon where   by implementing an algorithmic system this  sort of ins can sometimes um uh prevent an   institution like this like a policing institution  from reflecting on their fundamental approach   to what they're doing right the sort of whether  or not we should be policing in this way   by going to people's homes and knocking on their  doors and citing them for minor offenses in order   to root out their juvenile uh you know offending  children right um the algorithm doesn't answer   that question right and it can't so um okay well  i i would like to thank you very much for today's   workshop i think we all learned a lot a whole  bunch of people have hung on here right to the   very end and i congratulate you on that and i  think we've all learned a whole lot today and   i uh once again thank you very much for your talk  thanks everybody thanks for coming bye everybody 