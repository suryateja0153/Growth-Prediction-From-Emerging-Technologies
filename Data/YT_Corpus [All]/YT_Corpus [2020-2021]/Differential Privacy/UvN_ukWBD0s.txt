 [INTERPOSING VOICES] ANNA KARLIN: It's a great pleasure to introduce Abie Flaxman, who's a professor in the Institute for Health Metrics and Evaluation, and he'll be telling us about anticipating differential privacy in the 2020 US Census. ABRAHAM FLAXMAN: Thanks so much, and thanks, everyone, for coming. As Anna said, my name is Abraham Flaxman, or Abie for short. I am at the Institute for Health Metrics and Evaluation here at UW. That's in the Department of Global Health Metrics in the School of Medicine. But I used to be sort of a theorist. I used to definitely be a fan of a theory seminar. And I encourage you to ask questions and make comments as we go. I like to keep this very much an informal kind of thing, despite having gone over to the School of Medicine and starting to absorb some of their strange customs. I'll give you one custom that I've been getting from my colleagues in medicine and public health, which is a land acknowledgment. The research that was conducted that I'm going to talk about was conducted on the traditional lands of the Coast Salish people. I'll give you another School of Medicine custom, which is a disclosure, and this is a way for you to judge if there's some potential bias from big pharma or other actors in this work. This research was supported by the Alfred P. Sloan Foundation and the National Science Foundation. And other work I'm doing is supported by the Bill and Melinda Gates Foundation. And I also do some consulting work for some of these private sector actors on this bottom line. What I want to tell you about today is the 2020 US Census. And if you've been hearing about this in the news, maybe a couple months ago when people were fighting about whether there would be a question about citizenship on it, you might not have heard the exciting underneath-the-headline fight about using differential privacy in the 2020 census. And if you haven't heard that much about differential privacy, I'll be happy to tell you about that, too, because that is a theory thing. And it's a theory thing that's like, in my time as a graduate student to professor, has gone from not existing to a full thing from theory that's going to be used, which I think is so cool and exciting. And on the other hand, it hasn't made it into required multi-course part of the computer science curriculum, so I don't know who's seen a lot of it, who's seen a little bit of it, and who's maybe just heard about it before and not seen too much about it. Wherever you're at, this will be a talk, I hope, with something for you. And so where we're going from census through differential privacy is to bring it all together to how differential privacy is going to be achieved, we think, in the 2020 US Census. And not only how are they going to make things private, but what effect is this going to have on the results that they're going to produce that my colleagues say in School of Medicine, public health, social sciences are all a little bit nervous about because they rely on this for their next decade of research? AUDIENCE: When do they start doing the census? ABRAHAM FLAXMAN: They started already. AUDIENCE: Oh, really? ABRAHAM FLAXMAN: Yeah, so the decennial census happens every 10 years, and its goal is to count every person in the United States. And their goal really is to say, how many people are there, and where are they staying on April 1 of 2020? But because there's so many people, and some of them are so far and hard to get to, they have already started counting people in rural Alaska in January. So it's happening. And so I'll conclude just by talking about why I think privacy in the 2020 census might be a good thing for everybody, not just for theorists who are excited to see their theoretical results used in a wide-scale practice. And what is still to be decided and then bringing it back to you all, what problems might this inspire for you and for us to solve to set things up well for the next decennial census in 2030? Any questions? So I'm going to start with what is in the 2020 census and why might we be concerned about its privacy. AUDIENCE: Are you talking about the short form or long form? ABRAHAM FLAXMAN: Great question, and for those of you who don't know what it's about, I'm talking about the short form. I'll show it to you. It fits on one slide. It's short. And I will allude later to the long form census, also called the American Communities Survey, for which the Census Bureau has no idea how to protect privacy, and to allay the concerns of social scientists, has promised not even to try until 2025. In the short form census, also known as the decennial census, there are really not that many questions. The first thing we ask everyone is to print their name, and if they live somewhere else than where they are being recorded, and how they're related to person one, of which there are 16 choices. This is person two. If I was on person one, there wouldn't be any choices. They are person one. We then write down this person's sex, a binary choice, and this person's age and date of birth. And then we get into this interesting stuff, but there's not that much of it. Race and ethnicity in the 2020 census will be captured in two questions. Is this person of Hispanic, Latino, or Spanish origin, number six? And what is this person's race, number seven? And that's it. Those are the seven questions to be answered for 330 or so million people on April 1 of 2020. When Census Bureau folks think about what they're collecting here, they think about it in something that they call a database schema, and I don't know what computer science department will think of them calling it a database schema. But it's a table, and it's a table where they're going to say a location for each of these counts, one of 10 million census blocks, which if you think of those as actual blocks is not too far off. And for each of those 10 million blocks, they'll be able to tell you for one of two sexes, for one of 115 ages, for one of 126 race and ethnicity categories, which they make out of that question six and seven from the previous slide, for one of 17 relationships to person one, meaning the 16 that we looked at or being person one, how many people fit that description. So that's about 10 million locations times 500,000 distinct histogram cells. I don't know. What are we talking about? 5, 10 trillion values that they want to make account of. There's only 300 million people here. So when you go through even just which of those numbers are published-- because they don't publish all 10 trillion of those. But in something called the public law 94 table 171 book, they published 2.7 billion of those numbers. In summary file 1, which includes PL94-171, they publish another 2.8 billion of those, all told just in terms of the things that they publish because they think people really will actually be using them. We're talking about 7 or 8 billion measured numbers for 300 million people. So, like, 25 statistics per person, which we're going to get into. They want to maintain everyone's privacy while doing. But what do we do with these numbers? Because you may have been consumers of Census Bureau data and maybe not even realized it before. And this is a good time to test who's in the audience, because although this is the theory seminar, there could be social scientists who've infiltrated it. Who, show of hands, uses Census Bureau data in their research? AUDIENCE: I've touched it. ABRAHAM FLAXMAN: And who has read or used other people's research output that you know or think has used Census Bureau data? And who's heard of the Census Bureau before? So, yeah, it's a wide range, roughly as I expected. If you ever read columns by the FYI Guy in The Seattle Times, this fellow, Gene Balk really makes a lot of his columns out of Census Bureau data. Here's one that he did recently. He says, "For the first time this decade, a dip in King County white population, census data shows." And he's got these beautiful charts that he's put together and a whole bunch of prose that I didn't put on the slide that show the different interesting findings that he's found from-- this is the long-form census, the one that now is done as a rolling survey, the American Community Survey, every year. But the same sort of data that we've gone out, asked a lot, a lot, a lot of people, from which we can say we believe that from 2017 to 2018 the number of people who are Asian in King County has gone up. The number of people who are white in King County has gone down, and the other race and ethnic groups that he has included in this plot are in between. Now, for people who do use this data, there's a couple of aspects of it that are unique and are relevant because of how they require us to care especially about privacy. First of all, participation is mandatory. When you get something in the mail from the census in the next couple months, it's going to say on this envelope, "Your response is required by law." You can be fined for not filling it out. No one has ever been fined in recorded history, but you could be. This is not informed consent, human subjects research. This is something that everyone has to take part in. And so as social researchers, when we use this, we need to make sure that we're protecting people's privacy. We are allowed to use it if it's anonymous data, because if it was not anonymous data that didn't have informed consent, that would be not allowed under the kind of human subjects laws that govern human subjects research. AUDIENCE: So when you say response is required, is that only the case if you receive the form in the mail? So the reason I'm-- ABRAHAM FLAXMAN: Continue. AUDIENCE: So most of the issues that come from under reporting come from people who don't have a static address, necessarily, or I don't always reside in the same place. And so in theory, an administration could fine every homeless person for not-- ABRAHAM FLAXMAN: Yes, that is one of the reasons why these fines are not enforced. That would be cruel and useless. But the Census Bureau goes to great lengths-- AUDIENCE: I do know that. ABRAHAM FLAXMAN: --to find who lives in every household. Throwing back to that earlier comment about citizenship questions, it's going to be extra hard this time around, I bet. AUDIENCE: How do they handle homeless people? ABRAHAM FLAXMAN: There is something called the household and group quarters status. And so a lot of what we just looked at is the form that goes to households. There are also forms that go to what's called group quarters. That includes homeless shelters, nursing homes, college dormitories, prisons and so there's a special extra effort to get people who are not in traditional household residences. AUDIENCE: What if a homeless person doesn't living in any of those things? ABRAHAM FLAXMAN: Yeah, well, it's always in flux. This time around, for example, people will be able to submit forms electronically. So someone could, for example, go to the library and submit their census information through a library computer. AUDIENCE: Do any census people go to homeless places and interview people and so forth, fill out forms that way? ABRAHAM FLAXMAN: I don't know enough to answer that in great detail. But I will connect it up to where we're going later, because this is getting at the kind of vulnerable populations where protecting privacy is-- AUDIENCE: --very important. ABRAHAM FLAXMAN: --particularly important. And that brings us to a unique feature two of this data. Besides the ethical imperative to protect privacy and the requirement to do it if we want to do human subjects research on it, it's also law. And you will hear Census Bureau people talk about title 13, because that's such an important part of their considerations. And this is the United States Code Title 13.9.2 that says, neither the secretary nor blah, blah, blah, anyone working for the Census Bureau, will do anything that blah, blah, blah, makes a publication whereby the data furnished by any particular establishment or individual can be identified, which I call out in part, because this was written a long time before differential privacy was defined. But for those of you who already have the definition of differential privacy on your mind, it sounds a lot like how differential privacy was defined. And I'm not a lawyer. I can't tell you that that is what this means. But as we get into how differential privacy has been defined is going to be used for protecting privacy and census, keep in mind, this is the legal requirement that Census Bureau is operating under when they're thinking about how they must protect privacy. Let me give you a creepy picture that's a reminder to me why privacy of these things are important. This is a map that I found on the UW archives of some maps of segregation that our colleagues in the sociology department were making in 1939. And they have these for lots of different groups. Last time I was giving this talk, I was here. This is Seattle. This is actually a very detailed map of the neighborhood that I live just off the map of in Seattle. And while I don't see people making these creepy maps day today, it's all embedded in census data. Here overlaid on the old 1939 map is a map that I got off of the Census Bureau's website showing census blocks, so that's why I said thinking of these blocks as actual blocks won't be too far off. And zooming in on that block, here, from the decennial census of 2010, are marked with little circles the same style thing, now for the households, where the head of the household is American Indian or Alaskan Native, AIAN, in the census parlance. And I was giving this talk at the Urban Indian Health Institute, which is there in census block 1024 of census tract 91. So that was very, this is the people in the room being mapped in a way that people aren't making a map of, but right there in the table if somebody ever wanted to. So I think, to me, it's clear why there's a moral imperative to protect privacy in this. There's also a legal requirement to. Also, for people who want to use it for social research, they need to know or believe that it is anonymous. So timing-wise, making great time. Feel free to keep asking questions as we go. AUDIENCE: If someone cannot read or write, how can they fill the form? ABRAHAM FLAXMAN: They are allowed to get help filling out a form, and so especially children will have the form filled out by their parents. Not just anyone can help fill out a form. Because of title 13, it is supposed to be a census worker who helps someone fill out a form if it's not someone from their household and someone in the census who's doing that is sworn to that title 13 secrecy. They will not reveal what they found from that person. Other questions? Differential privacy-- you've been very patient with me, theorists. Let's talk about theory a little bit. So the most important thing, not for you-- this is for if you're talking to social scientists who are nervous about this, and they are not clear on what differential privacy is. The most important thing for you to get them to understand is differential privacy is not an app. It's not a thing you can get from Microsoft and run. Differential privacy is a definition, not an algorithm. And if you haven't seen this intuitive definition of it before, the idea we're trying to get at with this definition is that regardless of external knowledge, someone with the results of this calculation will draw the same conclusions whether my data is in this database or is not in the database. And this is formalized in a paper, in part by [INAUDIBLE] student, Frank McSherry, with this mathematical definition that a randomized algorithm A is called epsilon differentially private if for each possible event P, for any pair of databases that are the same everywhere except for on one person's data, the probability that the algorithm will run on one database D is in P is less than or equal to E to the epsilon. The probability the algorithm will run on D prime is in p. I wrote equals because I don't want to talk about the events, and sigma algebras, and stuff when I'm talking to social scientists. I forgot to change it for you all, but you know what I'm saying. So another know-my-audience kind of thing-- who has done research on differential privacy? And who's seen this definition before in their studies in computer science? And that is cool. For those of you who are seeing this for the first time, I thought I should prove something. This is a theory seminar, after all. And one cool thing about differential privacy that, even though I've been out of the theory business for a decade, I think I can prove in front of an audience is the post-processing theorem. And what this says is that if you have an algorithm A differential private and you have any function that takes that output and does something to it, then that composition, the function applied to A, is also epsilon DP. Has everyone's seen this before? Is this something that does yawn to prove? Or is this like, I'm doing real theory in the theory seminar to prove? I can't tell. I'm going to prove it. I'm going to try. AUDIENCE: [INAUDIBLE] f? ABRAHAM FLAXMAN: Well, it's a function. It's got to take whatever the output of A is. So what is A? It's an algorithm. You give it a database and out comes-- let's say that it's some n-dimensional real value, but if you wanted something else, we probably could do it with anything. And then f is going to take an n-dimensional real value and give us something else. And we're not saying that it's computable or anything. It's just f's and anything. And this notation, which maybe is a little bit sloppy, means you run A on something, and then you run f on what comes out of it. And that is, itself, epsilon differentially private. Well, what is the probability that f of A of D is in some-- I forget what I called it P at all, output, property? Why? That is the probability that A of D is in the pre-image of that property. Property? Is that OK? And that-- well, it's just some other event. I can do that for any event-- has my epsilon DP property, because it's A. It's less than [INAUDIBLE] to E to the epsilon times the probability that A run on that other database is in that weird set. So bring it back to the other side. Then you got it, pre-processing theorem. You can tell I'm very happy with myself for being able to prove something after all this time. Nobody in the School of Medicine wants to prove anything. Or it's a different standard of proof. Now, what is a database? When I'm thinking about this, I have a particular kind of data in my mind, and it looks like this. This comes from the 1940 decennial census, where there actually are ledgers, where census workers wrote down everything by hand, that because it's been so long since 1940 are now available. You can look at them. And in epidemiology, my colleagues sometimes call this a line list. In health services research, they sometimes call it individual patient data. It's got a lot of different names in a lot of different places, but visually, digitized, it's a bunch of rows, 1.7 million rows. This is probably Washington state in 1940. And each row is a person, and they've got columns for their age, and their Hispanic ethnic identity, and their race coded in the strange racial categories that they used in 1940, and so on. And so the kinds of things that our epsilon DP algorithms are to do are to count things from data like this. This is very much a health kind of example. There's a program called Stata, and if you have never heard of it, you're lucky. It's a data analysis program that is very easy to teach, but it's a very shallow learning curve that then just hits a wall, and there's no way over it. If you wanted to count how many people were in each county in Washington state from that data, though, you type "tab county," and it tells you. And if you are thinking about this kind of epsilon differential privacy for the first time, here is a test-your-knowledge kind of test. Is this algorithm-- count how many people are in each county-- differentially private? And just so we know it's real data, this one down to the bottom, 770, is Yakima. It's a county in Washington state. In 1940, the census counted 99,424 people in there. Did you think it was epsilon differentially private? No, it can't be. It's a deterministic algorithm. No deterministic algorithm can be differentially private because when you change the input data, the output data changes. Maybe there's no algorithm or something. AUDIENCE: [INAUDIBLE] ABRAHAM FLAXMAN: Yeah, good, I'm back in the theory world. There is one that's differentially private. But to make a count like that differentially private, you add noise. So go and use R. There's something that I thought also wouldn't come up that much in the theory talk. But this is my attempt to write in this cool, new R notation. That way, you can pipe data around. And so you tab the county, and then you add noise, and the noise has to have something to do with epsilon to make it epsilon differentially private. And out comes counts that are a little bit different. This table comes from the Census Bureau, so it's a little bit more complicated than tabbing county and just adding noise. And I'll tell you how it's more complicated soon. But the important thing is, in Yakima, where the exact count was 99,424, depending on epsilon, we got something that was a little bit lower, or a little bit lower, or as epsilon gets bigger, gets much more and more closer to the value there. This is one part where theorists should know how people take your results and do weird things with them. Everyone in this room knows epsilon is a small positive number, but when you take that out to the theory of the theory department and into the Census Bureau, you got to make sure they know epsilon is positive, first of all. And then they'll do things like say, well, what if we make epsilon 8? Well, no one said epsilon had to be smaller than delta. Yeah, epsilon equals 8 is a legitimate application of this definition. It's just not what we expected them to do at all. And so it obeys a theoretical privacy guarantee, but what does that mean? That's not exactly a theory question. That's between theory and applications. We'll throw back to it later. What I want to tell you now is how Census Bureau plans to actually do this for-- what did we say-- the 10 trillion values that they have under consideration for what they might publish. Because in short, what they're going to do is take geometric distributed noise, which is to say like the Laplace noise that you might often see in these kinds of things. But let's say we have some count A sub i, where i is between 1 and 10 trillion. That's the precise count. And then form B sub i by adding in geometric noise with some parameter z that's carefully chosen based on epsilon. And if you're not familiar with geometric random noise, the probability that G of z is equal to K is proportional to E to the minus z times the absolute value of K. So it's like a discrete version of the Laplace noise. That is almost all there is to their approach, except they've made it look very complicated where they say, create the histogram. Make the 10 trillion measurements, and then add noise to them. And then optimize something. And then people allocated to states-- and then optimize something. And allocated to counties-- and optimize something. Tracks, block groups, blocks and houses-- There's two things that are hidden in this inscrutable diagram. They get at what they're going to do beyond just add noise to 10 trillion values. First of all, they're going to do it for more than those 10 trillion values, and they're going to do it in a way that's geographically nested, hierarchically. First, they'll just do half a million values for the entire United States, and then they'll do 50 times that work, once for each state. And then what optimize is getting at is if they just did that, they would be inconsistent. So let me put a L up here for what level we're at. And then they'll optimize to enforce consistency between the 50 times half a million measurements for each state-- AUDIENCE: So if you added them together-- [INTERPOSING VOICES] ABRAHAM FLAXMAN: --and the national measurements. And they want to be able to add them together so that they add up right. Oh, and they want them all to be positive, well, not negative. No negative numbers of people in any of these histograms cells. And that is, in part, because they don't want people making fun of them for having negative numbers of people, but also because at the very end, they want to be able to take these histograms cells and turn them into a 300 million row file that is a synthetic file of people, which means they also want them to be integers. So that optimize includes step one, make everything consistent, and step two, also make them integers. AUDIENCE: So if you didn't this, you would be actually potentially getting more information by combining-- if you had two fuzzy counts, combining with another fuzzy count you see they don't add up, you'd be able to get some sense [INAUDIBLE]?? ABRAHAM FLAXMAN: And there's a computational limit in here, because they know that with 10 trillion measurements at the block level and still, millions of measurements at the higher levels, if they put those all together in one grand synthesis, they'd squeeze the most accuracy out of it. They don't have the computational wherewithal to do it. I don't know how to do it either. And so what they have done by breaking it down this way-- this doesn't actually say "breaking it down this way," but it's got the same figure that breaks it down this way-- is have a natural way to paralyze all of this. And so for example, of those of 50 states are each broken out into counties, but you can use 50 computers to do each one separately. AUDIENCE: We don't really know how to optimize most things with constraints. Some people like subject to differential privacy, so. ABRAHAM FLAXMAN: Well, the great thing about this is-- AUDIENCE: [INAUDIBLE] ABRAHAM FLAXMAN: --post-processing. We've got differential privacy from when we added the noise, so now all of the optimization that we want to do is not going to break differential privacy. I didn't mention two important differential privacy things that go into all of this. And if you are getting a first time that you've ever really gotten interested in differential privacy, which is kind of where I am, from seeing it used in the census, you will want to know two important tools for doing all of this, which I will not try to prove in front of a live audience. But they're both in Frank McSherry's paper. What's his paper called? Frank McSherry's 2009 paper, "Privacy Integrated Queries." And they're in other places too, but he plays them out very nicely there. First of all, sequential composition-- if you have algorithms A1 and A2 that are respectively epsilon 1 and epsilon 2 differentially private and you put them together-- and figuring out how to say this in concise terms is a little bit of a challenge, but let's just say, G of A1 and A2 is a general sort of function that puts together the results of running A1 on something and A2 on something. That will be, if this, then is epsilon 1 plus epsilon 2 differentially private? This is a fun one to prove. If I wasn't feeling nervous about proving things in front of an audience, we could probably do this one. Another one that's a little bit of a challenge to write down and to prove, but also, a good, fun one for you to figure out on your own if you get into this stuff is a parallel composition theorem. Because this one's totally general, but it adds. If we were a little bit more precise and said that we were going to run A1 and A2 on disjoint subsets-- which I'm not really even sure how to write down for these purposes. But A1 is epsilon DP. A2 is epsilon DP. Then A1 on D1, A2 on D2, where D1 and D2 are disjoint, is also epsilon DP. And I guess I had that in mind when I was talking about all of this, because-- I didn't go through the details, but taking a count and adding something to it is epsilon DP. And then doing that for all of these different disjoint counts for the 10 trillion different things we want to count in census is parallel composition. That's still epsilon DP. And then putting things together between different levels-- well, that's not parallel. There's things that are in the state that are also in the county. That's sequential composition, and so there we're going to have to add up the epsilon from level one and level two. And that is the art of breaking down the epsilon between these different levels. The convex optimization to put it all back together is-- I didn't write it on here. Is it on my next slide what it is? I my next slide it says, write the convex optimization problem in the general form. You want to see what it is? We should see it. And I've heard that the video will actually show what I'm writing if I write it over here, so let's try that. So what they're going to do, when they've got A sub i for each level, is the precise count. They're going to form the B sub i for each level, which is the-- what did I call it on the last side? I just wrote down what it was. A sub i plus some geometric noise, where z is carefully chosen so that it's all going to add up to epsilon-- AUDIENCE: Maybe use a darker pen. ABRAHAM FLAXMAN: You can't see that? Maybe I should just go back to the board where I have that already written down, yeah, like that. The optimization is going to find C-values. And there is a bit more detail in it than seems right to drag you through on a whiteboard, but in short, we want to minimize the sum of squared differences between the Ci values and the noisy measurements in the Bi values. Subject to constraint's about the Ci values in the previous level, which for something simple, we could just say, the sum of the Ci values for this level for all the i's is equal to a single C-value from one level higher. And that is something they can solve. Oh, yeah, we don't want to be embarrassed by having any of these values be negative. So this is, in the econy/statsy lingo, where the Census Bureau folks are coming from, non-negatively squares. Or the way I think about it, constraint convex optimization. We've got a convex objective. We've got a linear constraint. We've got an inequality constraint. We can do that, in theory. A really cool thing that's happened since I was in theory department and now is we can also actually do that with open-source software, and so I was very pleasantly surprised to find, not only can we write on the board this and say there is a polynomial time solution, but we could also head back to our desks and get a computer to do this for 500,000 samples in a reasonable amount of time. AUDIENCE: Would you want to bound the absolute difference of Ci to Bi? ABRAHAM FLAXMAN: Yeah, but can you? What would you bound it as? AUDIENCE: I don't know, the absolute difference that was on epsilon Bi or something. Because you can protect one of the Bi's too much. ABRAHAM FLAXMAN: And that's actually the problem where I'm heading with all of this. So let me show you first, for something where it works well, how this might play out. Because remember, the FYI Guy's column? I told the FYI Guy, I know everyone's worried about this differential privacy stuff, but I'll check it out. If they added the amount of noise that they're talking about to the census level, county level measurements that went into your story, here's how things might be. And this is a hypothetical outcomes plot, so the way it's wiggling around is all the different possible values that might have come out of his plot. The story is always the same, that the number change from 2017 to 2018 is going down for the white population, going up most sharply for the Asian population. I was doing this, I told you, last week at the Urban Indian Health Institute, where, actually, there is a difference between the direction that the Native American bar goes in these different hypothetical outcomes. But for the most part, the story he was telling stays the same. That said, that is not the case in all of the interesting things that people want to do with census data. And so the big open problem right now, which is borderline between theory and practical problem, is, how do we tune this up to get something that's acceptable, that's as best as we can get, that's good enough to use? Now, I already alluded to this, but that, by the way, that I was just looking at comes from the American Community Survey. Census Bureau has promised that they're not going to try do differential privacy on the American Community Survey until at least 2025. But we're not talking about six questions there. This is a 20-page long survey. It's got 44 questions. The questions have multiple parts. They start with, where was this person born? They end with, what was their total income in the past 12 months? And how to do DP for this in a way that's going to be acceptable to the social scientists? Huge, huge problem, not a nice, hand-off-to-you little problem. AUDIENCE: Is race considered a [INAUDIBLE] categorical variable? This is something that I should know, but I don't know the answer to. ABRAHAM FLAXMAN: Race is going to change, and currently, there were six categories. Well, there were a bunch of things that you could mark, and it said, mark as many as apply. It's not Latinx. If you're Hispanic or Latino, that's a separate category ethnicity, yes/no. And then white, black, Asian, American Indian/Alaskan Native, some other race, Pacific Islander are six different categories. AUDIENCE: [INAUDIBLE] correlation in any box you check. ABRAHAM FLAXMAN: And so those six categories are described in any combination, of which there are 63, because you can't mark nothing. So if you mark nothing, they come up with something for you. AUDIENCE: So when you were showing the wiggling stuff, what kind of values of epsilon were you using there? ABRAHAM FLAXMAN: In making this, I simply added geometric noise with epsilon equals 0.01. And so that's basically saying, I'm not going to figure out the optimization. Nobody knows how to do a [INAUDIBLE] anyway. But roughly the amount of variation that they see in the census tracts at this point, based on an experiment Census Bureau did with data from 1940 made me think that would be around the kind of wiggles that we would see. Since then, they actually have run their approach with a proposed set of epsilons that they might use on data from 2010, and so I could go back and update that based on seeing more closely where they might be headed with this. And actually, I've got a few slides that show this, which I'm going to do in five minutes or less, because I told you they ran it on data from the 1940s. And in the 1940s, things were a little bit different than they are in a modern census. They didn't have census box, for example. They used enumeration districts. And in 1940, the University of Washington was in enumeration district 41. It was mostly white, and the enumerated value was exactly 1,955 people who marked race as white in the University of Washington enumeration district. For different privacy budgets that they tested out in this early test run, you can see how the size of the population in four different runs of the algorithm is different than the exact value of 1,955, for epsilon equals 0.25, which from my computer science days, seems like a large epsilon-- you see that kind of variation that they get there-- and from epsilon, equals 8.0, which doesn't sound like an epsilon at all to my computer science ears. You see that things are much more precise. They're still a little bit varied around that exact value. And for other races that are also in the 1940 census, where the number that was counted exactly is smaller, the variation is still roughly the same size. Although it never goes negative-- and that comes back to one of these major problems that when we're thinking about how we might want to tune up this optimization is a major thing we have to take into account. Because, going back to how this might work for, say, the AIAN counts that I was looking at when I was at Urban Indian Health Institute, if it works like this, they're OK with it. In the precise count in 2010, in this census block, census tract, there were five people, and there were 3,110 in the blocks that had more than 0 people in either of these versions. And in the perturbed one, there were four. So it's a little bit different, but roughly the same, and some were in different blocks. However, in many of the other census tracts, there were originally more and ended up with less, including the block around that institute where there were the most AIAN-headed households. And where after differential privacy was applied, there were none. And so this is the in-between pieces that Census Bureau wasn't really focused on when they were developing their approach. But as they put it out there for people to vet, very quickly it became clear-- and this is work by Professor Randall Akee from UCLA, who is particularly interested in what's happening in Indian country-- and found that, OK, if you look at the official report on the x-axis, for this is the population size of AIAN, alone, meaning they ticked that box for race and no other, and then in this one, what happened in the demonstration product where they applied differential privacy-- At this high level, it looks like things are pretty close to the line y equals x. But zooming in here by restricting the axes just to go up to 5,000, it looks like the points are always below the line. That is to say, the differentially private count is pretty much always lower than the value that was published in 2010. That is bad, and it's nothing intentional. There was no malicious part of this code. They didn't do anything special about AIAN in their code. It's just the way, somehow, these different pieces have come together in the size of these different values and the optimization and the constraints that has led to this bias. AUDIENCE: Could you, instead, [INAUDIBLE] thinking about [INAUDIBLE] squared error look at some multiplicative term? So I'm not saying we should do this, but an additive term of 200 probably matters a lot if you have the sample size of 20. It probably matters not at all for a sample size of 2 million. ABRAHAM FLAXMAN: And a lot of the appeal of this approach is when we're very zoomed out, and we're saying, so in big numbers, we're going to be very close. But yeah, I think that there is something to be said about, at some point, you want to focus in more. And on the other hand, for accounts of 0 and 1, multiplicative is very hard. AUDIENCE: So is this basically a function of this non-negativity, that you've got a small number, you got to have this small amount less than the value [INAUDIBLE] to all the possible, big things you could add about the value? Is there that what's going on? ABRAHAM FLAXMAN: It's got to be. But I don't know how to make that precise, let alone fix it. I'll just show you some of the evidence I've got, though, which my colleague in the back row, Beatrix Haddock, put together, which is replicating this work for the urban American Indian and Alaskan native counts. Because Dr. Akee was looking at what was happening on reservations, we repeated this work looking county by county at was is happening in the census blocks that are coded as urban county by county. And we see that same sort of pattern. There's more blocks. There's more noise. But in the census tracts, by and large, when we zoom in, the differentially private output is below the value that the official counts produced in 2010. If we switch the same picture to look at rural, it's above, and so there's some sort of counterbalance here, probably, I think, to do with non-negativity, fixing this up. Question? AUDIENCE: What is a dot here? Is it a block? ABRAHAM FLAXMAN: It is a county. I'll look at Beatrix to make sure I got that right. And so each county may have urban and rural blocks in it. And this is looking just at the aggregation of the blocks in each county that are considered, in this case, rural. Home stretch-- because especially when I give this talk to social scientists who want to use this data, I have really scared them by this point in the talk. I do want to say that privacy in the 2020 census might still be a good thing, and it still remains to figure out the details of how they're going to run it, including details of, perhaps, how they do things in the optimization, but certainly, what epsilon they're going to pick, how they're going to break it up between different levels. And so fundamentally, it's a good thing to know how they're doing this kind of disclosure avoidance. Disclosure avoidance has always been happening since the 1970s US Census. And I didn't tell you about how they used to do it, because it's not differentially private, and it's a privacy approach that requires secrecy. So they can't tell anyone how they did it. And if you've taken computer security classes, that is not considered a best practice. Everyone has been operating under the assumption that it doesn't add much error, but we don't know. So a lot of the frustration on the side of data users is from, we didn't know, so we didn't have to worry about it. I guess, in theory, it's good that you're telling me just how you're doing it, that it's going to help us get more truth, but it has really added to the amount of things I need to worry about. And besides that we'll know what's happening, we also have an opportunity for releasing more data than has previously been released. For example, all 10 trillion of those measurements could now be published. They're all differentially private. We could even publish the before and after versions of those so that we could say, here's some research data where it says there's a negative number of people. Use at your own risk. You have to understand why that is inappropriate in some places and have ways to fix it. But it could be available to researchers to have all of this intermediate data. And there's a gazillion other parts to this, because this is a very stripped-down version of the optimization they're really running. That will have to be figured out before they really do it, such as overall epsilon, how to split this between different levels, at each level, how to split it between those 500,000 what they call "detailed queries," and then other aggregate queries that they bring in as additional either constraints or terms in their objective function for their optimization. Oh my goodness. I went this whole way without saying invariant. Ask a question, and then I'll come back to invariant, which is one of the funnest things for theory. AUDIENCE: Yeah, so my question is about the definition of privacy. Have they decided to go with differential privacy? ABRAHAM FLAXMAN: Absolutely, 100% in for classic epsilon differential privacy. And so some of the social researchers are like, what other things have you considered? Isn't there anything else you might want to use? And to that, the Census Bureau's answer is like, well, we don't want to use Gaussian differential privacy, or Renyi differential privacy, these exotic things that computer scientists are excited about because they aren't established enough. If you have a totally different approach to privacy that you want us to consider, well, let's talk. We don't know of an alternative to some variant of differential privacy that we think will be in compliance with title 13. AUDIENCE: One of the benefits, especially in rural places, is you could be the only Jew in your high school, for example. I'm sure I'm not the only one in this room for whom that was true. Individual guarantees, not just statistical guarantees, are actually quite important. ABRAHAM FLAXMAN: I think the concordance between the theorem and the statement of title 13 is what's really appealing to the Census Bureau folks who are thinking about implementing this. AUDIENCE: So my concern was that differential privacy doesn't work that well with correlated data, and a lot of the census data is heavily correlated. Like, race by location is-- there's very strong correlations there, as you showed in that block earlier with Jewish populations, so. ABRAHAM FLAXMAN: What else you got? AUDIENCE: Oh, no, I was just wondering what sort of privacy definitions. ABRAHAM FLAXMAN: Yeah, so I think that-- AUDIENCE: Because it's something called the dependent differential privacy? ABRAHAM FLAXMAN: In the next year, I think that they're committed to epsilon differential, like classic, as it was first defined. In the next decade, I think there is room for advancing alternatives that are more appropriate from the direction of differential privacy or are completely innovating in terms of definitions of privacy that might be better suited to this challenge. I can't believe I didn't say invariance, because that is the most fun, weird tweak on all of this that Census Bureau is doing. Because besides having these precise counts that they're keeping private, having these noisy counts that they're tuning up with optimization, they also sometimes want to publish a precise count. And they call that an invariant because they're not adding any variation to it. AUDIENCE: [INAUDIBLE] country by population exactly, for example? ABRAHAM FLAXMAN: In particular, the Constitution says house of representative apportionment will come from the number of people in each state. The number of people in each state is not going to have any noise added to it, and so they will have additional constraints perfectly suited to this convex constrained optimization. Throw in additional constraints that say, the number of people in Washington state is exactly precisely the number we counted here. What does that do to differential privacy? No theory for that-- we never thought about it until we heard Census Bureau say, well, yeah, of course we're going to do that. The Constitution says we need to. So I'm wrapping up there, and I've had a lot of questions during this, which I appreciate. We'll have a little bit of time for more questions. I want to thank Randall Akee, who called our attention to this most important thing. ACO grad student, Sam Petti, who really helped me understand what's happening with this code. Simson Garfinkel, Phillip Lecierc at Census Bureau-- I didn't I'd be a tricks to this yet-- also, my colleague, Beatrix Haddock, who made those graphs about AIAN population. And to close out with a few of the things that are directions, as I see it, that you could take and run from here if I've interested you, how to formalize DP with invariants? Good thing I told you what invariants where 30 seconds ago. What sequence of partitions will yield the best privacy tradeoff? And so what I'm getting at here and didn't allude to as much as I wish I had is the way they broke things down to go from the top down was the way they break things down to send out Census Bureau workers who have to go to a single place and walk around that block. We're back in our office with all the data. We don't need to break things according to that geographic hierarchy. Any sequence of nested partitions would be valid for these levels. Is there a better one? What's the best one? Similarly, post-processing optimization-- thanks for the idea to add some kind of constraint on how these deviate. What's the best way to do that? And finally, the real unknown from my perspective that is too hard for me is, what do we do for ACS? If you have 44 questions instead of six, how do you make something that's both private and usable? AUDIENCE: What's ACS? ABRAHAM FLAXMAN: The American Community Survey, previously known as the long-form census. So thank you so much. AUDIENCE: So you're just basing everything on the VIOs? You're totally safe? You could do any optimization that you wanted-- AUDIENCE: --assuming you don't look at the Ai's. AUDIENCE: --assuming you don't look at the Ai's and of course, modulo the fact that we do. ABRAHAM FLAXMAN: And what's so funny about it is the way differential privacy is set up is like, well, that's somebody else's problem. If somebody leaks the sum of some of these Ai's, we protect it against it. But Census Bureau has very responsibly said, oh, we're the ones who leaked the Ai's. We're responsible for that. So yeah, if you don't look at the Ai's, you're allowed to do anything you want in post-processing. AUDIENCE: So if they're leaking the Ai's, are they enforcing that some of the Bi's are equal to the Ai's then doing-- they're not separately creating a Bi associated with an Ai that they've already leaked, right? ABRAHAM FLAXMAN: They're not, and that's part of-- AUDIENCE: So probably, the country-wide, state-wide numbers are exact, and then they're only adding noise to more local levers. ABRAHAM FLAXMAN: But they sort of do, also. So I didn't even say DP query-- they've got this very confusing jargon-- that the detailed queries are the 10 trillion values at the most detailed level of this aggregation. They also make these noisy measurements for things that they think might be important, like just the total number of people living in households and the total number of people living in prisons. There's only seven of those. So those are Bi 1, 2, 3, 4, 5, 6, 7, and they add up to the sum of the Ai's for all of those. That's released precisely because it's the national total. AUDIENCE: So particularly about the long-form thing, if someone's very concerned about their privacy, they might not want to report their true information, and especially since they don't trust the government or something. Do you have any thoughts on how to incentivize people to report their true information? ABRAHAM FLAXMAN: Oh, the most fascinating thing is bringing the theory of this, where it's like, well, what if we prove to them that it will be private to the social researchers, who like, well, go ask some people, professor. And what the social research has shown is that-- don't even talk about how this is private. Don't even talk about how you've done a good job protecting privacy, because-- AUDIENCE: They're now worried that there was some concern. ABRAHAM FLAXMAN: As you've called attention to it, you've made people less likely to answer your questions, no matter how well you promised them you've protected it. Yeah, we're better off just hoping they didn't think about it. AUDIENCE: What if some of the [INAUDIBLE] is right, like the set of things in the short form release. It's like, OK, these are not things that necessarily you're particularly concerned about the government knowing about you. Maybe you are, but at least historically, the government can defer these things with pretty high fidelity. ABRAHAM FLAXMAN: Exactly, that's the theory department approach. And the in-practice approach is so opposite that I love it. It's like, we're going to partner with trusted organizations. There's a huge effort right now to get the best count ever of American Indian and Alaska Natives. They're going through Urban Indian Health Institute and through other trusted partners in those communities to basically go out and make the case. And so when it comes to the privacy tech, the partners who are doing this want to know. The head Census Bureau is taking this very seriously. They might not tell anyone about it. They like to know in case someone asks them, and they like for their own peace of mind. But they're going to go out and work with the communities that they serve to convince them that this is important. Whatever motivates them based knowing their own community, there's a reason to fill out this information. AUDIENCE: But if someone wants to know, can't they find? Like, is it privately private? ABRAHAM FLAXMAN: Like what? AUDIENCE: Like if someone has to know what exactly they're using, like what kind of differential privacy mechanism. ABRAHAM FLAXMAN: It's public, yeah. So anyone can go to GitHub and see the code that they have used for their test products so far. It's not like open science, where you can see what they've done in the two months since they did it. AUDIENCE: [INAUDIBLE] value? ABRAHAM FLAXMAN: But yeah, you can you can go-- I even pasted it into one of these slides, all of the weird config values that they have for how they did one of these versions. It doesn't matter. But yeah, epsilon budget total equals 4.0 split 0.2, 0.2, 0.12, 0.12, blah, blah, blah. AUDIENCE: So I know that Apple does have a privacy budget for some of the things they're sending from their phone. In remember seeing that the number added up to, like, 20 or something like that. And I could do E to the 20. That's bigger on the ratio, like, it's one out of the entire population of the world. ABRAHAM FLAXMAN: They have computer degrees. What's their excuse? AUDIENCE: I don't now. AUDIENCE: Their excuse is that they're hiring people who know differential privacy isn't begging them to certify what they're doing is differentially private. AUDIENCE: But it has privacy budget of 20. AUDIENCE: Also that-- ABRAHAM FLAXMAN: Well, one important thing about sequential composition, which was written here and then the erased, is that it's an inequality. So it's very possible that the epsilon budget of 4.0 is, in practice, actually much smaller. And that's one of the reasons people have gotten interested in, say, any differential privacy or Gaussian differential privacy is that the sequential composition theorems you get for those approaches are tighter. In this case, I did some empirics, which would be great, one day, to see if anyone believes, from a theory perspective to say, we've put it in variance. So we could have totally lost privacy. And on the other hand, we've used sequential composition, so we might have much more privacy than what we're finding. And I didn't put it in this slides, but I find, empirically, things are acting like they're in the they're 10 times smaller than the epsilon that's in there up to a limit. So epsilon of 1 is pretty much the same as epsilon of 0.5. But epsilon of 2 is still really empirically epsilon less than 1. ANNA KARLIN: Let's take remaining questions offline and thank you, Abie. 