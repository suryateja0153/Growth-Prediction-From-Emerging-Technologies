 I welcome you to our second and last panel for today, which is roughly titled We Value Your Privacy, privacy protections and practice. But what we're gonna be talking about, so what we heard about today is really from from Sarah St. Vincent. Why privacy is important, right? Why is privacy human right? And then we talked about all the challenges and actually maintaining privacy in public spaces and whether there's even such a thing as reasonable expectations of privacy. Both in the physical world, but also in the online world. Now we kind of shifting gears a little bit, and we're looking more like what are the challenges if we want to protect privacy? All right, so we talked about rights. And now we're talking about what can we do to protect privacy? And what are the challenges that make it hard to maintain privacy when we're online? But also what are maybe avenues for improving where we are and kind of moving the status quo a little bit. So, when we think about privacy in practice, right? We think of things like privacy policies. Who here has read the privacy policy recently? It's the wrong crowd to ask this question.  [LAUGH]  It's not the reaction I usually get. So that's great, right? Some of you read these privacy policies but most people don't. Similarly, many online services have privacy settings but people don't use them. You can opt out of targeted advertising. Few people do. You can actually they still collecting your data and they're still showing you ads that just won't be targeted. There are these people search engine aggregators that aggregate information about individuals right? You can type in the name and then find lots of information about these people. You can actually opt out of those as well. It's just a little bit like playing whack a mole because you have to go to everyone and basically fill out the same forms over and over again and then there's the 210th one that pops up and has your information. So where does it leave us, right? So in the end, people often just click OK, and they move on and they might not be happy with what's going on. And they actually might be resigned. And what we're gonna look at is, how can we address these challenges, and actually provide privacy and practice? So let me introduce our panelists for today. So you will notice in the program it says we will have Anna Gilbert on the program. Unfortunately she couldn't join us because of a family emergency. I'm gonna be stepping in for her. So I'm gonna be your moderator as well as one of the panelists. I hope that's okay. I'm an assistant professor in the School of Information. I also hold a courtesy appointment in computer science and engineering here. And then our second panel of speakers, Sarita Yardi Schoenebeck. And Sarita is associate professor of information in the school of information here. And I asked each of the panelists to provide a fun fact about themselves. So Sarita told me that almost 12 years ago, she tweeted her first tweet. Almost to the day, so it was February 2nd, I think she said, and it was a very reflective tweet because it just said eating. Okay, and then we got Atul Prakash, Atul professor of electrical engineering and computer science and the computer science and engineering division here in our College of Engineering. When I asked Atul what was a little known fact about him he said well he actually started studying in the CSCW field, Computer Supported Collaborative Work, which is much more socially oriented, but now his research is much more focused on security. Okay, and I also have a fun fact about myself. So I almost ended up studying chemistry and majoring in chemistry rather than computer science, and going down this information and privacy road. So they are little decisions that shape your life. Okay, and with that, I'm happy to hand it over to Sarita.  Okay, can you guys hear me? Yes. Well, tell me if you can't. It doesn't go any lower.  You're good.  All right, thank you. I chose that fun fact tweet from 12 years ago. I've had these fun memories of Twitter when it was happy, and the world felt happy. And now I'm gonna tell you about how Twitter is like a cesspool.  [LAUGH]  I'm gonna show you. So there's some slides here that are a little bit offensive, and I apologize for that. But I felt it was useful for the story. So I'm gonna talk about how people's desire for justice online can lead them to erode other people's privacy. And the conversations today have been almost well exclusively about two entities, which is companies. So Facebook and Google and Amazon, who we love to hate when we all use them, and then also the government, who I guess we also maybe love to hate what we also rely on sometimes. So we hear language like Big Brother surveillance capitalism. So I'm going to focus this little talk on the third aspect, which is people. How people invade other people's privacy? Let me tell you how I'm gonna do that. This first slide. In 2009, a graduate student at the UC Berkeley, school of information, our sister school over there, tweeted, Cisco just offered me a job. Now I have to weigh the utility of a fatty paycheck against the daily commute to San Jose and hating the work. And hiring manager at Cisco saw the tweet and responded who is the hiring manager? I'm sure they'd love to know that you will hate the work. We here at Cisco are versed in the web. The story was picked up on the news. And then people started claiming her, harassing her. Someone made a website called Ciscofatty.com is now defunct and called her names etc. In 2013, Justine Sacco is anyone heard the story? This was more high profile. 2013 she tweeted going to Africa hope I don't get AIDS. Just kidding, I'm white. She's in fact she was from South Africa. And the tweet was supposed to be kind of a commentary on white people's attitudes going to African countries. But that wasn't obvious from the tweet, and she was also employed as a Director of Communications PR basically, at a large company. So people felt this was all kinds of irony that she was tweeting this offensive thing while being a PR person. And she was on an 11 hour flight to South Africa. She got on the plane in London, got on the flight, took it and slept. And within that 11 hours, it was picked up by Docka News. A news website that doesn't exist anymore, I don't think. And by the time she landed, she was the top trending topic on Twitter. And people were harassing her on Twitter and elsewhere, finding out everything about her. She'd been fired from her job within eleven hours just based on that Tweet. She only had a hundred and seventy followers at the time, but someone picked it up. So it went viral. People were angry. Also, and these are some of the tweets that people said, I hope you are on an airplane and have no idea how screwed you are. You deserve all of this, you are a bad human. Another said, you deserve every bit of backlash and more. Another said, now she quit Twitter, lose her job, and **** her future, you deserve this. So pretty hateful. Another example do people know this one? Anyone heard Adrian Richards, the name? This was somewhat high profile but more within tech communities. She was at a tech conference Python and she said not cool jokes about forking repose in a sexual way in big dongles right behind me. So these two men were making sexual innuendos in a extensively private conversation but in a room like this where people could hear it. And she tweeted the photo and sent it out. People got angry at these men for saying these sexual innuendos. And this is in the height of tech conferences are unfriendly to women and people of color. And so it got a lot of emotions rising. One of them was fired from his job not too long after this. And then there's also the backlash against her. Why did she deal with this publicly versus privately? For example, going to the conference organizers and saying these people said this, can you handle it? And she was subsequently fired from her job as well. Everyone was fired. Another example, this is less commonly known. This is Alicia Lynch, she decided to be a the Boston marathon runner for Halloween, a poor choice of judgment of course. And she posted her photo, Twitter and Instagram. People got angry, as you might guess. They found a driver's license, copy of her on the web, and then they found of course her home address. And so she said she's been fired from my job and I'm paying for what I thought was a simple joke. I know it was wrong now, I know I wasn't thinking. She pleaded with people, please stop with the death threats towards my parents, they did nothing wrong. I was one of the wrong and I'm paying for being insensitive. Please stop spreading around my parents number they did nothing wrong, okay? And finally in the height of Gamergate when women were being harassed for protesting sexism in video games, Anita Sarkeesian is a well known name in that space. And she was getting harassment ongoing for weeks and months of this nature. It's very offensive, this is her timeline, right? So you can imagine what that must be like. She tries to speak publicly at universities for example and has had to call it off because of bomb threats, anonymous bomb threats, right? So these are serious real world consequences. She’s pleaded, contacting authorities now. And sought protection from the law with limited success from that. So the central theme through these stories is I’m going to argue privacy. And that’s the key point today. Which is that the most harmful thing you can do to someone online is violate or erode their privacy, okay? So what is privacy? One definition would be, the right to be left alone and to live life free from unwarranted publicity. Now keep these tweets in mind. What's happening here is that take someone's real name, for example, finding that on the Internet when you didn't expect your real name to be revealed in some context like Reddit. Is a violation of someone's expectation of privacy. Combining different profiles, like your Reddit profile and your Facebook profile, can tell us a lot more about a person than they might have expected on each individual account, okay? And my colleague David Jurgens, in the panel before talked about how he thrives on doing this, but we let him get away with that. Finding someone's physical address, right? That's very invasive, many people, aside from delivery services, which are it's own privacy anomaly like Uber and Airbnb. Most people are online not expecting their private home address to be associated with their online profile, This is what people were doing. Employment, this is an interesting question from employment lawyers, which is when you tweet something offensive or do something offensive on the internet on your own personal time, is that grounds for being fired? Certainly, as we've seen, it happens. And then, family and friends, this is probably the most invasive kind of violation of privacy, is to reveal family and friends identities and harass them and associate their offline identities, right? As we saw was happening. So why do people do this? The argument we've made in my lab is that people who are intentionally committed events should receive a proportional punishment. This is the bedrock of this idea called retributive justice. And this is how our criminal justice system in the US operates as it's come up, I think to some extent already today,which is that people do some offence, they commit some offense, they should be punished. And we see that in offline context. So if you run a red light, and you're caught, you get a ticket and you pay a small fine. If you robbed a bank, you probably go to jail, right? So there's these clear in the systems have been built out over centuries and with juries and judges clearly refined. But online, we don't have these kinds of legal systems and they're lagging in the cases that we do. For example, sending harassing tweets is not illegal and for the most part, law enforcement can't and doesn't do anything about it. You have to have a direct threat on your life. Would like an address for them to decide to do something, okay? So it's these kinds of things that we're seeing on the internet. There's no recourse. Twitter also doesn't want to do anything because they want to protect what they feel is their free speech for their users, as we've also talked about and can talk about more So what do people do? They engage in these kinds of vigilante justice behaviors. There's no legal or other kinds of recourse for this offensive behavior online. People turn to their own kind of harassing behaviors, they reveal people's identities. And again, this is what they do, they make people's identities public as a way to when they didn't expect it, as a way to get back at them, okay? As a way to enact justice. So I'm gonna briefly mention a study my PhD student Lindsey Blackwell did to test this theory. This was a study with 529 online participants, and it was a series of studies, and we were testing are people in fact, do they in fact believe that it's more okay to harass people to reveal their public identity, if they commit some offense. So this is what we test, we connected randomized experiment, we find that they do indeed do that. So they feel that it's more okay to harass someone, it's more justified, more deserved if they have committed some offense before that, okay? That explains these back and forths you see on Twitter and Facebook and other places, where people are behaving in these ways. So what can we do about this? What can we do to protect people's right to privacy and from unwarranted publicity in online context? I put forth two ideas, two provocations. The first is to teach people, including every day Internet users like people in this room and elsewhere about principles of procedural justice. This is fairness is process. If someone commits some offense, say they post an offensive Halloween costume, what is an appropriate punishment? It's not a mob, but there's some sort of recourse that needs to happen, right? Teach people about what would be the appropriate kind of response in online context, and how many people have to respond to something. To be transparent in those decisions as we have in offline legal systems. To allow people who committed the offense to have a voice in those interactions, right? They hide, they run and hide when flash mobs come online, to be impartial in decision making, and then we need to approach to online justice that is morally driven rather than mob driven, which is what it is now. So theories of justice and teaching people about them. The second would be increased regulation. Treating online identity as important as offline identity. One example would be developing metaphors for the physical home, right? We've talked about, earlier, how the physical home is protected in a variety of ways. People have right to privacy in their home. And we need something like that online context for their social media identities. How to do that, I don't know, but I think we need that. And then protecting multiple spheres of identities online. So you might be anonymous in some contexts like health boards where you have a right to be private, that shouldn't be connected with your say public Facebook profile. And I'll just add one plug. We have a new project that's drawing on some of these ideas. And this is with co-investigators, Cliff Lampe at UMSI, JJ Prescott at Law. We're looking at how do you develop theories of justice to respond to these kinds of online behaviors where people are compromising each other's privacy. So if anyone's interested, love to talk more offline about that. [APPLAUSE]  Okay, so, I'm not going to use lights just to give an introduction about myself. I'm a professor in computer science. And as Florian mentioned, some of my early work actually when I joined Michigan was in the CSEW field and I was regularly going to that conference. Kind of interacting with people on the social science side, although I was still doing, I think, somewhat hardcore computer science in terms of building systems. And I was probably one of the few at that time, but one of the things that kind of talking about privacy. One of the papers that comes to my mind, that some of the students in my class worked with me on. Was the 2008 paper in CCW that was on dangers of context aware spam that we wrote and we were particularly comparing systems like Facebook and LinkedIn. And it was kind of interesting that essentially, we found differences in the way these two networks dealt with what information they exposed to users. And that had profound implications on what you could learn about other users as a result. For example, on Facebook, turned out that in that paper, we talked about, that as long as you could get an email account somehow on the University of Michigan network, say by hacking of one single machine or by being a legitimate user. You could at that time, navigate almost like 70,000, 80,000 users who were connected in University of Michigan, who had affiliation with University of Michigan. You could navigate to their profiles and get all sorts of information on Facebook network and as a result of that, from that, you could then profile the users, you could learn where they'd been. And then that paper was about being able to target them with the email that had a lot of context. And as a result you could, for example, pretend to be them to their relatives, for example, or to their friends. And then as a result, perhaps, learn even more or target them in malicious ways. And I'm not sure how much the paper got noticed at that time. But it's kind of like, I think it's taken ten years, for I think some of the risks with being able to navigate networks and connections kind of come home, I guess, for Facebook. But at the same time, one of the insights is that Facebook has a much richer valuation thank LinkedIn. I think if you kind of think LinkedIn is part of Microsoft, but if you think of what the value is, Facebook has a much higher valuation in my opinion. And so, data has a lot of value, so whatever Facebook did at that time despite of the kind of negative tone in terms of them revealing the data. It definitely has value and I think that's kind of attention that revealing data and sharing it with third parties, there just so much value there for companies. That, you know, just technology isn't necessarily going to solve the problem because this kind of a willingness to share data and that to private data. I mean, so I think that is one tension that we have to figure out and it might require legal solutions beyond technical side. The other side that I think and the interesting thing is even if there is technical solutions to this like. For example, on the technical side techniques for anonymization, there are tools for encryption. In our group, we have done a lot of work on sand boxing technology so you can run computations inside sand boxes with some guarantees of, the data doesn't leak. And you can still draw inferences and do useful work and their techniques on secure computations and cryptographic community, it's all there. But attention is emerging between sort of anonymity, you know, how much anonymity you have. And then because you got malicious actors as well, so it's not just that legitimate users want privacy, but you also got malicious actors in the same ecosystem. And there's value there too, in terms of malicious news and fake news and everything else that's coming out. And so there's kind of a tension, I think that's playing out, which is sometimes in very sound direction. Like, I think in Australia, for example, now there's a backdoor that the government is starting to require on social media communication tools. Which has an implication for privacy in because people will have less expectation of privacy, especially from the government. So I'm not sure again,you know, what the solution there will be but it's because the law is kind of favors of government in that sense, but I think that's another kind of emerging issue. Third thing is that, yes there are techniques available for things like sandboxing systems and I think at the low level of the architecture. We got lots of tools to keep information secure and sandboxed, and there are also techniques like differential privacy, which are useful. The problem is it runs into two things, one is again the data is valuable, so take Apple versus Google. Apple claims to provide more privacy in some sense, they say that when they analyze a data, they use differentially private techniques. Google on the other hand, seems to take everything and say, well, just trust us with the data you give to us. But question is, do you like Siri mode, do you like which is maybe, perhaps uses as Apple claims maybe more differentially private techniques. Or do you like Google Assistant more or Alexa more? So there's a tension between how much the tools can protect data which is the functionality and the value that consumers see, and again, I think there are these tensions. Finally there is user awareness issue on privacy, I don't think most of us are aware exactly what data is private. Even the companies that we need to trust, companies like Google like we use in the University, so many Google tools. And even otherwise, I mean, they're kind of indispensable in a way, but what happens to that data, It's not always clear, especially with respect to third party sharing and ads that they deliver. So I see lots of unsolved problems there, a lot of interesting problems, and I think I'll leave it at that.  [APPLAUSE]  All right, so I'm putting on my speaker hat,and I wanna talk a little bit about the privacy paradox, who have you has heard of the privacy paradox? Okay, one, two, a couple of hands. All right, so the privacy paradox basically says, people say they care about privacy, but then they act in a way that is contradictory to their stated preferences. And I wanna talk a little bit about some research we've done that kind of shows these contradictions. But then also tries to get at what might be reasons for those contradictions. And then finally, how can we overcome this? So some research we did not too long ago, focused on smart speakers. All right, so we talked to people who actually own smart speakers, but also to people who decided not to own smart speakers. And our goal here was really to understand how do they reason about privacy risks and perception with these devices. For people who are not familiar with smart speakers, the way they work is that they have a microphone and you can interact with them with voice commands. And they continuously listen for activation keywords such as, Hey Alexa in the case of Amazon's smart speaker. And then you can talk, right? But there are all these issues H.A. Wiesans talked earlier about some of the problems with smart speakers. Alexa snickering at you in the middle of the night, right? [LAUGH] It's very creepy, weird stuff going on. So, what we found in our research, which was surprising to us a little bit is so we went in kind of into being interested in how do people behave around smart speakers why people particularly people who own smart speakers, do they cover the device do they unplug the device like like what are their privacy behaviors around these smart speakers. Turns out, they don't have any, and they basically don't do anything to limit what the device can learn about them. Quite the opposite. They actually play smart speakers in places in the home where the smart speakers most likely to pick up commands from everywhere in the house. And so. And now diving deeper, what we find is that for for many of the people, this is not because they are necessarily don't care about privacy, but rather they want the functionality and the convenience and the benefits that these smart speakers bring for them, which is the rightful way to do that, which is fine, right? But the problem is that in order to enjoy these benefits, they really have to make a privacy tradeoff. And some people make that consciously, and they say, I'm really not happy with this smart speaker being always on. But I really like it if I can ask Alexa to read my schedule to me in the morning or play the news. Or my children like to play with it. But so and then, the others who just completely ignore the privacy implications of those devices and really rely on the sociotechnical context of these devices' existence. By that, I mean, they basically rely on, these companies can't do anything bad with my data because otherwise, that would be bad for the company. And so there's kind of this implicit assumption that device makers are going to look out for my privacy. If you paid attention today, right, that's really not what's happening. So that's kind of concerning. And then we look further at one other reason why people are not using privacy controls that come with smart speakers. You can see here, there are actually buttons on top. There's a physical button you can press to mute the microphone, all right? There's also a companion app where you can go back in and delete commands and basically delete recordings that the smart speaker has recorded. But the prolem is people, most of our participants have not actually used these controls or had wrong perceptions about what they actually meant. So the mute button was understood as like it mutes the speaker. So the speaker is not playing sound again or it wasn't understood as a privacy feature. Similarly, these audio logs in the companion apps were actually really convenient feature to listen to what the babysitter or the house sitter has been up to with a smart speaker. Right, so these privacy controls are not well positioned as actual controls for your privacy. And one of the reasons is that they're not integrated in how you interact with the device. This is a voice activated device and you interact through the device with voice. But then if you want to mute the microphone, you actually have to go there and press a button. And so you need to shift your interaction with Lt. At the same time it will be not too hard to actually support commands that say like, hey, Alexa, forget everything I said in the last hour. Or Alexa, don't listen for the next ten minutes, right? These are things that could easily be implemented, but then they don't have that yet. One thing we've been experimenting in my group with is other ways where we can basically better determine when this microphone should be on or off, right? So we've been looking at what we call conversational cues that we can use for this. So can we only activate the microphone when someone's actually looking at the smart speaker, similar to how you look at the person you're addressing. Or can we use voice volume to allow you to speak loudly? So Alexa listens to you, but you can also whisper and then Alexa doesn't hear it, right? And we built prototypes with this and have conducted studies with this and what we see is that by doing this, we can actually substantially reduce the time when the time when the microphone is on. Now on the other hand, we're introducing additional sensors as well, right? Now we need to listen for ambient noise volume or we need to have kinda like a camera with a 3-D camera and the other camera can kind of detect so they're always tradeoffs, right? But what we're trying to do is show that there are opportunities to go beyond what we're currently used to. Another example of this privacy paradox is data breaches. We heard about the data breaches before as well, today before as well, and we've done some research on how people react to data breaches. And what we found is that people often do nothing after their victim of the data breach. We particularly studied this in the context of the Equifax Data Breach, which affected about half of the US population in 2017. And less than half of our participants had actually checked whether they were affected by the breach. And only two, I think four people had actually done any kind of protective measure such as signing up for identity theft protection or freezing their credit. Now, if we look at why this is happening, well, we see very well known cognitive biases and decision heuristics playing into this. One of them is optimism bias. What that means is people assume that they are an unlikely target, right? So many people were effected, why should I be the target of something? Or what I find even more concerning, we had particular participants who have lower incomes assume they have nothing to lose, right? Like I already have a bad credit score, I don't have much money. Why would an identity thief go after my identity? Now other research has shown that people of low socioeconomic status are actually disproportionately affected by identity theft. Because that's not really how identity thieves work. They don't do substantial background research to see whether you choose the target. They just use your identity and either works or doesn't. So there are lots of mismatches in how people reason about privacy. And what we need to do to really help people make better decisions is three important steps, in my opinion. One is, we need to provide people with information that is actually relevant to them in the moment when it matters, right? So giving people long privacy statements or privacy policies, and assuming that they will read them. Well, actually, everyone knows they're not reading them, we just say they're there so you must have read it. It's not really useful, right? Instead, we need to integrate privacy information into how you interact with technology. The warnings need to be there where it actually matters and the information needs to be actionable. If you just tell people like this is how the smart speaker works, okay, why should they read this, right? There's very little that they can do with it. Instead if you give them choices and explain how they, for example, can use the smart speaker when they're not using it, what are the benefits but maybe also the privacy risks of the technology. And people can actually start making decisions. And I hope that we can kind of move towards these more usable privacy controls to get to a world where people can actually more effectively reason about the digital privacy challenges they are facing. Okay, that is all I had to say and now I am switching back to my moderator. [APPLAUSE]  Thank you Florian. [LAUGH]  So all three of us talked about kinda the need of education and awareness, all right. So and my question to both of you would be, what do you think are useful ways to really teach people about procedural justice for example, or teach people about scams and the risk they face in online social networks?  A great question, I wish I knew the answer. I mean, even my dad got scammed so and I'm a security researcher so, and I keep talking about security. And he was trying to avoid this kind of a, so it's kind of a funny story and sad story. He was, he's kind of in the AD, around AD and obviously not tech savvy. So he was trying to avoid using as much as possible online accounts for banking and everything. And he still got scammed from a banking thing where somebody called them pretended to be a bank official and took him through one time passwords and everything. So there's in India, you got one time passwords where it pretty much every transaction. But somebody managed to convince him to give the one time password over the phone. Right, and then withdraw money from his account even though he had no online account. And so I think the world has become a lot risky because things can kind of bite you, even when you avoid technology. And I think that's one thing so and what you do at that point is tell people give examples of people who have been, I think, hurt by scams or privacy leaks. And so when that happens, you end up using technology to communicate that too. So like on WhatsApp, I would tell examples of what might have happened to family members, watch out for this. I think kids, it's little easier. So I think somehow, I think you can to start at an early age and start telling them that. So to be maybe, that there are risks. Yes, you probably end up using social networks but be aware of the risks. I guess it's a little difficult I think, for most of the population that's kind of our age and older because the technology is moving too fast. And risks are going too much and features are being added too fast by banks and by everybody else, by social networks, where people don't realize the risks. And I think what's changed is that like in the old days, I mean, there was sort of, if you shouted, if you said something inside a village or inside your town, some people knew but it wasn't totally public. Now it's kind of flipped, everything is public yet nobody knows. So from a privacy perspective, the problem is that there was kind of a balance between privacy and risk and Anonymity in some sense. So, I mean responsible, you have to be responsible in some sense like I think there was at least some expectation. And I think that's kind of hitting a lot of people in unexpected ways, where like in India, for example, on WhatsApp, people have been lynched literally, because of Misleading information. Like there's a kidnapper in the town in a village and people just assume that it's true just because it used to be true in the old days when somebody said something to you, then you knew. And that's, I think it's a very challenging problem, how to educate at multiple levels in terms of distinguishing what to believe or not to believe, as well as how to handle that.  Ideas or how to deal that. One human mentioned starting with kids, k12, so kids are clearly, people here have kids. If you have kids who are above the age of one you've thought about screen time with them. And kids are taught not to use devices too much, and when they use them, then you might want to try to watch what they're doing or get out of SnapChat or whatever it is. And there's been no education about accepting the fact that kids are on these things and then what do we do about it when there are these behaviors where people are responding to each other inappropriately. So we might say, well, can we teach kids something like procedural justice where we say, what is a fair response? If someone is harassing a boy at school or harassing a girl, what is the appropriate response to that? If you are all going to handle yourselves in this online environment, should 20 people go harass him back? Should one or two of your friends say hey, that's not cool. You get into bystander intervention literature here, which has a lot to say about providing support to targets and then trying to intervene, certain people are more able to intervene than others. So I think teaching kids about learning how to manage these interactions online by themselves, rather than saying, don't bully people or tell a parent which is also maybe productive. But teaching some of these theories to kids at a young age and it will be useful in schools. The second idea with adults would be, and this is more of a Blue Sky thought experiment, is trying to model or reproduce justice systems, for example, juries, right? Juries in practice are kind of inconceivable. If we didn't have juries, you'd say, you can't just pull random civilians off the street and expect them to go sit in rooms and make these life and death decisions about other people's futures. But we do in fact do that. We can do that offline, it seems a lot simpler to do it online in theory. So maybe pulling on some of these offline behaviors that we use and that work, we figured out how to make them work. Could you do this on sites like Reddit or Facebook or Twitter? And there's a lot of questions like why would people do this offline? You have to because the government tells you to, so the government could tell you to online too but that would be approach you could test out. We could certainly run experiment in smaller scales outside of Twitter and have people go in and do this and spend some studies for example, on Twitter and people are being racist. One study showed that bots that intervene in the person who's posting racist things will reduce future incidences of racism. So just coming in and intervening with someone who's behaving inappropriately does in fact, reduce their behavior. So you can imagine scaling up those kind of interventions. Okay, thanks, so one thing we hear a lot about when it comes to online harassment, fake news and so forth is the calls for more moderation, right? Moderating posts on social media and so forth, and can you both maybe reflect on whether that's an effective strategy or what might be challenges with content moderation.  I can speak to a variety of the challenges, it's not sustainable or scalable. Right now, Facebook has, last I heard was 20,000 content moderators, mostly in developing countries who seems not paid particularly well and don't have access to therapy cuz they're viewing the worst of the worst on the Internet. Right, so this isn't seem sustainable, and this is currently how it's done. So Zuckerberg talks about AI and all these things but in fact there's a human sitting there moderating content. Probably the bigger problem, even if you could hire enough people to do it is that, the rule book, have people heard about their PowerPoint slides that were released a few months ago. Does anyone know about this? So Facebook basically has this literally a set of a PowerPoint deck that's huge and ever changing that provides the rules for what kinds of content should be moderated off the site or not. And it's impossible for anyone human to remember it all let alone a low paid worker, unskilled workers what they are. And they don't have time to reference it cuz they're paid by each kind of moderation tactic. So basically Facebook just is in this mess where it's inconsistent, it's a woman is being harassed by man. If it doesn't violate some rule or some person another country says it's not violating any rule, it goes past and other times it is a violation, so the whole system doesn't seem to be working. Maybe you know how to fix it. [LAUGH]  So I guess the the solution is AI. [LAUGH]  Well, that's where I think the companies are going in terms, because scalability I think is a very real problem. And I mean eventually it might end up more towards moderation tools available to users directly. So before you post, let's say you did have some checker eventually, which can check if your post is follow certain community standards that you participate in. I mean, that would be nice. Because I think these companies cannot do that for you like I think they cannot understand different communities and different standards that exist, it's really hard. Scaling that up is also hard, because there isn't really one right answer for different countries and different communities. So I think that is a good challenge. But I think that's where things are going. You could imagine some sort of collector moderation, which is that things don't become public right away. They're kind of held for a while, and maybe if you had those extra 11 hours or ten hours given that example and had a chance to reflect on what was posted and how it could be taken negatively. Maybe, in that flight example, maybe things might have been different. So they might it might be tools like that technology where technology can help. So, moderation tools are kind of built in, in some way. Where, because most people, many people, I mean like when you tweet, don't think as much in terms of what the implications might be. But if you had a chance to think, like chases. Yeah, I'm saying that if you had a chance to, if the tools gave you a chance to think, you know, I mean, like, I think Gmail is contrary to something a little bit like that where. I think you'll get a few minutes to undo your email that you just mailed. So occasionally it's useful where you say, okay, I got it got it got some spelling errors here, or maybe I shouldn't write this so, so things like that could be a start of a cooldown phase or something like that. Yeah, so I think this discussion about moderation also reminds me of something Sarah said this morning, this noon, I guess, earlier today as how surveillance reinforces power disparities. And and, you know, I think one of the big risks of particularly AI for content moderation is the key potentially suppressing free speech. But at the same time, I think we're still looking for that holy grail, that kind of somehow allows us to have the benefits of social media and I think maybe we should talk about those a little bit more as well. Why kind of curtailing the negative aspects. And that seems to be like the big struggle we're facing right now to see, okay, how do we actually benefit from these services? And yeah, I don't know, either if you wanna comment on that.  On that, it's not really addressing the question, but there's a prevailing attitude from tech companies that they are neutral. So they are not the arbiters of what is okay to say or not. They're just coming with this rulebook and people apply it and it's not their fault. And a lot of scholars are saying that is absolutely not the case and it harms marginalized people which Sarah touched on briefly as well. But there are cases where your gender or race issues and and say women of color sanctioned. Because they're trying to speak out against racism, for example, but the language they used to do so means that they become censored. And that person who said something offensive to them was not censored, right? There's lots of stories that these kinds of things and I think that the point is that these companies can't pretend to be neutral. And ignore social and historical kind of histories that lead to things like race and gender not just being these neutral kind of categories that should be addressed or treated equally, which AI also would not know. Not that you're saying that AI will solve this but that will be the biggest problem is I would fail as miserable as people do on those things.  The problem is that with the AI is that when you're done work which shows you can like normally this is things learned from online. You got it usually a training is required to teach the systems how to behave. But it turns out that adversary if you acted to serially to these, you can cause sometimes training to be way of using just one bad input. And they've been examples of papers which talked about in computer vision realm was called a one pixel attack that if you strategically modify one pixel, that's enough to cause like a train model to make a completely wrong decision number one. On a classification decision, you just got to pick the right pixel and also a single input can cause a bad training example, can cause the algorithm to make a desired incorrect decision. On an input that was previously correctly classifying, so this is like in the online learning thing. And there was an example with a Microsoft bot, for example, which became racist and sexist through bad input very quickly, in a day or two. So I think there's a huge risk. And we don't completely understand the foundations why that happens and how that happens with a lot of work in AI right now. So a lot of work, I think needs to happen there as well. But I mean, so I think there's a scale problem. So that needs to be solved. And AI seems to be the panacea for that. But at the same time, that has a lot of problems as well. So I don't think we have the right answers at this point on that.  So we talk a lot about the challenges for privacy, right? So I was wondering if either of you maybe have a positive example where you feel like privacy is actually working. And that could be a technical solution, like company that has done really well in terms of implementing privacy settings. Or it could be a context where social norms, basically, ensure privacy and respect privacy in a meaningful way.  I'll give one example. I think Reddit's doing pretty well. Who's familiar with Reddit? Or who's not familiar? Reddit's an online discussion board. Very popular, it's one of the top visited sites. And people use pseudonyms. So it's essentially anonymous. And they are pretty responsive to research results and to taking stances. So I talked about companies not being neutral, they will take stances. For example, when there was Coontown which is as offensive as it sounds, they shut it down, right? And then our colleague Eric Gilbert did a study looking at what happened to all the people who are posting racist things on Reddit. Twitter, all these companies, and looking at where they went next. And Reddit also has introduce norm. So I talked about revealing people's private home addresses, which is called doxxing. It's revealing their private information in these public spaces. Reddit has banned that. So if they find that content that will be removed, just flat out removed. So they're taking pretty good stances which Twitter has not done. And then, since he's here, my student Tophiq has been studying Reddit in context of fathers. But looking at how people from different kind of marginalized populations can use Reddit anonymously to talk about things they wouldn't talk about on Facebook, for example, getting a divorce or whatever. He's over there [LAUGH].  I'm I think more pessimistic in terms of what the solutions [LAUGH] are there.  If you want?  Right so because the problem is what you don't know is the problem. In the sense that even with systems like Reddit, what we don't know is okay, down the road is there going to be a hack down there. And where everything is going to be revealed in terms of the identities and so on. What information they have. And so many systems have been compromised which we thought were relatively safe. And I think that's the problem. That now something that seems private may not really be private once you take stock of what information is actually there in the system on the back end that you don't see. That I think is a problem, and we don't have good visibility into that ourselves. I haven't look at Reddit's privacy policy, if they say well information is not kept for a certain amount on their back ends. If the information expires, then I will be a little happier. If the information is-  No, they keep it.  If they keep it all, you have to ask the question if it ever gets leaked, then publicly, everything gets leaked, then what are the risks at that point? Would you still have privacy in that in those discussions? I think that's the big question, yeah.  Facebook secret groups would be even worse if those were leaked, right? The Facebook secret groups people have a very strong perception of privacy because people don't even know they're in them. Other people aren't in them. Those leaking, I think, would be far more damaging.  So it's not that companies don't necessarily know how to deal with some of this. So I spent my sabbatical at a company in West Coast. And I can say that their internal policy that emails and Hangout chat messages, I think, from my recollection, would expire after a certain period. So six months or a year or something like that unless you tag them in a certain way. So I think there was some expectation that information will go away. And they might have been doing that for their own legal protection. But that doesn't apply to a lot of information the same company collects on all of us. So you need something like that where their information will disappear. So at least the risks are reduced in case it gets leaked. But I think as a person, you have to assume that anything you post could be linked back to you.  And at the same time, right, that is kind of directly counter to many of the advantages we have of online communication. That you can actually be or live part of your personality in specific or be different people in different circles. But then you have to deal with this context collapse that at any point there could be the this major data breach that affects everyone. All right, on that bombshell. Are there any questions from the audience?  Can you go to the microphone?  Hi, you spoke, all three of you, a little bit about sort of inadvertent sharing by third parties, by surrounding community, by family, by other Twitter or Facebook users, of certain information. Which strikes me as a deeply troubling thing. Or Alexa, if a friend has an Alexa, you can't be like, hey, get rid of your Alexa while I'm here. So I'm wondering if y'all have seen any research or thought about what are ways that an individual can think about protecting themselves in contexts where they know that their information may be shared inadvertently? I guess it's hard, right? We do that in a human context day-to-day. But in a digital context, is it different, is it the same?  It's a great question and there's no easy answer. One context I have studied quite a lot is parents sharing information about their kids on social media. Where you do have a very prevalent question of is it their right to share information about their children starting from even before they're even born. And then, as they get older, they start to speak out about how much their parents can share about them. So parents become more reactive and more sensitive to that. One thing I think companies like Facebook should do is give kids the ability to take ownership over that content as they become 18 or some designated age. Or point at which the parent also says, hey, my kid has been in all these photos. Which facial recognition can identify as seems reliably if you ignore the bias problem, should be able to give people more ownership over that content. So the same thing would work for family or friends or other contexts as well, is that as long as people are now able to be recognized, even David talked about this, you can infer a lot from people by their ties. You think we should be able to try to give people control over that other information that is shared about them.  You're a participant too. [LAUGH]  So going back to the smart speakers, right? I think, thinking about ways how you can kind of have a guest mode or Just unplug the device, right? Like why is it so socially unacceptable to say like, you have a smart speaker? Do you mind unplugging it while I'm here? [LAUGH] I mean, that's also maybe a way of starting a conversation about what are the privacy concerns, and are they actually reasonable concerns or unreasonable concerns, and why is the other person comfortable with the device? So I think really talking more about these issues rather than kind of just running into this, I've got nothing to hide argument, like, I don't care about privacy. It's overblown, or, Privacy get over it, right? So I think John these assumptions can be useful. And then there have been some really interesting, almost more design and art projects that look at what you can do to take back your privacy right from dresses or clothing that reflect when it's actually photographed to classes that have low lasers in it. So when there's a camera that's trying to do facial recognition and actually blinds the camera, but you don't really know what's happening, cuz it doesn't hurt humans, right. So they're really kinda interesting, maybe more gimmicky ways of dealing with that. But I think it's a really good way of kinda pushing in a more playful way how we engage with some of these really concerning technologies.  To understand your question, I mean, you were talking about inadvertent capture. Was is it the scenario that information gets shared with third parties by the system that's collecting data that should not be, or the other way round? That, like I walk into somebody's house and they've got Alexa on and Alexa is capturing my audio as well.  [INAUDIBLE]  Sure.  [INAUDIBLE]  Right, and the face recognition systems, can perhaps, even though, like if you were in a photo and yeah you were sharing it just with your family, that would be fine, but if the faces, and they would not necessarily know. Let's say, suppose you know me, but don't say though, Florian, then you would not kind of, that would perhaps, be okay in a way, right? Because then I'm being identified. But if the system is identifying all the faces, then it becomes a problem. I think that's where it becomes a problem, I think from a privacy perspective.  A really good research problem domain for anyone looking for one is in schools and youth organizations. So the YMCA, public school systems, they're all taking photos of groups of kids and posting them on the Internet. There's no way to get consent from parents and I think folks have kinda given up on trying. I think there's a bunch of interesting questions and opportunities for trying to do something better, do anything about it?  Yeah, I mean Facebook was tagging me for a while, I think they had some controls in terms of whether you wanted to be tagged in photos taken by others. But I'm not sure how many people are aware of this controls, and in some sense, Facebook still perhaps, knows what photos I am in, even though I may not want to be tagged. So the risks are still there. I think that's a problem, and it's gonna get worse because people are talking about activity and friends. So gestures can identify people, how you walk can identify people. So it's just gonna get worse I think.  Right, maybe one last question.  Actually a little bit ancillary to Angela's question which is earlier on, you mentioned [COUGH] specifically an issue with Alexa and the babysitter, and this might actually be a question for Saul, I'm not sure. [LAUGH] I was just trying to think of how that would work in terms of privacy if you're in a one party state, or if this particular person never actually gave their consent other than the fact that they're entering your home to babysit your child. And how that would affect privacy if for whatever reason those recordings got out, or whatever based on just activity. Because there's not really much in the way of consent saying, hey, you're coming into my house, I'm gonna review you later on to see what the babysitter's up to. So unless there's some kind of consent there, I'm just kinda curious to know what the legal ramifications might be, and how that would affect privacy going down the road.  Not a lawyer [LAUGH] but so, I think yes there are definitely potential issues with wiretapping loss, right? And at the same time, we are living in a time where everyone of us has surveillance device in their pockets, all right? And that can be used to survey them, but also can be used to survey others, record others, and I think that's pretty much at the center of many of the problems we face today. And I think we haven't really figured out how to deal with that. And then, societal, but also legal perspective, I think. And so I think what this really means is that we see more and more contacts collapse. That basically it's very easy to take something, take a picture or take a video or take an audio recording, and it being used in a very different context where you never anticipated that it would show up, right? And it's typically a context in which you're vulnerable. So that might be in your work context, that might be in your social circles, and that might also be when you're trying to apply for some kind of social service. Or when your child ends up applying to college, right? I don't know. So I think that's really the big issue we're facing and really, at least limiting how data can be used within context. So mom created this concept of contextual integrity, right? So that we have a good understanding when data is created for a specific reason that we're typically comfortable with that, but as soon as this data leaves that particular context that's sphere for which it was intended. That's when the problem start, when privacy problem start. And I think we need to find better ways of protecting these spheres of interaction in this contexts in a better way. I think right now, we're still areas, but we're still in this like, whatever you say in public is public, kind of notion, and that's just challenging.  In your home, [INAUDIBLE] outside, so for example, [INAUDIBLE]  Question was about if there is a sense of the home being private and if someone is now coming into the home, does that change that expectation of privacy, right? Some person like a babysitter who's not a family member? I don't know. I mean, the one we're trying to be positive, I have a little slice of positive which is, it could be optimistic that things like Alexa are very physically visible. So someone comes into the home they can see that there is some surveillance, they can probably infer there's probably other kinds of surveillance. The nanny cam industry used to be super closeted, and feels like a very New York city kind of, we have nanny cams, we don't tell the nanny, so they don't know that they're being watched. And now, at least there's a little more sense of disclosure that probably a lot of families have this. So you might at least expect you're being watched, whether or not someone tells you explicitly.  Yeah, but I think the way the technology is going, lot of things will become invisible. So, I mean,  And everything I tried to say.  [LAUGH]  I know, but that's the reality. I mean, I think one of the things is technology is way ahead of things, always the legal side in some sense. So, I mean, phones have already become very small. I mean, you know which pocket do I have home? You have phone you don't know necessarily, right? And whether you have a phone, I don't know whether there are any sensors you have you don't know. Right, so, whether things are being recorded, we don't know. So,  Now you're in Iivestreamed.  [LAUGH] I didn't know that. So, [LAUGH] right, okay.  [LAUGH]  So I think actually in a home it's gonna become problematic, I think from that perspective in classrooms down the road. And that's a problem to be solved as well. That are there any legal safeguards we need to make sure that that transparency is there both ways, like when is the technology potentially recording you? Does a babysitter really need to know that he or she is being recorded? So I think we need to have that discussion and figure solutions out.  And I think what this tells me is really that technology on its own is not gonna solve this. Legal solutions are not gonna solve this, or at least not in the foreseeable future. Or maybe Europe, we'll see. And what we really end up needing are better social norms around what is okay, all right? Like what should data be used for? What is acceptable? What is unacceptable? And then, I think right now we kinda live in a world where data is being used for unacceptable users, and everyone kinda shrugs it off. I think we need to a point where that's actually a bad thing to do, right? And people get told off for doing that. So, yeah, I don't know. Maybe we ought to just the transitional generation, right? And the next generation has it all figured out because they grew up with touchscreens and whatnot from the beginning.  Right.  Or the opposite.  We'll see.  [LAUGH]  All right, let's thank the speakers.  [APPLAUSE] 