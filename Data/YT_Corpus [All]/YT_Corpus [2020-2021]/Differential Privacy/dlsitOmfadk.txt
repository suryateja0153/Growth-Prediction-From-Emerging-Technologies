 (air whooshing) (upbeat music) - Welcome to the first of the fall 2020 programs, an Exploring Ethics series of the Center for Ethics in Science and Technology. I'm Mike Kalichman, Director of the Ethics Center and a professor at UC San Diego. These method programs are supported in part by the University of California, San Diego and normally hosted at San Diego's Fleet Science Center. However, in the face of the ongoing pandemic, we will be convening our programs online for the foreseeable future. Given that this is still new for us, in the spirit of science, we ask your patience and invite you to join us in conducting this experiment. For those of you new to these programs, our Exploring Ethics series is designed to explore new developments in science and technology with members of the community. Identify ethical challenges, how the research is done, how the technology is applied and then finally, identify possible solutions to those challenges. Because one of the goals of these programs is defined by audience engagement, we ask that you take advantage of various mechanisms to provide feedback including the option for Q&A in our webinar. So, in the midst of the COVID-19 pandemic, we are acutely aware of the challenge of quickly collecting medical data, so as to better understand, detect, treat and sometimes cure diseases. However, collecting such data does go hand in hand with the need to also collect and store information that we might prefer to keep private. The focus of tonight's program will be on mechanisms to make the collection of medical data more secure and an informed consent process that gives patients more control over how their data will be shared. We are unusually fortunate to welcome Dr. Lucila Ohno-Machado to help us navigate these challenging questions. As you will have seen from our biographical sketch on the ethics center website, Dr. Ohno-Machado is an MD, MBA and PhD. She's a professor of medicine and chair of the UC San Diego Health Department of Biomedical Informatics, both at UC San Diego. She's also associate Dean for informatics and technology at UC San Diego and the founding chair of our health system department of biomedical informatics. Please join me in welcoming Dr. Machado to this evening's program. - Thank you, Dr. Kalichman for the invitation to present today on the Exploring Ethics seminar. I'll be talking about Protecting Privacy While Sharing Biomedical Data for Machine Learning. First, I wanna say a few things about biomedical informatics in privacy technology and data sharing in de-identification is one of the many areas in which our personnel specializes. So I'll talk today about the intersection of privacy technology and ethics. First, I would like to say everyone is different and personalizing prevention and care requires that we collect a lot of information so we can personalize medicine. So, here's an example on variables that are collected for each individual in order to try to predict cardiovascular risk and while those in black are traditional, more and more we have genomic information and exposure information that is being added to everyone's record. Big data for predictive models requires, as I mentioned before, a lot of data. And some examples of clinical prediction are listed here and we have done those models. Select best embryos for transfer in in-vitro fertilization, for example. Predicting complications in coronary interventions, length of stay in the ICU and so forth. Again, we need the data from a lot of patients, typically more than one hospital in order to get to understand those patterns and make accurate predictions. Here's an example, suppose you wanna find all data about type one diabetes and HLA mutations. How do you go about it? Even though there are online sources of data, the best data are still found in health care, in hospitals and medical systems. And we have international collaborations as well in terms of transforming big data in using artificial intelligence to build predictive models but doing all of that with a notion of preserving privacy. So, I would like to talk a little bit about the United States situation in which we are allowed to share de-identified data without a patient's explicit consent. However, our group and others have been studying re-identification and we can tell that data are re-identified no matter how you mask or remove certain explicit identifiers and the success of this de-identification just depends on what is disclosed, to whom and how much in terms of time and effort, the attackers are going to spend on trying to re-identify a particular piece of data. Now, let's talk about the general data protection regulation which is in European countries and we have several collaborations with these countries. How are they addressing this differently than the U.S. regulations? So they explicitly state rules for business and organizations and the rights for citizens. In the GDPR, the consent has to be specific and opt-out selected by default is a violation of the GDPR and multiple types of processing may not be bundled together. So this is in general but for healthcare data, it also applies. The GDPR in artificial intelligence as opposed to what many have said, it does not necessarily stifle European innovation. However, it requires more care in terms of how to utilize the data. It requires the data to be anonymized and to have patients receive information about the logic used in decision support systems that apply to decisions made for them. It is explicit that pseudonymization is not true anonymization. So if there is a way to re-identify the individual then that's not called anonymization. That needs to be very clear because it's very different from the Health Insurance Portability and Accountability Act, HIPAA in which de-identified data are considered those data sets in which 18 identifiers are removed and they are listed in the regulation. Biometrics are part of them, dates are part of them, names, social security number, et cetera. There is another way to declare a dataset de-identified which is by expert certification. In terms of how people think of re-identification, I wanna make it clear, it's not that you receive a data set like in this table here and you immediately can find the names, that's not the attack that we're thinking about and rather it's a different thing. Suppose you know something about the target person. Suppose you have their fingerprint, which is a biometric, for example, or her genome code. If you have this and you can go to this table and then read out the rest of the table, you discover that this person whose fingerprint you have is pregnant and has an income of 60k. You don't need necessarily to know the name of the person but you have this extra information about someone whose biometric you have and who is uniquely identifiable in that sense. So you know something about the person you want to know. Do you wanna read out the rest? Combinations of variables, if unique might lead you to the same conclusion in the same disclosure. So combinations may make someone unique and therefore re-identifiable. And sometimes you don't even need to be unique, if all the combinations, for example in this simple example, if the diagnosis is the same then you don't need to know if your person of interest, Lisa, is this row or this row because they have the same information that you're looking for. So privacy breaches can happen in many ways and it's very hard to detect them. There are several techniques for this and I won't go into details but I wanted to emphasize the genomes are the best biometrics possible and therefore, they should be protected as such. Sensor data can be a proxy for biometrics too because there are very unique combinations in the data provided by sensors. So re-identification using wearables, for example, there are plenty of articles out there about it. More important, it goes beyond the individual. So a few years ago, the golden state killer was identified because of a genealogy database and not because the person was in that database but a relative was in that database. So this re-identification aspects expand now if you use DNA beyond the biometric of an individual to the biometrics of all blood relatives. Sometimes even the name of a database can be a private breach. So, imagine a public de-identified database of some stigmatizing condition, X. And again, this is very subjective of what this condition is but let's say this disease X in here and this database, a lot of subjects who have this disease. So the mere fact that if you can identify that your person of interest is in that database, you immediately know they have that condition X. So even the name of the database can itself be revealing of information. So we got to this area in which are we exploring data? Are we exploiting data of individuals and what are the different aspects we need to consider? So we have to do the right thing. Is it ethical to share when people have not been explicitly asked and don't know who is sharing which data? Is it ethical not to share since discoveries could be made because we need those large volumes of data? Is it also possible that people could choose what are the implications of that? And I'll talk a little bit about that. So I'll talk about the technical aspects in a little bit about also this possibility of choosing. So this privacy technology solutions which I will emphasize our mitigation strategies as opposed to a complete solutions, can be divided into data-centric in which you manipulate the data or institution and people-centric in which you manipulate or you create policies to protect the data. And ideally you combine both. We had a center for biomedical computing focused on data sharing and analysis. So I will present a few of the techniques that we explored in that particular center. One is called differential privacy and it's typically when you share data sets or you share a large amounts of information that the user takes home and does the analysis there. There are more security aspects called homomorphic encryption that I wanna talk about but essentially you analyze the data while the data are still encrypted. So you never decrypt the data. There is also secure multi-party computation, in which you do not put the data all together and you share certain information but not that data themselves that allow you to compute that way and I'll talk a little bit more about this third item. And this is a collaboration that we have with several hospitals in which each one, each institute keeps their data separate under their own firewalls and we still can compute with all those data for simple computation, as well as more complex ones. So a user requests data, there is a trusted broker who decides if that question can and should be answered, contacts the institutions, they send intermediary results, they do not send actual data and then the answer is computed and returned to the user. This slide is not a joke, it is just to show that you can decompose very complex calculations if you can decompose by sites. So the first derivative in the Hessian matrix are two calculations that are very important to do a logistic regression, for example. We show here that if you can decompose by site, so the sums are done by site, you sum the data from your patients in site A and site B and so on and those sites only share the sums with each other or with a center site. Then you can do this calculations without ever putting all the data together. So, no patient data needs to be sent from sites, only the aggregates. So this is what we call horizontal partition. You have one hospital with their patients' data, another hospital with their patients' data and you compute with this data without necessarily physically putting them together. There was another way of partitioning called vertical partitioning and one side has some portion of the data. Another side has another portion and you also can do this distributed computation. So this is all done by decomposing what can be very complex analysis and doing by site. So, for a long time, almost 10 years now, we have this network, the clinical data research network in which we have all the University of California sites plus our collaborators in Los Angeles and the Bay area and also some other institutions in which we cover a whole lot of people. The VA system is also the largest one here and we can consult data using these techniques that I talked about, so how it works? We need to have agreements and we need to utilize a common data model. So, we're talking about the same data across the systems in shared ethics principles. So, in here is our latest work, our project related to COVID-19, in which the blue crosses here represent one hospital in each of the VA systems and then we have also the health systems predominantly in California but a few others across the country. And also one in Germany to show that we can do this international collaborations consulting with patient data without violating the privacy of individuals or of institutions. So, in this particular COVID-19 collaboration, we have our partners from before, as well as new ones, the one from Germany, Texas and Colorado. The data warehouses have over 50 million total patients. We use a common data model and again, we've been working together for a long time. This diagram just shows that the aspect of the system, the user asks the question. If the question is already answered, we return that. If not, we transform that questions into code. The code goes distributed to different centers and they provide the answers. We aggregate the answers and return all the way back to the user. In this case, this is a public website. So anyone can ask a question and we will prioritize and select the questions that we want to answer. This is the website one and then you have questions and answers with some disclaimers, caveats of observational data that people should know about. And then you can get answers, for example, for the question, how the use of glucocorticoids up to a year prior to hospitalization to patients taking that have worse outcomes than the ones who do not? And the answer seems to be no. Their mortality is in fact slightly lower than those not taking these medications. We also did per race, for example and there are differences there that we need to further investigate. Another question was in hospital mortality, how does it compare by age, ethnicity, gender and race? So you see by age here, that is a very marked increase in mortality for patients older than 80 in that that goes down, the younger the patients are. So this is very interesting because also another finding of ours from almost 11,000 patients in 10 institutions was that patients declared as Hispanic or Latinx ethnicity had lower mortality and how can that be when we hear a lot more the opposite and the answer is exactly by doing those adjustments. So logistic regression done across the sites and in this case, eight institutions without moving the data around. So this line, the log odds ratio, if it crosses here, it means that the variable, the factor has no influence on mortality. We see that age is a strong factor but gender, race and ethnicity don't seem to be once you adjust for age, important factor. So this is important to know and again, doing this calculation without aggregating data is quite novel in clinical data research networks. So I will also like to quickly address what I call the people-centric alternatives in which is to get informed consent for data, using research and informed consent, potentially to transfer those data to other institutions. So currently the status quo is if a user asks the question, there are data use agreements and the healthcare institutions decide on behalf of the patients, of course removing key identifiers, names and social security numbers in delivering those data to the user. We had thought of a different model in which could we ask the patients and whether they want to do this or not and then issue or transmit the data from the patients who said yes? So we implemented that this study at UCSD a few years ago, in terms of not only which items you would be willing to share but also with whom? With UCSD and VA hospital, researchers only or any researcher from nonprofit organizations or also with for-profit organizations. We honored those preference as well. The study was happening at UCSD, in which subsequently extended to UC Irvine as well. We included items that have been shown in the literature to be controversial. The patients might or might not want to share. We did a post-election survey in which I would summarize that the people feel good when they are asked even if they don't change at all, the status quo of sharing those data. So, even some felt more willing to share now that they know that they've been asked. So do you feel more comfortable knowing who it is? Yes, so that's important to know, who is getting those data? And interestingly to us, people seem to want to know each time their data are being used and to be notified which we thought people would try not to be bothered so to speak about that but that wasn't the case in the study. So again, it works by having the data from electronic health records, from the process of usual care to go into clinical data warehouse and to tag that warehouse with the preferences of the patients. And this was published a few years ago and I will walk you through this slide but the extremes are important. So in red here are per item, the percentage of patients who did not want to share those items with anyone, no matter who was the recipient. In light blue on the other extreme, is the people who would share that item no matter who was asking. In this middle area here, in purple is those who would like to share but only with UCSD and VA hospital, so keep it locally. And those who would like to share with any nonprofit institutions. So you can see here that we were probably doing the right thing for 50% of the people, not for 10% of the people and then depending on what the study is, we might be doing what people want for those here in the middle category. We also extended this research to know if it was important to have every single item or just categories. So we randomized into detailed in simple forms to see whether there was a difference and I can tell you upfront, practically there is no difference. So it is feasible to do a much simpler form than to go into every single detail. We also did an opt-in and opt-out but in that case, it was very clear that there is much more sharing if you leave an opt-out version than if people have to opt in for sharing. Again, this is not a large sample, it's 1200 patients from two medical centers but it is revealing of what people want to do. So we concluded that the majority of patients do want to share data and biospecimens for research. Some are more selective in terms of who they want to share with and there were higher rates of sharing for people who are older and who were adequate health literacy by a standardized scale. And this is just showing, again, the one line in this case, the odds ratio here being neutral but showing that every single item we got more sharing, if it was an opt-out version versus an opt-in version. So this is interesting to know and also there was a very positive patient experience. Again, people who participated were likely to be interested in privacy and to understand their privacy preferences. So this was published last year and then we now extended the study with a mobile platform in many more institutions than before. So finally, I wanted to conclude that the GDPR seems to be protecting the privacy of individuals a little bit ahead of what the U.S. regulation is doing right now. With international collaboration, we will improve AI models and data. So there is societal benefit to do this, but the need for consent needs to be carefully examined because we now have evidence that patients do think differently about their patient preferences. And the GDPR, like implementation will require funding. We require investments in the U.S. but it is a well worthwhile endeavor. So, finally I would say asking people is feasible and we have shown that and I think more exploration of this venue needs to be done overall. And I would like to thank the funding sources and also thank Dr. Kalichman again for the invitation to be in the seminar. - So thank you, Lucila. That was an excellent comprehensive presentation covering a lot of interesting issues. One of the things that comes to mind is that it's a complex topic and yet very important. So I wanted to try and dig down into a few issues with you and I'm dividing the world into sort of three categories of things. One is the value of the data that can be collected this way. The second is what kinds of things you and others can do to try and protect privacy for people whose data is being collected? And then the third thing is that informed consent process which actually has two aspects to it. One is that somebody really understands what it is that they are consenting to and the other thing is that they have consented, that they've agreed that they want to do this. So, I'm gonna begin with a focus a bit on the question of the data itself and that terminology de-identification. So, just for the record, I mean, whenever you collect information on anything there's identifiable information that goes with it. And when we say de-identification, our goal is to try and minimize information that might point to an individual. But you said that even though you try and de-identify, it can still be identifiable. So could you give an example of just how something you would think wouldn't be possible is possible, what would be an example of that? - One example would be, let's do a survey of all faculty at UCSD and then let's ask a few items about them, what their age is, for example, the gender, the area that they work, the school and department and then whether they were originally born in the United States or not. If they speak another language what language that is and we provide no names whatsoever. I'm pretty sure we can re-identify a whole lot of people who are not the majority that would have been born in the U.S. and would be within a certain age range or females in certain departments and so on. So I think there is an illusion of de-identification that is just, identification is just the degree. It would be much easier if you provide the faculty's name but with a few more steps, you can re-identify people. - Yeah, so clearly with information that might not directly point to an individual, somebody could figure out who that individual is. A few years ago, I heard an example of how extreme this is, that apparently if you just know the birth date of someone and their zip code, you can almost uniquely identify a really high percentage of people. And most of us would think well, that couldn't be possible but there's even a website, if I recall that allows you to try this yourself and find out how many other people have your birth date and your zip code and it's typically very small. So that's an important aspect of this why we're worried. So there's a question that's come through that's relevant to this opt-in and opt-out juxtaposition. And so before we get to the question, I just wanna be clear because the terminology ends up being confusing if people aren't immersed in the field. So when somebody says we are doing something with an opt-out situation, that means that they have to choose to be out. They have to choose the option to be out and if they don't, the default is they're gonna be in. So for example, if you say, it's opt out for people's data to be used, then all of their data is gonna be used unless they explicitly choose to opt out. Conversely, to opt-in means that you automatically will not be included unless you choose to opt in. So, first off, I think I have that correct but correct me if I'm wrong. - That's right. - If that's correct and part of the reason I wanted to go through that is 'cause the question I think might be looking at it the other way around. So the question is by setting the default as opt in, that concerns me due to the ages of my friends and their tech abilities. So I think what they're saying is, if you set the default that people automatically be opted in, then that's where it becomes a worry. So what do you think, should we be worried about that kind of situation? - Yeah, what we have found in our research and confirms what many have seen in other fields, whatever it takes the most energy to do will be done less, meaning you will leave your data to be shared if you have to actively go somewhere and opt out of it and you will share less, if you actively have to say, I want my data to be shared. Now, which one is the right way? It's unclear, right? If you're a conservative, you'd say well, don't share until someone actively authorizes you to but the reverse can be concerning as well. By not sharing, are you depriving society of certain individuals who just are being excluded because of that? - It sounds like you personally have not made a judgment yet which way you feel about that. You aren't sure yet whether the good of society should take more precedence over people's ability to choose whether to be involved or not. Or do you have a personal view about where that should go now? - I think we and others probably have a feeling that it depends on the circumstance. It depends on how much that value is expected to be there or how low value it is. So it's a trade off as with everything else, right? What's the value of having a lot of data but risking privacy of individuals versus not sharing as in in many other countries and therefore possibly not doing discoveries that you might do? So it's very hard to measure this since we don't know what discoveries could or could not have been made. - So, I'm interested in that and might come back to it later if we have time but I wanna emphasize the questions from the audience to the extent we can. So, one of the participants says, in your last slide, you shared that data from social media can be used as well. Can you please explain how we can use this data safely in clinical trials without infringing any privacy laws in studies related to behavioral health or mood disorders? And then the second question they have has to do with the IRB but let's do to the first initially. - Well again, social media data are being shared quite a lot as we know, right? By the companies that collect them, by researchers who purchase those data sets many times and by simply participating in certain forums. People do provide their data for free and those data might be sold or exchanged for value by the companies. So in a way, it is somewhat of a contradiction that data like that can be freely shared whereas other data that might have more value in sharing cannot. So I think that's again, the regulations, not only for social media, for entities collecting data that are not healthcare entities, covered entities is completely different. So we need to just try to over time, I think there should be more convergence and coherence in terms of what is it that we're trying to protect? - So again, might be my case question. So, the second half of this person's question regarding social media refers to the IRB. I'm not sure if all of the participants are familiar with that. The IRB, that means Institutional Review Board, institutions like UC San Diego in order to receive federal funding are required to have a review process to look at any research involving human subjects, so that we protect the welfare of the participants in the research and that group that does that is called the Institutional Review Board or the IRB. So their question is, is that review process required when using information from social media? - It should be required, if you were researcher at UCSD, for example and you're doing that type of research. I think you should, at the very least consult with the IRB because even though it's not UCSD's patient data and so on, it's still data about human subjects who might be re-identified and who might not even know how those data went out and were shared freely. - Yeah, so for the record I do not serve on the IRB but my very strong presumption is that anything involving research and human subjects that you would need to go to the IRB and at least ask, if there's a special circumstance where perhaps you don't need their review but under normal circumstances, the default here is that you would have to get IRB approval to collect data from social media for the purposes of research. So, I'm just gonna go down through some of the questions here. So next one is what is the practical concern about the data? It seems to me that it is completely understandable that with Google type resources, all of our data could be made transparent. What are the practicalities and concerns? I'm gonna try and interpret that question a little bit 'cause I'm not quite sure where they're going. I think they are saying in some sense that so much information is already out there. It's with Google, we can search all kinds of information and in some ways there is no question of privacy anymore. There's an author in San Diego, David Brin, who has argued that privacy doesn't exist anymore. We should just get over it. So I think that's what this person is concerned with or pointing out and they can come back with another question if I haven't got that quite right but what are your thoughts on whether all this information, everything's out there anyway and why should we have practical concerns? - Well, I would say people are different and there are some who we're not worried about. That precisely because of that, they post pictures of themselves and family and freely share on social media what's happening, what's not happening, health data and so on. However, there are people who are and so again, that's a difference. And I remember very well traveling to Switzerland and wanting to take a picture of a church or some other landmark and people, they paused and said, well, let's allow those people to get out of the way because people don't wanna be showing up in someone else's picture, for example. Or before it was an issue of social distance right now, it was a distance in counters. You need to be some feet away from the person who is at the airline counter for example, discussing their trip and so on. And I felt that there are people who are concerned about those privacy issues and I think we shouldn't assume either way because I think some way that everyone should think the same way and I don't think privacy is completely lost. I would say you can still would like not to have Alexa listening to you or other things happening, right? You can turn your cameras off. There are a lot of things and I think this thinking of all is lost and so let it be just adds to somewhat of defeat. We can't control privacy anymore and I think we can. - So just for the record, I think David Brin would argue as I recall that, although it seems like he's giving in to defeat. He's saying what he does want is that we should know who is using our data and what they're doing with it, which gets back to your point that many people said they wanna know what you're doing with their data. So that seems to be at least, you know, there are many questions that might not be answerable, that one apparently is. People probably wanna know what's done with their data. So, somebody's asked an interesting question about when we should be having these ethical discussions. So how much of what we are hearing is the typical technology paradigm, which is ready, fire, aim, where in the ethics come after fire instead of before? In other words, we do all of the technology work and we get the technology applied but we don't do the ethics until later and they're saying ethics should be part of the aiming, should proceed deployment of the technology. And they're asking, can you give some examples of where we got ready then aimed then fired in this field? - I think at least if you think about the clinical trials, for example, right? I think you can't execute them until you go through ethical review boards, institutional review boards and so on. So I think in many areas, it has been regulated and in devices too for safety, for the security of personal monitoring or other medical devices. So in the area of behavior though, is where there wasn't, right? And social media came and people started sharing without actually thinking of what else was going on. But I think in the medical tradition, not always, but in recent decades, ethics came first and then the drugs or devices were tried and then decided whether there was benefit or not. I think this reversal in this somewhat culture of let's do it and ask for forgiveness later is wrong. It's just wrong. - It happens but I think your goal is to try and do otherwise. We have a comment from somebody who I won't name unless she comes back on and says she wants to be named but somebody who I have reason to believe knows what she's talking about. So she said for social research, we were talking before about whether if you were doing something with social media, they say, well, IRB is always recommended. It is important to note that IRB only required if the research is federally funded under a common rule that the federal government has in place for human subjects research. Having said that, I am reasonably confident being at UCSD that the university is trying to apply these approaches uniformly and not worrying about whether it's federally funded or not but instead would ask the same questions. So if anybody on this call, participating today, at this meeting is at UCSD and you wanna do research on social media, ask the IRB to make sure if you need to get a review of that or not. So here's a question related to COVID. One of the fundamental issues that have come out of the COVID data has to do with location of people who have tested positive. This has huge implications for people reacting to how they behave and interact with others in the community. What is your ethical perspective on how this data should be handled? I think the ethical question we might have here is should we err on the side of sharing that information as opposed to protecting people's privacy? - Well, again, I would say depends on who is handling that data for what purposes, right? And we work more in the area of research, less so on public health, of measures and what is expected of public health officials. But from the research side, I think one of the reasons IRB or other committees are involved is do you really need this data? What are you going to do with it? Does it seem reasonable that you have the street addresses of everyone who tested positive? What are you going to do with it? And if you're just collecting general statistics, then maybe you don't need the addresses. Again, it's a lot of who is doing it, for what purpose and is there any chance that whatever you're doing will result in overall benefit that is not just for profit or for some reason that is not more important for society? - Yeah, so what you've pointed out here is an important distinction that I was hoping we would get to and that's the difference between research and for example, public health. Where the standards you might have would be very different because of the context. We ask a lot of researchers, I would argue because it's in a certain sense a privilege to do research. It's not something that we can do whatever we want. We have some sorts of obligations that go with doing that research and one of those is to hold a high standard and it's not just to be ethical per se. It's so that other people who might be recruited for research studies in the future will feel they can trust us because they know in the past, we took good care with what we're doing. But in the public health arena, it's a different sort of question. There their highest responsibility is to protect the public's health, not to get new information for science. So, along those lines, related to the difference between research and other uses, another question is, isn't a lot of our data such as medical data already shared via insurance, such as Medicare? Researchers have access or potentially could have access to that data, so are there other ways to access data via insurance versus having to get data from the individuals themselves? How does that change the equation here? - Well, the so-called claims data, right? The data that insurance has is not always complete. So for example, insurance will know that you had an x-ray. It may or may not know what the result of that x-ray was or more details about it. It might know that you took a test for a particular condition. So there was suspicion of that condition. Otherwise, a test would probably not be ordered but the result of the test itself may not be known by the insurance company. So I think yes, it's a good source of data. It may cost people a lot to acquire it. So that's one reason that not everyone has data about everyone and again, the more sources you have the more you can integrate data, right? So in research from the past, some researchers got voter registration records and linked to medical records, so-called de-identified and were able to read out who the people were, right? So the more data out there, the more possibilities of doing these linkages and again, it's just two things, how much people are interested in your particular record and how much effort and money they're going to spend on trying to find something? So there is no, again, complete privacy. Is just you have control of certain things and not others but I think one of the biggest dilemmas is who already has your data, right? So what is the incremental risk for this other data, if I allow it to be used? - Yeah, so this is a really important point about our data and what we understand and it gets to the informed consent issue. That is one of the things you're thinking about, what should we ask people they want and they don't want. And so, what we're doing today I hope is helping people understand that something that is not obvious, that information that you give that you think might be innocuous in one place and give some other information that you think would not be harmful in other place, those two pieces of information could be put together to learn something about you and identify you, that you might not have wanted known. So, this is the informed part of informed consent. How much do people understand of what these risks are? So I know you've been doing some studies to try and understand what people prefer in terms of data sharing but to what extent have you felt that, I hesitate to ask you for a percentage but I'm going to anyway, what percentage of the population you've sampled understood these risks before you explained them to them? - That's a good question and whether we will ever know how much people really understand because I had to spend what? 30 minutes here talking about how you read out things how a biometric is the best identifier but you can have other identifiers or combinations that identify people. I don't think it's a complete understanding and that we, ourselves research in that particular area have. So we can't put a number into what's the risk of re-identification. If we could, then I would say but, well, if it's over 50%, I'll do this, if it's less, I'll do that but that's not the case because we don't have control of what other information about you is already there, right? And how to remove it there. I mean, it's a whole economy of collecting the information and then another one of removing that information for you and things like that. So I think it is kind of the very primitive. So, if we look at where we are now, 30 years from now, 20 years from now even, we'll say, wow, I can't believe, it was like the wild west, people could do a whole lot of things. There were so many loopholes, that look what happened. So there will be, I believe, plenty of case studies of what's happening today that people will come up and then say, how come they didn't think about this at that time? - Yeah, I mean, so these are all important points but when I see how many scientists working in the field similar to this are sometimes just coming to the realization of how much risk there is, I am pretty confident that most people don't know. So there's this interesting situation where, if you ask somebody to choose to share their data, they have no reason or priority to worry about sharing their data. They might not see any risks to that. So your first job is to cause them to see the risks so that they might choose not to share their data but then you have to convince them that you're doing something to protect against those risks so that they might be willing to share their data which is a lot to cover. As you say, you spoke half an hour today and I'm not sure that would be enough to convey all of that to somebody. - Well, I think we definitely need to do that but we need to do the other part too, is what is the benefit? What is the potential benefit of this? And is it worth it? Then for you in your particular today, in these circumstances which might change, 20 years from today and we did see that older patients were more willing to share than middle-aged patients, for example. 'Cause there might be less to lose at some point or more altruism and more understanding or perhaps the risks might be lower. There might be several reasons and again, we generalize by age group and so on but we shouldn't because there are variations everywhere. So, I think that's what I would use technology for. Technology does allow us to personalize things, ask the question, do you wanna do this or not? Do you wanna be included or not? And then study it whether people who are more conservative about their own data does the exclusion of those data bias research in some way? So we need to study all of these topics, I think before just prescribing we should do this or we should do that or what we haven't been doing, I think is actually looking at the problem as a problem and trying to find out more about it and how people behave, what their fears are, what their satisfaction in sharing data is and so on. - So, I could dig deeper on that but there's a couple of questions that I think they're intriguing from the perspective of ethics. One is what is the process for determining what is ethical? Are there specific ethical frameworks that are consulted which I will help address in the extent you want to but another comment that's related to this, I think very closely related is they are saying they have not heard any comments that are useful relative to the ethical decision points. So far has really discussion about how personal data can be derived from collected data. That is not a discussion of the ethical considerations. So far all I've heard is that it depends. So I'm happy to address a little bit generally the ethical part of this but maybe you can bring it back, Lucila then to your own work and how this fits. So I am not a philosopher, I'm not a trained philosopher. I'm interested in philosophy and ethics but I'm not an expert in that, I'm a scientist and the ethics center is based, I think largely on that prevalence of science first and then asking, how do we answer the question, how should we act? Which is a question about ethics. And as suggested by the first of those two questions, there are many different frameworks people use, different ways of thinking about ethical questions. So for example, you could ask what is the best we can do for the maximum number of people? You might ask a very different ethical question, what are my obligations to an individual to protect them from harm? And sometimes those two issues come into conflict and that's clearly the case here when we talk about public health or research. We're getting all of this data that you're working on, Lucila and others like you. Getting all that data will help us do better for society. So more people will be protected from illness or we'll be able to handle that better but the flip side of that is that to get that information, individuals might lose something. They might lose their privacy and so this particular individual might have something known that they don't want known. It might harm them in a variety of ways. So those are just two examples of ethical frameworks people might think about when they are saying, is this ethical or not? And unfortunately there is no sort of computer program you can plug into to get the answer to what is ethical and what isn't ethical. What we do, whether it's through an institutional review board or a researcher who is paying attention to ethical questions, in both cases, you do your best to try and consider a variety of ethical perspectives. Now, I am hearing from you, Lucila, I would argue, you are right now in some senses choosing to err on the side of protecting the individual because you wanna give people the choice about which data should be collected or which data shouldn't and recognizing that there's a risk that you might learn less because you can't get as many people in the research study but I don't wanna put words in your mouth. I'm just trying to put it in that framework. Is that sort of where you are in thinking about this? Or what do you think? - Well, I think it's not just what is right in general and let me just put it this way. I don't have training in ethics or the frameworks. So what I say would be probably very naive on that area but there are different cultures that protect different things in terms of privacy, right? That some are very upfront about your diagnosis of cancer and some people are very reserved or have considered that somewhat something not to be disclosed for example. So, what bothers me is the contradictions that we see out there, right? If we can't have a national ID because that somehow violates some right to be forgotten, some very individual aspects of things, then why can't we also have a choice on donating data or not as we have of donating organs or not. So, that's search for coherence, it's at the heart of trying to pursue things that are on the side of being consistent with allowing individuals to choose a lot of things but somehow not allowing them to choose in this specific area. - Yeah, so I think, in part, you've said something that the second questioner was concerned about, the idea that in some ways it depends 'cause it depends on the cultural perspective in the background, for example, of the individual who had may put greater or lesser value on privacy, for example. And yeah, I think the answer is it does depend. It does depend on these things and it is probably not going to be in the best interest of science to say we're gonna have a one-size-fits-all answer for ethics. So we're gonna decide everybody's data should be shared because we've decided that's the most ethical thing for society. We're trying to give some deference to the individuals. Having said that, there is variation in individuals and the next question is I think a really important one. So this person says, thanks for a really interesting talk. My question is about issues around protecting the privacy of individuals from underrepresented groups. In your work, have you surveyed their answers, oh, sorry, surveyed their attitudes to and perspectives on the adequacy of the information they are given in advance and their level of comfort with the ways informed consenting has done around data sharing? These are groups who are very legitimately have been hesitant to participate in research and large part due to a darker history of medical exploitation in this country. So for example, people of color, sexual minorities, gender minorities, et cetera, can you comment on how some of these issues of data privacy might be particularly acute to them and how those might be mitigated? So, is this something you've looked at at all, underrepresented groups and their attitudes? - Our study was small in numbers so that we couldn't slice into groups that would make statistical sense. So I think it is definitely something to be studied and it's relatively understudied, I must say. It's studied primarily as surveys but less so in actual action, right? Would you remove your data or not and so on? And also there is the very important perspective of the tribal consultations, for example, with the American Indian nations. Because in that part, maybe the intent of the individual does not overcome the intent of the tribe as a whole declared by the leaders. So I think there's a lot of understudied aspects of this that I don't have answers for but I do agree that some segments and now when generalized necessarily into racial ethnic or cultural or it depends a lot on which environment you are but I think that there are marked differences from one person to the other, it seems quite evident to us. - Yeah, so at this point, thank you. There's a complexity here about differences and digging into the data that I think is worth spending a moment on. And if I could, I'm going to share one of your slides 'cause you gave me a copy of your slides. We now have a chance to sort of step back for a moment because there's I think a really important question embedded in this slide and part of your conversation. You were talking about the question of whether there are disparities or not with different racial groups or different groups. And I realized this slide gives us an opportunity to see something that's relevant to lots of questions like this. And I hesitate to use the words, alternative facts but I'm going to use them because you really can come up with a factual conclusion that is accurate but you have to be very careful about what you're looking at. And for those people looking at this slide, I realized that the font is small on Lucila's slide. So it's a little bit hard to tell but there are four sets of bars here. Every set is a blue bar and a yellow bar. So for what I'm gonna talk about now, ignore whether it's blue or yellow because what I'm really interested in is in the mortality for each of these groups. So the second set of bars is black or African-American and the last set of bars is white. So somebody might say, is there a racial disparity between these groups and the impact of this disease? So you can actually infer three different answers to that question looking at these data and I think this is part of the direction you were going with, Lucila. So correct me when I'm done, if I'm wrong about this or not but you could conclude that there's really not much of a disparity because if you look at the black or African-American groups, there's about 11% mortality on average between those bars and it's pretty similar for the white groups. It's around 11%, maybe a little bit higher but somewhere on that order. So you'd say, there's no difference for black or African Americans versus white in terms of mortality. But then you could look at the data a little more closely and you see the number of white patients who are here that are dying, it's actually a fairly large group compared to the African-American group. So 4,000 black or African-American patients versus 6,400 white patients. So that suggests that more white patients are showing up and being at risk of disease and by the way, I'm getting these numbers by adding up the numbers within each of the columns. So that data are here, it's just that whites are more at risk. But then if you dig still deeper you should remember that the percentage of our population, that's African-American is only about 15%. It's less than 15% in this country. So the white population should be four times what the African population is but it's not, it's very close or it's closer than it should be. Which means that African-Americans are disproportionately affected. So, what I'm showing, I hope here and Lucila, I wanna come back to the point you made in your talk, what I'm showing here is that depending on which question you ask, you can get ostensibly very different answers and Lucila, I think in your talk, you talked about that you also have to look at the age of the groups that are present to ask about whether one group is at risk more than another. Because if everybody in one group is older, then they're gonna be at higher risk because we now know pretty solidly that elderly patients are more at risk. So, sorry for that long diversion. But I hope that helps clarify some of the difficulties here. - I wanted to make it clear that that was an unadjusted analysis, right? Just the statistics there and when you do the adjustment, it was reassuring to us to see for those patients who did come to the hospital, who got hospitalized because we know there are tremendous disparities even to get into that point. But assuming a patient is hospitalized, it did appear in our adjusted analysis that there wasn't a difference in mortality related to ethnicity or race and that was important because it kind of removed certain speculations about biology, about biological aspects of the disease that might be manifesting differently. It's not that we can exclude that but looking at relatively large numbers of hospitalizations, when adjusted for age, the mortality does not seem to be different and again, that's adjusting for what we know what to adjust, imagine other things that are there and so on. So I think now more than ever, it's on the part of scientists and science to be very clear about limitations of analysis and of data and of everything that comes out there because the mistrust in facts or data or how people talk about spinning things this way or spinning that way, there is no such thing, right? The data are there and there are ways to interpret with adjustment, without adjustment but always with the caveats that we don't have all the information that we wish we had. So, I would point out that that's where the notions of let's have more people look at everyone's data because someone smarter may show up and citizen scientist is in a lot of those things. I think it needs to be also examined in the light of, can you control how people behave with those data? And there are consequences of mishandling the data as there would be with researchers, right? People lose their jobs, people lose their licenses and a whole lot of things. So, in the enthusiasm of sharing more because we are very desperate to find answers to a lot of medical problems, sometimes we should be very careful not cutting corners because all of these regulations in IRB and things, they were there for a reason. - So I want to get to another question, which is related to groups that might otherwise be underrepresented in various ways and related to informed consent. A good example of the proper consent of indigenous peoples in the U.S. is the study of INH among the Navajo, a very large population in the mid 1950s. A less noble example is the research done on the Havasupai, a small group of only 300 individuals. What are you thoughts about ethics involved in both cases? And I don't know how much you know about either of those cases, Lucila. Do you have any thoughts about research that's been done with indigenous groups and paying attention to the special circumstances? - Yeah, I would feel a big imposter in commenting on those cases because scholars who really understand the aspects would be much better sources. I'll just say that definitely the bad handling of cases costed future researchers a whole lot and again, one thing gone wrong puts the whole field back decades and that's something you observed with COVID data and the retractions of journal papers. On papers that allegedly had consulted a lot of hospitals and came back with these answers about hydroxychloroquine, about many other things and now it's extremely hard for us who then work with the real data and real hospitals and can reproduce the numbers to get journals to consider. Because I think those studies tainted the field of studying electronic health records and set us back many, many years. - I think, yeah, all of that is very true but I would add I think one of the aspects of some of these studies on indigenous groups that they're really fraught because especially if it's a small population, something you might learn about that group could be very interesting scientifically and maybe would be even of interest to some people who are part of that group but the result might reflect in a bad way on the entire group. So I know that institutional review boards now typically try and take that into account, not just how do we protect the individual who might be part of this study but what are the steps we should be taking to consider what the group would prefer to do? And so there are indigenous studies that have been done in the U.S. and Canada, in Alaska and other places in the world, South America, where there's an increasing emphasis on engaging members of the community in the design of the study and the review of the study, decide what's ethical and what's not and even in decisions about whether the research should be published because of the way it might reflect on the community. If you wanted to air on the side of, we want all research 'cause all research is good and all data are good and want it all out there, you would never allow that. You would say we can't do that but part of the point here is that again, the research community, I think chooses to try and set itself to a higher standard and would rather lose being able to produce new information than risk having to compromise a community or even individuals as we discussed before. So it's a quick question and I'm not sure you'll be able to answer it for your study but I think it's an important kind of thing we should be asking. So someone says, but where do Hispanics fall? This is from somebody who I presume is Hispanic because they say by most surveys, they lump us as white. So, in those surveys groups you had, I believe you had categories of white, you had black and African-American, but I don't think there was a Hispanic group listed, so... - I did not show there. We examined by race and ethnicity separately because race is more biological. It's certain characteristics and ethnicity being more cultural. So there are two different types of variables 'cause you can be Hispanic and black. You can be Hispanic and white. There are combinations in there and I think it's something that myself, I feel very uncomfortable with the racial categories, for example because if you're a 50-50, where do you count, right? This way or that way and so on? So I think that's something that I'll bet to 20 years from now. I'll say, well, I can't believe they categorize people and labeled people this way or that way. I think it will be just something very odd to future generations. - Yeah, so I think there've been quite a few conversations lately challenging the idea that race is biological which is, I think part of what's behind your point that to some extent, race is a socially constructed concept and somebody, because their skin is a particular color might be believed to be part of a particular race or they might see them as part of a particular race which is different than trying to look genetically and seeing where their roots are and as all indications are all of us have roots in Africa. It's just that some of us are more recent than others. So we are one species with a lot of mixing and that's going on of our genes. So anyway, that's an aside, these categories are difficult to try and sort out and what they mean. I think we've got time for one or two more questions. I don't know to what extent you're watching this, Lucila but it is an important question about what to expect you would find if you want to get people to participate in these studies. So it's saying, what's the trend in the U.S. of the public towards the sharing of data? Are attitudes becoming hardened against sharing of data? People are becoming that they don't wanna share it or are they becoming more resigned to the notion that you can't fight it? And I think it'd be a bridge too far to then ask and what are the differences between age, gender, economic, social class and race in those but do you have any sense of what direction has been going here of people's willingness to have their data shared? - I can't speculate. I think in Europe, it was much more clear, right? And the GDPR put some clauses there exactly to address what people's concerns were about privacy and here, it's very unclear to me, it is very unclear that people know that there are privacy issues and don't care or they don't know and therefore they can't care and things like that. So I think it's yet to be determined and which worries me because if the first thing happens is some breach of privacy, again, it is the same issue, right? If privacy is breached in a particular type of study is revealed to be unethical and not protective of data enough, it doesn't mean that all of them should be punished by the lack of preparation of one group, for example. But that would happen. So I think it's very unclear and I don't know if more studies on this are not done because we don't want to know or because it is not important. 'Cause I heard these two things from researchers. Why are you giving patients options? And one person was because it's obvious that there's nothing wrong with what we're doing today, so it should continue and on the other side was wow, they might all decline participation and then research is dead. But both were somewhat opposed to asking for different reasons. - This is an area that worries me quite a bit. So I remember some years ago, a researcher, I won't say where but a researcher was arguing that we should set things up so that all clinical data collected in the hospitals should be made available for research, that should just be the default that everybody's data should be made available for research. And they argued this would be great for science and medicine for us to learn things that will be important for society. And from that ethical perspective, they may very well be right. That on balance, more people would benefit from that than not. But I pulled the person aside afterwards. I said, "If you had the choice, "would you want your data shared?" He said, "Oh no, absolutely not." And what worries me greatly about that is sort of the paternalism that goes with thinking well, we know what the problems are with this but you don't. So we wanna collect your data. So maybe this would be a good place to conclude now with a couple of thoughts. So it sounds like the direction you've been going is trying to identify a few questions that you could give people options about what they would share, what they wanna share, what they don't wanna share and I'm just wondering, so how far along is that approach now in practice? I mean, is this being used at UCSD now? And if it is, how's it going? - It's not being used for patients at large, as it is still a research project and the next step of this research that we decided to pursue is looking at the deficiencies of the previous studies and seeing that one very important factor that we didn't probe was what was the research about specifically? Because we found from people that that's probably the most important aspect, is not who is handling the data or any of that but rather, is it going to create a new drug for this disease or is it more of a preventive thing or is it for a disease that I don't care about? Or is it just for wellness and so on? So I think that this new phase of our study is to understand that, is to understand the impact of the the study itself, what it is trying to accomplish. Still, I think there would be reservations on whether it's coming from a for-profit or a nonprofit. What we need to study more is that people seem resentful that others are profiting from their data and that's again, an interesting observation I think. Even if they would not have retail value on their own data, they don't want others to sell those data to anyone, which is understandable but again, it's one of many factors that I think need to be considered. - Yeah, thank you. Actually, this is a really good place to end an ethics program because what you're saying is that your perception is the patients are trying to make an ethical decision, they're balancing the risks and benefits. They're saying, what would be the value of this? How will it be used, who will use it? Against the risk that they might have in having their data shared? And that choice is one you're trying to help them with. So Lucila, we've kind of grilled you for quite a long time here. I really appreciate your time at the end of the day. Thank you very much for joining us this evening and although we can't do this with the normal applause, everybody is joining me in thanking you for your time and your insights tonight. (upbeat music) 