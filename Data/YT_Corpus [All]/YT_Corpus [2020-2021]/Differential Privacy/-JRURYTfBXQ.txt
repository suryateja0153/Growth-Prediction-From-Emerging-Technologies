 [SOUND] 911 operator, what's your emergency?  My wife is really sick, she's having trouble breathing.  What's your address?  [SOUND] 1235 right way, Greenbelt.  And what is your wife's name?  Karen Brown.  I'm sending help, please stay on the line.  This is a typical call that 911 operators receive every single day. Every call provides a wealth of information about a person in need. In addition to contact information, EMS and hospital staff and health information to Karen's medical record as they treat her. Most of this data is PII or personally identifiable information, which is key to a person's identity. This information is stored in large databases all across the country. [MUSIC] And use of this data is protected by law in order to preserve your privacy. But what if we could use this data without compromising your privacy? Imagine if we could aggregate data to see trends. What if access to Karen's and others' data show that 911 had received multiple calls about breathing problems around the same time? We might pinpoint the culprit. Right now, use of data in this way is allowed as long as we redact the personally identifiable information. But by removing PII, we drastically limit the data's usefulness and we don't always protect identities. For example, if Karen lived on a small street, her identity could easily be revealed just by sharing her age and street name. NIST scientists are seeking ways to improve the sharing of data while protecting individual identities through measurement and devaluation. By slightly altering the personal information, for example, Karen's age or home address, we reduce the risk of compromising privacy while still allowing data to be shared. Altering the data is like adding a bit of static noise to a radio broadcast. When your radio is tuned into the exact frequency of a station, you clearly hear what's being said. But if you move the frequency a bit to the left or right, static noise muddies the sound, but doesn't compromise the message. This process of adding carefully tailored noise and altering identifiable information within whole data sets is called differential privacy. When this process is applied to large amounts of data, it becomes a provable guarantee of privacy. Differential privacy allows all of the information to become useful and actionable data. This can lead to identifying underlying problems or trends in neighborhoods, like reasons for spikes and crimes or a higher number of accidents due to poor road lighting. In Karen's case, access to data revealed that her community was experiencing breathing problems due to overapplication of a new lawn pesticide, a problem that was quickly rectified. In today's data-driven world, differential privacy allows us to make more educated decisions that could improve the quality of our lives. [MUSIC] 