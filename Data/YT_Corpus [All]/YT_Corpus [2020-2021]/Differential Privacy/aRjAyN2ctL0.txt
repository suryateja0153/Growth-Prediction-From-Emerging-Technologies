 [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] hello and welcome back for another day of sparking AI summit we have another great lineup for you today but before we dive in I want to start by announcing our data teams unite hackathon winners we launched this hackathon back in May to see what kind of solutions data teams could come up with to help address issues around kovat 19 crisis climate change and social issues in their communities received over 50 submissions but three in particular bubbled up to the top as the winners of this hackathon I'll start with a third-place winner in third place congratulations to the data team from Booz Allen Hamilton who focused on climate resiliency for the Chesapeake Bay as a price the air box will donate $5,000 to the alliance of Chesapeake Bay on their behalf the team combined data from nitrogen in the water climate data and land use data to build a model that could predict nitrogen concentrations in the bay based on historical conditions congratulations again to the data team from Booz Allen Hamilton now for second place winners a big congratulations to the data team from shell and Umbridge who built wildfire a real-time detection system using satellite imagery as a prize the Arabic who donate $10,000 to the Amazon Conservation Association on their behalf this team trained the unit based on a commoner network using MFO given a satellite image it predicts the mask of the burned area of an image of a forest and estimates the total co2 emissions so great job and congratulations again to the data team from shell and Umbridge and last but not least our grand prize winner for a data teams unite hackathon is the data team from rev gen in Denver Colorado who led a project called taking it to the streets as a grand prize winners data box will donate twenty thousand dollars on their behalf to an organization called The Gathering Place which is a Denver based drop-in center serving impoverished or homeless women transgender individuals and their children for their project this team focused on the Kovic crisis they wanted to help businesses expand into the roads increasing capacity while also keeping customers safe so they use data science to determine on a street by street level the economic costs and benefits of street closures they found that restaurants benefited the most of all types of businesses when streets were closed they use Python sequel R on Delta Lake to take advantage of the team's diverse skill set and they've already begun conversations with the city of Denver officials about the analysis congratulations again to the Rev gen team in addition to supporting these important causes on behalf of the winners I'm going to remind you about our fundraising goal here at SPARC in AI summit the Center for policing equity and the n-double-a-cp legal defense and education fund are two organizations that are doing incredible work to help drive racial and social justice we set an ambitious goal of raising hundred thousand dollars yesterday and we've made a great progress on this so far but we've got more work to do the Olympics will match all donations up to hundred thousand dollars so together we have the ability to raise two hundred thousand dollars for these important causes please do take a moment and click donate now in your dashboard and help us reach your goal I hope you were able to catch the keynote from dr. Gough from CPE along with the other keynote presenters yesterday we have another great lineup of speakers for you today both in this morning session and in the afternoon and to get us started I'd like to quickly pick up where we left off yesterday yesterday we talked about the lake house pattern and what we see emerging and how open source projects like SPARC 3.0 delta lake and we - as well as innovations like delta engine make this a viable architecture to support a broad range of data use cases today we're going to zoom in to two of those use cases first we'll talk about how we can enable data teams to work better together with collaborative data science and second Mattei will lead the talk enabling a full lifecycle of machine learning with ml flow which has quickly become one of the fastest growing open-source projects in our space so without further ado I'll summon Clements melt to talk about the new data science workspace whoa thanks Ally it's great to drop in and let's jump right into the next general data science workspace and I'd like to start with some old news if you're attending this conference you probably don't need any more convincing the data and the AI trend has been transforming all industries for a couple of years now almost all major companies are realizing that these investments lead to competitive advantages now if you ask me I don't know what those remaining twelve fortune 1000 companies are up to and as a result the job market has been playing catch-up to meet the demand three out of the top eight chops in linkedin's latest emerging jobs report are data and the eye rolls congratulations to you if in any one of these categories I'd say you have some pretty good job security ahead of you now one thing you will notice is that there's a diversity of roles needed to deliver on these strategic initiatives it's not just one abstract data and the AI engineer solving the world's toughest problems really is a team sport and you need all of these functions working together as a team and not in isolation of course data breaks we call this unified data analytics and we've built a platform for that collaborative data science workspace becomes the focal point of such a platform it needs to cater to all of these users needs and bring them together today I'm going to introduce you to the next generation of our data science workspace and open and unified experience for modern data teams but before we do that let's look at where we're coming from and we've already come a long way in simplifying data and the I to help data teams in a rate faster on the left here you can see a MapReduce program to count words you should probably remember this just in case it ever comes up in an interview of course writing this code snippet is just the first step back when this was still cutting-edge you would still have to worry about setting up a cluster writing a couple of more lines of configuration and hope you get it all right thankfully today you don't have to worry about any of this in data breaks you just ride a spark sequel query and hit run the cluster in the background all the scales for you and you just sit back and wait for the result now of course this is a very simple example but it extrapolates to the full complexity of data engineering at scale but as mentioned this is only part of the solution where are we when it comes to data science today when it comes to production izing data science projects today we're still dealing with a big mess if you've ever tried to do statistical data analysis or maybe train a machine learning model you know that the tools that are supposed to make a life easier are still very difficult to use in fact according to one study only about 15% of those Fortune 1000 companies have deployed the AI capabilities into widespread production the reason of course is that the tools available to these companies specifically in enterprise software haven't kept up with new emerging practices in data science and machine learning and as a result we are made to choose between three bad options that are all not that great the first and for many the most natural option is to just give everyone the freedom to do whatever they want on their laptop of course data scientists love that you have full freedom to install anything you need and you can move fast however you're pretty far away from your data you'll need to down sample and copied it onto your laptop and of course you don't have to sit in compliance to know that moving sensitive data onto your laptop is generally a bad idea and the folks who are maintaining your production systems are definitely not going to be happy to try to reproduce your local environment to address some of these concerns some vendors take the approach to just put those same tools you use on your laptop into the cloud essentially they're giving your virtual laptop now however just hosting to power and giving you a VM with scikit-learn or tensorflow pre-installed isn't that much of an improvement sure you no longer have to copy date onto your laptop but aside from security and governance there are no obvious benefits just more constraints and finally you may be asking yourself the question why not iterate on our production infrastructure directly well unfortunately those production hardened systems are not really ideal flexpay ssin and most data scientists will not be happy if you try to teach them kubernetes so you're left with a hard choice full freedom of a laptop slightly worse experience with the same tools in the clouds or a fully production harden system that no data scientist will want to use thankfully we're in a 21st century and with a little customer obsession and engineering we've been able to navigate these trade-offs our solution for modern data teams starts with the premise that developer environments need to be open and separative our workspace follows opens our standards and provides a collaborative notebook environment on a secure and scalable platform next the industry has already figured out best practices for versioning and CCD and delicate pastes so we integrate our platform with this ecosystem and provide those best practices to data engineering and data science where reproducibility is becoming more and more important finally to reduce the time from experimentation to production the same environment can be scaled to production deployments allowing you to manage the full lifecycle within one platform and bringing all of this together I'm extremely excited to announce the next generation data science works based on database and without further ado let me walk you through some of this innovation step by step before we give you a demo this is a screenshot of the current workspace navigation in database it brings together all of the components you need for collaborative data science notebooks clusters jobs models and access to all of your data at any scale for those of you are familiar with this interface you may notice something different we're introducing a new concept called projects to the left navigation panel one of the most common ways that data scientists start to work is to clone a kit based repository with projects you can bring all of your work to data breaks where you can access all of your data and use best-of-breed status or open source tools in a secure and scalable environment because party circuit based you can keep them in sync by pushing and pulling changes of course you can also switch between branches or create new ones this basic functionality provides you with a powerful set of capabilities to integrate your data books workflows with your C sed automation and then enables you to follow best practices when you move from experimentation to production and you don't have to learn DevOps tools now many of our customers have been waiting for this part its feature and I'm happy to announce that it is available in preview today now we have many more exciting features coming and I'll give you a quick sneak peek of those as well at the intersection of kit based project and Environment Management is the ability to store your environment configuration alongside your code this integration will allow us to automatically detect an environment removing the need for you to worry about installing library dependencies yourself and you know sometimes saying it just works is the most powerful statement and in this case we make it just work following the same behavior you're used to on your laptop we give you an environment that matches your environment specification and make it available consistently on all workers on an auto scaling cluster so now that your environment is all set up let's look at your code you can already import ipython notebooks to data breaks this allows you to convert you to paternal books to data picks notebooks and vice versa in the future we will store these notebooks in their native format removing the need for conversion this not only makes data picks more standards compliant but it also enables us to support alternative editors so if you want to we allow you to open these notebooks into pa'dar right here in database and similarly by the way for the our users among you we also support our studio however as mentioned earlier just providing you with cloud hosted open-source tools is not quite good enough so by default we will open these notebooks with the data picks notebook editor the data picks notebook editor can open to paternal books and in addition provide you with collaborative features like co-presence as indicated in the top right of the screen and real-time crediting is indicated by a colored cursor and to facilitate collaboration even more data picks notebooks also allow you to comment and leave comments for your colleagues all in one cloud-based environment now by allowing you to open to paternal books in the database notebook editor you will no longer have to make the trade-off between using standard formats and the collaborative features and benefits that database provides so in summary this is our solution for modern data teams I showed you how we provide collaborative notebooks based on open standards integrate with the kit ecosystem for collaboration and reproducibility and provide integrations with cs-80 systems for a robust workflow from experimentation to production all on a secure and scalable cloud platform but you don't just have to take my word for it so let me introduce Laura and Ritchie who's going to give you a demo thanks comments for the introduction let's jump right into the demo for the purposes of this demo imagine I work at a big retail company we used to do our forecasts on a quarterly basis it's a big effort for our data scientists to come up with those forecasts using many different tools and once that is done we print them to PDFs and send them out by email that has led to lags and decision making because we use outdated forecasts of course these days the world is changing at a rapid pace and our leadership team has asked us to move from a quarterly to a weekly basis that will significantly improve the quality of our business decisions like how much inventory to order the amount of manual work involved in producing this forecast is prohibitive in doing this more frequently so as a good data scientist and determined to automate this process and provide our decision-makers with an interactive dashboard that always has the latest forecasts ready first I'm wondering if there's a better tool for those forecasts so I've searched Google for Python forecast libraries because I know that there's a lot of innovation and I find this library called profit which is open source by Facebook I read about it online and heard good things so I'll check it out of course there's a lot of time examples available online so I found one and forked into my github account here as you can see it comes with a data set and a Jupiter notebook that shows you how to create a forecast this is great usually the way I go about this is to take an example like this and try to recreate it just to make sure it's not outdated so I click clone to get the repo URL because I'll need it in a minute so here I am in my data bricks environment which we call the workspace in the past it would have been pretty difficult to get code from the git repository into data bricks however as Clemens mentioned we now have this new feature called projects which allows you to easily clone a git repository when you click create project you provide the path to a git repository so I just paste the URL that I just copied earlier into this text box when you put create we clone this repository and make it available in your user folder as you can see it indicates which branch you're on and when you click into the project you see that all of the files were clones so the first thing that I'll do is to create a branch because I don't want to mess around with the master branch because that will be used to run our production job I open the get dialog and I can just start typing a branch name into this text field now I click on create branch from master and I'm ready to go as you can tell we're trying to make the most common workflows super-easy without having to leave this environment now that I'm in my future branch I'll click on the jupiter notebook and you'll see that we open it in the data bricks notebook editor in addition to supporting standard formats this editor gives you several collaborative features that we'll highlight as part of this demo so let's get started data bricks provides you with a scalable compute back-end I can attach a notebook to an existing cluster or create a new cluster let me attach this notebook to this cluster called ml cluster that's already running now usually I would have to worry about the environment that is set up on this cluster and the libraries that are installed but with the integration of the projects feature and our runtime that's running on this cluster you will see that we automatically detect the presence of this requirements text and as soon as I run any cell in this notebook the cluster will make sure that the environment matches those requirements so what's happening now is that profit is being installed in the background you already preinstalled many popular libraries like pandas and numpy and we adjust their versions if needed so let's just run this entire notebook and see if it works as you can see this reads in the csv file from the data folder and loads it into a panda's data frame the file has two columns one for date and one for the historic values of the column that we will try to forecast in this plot you'll see that we have data up until 2016 and then we forecast for another year after that this is great because that's usually how you get started find an example and make sure that it works however this is just using toy data and we have lots more data on our actual stores so let's see if we can adjust this example to actually scale to our needs I don't actually know where our sales data lives so I leave a comment to ask my colleague to help me out here okay let's see if he's online great coincidentally it looks like he's online and ready to help you can see he opened up the notebook from the indicator up here which shows which users are present in the notebook ok looks like he responded and he created a cell so I'll assume share some code ok great he's actually using koalas koalas is an open source library developed by data bricks that provides the exact same API s but uses spark in the backend to scale computation this way I won't have to change any of the other code the data frame is still named DF and it should just work so if I run this cell you'll see that we're running a spark job in the background and hand you back a dataframe this is now different from the toy example before we have an additional column that indicates the store this data is from in this case we only have three for now San Francisco Amsterdam and New York of course I don't want to generate a forecast across all stars but I want to have a forecast for each store individually so instead of just running all of this through one big forecast I group the data frame by a store ID and apply the forecast as UDS what will happen is that we'll run a spark job group the data frame and then for each store we'll run this forecast and I can also delete all the other cells because we won't need them now the only thing missing is that I want to write the forecasts out to adults a table to actually use it in my dashboard I don't just want it on the forecast every time someone wants to look at it so we add this code snippet that takes the forecast and stores today's date and the store ID so that I can query by that later when I hit run we'll bring up some spark jobs in the background that crunched through those data and write out my predictions now this is great I just clone an example I found online data brings configured the environment for me and I could easily change it all to scale all of my data by using koalas and writing out forecasts so Delta now as mentioned earlier if for whatever reason you want to use Jupiter you can right-click on this notebook to open it with Jupiter right here in data bricks unfortunately the collaborative features you just saw are not available in Jupiter as a side note of course you could go back and forth between the two editors whenever you want okay let's go back to the data Brooks notebook editor of course I've been working off of my feature branch here so now I can go and check that code and to get I open the get dialog provide a commit summary and click commit and push we won't show this in the demo but usually you would go through the typical CI CD workflow to create a PR get it reviewed and merge it into master and here we set up our git automation to automatically check out the master branch of this repo in this production folder whenever a new PRI gets merged from here I can just open up the notebook now this is the master branch version that I want to automatically run you can click on this calendar icon which allows you to schedule this notebook as a dataflux job let's configure it to once a week and this way I'll automatically get the new forecasts written into my Delta table I'll click OK and we're done here now this is a full end-to-end lifecycle of experimenting with code in my own future branch checking in to get and pulling the master version into a production folder that is used to run the schedule job let's quickly take a look at the dashboard that I put together in this notebook you can see that I use a feature of data bricks notebooks called notebook widgets I can just create two widgets that give me the female it's available on the Delta table those widgets will update whenever I get new data and then the data that is shown is automatically filtered by my selection I can create a dashboard from this notebook that embeds the table and visualization and also integrates the widgets to control the parameters of the sequel query so when I click present you will see that the version of this dashboard that I'll share with our decision-makers here they can select which forecast they want to see for which store all updated and without having to write any code and that's it for the demo so just to summarize in this demo I showed you our support for the native Jupiter format and how the data breaks notebook editor provides collaborative features like co-presence Co editing and commenting we saw the project-based git integration and how easy it was to start by cloning a git repository and creating a new branch for development and finally we productionize the forecast by pulling the master branch into a production project scheduling a job and creating a notebook dashboard to share the latest results with our business stakeholders now in theory I could even update these forecasts every day simply by scheduling the production job to run daily that's a massive improvement from the quarterly cadence that we were used to and with this I'll hand it back to Clemens Thank You Lauren for this amazing demo I hope that everyone watching is as excited about the next generation data workspace as we are to learn more check out data picks com and with that I'm gonna bring back Ali Thank You Clemens I hope you're as excited as we are about the new and improved data bricks workspace as companies continue to mature in their machine learning practice the need for robust and reliable platforms capable of handling the entire ml lifecycle from experiment tracking to model deployment and management is becoming crucial for successful outcomes two years ago at SPARC an AI summit we unveiled ml flow an open-source machine learning platform for the whole lifecycle and today we have some very exciting updates to share with you about ml flow so please join me in welcoming ml Flo's original creator Matea haria back to the virtual stage hi I'm excited to give you an update on ml flow the open source machine learning platform so I'll start by talking today about the need for machine learning platform and what these software systems are then I'll give you an update on the ml flow open source community where we have some exciting announcements and finally I'll talk about what's coming next for ml flaw I everyone at this conference is aware that machine learning is transforming all major industries and it's involved in day-to-day and even second to second decisions in a lot of applications and business processes but at the same time despite the huge potential and value of machine learning building machine learning applications is highly complex and it's quite a bit more difficult than traditional software engineering so this is for a few reasons first of all machine learning development is a continuous iterative process the real world keeps changing and so ml applications and models have to be continuously updated over time second these applications are highly dependent on data so we need to have very solid and reliable data pipelines both during training and production and finally as a result of this many different teams and systems have to be involved in these applications including data engineering machine learning or data science and application development teams and this complexity makes it very challenging to build and then operate machine learning applications now as a result of this complexity there's actually a whole new category of solutions that's emerging in the industry called machine learning black these are software platforms to manage the machine learning application lifecycle all the way from data to experimentation and finally to production and these have already been very successful in the largest companies that use machine learning so for example Google's the FX platform Facebook FB Lerner and uber Michelangelo all internal platforms that power hundreds of applications of these companies but we found that every company using machine learning in a serious way is developing some kind of platform regardless of their scale and these platforms have some some key benefits the main one is that they standardized the ml development and management process within each company to make it much simpler but the way that companies are building them today also has some significant challenges in particular today each company is building and maintaining its own platform and that's a lot of replicated work and it's it's also quite difficult to do because machine learning technology is changing very quickly so if you're developing one of these platforms you constantly have to add support for new versions of ml libraries you know new algorithms new deployment processes and so on so I theta brakes and we started looking at this problem about two years ago of how to provide an ml platform we asked ourselves whether we can take a very different approach and unlike these existing company internal platforms you wanted to create a platform that's open source and that aboard community can collaborate on so that it's very easy for everyone to have the latest and greatest support in their machine learning platform and that's what we did in ml flow the first open source end-to-end machine learning platform that we launched actually two years ago at the same conference at the spark a I solved so ml flow can consist of four components that work together to manage the machine learning lifecycle they include ml flow tracking for experiment management ml flow projects where you produce abalones ml flow models to package models and then deploy them in a reliable way to different deployment backends and finally the model registry which is the newest component it's a centralized hub to share and view models for for an organization and these components are all work together any individual involved in machine learning can use them and they're also designed around this open API philosophy where everything is using REST API and command-line interfaces that are easy to integrate with existing tools so in particular this means you can use any programming language with ml flows through these api's and you can also use any machine learning library and the community has actually built great integrations with a lot of very popular libraries but it's also very easy to integrate ml flow with your own custom processes so when we launched ml flow we have no idea how this project will do you know would it be possible to design an open-source platform that can actually provide value to all the companies that have to build their own custom process and also would there be any kind of community forming around it and we were really blown away by how quickly the community has grown today two years later ml floor is already up to 2 million downloads per month just in Python alone that's quite a bit more than many individual machine learning libraries and it's also up to 200 contributors from a hundred different organizations you know just for comparison our team working at a database is less than 20 engineers we're also seeing very high usage just on our own cloud platform we see over a million experiment ones tracked per week with ml flow and a hundred thousand models registered that people want to share with their team and review and these numbers both usage and downloads are going out of here at a rate of four times you're over you're very fast growth rate at this stage just to give you an example of the breadth of use cases at this summit we have you know a number of talks our ml flow including the fee that I'm highlighting here so T Mobile has been using M also to maintain the add fraud detection platform where they need the monitor over 200 metrics from different data sources to make sure that the models are not drifting and are giving accurate results ExxonMobil is using ml flow to deploy and then monitor thousands of models for prediction for predictive maintenance across their business and finally virgin Hyperloop one is designing this high-speed Hyperloop transport system and they're using I will flow for experiment management to look at the results of thousands of simulations about all aspects of the system not just machine learning and they've been able to increase you know the projected efficiency of the system quite dramatically by running all these experiments so these are just some example use cases that you'll see fall talks about because the community has been going so quickly we also wanted to make sure that it can keep doing that in in the future and I'm really excited to announce at this summit their data books has donated ml flow into the Linux Foundation so that now there's a large you know nonprofit vendor-neutral Foundation that's managing the project and that'll make it very easy for a wide range of organizations to continue collaborating on ml flow and we're really excited to see how this will how this can lead to even more organizations building on and contributing to ml flow in the next few years okay so that's been a little bit of background on ml flow and an update on the community the last thing that I want to talk today is what's next at data breaks we're doubling down our investment in ml flow and quite a few other companies are contributing heavily to it as well so in particular I'm going to talk about three ongoing efforts auto-locking model governance and model deployment so let's start with auto locking in ml flow it's easy to record custom information about your training ones and one of the tools that we added to do that is these integrations called Auto logging into a lot of popular machine learning library so you can just write one line of code and on ml flow will automatically record a lot of details about the models you trained using that library and the metrics about them so all the parameters in your model the actual model file itself so you can then deploy it in different places all the training course and so on we started providing auto logging features last year and we've seen you know very faster option of them so we're actually adding quite a few exciting new features one of the ones that I want to talk about that came out and I'm also one point eight recently is Auto logging for spark greener sources which is our first foray into data versioning with ml flow so if you're using spark to create your your features or your training pipeline you can automatic an turn on spark auto-locking and automatically record exactly which data was great and in addition to that if you're using Delta lake which has support for table versioning and traveling back in time to see an old version of the data we also record exactly which version number was used so what this means is that if you're running your job with spark Auto logging you'll get this information about what was there and you can then we load the exact same table version later and this is incredibly useful for debugging your model later on if you want to see what data went into it or for creating new models to compare with it and so on it's one of the biggest pain points that we've seen users have when they manage data and and they're trying to do machine learning so this is already out we're excited to see what people do with it there's also ml flow Auto logging support in quite a few other libraries there are six libraries that are already supported in the main repository and there's also a team at Facebook that's contributing fight or trade or logging and our data breaks were working on scikit-learn and we expect to see new Idol logging integrations with a lot of other systems as well the final new feature I wanted to mention with Auto logging is that if you're a data breaks customer we've integrated Auto logging with the cluster management and environment features in database so if you are doing experiment runs on database for example in a notebook or in a job we will automatically according of the notebook that you used and an exact snapshot of it and also the Coster configuration in any libraries that you attached and then we provide this clone on button so it's very easy to create a new cluster with exactly the same configuration and libraries as your notebook and create a snapshot of the notebook look back then and on it again or modify it so this really shows the power of auto-login it's going to be super easy now to get the exact same environment and we produce an iterate on results okay so that's the first feature I wanted to talk about autoerotic the next important set of features is of course once you've done a bunch of experiments you've built machine learning models and we see one of the biggest pain points of organizations is own model governance and reviewing and deployment so we're actually adding a couple of very exciting features to the ml flow model registry to help further with this process the first feature is ml flow model schemas so we found one of the most common pain points with deploying models is that you might have you know the data that the model was trained on might have different feature names or different you know different properties then then the data that you have in production or even the data used in your previous model and so we're extending the ml flow model format to include support for schemas which store the input and output data types for your models and whenever you call one of the log model api's in ml flow it will now also record these schemas so it will record you know what fields are required as input for your model and also what it produces and then we'll use these in the rest of the ml flow tools to check for compatibility and warn you if you're deploying a model that needs to work on a different type of data from your previous one you can just compare two of the models and get a warning that these are incompatible with each other and we think this will be extremely useful you know when you plug in through the API into different deployment tools and other custom review tools that you might have so that's model schemas and the second feature I want to talk about is extending the model registry with custom tags so we found that a lot of organizations have very custom internal processes for validating models for example maybe your model has to pass Legally view for gdpr compliance or maybe it has to pass a performance test so that you can check whether it's fast enough to deploy on the edge devices so we're adding these as a mechanism to add your own custom metadata for these models and keep track of their state and this also integrates with API so you can on automatic CI CD tools that test your models add these tags and check off on them and make it very easy to check whether your model is ready for deployment and we think this will help a lot of workflows our own model management get quite a bit easier and more automated okay and the final set of features I want to talk around our model deployment ml so already has integrations with a lot of model deployment systems is very easy to push your model to them but because we're seeing so many of them contributed we actually wanted to make deployments a first class concept and give you a single API to manage all these types of deployments and so we're creating this dis deployments API for for managing and creating deployment endpoints that will give you the same commands to deploy it into these different kind of environment so you don't have to worry about the individual details this is already being used to develop two new endpoints for NSA I and Google cloud platform and we're all supporting a lot of the best integrations that we had for different deployment systems to go behind this API so this will give you a simple uniform way to manage these deployments and to push the same model to different serving platforms and for data based customers were also really excited to announce a model surfing system integrated directly into database so if you use open-source ml flow today you can already track models and use the model registry to manage the different versions of them and push them to different serving systems but if you happen to be doing the some data breaks there's also now a turnkey way to directly deploy models from the registry as soon as you promote a new model to production it will automatically be served in a rest and point and so data scientists can do this without involving anyone else at all in upgrading the service and of course you can hold back by just crying a different version or moving the version out of production so a very simple turnkey way to do this but we think it will make it even easier if you are using our hosted ml flow or service to production eyes your models okay so that's an overview of the new features it's nice to hear about them but it's even more exciting to see them in action and for this purpose I'd like to invite Sue Ann Homme a staff engineer on thermal flow team a database to give you a demo thanks for taking there's a lot going on in the world these days so I've been reading a ton of puppy news but even puppy news can be overwhelming for example as a new mom I don't know how I feel about 14 puppies so I decided to build an AI backed browser extension to tell me how to feel let's turn this on all right I think images make internet better and I think it behind 14 puppies being good news they're very cute see it's not always correct but it looks pretty good so today I want to tell you how I built this real time machine learning application in just a few minutes and how I can make it even better using ml flow experiment tracking while registry and data breaks model serving first I need a sentiment analysis algorithm that will take text and return on emoji I'm using Vader here which is a real based algorithm I've constructed the model and I want to lock some information about it to ml flow tracking for posterity I'm going to log the algorithm name as a parameter as well as number of sentiments and we'll also log the test accuracy as a metric accuracy is not bad so we'll try putting this into production by logging the model to Emma Florin and model registry by specifying this registered model name parameter will also allow the model schema or signature which is a new feature that meta mentioned earlier and this model takes in a string and up to another string which should be the emoji we've created version 1 of model text to cinema in model literacy 2 see all that we've logged here we can go to the run sidebar which lists all of the runs that were logged from this notebook clicking on one takes us to the run details page which has a link to the source notebook snapshot parameters metrics and the model that we just locked to go to the model registry we can click on this link or we can use the model sidebar and search finding we have our first one one here and I really really want to use it in my application right now so I'm going to transition its stage to production now we can go to the new serving tab and enable real time model serving behind the rest API interface so with one click we're putting up an endpoint that will call these machine learning models on demand our version 1 is pending but once it's ready it will be available at this URL that is the workspace domain the model name as well as the version number or the stage that it's in so this production URL for this model is pointing to version 1 right now let me copy this URL and while the endpoint comes up I'm going to go work on my browser extension so here's the part of my browser extension code that calls the scoring endpoint I'm going to make a simple HTTP request and I can paste in my URL here and you can see the old model I was using and when the scoring endpoint turns an emoji this onload callback function will modify the headlines with the emojis alright our endpoints ready now so let's go update the browser plugins as we change the endpoint name in the code and it still works so in just a few minutes we set up a wrested API endpoint for model scoring and built an application that changes the web content in real time not bad but I promise to do better so I've asked a friend of mine to build a better model for us and it looks like she's been very busy and added two new versions to our model and the serving system has automatically created end points for them already now I want to go and see the latest version since it's probably the most sophisticated one and it's using torch mochi a pint or two implementation of deep mochi a by Alice Thien based text to emoji translation model and she's also added contextual features for better accuracy this sounds fantastic but before I transition this model to production I need to do some due diligence so I'm going to compare the new versions to the old one and in this view we can see the information about each version that comes from both model registry and the source runs like the parameters that we had logged the Yellow Rose highlight the differences between the versions and it looks like version 3 has a different model schema than version is one and two which makes sense is my friends that she added new features to improve the accuracy so if I deployed this version now my application won't break because it's not sending in these new features but we have version 2 here that has better accuracy than version 1 using torch mochi as well and has a matching model schema - version 1 so let's put this one into production now back in the serving tab we see that the production URL points to version 2 now which is great because it means I don't have to change my application code to use a new version I am feeling a little nervous about that model schema difference I saw so I'm going to try out the endpoint using this test box version 2 looks good and version 3 errors as expected now it's time to see the new version in action all right so I'm very happy because one the model version update when seamlessly into the new model does seem better and now my puppy page is even more heartwarming so in under ten minutes we made the internet better twice and even if you are not a subscriber of puppy news you can imagine applying the same techniques to other important domains like support tickets user surveys or customer feedback to recap today I showed you easy deployment of models model versioning and collaboration and finally seamless application updates using ml flows end-to-end model management tools thank you for watching and I hope this demo brought a smile to your face back to you might a and here's a puppy whoa thanks Ron that was a great demo so the last thing I want to end with is how to get started with ml slow we made it very easy to get started with the platform you can just install it through pip and it'll just on on your laptop you don't need to set up a server or create an account anywhere to use it just to our tutorial and you have an audio if you want to try it in data breaks it's available in the free database Community Edition and we also have an in-depth tutorial today of the summit at 11 a.m. for you to try it out finally my talk was just scratching the surface of what's possible with ml flow there are over 20 other talks at the summit from a wide range of organizations that are using it and I invite you to check out these talks to see how their production izing machine learning with a valve low thanks moot a I'm excited to have Rohan Kumar join us again at spark an AI summit this year Rohan is the corporate vice president of Azure data and the engineering leader responsible for the product development of all of azure data services including of course Azure database Rohan and the Microsoft team have been our strategic partners at their breaks and together we joined to developed as a dynamic service that launched just over two years ago today thousands of customers use Azure data bricks who run millions of virtual machine hours every day Rohan is joined by Sarah Byrd who leads research and emerging Technology Strategy for a zuray eye she has a PhD in computer science from UC Berkeley which is a poplar university among the founders of theta bricks please welcome Rohan and Sarah to talk about the latest research and development in responsible machine learning [Music] thanks everyone for joining in and thank you for having me I look forward to the summit every year and this year is no exception I'm very excited to talk to you about analytics in AI and how you're reimagining the boundaries of what can achieve so many new AI capabilities have been talked about at the summit already and today I wanted to draw your attention to something that should be at the crux of all this innovation responsible AI practices looking at how the world has changed around us over the last few months and more so over the last few weeks has greatly amplified the need for us to ensure fair practices and ethical behavior across all sections of society to get a sense of the intensity of this topic Capgemini published late last year that nearly nine out of ten organizations have encountered ethical issues resulting from the use of AI these executives cited reasons including a lack of resources dedicated to ethical AI systems the failure to consider ethics when constructing AI systems and the pace at which AI is touching every part of our lives the statistic is alarming v as a community have a duty to address these issues head-on and empower organizations to use AI with a sense of responsibility at Microsoft we've also heard these concerns firsthand from our customers as a result we've invested in the advancement of AI driven by Responsible principles that put people first our approach to Responsible AI has anchored around the following focus areas keeping ethical principles at the core of everything we build enabling governance and operational transparency to drive write practices leading efforts for public policy at the highest levels and finally empowering teams with cutting-edge tools to encourage responsible AI practices now since the spark and AI community comprises majorly of developers and data scientists will focus on the tools in collaboration with Microsoft Research we are bringing the latest of research in responsible AI to Azure over the next few minutes I will talk about how the new responsible machine learning capabilities in Azure and our open source tool kits empower data scientists and developers to understand machine learning models protect people and their data and control the end to end machine learning process let's look at each of these three pillars in detail first let's look at the understand pillar it comprises driving an understanding of the models fairness index and its interpretability but what does that mean one challenge with building AI systems today is the inability to assess and mitigate unfairness in the models to address this challenge we recently open sourced a toolkit called failure there are two components to failure first focused on fairness assessment is a dashboard with both high-level and detailed views for assessing which groups are negatively impacted second focused on mitigation is a set of algorithms for mitigating the observed fairness issues these strategies are based on a variety of supported fairness definitions such as demographic parity or equalized odds for classification tasks the supported unfairness mitigation techniques can easily be incorporated into existing machine learning pipelines and assessed model fairness during both model training and deployment together these two components enable data scientists and business leaders to navigate any trade-offs between fairness and performance and to select the mitigation strategy that best fits their needs using fair learn developers and data scientists can leverage specialized algorithms to ensure fair outcomes for everyone another important aspect of understanding model is the ability to interpret or explain its results this is where interpret ml comes in as the name suggests it helps interpret models and their results interpret ml is another toolkit we've open sourced and can be used for both glass box and black box models glass box models are designed to be interpretable they provide lossless explained ability as they use interpretable algorithms like decision trees or explainable boosted machines interpret ml can help visualize the various features affecting the outcome of a model in an interactive dashboard and even allow for what-if analysis on the other hand black box models are based on more complex techniques like neural networks and can be a little tricky to interpret you can still achieve approximate explain ability with black box models using explainers like lime or sharp imprint ml uses these explainers to surface a dashboard with feature importance interpretability is needed to ensure that there is optimal transparency within models to assess and reason through the predictions it generates or the recommendations it creates with falen and interpret ml you can a very good understanding of your models let's now look at the protect pillar protect reference to protecting data against any potential misuse used to train the models to understand this better let's envision a scenario where a Dean a scientist wants to use some sensitive data to create a model in a non protected scenario this would result in submitting a query to the private sensitive data set and provided the right credentials that receive sensitive data as a response to their query well this is problematic now the data scientist can look at the sensitive data and potentially reduce PII information about an individual this is clearly not at all compliant or ethical now let's introduce a differential privacy toolkit within the open DP initiator that has been developed by Microsoft in collaboration with the researchers at Howard's Institute for quantitative social science this toolkit ensures differential privacy by first managing exposure risk by tracking the information budget used by individual queries and limiting further queries as appropriate in our example the queries first validated against the information privacy budget and processes only if it isn't over second it injects statistical noise in beta without significant accuracy loss to help prevent disclosure of private information again in our example because the query response may be sensitive before it reaches the user it is injected with noise to make it differentially private in addition to differential privacy you can also protect your data on either using Hardware back confidential machine learning capabilities and homomorphic encryption and ensure your data isn't compromised at any stage of the machine learning lifecycle the third pillar of a responsible ml is control it refers to tracing the lineage of models and supporting a standardized format of documenting it for cataloging and reusability needs on the control side there are some features and capabilities that allow you to manage your audit trail and data sheets for the models within Asia all the actions undertaken through our the lifecycle of machine learning model can be tracked for auditing purposes this audit trail enables us to trace the lineage of the training data the operations performed on it its drift and most importantly the model versions and the various deployment endpoints now this is important for compliance reasons organizations can leverage this audit trail to trace how and why a models predictions showed a certain behavior additionally once you have a model it is important to capture its properties and purpose alongside any other metadata that could be helpful from a reusability standpoint in Asia we enable this with custom tags you can define for the models and capture all its information so that it travels with the model all the trails and data sheets enable accountability for models and are an integral part an integral component of responsible amount I've been talking about quite a few capabilities so let's bring them to life with a demo let me invite Sarah bird to demo these tool kits and show how you can use them Sarah thanks Rohan hello everyone today I'm gonna be walking through a demo of some of our responsible AI tools on Azure as we progress through the ML lifecycle I'm gonna be demonstrating three key capabilities protecting sensitive information with differential privacy assessing and improving model fairness with fair learn and analyzing an explaining model behavior with interpret ml in this demo I'm gonna be building a model for loan applications to decide which to accept in which to reject let's dive in here I'm using Azure data bricks but you could also use a Jupiter notebook or your favorite Python environment like any good model building scenario the first thing I want to do is look at my data here I'm looking at aggregate information as well as individual features to help give me a feel for the data set and uncover any potential issues up front it looks like this data set contains a lot of great data to help me build my model it also contains highly sensitive information that must be kept private obviously I can limit direct access to the data with the data store however what many people don't realize is that aggregate values can still be used to reveal private information about individuals let's go to an example to see what I mean so here I'm going to demonstrate how we can use aggregate information to reconstruct the data set so in this case I'm going to assume that I know aggregates about individuals income as well as a little bit of individual information about two individuals in the data set and with that I can take it off the shelf Sat solver and use that to reconstruct a data set that's consistent with the information that we know and so if we run our sets over here it's working to reconstruct that data set and we see that it can actually find a result and now we're gonna compare that to the the real data set and see how well our attacker did so in this case it looks like the attacker did pretty well we're able to correctly reconstruct the incomes of individuals within five thousand dollars for more than 20 percent of the data set which could be a significant privacy violation so what do we do about attacks like these how do we use data but still protect privacy and this is where differential privacy can help so differential privacy hides the contribution of individuals in the output computation by adding a small amount of statistical noise to the query the noise is significant enough to protect the privacy of individuals but often small enough to still allow us to use the data differential privacy also calculates the amount of information that is revealed in the query and subtracts that from an overall privacy loss budget let's look at how this works so here I'm going to use our open source differential privacy platform and now I'm going to add that statistical noise to the the income information and then I can retry my attack with my Sat solver and what you'll see here is that we get a very different result we aren't able to reconstruct the data set that's consistent with the differential private incomes and so we don't have any guesses at any private information however it's not enough to just protect privacy we still want to be able to use the data so let's look at how we're doing here in this case you can see that we're we're looking pretty good we're you know pretty close to the original information you can definitely see some noise added particularly good so this is a small data set however now I can take this and go and use it in my problem knowing that I don't have to be concerned about privacy attacks so one of the ways that we can use differential privacy in machine learning is to generate synthetic data sets so here I'm gonna use my open source toolkit and I'm gonna create a dataset that matches the patterns and trends of my original data set but it doesn't reveal any private information and that way I can take that and give it to my data scientist without having to worry about privacy so I'm gonna generate my dataset here and now that I have my private data set I can go and build my model so here I'm going to build a scikit-learn model for my problem and now that I've built a model I want to actually analyze the performance and see is it good enough for my problem so of course I can look at traditional metrics like accuracy however since this is a problem where the wrong decision could have a significant impact on an individual's life it's important that I go a step farther and I also consider the fairness of my model so in this case I'm going to use fair learn so that I can understand how well my model works for different groups of people and make sure that it's not using sensitive attributes to make decisions so in this case I'm going to use fair learn and fair learn has an interactive dashboard - how we better understand the fairness assessment so I'm gonna click get started here and my dataset has two values sex and race so let's click into sex and understand how well my mom is doing but I also need to provide a performance metric which metrics important for my problems so in this case its accuracy so I'm going to click through and the first thing I have is a graph here that helps me understand the difference in model performance across these two groups so I can see that my mom is more accurate for women than men and it's about 85% accurate overall however I'm really interested in the outcome of this model what's the difference in who I'm offering loans to so in this case I can see that I'm offering loans to 19% of people overall however I have a significant difference in the number of loans I'm offering to women in men I'm offering loans to about 7% of women that apply and about 25% of men that apply now I don't necessarily know if this is a problem it could be that these two groups have very different distributions however I do know that it's a sign that I want to dive in and investigate further so for that I can use interpret ml and it's black box explanations so I'm going to explain generate explanations for my model here and then I'm gonna use its interactive dashboard to explore those and understand how it's making its decisions so in this case I'm going to set up a two different cohorts so let's set up one for women so we can understand what's going on there so I'm going to add a filter for sex and women are zero in this data set so we can add this filter and I'm gonna save then I'm gonna have a second cohort here for men and I'm going to select this and in this case I'm saying it for men and women since I want to look at this fairness problem but you can use lots of different cohorts like train and test to compare as well so in this case what we can see is the model performance just like we saw in a fair loan but in a lot more detail however what I want to do here is a jump over to the explanations to understand the future importance and how the models making decisions and what I can see right away is that the model is using sex directly as a feature to make predictions for women but not necessarily for men and if I click on this I can actually dive in and see that for for women for every single woman in the data set the value of sex being zero is directly contributing to the prediction that they are rejected which is pretty concerning and so what I want to do at this point is move on and explore mitigation techniques so for that I can go back to fair learn and use some of the built in mitigation algorithms that allow me to reduce disparity either during training or after so in this case I'm going to use the grid search approach where I'm going to retrain many different models by rereading the data to try to reduce the disparity so I've run my model training here and now I can go back to the fair learned dashboard to look at the best models so in this case now I have a range of models that are along the the curve of difference in predictions and accuracy so here's my original model up here and now I'm actually going to click through and look at this model which is a little less accurate about 82% accurate but if I jump down to the disparity in predictions I can see that I have a much different outcome now I'm offering loans to 15% of men and 14% of women that apply so this might be a much more promising model for my problem and so what I'm going to do now is actually save all of my work so that I can share it with collaborators or show it to other stakeholders as we make a decision about whether or not we want to move forward with this model so I'm gonna register all of my models here I can upload my dashboards and my explanations everything to Azure machine learning then if I move over to my Azure machine learning studio here then I can click on a particular run and so here's my different runs and I can click into one and now I can actually have my dashboard stored directly with my model as well as the fairness and so now I can show this other stakeholders are compared to previous models as I decide whether or not they want to take this into production back to you Rohan thanks Sarah it was so nice to see all of these machine learning tools come to life with the loan scenario just to add more color this demo is very similar to what eva is doing with these tool kits the using feather and extensively to mitigate unfairness in models they've been able to reduce their disparity by more than 90 percent which is amazing and many more customers are using these tool kits to build responsible machine learning practices I want to take this opportunity to invite the community to get involved contribute to the open source projects powering Responsible machine learning capabilities create scalable impact with the objective of having ethically I practices and solutions all these tools are hosted on github and already have hundreds of downloads within the last few weeks if you want to try them in action please use the agile links to try them with an agile machine learning or agitator breaks with that I want to thank wait did I forget something well I must let you in on a little secret this one about my presentation environment I'm sure most of you think I'm in front of a green screen here and there are some graphics people doing their magic in the background we thought of experimenting with AI a little bit and putting it into the use just for the summit first let me show you where I am this is not a formal studio it is a normal living space when I just came in to record the stock now let me show you how it was converted to a presentation stage for me we use connect and the azured Connect def care to bring AI and AR together to simulate this stage here's an architecture of the solution that produced the video for the stock amazing isn't it we use normal camera images and depth-sensing connect camera images to replace my actual background with a stage and design of our choice I thought about a beach in Hawaii but opted for a bit more presentation friendly option this is just an example of how together we can stretch the boundaries of AI the solution has been published in a github repo linked here for you to use for your next meeting or video call again thank you for the opportunity to share I hope you have a great summer please stay safe [Music] thanks Rohan isara next up we're going to hear from a company that is leading the way in the financial services industry they are doing this by embracing open cloud-based platforms like Azure data breaks in ml flow that enable the democratization of data and machine learning across the business I'm pleased to introduce anorak Segal managing director of data analytics AI and digital innovation at Credit Suisse hello everyone happy to be here at the SPARC in AI summit my name is Anurag Siegel and I don't data analytics and machine learning in AI at credit service but in the global markets division I'm here to talk about how data and digital are transforming all aspects of how we do business as a bank we've gone from a traditional approach a human centric customer approach to a digitally connected client journey where digital solutions augment human expertise there's an exponential demand for insights and data-driven answers from our clients our business models have gone from a predominantly bespoke and expert judgment driven approach to digital platforms that allow us to drive into new markets or new routes to customers the value proposition for our sales trading and research teams continues to evolve as well our our trading teams are looking for real-time intelligence on how to price how to manage inventory how to drive rfqs our sales teams are looking for insights and in what clients are interested in what products they're interested in what stocks they're interested in and our research teams are looking to differentiate our content and our advice to clients with dated data and distinct and differentiated analytics data analytics is at the core of everything we're doing we've gone from leveraging traditional financial market data price data to leveraging alternative forms nascent forms of data in driving unique insights intelligence and recommendations for clients data and AI machine learning are a core driver of how we evaluate where we go next in order to achieve this transformation our goal is to foster the generation of ideas new ideas in enhancing and growing our business with data analytics allowing for experimentation rapid prototyping and having a VC mindset in leveraging and driving mature commercially viable ideas into products and business ventures in being able to do that we're setting up ideation events forums where ideas come to life be it shark tanks hackathons or Design Thinking workshops in in partnering with digital champions and digital advocates across all our businesses in rethinking reimagining our business and business process and and how we deliver most value to our clients we're looking at how we go from ideas to pilots prototypes to Minimum Viable products and having a clear digital product strategy and how we go to market this wouldn't be possible if we weren't prioritizing our idea funnel with a very clear you know mindset of looking at ideas that are most viable and most critical for us to take our business forward none of this would be possible without having a core foundational platform that allows us to drive rapid prototyping rapid ideation experimentation and take the ideas that are most commercially viable to that scale out more so what does a core foundational platform looks like well our goal is to look at large amounts of both internal and external data in ingesting large amounts of data onto our platform in looking at not only structured data but unstructured data and looking at all kinds of real-time streaming data and so the forms and types of data that were ingesting into a platform continuously evolve we are we've set up a metadata driven data injection framework that allows us to go from define metadata to ingesting data to enriching those metadata sets to being able to look at quality monitoring of our of our data and so data quality plays a key role in in in in the consumption of this data in consumers gaining confidence on the platform and on the data that they're using we're using anomaly detection to be able to look at data quality in very different ways analytics is a core part of the foundation machine learning in AI is a core part of how we achieve success with growth and and in the use of machine learning our goal is to democratize the use of machine learning in AI so the way we're achieving that is through machine learning sandboxes in sandbox environments that allow our data scientists to be able to develop deploy and test models rapidly in being able to look at the right trading signals and factors that that can be derived from the unlined raw and enriched data in recommendation engines and NLP engines that allow us to very quickly consume unstructured forms of data and create insight from it obviously part of this ecosystem includes the ability to you know have that lifecycle management in in models and in algorithms that we develop on the platform the insights from these algorithms and and the data insights that we're driving is delivered to our clients to all our internal personas through various different channels one of the key channels is is through the visualization and and reporting and business intelligence layer but we deliver insight to our end users where they need it at the moment and point the need attack so for our clients is to applying portals through ABI is for our sales and trading teams it is through our sales trading desk tops at the point where they're making decisions on rfqs or on Io is on any axis an API is play a key role in in the sort of flexibility that the platform provides us as we embarked on the platform we needed to set some core design principles some core architecture principles in what would drive the success for us in the short mid mid and long term being on the public cloud was a fart most important to us having platform-as-a-service capabilities that provide you know multi cloud enablement that provide for cloud agnostic capabilities was critical to us the ability to scale up and scale down is something that we did not have on-premise and and the ability to automatically provision infrastructure on the cloud was it was a key feature we needed next-gen technologies for machine learning AI and a consistent user experience for our data scientists our data engineer as a business analyst was a key part of the principles we set ourselves up with data breaks and ml flow our key tenants off of how we achieved these these capabilities on the cloud data security being you know being comfortable with putting confidential data on the cloud was a journey for us providing for efficiency in how we deliver and what we deliver you know with CI CD with continues you know continuous improvement with integration with sprint based deliveries with agile capabilities was was a big part of our core principles as I mentioned machine learning and data mining was a key focus for us in in this journey and so the types of things were doing around machine learning in AI include you know within national language processing include tonality and sentiment analysis topic detection event detection and in text summarization graph analytics and pattern mining is extremely important to us from you know from evaluating strongly connected entities to looking at sequence of events to look at a community of community detection predictive analytics was was important in areas around anomaly detection of market data to recommendation engines for sales to pricing and fundamental analysis for trading teams and our con teams to achieve this we very early set out to you know create a sandbox environment for data scientist to be able to very quickly you know enable a sandbox with with all kinds of open source libraries open so machine learning libraries you know that that allowed them to develop and deploy models fairly quickly so we created an NLP image and a quant predictive image that allowed for us to fairly quickly take all kinds of new machine learning libraries and put them into the hands of our data scientists on this cloud environment obviously with you know these images and these these these sandbox environments are our key first off for data scientists but you know the development environment for models to deployment you know and lifecycle management of models was ask critical feature in this in this process and so you know for a data scientist to develop models their their ID environment was was the data breaks notebook for our data scientists to deploy and test these models you know we allow for compute capabilities both on scale out and scale up modes so scale out using spark and scale up using you know dynamic GPU cluster management to manage the model lifecycle you know we use ml flow and we'll talk a little bit more about model lifecycle management our approach to model lifecycle management but also to be able to visualize the results and the outcomes from these models our data scientists use all kinds of capabilities you know from data breaks to tableau to you know to other features and other visualization libraries so as I mentioned model machine learning and AI model lifecycle management is critical to us and you know our approach to machine learning model lifecycle management is very similar to all other models we gone for you know in in risk or in in our trading training systems and so model registry and versioning of all our models but you know essentially includes aspects of all aspects of data all aspects of model code logging and versioning of parameters and hyper parameters worsening of taxonomy used in a model corpuses used in a model of validation and monitoring metrics used by model all of that is is control is registered as version control through this life cycle has three case studies and examples of how the platform is starting to create value for our business we're seeing recommendation engines deliver personalized recommendations to our sales teams around products our clients are interested in based on a variety of signals and and datasets we're using alternative data insights in unique ways to measure company performance to look at sector performance to look at macroeconomic views as an example to look at economic recovery from from covert 19 look at trends in in the ESG space and enhance our products with differentiated alternative data insights we're starting to look at digital platforms and the use of digital platforms that is allowing us to expand product coverage and distribution capabilities to offer unique new products and services and reduce operational friction and create efficiency in our business process so as we look to the future our goal is to continue to enable both offense and defense value cases in how we empower our business with state-of-the-art digital capabilities and data-driven intelligence to both enhance our core products but also offer new products and look at how we can drive continue to drive efficiency and growth you know leveraging leveraging data and digital platform based solutions you know as we look to embark on network effects and and you know gain the value of platform we want these network effects to continue to play out we want to leverage digital platforms to create new markets and new routes to customers and deploy more and more product our digital platforms and lastly we want to continue to turbocharge data analytics embedding machine learning and AI in all aspects of how we do business in creating value for our clients as well as for our internal sales training and research teams I'd like to thank you all for joining me at the spark in AI summit today I hope you found my presentation useful and informative thank you thanks on a raagh and many thanks again to all of our speakers this more some quick reminders for you all first please do attend the breakouts they're awesome also take some time to visit the dev hub and the Expo in your dashboard also this is a great opportunity to network connect with your peers engage on social media take a selfie and share it use hash tag data teams and hash tag spark AI summit and finally we have a chance together to raise $200,000 for CPE and n-double-a-cp in support of racial and social justice just click donate now in the dashboard and donate any amount thanks for attending spark an AI summit [Music] [Music] [Applause] [Music] [Music] [Music] [Applause] [Music] [Music] [Music] [Music] [Music] [Applause] [Music] [Music] you [Music] 