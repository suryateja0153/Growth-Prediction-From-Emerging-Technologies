 (peculiar music) - [Narrator] Innovation hinges on our ability to see things differently. Breaking boundaries and looking between the lines in an afford to solve some of the world's toughest challenges. (playful music) - AI is a key component in many of our modern applications in business and society. It's essential that these AI systems are developed responsibly. At Microsoft, we're releasing new tools and technologies to empower developers and data scientists to play their part in responsible AI development. We have new libraries that enable you to gain a deeper understanding of your models through techniques like interpretability and fairness. And we have new technologies that enable you to protect privacy and preserve confidentiality. And we're adding features to our platforms that enable you to have complete control over your AI development life cycle and the resulting AI systems. AI is powered by data but data isn't just data. Data represents people, and it's essential that we think about their privacy when we're developing our AI systems. Differential privacy is a technology that hides the contribution of the individual in the output computation. Let's go to a demo to see why this is important. Here for my demo I'm gonna use the Poon's reports published by the US Census as an example. So in this case I'm generating a synthetic report of the same style. And I'm gonna use it for an income analysis to understand the distribution of incomes for people with different education levels. So in this case, I have a histogram here where I can see the income distribution for individuals with high school degree, which is great and I can use this in my analysis. However, what many people don't know is this type of information, particularly when combined with other public information, could be used to reconstruct private data. So in this case, I'm gonna take it off-the-shelf SAT solver. Combine with the a little bit of information I know about two people in the data set. And I'm gonna use it to try to recreate the underline data set. And so as I run my solver here, it is able to find a underline data set that lines up with the reports that we're published. And if we take those results and compare them to the real data set since we know that we can see that in this case it was able to correctly predict the income of a 117 individual out of 500 within $5,000, which could be a significant privacy violation. Ooh, scary stuff, but let's look at how differential privacy can help. Differential privacy works through two mechanisms. First, we're going to add noise to the results of the query to mask the contribution of any individual row in the data set. Second, we're going to calculate the amount of information revealed in the query and subtract that from our privacy budget. You can learn more about how this works in the Companion app. Subtracting budget and calculating noise requires sophisticated differential privacy algorithms. But we wanted non-experts to be able to use this technology. And so we've partnered with differential privacy experts from Harvard's Institute of Quantitative Social Science and School of Engineering to release an open source platform called WhiteNoise. WhiteNoise can be put on top of many different data stores to enable you to add differential privacy to your machine learning and analytics. Let's go back to our demo to see how this works. So now I'm going to use WhiteNoise to generate the same report, but with the results being differentially private. So now let's try to do the attack on that report. This time we get a very different result. It is not able to find a underlying data set that satisfies the resulting report, which means that it doesn't have any guesses at people's private information. However, if I look I can still go and do my analysis, so here's my same histogram where I can compare the distribution of individuals, but now looking at the private data and the non-private data. And as we can see, overall distribution looks similar, however we have some inaccuracies in the data due to the privacy results. Now, I'm using a very small data set, so the results are going to look better with a larger data set and they're going to be more accurate, which will enable me to do my analysis well still protecting people's private information. Differential privacy is an emerging technology. And it won't work for every application. However, we're hoping that by developing in the open and maturing the technology with the community, that in the future, all data applications will be able to leverage a privacy preserving technology such as differential privacy. We hope that you'll check out WhiteNoise and use it as part of your responsible AI development tool kit. (peculiar music) - [Narrator] Innovation hinges on our ability to see things differently. Breaking boundaries and looking between the lines in an afford to solve some of the world's toughest challenges. 