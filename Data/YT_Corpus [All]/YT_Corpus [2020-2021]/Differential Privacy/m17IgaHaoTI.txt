 my name is Chris I'm leading the flow federated in Seattle and I'm here to tell you about threaded learning and the platform we've built to support it and there are two parts of this talk first I'll talk about filtering how it works and why and then I'll switch to talk about the platform right let's do it this is a machine learning story so it begins with data of course and today the most exciting date out there is all born decentralized on billions of personal devices like cell phones right so how can we create intelligence and better products from data that's decentralized traditionally what we do is there's a server in the cloud that is hosting the machine learning model in terms of law and clients all talk to it to make predictions on their behalf and as they do the client data accumulates of the server right next to the model so model and data it's all in one place very easy we can use traditional techniques that we all know and what's also great about this scenario is that the same model is exposed to data from all clients and so you know for millions of clients and so it's very efficient all right if it's so good why I change that right but actually in some applications it's not so great and first it doesn't work offline there's high latency so you know applications that need fast turnaround may not work all these natural communications consuming button life and bandwidth and and some data is too sensitive so collecting is not an optional or too large some sense of data could be large all right so okay what can we do maybe we go to the complete other extreme so ditch the server and the clouds now each client is its own client bubble right it has its own terms of a runtime its own model and it's training its granting of its data to train and doesn't communicate with anything so now of course nothing leads the device none of the concerns from the preceding slide apply but you have other problems a single client just doesn't have enough data very often that doesn't have enough data to create a good model on its own so okay so this this doesn't always work what if he bring the server back but the clients are actually on the receiving data from the server could that work alright so if you have some proxy date on the server that's similar to their own device data you could use it you could retrain the model of the seven then deployed the clients and then let it potentially evolve further okay so that could work except very often there's no good proxy data or not enough of it too for the kinds of on device data you're interested in a second problem is that this this here the intelligence we're creating is kind of frozen in time right a sense that you know as I mentioned clients won't be able to do a whole lot on their own and why does it matter and here's one concrete example the from actually production application um because it's a smart Keyboard that's trying to learn to autocomplete if you train a model no7 deploy it now some of the millions of people start using a new word what happens you think hey it's a strong signal means of people but if you're not one of those minions your phone has no clue right and you take a lot of punching into that phone to make it notice that something new has happened right so so yeah this is this is not what you want um we really need the clients to somehow contribute back towards the common good so they can all benefit but that the traveling is one way to do that here we start with initial model provided by the server this one is not pre-trained we don't assume we have proxy data it doesn't matter it can be just zeros sent to the client the client now trains it locally nodes on its own data and this is more than just one step of gradient descent but it's also not training to convergence typically you just make a few passes of it over the data on the client and then produce a locally trained model and send it to the server and now all the clients are training independently but they all use the same initial model to start with and servers job is to orchestrate this process to make it happen and produce the same feed the same initial model to other clients so now one of the ones of circlex that they're locally trained models from clients it aggregates them into a so called federated model and typically what what we do is simply average the model parameters across all clients so it's several just as the numbers and that's it okay so this federated model it has been influenced by data from all clients right because it's been influenced by all the client models and those in turn have been influenced by client data so so we do get those benefits of scale in this scenario that's so that's great but there's one concern what happened to privacy so let's look at this closely first client data never left the device right only the models trained on this data were shared now the service annexed the server does not retain store any of the client models it simply adds them up and then throw them away the deletes them right so so they are ephemeral now here you they ask you how they know that this is what the server is doing maybe server secretly somehow logging something on side right so there are cryptographic protocols that we can use to ensure that that's you know that's ology so with those protocols the server will only see the final result of aggregation it will not have access to any of the entire client contributions and we use those in practice so hopefully to put your mind at rest so okay so the sort of only ever sees the final aggregate you can still wonder how do we know that that doesn't concern it I think sensitive so this is where you would use a differential privacy in a nutshell this each client clips its update and a little bit of noise so once the final aggregate images of the server there's enough noise to sort of mask out any of the individual contributions but but there's still enough signal to make progress so not not only add too much into the detail but this is also a technique we use in production differential privacy is an established and commonly used way to provide okay um if you have any more concerns I'll be happy to discuss them offline okay so how this evolved in practice first it's not enough to just do it once right so once you produce it for that model you'll feed it back on the server as an inshore model for the next round then execute many thousands of rounds potentially that's how long it takes to converge and so in this scenario both clients and several several clients are doing all the learning that's where all the machine learning is sits and server is just orchestrating the you know aggregating and also providing continuity to this process as we move from one around to another server is what care is the state between rounds and to do it into this little bit more in a practical applications clients are not all available to synthesis at the same time you mentioned you remember those concerns I mentioned about you know consuming battle lines and life in bandwidth we want to give users a good experience so we want it to be non-disruptive so will only perform training when the device is connected into the power source on a Wi-Fi network and idle so that you know the user is not negatively affected and so that means out of the billions of glass out there only a small fraction are available at any given time for training and this was illustrated on this diagram here from actual fracture insist M Ricky's gonna see when you wanna track the number of rounds per hour the server completes across time can see kind of maxes out you know at night when everyone's asleep and their phone is connected and and you know dips at lunch when everybody's punching onto the phone while eating so yeah so the clients keep coming and going and so that means that as we move across rounds from one room to another the sort of participating clients will change you know for various reasons including that that you know some of them lose connectivity so clients can enjoy a drop out at any time so that's in an actual production system when it's deployed there's always a client selection phase where the exact numbers in a set of participants is chosen and and there are many factors that go into it including you know concerns about bias but you know the Folies talk about all that that's important to remember is that the sort of clients in each draft is different okay so in a nutshell the characteristics or production scenarios in plants there are many of them millions billions they don't talk to each other this is these are cell phones so no peer to peer connectivity everything is in communication is the bottleneck in those halls in the in the whole system clients want to be anonymous and for the for the most part they're interchangeable in the sense that you know in the grand scheme of things whether a particular device contributed their data or not it doesn't affect the result in any way any clients are available and they can drop on it in time therefore we have to actively consider them as stateless even if they have some memory there's no guarantee when there'll be back so we treat them as stateless low compute nodes and finally the distribution of data on clients is is very non-uniform because you know people differ ok um so is it only for mobile devices no no you could use fitted learning for things like you know a group of hospitals wanting to learn something together or a group of financial institutions so the job approach is the same of course the details will differ a little bit in in this case plants are very reliable potentially very capable but there are fewer of them so for example some of the cryptographic protocols that we're using they work better when there are more clients and so here you may have to work harder or you some more specialized variants all right so how does it work in practice we've deployed it at Google in several applications including and the smart Keyboard that I mentioned and so it runs in truck from the University Isis and many compare and performance of a an autocomplete model that tries that learns on federated data it's clearly better higher accuracy more user clicks than the former the model trained on the server and this is illustrated on these diagrams here and you see on the right side the federal model stabilizes at a better performance and really the reason for that is that the on device data is you know a good data higher quality data than the proxy data on the server also I mentioned before that non federated models were limited in you know they would not wouldn't necessary be able to adapt to changes in the environment and pick up changes over time so so here we demonstrate the federated model can actually learn new words that were not initially in the vocabulary and notice that the people are using them include them it's it's worth pointing out this was definitely one example of application and definitely want to use differential privacy to make sure that the only thing you're learning is common things and nothing sensitive gets through ok so it worked at Google of course what you really want to know is it will work for your application so some Drive guidelines here you know mostly common-sense stuff like if their own device data is a high quality or if it's sensitive or large you know good reason to you started learning of course you also need the labels right for for training and so we can't pay someone to go and label the data right because it's on device we can you know access it so in some cases labels are just part of the data like in the smart Keyboard you know they're all the characters you're trying to predict people eventually type those characters and so that's where the labels are in some cases you will have to work harder to you know wire up additional signal interrupt your application to to have those labels okay you know but other than that it's a no fertility is an area of active research and many variants many extensions exist in you know lots of publication hundreds publications several workshops just this year one of them organized at Google you see a little picture of you know in this workshop yeah so you know it's not guaranteed that any particular count solution will immediately work for you you have to just try things out and see what works and and what you know what we have to all collectively do to to to advance this this area this promising field is to explore together and so that's why we've built steady-flow federated and so let's let's get get to it all right so toughen up for further edits what is it development environments that is designed specifically for that are learning although it's also you know applicable to marginal kinds of computations that I'll get to in a minute it provides a new programming language that's it's not in your face its embedded in Python so you can have done don't notice it but there's actually kind of a programming like this that combines tensorflow and distributed communication in that language you have implemented a number of a number of further algorithms and so we provide everything you need for simulations so the runtimes data certain everything is there it's for past part of tensorflow and it's on github so everything is open-source and modifiable okay whom is it further to main audiences wise the researchers here's what we want to enables is for people to very quickly get started and so we provide this pseudocode like high-level language with suitable high-level abstractions and so so that it's very easy for you to express your your ideas in a way that's super compact and you can see you know what you're doing also a number of things you can copy/paste and fork in and modify so that includes the visiting are good implementations but also we will it's still kind of emerging but we'll have full end-to-end examples of research reproduced with scripts you can run and you know modify and do whatever in data set and and also and then the simulation infrastructure is designed to be modular so that whatever kind of resources you you might have but it's you know a cluster in a basement or something else you can configure things in such a way that works on your hardware the second equally important audience is practitioners so we want to be able to take all the latest research and immediately use it in production assuming this all implemented in tff hopefully that will happen and so we made a number of decisions to support that one is the the language that I keep mentioning and the abstractions are designed in such a way that you know we're thinking of production deployment from day one even though production deployment options were not something we provided on day one but they've been on our mind from day one also we designed the system in such a way that whatever code you write your writing in tff to you know to run in a simulation you can take the same code without any changes you can move into production I'll get to do it later and also the system is compatible so that you know you can pick the things you want and compose them together and make it work and modify whatever you want using the pseudo code like language because the code is in a form that you should be able to actually read it and understand and and perhaps most importantly we're actually eating our own dog food and using it at mobile so so we are investing our resources to make sure the project evolves in such a way that in a way that's relevant for production deployment right so I keep measuring a new language why why do you need a new language for the further that learning um the reason for that is that further that programs are distributed right so they include clients Cervera and everything in between and so communication is essentially an essential part of the program it's not just some systems concerned that's you know second thought and so it is kind of expected that just just as intensive law you know you are expected to be engineering your model architectures and tinkering with models and you know add new operators in their same feathers are layering except now your you know your data from that diagram kind of spans the entire network right and so obviously communication is also something that you should be able to engineer and play with and we want to give you programming language abstractions that make it super easy to do that and things like point-to-point messaging or you know taking and restoring checkpoints we've tried to use dolls that's what our initial implementations of related learning were like we it was unreadable it was very very difficult to work with so we've designed a new system based on higher level abstractions just for the basis and hopefully you see how how this is done in tff and that that you like it why stress portability between research in production you know when we think about it if you you know your ideal is I yield I started learning environments you can't look at the data a lot of things that we take for granted become more interesting like you know you can just look at the data so you so you may not be easy to see you know but where the outliers are or debug problems with your predictions or trying out various models there is the ways to do some of those things but you know they're not not obvious and so a product for example you may want to just go ahead and deploy your model into a live system to run on real devices maybe in dry mode so nothing gets affected but it kind of runs there you can see how well it's doing and in iterate in this manner so so the kind of traditional boundary between you know production versus research all this gets a bit more fuzzy you know sometimes may have to experiment in production and so because of that and the general desire to transfer new research into production ASAP its it's essential in our mind to provide this kind of portability so you write one one version of code and it works whether at the research or simulation or production it's the same code and number of decisions in tff reflect that like the fact that everything is kind of language agnostic and platform agnostic in everything is expressed declaratively so that you can compile it into different kind of moments okay so what do you start the basis of building problems in tff is a fairly computation this is a generalization of theater learning so you know we have clients that have sensitive data you know very many of them they do all the trading server orchestrates these computations and provides continuity over time the clients want to be anonymous so whatever operations we don't have to be an aggregate that's in essence you know what what defines a federated computation how do we create those so let's go through the various abstractions that we have in tff one by one values this is a set of clients let's say each of them has a temperature sensor that you know produces some reading let's say floating point number we're going to refer to the collective of all those numbers as a single federated value so if either the value can be a multi set of those you know individual country contributions from clients right this varied value described also further the types in this case is going to float flaunted five further the floatin clients the curly braces indicate that it's a multi set in general type consists of the type of the individual constituents and and and what we call a placement which is a session the entity of the the group of system participants we have a little placement algebra in tff I won't get into it but you know for starters you you'd only use clients and servers now suppose you have a server and there's a number of the server would say it's also some flaws we can also call it the federated body in this case it's not a multi set because there's just one one sample of it so it's a float on the server now let's get to operators suppose there is a distribution protocol that is picking up numbers from the clients and depositing let's say the average or something like that on the server right so unlike in a property in a program Allah nature like Python here in the FF you can think of it as a function in this case the inputs to that function are in different place than the output but that's okay because the FF is essentially a programming framework for creating distributed systems this is a little distributive system so you can model is as a function in fact and you can even give it a functional type in this function takes float and clients and produces a floating server in the FF we also have a little library of commonly used functions like ribbon federated mean will take a federated float on clients and and produce the average of those on the server and others are available now we know that I've introduced a you can associate writing programs so let's write a very very simple potentially the simplest possible for that computation it goes like this first tff is a strongly typed programming language and so you always start by defining the types of things I mentioned we have the filler that flow on clients and so there it goes next you're going to actually write a computation and so tff code is not Python code but you express it in Python and you know it's it's really the same idea as what you have in terms of law write in terms of law that's a flow code is not Python code it's tensorflow these are tests of lot of things that are excited about this test flow runtime but you can express them in Python write the Python is the language and you to constructor it's the same idea here right so you you write a little python function you decorate it as a so-called filler to computation you specify the different type of the input now in the body of this Python function the sensor readings parameter represents the the the federated float that came on as the input and now we can use Feathertop PFF to to slice and dice that's the value in this case we just call the feathered mean and that's it we return okay so now what happens here is that just those in terms of law the Python function gets traced and we construct a little tense of law computation representation in a civilized form and store it underneath that symbol it's kind of the same idea the TF function getting traced and you know tensor flow graph getting stored in a service form behind so that's that's what happens so so when I say that tff programs are not Python that's that's that's what it the get average temperature on both now represents a civilized representation of a code in tff okay and the reason that's important because again we want to run those things on devices and so they are not going to be interpreted by a regular Python okay so now let's look at something solely larger let's say you have this set of clients each of them has a temperature sensor and the analyst on the server wants to know what fraction of the clients have temperatures reading over some threshold right so have two inputs here the the you know red and blue data is sensitive we can't collect in and so what we do instead we use a federated broadcast operator to move the threshold from the server to the clients now that every client has both threshold and its own reading and they can compare it run a little block of tensorflow to produce one it see if it's over the threshold in zero otherwise and so you can think of this as you know like a map step in Map Reduce and we provide a federated map operator for those kinds of things as well then finally so what emerges is a federated float composed of ones and zeroes which you can feed as an input to the Federated mean operator and produce the final aeration the server so that's it that's the whole program in a diagram form now if you want to write code you kind of looks the same except you know it's called so you start by defining a Python function decorated as a tff computation you specify all the inputs as formal parameters and so you see the readings input these other you know temperatures the threshold on the server the inputs can be on anywhere words clients on server just you know at least all of them here and now in the barrios front of this function you can again use the favorite operators to slice and dice those things so you see the broadcast here again you see the map and mean and so on the client-side processing I mentioned it was in terms of flow and so in this case the parameter to the map function that that represents this this processing is implemented in ordinary tensorflow code and that's it you just either slap the types in top of it to make sure that everything is wrong strongly typed because tff likes things to be strongly typed and and into a type type check for you and that's it that's the whole program you can go in there and I think we have a version of this into tutorial as well okay so now let's let's rerun at all let's let's try further the training and I'm going to show just a small example of what we have described in a tutorial on that on the test flow website and I'm going to focus just on the computation that represents a single round of feather that averaging just like what we have discussed at the very beginning of this presentation okay so so this computation takes three parameters there's a model on the server that the server wants to feed to the clients there's a learning rate which now let's make it interesting and there's a set of on device data okay so the first thing we'll do is just as before with broadcast the model and the learning rate from the server to the clients now that the clients have everything model learning rate in their own slice of data they can perform their client-side training and likewise like in the preceding example is the ferret map operator for that and the local trade function will be another computation presumably implemented in terms of law that I won't show you know it would look as it always does and finally so the map function produces a set of client-side models locally trained models right and now we just called the fitted mean operator to average them out you can apply that operator to any kind of value including structured values and so so yeah that's it the output is the average of client-side models and that's what that's the algorithm that we had yeah and so that that's the whole program and and the diversion of it in the tutorial is you can see that actually runs okay so this was of course a very simplified example how we can you know start extending it and make it more interesting just just two very very very short examples I'm going to show one common thing to do is wanting to inject operation in worse places in to address various kinds of system concerns and so for example if you want to encode and compress data during broadcast apply encoding on the server before you broadcast and then use a federated map function to decode on the clients after broadcast and so you can see how basically you know two lines of code get you what you want with the decode and encode presumably being implemented in terms of second example if you want the differential privacy very easy before before you call further that mean to average your values you just call a federated map operator to them to the you know to the arguments and to add some clipping and noise that I'm representing here symbolically but that's what something that you'd you know let me just write it in terms of law so again one liner change for a change like this yeah and and you can you can sort of imagine other modifications you can do like this okay so how can you run it even though I mentioned tff code is not Python you can call it in Python like a function and it any trance in Python will happens under the hood we spawn a little runtime for t FF and run a simulation there and return the numbers in Python so its own score works seamlessly as it as if it were Python so in this case if let's say want to run five five runs on training but this is how how we'd write it it's just kind of what you would expect and full version of it is again in the tutorial so you just called it called the computation and get the numbers part back and then the model is represented doesn't compile structure okay where do you get data for our simulations you can of course make your own but we also provide a couple datasets and and and many more on the way in the tff simulations datasets module each of these has a load data function when you call it you get a pair of Python objects with the present train and test data and these objects allow you to inspect them now I mentioned before tff computations don't let you deal with individual clients or the IDs so this is you know things like inspecting with what clients are I'm in my data so that's something that you can only do when orchestrating your simulation in Python you cannot do an interfaith for you no privacy reasons so in this case you can you can look at the client IDs for example so that you can so you can simulate what I discussed previously the client selection right so here you're taking all the clients at these picking a random sample of them those are my clients for this round and now I call that the trainer object to construct the TF data data set this is an eager data set in the instance of law for that particular client and you know apply whatever pretty post-processing you want a pre person you want using the regular TF data API s and once you you know you could create a list of those those are doesn't my class those are my data sets you can feed it as an arguments into the computation just as I've shown before and and continue to flesh it out here to the Python table so it's very easy very natural to do okay if you don't want to implement everything from scratch as we sort of did in this tutorial you might use one of the account API so like the tff dot learning module so for example here's here's one in a function that constructs federated training computations it's easiest to use with Kara's if you have a Kozma if we do you don't have to use Kara's but it's it's much easier if you do so if you have a care of model you just kind of one line or function to convert it into a form that tff can absorb and and then these one liar are calls shown here take that model and construct computations that you can use for training and evaluation and you use them in the same way as you know you write little python loops as those that you've seen before so the trained objects has an initialized computation has a pair of computations trained initialize creates state on the server the initial state for the first round and then the the next computation represents a single round of training so it will take the initial state before they run started and produced in your state after the round completed and that state includes the modulus whereas various kinds of counters and things like that and yeah and in each round as I saw before we we can perform client selection and simulates you know various kinds of system behavior and things like that yeah so so it's very easy to use the same for evaluation you can take that final state after training extract the model out of it and feed it to the evaluation computation so the e value is a computation again you just call it like a Python function and that gets you the metrics back and things like this okay so yes this by default all when when you just invoke computations like functions others as I shown it kind of all just lands on your machine in in your process there are various ways to speed it up we provide a whole framework for constructing simulation runtimes there is right now there is one sort of a you know ready to use solution if you want to run multi-threaded simulations at this snippet of code that I'm showing here you can you know with one line you create a local executor that has multiple threads in it ends and then make it the default and whatever you type will run in that so to you know if you want something more powerful not not long from now we'll we'll have a kind of all-inclusive you know ready to use solution for running things on Google Cloud and kubernetes in a multi machine setting if you don't want to wait for that the you can actually just go and stitch it up yourself because all the components are basically there in that TF f dot framework namespace and those various kinds of little executor so you can stack up together indicator stack that you can use to construct a very small time machine architectures of multiple tiers of aggregation support for GPUs and things like that and and it's designed to be extensible so that people can plug in various kinds of components to it now if you want to go beyond just running simulations it is also possible for that the options are still emerging but but the two that are already exist on the table may may involve a bit of effort but you know possible one is you can actually plug in your physical devices into this English and frameworks so for example you can you can implement a simple of G RPC back in a back-end interface that we supply you know say to run on your Arduino device or something and then you can plot that as a little worker nodes into a simulation framework and now you can run on your physical devices a scarf that's not something you'd use for a large-scale production setting but it's certainly doable for small smaller scale experiments and also we have an emerging set of compiler tools that take tff computations and and transform them into a form that's more amenable for execution in a particular color of black and so for example there's there's a body of code emerging that supports MapReduce like systems in that takes computations and makes them look like mapreduces so that you know it's not it's usable not quite finished but somewhat usable if you're interested in pursuing either of those options I'll be happy to discuss apply and more display options on the way I can't really talk about them but yeah stay tuned for updates ok if if you need something that we haven't provided this is intended to be an open framework and a community project so by all means please contribute just you know implement it and send out that people requesting and so that everyone can benefit and there are many ways you can contribute you can if you're a modular you can contribute models and then datasets and things like that if you interested in Martian learning it further to learning algorithms you can contribute our garden to the framework can or helped us you know we architected to make it easier to use cultivate core abstractions also you know new types of backends as I mentioned this back-end support for for actually deploying things is emerging and if you have an ideas perhaps you can contribute to the tff right that's all I have thank you very much so yeah the quick two questions are when clients start learning and on their own data and then you have an averaged model on the server do you ever send the average model back to the clients for like performance boost or do clients just spin off on their own afterwards and then the second question is how do you start the model you use proxy data initially and how do you iterate with your models accuracy and things like that yeah so for the first question in a system we have running in production the way it works and that's the scarf that's different from tiara that's just a deployed platform right and so there are many ways you can engineer this but just talking about the particular example a production system the clients periodically come back to the server so everything every time clients gets involved in a new round of training they automatically get that new model so that that's one way you can arrange for this to happen that's probably the easiest so you're kind of contributing as well as benefiting by getting latest and the other question was how do you get started on building models and so if you do have proxy data and you think it's useful then you know it certainly helps to to play with it at least you can get some idea of what model architectures are good you know you can never be sure because proxy data is you know on this so good and this you never looked at that on device that I'd never really know for sure how good your proxy data might be right so the a so you might use proxy data but what you might also choose not oh right you could simply you know try different model architectures deploy them on devices in like as I mentioned remote so there would be kind of running on devices and getting evaluated but not affecting anything other than consumer ability resources you could you know you could deploy a hundred of those at the same time on different subsets of the clients and see okay which are the most promising that second route will be more of a you know pure approach that applies to any kind of on device data including when you have absolutely no idea how you know where we were to get proxy data like you know some weird sensor data my look like that yeah and both boss are possible and so a first question I have is does the TF tff library does it integrate with TF light and the second question I have is since its language platform agnostic are you able to use it in non Python and you know it can I use it in the language that's not Python yeah okay we start from the second one so so tff computations are not Python there's a I think I had it linked on the slide Ethan all I can follow up later I am there's a protocol buffer definition that describes what the tff computation is and it's it's you know it's a data structure that has absolutely no relationship to Python so yeah you could you could take it and you could execute it in a completely different environment that has nothing to do with Python and test of law code in the inside of that computation is represented as graph deaths there's a flow graph deaths so if you were to run it on a different kind of tensorflow runtime to the extent we can take those graph this and convert it for that other runtime you know maybe converting the observe whatever and that's odd option so tff itself doesn't integrate with TF light because TF f itself it does does not include the platform for on device execution the FF is more like the best thing of it to could you think of it is more like a compiler framework and dev environment but yes you could use it with your flight so you know you could define your computations and you know maybe apply some conversion tools to convert all the terms of local in those computations into a form that TF light can can absorb and then arrange for it to be executed thank you there's one yeah good talk thank you I had a couple of questions um so uh does the client do the two models train until convergence so bigger try on the clients do they train until convergence today or no no typically you would make a few passes over the client data set you don't have to train for convention you gonna you're gonna round you know ten thousand rounds anyway so and and when when the average model doesn't have access to the data how do you measure its performance and how do you know it's good enough to now deploy send it back to all the clients if sorry if average mother is so the average model is on your local server I don't it's and then you don't have access to the data how do you measure the performance of the average model how do you know when to deploy that model back yeah so so I did not describe federated evaluation but you it basically it's like the temperature sensor example right you can take that model broadcast it to the clients now the clients have the model and the data they can evaluate each produces you know some whatever accuracy metric average those those out or you know computer distribution and there you go so for that evaluation is kinda the same idea just simpler okay and another question was is there a way in the federated learning intensive flow where you can share parts of the for example the clients so that the clients have different labels assuming but they have similar data is there a mechanism where you can say the client share most of the model but they have their own couple of layers for them maybe the last layer for the network is specific data line but not shared across clients or is the entire does the entire model have to be shared across all the clients yeah it's it's not a capability that we included at the moments but sounds like conceivably something we could do if you yeah maybe you can fill up a flight medic you can continue yeah thanks Hey so one question that I had was that like when you kind of aggregate all of these models into a central server it seems like it seems like one of the problems that federated learning solves is I guess distributing computation but when you get to like a million people using the Google keyboard or a lot more actually it seems like either the server's gonna have to reject some gradient computations or there's some hierarchical aggregation system where you kind of like aggregate the models upstream or whatever so I'm wondering if if if the second is true are there latency issues with gradients reaching the central model like after like by the time that the models changed so much that it might corrupt it a little bit so a couple of things first with this is not the same as gradient descent in the sense that each client does a whole bunch of you know computation right it trains on you know for a while so what we said is what client send to the server are not that gradients their updates differences between trend models and in show models that include a whole bunch of clients a train just that's just one thing the second one it with respect to which clients have to add participating computations so not all clients if you say have I don't know a million clients you could pick you know thousand clients samples and first it you know first train may make an iteration of the model on the first thousand clients than making iteration and another thousand lines right you don't have to include all the clients at once you know really the only thing that matters is that eventually most clients participate so that most clients have a chance to influence the model the trading process at some point but they don't have to simultaneously be present but with respect to hierarchical aggregations that's also true so the both are both both are true right we do you do have hierarchical aggregation in your system because you know you don't want a single server to be talking to that on ten thousand machines but you also don't have to include the entire population I think I answer to all of them but okay alright thank you 