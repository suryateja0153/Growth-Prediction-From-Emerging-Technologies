 hello everyone my name is Makar except aveer I'm a senior software developer with IQ view and today I'm going to present top-down specialization using a batteries part so today we're going to go over K n on a weak theory which this work is based on followed by an overview of top-down specialization algorithm how the data was pre-processed how the algorithm was adopted for spark the test environment that I used key results from my experiments and key takeaways that I've learned so before I begin I would like to acknowledge the two papers that this work was based on the first one was top-down specialization for information on privacy preservation by Fung and Wang from Simon River Fraser University along with Phillip you from IBM research and the second one is that top-down key anonymization implementation for apache spark which was the first paper that attempted to adopt this top-down a specialization algorithm for spark where I base my work on and I aim justice so what exactly is key anonymity so ok honey anonymity basically is a privacy preserving data publishing theory that aims to protect personal data before a data set is shared for secondary use such as research or analytics so it basically states that a data set is called key anonymous when for every record there's at least K minus 1 records with the same quasi identifier values so quasi identifiers are basically those attributes that win are when we look at them on their own we cannot identify an individual but when they're used collectively they can identify an individual so if we look at this data set for example we see that there are two females with grade 12 education who live in a peon two other females who with an associate degree believe in Kannada but however we do see three unique records that can identify an individual so an adversary for example who with the pre knowledge of the existence of a mail from Orleans with a master's degree will be able to tell that they make $50,000 a year so in this case the quasi and the fires are the education gender and city and the income is what we call a sensitive actually which is the attribute that we're trying to protect that so how can we make this data set satisfy K in on a meeting we noticed for example that bachelor's master's PhD can be generalized to the same parent value which is post-secondary we also known as that Chapel Hill Orleans and Beacon Hill are all three neighborhoods in the auto East area so what we can do is that we can cross out these six values and then replace them with their apparent generalized value which in this case is both secondary and auto East for education and second city so now the same adversary will not be able to tell which one of these three records are the male with the master's degree for Marlene's so before they will not be able to tell what their income is and this data set is now to anonymous so that means for every record there's at least one other record with the same quality identifier value so what exactly is top-down specialization then so it is a way to achieve this transition that I was talking about but in a way that aims to balance data utility and privacy because as the data becomes more private it becomes less useful for research or analytics so for every attribute that we wish to for every akua's identifier attribute in the data set that we wish to anonymize there's a corresponding taxonomy tree so at Exxon Demetri is basically a hierarchy that represents the classes that all the distinct values belong to so what we're looking at right now for example is the following tree for the education attribute so the root note is the most generalized value and the leaf nodes are the distinct values are in the dataset so top-down specialization goes through each level of that tree which we're going to call anonymization level from orang from now on and it aims basically to keep specializing value starts with the most general I starts from from the root node it keeps specializing the values until K is violated so until the data is very specialized at every level what we're going to all call an iteration so every anonymization level is an iteration and the algorithm for every anonymization level it calculates what we call us for so the score basically comprises information game privacy loss if I were to specialize this attribute to this anonymization level how much information a like in and versus how much privacy will a dataset lose so for example I pick the all the values that belong it to the left subtree and I draw eyes all those two without post-secondary and on the right subtree I take masters for example in doctorate and all these values from the leaf node on the right subtree and generalize all those to post-secondary I make the calculation calculate the score and I do this for every quasi identifier attribute and the attribute with the highest score is the attribute that will end up specializing and keep going all the way from the top down until K is violated and then we stop and this will be the our key anonymous data set so pre-processing of the data set involves removal of all the non quasi identifiers so that our data set is basically an error so from now on I'm going to refer to cause any viruses Q IDs and so those who are the Q IDs and the distinct values of SAS which is the sensitive attribute in our example was income these are all grouped together together with the count also to make to make it a little bit simpler instead of using the numeric value of the income we're going to use a binary value for a less than or equal 50,000 versus over 50,000 once we have that as you saw from the top from the top down overview is that we this is a very iterative algorithm so we are going to go through that tree over and over again for every anonymization level and can calculate that score so it would be neat if we can find the parent for that this particular distinct value belongs to without having to traverse the tree over over again in run time so it'd be nice if we can do that actually in constant time so in order to achieve that well we do what I call a building a path map so we're going to traverse the tree in a breadth-first manner so we started from the top and we were going to maintain a queue on the left-hand side along with a map on the right-hand side that map is basically the child parent mapping for every child and what their parent is in the tree and the queue on the left-hand side it will contain the number the the node that I'm traversing as many times as if they have child loans so in this case for example in the first level I'm traversing the known any so I insert any twice to the that Q because any has two child nodes without posting their in post-secondary and then I keep moving to the next level and when I Traverse with that post-secondary I pop or DQ one element from the queue and this represents that parent of this particular node so on the right hand side I have the child parent mapping so I'm saying okay without posting Larry's parent is any and then I Traverse post-secondary and then I talk or i DQ another element from the queue and this also represents any or it represents a parent for this particular node so I have seven errands Baron is any and post-secondary parent is also any and I while I maintain that queue you notice that I inserted without post-secondary three times because without post-secondary have three child nodes and I inserted post-secondary three times because it also has three trombones and I keep going through all these levels until I end up with with an empty queue and I've child child parent mapping as you can see on the right right hand side so for example a preschool whose parent is without post-secondary and so on and so forth now with that map I can recursively go through it and build a path basically from all the distinct values all the way to the top so for example if I'm interested in Darren finding the path for a ninth-grade element I know that its parent is junior secondary whose parent is secondary whose parents the post secondary host parent is any but since we're traversing that tree from the top down so I simply reversed that path and now I have the ninth grade in order to get to the ninth grade I grew from any tool that wasn't there to secondary to junior secondary and then to the ninth grade so now with these path maps I'll be able to tell what parent that or the anonymization anonymization level that I'm at for for every element in constant time because basically it's a kilo cup with a ninth grade which is the value in the dataset I won't be able to tell that the parent right away is any and that was in there and so on and so forth so how is this adopted for spark so basically before we start we needed to generalize all the croisé identifiers to the root of their non organization level so any is always the root for all the sonic trees so in this particular example you will see here that education gender and city are all generalized to the top of the Sonic of the corresponding photometry which is any in this case I'm also maintaining the aggregate for every for every collection of Kwas identifiers so for example the count or the aggregate of records with all any is 21 which is basically the whole the whole data set once I have that then for every anonymization level I need to calculate that score which basically involves the information being and privacy loss it uses a formula whose parameters are plugged in using these aggregates that's why we need all these aggregates so that we can calculate the information again our privacy law support at every level so we have education for example there are seven records that can be generalized without post-secondary we have 14 records that can be generalized to post-secondary and we do the same thing for a city and we have the corresponding areas for both east and west and now once we have that we need to score the best option so all the anonymization levels level values are aggregated forever at every partition and then they are merged into a single row table with the totals so I need to basic compare the score for education first as the score for four city using using these aggregates so now I have here the city and I compare that accurate with with education so and then once they have all the aggregates I need to basically merge all these attributes from all these partitions into one roll data frame that has all these aggregates so the Y and the N represent the the the the aggregate of the records that can be generalized to any that has over them $50,000 as a sensitive attribute which is the income in this case and the n represents the second value of the sensitive attribute which is less than 50k so for example there are if I start from the top there is 21 records that can be generalized to education any there are twelve records that can be generalized education any with over 50,000 as the income non records that can be drawn lines to any with less than 50 tests and so on and so forth for every note as well as its children so the idea is that we want to know what the score is or what the information again care privacy laws if we were to specialize this any known to its children which is an artist was without post secondary and post secondary so we do with this for all be quasi identifier attributes I only included the example from education because as you can see the this will be a very wide data frame so I only included the areas from education as an example but this would be for all the qualifiers that we into compare once we have all these numbers these are basically plugged in to the formula that calculates information game and privacy laws and now I have that one final number that tells me what's the score if I were to specialize any from the education attributes to its children off without posting their impose in there and it will the same thing for all the other edge fields once I have that my best option I need to then reiterate before I reiterate let's say for example that city is the highest score anonymization taxonomy 3 so I need to basically remove the route and take its sub children and add them to ad anonymization level set so in my next iteration I'm no longer comparing to anonymization level trees I'm comparing now 3 which is basically the sub children of the highest score photometry along with whatever other trees that I have in the set that basically did not have the high score so I keep doing this until basically K is violated and now I have my final my final score sorry my final data set with all the join or with all the specialized attributes using this these iterations so how is this adopted Forest Park so basically starting with the preprocessdataset that was transformed to partitioned data frames so the preprocessdataset as the data is the example that we saw earlier which basically has these group by and count data set and then we start partitioning all these this data set to n number of partitions I'm going to over the partitions as well but we basically go over n number of partitions and we calculate the aggregates or sort of prepare the sum function some map functions which is a built-in support function to calculate the aggregates for every for every attribute that we are looking at to calculating their score and then once we have the these map functions then we collect to one single row data frame that we saw here and now I have all these total aggregates collected and now I have my parameters that I can plug in to the formula and calculate the score for every attribute and for every anonymization level and we use this the sparts to to local generator function in order to come up or in order to collect these aggregates to one single roll so I ran multiple experiments basically to assess the parallelization of this algorithm or how well it paralyzes on spark so which was the the main actually outcome of this project is that assessment to see how it how well it Carol Isis so before I begin looking at the results I need to tell you what my test environment is so basically I used an openstack cluster with 32 gigabit of a game gigabyte disk 8 gigabyte of RAM and 4 virtual CPUs per node the key was always set to 100 and I ran these experiments on one node 2 4 8 and 16 nodes in the cluster I also used data set sizes of 250 thousand 5 10 and 20 million rows the SPARC manager that he was was for these Springs was the standalone which comes out of the box version 2 for 2 at the time with Scala 12 and Java 8 and the dataset is called the adult dataset which appears to be a very popular dataset for privacy preserving algorithms research so basically it's very similar to the examples that I use it it has demographic data of education city marital status gender also all sorts of different demographic data but it was only 32,000 rows which was obviously not enough to assess any paralyzation data it's our experiments so I enlarged by generating random values from the list of distinct values in the original data set to come up to actually to come up with this these different variations of the different sizes Catina set in my experiments I also used a categorical queue IDs that were tested that that we aim to assess its core and anonymize so the first experiment that we wanted to look at is what was the what's the n number of partitions that we can use to achieve the best front time so for this experiment I used the eight node cluster and the five million row dataset so as you can see here we ran this experiment eight times the best-performing was 32 in this case if you recall my cluster every node had four VC a virtual CPUs or four for virtual cores so the best-performing number of partitions was 32 while the default was 200 so as you can see that the best-performing was a lot lower than the default that spark came with so these experiments were necessary in order to see what the best performing partition is and this number was always used for all the experiments so when I ran one experimental one node I used four on two nodes I used eight as a number of partitions and so on so forth all the way it tells my sixteen experiment the next one was assessing the speed-up so I ran the speed-up tests on all the four latest data set sizes that I mentioned so the first one was two hundred sixty thousand rows data set test so as you can see the the bottom line here is the optimal run time so we would expect if you double the number of nodes you your run time will be cut in half because every node is doing is dividing the work but as you can see here this was not achieved with the smaller data set of 250,000 rolled the actual time frame the actual runtime was far away from the optimal and as a matter of fact going from two nodes to four nodes did not improve the run time at all and going from eight nodes to 16 nodes also had a flat run time the run time did not was not reduced at all so there are two theories that could explain this behavior is that the first one is that when your partitions are very small and the overhead of the partitioning and the network traffic is basically outweighs the benefits that you will get from the run time because every partition is very small the second theory is that all this data set was partitioned over a randomly generated unique ID so bustable that a custom hash hash function could have helped in this in these small sized data sets to achieve better better parallelization as you notice here as we keep increasing the data set the actual runtime gets the gap between the actual on optimal runtime gets very close together so for the five million row data set it's very close to optimal but not yet quite yet because as you can see also from going from eight knows to sixteen knows the wrong time was was flat it did not cut in half the story gets a little better if we keep increasing the data set so with for the 10 million row data set it got even closer and even better for the 20 million row data set which is the closest that we could get to the optimal as you can see from this graph so the scale of performance it is basically another way to interpret the the previous slide or the previous results for the 20 million row data set as you can see overall when the the hundred percent increase in the data set size only resulted in like 55 percent to 65 percent increase in time and this could be also explained by just by the speed-up graph is that going from 10 to 20 million row now every node is doing a lot more work than before and that's why the runtime did not significantly increase and as a matter of fact remained almost virtually flat if you compare it with with the data set size increase so what did I learn from these experiments so there are multiple key takeaways that I learned the first one is SPARC is actually best suited for iterative algorithms such as top-down specialization it was actually a research paper that I came across during my literature review when I was working on this project that compared MapReduce framework with spark for iterative algorithms and it found that spark performed up to 5 times better for iterative algorithms because you can take advantage of all the caching and partitioning features that come with spark out-of-the-box the second point is that there's magic number four partitions it has to be analyzed for every algorithm and for every data set size and I also came across a lot of literature that we're saying twice the number cores is that is that is what you want to set your partitions to these numbers are all algorithm specific as you can see in my case double the number course didn't actually perform poorer than n number of course when I set the partitions to end on their course the other thing is the dataset size so there's no guarantee that beyond the 20 million row data set I would have been able to use also n number of cores for my partition numbers because they were like 40 million rows or 100 million rows it could have crashed if I keep if I kept the number of partitions at n number course so it has to be analyzed for the data set size that you're using it also has to be analyzed for the algorithm that you're running on SPARC the third point did not really have a significant improvement over in my runtime or my performance but it was significant enough to put it down is that if you're familiar with Scala's tail recursion there's a lot of optimization that comes with tail recursion over loops for the code that runs on executor so in my implementation I for any code that runs for any iteration that runs on SPARC executor I used tail recursion instead of over instead of for loops and also from a developer perspective tail recursion is just fun to work with and belabor it for loops I guess the next two points are almost related so the first one in hindsight it was not very surprising we should always aim to minimize the number of operations or collect calls such as aggregations to only a maximum of one per iteration obviously this depends on the algorithm so you may not have the luxury to minimize them all to one aggregation per iteration but in my case that was good enough and that's why I had this wide data frame that I collected from all the partitions that's why I have only one map function that creates them all at the same time and collect calls to a wipe data frame was actually performing a much better up to 16 times better in terms of runtime then if I do small aggregations on every partition so my brute-force attempt was that okay I it makes sense that it would perform better if I run these small aggregations on every partition so that I have less to work with but actually on the contrary having that huge collect calls from all the partitions to a single row data frame using the to local iterator.call actually had 16 times improvement in the runtime and my last point is the hash function so the partitioning over a tidy as you can see achieved close to optimal speed up for datasets larger than 5 million but what about datasets that are less than 5 million rows or finally rows and less can a custom hash function help with partitioning on smaller data set so I guess this is an app point for future reach research to consider so all the the implementation that I talked about the resources that I used in my research are on this page I'm gonna pause here for a couple seconds if you want to write it down or you can ask me in the chat I will be happy to share some some of the papers there are behind and I Triple E NaCl SEM firewall so but again if you may be able to get them from Google Scholar if you're interested in one of the papers by the title and also see the experiments a spreadsheet that I have for my experiments and as well as the actual implementation so that wraps it up for me so if you have any questions feel free to ask and please don't write don't forget to rate the presentation thank you very much you 