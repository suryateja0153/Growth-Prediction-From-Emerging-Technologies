 hi I'm Simpson Garfinkel I'm the senior computer scientist for confidentiality and data access at the US Census Bureau I'm going to talk today about our work using spark and differential privacy to protect the privacy of the 2020 census respondents this is a pre official presentation but the views that are in this presentation are mine those are the author and that is not in the US Census Bureau and this abstract this year so that Google will be able to find and index this talk this is the work of a large team at the US Census Bureau the team is headed by John about the chief scientist Dan Kiefer the scientifically came up with the top-down algorithm and we have a team with many people on it so the outline is I'm going to first talk about the motivation behind our use of spark and differential privacy and I'm going to talk about how we're using the differential privacy in the 2020 census I'm going to talk about the top-down algorithm and then I'm going to talk about the work that we've done monitoring spark and this is largely a homegrown system and I'll say that if anybody in the audience can think of a better way of doing this please contact me because we are actively looking for additional ideas so the census is called for in the Constitution article 1 section 2 calls for Congress to make an actual enumeration of the people of the United States every 10 years and that information is used to apportion the House of Representatives it's also used to distribute 675 million dollars in 2015 and presumably more today in federal funds and it's also used by the Department of Justice for enforcing the Voting Rights Act of 1965 and so it's very very important also that the soneul census is used to calibrate the American Community Survey and then other projects that we have at the Census Bureau it is the reference writing so for the decennial census and for all data products that the US Census Bureau puts out we also have to follow strict confidentiality requirements as specified in law in title 13 section 9 of the US Code which is title 13 is the census Act and section 9 says that we cannot make any publication that reveals the data provided by a person or an establishment so one way of thinking about this is that we can't make any publications so that you can draw a line from the person or the establishment that provided the data to the data publication and we our Census Bureau employees are sworn for life to protect that respondent data and the data can't be used for non statistical purposes which means it cannot be we cannot do special medial or confidential data for law enforcement or immigration or anything like that now for the 2010 census we collected data on 308 million seven hundred forty five thousand five hundred thirty-eight people in the United States but for each one we collected where they lived the household they were in their sex their age their race their ethnicity and their relationship to that householder so you could be the householder you could be the householder spouse you could be their child their parent you could be an unrelated person living with the householder it comes to about 44 bits of data per person and so the entire 2010 census the raw confidential data is 1.7 gigabytes which doesn't really seem like that much data in fact if you think about it in terms of integers we basically collected six integers per person but we publish that data in many different ways we we collected two we summarized it we tabulate it we put out publications that say the number of people living on a block the number of people of each Reis living on a bar we publish age pyramids at the census track level in total we published around 2.7 billion integers counts in the redistricting file which we call the pl 94 171 file after public law 94-142 is provided to the states but the balance of that we put in what's called summary file 1 so in total we published around 5.5 billion integers now from high school math you may remember that if you have more equations than you do unknowns you can actually solve for the unknowns and you can literally build a system with 5.5 billion equations and solve for the roughly one point eight billion unknowns and we've done that that's called database reconstruction so we've performed that database reconstruction attack on our published data the data that we published in 2010 using only public microdata we were able to reconstruct all three point three hundred eight million seven hundred forty five thousand five hundred thirty-eight microdata records we did that we performed a database reconstruction and reaiiy denta fication attack on all of the published data from the 2010 census and we reconstructed three hundred eight million seven hundred forty five thousand five hundred thirty-eight microdata records and then those records have age address sex and ethnicity but they don't have name so we then bought a commercial database in 2010 that had names addresses ages and sex and if you link the two databases together you now have linkage between name and ethnicity and race and that's considered by most Americans to be very confidential data we also have linkage to ages of children which also most Americans considered to be quite confidential it's actually hard to buy information on children in the night states are linkage rate was about 45% and the week we know the confidential data so we actually then peaked and we saw how good we got it and the reai denta fication rate was around 38% of the length data which means that for 17 percent of the u.s. population we could reconstruct the confidential data so you shouldn't worry that your confidential data may be revealed by the Census Bureau we didn't publish names in 2010 we're never going to publish those names and we have prepared this public service announcement which you can view later explaining how we're going to keep your data safe but another thing we've done for 2020 is that we are modifying the privacy protection mechanism that we're using now to explain to understand how we're doing it you need to drill down on how we actually publish data what Statistical agencies do is we take data from people like here we have seven people on a blog and we tabulate that day two different ways we might publish the total number of people on the blog their median age their mean age maybe the number of females on the block the number of males number of people in each racial category you can take those counts and those means and you can create a set of equations and in this case it comes to about a hundred and sixty four separate equations so I did that on my macbook pro my macbook pro can solve those equations in about 0.2 seconds and come up with the confidential data based solely on this published statistics so the basic idea of differential privacy is to take the data that we would publish and add noise to it so in this case the count of that block might be nine people but we might publish eight people and the median of age on that block be 35 and we might publish 45 so adding noise protects the confidentiality of the respondents now what differential privacy does is it allows us to control the amount of noise that we add so that the statistics that we care about are modified the least and the statistics that we don't care about are modified the most and there is a minute physics video that we work with the creative men in physics to do and it's it's more than a minute but it's totally worth watching we're not going to watch it now differential privacy allows us to control that privacy loss versus accuracy trade-off it allows us to put the accuracy where we need it and we say that it's future-proof because no matter what breakthroughs there are in the future the respondent data protected with differential privacy remains protected now the question is how much noise should we add so that's really a policy decision and you can see on this graph that if we add a little bit of noise we get highly accurate data that's off on the right we get highly accurate data but there's a high privacy loss and if we don't put a lot of noise in over on the left um that's less privacy loss but the data is less accurate so that's also more noise so the way the top-down algorithm works is that we basically compute the statistics at each layer of the census we compute statistics at the National layer at the state layer at the county for every track and for every block we then add noise to each of those sets of statistics and we end up with a set of noisy statistics for each different level and then we use a commercial optimizer to make sure that all of those noisy statistics are consistent with each other now there's no off-the-shelf system for applying differential privacy to a national census so we had to create one we had to create a system that would produce high quality statistics it densely populated geographies like in cities and for the nation and produce consistent tables so we created this new algorithm that as I said it it processes statistics top-down it starts at the national level it then computes the statistics of the state level at the county level at truck bought group and funding at the block level and this system fits into the decennial production system so the way that disclosure avoidance system works is that it takes what we call the census edited file it then performs the differential privacy mechanism we call that disclosure avoidance because we want to avoid an improper disclosure and then it produces the microdata detail file from which the tabulations are made I think it's a more enlarged diagram of what we're doing we actually take what's called the decennial response file that people census returns it's also three ports from the numerators who visit house the house that goes into something called the census unedited file it is then edited for correctness for example you might say if you're filling out the census form for your household you might say that your mother is 30 years old and your daughter is 95 years old and so maybe they're both living with you and you swap them so we might reverse those two ages that's an example we then take that census edited file and we apply the global confidentiality and protection process we do the differential privacy we make the microdata detail file and that microdata detail file is then what's used for the official tabulations so we actually don't work with microdata in the census system we actually work with histograms of people who live on that block and so to understand what that means is this book that our sample block which had seven people on it you can think of it as a histogram which is largely sparse it's a histogram with sixth amendment with five dimensions one dimension is age one dimension the sex when dimension is race when dimension is ethnicity and one dimension is relationship to the householder so it has about 500 thousand cells and most of those cells are zero but seven of those cells have ones in them we take that histogram and we apply differential privacy to that histogram and then we have a noisy histogram and from that we reconstruct the microdata that is the most consistent with that noisy histogram and so we end up with different kinds of people but there's a relationship and if epsilon is high we add a very small amount of noise and if epsilon is low we add a lot of noise and if we add a very small amount of noise when the histogram looks a lot like the input histogram so our original mechanism which I call the block by block mechanism started with eight million habitable blocks it added noise through each histogram and it created eight million protected blocks now apparently this version of the histogram is about two hundred and seventeen thousand cells but if you look at that we we start off with 5.3 petabytes of zeros 1.7 gigabytes of data but it's farce we've run that through the disclosure of winning system and then we coalesce that into a new set of microdata so we go from one point seven gigabytes the 5.3 petabytes back to one point seven gigabytes that's beginning to sound like something you would want to do with spark right so here is actually a map of how we do that with spark we have sixteen gigabytes comma separated values file we create an RDD with eight million eight million cycles farce histograms we use a map operation to apply the noise we then use the Roby linear program mixed integer linear program to solve all those histograms to be consistent we then map it to create microdata we save it as a text file so we very clearly distinguish the part where the noise gets the I mean if noise the microdata the the protection process of differential privacy from the post-processing which is done with Roby and most of the work is going to be in the post-processing so the problem with that mechanism is that every block ends up with a lot of noise and when you create those blocks up to get counties and states the noise gets added up to so in 2018 we invented the top-down algorithm which actually first works on a national histogram and then it works on the 51 state disagrees because we treat the District of Columbia as a state for this process we then work on the county histograms on the census tract histograms and eventually on the block level histograms so each set of records is turned into a histogram we add the noise we get the noisy histograms we then take those noisy histograms at both the national level and the state level we make them consistent with Grobe we then take the state level histograms and we make them consistent with the county level histograms again using groovy and you can see that the histogram storage gets bigger and bigger and bigger as we go down the first histogram only takes 869 kilobytes the states think 44 megabytes and then when we're at the block level it's 5 terabytes and we actually need to store 2 histograms so it's a lot of data right and in terms of counts um we still when we're done with this we only want to have you know each of those histograms has 217 integers in it so it's a total of 5 terabytes those slides I think we're out of orbit so so all this is going on and you might get the idea that our rdd's are not very well balanced at all because we have one RDD for the national level and it's like a megabyte and we have one RDD at the block level and it's like you know 5 terabytes we have another design but we haven't moved to it yet because this design works and you don't have a lot of time left so the top-down algorithm is not balanced most of our computation is done in guro B so we call Berube as a shared library in that mapper so that's even invisible to the spark it's the it's like this big Python process going on we typically have to perform around 800,000 separate optimizations for a typical run we're looking at other algorithms for we'll actually need to do around 3 million optimizations per run core OB is a multi-threaded shared library so we can tell grow be you know Python thinks it only has one thread and spark is giving 4 threads for each executor but you can go use 90 threads because we're running on the machine with 96 cores and it's very unlikely that all this course are going to be used at any given time we're running in the AWS gov cloud and it turned out that the AWS monitoring tools like cloud watch were just two course they weren't recording data fast enough they were throwing the data way after when we want to look at it and there's the data that they were collecting wasn't really synched up with with our runs so we ended up developing our own monitoring system specifically for this application now we're only going to run the top-down algorithm once in 2021 because we're still collecting the data now in 2020 but to develop into this algorithm we have to run thousands of trials thousands of tests so we also need to have a system for keeping track of all those runs and so we could see how our algorithm got better so to do that we built a separate monitoring framework we have an agent that runs on every node on the master nodes and on the worker nodes they send data separately to a rest server that rest server stores that's an Apache server using the rest protocol that stores data in a MySQL database we're using the Amazon RDS service and we store the results and we have a web-based display so here's an example of our cluster list we typically have anywhere between three and ten separate EMR clusters running at a time the clusters are we bring them up when we bring them down all the time so the Amazon cluster list I'd enter the cluster identifier is difficult to work with so the clusters when they start up they they pick randomly chosen names for themselves so this one the first cluster picked the name about a Bab dat and the second one picked the name Bri OS each one has a potentially a different version of EMR running and then we have our own release system you can see that the eye box has eight workers right now with a total of six gig of ram excuse me um six yeah six gigabytes of ram and the the second one has six workers but it only has four point six gigabytes of ram the first one has 768 virtual CPUs excuse me not kicking bytes terabytes that's running on machines that have a 768 gigabytes of RAM and then so sorry about that terabytes of RAM not gigabytes of RAM um anyway we we track the CPU load on each machine with these little graphs and if you click the show button it would actually show you each of the workers so you can see how balanced it is and here I do that with the brios cluster so you can actually see graphically the CPU load on yum each machine and the memory load we could create and kill the clusters for each separate run but for various reasons we decided not to do that the clusters just think too long to start up and and you know typically we'll have like a hundred of these machines running at a time and there's some capacity issues that we've run across in the past so each run of the desk we call a mission and the mission names are randomly chosen with an adjective and a noun so here we have litigious welcome as the first mission and we have weightless unique is the second mission each of these missions for like a test run we try to run small run so we actually have these small data sets or we might be trying to just work on one aspect of the algorithm you can see there the fourth mission intravenous bread if it crashed so it has an execute of one and it only ran for a hundred and fifteen seconds but the the first one litigious welcomed it ran for 72,000 seconds and we tracked from the Robi version we tracked other information we actually tracked about a hundred variables for each of these missions for just the mission itself itself this way itself each mission there it can go do logging I'll go back to that previous slide there's a log statement and so the missions can put logs locally but they can also send log messages to the centralized console so you can go to this website and see interleave the log files of all the missions or you can see just the log for the particular mission and you can see that while they're executing and you can see that when they're finished you can also click and see the entire law yarn log from that mission or you can see the yarn log for a particular node this graph here shows for a different mission the execution of the top-down algorithm so all the way off on the left that's when the noisy measurements are being computed and then the first blue green bunk that's when the nation' the nation's a nation-state is because nation-to-nation saw was being done and that can only be done on one machine but it's using 96 cores and so the load goes up to 100 then we do the nations of state saw that's the second green bump there and that bump is again it's using all 96 cores but it's not using any of the other nodes now you might say well why don't you just shut down the other nodes because it looks like you spent four hours well we could but those other nodes are holding the noisy measurements right now and so if we shut them down we'd have to put them off in hdfs or something that we have loaded back and it's petabytes of data so we keep it around anyway I guess here it's it's like high terabytes of data but we keep it in RAM then we do each different solve and then you can see that that big noisy part in the middle that's um that's when the I think that's the tractor block groups all happening and then the next big bump that's the block group to block solve happening and then there's a whole bunch of statistics being computed at the end and it's not using a lot of stuff on the nodes but the red the red bus of the red line that's the master you can see there's a lot of stuff that the master is orchestrating and it keeps getting all the other clusters all the other workers and then finally everything gets saved out so you know this is a run that is thousands of dollars in computer time it's running on machines that are seven dollars an hour each and I think we have you know you could count like some large number of nodes um we track the free memory now we used to run out of memory and our machines kept crashing but now we know why and we stop that from happening but we still want to be able to track the free memory because if it gets down the zero that would be really bad and you can see the master node only has about 200 Giga free memory and the worker nodes there okay we also tracked the HDFS usage we track include in Elko if we track actually all the parameters I'm just not going to show you all this graphs so in summary in 2020 so let me just see the reason we track this is if we're trying to make me hunker in the more efficient at the same time we're trying to make the algorithm better so the word efficiency is double loaded the statisticians the computers I just they like to think of the efficiency the statistical efficiency how little noise gets into the statistics that we care about and keeping that noise and the statistics that we don't care about and the engineering team once the machines be used as efficiently as possible we would like to see the load on every machine at a constant 100 they have 96 cores it's ok if they go up to 600 loads that just means it's going to take 10 times longer but at least all the machines are running with a load of 600 we could increase the number of clusters and run that faster but it would the number of workers but it would then take longer sorry we then cost more so in summary the 2020 census will use differential privacy as its privacy protection mechanism differential privacy allows us to add carefully controlled enough controlled noise to the statistical queries most of the work is in the post-processing to make the microdata for legacy users you might remember from that graph the noise was added all at the beginning and then it was all the other parts of the top-down algorithm and we use smart to distribute the load to a commercial optimizer we use Aerobie and we solve in that one I showed you around 800,000 mixed integer linear programs the new version of the top 10 algorithm will probably solve around 2.4 million mixed integer linear programs maybe more so with that I want to thank you all if you're interested in this more you can get the ACM Communications article that we published on understanding database reconstruction and that has that example with seven people worked out there was this lovely article in The Wall Street Journal and there was an article in science and there's been a lot of other coverage about our move the differential privacy if you want the actual details you can actually get the source code which we've put on github this is the first time ever that the Census Bureau has published source good for its disclosure mechanism and we've also published our detailed design specifications for the system and there's a scientific paper describing the mechanism as well if you want more background it's here and with that I guess I'm free to take questions so thank you very much for watching you 