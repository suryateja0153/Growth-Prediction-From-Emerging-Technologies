  Welcome. Thank you so much for being here. It's a great pleasure and honor for me to introduce to you professor Michael Kearns today, the 2019 Ben Taskar Memorial Lecturer. The list of honors and awards and titles that he holds are just way too long for me to list here. What I will note is the sheer volume of diverse and very broad list of topics that he has published on with diverse areas and major contribution to computer science, data science, social systems engineering, economics, statistics, social decision-making, and, most recently, to ethics. And that's what we'll hear about today. As someone who is deeply concerned about issues having to do with computer science, as well as their social implications, I believe he's the most suitable candidate for the Ben Taskar Memorial Lecture today. Thank you so much for being here. Thank you. [APPLAUSE] OK, thanks a lot for the kind introduction. Let me just start by saying it's a great honor to give a lecture in honor of our wonderful late colleague and friend Ben Taskar. There were many, many wonderful things about Ben, as I know many people in this room know. So I will just briefly relate my favorite story about Ben. It has no punchline to it. But, not long before he moved, unfortunately, from Penn to the University of Washington, Ben and I happened to just independently be in Cambridge, England at roughly the same time around this time of year when the weather is getting great, and it's nice to sit and have a pint down by the river. And so we were at one of the riverside pubs there. And just I remember, after he died, remembering that conversation and how quintessentially Ben it was because we were there just talking together for about an hour or so. And, if you asked me in hindsight, well, what did we talk about it, it was kind of like everything and nothing at the same time because he had such an easy manner about him, which is many people know, in a competitive field like academia, it's rare to find somebody who's so good at what they do and is so low key about it. And we just managed to drift from academic gossip to shop talk to things that had nothing to do with research whatsoever in just this sort of meandering way, almost like the river that we were next to. And I remember just feeling afterwards like it's so rare to have a colleague that you feel just so relaxed talking to about absolutely anything that you just sort of don't notice the time passing. And that's something I'll always value and miss greatly in Ben. And, sometimes, a tragedy is just a tragedy. But, sometimes, a tragedy leads someone to be inspired and to kind of take their difficult circumstances and to make the world a better place. And so I also want to kind of think Anat for everything she's done, both for the University of Washington, but also the world at large, in the aftermath of Ben's departure. So it's a great honor to be here speaking at a lecture named for him. So this is going to be a somewhat unusual lecture within a computer science department at least. So Aaron Roth, my good friend and colleague at the University of Pennsylvania, and I, we've written a book. And we've actually written what we intend-- we'll find out-- but what we intend to be a fairly general-audience book. So we view this as a book intended for like the educated lay reader who's broadly interested in things like technology, science, and the like, but with no particular technical background whatsoever. So, in particular, at a minimum, we're trying to reach an audience that would include things like legal scholars, philosophy, and policymakers who are interested in the intersection of technology and society, as well, of course, as more technical people, but also just people that read science books in general. This is what I learned recently, when I started going through this process, is what they call a trade book in the publishing industry, which I always thought would mean something like a manual of plumbing components with numbers and prices on them. But, no, this is meant to be a trade book. And so I'm going to give a completely non-technical talk today for the most part. Partly as a hedge against that, I do have a section at the end that, if I get to it, is meant to provide some perhaps more satisfying fodder for more quantitative people in the audience. But that's what I'm going to try to do in this talk. In some ways, our book is kind of an indirect response to a number of other books and events that have happened in recent years. So, first of all, to kind of set the broader context here, which I imagine most of you are familiar with, quickly on the heels of the excitement and outright media frenzy around the successes of machine learning and AI has come the backlash and the increasing realization that there are negative side effects or kind of collateral damage caused by the wide-scale deployment of a lot of the models and algorithms that we're used to using. And there have been some very nice books written on these topics. And I'm showing you just a sample of a few covers of some that I like here. And, when I say that our book is an indirect response to these books, what I mean by that is I think, in our view, these books do an excellent job of for the lay reader identifying some real problems in very human and visceral ways. And we are grateful for these books for that. What I think we find a little bit unsatisfying about them is, when you get to the part of the book where it says like, OK, so what do we do about these problems, in general, the answer-- and, again, I crudely summarize a large literature that is of variable quality as well. You know, the answer is like, well, we really just need to keep an eye on this stuff. We need better laws. We need better regulations. We need better policies. We need watchdog groups, et cetera. In other words, they're talking about kind of human solutions to technological problems. And let me just say at the front, I completely agree that we need all of that stuff. But, as a computer scientist, one of the questions that comes to mind first is like, well, what about making the algorithms better in the first place so that some of these problems are, if not eradicated, at least reduced to some extent. And since, of course, that's the research area that Aaron and I have been most involved in in the past few years, we kind of knew a lot about what was going on in the communities of people working on this. So, as many people in this room know, you know, the sort of FATE community-- Fairness, Accountability, Transparency, and Ethics-- is a large and growing subset of the machine-learning community, and not just the machine-learning community, but like all of the other communities that you should be interacting with in talking about solutions like this, including legal scholars, policymakers, social workers, government officials, and the like. And so, in some ways, our book is on the same topic as these books, but we're going to try to describe to the lay reader what we view as promising if preliminary, fledgling, algorithmic solutions to some of these social ills caused by modern technology. And I think, in a relative sense, it's an optimistic book. And I'm ready for one of the major criticism of this book, which we got from one reviewer already, that's like an extreme act of technological Utopianism, right? And it's not that we don't think that all these other things like laws, regulations are important or that we think that better algorithms will solve everything, but it is what we know. It is what we're good. We are not social workers. We are not criminal justice experts. And so we're writing about what we know. And we're optimistic about it. And we're kind of unapologetic about that. And so what I want to do is just kind of give you a very whirlwind tour of what we cover in the book and sort of what our thoughts about it are. Many of these topics will be familiar to people in this room. And so, if you're kind of bored by the actual content or, as I go through slides, you're thinking like yeah, yeah, yeah, I know all about this, I'm hoping that at least you might find a morsel of interest in the way we attempt to try to present it to a much more general audience. And, again, at the end, there is some more technical stuff for those that just can't stomach the high-level stuff. So the book is divided roughly into like a handful of chapters. One of them is on what we might call kind of algorithmic privacy. Another one is on algorithmic discrimination or fairness. And, in our view, those are the two sub-areas of this whole movement on which the most has been said. There are the most kind of well-understood models. There are the most well-understood algorithms. There's the most experimental and empirical work. But then in the later part of the book, which, in some ways, because it's a bit more open-ended and a little bit more nascent, is I think more interesting, we also talk about kind of situations-- I mean, in some ways, many of you will recognize this as the algorithmic game theory chapter. But this is of meant to talk about instances in which the social side effect that we don't like can't really just be blamed on the algorithm, but somehow the users of the algorithm are complicit as well. And then we go the further step to kind of turn that lens back on scientific research itself and sort of talk about the so-called reproducibility crisis in the sciences, also known as p-hacking, and about how, in some ways, it's fair to think about that as a game theoretic situation and what game theoretic solutions there might be to the social ills that are caused by things like p-hacking and false discovery. We have a final chapter in which we lump a lot of other things that get a lot of discussion on which we don't have as much to say. And we think there is not as much that has been said yet. So, on things like interpretability and transparency, we think that there is sort of, at least from an algorithmic technical perspective, most of the work remains ahead of us and might, in some sense, be different than these earlier topics. Because I think, for instance, if you talk about things like interpretability, immediately you have to think about, well, who is the observer here. Interpretable to whom? And then you have to get to questions of like what is the numeracy of the population that you're trying to explain things to. And then, of course, kind of going further and further into the realm of abstraction and generality, we have things like morality and, of course, every AI alarmist's favorite topic, the singularity. And so we kind of talk a little bit about all those in a final chapter. And, if I get to it, maybe I'll say a little bit. OK, just again to take a little bit of time upfront to of say what kind of underlying not topics, but kind of attitude of the book is, one basic point we try to explain to the reader is that things like privacy, fairness, and the like, it's not like discrimination and decision-making was invented with machine learning, right? That was present in human decision-making for time immemorial. And it's not like violations of privacy were invented with the internet, OK? It might have gotten worse, but these problems have been around for a long time. And people and other communities have thought about these topics for a very long time. But, if you're really going to come up with a definition of fairness or privacy that is so precise that you can explain it to an algorithm-- you can actually put it in the code of an algorithm-- it's a level of precision that's never been asked for before. And, in some ways, that's the hardest part of this work is coming up with the right definition. And, oftentimes, when you're really precise about what it is you want, you realize the flaws in your thinking when you didn't force yourself to be so precise. And I think I'll give a couple of examples of that here, but that's sort of one of the underlying kind of sub-currents of the book. Another underlying sub-current of the book that I think all of the ML people in the audience will immediately agree with is that people talk these days-- incredibly, you can pick up The Wall Street Journal and see discussions about algorithmic opacity and interpretability and things like this. And even people that should know better often talk about this in kind of this very convoluted way that doesn't acknowledge like, well, your typical machine learning pipeline has data that gets fed to an algorithm, like backpropagation for neural networks, that then outputs a model that then makes decisions, OK? And, in some sense, the algorithms that we use to train models-- you know, nothing could be simpler and more sensible, right? So this is literally the Wikipedia pseudocode for the backpropagation algorithm. And it's in the book because we think, with a little bit of just annotation, you can explain to the reader that all this is doing is going through each data point and saying like, well, compute the error on this data point, and then reduce the error-- you know, move the knobs of your model around to reduce the error a little bit. And so not only is this incredibly simple and opaque at some level, it has a very clear objective function. You know, it's not these sorts of things that are the problem, right? It's the interaction between these things and their inputs and their outputs. It's the development of very complex models. And we say the perhaps trite-to-this-audience thing early in the book, which is, in many ways, a lot of the side effects that we don't like are a result of the fact that, in machine learning, you should never expect a model to give you something for free that you didn't explicitly ask for. And you should never expect it to do something that you didn't want it to do that you didn't explicitly tell it not to do. And, as we move to richer and richer model spaces, which seems to be the trend these days, we create model spaces that have these very sharp corners that might minimize this objective function at the expense of other things that we care about like fairness or privacy. OK, so to kind of go through a little bit about what we say at a very high level in each of these chapters, we have a chapter on algorithmic privacy. And, to give examples, there's this famous quote now-- or at least famous in the small circles I run around in-- from Cynthia Dwork, which is, "Anonymized data isn't." OK, and this is an example of where, if you really think precisely about what you should mean or want from a general definition of privacy, sort of, suddenly, the scales fall from your eyes. And you realize all of these definitions, including almost every corporate privacy policy that I've ever looked or scanned before, are all based on some notion of anonymity, right, some idea of we're going to remove your personally identifiable information, or we're going to coarse in certain fields or redact certain other fields. And, of course, at a technical level, it's easy to kind of point out the flaws in this. Like so one flaw is kind of re-identification or linkage, as it's sometimes called, or triangulation where here we have two toy medical databases. And you have a neighbor-- let's say Rebecca-- that you happen to know is 55 years old. And you know this because-- you know she's a 55-year-old woman because she's your neighbor. And you know her name, but these clever hospitals have decided to redact name. They've decided to bin ages into decades. And they've redacted parts of zip codes in order to provide some kind of anonymity, right? And there are technical definitions of anonymity, which sort of say things like, well, a database is k-anonymous if, for any row that is in the database at all, there are k other identical rows, right? So this top database is 2-anonymous by this definition. There are two matching rows for your neighbor, Rebecca. The bottom one is 3-anonymous. And you can immediately see what the problem is, which is that if I take-- all I need to do is to look at this database and know that my neighbor is 55 and female to know that like, well, in the top database, it must be one of the two red records that correspond to her. And, in the bottom database, it's one of these three. And now, by combining those two, whereas before I had some confusion about which difficult medical condition she had, now I unambiguously know that she has HIV. So this is a technical problem with this definition. The real conceptual problem with this definition, of which the technical version is kind of an artifact or a shadow, is that these definitions of anonymity all pretend like the data set in front of you is the only data set in the world and is the only data set that will ever exist in the world in the future. And, unfortunately, maybe there was a previous era where that was a close enough approximation to the truth that you didn't have these concerns, but virtually every sort of privacy breach that we've seen in recent years versus security breaches, on which I'll say something separate in a second, is kind of of this kind of re-identification, triangulation form. So this is an example where being precise about what you want from your definition helps expose flaws in previous definitions. Another solution concept for privacy that we argue in the book, well, a very valuable tool-- it is not a solution for the type of privacy we're asking-- is cryptography in the form of sort of standard cryptography or its generalizations like secure multiparty function computation. And the reason we basically view cryptography as generally the right tool for when you want to lock data down and prevent access to it. But it's not the right tool in situations where you want to use the data, and you want to release something to the world based on the data, whether it's in the form of literally publishing the coefficients of a trained model or whether it's the use of that model in the world, which then, of course, one sees observable decisions or consequences or predictions made by the model. And, if you think about it for a little bit, and you know a bit about cryptography and these fields, you'll realize that cryptography will promise you something like, well, if I run backpropagation in a secure multiparty computation way, then my promise is that I won't learn anything more about the original data other than what is implied by the neural network that was output. But what does that already imply, right? And, as many of you know, it's easy, for instance, to figure out whether an individual data record was used in the training of a neural network or not just by the confidence level that the network exhibits on that data point. So these are really solving orthogonal problems. And I think, you know, I'm not here to minimize the importance of cryptography. It's obviously an incredibly valuable tool. And, even though, in relative terms, it's a mathematically solved problem, nevertheless, every six months, there's a major breach due to the implementation details. But here we're talking about a different type of privacy, which is preventing unwanted inferences from the use or publication of trained models. OK, so this is all complaining about exigent definitions or notions of privacy. But so what's the right solution then? And what, at an abstract level, is it that we really want from a definition of privacy? And so many of you know that I'm leading up to differential privacy, but an important I think step in the development of differential privacy was just kind of delimiting what it was reasonable to ask for and what it wasn't reasonable to ask for at a conceptual level before one committed to a technical definition. And so one thought experiment that's often offered when sort of motivating differential privacy goes along the lines of, you know, suppose it's 1950. And you're a smoker. And everybody is a smoker in 1950, right? If this was 1950, in this lecture hall, all of us would be lit up, including me. And I'm not joking because my parents reported that, when they were in school at Berkeley in the late 1950s, like the faculty member would be-- everybody would be smoking. This room would be a cloud of smoke because, you know, there was no stigma associated with it. There were no known negative health associations with it. It was seen as kind of glamorous, OK? So suppose that was you in 1950. And you were contemplating whether to allow your medical record, which included the fact that you were a smoker, to be used in the series of studies that in the 1950s established a strong link between smoking and lung cancer, OK? And so those studies are done. Your data was included in them. And we can argue that, as a result of those studies, real harm befell you, OK? So now the world knows that smoking and lung cancer are related. The world also knows that you're a smoker. And, in particular, your insurance company might now know about both of these facts and raise your premiums. So there's actual been real economic harm caused to you by this study, OK? So, if we ask for a definition of privacy that prevents such harm, we couldn't have done this study. But, if you instead ask the counterfactual question like, well, was it really your data that was crucial to the establishment of this connection between smoking and lung cancer, the answer is no, right? So we're going to distinguish between harms that befell you because of the inclusion of your specific data in the study and harms that befell you that would have befallen you anyway. OK, and we're going to protect you from the first kind of harm and not protect you from the second kind of harm. And, at a high level, the reason this makes a lot of sense is that the connection between smoking and lung cancer is a fact about the world. It's not a fact about you. It's a fact about the world that perhaps your data incrementally contributed to the discovery of, but this fact was going to be discovered whether your particular data set was included in this study or not. And so this is exactly kind of the scope of differential privacy. And here's a cartoon of just what I said. Here we have a bunch of data sets. And here's one particular user. And we're looking at the counterfactual of like what would the output of the algorithm been with this particular data set included. And what would it have been if everybody else's data was included, but your data was not included? And we say that an algorithm is differentially private as long as those two, the outputs under those two situations, are sort of controllably close. To get into the weeds, these outputs here are actually probability distributions because differential privacy fundamentally requires randomized algorithms. The way you get differential privacy is by adding noise to computations. The first example, to my knowledge or I think the community's knowledge, of a differentially private algorithm actually predates the definition of differential privacy by several decades. It's from the social psychology literature. And it's called randomized response. And probably many of you have heard about it. Suppose you want to do a survey. You want to do a survey about an embarrassing question or at least where one of the answers has a stigma associated with it. So suppose I wanted to serve a Penn undergraduate with the question have you ever cheated on an exam at Penn, OK? So I could do that in the most straightforward way and say like, OK, you know, I've created this website. You won't have to put your name in it all. I'm not going to track your IP address. Just go to this website and answer yes or no. Have you cheated on an exam at Penn? No matter what promises I make, there's going to be a massive under-reporting of cheating because people just feel worried about writing down, yes, I've cheated, even if I make anonymity promises or that I'm not tracking them, OK? So randomized response is this of very clever randomized mechanism that tries to provide some cover for you if you cheated. So I'll just say in English what this diagram is saying. It basically says, look, flip a coin. If the coin comes up heads, then answer truthfully. So, if the coin comes up heads, and you've cheated, say you've cheated. If the coin comes up heads, and you haven't cheated, say you haven't cheated. If the coin comes up tails, then flip another coin, and just give an answer according to the flip of the coin. Heads means you say yes. Tails you say no, OK? So, if you think about this, the genius of this, even without the definition of differential privacy, is that everybody has plausible denial-- everybody that answers yes to this survey has plausible deniability. You know, even if you come to me and say like I lied to you-- we tracked your IP address, and we know that you answered yes-- I can say, well, of course I answered yes. I flipped the coin. It came up tails. So I didn't answer truthfully because I haven't cheated. And then, when I flipped the coin a second time, it told me to say I cheated, OK? So now everybody is incentivized to follow this protocol truthfully. And, of course, the brilliance is that, with a large enough user population, I can still back out an extremely accurate estimate of the true fraction of cheaters, right? It will be plus or minus 1 over square root of the population size. And it's not unfair at least to a lay audience to say that differential privacy makes two major contributions. One is definitional, which is to formalize this idea in a much broader computational setting. So you don't have to just talk about like answering survey questions, but you can sensibly ask questions like can I develop a version of backpropagation that promises differential privacy. And the subsequent development of a very rich algorithmic tool kit, sort of allowing us to build richer and richer and more complicated algorithms meeting differential privacy-- and I think it's not unfair to say that we know enough now to say that pretty much everything that we do in machine learning or statistics has a variant that is differentially private. OK, there's some overhead to it. There's some trade-off between accuracy and the amount of privacy you get. But, in principle, the road, the technical road, to doing almost everything we do in machine learning with privacy guarantees of this kind is clear. This is not to say that it's solved, but like we know how to proceed from here. OK, differential privacy is an exciting example of something that kind of started in the lab, really not just in the lab, but on the whiteboard from the theory community. And it's a great success story from an algorithmic, scientific standpoint and more and more from a practical standpoint. So differential privacy is really getting out into the world now. There's so much excitement about it that it's causing usually tasteful companies like Apple to do things like this, which is to take out-- paint the side of a hotel at the Consumer Electronics Show in Las Vegas kind of advertising. But it is true. The iPhone and many other Apple devices implement differential privacy in many of the statistics they report to the mother ship about your usage of apps on your iPhone in a way that, if we looked at the thing that your phone sent to Apple, we wouldn't really learn anything about your particular app usage, but, if we aggregate everybody's histograms, we learned very detailed information, OK? So this is a great success story. Even more ambitiously, as many of you may know, the US census has decided-- the 2020 US census has decided that every single statistic that is published and released as a result of the 2020 census will be released under the constraint of differential privacy. So this is an extremely ambitious undertaking. There's many, many fundamental decisions that it's not clear how they'll make. And, since it is the government, you hope that they're going to make them the right way. But, you know. And there's many things that, as I'm sure many of you that know a little bit about differential privacy know, that I'm sweeping under the rug like how do you set the privacy parameter. And how do you set the privacy parameter in light of the fact that every little differentially private computation leaks a little bit of your privacy? And so you should have some lifetime budget of privacy that you think about. And so how do you decide how to set that knob right now, today, on this computation? But I think, again, these are important technical problems, but it's a great advance to have a framework that lets us actually talk about them technically, which I think was absent say, you know, 10 or 15 years ago. OK, so now let me go on to algorithmic fairness. AUDIENCE: Can you talk about the connection here? MICHAEL KEARNS: So one thing we know about the study of algorithmic fairness, compared to algorithmic privacy, is that it's unfortunately going to be messier, OK? So I claim-- you know, this is not a mathematical statement-- but I claim that, if you think hard and long enough about privacy and sort of what it's reasonable to hope for, that you would arrive at a definition like differential privacy eventually. And, you know, there might be debate about the exact details, but you'd arrive at something like it. We know this is not going to be the case in fairness. So, for instance, in just the last few years, there have been a couple of papers that have the following form. They basically say like, well, here are three properties that it seems like you would want any definition of algorithmic fairness to obey, right? And you, the reader, look at those and you say, yes, yes, these are such weak conditions. I would definitely want at least these three conditions and a lot more stuff too. And then, of course, the point of the paper is like, you know, theorem, you cannot simultaneously satisfy-- so these are very much like Arrow's social choice axiom theorems from the '50s or so. So we already know that there's not going to be a monolithic right definition of algorithmic fairness, the way perhaps there is for differential privacy. And that's just life. But, perhaps, I get ahead of myself a bit here. Let me just talk a little bit about one definition-- one type of definition that's quite common, the most common type of definition of fairness. So let's imagine an application where you were-- so just to kind of pop up a level, the vast majority of definitions of fairness for algorithmic decision-making or model-based decision-making kind of require the user in advance to specify a couple of things-- who you're worried about being harmed and what harm constitutes. OK, so let's take an example. Suppose that we have applicants to a college, OK? And we're going to try to build a model, a predictive-- a model to decide who to admit to the college based strictly on SAT score. OK, so I'm just doing this to make things one-dimensional. The situation I'm going to describe to you will only get worse in higher dimensions. OK, and there's two groups. OK, there's the squares, and there's the circles. We had to render all these images in black and white for the book, unfortunately. Otherwise, I would have used colors. So there's the circles and the squares. So these might be two different races, for example, or two different demographic groups. And let's pick a definition of success in college, OK? So maybe success in college means you graduate within five years of matriculating with at least a 3.0 GPA. Or maybe it means that, within 20 years of graduation, you donate a building back to your alma mater. You pick whatever objective function you want. And let's suppose we have a historical data set of individuals from the square population with their SAT score located along the one-dimensional axis here. And then the pluses and minuses indicate whether that historical individual did succeed or not according to our definition of collegiate success. And so we have two different populations here. And I want you to just notice that there's more circles than squares. Like the squares are the minority population, but there's not really a difference in the rate of success between the squares and the circles, right? There's essentially the same fraction of plus and minus squares as there are plus and minus circles. But, somehow, the SAT scores of the circles are systematically higher. And maybe that's because, for instance, the circles come from a wealthier demographic that can afford to pay for SAT tutoring classes, Photoshopped sailing portfolios, multiple retakes of the exams. And, you know, they can just afford all this stuff. And the other population, they can't do that. They just take the exam once with self-study, OK? And so it's not that the circles are more qualified, but, on this particular thing that we're measuring, there's a systematic shift, OK? So, if you just do the standard machine-learning thing here and pick the model, which, in this case, like the only sensible thing to do is pick an SAT score threshold and say, if your score is above that threshold, then you get in, otherwise, you don't. If you looked at this data long enough, this would be the optimal model, but it would be unfair by the measure of false rejection rate. So, if we think of this application as the harm that could be inflicted on an individual as you would have succeeded, but we rejected you, versus false positives, which is we shouldn't have admitted you, but we did, so you got lucky-- so maybe what we're really concerned about is equalizing the false rejection rate between the two populations. So this is the error optimal model, but it's unfair in the sense that it falsely rejects one, two, three, four, five squares and falsely rejects only one circle here, OK? So, on the other hand, we can say like, well, we want a fair model, OK? And so a more fair model would be this one here, which sort of fixes two of those false rejections of the squares. But it's going to make more mistakes too, right, because now it's accepted these three circles, OK? So, first of all, one very basic point here is like, once you introduce and formalize this additional fairness criterion, you have trade-offs. You face trade-offs. If you ask for the error optimal model, it will not be the most fair model. It might very well be the least fair model. If I ask for a more fair model, it will have higher error. And that's just life, OK? And we have to get used to thinking in those terms. Now you might also argue with me and say like, well, look, in this particular example you've given me, I should have just noticed in the data in front of me that there was this systematic shift upwards in the SAT score of the circles without sort of a commensurate improvement in their collegiate preparedness or success. And so what I really should do is build two models, right? I should have a cutoff for the circles, which is up here, and a cutoff or the squares, which is down lower, OK? And, if you work through the numbers in this toy example, you will find that, actually, not only is that model more fair, but it also gives you lower error. Well, not surprisingly, it's a more complicated model, OK? So this might seem like a good idea, but, unfortunately, this would run afoul of laws and regulations in many domains and applications. So, for instance, for a while now, it has been forbidden by US law to use race as an input variable in a credit-scoring or consumer-lending model. And this might have made sense back in say the '70s or the '60s where HNC Corporation, which is the neural network startup that is the predecessor of what's now called Fair Isaac today, which, basically, is one of the few huge credit-scoring companies, you know, back when they were building models, the only data they might have on you is what you submitted on your application. OK, so it's pretty easy to sort of say like, well, let's forbid the use of race, OK? Now so I'm sort of making two points at once here. There's a number of reasons why this sort of forbidding certain inputs into a model is a flawed notion of fairness in the same broad sense that forbid-- sort of using anonymity is a bad definition of privacy. It's well-intentioned, right? It's like somehow, by preventing you from using race as an input, I'm preventing you from building a racially-biased model. So this is a very clean example where, in fact, by forbidding the use of race as an input, you actually cause me to harm the very group that you are trying to protect because, just to be clear, this model down here uses race as an input, right, because I first look at whether you're a circle or a square as this first step in my decision. And then I decide which of these cutoffs to use. So that's using the race variable as an input. The other reason why this is a really flawed definition of fairness, the sort of forbidden inputs, is, these days, we have so much data on people that there's so many proxies for things like race. OK, so in this, depending on your world view, either completely trivial, unsurprising or rather alarming paper from a few years ago that, by the way, is written by-- these are all good people, even though the data set and effort here are precursors in a very direct way of the whole Cambridge Analytica scandal. In this paper, they took Facebook data married with this personality survey data from Facebook and showed that, just from your like behavior on Facebook, not anything that you've put about your demographics, not who your friends are, nothing about what you post, just which content you clicked the thumbs up button on, that, in the aggregate, they can predict from that data a number of surprising variables, certainly your race with something like 95% accuracy, at least between Caucasian and African-American, but, you know, whether you're single or in a relationship, your drug and alcohol use, your sexual orientation, whether you are the child of divorced parents, et cetera. So now that we're in this era where there's so many different sources of data that can be brought to bear on any problem, like consumer lending or college admissions, we're fooling ourselves if we say like don't use race as an input. So like the last slide showed that we can shoot ourselves in the foot by that. But, even if we're not shooting ourselves in the foot, we're fooling ourselves if we sort of create laws and regulations like that. OK, so where does all of this lead? Well, I mean, everything in the book is very much a work in progress. But I think, as computer scientists, one thing we need to think about in areas like privacy and fairness is what is a realistic, useful, interface between what we do as scientists and the people that will have to kind of take that work forward and implement it in the real world, right? And so one thing that's important-- I think one important type of interface is just tracing out these of so-called efficient or Pareto frontiers. So almost all kind of Eggleston definitions of statistical or algorithmic fairness have the flavors of this equality of false positive or false negative rate that it gave on the last slide. And those definitions all have a knob, right? I could basically say, look, the false rejection rate between squares and circles has to be 0.0, OK? Or I could relax that. I could say, well, they have to be close. They have to be within one 1% or 5% or 25%. And, of course, if I asked them to be within 100% of each other, then it's like I didn't ask for anything at all. So this is very nice because it gives you a knob, which lets me sort of tune the relative trade-off between accuracy and fairness. OK, and so what I'm showing you here are the actual sort of Pareto-- the sort of undominated frontiers that you get from three real data sets where on the x-axis is the predictive error and on the y-axis is the violation of fairness under a definition of the flavor of the type that I just described. And there's a few things to say about this, right? So, first of all, there are real trade-offs here. So, at the upper left here, you have the error optimal model, but you have the worst fairness. Over here, you have the fairness optimal model and the worst error. And, in between, you get something in between. And I'm not just sort of proposing these as like pretty pictures to understand the concept. In real applications, like one should be looking at these pictures and looking at the numbers on the different axes, which you can't see, and really asking questions like, OK, if it's gender bias in Google advertising, how much fairness are we willing to sacrifice for greater personalization and targeting. And the answer we might give there would hopefully be quite different than the answer to the question like, OK, in a criminal sentencing application, what's our relative tolerance for mistakes versus fairness because remember mistakes there might mean not like, oh, you saw an irrelevant ad or somebody else didn't see a relevant ad. Now, you know, changes in error mean like incarcerating innocent people or letting guilty people go. And so like you need to start with these curves, but then take the stakes into account. But I think, in the near term, this is the type of thing that non-quantitative people could be brought to understand and think that was important at least as some guide to their thought about how they use algorithmic decision-making and statistical models. OK, so this is kind of the first two chapters of the book. And, as I mentioned, these are the areas where there's the most to say scientifically. And so those chapters, hopefully, people will find them engaging, but there's they're of real science to describe there. I mean, by real science, I mean like there are frameworks that are populated with results, both theoretical and empirical, on which there's interesting things to say. In the middle of the book, we kind of take a bit of a left turn, but I think it's an important left turn. So, if you think about the way at least I've been talking about privacy and fairness so far, you know, the harms that we're talking about there might be inflicted on somebody without them even knowing it, right? So you might submit an application for a loan and be denied the loan and not even realize that, well, what really happened was other people's data was taken and used to make a decision about you. And that's why you got your loan rejected. Or you might not know that your own data was used to train such a model, OK? So, in other words, we think in these cases of civilians or consumers as the victims of the harms inflicted by these models. There are a lot of other modern settings where you can't quite place the blame so firmly on the algorithm. It's like the users of the algorithm are complicit in whatever sort of social ill that we're worried about, OK? And, to preview a little bit, there are many, many services and apps we use today that are helping us do things that we would have wanted to do back when I was a kid, for instance, and to optimize it in a way that it was just not possible to optimize when I was a kid. And, from a game theoretic perspective, what these apps are really doing is helping us all compute our best response in some strategic setting and, in some ways, driving us towards a competitive equilibrium, OK? But we can't of blame the apps themselves alone because they're doing the most natural thing. They're sort of optimizing according to our preferences. But we could still ask the question like are we actually better off by being led to the competitive equilibrium that this optimization is helping us all do or not. OK, and so one classic example within the algorithmic game theory community is on kind of what there is called selfish routing and here I'm calling the commuting game, OK? So think about apps like Waze and Google Maps. What could be better? Like, again, think back to when I was a kid. It's not like, when I was a kid, my parents when we were driving didn't want to get from A to B as quickly as possible. And everybody realized it wasn't about like the actual arc length of the roads that you drove on. Much more important was how much traffic was going to be on them, OK? So you had maps to kind of help you with the arc length problem. And you had like half-hourly radio reports of the traffic on the major freeways that might be stale by the time you heard it, and it might not be where you were driving anyway. Now we have these apps that collect all of our GPS data and, at any given moment, can tell you like, oh, given what everybody else is doing right now, this is your fastest route, OK? Now this is literally a game in the technical game theory sense. So let me just walk us through that for a little bit. There are players. It's all the people on the roadways. Every player has a utility or a payoff function. They want to go from some-- their own point A to their own point B and minimize their driving time. It's a game versus just an optimization problem because, of course, your fastest route doesn't just depend on what route you decide to take, but on what everybody else is doing because that's what creates traffic, right? And so it really is a game in the little game theoretic sense. And so we know that it has a Nash equilibrium, OK? But, you know, back in the '60s, you could have made this observation, and people did, but there was sort of nothing to do about it as a user. But now we have these apps, OK? And I won't go through this slide, but it's easy to come up with like toy examples-- and there are real world cases as well-- where, in fact, we're all collectively worse off because of this optimization than we would be under some other solution. Now the problem with these other solutions is always like, well, but it really is the case that, given what you're all doing, that Waze is recommending my optimal response. So why wouldn't I want to do that? Why would I want to adopt something that kind of lifted the overall social welfare at the expense of like being greedy for myself right now, OK? So there's like an incentives problem. You can't just think about this from the purely algorithmic perspective. You have to think about incentives. I mean, I think there's a couple of interesting answers to this in the case of driving in particular and perhaps more generally. One is that-- so, first off, there's no problem right now for Waze or Google Maps, who already have all the data and the GPS sensors out on the roads, to decide instead to change their algorithm and say like, instead of now recommending routes to all of you that are selfish and greedy and lead us to the competitive equilibrium, we're going to compute the maximum social welfare solution. We're going to try to minimize the average driving time, rather than minimize the myopic driving time for each of you separately, which leads to a potentially worse solution. So this could be implemented already. The problem would be like, if Google Maps did this, and Waze didn't, you might look at what Google Maps recommended and then see that Waze is recommending something else and say like, oh, well, I'm not going to do that. So you have this defection problem. But, you know, if an era of self-driving cars is not that far off, perhaps these cars will be self-navigating as well. So you'll just get in the car and say where you want to go. And not only will it drive you there, it'll decide the route. So you could then implement a centralized kind of max social welfare algorithm. And, if this seems like a radical idea to you, I would point out to you we're already used to this, for instance, in things like air travel, right? If I want to fly from here to, you know, Sicily in Italy, I can't just sort of say to American Airlines, OK, I want to fly from right here to right there using like the great circle, which would be best for me. For the greater good and for many other profitability and efficiency reasons, we're already used to of having-- we say we want to go here, and we're kind of told the route we have to go. So self-driving cars may offer an opportunity to create great efficiencies in sort of collective driving and commuting time as well. Now, to kind of riff on this topic for a while, you can think there's many other domains in which you used to have very poor tools to self-optimize that now you have great tools. So product recommendation is another one. So, again, back circa in the 1960s, if you wanted to pick the best product in some category or just find stuff that you were interested in that you didn't know about, you had very few tools available at your disposal. For like a big purchase like a car or a refrigerator, there might be Consumer Reports. But sort of this idea of optimized discovery of stuff that matches your taste, you didn't have such tools. Now you have things like recommendation engines at Amazon that know that, if you like Gravity's Rainbow, maybe you would like Infinite Jest also, for example. And this is, again, at some level a game, right? All of our collective data is being used to build a predictive model that then optimizes for the particular individual, much like Waze and Google Maps do for driving. Now you might legitimately say like, well, OK, I see the traffic example. I see how every one of us inflicts what an economist would call a negative externality on everybody else every time we get on the road in the form of an incremental unit of traffic. But like what is the negative externality that you inflict on me when an engine optimizes your shopping behavior for you, OK? It's not as clear. So maybe there are some situations-- and maybe product recommendation is one of them-- where these apps that are helping us compute our best response in some very complicated, collective interaction-- it is a collective interaction because for sure these recommendations are using everybody's data to decide like, well, I'm like this other person. And this other person liked this thing. So they should recommend that to me. But maybe here there's sort of no negative externalities, and we're perfectly happy for this sort of optimization to happen. But, if you think that, I offer you a very, very similar situation in which choices are being recommended and optimized and personalized for us that feels the same, yet society has come to a very different conclusion about. So, for instance, here's a snapshot from my Facebook page this morning. And, as a selfish, myopic user, like what could make me happier than to see my friends Charles, Isabel, and Ed Lazowska out on the terrace of the place that I'm going to be this morning? And, you know, yes, I like the Philadelphia Orchestra, and I might want to go see Mahler's 9th. So what could be better? It's the same kind of thing as Amazon product recommendation, yet it feels like, as a society, we've become much more concerned with this optimization and recommendation on Facebook than we have about Amazon. And one of it is that we've seen-- we believe we've seen highly negative collective consequences of this optimization. At any given moment, yes, I'd rather see the stuff that I want to see, but maybe this has come at the cost of, for instance, like intelligent discourse in society and polarization, right? And, again, in the spirit of our book, we try, whenever we bring these problems up, to say a little bit about kind of algorithmic solutions to them. And I think there are very simple ideas here that any of the machine-learning people in the crowd have already thought of. These same models that are sort of placing us in some metric space and saying how similar we are or how much we'd like different products, these are all models with precise-- not only do they know who's like you, they also, because of the same reason, know who's quite unlike you. So, if Facebook wanted to or Amazon wanted to, they could start deliberately injecting diversity into your news feed or not. And they could do it in a lot of different ways. They could give users a knob and say like, you know, I don't want to see any crap that I'm not interested in, or I don't want to hear any opposing viewpoints on any topic. Or you could let me experiment with that dial, OK? And I really think-- this is probably a system's overstatement. But, from the conceptual standpoint, this would be only a minor variation on what they're already doing algorithmically under the hood. Like all of the information they need to inject diversity into your product recommendations or your news feed, they already have because that's how they do what they're doing now, OK? So I'm almost out of time, but I'll take a few minutes to talk about this kind of last chapter, other than the chapter on the singularity and stuff, which our code name for many months was the crazy chapter. So we had like crazy.docx was the document for that chapter. But, if you think about what I've talked about so far, which is this kind of very modern, free-form hybrid of huge, diverse data sets with machine learning applied to it and then, furthermore, sometimes, there being the users themselves causing a feedback loop with the algorithm, which is modifying its behavior on the data and response to it, we kind of argue in the last chapter that this is really, in many ways, the cause of the reproducibility crisis in the sciences. Now I'm not sure how many of you have heard of it referred to by that expression. But does anybody know who this woman is? So this is Amy Cuddy who first became very famous-- her TEDx video-- one of her TEDx video has over 50 million views-- and then subsequently became very infamous. She was the researcher who basically discovered the sort of whole power poses phenomenon that, if every day you spent just a few minutes in sort of assertive poses, like the one she's putting here, or sat in certain ways that kind of were kind of empowering, that, not only would it just make you kind of feel better, but it would actually induce measurable hormonal and chemical changes in your physiology that were good for productivity and social interactions and the like. And what later came out-- and Aaron and I always debate whether we should include pictures like this because, you know, on the one hand, it's a very compelling case study. But, on the other hand, you feel like, oh my god, you know, she's been beaten up on so much already. But what was discovered is that the sort of form of the research was like let's get a data set and look for a bunch of things, right? Let's start doing what is called adaptive data analysis. You sort of say like, well, let's first look for this thing. And, oh, that didn't quite work out. So let's modify it a little bit, and now it looks a little more interesting. And, actually, maybe just for fairness on the next time I give this talk, I'll put up a slide of the Cornell nutrition science professor. In some ways, nutrition science is even worse than sort of social psychology in this regard. And we all see it in the form of these click-bait articles like, you know, acai berries will lengthen your life by 10 years. Or eat more chocolate, or eat less chocolate. Or drink more red wine, or drink less red wine. And so there's a number of notable instances there in which like the researchers like are on the record just like go to the buffet, and observe people's behaviors. And like write down like all of the variables you can think of. And then start doing, essentially, kind of data dredging, looking for correlations in the data set. And so this is sort of overfitting on a grand scale, but now there's like a social component to it. And lest we beat up on the scientific community-- like the two main sources of this problem are really scale and adaptivity. It's that we can now kind of try a huge number of hypotheses or data analyses and then condition what we do next on the outcome of that huge number of experiments. And, to give other examples where this is common, like so hedge funds in the United States are lightly regulated. And they're not required to report their returns unless they want to. And there are publicly available databases like this Thomson Reuters Lipper database that like will tell you what the returns of hedge funds that choose to report their returns are, OK? And so, again, you have a huge number of independent parties. And like it's hard to say that any one of them is doing something wrong, given the regulations they face, but, of course, they report when they have good returns. And they don't report when they have bad returns. And so you have many, many experiments going on. And you only hear about the winners. And so you have this inflated sense of the profitability of hedge funds. And one of the things we argue in this later chapter is that, actually, the poster child for this problem has now become the machine-learning community itself, OK? So many of you know that it's not unfair to say that machine learning, at least the empirical side of it, has become a competitive sport in very many literal ways, right? So you have these benchmark data sets. And a lot of focus is paid-- a lot of attention is paid to performing well on those data sets. The scientific publication process is such that you don't hear about negative results. So you have this small number of benchmarks, huge numbers of research groups trying huge numbers of models, and then you only hear about the winners. And then you have literal computation competitions like Kaggle, which is now kind of almost exclusively a machine-learning kind of leaderboard site. And we shouldn't be surprised if this leads to like a lot of false discovery and p-hacking and overfitting. And I know I'm running out of time. One interesting connection to an earlier chapter is that like one extreme approach to this is what's called the pre-registration movement where, before you gather any data at all, you say exactly what you're going to try and what your hypothesis is. You then gather that data. You do exactly the test that you said you were going to do. You report success or failure. And you burn that data set forever, OK? So this has very strong guarantees, but is extremely conservative and is basically like we won't make progress in the same rate, even real progress-- I'm not talking about p-hack progress-- if we follow this route. But one very interesting development in just the last few years is that it turns out that, for reasons closely related to differential privacy, adding noise to your data set and then doing as many analysis as adaptively as you want on that data set provides very strong guarantees against false discovery because the idea is that like, at a high level, the same guarantee that you get from differential privacy, which is somehow I shouldn't be able to figure out from the output anything about any individual's data, is sort of exactly kind of preventing overfitting. In some ways, the definition of overfitting is coming up with findings that are too specific to the data of specific individuals in the data set. OK, I'm out of time. We have a chapter on crazy things like algorithmic morality and like whether the self-driving car should drive and kill the driver and things like that and also the singularity. But let me stop there and take any questions. [APPLAUSE] Yeah? AUDIENCE: Thank you for your talk. I was wondering what you think, just sort of riffing on the traffic example, of how an ethical algorithm should deal with the things that are outside its domain entirely. So, in particular, I'm thinking about something like cut-through traffic, which impacts not the people in the cars-- so nobody in the game is necessarily impacted by it-- but the people in the neighborhood that the cars are going through. And I'm wondering sort of what model would allow you to give some kind of treatment to those externalities in a way that would support fairness. MICHAEL KEARNS: Yeah, so I'll repeat the question for the video, which is, in the example of Waze, there's not just negative externalities amongst the players in the game who are the drivers. But, if Waze is kind of routing a lot of traffic through your residential neighborhood, then you're suffering an externality, even though you're not a participant in this game. I mean, I don't think I have a great technical answer to that, other than that, if you can articulate it, you can of course design your sort of social welfare objective to not just include-- I mean, and this is a common thing in game theory, right? You sort of define some game. There are people that are players in the game. Then there's the outside world, which might influence the game. But sort of max social welfare is amongst the players in the game. And so the usual response would be like, well, you need to include those parties in the game. You might already be aware of this, but this sort of cut-through traffic externality is a real thing in that there was even a Wall Street Journal article a few years back where, on the 405 in Southern California, it was becoming so congested that Waze decided that it was a good idea to start routing people through entirely residential neighborhoods that were parallel to 405 to the point where the residents of that neighborhood, when they wanted to go out for work, they were facing like a traffic jam on their residential block. And, of course, in like a great game theoretic example, some of them started deciding that they would leave their phone on in the night saying that they were driving so that Waze would already think that there was a traffic jam there from these stationary GPS readings and then say like, OK, well, we shouldn't send them there in which case Waze probably just sent them to the next neighborhood over or something like that. So there you have like players who are not in the game who are able to actually like influence the algorithm. But, yeah. Yes? AUDIENCE: So in the-- there's something you just talked about at the very end about statistical validity and adaptive data analysis. In stuff I've seen, it works kind of like differential privacy where you have some budget. And, every time you ask a query, you sort of use up a little bit of that budget to try to learn something with the idea being that, once you use up this sort of budget, you're done, right? And you can't use this data anymore because you've sort of used up all your privacy I guess. But my problem with that is that people are just going to like reboot the system and try over again, right? Like what is the incentive? Or have you thought at all about how to enforce this? MICHAEL KEARNS: Yeah, so this whole issue of the privacy budget or p-hacking budget is a real issue I think with both of these topics. But there are special cases where you can kind of do better. So one special case is-- this is not the sort of most powerful theoretical use of noise addition to prevent over-- to prevent false discovery, but one thing you can do is take your original data set and create a synthetic data set, right? And so you basically add noise to the original data set. And then you release that synthetic data set. So, rather than sort of having this interaction between an interface between you and the data, I just give you a data set that's noised-up, right? And now I can let you do anything you want to for as long as you want. Like the budget was like already set when I created that data set, OK? And I sort of get like at least a mathematical guarantee that says the only things you can discover on that data set are things that are not the result of overfitting and same with differential privacy. Like, sometimes, you can come up with solutions where, instead of promising differential privacy in this incremental way that then you have to keep track of the privacy budget, you can actually just once and for all release a synthetic version of the data set. And then you have privacy guarantees forever, no matter what people do downstream. But that, when you dig into the weeds, it's not always that you can do that, right? So it's like the same thing with kind of like the distinction between client-side differential privacy versus centralized differential privacy. Like the example of randomized response or noisy histograms on Apple devices, that's the beautiful-- one of the beautiful cases where you can get client-side differential privacy, meaning like I don't have to trust some third party to have correctly implemented a differentially private algorithm. I add the-- I flip the coin myself in the privacy of my own home or my own iPhone and know that I have these guarantees. So if you ask for that stronger thing, you can't always get it. There are certain problems that we don't know how to solve in that distributed way. But, when you can, it's great and same with the sort of synthetic data set. ANAT CASPI: OK, please join me in-- [APPLAUSE] MICHAEL KEARNS: Thank you. [APPLAUSE] [MUSIC PLAYING] 