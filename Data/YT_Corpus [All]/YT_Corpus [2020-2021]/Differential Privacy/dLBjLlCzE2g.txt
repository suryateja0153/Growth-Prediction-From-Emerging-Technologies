 machine learning uh uh before coming to Chicago.  He was at Georgia Tech and before he was before   that he was at Yahoo research. He received his PhD  from Yale and his undergraduate degree from from   Princeton and he has served on steering committees  and as the associate editors editorial board of   generals and conferences as well as prone chair  of conferences. His research has been funded by   National Science Foundation under government  defense as well as he has earned several rewards.   This is our pleasure to have you here Lev  and thank you very much for taking the time   and i look forward to your talk today. Thank you for inviting me and it's nice   to virtually meet all of you um since  we want to leave time for questions   i i want to start right away but i just  want to have a disclaimer that this work   this is recent work in an interesting line of work  about differential privacy but machine learning   and all sorts of other things and it talks about  connections between all these interesting fields   which is why i find it so fascinating but i  can't really talk about my results without   putting them into a broader context and explaining  a little bit about all these other fields   and so it's very possible that i'm only going to  get to my results at the very end um but um if if   what you take away from this talk is not my work  but about these new connections in which it is   possible to do important and interesting research  i'll be more than happy and and so um i might not   get to our results at the end and you know that's  somewhat on purpose and somewhat because of the   time limit uh okay so the talk is labeled  differential privacy adaptive data analysis   and free speed ups from sampling um and it's with  ben fish who is a postdoc at msr montreal and mila   and ben rubinstein who's um at uh melbourne um  ben fish was my phd student and uh now he's a   postdoc and he's in the market this year and so  i i would be remiss if i didn't tell you he's   fantastic and if you have any openings you should  hire him okay um oh okay and uh the paper that   um this work you know you can read about in  detail is um sampling without compromising   accuracy and adaptive data analysis in of 2020  of this year okay now let's talk about uh the   problem that this work is situated in and that  is the data sets um are often reused and that's   not usually a thing that's allowed to happen in  particular you get the problem of overfitting   when you do this and uh this has many names  because this problem appears in many fields   uh false discovery garden of forking paths  p hacking i'm sure you've heard of a variant   of it and um the problem can be summarized in  a thing that people do all the time and that   is you have a data set this is your validation  set i'm assuming you guys can see the hand still   okay let me know if you can't see this  hand okay so there's a validation set   and that's kind of the data set you're uh uh so  you could be training on something and validate   it and encourage something or you could be just  training all in one thing it doesn't matter you   have some data set and you train in your own  network and in training in your network you're   already selecting the best parameters for a  neural network uh uh given the structure of   the neural network and so you're adapting here  that sort of adaptation is fine but then let's   say on the validation set your error is not good  enough and so you remake your world neural network   um you modify the structure etc and then you  measure loss again from traditional machine   learning we have no guarantees whatever anymore uh  once you look at your data set you're not allowed   to change the class of model where you're uh  what you're doing right and so this is called   adaptivity we're using a data set to choose a new  model or to kind of do something like if you say   i'm only optimizing decision trees on my data  set that's okay but if you use decision trees   and then you look at your data like i don't  like how decision trees did let's use boosting   or whatever not okay anymore okay and everyone  kind of knows this and so adaptivity the fact   that we can't actually in practice we draw data  sets for free makes it difficult to generalize. okay and um and so mort's heart who's one of  the the leaders of this field he's a faculty at   berkeley he um right just just has this this nice  little picture just what is static versus adaptive   static is you have a method you use the data  and you make an outcome and adaptive is the the   the method looks at the data you get an outcome  and that changes your method and you want to be   in this loop ever improving your method so this is  the right hand side is what adaptivity kind of is   um what's not adaptive by the way this is  another thing that people might think is   adaptive but that's not what we call adaptivity  for our purposes is if you have a method and you   use it on data and you don't like the outcome  if you change the method but then use new data   etc that's not adaptive because you are not  adapting to a data set instead you're drawing   new datasets so drawing new data sets is more  or less okay i mean uh there of course there   are it's all there are you have your own problems  with continually redrawing data sets because for   example um there's some probability you'll have a  mismeasurement on a given data set and if you keep   drawing a new dataset testing the same method  maybe eventually uh your method will look like   it's doing better on the new data set than it  really is but we can control for this sort of   thing by drawing large enough data sets etc  so this is not what i mean by adaptivity   okay and so we want um to be able to answer  queries adaptively on a fixed data set without   having to assume a anything about the nature  of that activity so we don't want to say well   you can change your algorithm but not by too much  or without assuming anything about the complexity   class from which our queries into the data come  from like bounded vc dimension so um let me   make queries could be i mean we'll define queries  later but what is the error rate of my hypothesis   is a perfectly fine query and so um measuring the  error rates of various things that you've trained   can very well be thought of as a query but queries  could be other things just any question onto   the data and um and hopefully uh many of you are  familiar with the traditional way of being able to   ask lots of questions into a data set um without  having to draw new data and the traditional way   is to measure something about the complexity of  your question so like if your questions or if   your hypothesis class has bounded vc dimension or  bounded rather mocker complexity or bounded size   or bounded something then very often we have these  outcome razor times theorems that are like at the   foundation of learning theory that let us just  say okay well then you can ask all these questions   at once and things are fine and so the goal of  this work is not to assume anything of this form okay and uh uh okay so this th  this framework was uh formalized in   a in in i guess um a field called adaptive  data analysis and this was uh by um   a group of superstars uh cynthia dwork uh  mort's heart aaron roth toni and potassium   i'm sure i'm forgetting some people  uh and this was a paper in science   uh and and and the idea is that we're trying to  convince scientists to adopt a new view of uh of kind of learning from data and that is because  of course in science you gather data you spend a   lot of money on it right and then very often  you keep testing different hypotheses on your   data set and you can fool yourself into thinking  you have a false discovery this is partly why so   many scientific results don't replicate and so  this this was actually picked up by mainstream   uh uh publications like cover all of science  and yet somehow this is not as well known in   within the theoretical computer science community  or the machine learning community as i think it   should be and so this is a framework there's  an unknown distribution and there's a data set   that's drawn the green thing from this unknown  distribution and you have a mechanism this is   like a question answering system that uh has  to answer questions uh using uh the sample   and uh and now you have an adversary that's  attacking the the system by asking questions and   the mechanism has to use the sample the  mechanism doesn't know the distribution   the mechanism just knows the sample and has to  answer the questions from the sample that it has   and the point is that the answers have  to be not necessarily true in the sample   though that would be nice but they have  to be true in this unknown distribution   and the adversary cracks the mechanism if  at a certain time it asks a question that   the mechanism can't answer correctly according  to the unknown distribution so a question   is some function from data like what is  the error rate of my hypothesis or what   fraction of the data have this property or  anything like that um and once the mechanism   is unable to answer questions truthfully  according to this unknown distribution   from which it has a sample we say that adversary  has broken the mechanism and so the goal here is   to design mechanisms that are as robust as  possible to adversaries trying to break them   now uh the motivation never had an adversary right  the motivation of this problem had you just trying   to learn from data but um the viewpoint adaptive  data analysis makes is that you are your own   adversary that is um as richard feynman said the  easiest person to fool is yourself and uh so um   so the idea is you in practice would implement  both sides of this the adversary is actually the   learner and then the mechanism is that something  that's protect sitting around the data set that's   protecting the learner from itself protecting  from the learner from making false discoveries   and the whole goal of this field is how many  queries can we answer and how long does it take   to answer the queries as a function of  for example the size of the sample we have   etc okay now um i know you guys are not used to  asking questions in the middle of these talks   as i was told but um maybe here would be one  exception because um if someone is lost about like   where we are what we're doing you know  this may be a good time so i'll pause   for 10 seconds and let anyone ask a question  about what the goal of this is if there is one so i have a clarification question so suppose  let's say you are trying to learn i don't know   maybe a circuit or a distinctly based on this  data that represents this now when adversely   asked question does it mean that the adversary  is going to give a decision tree and then say is   this the correct decision tree and the mechanism  is supposed to answer yes or no is that a valid   way of thinking well okay so so right so this is  um an okay question to ask although the mechanism   has a labeled data set it doesn't have a notion  of correctness but maybe you can ask does the   decision tree predict the labels correctly or it  might ask like um what is the error rate of this   decision tree on this data set or if there  is some model of a decision tree that the   mechanism has that's encoded in the data you  can ask such a question uh so mechanism only   looks at this data set but it might be doing  some inference based on the data set and then   based on that it would still answer the adverse  results yeah yeah the mechanism is allowed to do   whatever it wants it just fails when it um when  it no longer reflects reality which is in the   okay distribution you okay and okay so now uh  what are queries so this is this probably uh is   just just good to formulate so query is just any  function from the distribution on to let's say the   the real line or into real numbers on which  and the mechanism is just trying to return   function uh values that are close to the  function when evaluated on this distribution   and uh and for example um but of course  the mechanism doesn't have access to the   distribution so he needs to estimate it so like  a very typical thing like the most famous type of   query is called the statistical query and so the  function is specified from the data onto zero one   and then the mechanism for example can look at  the sample and just average the queer value on   the entire sample and return the sum of all the  queries on the sample divided by one size of s   and so um not only can the mechanism do  this but the expectation of q of s is q   of d does that make sense so can i ask you  a question what is x x is a support of d   no x is the domain of the data okay sorry that's  the typical machine learning notation so data is   always on x and this is on x we're going to stick  the labels into x also right so d is on x okay   and okay so so statistical queries are some  of the more basic queries i mean there are   more basic queries you can formulate but  actually statistical queries are are kind of   very simple to understand they're like point-wise  queries that you can average over a data set   they're very well defined in a distribution  but actually they're super duper powerful   and so um there's been a series of work this  is another thing that's um not known as well   as it should be and i've i've recently written a  survey on statistical queries because i want to uh   preach to the world about how  cool they are and that is that   statistical queries actually capture pretty  much every method that we have so any algorithm   that we have we could if we really wanted to  phrase it as a series of statistical queries   um on a data set and then updates just  based on the answers to statistical queries   and so um uh most methods that we have for  learning and actually not just learning can be   implemented via sq and and it's not like a theorem  that's true for just all the methods or something   um it's more like there's been a series of  work for each individual method to try to   figure out how to make it uh runnable through  statistical queries and so gradient descent   is a clear example you can ask for gradients and  move in the direction for gradients and ask your   queries like what is the gradient here what is  the gradient here expectation maximization support   vector machines boosting was a really interesting  result you can actually boost without re-weighing   distribution on specific examples uh linear and  convex optimization mcemc simulated annealing etc   it's it's actually really surprising uh how many  methods have been uh converted into this framework   and actually one big spiel that i have in  my research is is is that in order to make   progress in machine learning theory we  actually need to design non-statistical   query methods because statistical query  methods also have some limitations   and uh we are very hard-pressed to figure  out anything that's not statistical query   uh we have basically two techniques that are not  statistical queries one is hashing which doesn't   get you very much in machine learning and the  other is gaussian elimination which gets you a   little bit but also not so much and we are at a  loss kind of to think of anything else basically   okay so um so what are the theories  queries we can consider well we consider um um things like counting queries so counting query  asks what proportion of the data satisfies a given   property and uh it's specified as from data  onto zero comma one so uh instead uh so this is   kind of like a limited statistical query where  you either satisfy the property or you don't   and then here the question is here's  a property what fraction of the data   satisfies it and there's some true expectation  distribution um and and basically these counting   queries are just uh the right way to think  about statistical queries um we also think   of uh things like optimization queries this is a  more powerful thing um and more used in adaptive   well they're kind of popular these days um  an optimization query is the following um uh what you give is sorry i can't see my own  slides because of the okay i think i removed   the thing okay so what you give um to us to to an  optimization query is like a class of functions   and um the optimization query tells you what  parameter optimizes over these questions over   the distribution and so maybe i can give you an  example so um maybe your functions could be like   um like loss functions with a minimum and an  optimization query asks what where is this   function minimized and the min point would be  returned by an optimization query so uh um now   there are statistical query methods to implement  optimization queries but you can actually ask   about optimization queries on their own right  so this i'm just trying to give you the sort of   queries that people could ask um there's something  called a low sensitivity query a low sensitivity   query is um if any function that doesn't change  too much if any one data point is changed um uh that's another query you might want to  consider um and that's actually more powerful   than a statistical query uh because a  statistical query is a big average over   data set points and a low sensitivity query  is just any query that doesn't change so an   average is one of them but it could you could  have other things and of course you could   just say hey we're not going to think about any  restriction of queries we would just say query   but it's very hard to prove theorems about  just any query in fact we know we couldn't   okay and now um so if you're familiar with a  pac learning framework right in pac learning   this is supervised machine learning you have  like an error and a confidence parameter and   in uh adaptive data analysis you have something  very similar you have an alpha and beta parameter   so in pac we call them epsilon delta in adaptive  data analysis we call them alpha beta and so   we say a mechanism is alpha beta accurate on a  distribution for every single query the biggest   miss is bounded by alpha with probability  at least one minus beta so so alpha bounds   the difference between an answer returned  and the true answer and a distribution   and b one minus beta beta is like the failure  probability or bound on the failure probability   okay sorry and now uh uh zoom  has blocked my slides again   sorry okay and so um a typical question you  might ask is how many samples and does it take   to answer k adaptive queries efficiently without  i have a question sorry what is a here what is a   a is the answer so every query q i is the  mechanism answers with answer ai and what the   script is now the question is what is the biggest  difference between the answer that the mechanism and your probability is over m which is  mechanism the probability is over any   randomness the mechanism uses and and uh any  randomness that's in the data so like because   draws a sample the script is then the right it's  also right i'm sorry so it's it's also the script   a is adversary so the adversary can also  use randomness so the probability is taken   of all the over all the randomness in anyone who's  in anyone i see so a is asking the query yeah a is   the adversary asking the query queue i i see ai  is generated by the mechanism as the answer to   qi and the question is what's and then each query  has a true answer q i of d right and the question   is what is the biggest mistake like as in what's  yeah and so we want to bound the biggest mistake   in advance by some parameter alpha we want to  fail with probability no more than beta and this   is just like pac learning and the question is  how many samples did the mechanism need to draw   so i'm kind of confused about your adversary  are you not you're not seems to be specified   so with respect to a fixed adversary is this  is that right so we assume nothing about the   adversary for all i do is so we need this to be  for all adversaries right so i didn't of course   quantify all this there exists a mechanism such  as adversaries it's it's like a pac thing with   an algorithm such as it for any distribution  etcetera so for all adversaries this has to be   true to call it a mechanism to call it alpha beta  yes that's right great thank you yeah thank you   okay and um and okay and so the reason we ask like  how many samples does it take to answer k queries   well because there's always some way  to do it with enough samples here's how   for each query right each one query you know how  many samples you need right and that is to be   with an alpha with probability one minus beta what  you need is like uh one over alpha squared queries   times log one over beta and then by union bound  if you draw basically k times that amount you   can just divide your data set into independent  pieces and each independent piece would be used   to answer each query and so you could always  answer queries accurately if you redraw data for   each query and then you could just say oh if i had  this huge amount of data i could just simulate or   redraw the data the point is can we do much better  can we actually reuse data can we be adaptive   but not shoot ourselves in the foot okay this is  kind of like a theoretical framework for this um   again unfortunately i won't get into too much  of the theorem proving within this framework but   again i think it's so important to know about the  fact that this exists that i don't mind you know   wasting time for my own uh results or using time  for my own results in order to teach you about it   okay okay now um what would a pack a  standard machine learning algorithm do   well on pack there's basically one method and  that's called erm and what it basically does is   on a query queue it returns the value of  the query on s um and in some sense um   well it depends on what type of query it is but  in general forget about even erm if someone asks   like what is the area of this hypothesis the  best estimate is to compute its empirical error   and return that and then you use bounds to say  the empirical error is close to the true error   but for adaptive data analysis answering  queries truthfully turns out to be sub-optimal   as in if each time the mechanism would do this just for qi of d draw sample an  average over the sample um well first of all i   mean you you don't get to do that for every type  of query but let's say we have a query where this   is like an okay thing to do like a statistical  query it's going to be the case that we're going   to need to use a lot more samples to get alpha  beta accuracy than if we were a little smarter okay does everyone understand this  claim not the proof of it just the claim   okay and uh and and and then one of the coolest um  adaptations uh one of the coolest um explanation   or illustrations of this phenomenon is an attack  on a leaderboard so a leaderboard is something   that's used right now in like every data science  competition you have a holdout set of predictions   and you give a training set and then people submit  so it's a like kegel or something people submit   to their classifiers and what you do is you tell  them what their error rate is on the holdout set and uh and uh the person who has the smallest  error in the holdout set let's say wins   and so um averm blum and moritz hart thought of  this really cute attack called the boosting attack   on a holdout set so imagine  you have a holdout set of um   and data points what you could do is you  can pick k vectors uniformly at random   each vector just gives a random classification  of the data and so then you get k losses uh   which are what is my error rate  on the data and then you you just   find the set of them that do  better than random guessing   and then as your predictor you use the majority  of the ones that did better than random guessing   and now we might try to analyze what in the  world this method would do but before i do that   this is kind of like um like a theoretical  construct so let me give a picture so okay   so the leaderboard let's say has a holdout set  of n examples that's on the right on the upper   um these are the columns okay and so what this  attack would do is it would pick k different   um hypotheses each one just gives a random label  for each example and now uh the leader board   would have to answer all of these and tell them  what is the accuracy of each of these k vectors   and let's say the leaderboard is  answering honestly so let's say   it says the error rate of the first guy is  0.55 if the next guy is 0.48 then 0.53 etc   okay so what the leaderboard can do is it can  block out into gray all the guys that are red red   means they're doing worse than random guessing and  so it could only consider the ones that are green   the ones that did better than random guessing and  then choose a vector u star that is the majority   of the ones that did better than random guessing  so the majority for x1 would be zero one and zero   so the majority of that is zero but like here for  example one one and zero the majority here is one   and so it could afterwards use u star as its  predictor now notice if u star is a good predictor   uh we have broken uh this adaptive data analysis  why well u1 through uk are random vectors   and u star is a majority of a subset of random  vectors it should do better as well as random   it's true on the true distribution it should  be have error rate a half now if on the sample   it has every better than a half that means  we broke that activity and sure enough um um avraham blum and mort's heart pro showed  that with at least constant probability   the loss of this u star vector is one half minus  square root k over n which is a problem because   as k grows you're separated more and more from  a half as the number of random vectors grows   the reason it's a problem is  because if you were to redraw   data for each response you would continue  to have 50 error as you should but if you   do this if you give actual naive responses  if you actually tell people their true error   from these rand this random attack this u-star  vector's error rate will go down and down and down   to zero even though you've learned nothing and  actually this has been a problem for a while um   uh and this is why leaderboards now uh one way  to handle this problem is now they have like   a training set a holdout set and then a double  holdout set where uh you're evaluated on the   secret thing that nobody knows so if  you've over fit to the holdout set   um that's kind of like your problem but your  true ranking is evaluated on something that   nobody's ever seen but of course uh this  makes the leaderboard kind of really silly   because it's giving um up-to-date answers on  who's in first who's on second who's on third   on a holdout set that's not reflective of reality  at all and you can get into first place on such a   leaderboard without having anything on the true  holdout set moreover if they actually gave true   rankings on the true holdouts that would leak  information be open them up again to attack   and so the way leaderboards often deal with  us now is that they report a leaderboard   that has no bearing on reality okay hopefully  everyone understands a little bit of what i   said okay so now uh here comes the main idea  and i think it's one of the most beautiful ideas   i've seen in learning theory even though it's  a very simple idea this idea is the following   we're going to put a privacy layer around the  data and on query q we're not going to return   q of s we're going to return a private version  of q of s and the the intuitive idea is that   an algorithm that cannot learn the data itself  will not overfit to the data and so for this um   something called the transfer theorem is used and  the idea is that there's some notion of stability um and this notion of stability applies to  algorithms whose output doesn't change too   much when the inputs change by a little  and you can actually prove something   called a transfer theorem that says that stable  algorithms generalize better than non-standard   non-stable algorithms now most of you have  probably not heard of stability but there's   actually a very specific notion of stability  that probably everyone has heard of by now   and that is differential privacy and so  the most important and popular notion of   stability is differential privacy and  here's what differential privacy is   differential privacy uh says that a mechanism is  epsilon data delta private if when it gets two   samples that differ on just one point their  responses differ no by not too much they don't   differ by a multiplicative factor of either  that's one and an added a factor of delta what i mean by um their probabilities  don't differ is the probability it   outputs any given response is  similar for the two mechanisms and so um uh yeah i guess i could talk a little  bit about differential privacy like why this is e   to the epsilon and like why is one of the factors  an exponent and one uh just a linear term and the   reason is this is so that you can compose privacy  parameters but instead i'll just give you um um   an example so let's say uh s is some data set  and a differential private mechanism has a   distribution of responses over the  data set so the x-axis is outcomes   and the y-axis this is probability so this red  curve represents the probability of giving any   outcome and now we change the data set  from the red one to the blue data set   which differs just by one example and so now you  have a different blue probability of outcomes   and what differential privacy  says is the following that   for any outcome so for any value here on this  x axis the two y values are not too different   the probability of any one outcome is going  to be approximately the same if the data set   didn't change too much and the intuitive  motivation for this is that if the   if the probability of any given outcome doesn't  change by too much if the data set is just   changed by one example well then uh people  should be incentivized to participate in the   data set because their participation doesn't  change the probability over outcomes anyway   and now you could basically uh now we  have all sorts of algorithms that just   are there differential  privacy is a very mature field   and we know how to give differentially private  responses and leaderboards and anything to   so there are a lot of queries for which we can we  know how to do differentially private responses   and so if the queries are of the form of  which we can answer differential privately   now the the boosting attack will doesn't work  anymore for example and so the blue line is the   boosting attack now you can get a little bit  better than a half for technical reasons which   i don't want to go into um but um but you're not  really going to improve and that is because you're   the differentially private responses  they're close to the true errors   but they're hiding the data in a way that you  don't the majority doesn't give you anything   so this is one example of differential  privacy saving you from an attack now uh what's the standard uh differential privacy  um mechanism for answering uh statistical queries   well what you do is you return the true answer of  the query plus some noise that you might want to   add and they use laplace noise which is a little  more concentrated around the mean than normal   noise and uh i mean there are different theorems  you can prove but basically laplace noise gives   you a good trade-off between how inaccurate this  makes the query and how um how much privacy is   still preserved so for example laplace noise is um  why do we use laplace noise instead of the normal   distribution well because the normal distribution  makes the query less accurate right it's   uh it's a little less concentrated uh and so of  course the normal distribution would work it just   doesn't give you as good of a trade-off  as laplace noise so we use laplace noise and so um this mechanism is nice because on the  one hand it's accurate in the sample it's private   for statistical queries and it's sufficient  to guarantee accuracy on the distribution   as in you get a transfer theorem like the type  i mentioned um okay now i'm kind of rushing   through this part because i want to talk a  little bit about the flavor of my results   but i also know we don't have much time and  so um if at this point you're not going to   follow along that you followed before you already  learned kind of i think something very cool okay so before we go for i i think i  since there were a few questions   uh maybe it might be okay to go until um a little  bit later too okay we could do it however i could   also just wrap up quickly and leave more  time for questions now like i can finish   in five minutes i don't know yeah whichever  whichever way you want to do it i really do so just ask a question don't  ask a question it's unhappy   am i supposed to ask a question or no no question  i don't know go ahead go ahead go ahead please god   go ahead please yeah no i was just wondering  just if you go back to the previous slide   uh so the goal here is not to give the  adversary enough information uh so that   even if you ask many questions he still don't  reveal any information so it looks to me that   yeah is that the the goal of that okay so the goal  is to be accurate so the goal in adaptive data and   analysis is to be accurate for as long as  possible and a mechanism for doing this is   to answer differentially privately which means  that you're answering accurately but with some   noise and the whole tradeoff is how much noise  can we add such that we're still alpha accurate   alpha beta accurate so the idea is we can't  add too much noise because then we'll lose our   accuracy guarantee and if we add too little noise  uh the adversary will adapt quickly right so the   question is in some sense if you're going to use  um differential privacy for this like how do you   set the parameters and stuff you could also not be  differentially private you could do something else   and still remain alpha beta accurately so  differential privacy is just one mechanism for   answering statistical queries for example but it's  not necessarily the only one it's just a good one   let me go on yep um again this all gets very  confusing and technical that like like anything   right like the model is clean and cool and  then the results like okay and so there are   all sorts of known results so for example we know  for example how to do low sensitivity queries and   statistical queries in root k over alpha squared  and we know that this is at least tight in k   we know how to do convex optimization queries  and root kd where d is the dimension of the space   over alpha squared we know how to do like  hypothesis testing etc let me skip that part   but here's an interesting thing all of these  queries uh all of these mechanisms take linear   time per query so if you have a big data  set the way all of the known mechanisms do   it is they evaluate the query truthfully on the  entire data set um and and then they add noise   and if you have a giant huge data set of like  i don't know like uh data on 30 million people   uh this is actually doesn't correspond  to what people do in practice   and so here's a summary of our results  so we basically take this column where vasili at all who are the main people  who did this before um uh and what we do is   we basically improve our dependencies on k  let's say i mean we improve some other stuff   but like for example we usually go from square  root of k to log k or from k to log and uh   and how do we do this so uh k is again the  number of queries asked and let me let let me um let me just go back to our idea and maybe i'll  stop there because the idea itself is really cool   so instead of answering the query truthfully on  the entire data set and answer and adding noise   our idea is the following let's just sample some  small number l points uniformly at random and   give either the true answer for them or add noise  to that answer let's let's just consider adding   answering truthfully on this and so um you could  see an expectation this should all work right so   answering truthfully on a small subset should  be about the same as answering truthfully on the   whole set should be about the same as answering on  the whole set plus noise but interestingly enough we can prove exactly the same uh transfer theorem  on the sampling technique and let me just give you   the proof idea the main idea because i think  the main idea is an obvious thing that you   can remember and everything else is stuff you  can look up in the paper the main ideas i found   if instead of giving true answers and adding  noise what you do is you sub-sample your data sub-sampling your data set um loses accuracy  right so if i have a thousand examples and   instead i just give a true answer in five  examples this loses accuracy just because   i'm not using the whole data set on the  other hand that adds privacy because i don't need noise in some sense to now um uh  hide most of the data if i subsample just five   data points and report the true answer and  the five data points most of the data points   haven't even participated in the subsample  in which i've used to compute my answer   and so this subsampling technique intuitively  gives you worse estimators but on the other   hand gives you more privacy because most  data don't participate in the subsample   and now if you think about what a transfer  theorem requires that goes from differential   privacy to stability right the transfer into  i'm sorry from differential privacy into   adaptive data analysis the transfer would  get better as you have more privacy but   worse is you have more uh less accuracy  and so this technique basically makes your   accuracy worse but your privacy better and we  can prove that those two exactly cancel out   and that you get sub sampling for free so you  can speed up algorithms dependence on the number   of queries to linear per query to logarithmic per  query you need logarithmic because of a huffington   type bound you have to still be within reasonable  bounds for all the queries so your sample has to   be of size logarithmic in the total number  of queries asked but otherwise you can get   per query exponential speedups precisely  because everything that you lose in accuracy   you're gaining in privacy for this transfer from  differential privacy onto adaptive data analysis   and um there are really cool applications to  convex optimization which i'll skip except for   dimension one and that is there's this thing  called stochastic gradient descent stochastic   gradient descent is like gradient descent to do  optimization except for instead of computing the   true gradient and the entire data set what you do  is you pick a random example and you compute the   the true uh gradient on just a random example well  what this work does is it says actually this is   this gives a framework for just saying that this  is an okay thing to do because stochastic gradient   descent or stochastic block gradient descent  where you take a few examples and compute true   gradients instead of on everything exactly has  this tradeoff where you lose accuracy in your   estimate of the great gradient but what you  gain is actually privacy what you gain is the   ability to not over fit and our analysis gives a  completely different like our results just give a   completely different analysis of why stochastic  block gradient dissenter stochastic gradient   descent works you can use stochastic gradient  descent as something that just boosts the privacy   of a stable algorithm or a differentially private  algorithm um in order to give you the accuracy   and why do people use stochastic gradient descent  precisely because they want faster convergence to   the optimum and so this is another example of how  this framework explains something that uh was only   understood through a very specialized uh theorem  before um i can see that i'm at 1250 and i'm not   going to get to sampling counting queries but um  let me just say that faster adaptive data analysis   now is not only possible but like can be done very  quickly it has applications to convex optimization   and there are even things you can do with  constant numbers of samples that is meaningful   um and another thing is this kind of like  some people have commented on this work that   it kind of combines property testing and  adaptive data analysis the idea is you're   looking at a very small part of your data and  you're drawing conclusions that are important   and somehow this because privacy gets you so much  here and compensates for loss of accuracy this is   a really neat way where you can get free speedups  it's also the case that we still don't know the   optimal dependence on any of well no not on any  but on many of the parameters we still don't know   the optimal dependence for example we have one  over alpha squared dependence but only one over   alpha lower bounds for a lot of the problems and  so there's still a lot of interesting work to be   done in this field and a lot of work to design  practical algorithms et cetera and and so um and   and uh just a lot of interesting kind of um uh  things at least intuitively that are happening   here and so um i wanted to talk about this i'm  sorry i didn't get to talk about the details of   my work but we started five minutes late and we  want to end early and so i'll i think i'll just   stop here and hopefully you can check out my  paper if you're interested thank you so much thank you very much thank you for  your for your talk and i think   we still have time for a few questions uh uh hi live uh so this is kevin hi um i'm just  yeah i'm just curious uh you know when you do this   uh sampling uh do you have uh basically can  you tell me a little bit uh more detail how   how the sub sampling was done um well  so so right so the i mean the analysis   okay so this is theoretical work right so so  the analysis is just for uh a uniformly random   sub-sample of the data and actually we've done  two analyses one for with replacement and one mute uh i don't think i'm muted but can you hear me  now yeah yeah yeah yeah i think you were fine   okay so our theoretical analysis is for sub  sampling where we so the theorem gives you how   many samples you need and you can either take them  from the data set with replacement uniformly at   random or without replacement uniformly at random  we have theorems that both ways are okay to do um   and then actually we got into like a whole rabbit  hole about what is the complexity of sub-sampling   with or without replacement and uh in different  cases it's different and what you should do is you   should do the sub-sampling that's actually faster  because we have a proof that both ways are okay   and then um there's actually a technical reason  why we still have to add laplace noise even to the   subsample so our algorithm does a uniformly random  either with or without replacement subsample and   then we need to add laplace noise um even  to the subsample and the reason is because   um because basically um there's this notion  of natural privacy where if um where where um   it's um right there's there's some um there's some  uncertainty introduced by just taking a sub sample   but uncertainty goes away as as the true result  of the query approaches zero or one and and like   if a query is deterministically zero right the sub  sample will return zero regardless of which points   participate in in the subsample and that actually  violates differential privacy so the reason you   need to still add laplace noise is because of  these border cases which you can actually handle   in much cleaner ways but basically i mean the  theorem is very straightforward you you sample   wither without replacement uniformly at random  and you return your true answer on the subsample all right and as of sampling uh uh it seems  to me a kind it's sort of like a log of   k or something you improved the previous uh linear  uh dependence to some so does that mean that you   you only need like a uh some some log order uh  number of samples to to do that yeah that's a   very good question so we actually right this  is a point i did not have time to address we   we don't reduce the size of the underlying data  set the size of the underlying data set actually   has to stay the same and so there's a linear  dependence on k on in the size of the data set   and we still need that for our theorem what we get  to do is just make faster per query evaluations   but the theorems will fall apart if you  actually take a smaller sample overall okay and so it's only just not to  make anything more sample efficient   i see yeah very interesting i'm going to  check the paper for more details thank you we still have time for a couple more questions if  um if you would like to use the microphone please if not that's okay too i won't be offended uh looks like um that a lot of the questions  were asked during the talk on itself um thank you   um thank you again i think this was um  a very um helpful introduction to this   this new area where we have uh we have we don't  have um any individuals working in this area   and our in our uh group yet but this will be an  interesting area to explore and thank you again   for taking the time it's such a short notice to  give this talk i appreciated that thank you for   inviting me this was fun yeah thank you very  much and goodbye everyone this concludes the   seminar for today thank you thank you thank you 