 [Ali Jadbabaie]: Okay, next we're gonna have Aaron talk about the ethical algorithm, which is also the title of his new book. [Aaron Roth]: Thank you very much, thanks for having me. Yeah, it's not a not a coincidence that the title of the talk's the same as the title of the new book. In fact, we just picked out the cover, there it is, but I'm not gonna mostly talk about the book, although I'll suggest to you the thesis that we are trying to make. I mostly want to walk through a short case study of the kind of formal methodology we propose in the in the realm of fair machine learning. Okay, so to start things off, if you've been paying attention to the popular accounts of machine learning, say in the last decade, you probably noticed at least two clusters of articles in the popular press. The first one is marveling at all of the cool things we can do with machine learning. Okay, it used to be that the primary examples of machine learning were things like spam filtering and ad targeting and that's great, but now we're using machine learning to inform hiring decisions, HR departments at big companies have large machine learning toolkits. We're using machine learning to help make lending decisions. This article is actually about a colleague of mine in the criminology department who designs these tools for the state of Pennsylvania. Machine learning is used to inform bail and parole decisions and predictive policing algorithms which are driven by predictive engines are also, you know, fundamentally machine learning methods that are deploying police officers. So that's great, you know, we can improve lots of aspects of human life using these new tools, but these are all decision-making pipelines that, were human beings making the decisions, we would expect them to uphold a certain kind of ethical norms, and so maybe it's not surprising that there's also been this second thread of articles in the popular press that are basically worrying that the algorithms we're deploying to take the roles of human beings in these decision making pipelines are not living up to those ethical expectations. Okay, this is true both in privacy - so, starting about a decade ago we started seeing lots of articles in the press about privacy violations and more recently, we've been seeing lots of worrying about fairness. That algorithms might be perpetuating human biases or maybe even exacerbating them. So, you might be excused at this point for wondering why we can talk about ethical algorithms. Why are we imparting some kind of moral character on like a human artifact? We wouldn't talk about an ethical hammer. Okay, but I do want to argue that algorithms are different than hammers. I mean, so hammers, of course, can be used as instruments of violence, they can be used to do unethical things, but when that happens we don't say that it's a moral failing of the hammer. We say it's a moral failing of the human being wielding the hammer. Okay, and we can usually very easily blame a particular human being for any misdeed he did with a hammer. The same is not true of algorithms. When an algorithm causes someone harm, it's often very difficult to assign blame for that harm to the software developer who designed or deployed the algorithm. If you think about modern algorithms, and by that I typically mean models trained by machine learning methods, well they start with some data set. Typically a very large data set. We just heard a definition of big data and you might be a domain expert but there's a limit to, you know, how much you can understand a large data set. Okay, then we tuned parameters using something like stochastic gradient descent. You might be an expert in optimization and understands stochastic gradient descent as much as anyone does, which means you understand it at a very local level. But it's hard to really understand the outcome of stochastic gradient descent, and so when what we end up with is a vector of millions or billions of parameters, this is now several degrees of separation removed from the piece of code that the software developer, you know, coded up, and so when ultimately the trained model harms someone, it's not clear that this is the fault of the person who either designed or deployed the algorithm. In some sense, were it, you know, the fault of the designer of the algorithm, were all sort of problems that that come from machine learning and other automated decision-making the result of, somehow like, evil engineers, that would be easier to deal with. We have existing legal structures for dealing with, you know, human malfeasance but this can be something very different. You can have engineers who are following principled, you know, well understood approaches and lead to behavior that wasn't intended. Okay, so given that, the premise of our book and a lot of the research I've been doing recently is that there's therefore a need to somehow embed social values into the algorithms themselves. In order to do that, we have to be very precise about what we mean. It's not enough to say something like 'we want our algorithm to be private or fair.' If I want to design an algorithm that's going satisfy one of these criteria, I have to be mathematically precise about what I mean, and I need to explore what the consequences are of imposing those precise constraints. There's a lot of ethical or social values you could consider - I've written them here sort of in increasingly light grayscale, I would say in the order in which we've made progress trying to understand them. Relative progress - not that we've, you know, solved privacy by any means, but privacy, and by that I mostly mean differential privacy, I think, serves as a case study in which - that shows that this approach is feasible. So differential privacy was studied mostly as a theoretical curiosity for people like me for more than a decade now, but now it's widely deployed in the tech industry and the 2020 US census is going to release statistics subject to the protections of differential privacy. That's all I want to say about that. I want to talk mostly now about fairness which is much less well understood, but is a setting in which I think the precise and theoretical methodology that served us well in studying privacy can help us here as well. Because the definitions are important and hard to get right, and that's where theory shines. So before I say anything about fairness technically, I want to reflect on why machine learning algorithms might be unfair in the first place, because it's natural to think that, in fact, deploying a learning algorithm optimizing some clear objective like classification error is the solution to problems of fairness. That we remove biased human beings from the decision-making pipeline, that's how we get fairness. But I want to convince you that the story's a little more complicated than that. Okay, so here's a little cartoon. Imagine we're designing a college admissions system. We've got two features about applicants - SAT score and GPA. Maybe how much money they've bribed the basketball coach is not on this plot. So we've got positive and negative examples, okay, predicting whatever we care to predict, say, for your graduation rate, and I've got two populations here. Right now, you're just looking at the green population, okay. I want you to notice a couple of things. First, slightly less than half of the green population is qualified for college. Slightly less than half of them are positive examples and there's a, like a simple linear decision rule that does a pretty good job of telling them apart. Pretty good job but not perfect. All right, that's the green population. Here's the orange population. Okay, a couple of salient things to notice. Maybe the first is that they're a minority population, and by that all I mean is that there's fewer of them. There's more green points than there are orange points, that's what it means to be a minority. The second thing you might notice is that they're shifted down on this plot, they've got lower SAT scores for whatever reason - you can tell your favorite story about that. Maybe the green population takes the SAT three times, submits the best score out of the three, the orange population takes it once cold and submits that, but from the point of view of our classification task the orange population is actually better than the green population. A larger fraction of them are positive examples, in this case exactly half, and it's easier to tell the positive examples from the negative examples. There's now a perfect linear predictor. But, when I combine the two populations and do the seemingly reasonable thing of finding the linear predictor that minimizes classification error, what do I get? Well, I just get the rule that best fits the green population. In particular, it rejects every member of the orange population, even the qualified ones, even though the orange population was more qualified on average and it was easier to distinguish who was qualified and who was not. It's for a very simple reason, if I moved my predictor downwards to decrease my error on the orange population, I would have increased my error on the green population. The green population is the majority population so they contribute more to average error, and so it wouldn't have been worth it for the objective of average error. Note that, were I allowed to explicitly use group membership in making my decisions, I could have arrived at something that looks like a Pareto improvement. I could have used a different decision rule for both populations, improved my accuracy overall, seemingly improved fairness. Now, I'm doing similar things on the green population and on the orange population in terms of my error rates and nobody was harmed. Okay, so from this cartoon maybe I want you to observe two things. First, that unfairness - whatever that means - can arise naturally whenever you have two populations that have different statistical properties. Because optimizing for average error will fit the majority population typically to the detriment of the minority population, and although it might seem at first blush that to get fairness, if you want to avoid say, racial bias, you should not allow the decision rule to depend on race, sometimes the opposite can be true. Okay, so why did we think that this green classifier was unfair? Let's go through a little exercise to figure out what we maybe should mean by fairness. So one way you might start by asking that question is by asking 'who was harmed'? And one possible answer is that the people that were harmed were exactly the qualified applicants - the people who had positive, the ones who really should have been accepted - who were mistakenly rejected. Okay, and so that leads you to think about the false negative rate. That's the rate at which harm is done. If you think that the false negatives represent the people who were harmed, and so you might propose that perhaps what we meant by fairness, perhaps the reason that we objected to this classifier was that it had a very different false negative rate for the green population and for the orange population. Maybe false negative rate in the orange population was a hundred percent, where it was very low in the green population, and so you could propose that perhaps by fairness all we mean is that the false negative rate should be equal across groups and this is a popular definition. Now I want to dig a little deeper because maybe that's actually not quite what we wanted. Okay, and here's another toy example to see what can go wrong. This is a cartoon but again something that happens on real data as well. Imagine that we decided that we care about protecting groups across, you know, defined by two features. Maybe race and gender. So we've got men and women and we've got blue people and green people, and I say - okay I would like to find the best classifier that equalizes the false negative rate across blue people and green people and across men and women and suppose that that turns out race and gender are actually uncorrelated with label. Well, one classifier that satisfies these constraints is the one I'm showing you here that will, for example, admit people to college exactly when they are blue men or green women. The false negative rate is equal across the rows and columns - it's 50% - and yet this is cold comfort if you happen to be a green man or a blue woman, because for you, personally, nothing is promised. In fact you will be rejected with certainty. And so the problem is that equalizing false positive rates promises you as an individual nothing at all, because the rate - the word rate and false positive rate - is an average over the population. Okay, in order to interpret this as a promise to you, you have to believe that you are a uniformly random member of your population and if you know something additional about yourself like the combination of your race and gender, then it then it doesn't promise anything to you. So, you could try to formulate maybe what we actually meant which is that for any two people, okay, any two people who were qualified to get into college, the probability that they were rejected should have been the same. The algorithm shouldn't favor anyone over anyone else except by virtue of their qualifications. Okay, and this has strong semantics. You can study it by making, you know, strong assumptions and we have, but it's got a problem as well which is that this constraint is so strong that it essentially cannot be non-trivially satisfied unless you're willing to make especially strong assumptions that correspond to things like realizability. So can we get something of the best of both worlds? We had this sort of first attempt of equalizing false negative rates across a small number of coarsely defined groups. It was easy to to check, it was easy to maybe satisfy - at least if we ignore computational issues for now - but it didn't promise individuals much. On the other hand, we have this individual level guarantee that corresponds to equalizing false negative rates across groups of size one where maybe now by rate - well, we no longer have a group to average over, but we mean over the randomness of the classifier. This had very strong individual-level semantics but it was sort of too hard to satisfy, and so you could imagine a middle ground where what you ask for is equalizing false negative rates across groups, say, but not just over a small number of coarsely defined groups like race and gender but over exponentially many or infinitely many very finely defined groups so long as each individual group is not too small. So you can do this, it turns out that so long as your group structure is, you know, it can be large so long as it's not too complicated in the sense that it's VC dimension is bounded the statistical problem isn't so difficult to solve, okay. I can ask that I - I equalize, say, false negative rates across infinitely many groups so long as their VC dimension is bounded, but there remains now an interesting computational question, which is 'how can I learn classifiers that have this property?' It's not even obvious how to verify whether a classifier you give me has this property because I can no longer enumerate the protected groups. So you can do some math and I'll spare you that, but the upshot is that - what you can show is that learning the optimal fair classifier, where we now ask for these fairness constraints over infinitely many groups, is no harder than unconstrained learning, okay. Over the class you wanted to initially learn over, and over the class that's defining the groups. If you give me subroutines that can solve the unconstrained learning problem with polynomially many calls to these subroutines, you can solve the constraint learning problem. And once you've got an algorithm, okay, and in fact, this algorithm is one that you can't - you can not only prove theorems about, you can run. You can now on various data sets start asking what the trade-offs are between fairness and other things you care about, like accuracy. And one of the messages of this area is that you have to start thinking about trade-offs. We're used to thinking about things like fairness and other moral issues as binary things that we can have fairness or we can't, but when you have a precise definition you can start parametrizing it. Maybe i don't insist that false negative rates, for example, are exactly equal. Maybe I let them differ between groups by some parameter, and now if you've got an algorithm that provably finds you the most accurate to classify our subject to these constraints, I can ask - well if I'm willing to give up on a little bit more fairness, how much additional accuracy can I obtain and vice versa? Which leads to Pareto plots and what these Pareto curves look like are different on different data sets, but sometimes what you find is that they're very steep, which is good. It means that by sacrificing only a very small amount in terms of your accuracy, you can gain quite a bit in this fairness metric. So a couple of takeaways. First, in general, not just in fairness but in any of these issues, it's helpful to be precise about definitions. Mathematically precise, and that doesn't mean that we should expect to capture everything that is meant by words like privacy or fairness. One could never hope to do that with a single precise definition, but that's okay. Being precise about different kinds of privacy, different kinds of fairness give us a vocabulary to have a conversation to talk about what we mean by these things and how different goals have to trade off against one another. Once we fix a definition we need to explore tradeoffs, okay, there will generally be tradeoffs not just between a particular constraint of say fairness and accuracy but also between different things we might mean by fairness. And the scientific problem is not to decide where we as a society should live on these Pareto curves, but it's to understand the Pareto curves so that in different settings we can make informed decisions. And maybe most interestingly to this audience, this is a great time to do research in this area. The science is new and unsettled, it's important, and that makes it fun. So we wrote a book, it's not just about fairness it's about all sorts of other interesting things and thank you very much. *audience applause* 