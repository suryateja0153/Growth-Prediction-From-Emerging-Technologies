 well thank you for that introduction anytime you hear introduction like that you know you're getting old and you know over the years I have given about four or five hundred talks I visits little over hundreds you know departments I just say today when I walk into the department say wow I've never see a department has so grandiose and people are just incredibly nice and Nancy has been incredibly staff person and you guys just lucky and so I want to congratulate you first for being in such a great great department and the second I want to congratulate for living in Canada. You understand what I mean. So so what I want to talk about today is it's it's a kind of a slightly unusual talk I will be talking these three topics mostly on this election I want to show you the importance of data quality and that is part of my recent research I will talk about gerrymandering as well as the differential privacy protected census because there that creates a lot of new province that for people to for people to work on now these are all this things sort of in United States but there are statistical consequences are you know very broad and but before I do that I wanted to say a few words really in my as Jim mentioned that I'm now the founding editor of the Harvard data science review I want to say a few words about data science it's a question everybody's asked what is data science and now I can stop the talking now just sort of started debate with you what a data science is that could be you know kill the entire our stuff easily and what but I won't do that but I do want to sing some institution I do want to collect a little bit of data I know this is a talk in the Stat Department but how many of you are actually coming from outside the statistic and actuary science departments any of you okay all right so a few of you and welcome to this large stat community and because the data science itself has become really very broad so let me start by there are so many books now written on data size and this is one of them and this one is written by two computer scientists and their first page is what is data science now let me read a little bit because it's interesting instead of data science encompasses a set of principles problem definition the algorithm and the process for extract now obviously any useful patterns from large data sets all right it's not wrong many of the elements of data science have been developed in related fields such as machine learning and data mining alright right there all the statistics say we're statistics well let's read on if there's statistics in fact the turn date terms data science machine learning and the data mining are often used interchangeably the commonality across these discipline is a focus on improving decision-making through the analysis of data however although the data science powers from these other field is broad in scope machine learning focus on design data mining data science of things but there's no mentioning of statistics whatsoever okay I may say well that's just first page reading type book all right you will not find anywhere mentioning statistics except the one place and that not to be quite related to your department so not too long ago I was invited to attend a celebration of a birthday of a person I hope at least some of you remember him used to being this department Jeff Woo okay now some of you still you probably don't know he was here 1980 something - 1994 right and it was it was event to celebrate his 70th birthday I was invited to be the speaker for the banquet so this was my title is Jeff Woo a data scientist why do I do that well because Jeff now I don't know how many people know that he has been really credit for as one of the pioneers early on thinking about statistics called godean statistic into data signed in fact in this book he was credit it's this middle we found the book in 1997 he give a public lecture when he moved to a in the University of Michigan and he had a talk called the title it was statistics he could to data science with a question mark okay this is 1997 you had to think about that timeline right now at that time that Jeff's view of data signs then is not what we talk about the dsts science now because at that time he was really thinking about how to rebranding statistics to have this more broad emphasis on practicality connected with the practice sort of thinking about thinking about data science particularly to think about these large complex data so he won't truly visionary to anticipate this something is going to happen although I guess you know I talked to him that he didn't expect that the data science now become such a general term that it certainly goes beyond both statistics and computer science just for those who have not met Jeff this is was Jeff before he become a data scientist and this other Jeff afterwards so now I want to sort of say a little bit of few words about Harvard data science review because that is where I have been in the last one and a half years spend most of my time talk to all kinds of people really help me to think about data science as a broad that would tell you that it's not even single discipline in the end so the idea of Harvard Data Science Review is really coming out of inspired by the Harvard Business Review which is a popular magazine for all the fortune 500 company and others business leaders as well as the Harvard Law Review which is a legal scholar journal and so the when I when I start thinking about to have a data science review I was really thinking about this I wanted to pose I want to do the kind of scholarly you know deep end research I also want to reach out to the industry government everyone because that's what data science is now is not just academic a feud it's it's a very broad but on top of these two this is sort of my ambitious plan which I'm implementing it is I want also include education component where universities University a key mission of university is educated that's why most of you are would you're here so we'll be also publishing a lot of articles on data science education so this is a place where we publish research education as well as that kind of outreach public you know this so I wrote in a tutorial for the first issue summarizing the first fourteen articles in this journal and it's everything's online free okay so it's just Google it and the its you know everything is free so in this particular article at this editorial I emphasize this point I was using this term called ecosystem because I couldn't find a way to describe the design becomes such interconnected system with so many pieces that it's a really good you know that goes very very broad and in which particularly I define data science that part of being I had a trainee and pure mathematician you know in mathematics when we can describe something it's too broadly defined it as a compliment it's much easier to talk about what is not so I basically this point for this group I'm gonna go very quickly because it's sort of obvious first is I want to emphasize data science it's not just about machine learning and it's not just about statistics either way will be too narrow because you will see you know there are these things are we are we all we need all kinds of methodologies there are science is not just about a prediction although mostly in the interest rate prediction is very important problem but data science itself is really about inference about classification about a lot of a lot of other things that are science not just about a data analysis and that's the one typically for statistician we might have not appreciate this as much as people who actually doing dealing with these big data you know for anyone needing big data the dirty ones what they call that you know dirty data you're spending literally like eighty ninety percent of time clean the data are trying to make the work try to run anything the analysis part is really comes later so I think that's important for us to really understand this part as well and this is the part I'm started thinking about really more broadly I want to emphasize data science is not as discipline city merely within the stem field stem is for science technology engineering mathematics that's the traditional I don't know if you use the term in Canada but in U.S. that's quite a common term now we obviously you know obviously this is a very important part right computer science that it's statistics operation research or in stanfield but in a minute I'm gonna give you some specific example to really help you to sing about the data sign itself is really more broad than just think about as as a stem field I'm gonna tease you a little bit about this last point so this is the three articles in the first issue the middle this article was was done by a computer scientist but there's also article done by a social scientists as well as well as an article by by philosopher from you know from humanity and I want to just show you these articles in a little bit more detail to list this whole point about why I sing the data science it becomes too broad this was a article done by Jeannette Wing I don't know how many of you know Janelle when she is the computer scientist she's the director of the Data Science Institute at Columbia and she is known for this concept called a computational thinking in conscious to statistical thinking and this is the her view and she has a particular sort of way of defining what data science is but the most important she talked about this data lifecycle and for most of the probably seeing that this is pretty good description she talked about your data generation collections processing right processing and storage management analysis it's very important but it's only one of many visualization and in you know interpretation and allowing this whole way you also have to worry about all the privacy and the assets sort of consideration this could become increasingly important and for really good reasons and I'm gonna talk a little bit in a minute so when you look at this thing I think most of us would probably agree this is a pretty pretty good descriptions then and - I have a philosopher comes in and also of you know a pseudo scientist coming so this Sabina LeonelliI hope I pronounce her name correctly she is a she's a she's a philosopher and her point she actually wrote an article as well the way she think about this whole thing is that she emphasized like before you do all those things there is a really conceptual thought apart about data which her P point is that there's no such thing as raw data all right that's a philosopher's point the idea is whatever the data you have you collect the full purpose you you already think about how to collect them how to measure them a lot of things are that if there's a human construct or if they're the whole idea to emphasize that is you know how sometimes we emphasize oh it's a I you know it's you know I let data to speak right you know in the let data speed therefore it's very objective from philosophy philosophy point is that itself is never truly objective because there's always evil you decide what to collect already influenced essentially it's essentially has a selection component into it so that's sort of the from very beginning what's very interesting is that after you know the Jeanette's the whole home apart this is a Chris Christine Borgman she is a grueling of information science library science and her point from a lie lie boring science a perspective is talk about what did she call the after lives of the data which is very important these data we talk about open access open science right the idea open science is whatever data I collected I let everybody else to sort of use as as a part of it but if you think about that it turned out to be that it's not that easy just to use someone else data right you need to know how they collected how they document it a lot of things are about a data creation data province is actually not the typical things that we even teach but this is exactly what the what the librarians you know do there's about how how to preserve something historical others can you know can use so there's a all of us really scholarly approach to think about how do you preserve the data as well as with their entire record and this is a turn out to be also a crucial topic these days in this whole thing called reproducible replicable you know science because you need to check what other people do what are they doing and the most of the you know when you do studies you probably don't keep a record of everything okay I don't think anybody could it and they even sing about how to how to keep that how do you know that saw some notes like I made some decision along the ways these are all important in the end if you sing about this holistically as the whole process but the record how to record those things how to keep a record of the things I just are just incredibly hard right so this is the you know this is the sort of approach like sort of air after everything you talk about the the afterlife of data the real animation post these things post the philosophy was sinking and this kind of a library approach these are obviously very relevant very crucial for doing the data science and the whole but I don't think a ways that statistic department teach those things are not even in computer computer science department teach those same so you really have to think about data science in you know in a more pros away so my my sort of the punchline of my editorial was really to emphasize that by now the term data science has been evolved into a way in a way that the way I'm thinking about it is no longer a discipline by itself it's very much like an umbrella term so what do I mean by that so there's a consequence of that I'm actually really saying I know there university is thinking about creating department of data science I would say don't do that because you're being squeezing all those into one department it's not going to work well now the analogy I'm making here is you probably have all heard about their terms called a science and a scientists but you rarely see a department of science right could you you will have it you you will have a department of biology department physics right because it's that could become an umbrella term and you have social science as a social scientist you really see a department social science unless you're in a very small college sometimes they lump all the things together right the point then is you have the terms humanities and humanists right and you really see up department of of humanities the last one got myself was slightly into trouble I was telling people like you know there are engineers and engineers but there's rarely a Department of Engineering except I was giving this talk about a week ago at University of Cambridge and they do have a department engineer everybody was laughing but you know Department their department is really like a college literally like a college but the point I you know I hope you I hope you get my my point because the field becomes too big there's so many components that if you try to squeeze everything to one department what you would it would happen what will happen is you'll be teaching everybody a little bit of everything and the nothing really in you know really in-depth and so the way I've been thinking about this thing is really thinking about medical schools right Medical School you you you you do the training for the generalists but you also have also all sorts of specialists and yeah I think that's the right model to to think about because we need a lot of sort of data science generalist very much like a medical generalists that they look at the problem they can diagnosis they can say you should have see a heart specialist you should have see a lung specialist but they don't treat you themselves I know we need a lot of those people do those things but we also need the specialists we can't be all generalists we all can do a little bit of something but when I have real problem serious problem we cannot training them right and as sort of just to enter this part was this I have heard not too long ago I was in a conference called international dialogues on AI oh that's on other terms that is the hyper term and there's someone give this give this I don't know whether it's a joke whether it's what it's real the idea was that there this university that they were trying to get people to concentrate that this is what to recruit for undergraduate so they basically said if you want to solve all the problems in the world major in computer science and of course all the statistician gets very agitated by that like where are the statistics but just wait okay this is when they try to recruit people to the undergraduate then then they wanna recruit people to the graduate study and then they say if you want to solve all the problems created by computer science now enrolling graduate study at any of these field right if you think about this is actually pretty much true the reason we have all these problem now algorithm fairness accountability or stuff without the computer we won't have any of the AI and of data science I mean at least not at this scale so this is it's not quite this is not quite wrong I mean it's a humorless line but but it is like a lot of profit it's created by the computer science and the rest of us need to really need to solve it and I think that the right structure one of them at least is the Michael Jordan's you know being you they have led this effort at Berkeley started this thing called a data science division and I said them got it almost right is because they also have an interesting structure they basically have the university wide they have all the deans sort of report to this thing the only thing is slightly weird historically there are associate vice provost for this for this school for this division is also the head of the I school how many of you know what I school means information school so that's a kind of a slightly weird structure but that's a that's a historical thing at Burkley the basic point I'm making here is that if you really want to do something like data science these days like what the Harvard's doing other place doings we're pretty much create this structure at the university level okay you could that's where because the data science has permeate into into so many different different you you know you know different dimensions now having emphasized that data science is very broad now I want to come back to say we statisticians can contribute a lot right I'm not saying that I now it's it's just like anybody you know yes anybody can be more involved in data science but statisticians-- we can contribute a lot and particularly what I you know these are deep statistical thinking is really very much needed so the three problem I'm gonna talk about one is the predicting 2020 elections which and I I know you don't care that much but somebody cares and I won't talk about this in what I call the law of large population you have heard about law of large numbers but I'm talking about law of large population we explain to you what that means and I want to talk about little bit about the differential privacy with 2020s census and I think that that is something is going to create a lot of you know controversy which already already has created with there's a then there this is the same you know I know that probably not much here you have some I understand but it's a very intense in the United States essentially everybody trying to find the district than to get themselves voted okay voted it in so these are all being posing posed some really interesting statistic questions they are all you know they're they're not really much mathematics involved this one has a little bit they're a lot of things are very simple very basic ideas I'm going to talk about using simple Amin correlation variance concept but the statistic sinking goes a long way okay I don't think ever time to talk about these two which was I added one is about if I concur one is a deep learning how do you think these issues that you sort of as you know as a statistician but let me see if I can go through the first three okay so this was I this was a sort of a you know a picture from web the the night before the 2016 election at that time there was a lot of predictions of what's going to happen whether Clinton win I was very happy that this is not a Harvard this is not a Harvard okay first I was worried about that okay so but now as some of you know the five thirty eight silver ones was you know he was the one did the lots of things it was a very accurate a my here is 72 percent is much better than than the rest of them but 72 percent is still pretty high right especially if you if you if you want to win 72 percent sounds pretty good right now so the point I want to make and I want to say this is very important point is I'm not saying that there's anything wrong with each individual them in some sense because in any individual prediction we know could be wrong because you know as statistician the one good thing about statistics we never say anything for sure right so nobody can catch us to it is we always say 99% but this is 1% super high it could happen what get people into trouble is it collectively you see so many media's news outlets everyone give you this one thing like that you see that you know again and again of days nights weeks all the same message everybody would just say ok this is the reality right and but we all know what it really happened in reality right we all were we were all surprised by regardless this talk has nothing to do with ideology I have to make that very clear regardless who you vote who don't work dont like you always whatever whoever you are you you know I hope you want to know the choose right so this is a about how do we think about statistical evidence scale okay so in order to address this I want to take everyone back to something really old and really profound and I want to say why that was important and why that could mislead us the first thing I wanna say is a really the law of large number right and and some of you know the law of large numbers is initially proved in the simple form by by noting 1713 and what a lot of large numbers there's a technical definitions as you all know and if all the students if you do not know what the law of large number is don't tell anyone okay you will be kicked out of this department if you do so but fundamentally the law of large numbers sad right they're just you take a sample small n there's a large population the capital n is a large population and in the theory essentially the capital n is infinity the idea is that if I keep sampling we can eventually get a right okay that's all of the law of large numbers saying in term the average now mathematically is much harder than this but that's there that's there in the essence of law of large numbers that's not surprising at all that's you know you probably don't need a mathematical for most people would have believed that what's more interesting is that the law was central theorem and again if you don't know just don't say anything 1733 and for me what's important in theorem is not just the bell shape the bell shape was obviously important that's fascinating but there's a second message there is saying the error rate goes down as a 1 over square root n right as a statistician we all know this is by definition no no just a by default we all understand that now that's not obvious right that's not obvious that said if you want to increase the you know if you want double double your precision you need a quadruple your sample size and that kind of things like if you without them mathematical art why is that right now we all understand but here's the problem though now I know that none of you who sit there would be questioning this right because this is this thought for all these years but if someone walking that who had not studied statistics whatsoever and if somebody is smart enough look at his problem say that something here is bizarre right because this error rate is a completely independent of capital n that seems a bit bizarre is it. Should the arrow be related to the population size should be the case that the larger the population the harder it is to estimate what makes us just to ignore the the capital n in fact anyone who studied finite population survey literature you will see you look you know there is a there is a code of finite sampling correction term and the advising Journal is that when n is large enough ignore that that's all just the complete opposite right if n is large you know don't worry about so ok so this is a question ok now I wanted to bring back a little bit also history on on a survey this idea that take a small sample can infer the population mathematically you know was understood on you know long before it was actually being implemented but when the idea in 1895 this was actually during the I think the it's the ISI the Institute of Statistical International Institute a conference 1895 by Anders I can never pronounce this right it's Kiær he was a statistical you know founder when he proposed this idea using a sample size of n to infer a population size of capital n this was later described as intellectually violent a revolution if you think about we now all take that for granted but if you have never learned these things I think about like U.S. has 320 million people right and the idea that you can sample five hundred five thousand people or even 500 can learn 320 million people initially everybody say you're you're insane alright how could that possibly be okay and in fact this idea really was not just got accepted immediately until really pretty much into you Maurice Hansen at the US Census Bureau when this was implemented in 1940 in in a sample in sampling and a once this happens as you probably all understand once they become you know a government implement this and this becomes really a standard common standard right now these days if you do the social science work this is a doing surveys in questionnaire that's the sort of basic methodological approach right but it really took about 50 years to to really become this popular approach and the reason I mentioned is this this is what I could talk of a little bit about that 2020 census about the French privacy what I was thinking about that's another intellectually violent the revolution is going to take place next year but so let me first try to explain to you let's say someone do not understand this thing say well how can you explain to me why I can take you know a such a small sample to really make inference of a population regardless the population size well here is a very easy way to understand this thing and I think particularly I see lots of Chinese here any students here we all love like wonton or noodles or soups right okay suppose you are asked to taste a soup is a it's a salty enough or is it delicious enough right what do you do you mix it super well and a few spoons is all you need right and regardless whether it's a tiny soup or it's a gigantic soup right the container makes no difference after you mix it well is it clear after you mix well that's all you need is a little bit okay and that's pretty much is why this isn't working the whole idea is is a homogeneity right if it's a homogeneous it's Liberty one so now in real life you can't really mixing you know people well what the statistic is called a randomization right the whole randomization is essentially a physical device to makes everything representative okay so as simple as that but here is one problem though and one problem that we all have ignored this too long and it's coming back to really bite us right the progress doesn't even go back to think about physically think about physically you stir a small soup as long as not too small just much hard to a soup and a jagged bucket right it's much harder to stir a really big ones right but once you mix them well you assume the way the whole problem so now everything else we do is saying okay assuming it's well it's a statistical representative then let's find all the statistical errors right but then that this way or that size of capital n sort of goes away so my point here is that what I'm gonna present to you is that once you take into account that it did not stir well then you will see that as the thing is much much trickier okay which that statistician is known it's called a selection bias okay now response by selection bias what I've done here is to quantify this being less lack of had you know homogeneity how much damage it can do is that clear okay that's what I would do all right so in order to do that that's why we couldn't need the black ball there but let me first show you the real data from the 2016 2016 election so what I show here is a z-score because after there's a 50 states plus the DC there 51 data point these are the abbreviations of each states what I'm plotting here the the y-axis here is the actual difference between what was predicted by survey what would actually the vote divided by the standard deviation pretend it's simple random sampling so it's like a standard z-score okay so on the left side the Y side yes that's the z-score here is the log of the total number people turn out to vote in that state so that's the population size how many people actually voted okay what you see here you see this interesting phenomenon this is the traditional plus/minus to 95% bend okay you see what happened is the arrows are getting larger and the larger as the stay size and the more people vote the more arrow was in that prediction this is quite different than the traditional way of thinking about it's a small sample cause more variation now here is the large population size the more the look the large the population and this is the data this is real data you can anyone can can make this plot so I did not discover this I was doing theory this was the theory tells me that's going to happen then retrospectively I found the data and this actually really happened so what I'm gonna tell you is what the theory behind this that why this is happening okay so this should give you insight about this is what I called of law law of large population large population is going to destroy you because the capital n is coming back okay and I'm gonna show you how this works so what I need here is what I call a funament to a trivial by the fundamental identity and that's where I need good blackboard to to illustrate a little bit imagine the following okay that each of you I'm gonna ask each of you pretending you're in United States I'm gonna ask each of you like you know are you going to vote for Trump if you say you can vote for Trump one okay if not you say if you will be voted for Trump is one and if you do anything else including say vote for somebody else I don't want to tell you whatever it is is zero okay or not I don't want if I have not just decided everything is zero so I just just dichotomies that the form yourself does not require that that that be binary but it's much easier to to illustrate that now the the next quality is the one that it's it's entirely trivial for any of us have done survey literature research or in this sort of missing there the literature but it may not be familiar for others the idea here is that each of you also has what I call there's a response indicator if you tell me your answer whatever your answer is and if assuming you tell me honestly if you if you don't tell me honestly that's an entirely different story okay that's a much harder problem if you tell me honestly your R is one but if you refuse to give me answer you just just refuse right and then the this R is zero so clear so each one of you have two variables your true voting opinions and your response behavior whether you do or not because each one you have the two variable there's a correlation between them just simple linear simple Pearson correlation so here the formula okay and this formula may looks a little bit a little bit mysterious and I'm gonna show you here very quickly because it's very easy to show it but this is the fundamental formula that is going to tell you that telling the whole story here is the difference between the sample mean how many people in my sample tell me they going to vote for Trump this is the one I want to estimate this is the population mean it turned out the difference between them can be written as the correlation between the X and R how correlated is your answer to your reporting behavior this is entirely determined by the size of your you know your sample relative to the to the population and this is entirely determined has nothing to do with the survey this in the population how opinions varies okay I'm going to show you this is a this is a mathematical identity no assumptions required but let me first tell you what are the consequences of this thing this tells you that the or statistical error actual error is determined by three quantity and only three quantity first let me go this way is what I call the problem difficulty index if everybody is of the same opinion you just need to take a one person you'll learn everything okay so you can see that if everybody has the same opinion this is zero there no arrow so the larger is this standard deviation other is the problem and we understand that right the sigma are that's a sigma and this part is also pretty easy to understand right cuz you know if I have nobody in my sample this goes up because nothing to estimate if everyone in my sample everything the everybody in the population is in my sample and they responded honestly this is zero right okay so this is contused the quantity so this control the problem difficult this quantity this is the relatively new term and this is the one I'm gonna talk a most most about it this is what I call the quality quality index because if it's a really random sample whether you are in that in my sample or not is not determined by your answer is determined by a random device but if after you I take you and you decide because you don't like other people to see your answer you decide to refuse to answer me you you inject this correlation the larger this correlation the more bias it is is that clear so this is what I call that data data what I actually there's a term I use called it that's a data defect correlation but this is the this is the one explains all these selection bias it's all loaded here because this R essentially could include all kinds of reasons why you get to exclude it but collectively everything is this term okay now let me show you very quickly where this formula comes from because this is actually a kind of a slightly trick but it's a you know it's a very important one typically we write this one usual right and something like this right I equal to 1 to nxi right that's a typical way we write that's the sort of sample mean but in this calculation that you don't write this way you write it in terms of the population average you you put everybody in but you multiply by the indicator if it's a 1 it's seeing if it's a zero it's out right then you divide it by the total number of people who are in which is the sum the I right okay by doing that what you get is if it if you think about taking average is like an expectation with respect to the index uniform index this become index expectation of R times X divided by index expectation of R right and then you - so this is the XN R but the cap to XN R is simply the index of the entire X itself right so now you see the difference between them if you do the difference between them what you get is the difference between them is this saying now I hope you still remember how to take the common denominator if you take a common denominator the denominator is expectation of the R but then now the numerator is the expectation of the product minus the product of expectation and anyone who have studied statistics again if you don't know tell don't know if don't tell anyone that's the covariance right so now you have that is the that is the covariance of R + X divided by this mean of R provides its covariance you standardized by just standard deviations right and because R is binary you've got a P times 1 minus P kind of binary formula that's why you get the correlation then you get everything else okay is it clear now in real life before the election really took a place I cannot calculate this thing from the sample everyone in my sample I could do one right but after the election it's a beauty of the election I would know what that what the truth is I can back calculate what this calculation is I can tell you and back calculate how much people select it to respond okay and that's what I'm gonna use the  Sigma data to show you you know what was the correlation there there definitely there was a correlation but without a correlation we would not have any surprise ok any is it this clear this is the fundament if you don't remember any that's a formula you should remember okay because this is going to help you to understand all these things so here's what I did I apply this to the 2016 election data and this was the data that was the survey data was conducted by actually YouGov and it's called a corporate congressional election study Stephen Stephen Ansolabehere he is leading a scholar in the America election he's the one behind the CBS calling that the selection that election night he's the one behind the scene calling everything he's my colleague at the government department at Harvard and he is the key that kind of wonderful colleagues that when he I know you have six six positions to hire this is the kind of people you want to hire because when I asked him to give me the data he not only gave me the data he gave me his assistant that's kind of colleague you want because it's wonderful alright so actually I wrote this whole article this is online you can get it all the formula all those things is in the article but here I'm gonna show you what it really happened okay so remember I can do the back calculation roll into the z-score okay so once I have this thing I can do that to the calculation here is what happened this was people to answer predicting Clinton's whether you vote for Clinton or not okay taking Clinton as one and anything else including not make a decision is zero right over the 50 states plus DC you will see this correlation is pretty much centered around zero you never expected exactly zero right because there's always a little bit estimation but a spread in this way okay you can see the standard deviation is only another order of a you know you know actually this number this number actually is I don't know that this is fine yeah so this is for Clinton okay here's what happened for Trump see the thing is shifted so now it's a centered around about 0.5% 1/2 percent of correlation keep that in mind the correlation with a half percent you can't publish articles say I found a correlation half percent most time you know you just can't okay they will reject you all right but I'm going to show you how much damage this point five half percent of correlation has caused is that clear so this is from real good okay so now here what I do now I'm gonna ask like what is the consequence I have this half percent I'm going to conversing into something called effective sample size okay so the idea here is that now I know there's a data set with this half percent correlation which I know is bad but I can have another data set which they have no college I want to ask Oh a dataset with 0.5% correlation in terms of mean square errors predicting this election outcome what's the equivalent sample size if I did not have that correlation that gives you a sense we know that it's going to reduce the sample size that's because the error but how much reduction is there so that's the that's the calculation all right so here's what I did what I did is to say okay let me or you you can just do the factors up you you match these two mean square errors you can solve for it and you can get this expression it's a little rich is one of the rows square the coalition and this F is the relative size not the absolute size okay so this is actually also important to tell you when people talk a big data being this kind of population inference problem what's big you should ask is not the absolute size is the relative size how big it is how many how much percentage of population you cover okay now so here is a little bit of the thing I did is I look at about two to three months before the 2016 election and every day you will have 20 30 sometimes 50 surveys somewhere so I look at all of them say suppose I put all them together how many people's opinions do I actually have I did a rough calculation probably is over estimation it's about one percent of the voting population it's about 2.3 million people okay now to put something in perspective 2.3 million people means what means about 2,300 polls and each of them has thousand people now imagine you have opinions from all these places that seems a lot right I have 2,300 pools each has a thousand suppose you're working for any of these candidates you know campaign if you say I have this many if they all tell me one thing I'm pretty sure is I'm gonna lose or I'm gonna win okay but now I am going to put this into the formula F equal to 1% Rho equal to point 0.5 someone should be able to do very quickly guess what's the answer to this thing part of 400 and that is not a factorial sign okay so what has happened here is that that point five percent of correlation actually destroyed 99.98% of the sample size you just had a tiny bit of samples I'm not saying that 400 data point is not useful a good survey with 400 tells me a lot but your confidence should be entirely changed you're not gonna say I'm done if I tell you that I you're the worst of your answer statistically I can show you mathematically is equivalent to a very good survey with 400 what any campaign stop saying I'm done with the 400 no but when you say I have 2300 each with 1,000 people that sounds a lot right so I'm showing is that that this what this says is the trade-off between quality of the quantity is very extreme once you don't stir well it's actually the cause you know if you have some chunk of something somewhere it takes a lot larger many supposed to find it then just sort of a few tiny spoons okay and so this is what I what I've been calling the Big Data paradoxes if you don't take data quality into account the bigger the data - sure we fool yourself right because what happened here is when the data size is very large all these conferencing - will become very very tiny right because the whiz is the inverse with a square but if they are centered at the wrong place so what happens you guarantee yourself never get it right and the more the data that the tiny these intervals are right in fact that is the law of large number because it's it's a law of large number subsets I'm going to confirm confirm but it's confirming the bias if you don't know how to shift it right and another way of thinking about it see this is what I call the law of large population if the z-score here is written as Rho times this thing so you can see if the Rho is 0 it doesn't matter what that's what the population size is why do distorting well is essentially make the Rho zero or technically is making the Rho is 1 over square root n that kind of rate so this will be on all the one or two that's what the z-score is it like one or two but once this guy is fixed because the self-selection kind of bias then the larger than population the more the bias gets show up that's what you see why these big states started having more more so this is what I caught that they did so another way of thinking about this this is really the lot is another way of thinking about this is really think about if you're gonna aim in the moon like a tiny bias tiny arrow here as you go away by then then is it gonna be multiplied so you will never see the moon right okay so what I'm hoping is people really start thinking about thinking about this Rho and to think about paying attention to this large states because it is these large states that kind of selection bias is going to cause you know real problem but now let me in the next ten minutes to say very quickly these two other issues which is incredibly interesting one is this whole thing called the differential privacy for 2020 census is what I think it's should be labeled as intellectually violent second revolution what do I mean by that this this is a John Abowd he is the associate director of Census Bureau leading this whole he's like a technical leader of the 2020 census in a very simple term that 2020 census is going to be released that you will never see you don't see any real data because they are gonna adding noise to all the data they released except that I think the federal total and each state total and that's the constraint okay so what is what is this constant differential privacy there is a technical term by it let me just show you what this term is this is a slightly a little bit mathematics the idea is say whatever you're calculating let's say you're calculating a sample like a population as calculating a sample mean and the idea is that I can calculate sample mean with a person in it and I can take out that a person to read through the calculation and if statistically these two things in terms of their distribution only differ by a tiny amount you cannot tell who was in who was out that's called a differential privacy it's literally a derivative idea like a take a tiny perturbation see how much it changes on the distribution if the derivative is small and fine okay so what they do is now the census bureau they will they will choose some epsilon it's a lot of discussion how they choose that then basically there will be injecting the noise into whatever these these are accounts they are going to produce to make sure that this is satisfied okay this called it the differential privacy protected sensors now you will see what's the controversial here there's already outcry from the social scientists say well now you're gonna give me all the data is to analyze they're all have noise in it right and how do I know these noise are not destroying right what's interesting is that okay so this is a this is actually the Cynthia Dwork my colleague computer scientist at Harvard is the one is the leading act leading scholar in this whole idea of differential privatizing she's the one put down the idea is really not that new but she's the one put down the sort of mathematical foundations and to show how to do that typically adding like a Laplace arrows to show mathematically why do you things are falling so this is clearly a trade-off between information and the privacy the previous revolution is much about information and a cost trade-off right the idea our own take a little sample instead of whole population is a tremendous saving into human resources everything so so these are in real life these problems are always about a trade-off but now there's a different trade-off but what's interesting here is that you know the US Constitution require you collect data accurately and also require you protect everybody's privacy laws are law now the rest of us in this struggle or how do you actually implement that so that's the question here is what is the trade-off here this is actually the deep connection with statistic even this idea itself this is like a remember some of you know the random response you don't you flip a coin if it's one side you say the choose if not that's kind of related there's a whole idea will robust a statistic it's very much related if you think about you know there's enough error a noise edit there's also something which I don't have time to talk about it's also very much it's the idea of taking derivative with respect to to our data now with respect to parameter into measure selection in fact but what I also want to mention is that this is a first day it's already has a lot of reactions this was a article written in the New York Times about you know this causing people panic say to reduce to reduce privacy risk the census plans to report less accurate data the person who wrote it is Mark Henson who is the statistician I I wish mark did not use the word less accurate but rather less precise okay there's a difference between them right but this opened up a lot of research problem I said people talk about low-hanging fruit I said there are sorts of fruits on the floor you just pick them up robust estimators influence function the traditional taking derivative is the file it's all very much there there's another area I know some of you work on this here as well it's called a whole imprecise probability this idea essentially think about these everything bounds essentially you're creating bounds on probabilities so in this thing on this whole literature are one of my colleague are one of my former students wrapping has been just wrote this recent articles very interesting ABC's is called a approximated basing computation so what she shows that when they do a process of basing competition for the purpose of doing the computation that it's equivalent to doing exact computation for the approximate data so there's a so you can use this tech technology from ABC to do this basic competition now with you know with the noise data the idea is that you either add a noise adding the noise at the data level or you add a noise at the sort of computation level there is a correspondence to them the reason I mentioned this is trying to encourage every one particular students that there are lots of low-hanging fruit that to work on it because the data has not been released yet but we're anticipating that's you know that this gping is going to happen the last one I want to talk of very briefly is this whole idea of a redistricting or called a gerrymandering and I think that probably these these picture tells all this is actually from Chicago this is actually a Chicago council election the whole idea is that you know we have all these different parties where Republican Democratic and everybody wants them to have more more winning vote but you know how do you restrict these district is very important because if you're in a place which is your forty percent you could be in many places you're all forty percent but if you connect all these forty percent into one district is all your voters then you become like a lot more people right so what happened is that you create because of that depends on which party is in control you you see these kind of districts are gonna create it you look it's like a ridiculous like why did they avoid all those things right and this is not this is not the most ridiculous one you have see some of them we like a snake you know they go right and now we said well what's the regulation here what they are regulations but these are regulations are vague these are written by politicians and lawyers they will say for example the requirements say this district needs to be relatively compact now what does it mean these are not a mathematical compactness but not a complexity you know it's not compactors so how do you implement that how do you implement this sort of relatively uh so here is what people have been doing right people say okay now how do I know this one is too extreme imagine I have a mechanism to generate the many many of these district okay and on each district I'm going to calculate for example what's the percentage of the Republican vote in this thing so I can have I can have a calculus statistics from any district plan then I can get a histogram right I can get the histogram of I basically simulate the Northeast distribution right all the possible plan and each one has a numbers then I want to see like where's yours it's very much like hypothesis testing right is it the in the tell okay the only make this whole thing so interesting is does not agreed upon an  distribution there's nothing like you know the parties would agree the everybody agree some regulation is needed they don't want everyone go go complete crazy because they know that you know when you control you can go complete crazy but be careful next time somebody else included so they all need some rules okay they want to have some loose in place but these rules are not enough to guarantee a well-defined distribution and nevertheless they want to do things like this so there's a whole group of people which is mostly a mathematician and they don't lot of an MC MC and I call it more complicated MC so this is a module Qing from Tufts University and this is what this is an illustration what what they do is they create is this MC MC is like they change that the naive ones they basically change one district at a time one neighbor at a time so you create this Michael Chan not not of classical statistician in what we do it is we have a stationary distribution then it would create a Michael Chan for them is just construct a rule alright so these are different different people construct different rule and this is created by Moon Duchin group you see they're loose like in terms of same and the number of iteration is starting point these are rules are not even getting close but this looks much better like in terms creating things but how do I know this is actually correct right this MC MC rule can get stuck somewhere right so but nevertheless they have been using these kind of rules for example they show this they wrote an amicus brief for the Supreme Court case happened not too long ago they they show like these these are like a distribution created by these these simulations and they show how these different parties creating these plans they are all like in the tail and there was there was a judge suggest some plans which seems like a much more neutral now these are all fine except that I you know the lawyers are going to debate in the court if I'm the defense lawyer or not defense not as defense the other side lawyer I can a question like where did this distribution come from what's the theory behind cuz there's right and then and but nevertheless what happened is these kind of saying actually is used by this is this is you know Supreme Court judge Kagan and she wrote this she acted this dissent in this case but when she used that she actually said the approach which also has recently been used in Michigan Ohio litigation begin by using advanced the computing technology to randomly generate the large collection of district plan that incorporate the state physical and political but these are things are used in a court as evidence right for you know for judges these are all advanced statistic computing but all we're doing is this MCMC now so this creates an incredible interesting kind of a statistical problem which is very much related to this imprecise probability I want to say a few words is essentially you have situation the normal distribution is now well specified but there's some restriction to it and in that case how do you say somebody's a plan is too extreme for that kind of vaguely defined okay because nobody knows how to define this thing it's not subject to debate but it's statistically that something can be done so this goes beyond the traditional hypothesis testing against an or distribution so you can think about maybe there's a you know a class of the the second or distribution they all satisfy some notion look vaguely a sort of what we call the relatively compact and but if you can show that this point you create is extreme against this entire class maybe that is a strong evidence right because you know some of the plan is going to be so ridiculously bad but that the thing is like they in the court so interesting to coverage statistician usually don't get into the point that they need something you cannot say that that looks terrible to me right that's not a scientific evidence they want to create a some kind of wave to think about these scientific evidence to say what this is really going too far or this is acceptable but that kind of problem I think we need a lot more of us to work on because there are they're pushing the boundary they're putting about it in the sense that they use the kind of a properties this is a very much the MCMC property calculation but they're in this domain of being very vague and a lot of real-life problems is being just very vague but you still need to do something about it so that's how what I'm really hoping that at least I can motivate some of the students to working on these are much more challenging problems and in the end there's not even unique answer in the end you probably will be the best you can do is to be a court expert to go there to testify now that that induced the entirely different different issue of Education is you need a better communication skills because to to argue with all these lawyers so but the thing I will show these problem is like there are a lot of deep statistical thinking is needed and these all now belong to this big domain called data science right but we can you contribute a lot and I am not talking about anything today about you know la Sol's any fancy stuff but you can see the basic concept of bias variance correlation if you think of it in the right way goes a really long way thank you very much [applause] 