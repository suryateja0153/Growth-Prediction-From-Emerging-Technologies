 hi everyone welcome back from the break thanks for joining us today my name is joshua and we're really excited to show you some things that we've been working on sarah bird is with the azure ai team and she'll be talking about protecting privacy and confidentiality with ml i'll be watching the q a section for questions while sarah presents so please ask questions and make this session interactive and with that sarah hey everyone thanks for joining our second session on responsible ml in this session we're going to talk about protecting privacy and confidentiality with ml so we have a a variety of capabilities that we've been developing in order to make it easier for developers and data scientists and organizations to develop machine learning more responsibly the first set is what we covered in the previous session which is about analyzing your models and being able to gain a deeper understanding in models in this session we're going to focus on the technologies that we have available for protecting people's privacy and being able to develop machine learning more confidentially finally you can check out our our websites and some of our additional on-demand resources to learn more about our last pillar which is how do we control our machine learning process so you can have complete control over the end-to-end life cycle and the resulting systems a lot of these capabilities are new they're being very actively developed both in the research communities as well as in practice however there's a real need in many of these cases too think about these issues right now while we're developing machine learning and so we wanted to develop our capabilities in the open source so that we could move rapidly we could work directly with the research community but we still want to expose these capabilities to customers developing machine learning uh as soon as they're ready and so we take our open source libraries and then we integrate them in our platforms like azure machine learning so that customers can easily find them during their development process and use them at the right points in this session we're going to talk about protecting people's privacy and their data so one of the questions we need to think about when we're developing machine learning or doing analytics is how do i use this data without revealing uh private information you may not realize it but when we use data for example to develop a model we may actually accidentally reveal information about the underlying data set that we weren't expecting for example i could have a smart reply model where the model actually you know memorizes some of the data points in the data set that occur less frequently so if i type a rare sentence like my social security number is the model might autocomplete with a social security number that had seen in the data set which could be a significant privacy violation and so i really want to you know be able to use data but i have to think about privacy while i'm doing that so we're going to dive straight into a demo here to just kind of better understand uh what i'm talking about so from my demo i'm using azure machine learning because i really like to use the compute instances which are a managed vm that makes it easy azure machine learning just updates the vm and i can just interact with it as a jupiter notebook so i'm going to jump over to my jupiter notebook here and actually i'm going to restart my demo here so we can see it from the beginning so in this case uh what i'm doing is i'm going to make a synthetic data set but it's inspired by the a type of report that the census releases called the poom's data and in this case what i'm going to do is i'm just going to generate a synthetic version of the data here as if it were a published report and i want to take this data and i want to do an income analysis i want to understand how well is you know what's the distribution of income for individuals at a given education level so in this case i'm going to click through some different plots here and here's a histogram i have of what the income distribution looks like for individuals with a high school degree and great now i can take this i can use it in my uh to make a machine learning model or i could use it in my analytics or to put into a dashboard however it turns out that particularly if i combine this type of information with uh some additional information that may be public i can learn a lot more from this graph than than people realize and so what i'm going to demonstrate here is how i can actually reveal more information from that that published result so in this case i know a little bit more information about two people in my data set my data set has 500 people so it's not a significant percentage of the data set but i'm going to take an off-the-shelf sat solver and i'm going to use it to try to generate a data set that lines up with the published report so the sat solver will will find a satisfying result if it can find a data set where that's consistent with the statistics that were published so we're going to run my sets over here in the notebook and it was able to find a data set that would line up with the published results and since we know what the the real data set looks like we can compare that reconstructed data set with the real one and see how well is the attacker doing so in this case of the 500 individuals in the dataset it was able to estimate 126 of them within 5 000 of the uh the correct amount right so that could be a significant privacy violation particularly if i know even more information and could be more sure about which ones i got correct hey sarah there's a question coming in about that demo yeah uh so the question is you mentioned that this is a sat solver so is this a database attack tool and how do people get it yes so basically it's just an off-the-shelf stat solver it's not like specifically designed to attack databases uh we're just using it to demonstrate how you could use it to satisfy constraints and basically reconstruct the data set so it's just z3 i just did a pip install and there's nothing particularly fancy going on you can also check out this notebook in our github repo under attacks where we have a lot more explanation of what you're seeing so if you really want to dive in and learn more about what i just demonstrated you can check it out there so with that since i've been mentioning our github repo uh good news um there are some promising technologies that can help address this problem so i'm really excited to talk about one of these which is differential privacy and the idea of differential privacy is we want to hide the contribution of any one individual in the data set so that we can't pull it out and reconstruct that private information like i just demonstrated and so the way that differential privacy works is we're going to to do this with two steps in the first step we are going to add noise to the output of that computation and that helps hide the the contribution of any given individual the second thing is we need to calculate how much information was revealed in the query and subtract that from our privacy loss budget so that we make sure that we're not revealing too much information that then could be put back together so i'm very excited to announce that as of today we have released a new open source system for differential privacy with machine learning and analytics the system is called white noise and we have been developing it with our partners at harvard in the institute of quantitative social science and the school of engineering and applied sciences as part of a larger initiative and so the way the white noise system works is you can put it on top of many different types of data stores or data sets and then you query through the system and we will manage the adding noise and calculating the budget so that you get it in as a result statistics or a machine learning model that is differentially private so you have strong privacy assurances and don't need to worry about the types of attacks i just showed you so actually let's go sorry i'm clicking around too much let's go back to our demo here and uh what i'm gonna show you now is uh the next part which is how do we use that system white noise that we just talked about to to actually protect people's privacy so in this case what i'm going to do in this cell is i've just regenerated those same reports but this time i'm using the white noise system to output the report and so it's going to add that differentially private noise so that we can protect individual contributions so then the second thing i'm going to do now is let's go back to that same sad solver and let's try to do the same attack in this case we get a very different result right we get unset which means it wasn't able to find an a data set which lines up with the published statistics which means we don't have any guesses at people's income which is great however it's not enough just to protect privacy but we could also do that by not using the data at all and so we also want to look at how can we actually use this data to still do the analysis we were doing before so i'm going to click through and i'm going to generate my histogram here and in this case uh we have results right and uh the the private results they look a little bit different than the non-private uh they're similar in some ways but but also definitely a bit different and so you can see differential privacy working here and we can take these results and use them in our our downstream machine learning or analytics hi sarah so there's some questions about um the the results there like that middle bar it looks like it's like there is no private answer um and so is this the sort of thing that you'd get different results if you ran it again and is there an explanation for why some of those results are so different yeah um you should check out the notebook in the github there's uh you do actually in this case since we're generating the the synthetic results you get different results every time uh so sometimes they're a bit more accurate or not and what we're really seeing happen here is i'm using an extremely small data set as i said only 500 individuals and the reason that we're using such a small data set uh here was to demonstrate that attack in the notebook however uh differential privacy the you know for smaller numbers you're going to see more noise added which could be okay if the particular analytics that you're doing can tolerate that amount of noise however if you're using a larger data set it might be that the noise that's added is in fact basically negligible so if we look at our utility curves i have here what you can actually see is i'm going to generate two different plots so this first plot is looking at the accuracy of the private and non-private results for the larger groups in the data set which are just over 100 since my data sets not that large and you can see that the results are somewhat accurate i'm you know not perfect as we just saw in the histogram however if i look at this other chart you're really seeing differential privacy at work here where what we've done what you can see is like the results are all over the map for for small groups and uh the reason is we don't actually as far as privacy is concerned we don't want the results for very small groups to be accurate because that makes it much easier for us to see their contribution in the data set and so we're seeing that the system do actually what it's designed to do here which is add noise to to hide individuals so it's really a case where what we want to do is use it for those larger scale sort of questions definitely larger than the the size i have here in the data set so i'm going to jump back here and i'm going to go and say you know differential privacy is an emerging technology and so we're developing the white noise system as part of a larger initiative that was started by harvard which is called open dp open differential privacy and the idea is that we want to create a community around these technologies so that we can actually develop them push the state of the art and and really try to make them work in practice so that privacy technologies like differential privacy can be used in more problems so a lot of you are probably uh worried now that you're going to just you know have terrible quality data after what i showed you but uh the vision here really is that we want to use differential privacy to allow us to actually use data that we are not able to use right now due to privacy concerns right if you imagine a health data set it might be that we want to to be able to do machine learning on top of that in order to better understand you know some of the key questions in health care however and and that can really help people however uh right now you might be very concerned to let hard like anybody even use that data for for fear of leaking anyone's individual health care record which would be a very bad outcome and so this is a case where we really see the promise of this technology as enabling us to to use more data to solve society's hardest problems so i'm going to shift gears a little bit here and talk about another class of emerging technologies we have that uh help you do machine learning more confidentially so in this case we're investing in a range of technologies to allow you to have higher levels of kind of guarantee of confidentiality in your process so the most straightforward one is the case where i want to build a model but i don't want to let the data scientists see the data maybe [Music] maybe to protect the the data scientists to to protect the confidentiality of the data and so um in this next year we'll be releasing this in azure machine learning where a data scientist can generate a model and then you can actually have the azure machine learning train it without giving the data scientist access to the data we also have capabilities which you can learn more about in our on-demand videos in channel nine which are using basically doing encrypted machine learning either uh training a model that's encrypted or or doing inferencing encrypted which is leveraging the hardware built on top of azure confidential computing however today i'm going to talk about a technology that you can use right now which is homomorphic encryption and the idea with homomorphic encryption is i want to do machine learning inferencing and i want it to be encrypted but i don't want to rely on hardware like what we were just talking about so in this case um you can actually do the computation while keeping the everything encrypted so i can add encrypted one to encrypted two and get encrypted three back and so we have another open source library that's been out for a few years called microsoft seal that has many great implementations of homomorphic encryption algorithms and what we've released today is the some sample notebooks showing how you can use a seal in combination with azure machine learning to do this homomorphic inferencing so in this case i can send a prediction to the model but my request is encrypted and the response is encrypted and we didn't have to unencrypt it to calculate the the inferencing so you can get your prediction back without ever having exposed the features that you sent to the model for prediction so we have a lot of great resources that you can check out if you want to learn about any of these technologies i can go to the white noise github we have seal we have those notebooks to show you how to use an azure machine learning and i we have a lot more sessions if you want to learn about aiff build including additional sessions on responsible ml for analyzing your models and also for interpreting your text models so with that i'm really excited to say that our partners at harvard that have been developing the the white noise system and the open dp initiative are going to join for the q a so you can ask live questions to any of that so gary and sulil welcome and with that i'd love to open it up to questions thank you sarah so uh so we'll start with uh with the first question um you just showed two different ways to protect the privacy of your data and so the question is uh why would somebody use uh one versus the other are these like competing technologies yeah it's a really good question and um the answer is there's a lot of innovation happening and so uh people are exploring both in research and practice uh a wide range of technologies and each of them has a little bit of a different shape and so they they solve in some cases overlapping but slightly different problems and so the right one for your particular problem uh is is going to vary on the the parameters of your problem and how much overhead and what threat model you want to look at but in the case particularly of the the confidentiality versus privacy uh the confidentiality ones are really aimed at keeping that that process or that model confidential but it doesn't say anything about what happens when you use the model so if we think back to the smart reply example i gave you can still have that same thing happen with your encrypted model right the response could still be giving private information about the underlying data set where that's the problem the differential privacy solves and so what we see in a lot of cases is the most sensitive applications you're really using a combination of these right you want to keep the data as confidential as possible while you're using it for training and inferencing but then you also want to use privacy measures to make sure that you're not revealing that data when you go and use it awesome thank you okay so uh so another question um is opendp approved by government regulators um and i'll i'll direct this to gary i guess hi everybody thanks for thanks for having us um uh well um differential privacy is a mathematical equation so they don't approve mathematical equations but as a technological solution to a political problem it actually does work in fact it's pretty much the only thing that works that enables you to get insights from a large set of data when the individual individuals must be protected so yes it actually is quite useful at convincing regulators all over the world in different ways great thank you that's a great answer okay so uh next question uh what's the difference between open dp and white noise and i'll direct this to salil thanks for having me too um so white noise is the first of the open dp systems um it's something that that we've done in partnership harvard and microsoft together and we're really excited about that as a model for future industry academic collaborations to try and bring all the advances that are happening in the research literature to practice and and to help in in using data to solve important societal problems but what opendp is it's a larger community effort aimed to bring in the entire differential privacy community those who are researchers in academia as well as those in industry and government who are trying to bring differential privacy to practice and to share our collective insights with each other to build this larger trustworthy suite of differential privacy tools and that can grow and advance as the research in this field uh continues to advance which is happening at a very rapid pace you know josh could it if it's okay could i could i add something um so yes this the differential privacy is going to do that we're going to implement the things exactly as serial says and it does satisfy regulators but of course not providing any data at all would also satisfy regulators the interesting thing about differential privacy is it also satisfies the users right it also satisfies us as statisticians as social scientists and and it satisfies people in companies who try to try to use it so we saw the noise that sarah showed us and that seemed a little scary but if we were just doing a survey and we said here's the population what we would like to know who everybody's going to vote for in the next presidential election and instead of asking everybody we took a random sample of only a thousand people and if you look at it it actually differs from the truth and if you look at a different sample it will differ from the truth right and so we say well oh my gosh what what's how is that useful well it's useful because the properties of those samples we know mathematically and on average we're going to get the right answer and exactly the same thing is true of differential privacy so it's useful to regulators because it convinces them and it's useful to us as social scientists because we solve uh problems in the public public uh good and i think it will be useful to many of the people listening today because they'll be able to use data that they would never be able to touch otherwise great thank you all right so uh so this one i'll direct to sarah and so the question is uh you did all of your demos using azure um azure ml notebooks um but since this is about sharing data um does this does this work if somebody wants to share with somebody who doesn't have azure yeah so the white noise system is completely open source uh we've built it as a collection of flexible building blocks so you can plug it into different systems and and hopefully have kind of the right shape for your system uh one thing that i didn't mention that is worth mentioning here though is the vision uh is that you'll be able in the future to use this system in cases where for example two parties don't trust each other like i'm the the data owner and i want to let analysts use this but i don't want to let analysts directly use the data and uh that's that's still a scenario that we're aspiring to right now what we support in the white noise system is basically uh you have access to the data but you want to make sure that when you use it either for you know take your model out or take your statistics out that you don't have to worry about any of the the privacy concerns and so right now yes you can use it in kind of any any system you want but it needs to be an environment where you trust the analyst all right great um okay so this is a follow-up on the um on the the question about white noise versus open dp and i will direct this to gary so who can join opendp oh uh we would love for anybody interested either in the theory or the application or the software to absolutely join us there's lots of ways of joining us really it's a community we need to know in detail what what the kinds of applications are that would be useful to you and we also need to know all the way at the other end of the continuum what the theorists can can produce and i guess the continuum has a third point so we'll make it a triangle um uh for statistical for statistical usage we need to know exactly what the properties are of the of the of the analyst so if you're interested in any of those things um including even the legal applications of this uh for regulators we'd love uh for your participation great thank you all right so this one uh so so there's a lot of questions about the variance you know the variance versus the accuracy versus um privacy trade-off um and some of these questions seem to be geared towards the mechanics of how does a user like how does the analyst who's building these differentially private reports how do they know how how do they know how much noise to use like how do they calibrate that noise and i'll direct this to cilio yeah great so um differential privacy has tunable knobs um one of those knobs is tuned to how much privacy protection you want or are required to provide to your data subject and that's going to determine um what kind of noise infusion is done and the quantity of noise that's that's introduced um but the amount of noise will also depend on other other things it'll depend on how big a population as as sarah talked about earlier you're doing your analysis on on a larger population the effect of the noise for privacy will be drowned out by by other forms of noise like the sampling error that gary talked about the dimensionality of your data the complexity of the model that you're running and one of the things that we are working on doing in opendp is building differential privacy tools that will become a part of white noise that will expose that very directly um in interpretable ways to end users um in the meantime what an end user can also do is do what sarah showed us which is run experiments on synthetic data and that gives you an idea of how much noise is being introduced for a particular kind of analysis and level of privacy protection and i just want to add you can if you go to the github white noise samples we do have a section that helps you kind of understand what these utility curves could look like and the other thing is we we have an example there and in the blog where you can also explicitly specify how much budget you're using for a particular query which has a you know a direct effect on sort of how much noise is being added all right great um and it looks like we have about one minute left so uh so this one's for sarah um where should people go for next steps so we have a lot of great resources there's more div videos on demand on channel nine you can check out us directly on the the white noise website and the white noise github also there's resources for open dp and then in azure machine learning you can learn more about white noise as well as a whole variety of the other responsible ml tools that we've released we're so happy that you have joined the session and we hope you find it useful join the opendp community check out white noise it's very early so we would love feedback and usage and contributions so we hope to to meet you on github 