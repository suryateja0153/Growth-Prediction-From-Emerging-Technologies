 [Music] hi everyone uh thanks for attending the our session on axillary machine learning with confessional computing our first speaker uh today is antoine de la lava uh he's a principal researcher at msr cambridge and today he's going to talk about talk about multi-party machine learning with azure confessional computing thanks hello and welcome to this session of the frontiers in machine learning session today i'm going to talk to you about multi-party machine learning with azure confidential computing my name is antoine de linolevo and i'm a researcher at microsoft research lab in cambridge uk and the work i'm going to present is a joint work between the confidential computing team in microsoft research the azure confidential computing team in azure and machine learning team in azure as well so in case you don't know confidential computing is a new product launched by azure which is built on technology to help us evolve computing to be protecting the confidentiality of data through its lifecycle so in particular we want to move from computing in the clear to computing of our encrypted data and this allows us to reduce the trust both in the hardware and software stack and as well as in the azure operations so if you're using azure today we are already doing encryption for data when it's at rest and when it's in transit and the new proposition of confidential computing is to maintain data encrypted when it's in use inside the the cpu and in the memory and for that we are relying on a set of technologies that is called trusted execution environments so the idea of social execution environment is that it's a way to leverage new hardware capabilities provided by cpus and other computing platforms and in the case of intel uh the technology is called lgx and it's it's a new set of instructions that allows you to set aside private region or enclaves which contain the code and the data that is going to be protected from the rest of the software stack and that includes the operating system of your vm but also the hypervisor and the host os and in some cases even protects the code and data from physical access to the hardware so an important capability that you have in addition to this isolated enclave is remote attestation which is a capability for the elder to measure and to sign the code and the data that you load into the enclave and that's important because this is a technology that enables us to build systems where you can be confident that some computation that you do on the clouds is protecting the confidentiality of your data so what are the benefits of confidential computing so an obvious benefit is to remove the trust in the platform so you want to do your your data processing on azure but you don't want to trust azure with having access to your data and confidential computing it can guarantees that the computation that are performed on the clouds are exactly the one that you authorize and that are measured by remote attestation it also allows you to reduce the tcp of your application so instead of exposing your data through all of the software stack operating system hypervisor and host os you essentially know exactly the trusted parts of your application that can actually access your data and that will be running inside the enclave so the other benefits that is more specific to machine learning is to enable new data sharing scenarios so um one example of that is if you want to run a machine learning training of a model and you want to have multiple participants contribute some data for instance some hospitals that want to share patient data to build a diagnostic model but there is no trust between the entities that are sharing the data and in addition you also have no trust in the platform that will be running the computation as well and this is possible again because of remote attestation so you can only encrypt the data that you will contribute to a key that will be known to the secular enclave and this guarantees that nobody else including the cloud operator can actually have access to the data and that it is going to be only used to perform the authorized computation so as an example of that we have built a scenario where we can actually do diagnosis of kovi 19 from text extra pictures and for that we are combining two data sets so one of them is the kovi 19 case database from the university of moria and the other is an unrelated data set which is about pneumonia detection which has been published by the radiological society of north america and the idea that we want to combine these two data sets to create a machine learning model so in this case we are going to use a resnet model which is a standard for image recognition and is going to provide the better accuracy and enable diagnosis based on the submitted pictures so if you want to do a machine learning training operation like this today on azure you would typically express it using what is called the machine learning pipeline so a pipeline it represents the flow of data through a set of computations or computational operations so in this particular scenario we have two input data sets which we want to preprocess and to aggregate before we feed them to the resnet model which we are just going to take from a tensorflow and if you were to express this on azure email today you can actually create these kind of pipelines uh using this uh editor or designer where you can actually drag and drop some nodes which perform operations like executing some python scripts or loading some data sets and you can execute them on azure and the way that uh azure ml will actually execute these pipelines is uh internally the nodes in the pipelines in the graph actually represents containers that perform some data processing operation so in the case of for instance executing a python script you will have a container that will take the python script mount the associated data sets and write the outputs to some folder so when azuremy is actually executing these pipelines what it does is that it creates container images that is going to load on container registry and it's going to deploy these containers on your training compute and azure ml supports a wide range of training computes but in our case we are interested in executing pipelines on confidential computing machine virtual machines so we are going to use sccvms to actually deploy the containers so another thing is that as i was saying before secure enclaves typically are used to execute a small reduced tcb version but in our case what we actually want to do is we want to execute arbitrary python scripts and for that we depend on the ability to execute a machine learning framework like tons of floor pie torch and for that we need a lot of support for the python runtime which is why instead of trying to carve out a version that we can run from the enclave what we actually do instead is that we are going to provide operating system like functionality in order to actually execute the full runtime and the complete framework and it's a trade-off essentially between tcb size and complexity so at the cost of increasing the tcp we get the ability to actually execute unmodified framework like tensorflow and for that we rely on the linux kernel library which we execute on top of open enclave sdk and it provides a well-defined interface to the host for operations like memory management managing mounted file systems networking and signals so when we want to actually execute this kind of containers inside enclaves so the way it works is that we are going to encrypt every file system that is used and that includes the root file system which contains the code of the framework and all of the dependencies as well as all of the disk images that actually contain data files and this disk encryption operations are done using the linux device mapper where we use dmcrypt and the unverity for this encryption and this integrity and in addition we implement a key management system to actually automate management of these concretion keys using azure key management service and we also implement new protocols to export keys from the key management service into the lcl image and that's a process that must [Music] be performed without having to trust microsoft services and for that we rely on remote attestation so in order to export a key from the key manager to the request enclave we need to actually present a valid code or an attestation coming from from the enclave to the key manager and is going to rub the key using the lql crapping key to actually perform the export so going back to our scenario of covenanting diagnosis the way that we hide the pipeline is that every node in the pipeline becomes a node that is going to perform execution of encrypted container and we have we are going to have every input represents an encrypted file system that we mount inside this container runtime so in this case we are going to have two containers for processing one container for aggregation and one container for actual training so if you look at the full picture of execution of confidential pipelines the idea is that we have two stages so the first stage is an agreement stage where we have all of the parties that are going to contribute code and data to the training computation or to the pipeline they have to come together to create an agreement over the shape of the computation and this is reflected in the file that is called a pipeline manifest so this file is going to describe all of the containers that will be used encrypted in the pipeline and as well as all of the inputs data sets which are going to be encrypted by every data contributing party so after this agreement stage we are going to move to the execution stage and before execution every data contributing party must encrypt their data sets and upload their data set encrypted to azure storage and they must also install the encryption key for their data sets to the key management service with a key export policy that is going to authorize execution on the pipeline manifest that has been agreed so in this model we assume that every participant has their own subscription and scanned can independently manage their keys and their encrypted data sets but one of the parties is going to be hosting the computation so they also need to have a provisioned a cc virtual machine to actually perform the computation and this will be used by the other subscription that executes machine learning pipelines to deploy the confidential container image and perform the mounting of encrypted file systems and the exports of keys from every participant so at the end of this process we execute every nodes in the pipeline and the output of execution is an encrypted file system as well whose encryption key is also installed on the key management service and importantly the the encryption key for this output file systems has a key export policy that only authorizes the next stage in the pipeline so of course there is also the final stage which contains the trail model and for this one the idea is that we want to be able to deploy the trend model on a confidential inference service and for that we also have a design where we port the onyx runtime to open enclave and we can manage the encryption key for the infrared service to actually query the for inference the encrypted model without ever revealing the encryption key of the model to any party so it's also possible to do other configurations so you may want to release an encryption key for the trail model of your pipeline to a subset of the data contributing parties or you may want to only authorize the use of the trade model through confidential inference in which case the actual model is never available to any participant so this concludes my presentation on confidential multi-party training and i will move on to an actual demo where i show you how it looks like in practice when we want to train a cov19 diagnostic model to simplify the demo we are going to put all of our scripts in a single container for the pre-processing aggregation and training steps the first pre-processing script is for the kovit 19 data set from the university of morial for pre-processing we will decode re-sample and label each picture using this meta-data file containing the information about the patient the name of the audiology picture and the diagnostics such as bacteria pneumonia sars or covid19 some of the data has inconsistent levels so we are going to normalize to three levels one for normal one for pneumonia and one for kovid so then we are going to read the data sets and for every image we are going to decode standardize and resize and this gives us the first pre-processed data set the second dataset is the one from the radiological society of north america and it also comes with metadata file which has the name of the picture and a label and these datasets only contains normal and pneumonia labels which is why we need to aggregate with the other datasets for kovid finally we have the training script itself which is a simple wrapper around the function to invoke the model and just as extra arguments to set the inputs and output folders to the proper mounting points for the disk images here we have the docker file that contains all of the script and it needs to contain all of the dependencies including tensorflow and all of the python packages that we use in the preprocessing script next we need to build the docker container which normally takes a while but here we have most of the operations already cached and when this is done we need to encrypt and upload the container to storage normally encapting the full container and uploading it takes a few minutes so i'm going to skip ahead to show you that here we have computed the hood ash of the container and this can be used in the pipeline manifest which is shown here the main part of the manifest includes the definition of the container images that are used and as you can see we only have one which is the one that we just created the declaration of the encrypted disk images so you can see the two images corresponding to the input data sets as well as additional images that corresponds to the intermediate outputs of every nodes and we need that to know what size to allocate when we create the output disk images next i'm going to encrypt and upload the first data set and for that you only need to provide the folder that contains the data and the script is going to create the located disk image compute the root hash and uploads the image to storage finally it's going to install the encryption key for the image on the key manager enclave similarly we are going to do the same operation for the second data set it's going to create the disk image encrypt it upload it to storage and install the encryption key to the keyboard service and because this is a larger data set i'm going to skip ahead as you can see this takes a few minutes to complete okay now that we have encrypted and uploaded both the container and the disk images we are actually ready to execute the pipeline so i'm going to call this python script that is going to turn the manifest into an actual aml pipeline definition and here you can see that it's submitting the pipeline to aml so we get a link to the azure machine learning portal which essentially allows us to track the execution on the azure ml portal in this case you can see that every node in the pipeline has finished executing which is indicated by the green check mark and we can go ahead and click on one of these nodes to look at the outputs that have been collected by the execution engine some of these outputs have been generated by the management code of aml while others are coming from the lkl launcher including the configuration of lql that you can see here we show that we are calling the first processing script and this configuration we also have the output of lkl itself because lcl is attaching a console output of the enclave to the host and we can use that to actually look at the outputs of the core processing script itself this is the command line that we use to submit an influence request so in this case we have five sample patients that we want to run against the model and for that we first need to create an encrypted and attested secure channel between the clients and the influence service so here you see that we obtained a quote by the enclave that confirms that the key has been produced by the influence service and we are going to submit the influence request and get back the prediction so here you can see for each patient what is the probability for each label thank you for the presentation i'm happy to take questions hi my name is alex sheamus i'm from microsoft research cambridge and i'll be leading the q a i just want to say thank you antoine for your great session so i'm gonna ask the questions from the audience so um so uh does this uh enclave protect gpu-bound jobs yes so that's a great question so as the name implied confidential computing is uh based on special hardware capabilities and if you want to execute on gpus then we actually need uh specific um features on the gpu or whatever hardware accelerator that we want to execute on because if we run on this kind of training jobs on existing nvidia gpus you have just many ways to access gpu memory and to actually read the training data during execution so current gpus do not protect against active adversaries and they don't have the same guarantees so fortunately we have we have uh one of the next talks uh from emmett that is going to give some more details um great thank you so much um so the next question from emmett is are are there any side channels that are part of your threat model right so um so people know in that field section is pretty much one of their top concerns especially when we are executing on cpus because cpus they are essentially executing codes that is coming from different parties especially when we are in a cloud setting so if you execute your training uh on a vm that may be also shared by other tenants then there is a risk that some of the shared hardware features like shared caches uh can be used to try to extract some of your data so in general uh i think our long-term vision to mitigate such analysis we want to actually execute confidential training jobs in environment that have limited shared resources because that's kind of the most efficient way to to restrict these side channels as for current mitigation well we have a lot of software-based mitigations a lot of compilation strategies to actually flash caches and limit sharing as much as possible but of course they come with a performance cost so it's always a difficult balance to find thank you um so tristan ewan asks um is this enclave dependent on the presence of specific hardware um for example intel sgx and is it limited for use on other types of cpus yes so um so as it stands uh we need to have this specific cpu features to actually protect the enclave and to perform attestation what we have designed with azure financial computing is a system that is as agnostic as possible as to all of the specific details of the adware implementation so in particular our key management services our attestation services we designed them in such a way that you can use them for different other implementation of trusted execution environment and that's basically one of our important goals in designing these services is yes we require specific audio features but we try to abstract them at the platform level and can you talk a little bit about the availability of these of these type of cpus is it just sgx what other type of tes are available yes so uh so today um you have uh several uh implementations available so intel is uh has been leading uh that space for a while with azix on the amd side so amd is uh working on a technology called suv and that's providing you with similar guarantees but in a slightly different model so you can use it for essentially running full vms encrypted rather than just small enclaves there is also a dresden based technologies coming from arm and they have a new iterations that are also in the pipeline and then of course you have all of the implementation in accelerators and that's a kind of active research field in the industry thank you and can you tell me a little bit about what's the performance overhead of training my model within an enclave do i have any limitations that i would need to worry about yes so so we we try to make the experience as flawless as possible and that's the value proposition of using this container technology is we want to be able to execute arbitrary python training script as if you were running them locally in practice we have to [Music] we have some limitations that are coming from arduino from the details of the hardware implementation of trusted execution environments so for instance in the current generation of intel asgex we have a limited size for the enclave memory which is called the epc and what that means is if you're using more memory than this limit which in current azure vms you can get is about 256 megabytes then the intel driver is going to swap page using software encryption and that's going to be a very big performance if performance hits during training so at this stage uh with current intel you you have kind of a significant limitation in the number of parameters you can use in your model uh what's coming next is uh ice lake is getting deployed in azure uh pretty soon and that will lift a lot of the memory limitation that exists today and what we are left with is overheads that are coming from data encryption and in most cases these overheads are not very significant so certainly it's you are losing a few percent for this overhead but it's pretty close to the performance you have just training natively thank you and for one final question um how does the confidential computing compare to other privacy enhancing technologies such as homeomorphic encryption and multi-party computation yes so that's a great question and at microsoft we are actually interested in all of these technologies and we have research team that are working on all of them um so at a high level the the the main value of hardware security based confidential computing is that you have the smallest performance cost and certainly the performance cost for homomorphic encryption and multi-party computation has been going down by order of magnitude in the past few years but there is still quite a big gap and i believe that reluca is going to give some more details during our talk great thank you so much so now i'd like to introduce reluca adapa who's going to give her talk so hello everyone my name is ralucada popa and i'm a computer security professor at uc berkeley today i'm going to tell you about our work on a secure collaborative learning platform organizations often need to learn from data across different organizations but they often also cannot share it because it's sensitive let me give you an example from anti-money laundering efforts banks want to detect money laundering but criminals often hide their traces across different banks if a bank wants to detect money laundering and looks only at its own data they won't be able to detect money launders very effectively but if they can look over the data of multiple banks they can detect them much easier therefore banks would like to learn from data across different banks however they cannot share their transaction data with each other because they are in business competition this is a quote i like from the chief risk officer of scotiabank he says that in the future collaboration will be vital via analytics and advanced encryption mechanisms which will enhance yields in terms of finding these attacks of money laundering by orders of magnitude besides this financial use case there's many many other use cases also in the medical sector or even nuclear facility auditing or in cloud-based video analytics that's why my students and i have been developing our mcsquared platform for secure collaborative learning mcsquared stands for multi-party collaboration and competition and the philosophy is that an organization should be able to shared data but without showing it mc squared is an open source effort that i'm leading in the rights lab so within mc square we're taking two separate approaches one is using secure multi-party computation or mpc and the other is using hardware enclaves such as intel hdx for which there's for example azure confidential computing provides a cloud let me tell you a little bit about mpc at a high level because we'll need it later in the talk so in mpc there are n parties that have sensitive data that they're not willing to share with each other npc enables them to compute a function over their data and learn the result of that function but without sharing data with each other in the process essentially what happens is that the parties exchange encrypted data and compute on the encrypted data to obtain an encrypted result and then jointly decrypt that result alone within the process learn nothing else about the other party's data the reason we are using uh either an mpc approach or a hardware enclave approach in mc squared is that there's a trade-off between these two approaches different use cases need prefers one over the other and there's no winner so mpc has the downsides that it is slow and expensive for complex computation or big data and it requires a compute cluster at each organization hardware enclaves don't have these downsides but instead they require trusted hardware which brings assumptions and brings a required hardware setup and it also suffers from side channels my students and i have contributed in both of these directions and our projects aim to reduce the downsides of these approaches for example for mpc our students our projects are designed to provide efficient protocols for various tasks for example delphi does cnn inference helen does linear model training bostotal does classical ml inference and then we have a set of more recent works that try to design automatically and piece efficient mpc on the hardware enclave side our work has been to eliminate a large class of side channels through oblivious computation for example our work on visor provides cnn inference inside hardware enclaves using oblivious computation so i don't have the chance to tell you about all of these different works in this talk so instead i'm going to focus on one of them in particular namely helen so helen is joined to work with my student wanting zhang who deserves the credit for this work as well as with my colleagues joey gonzalez and jon stoick at uc berkeley helen provides a malicious silica keyword npc for collaboratively training regularized linear models and by maliciously secure what i mean is that p minus 1 out of p parties could be malicious could even collude with each other but the remaining part is still protected so if you engage in such a collaborative computation you only need to trust yourself with the confidentiality of your data so to clarify the scope of helen in helen we assume that the parties choose their inputs and that the final result the model that was trained is shared with all the parties there is a separate direction and complementary direction of work that tries to prevent against parties providing poisoned inputs or tries to provide privacy mechanisms so that the model doesn't reveal a lot about the um about data it was trained on such as a differential privacy but these are complementary to helen and helen focuses on the efficient mpc for the training so that you don't share data in the training process all right so there's been a bunch of prior work trying to develop efficient schemes for uh training linear models uh however they are not maliciously secure and many of them focus on only two parties the one class of protocols that actually are maliciously secure are generic mpc but generic mpc is slow it's expensive for example if you take a generic npc platform such as speeds and you train lasso using it and using a hgd stochastic gradient descent running in speeds for you know a reasonable data set it takes at least three months to train a model so this is not efficient instead with helen we're able to reduce this three months to less than three hours for the same level of security and accuracy so the reason is that helen is a synergy between systems machine learning and cryptographic design and let me give you a sense of how helen works at a high level so in helen we are considering a small number of organizations let's say around 10 and each organization has a lot of data so n the number of records is much larger than the dimension namely the number of different fields of each record so here is why if you just take sgd and run it for inside an existing genetic platform here's why it's low with hd you take a chunk of the data at a time and you run a generic mpc protocol that's quite expensive across the parties but then you repeat this for every chunk of the data so essentially you end up running the mpc a number of times that depends in n the large number of records and that's very very slow to be able to make the cryptographic computation efficient we really need to make sure that the npc computation does not depend in n so our first insight is to develop a specialized protocol cryptographic computation that scales independently of the number of records but of course maintain functionality and security so the first technique is to provide an alternative formulation of the problem that makes cryptographic computation more scalable and by alternative formulation i mean a reformulation of both the algorithm and the data so when it comes to reformulating the chaining algorithm we have realized that the algorithm admm called which stands for alternating direction method of multipliers is actually is an existing training algorithm but it is much more fit for cryptographic computation than sjd the reason is that with adm the parties scan their data and produce a short summary that does not depend on n and then with this summary they have to run a much smaller number of iterations using generic mpc so this is better for mpc because you run the mpc on a summary which does not depend on the number of records and also you run this mpc fewer times because you have fewer iterations with the dmm because you um are scanning the data initially an adm is well studied and has you know very good accuracy so this is great however this is not enough and the reason is that we are considering a malicious setting where parties could misbehave arbitrarily and in particular they might not compute their summaries correctly this is particularly a concern because a wrong summary could correspond to no there could be no actual inputs data that could have led to that summary and for example a summary could buy us the final result much more than any one parties data could okay so we really need to make sure that the parties are providing a summary for which there exists an original valid input data set now the parties are allowed to choose that original input data set but there must exist such input data set that led to the summary computation so another possibility to address this problem could be to use zero knowledge proofs so at a high level these proofs enable a party to prove that they computed the summary correctly but at the same time it's in zero knowledge so they're not revealing their data the problem with this is that they are also expensive cryptographic computations so now again the expensive cryptography depends on n the number of records so again things are going to be slow instead here is where we're going to use our alternative formulation of the input data so the insight is to try to find a smaller input that preserves the summary and for that input there should exist an original legitimate and by the input that led to the same summary so we realized that actually singular value decomposition of the inputs of the input data of a party can provide such a summary for us using singular value decomposition we're showing in the paper how we can actually the parties can input a much smaller data set d by d and for any such small data set there exists an original big legitimate n-body data set but at the same time this smaller data set preserves the summary so what this means is that now we can use a zero-knowledge proof to prove that the summary was computed correctly from one of these reformulated smaller inputs and this knowledge proof right now does not depend on n but depends only on d which is much more efficient so basically here's we are at this point we have this generic npc that runs between the parties it runs for fewer iterations because of our choice of adm it ranks on the summaries because of our choice of adm and we can have zero knowledge proofs to check that the summaries are correct and these knowledge proofs also only depend on d by d the number of dimensions and not on the n which is big actually in the paper we have another technique that reduces this generic npc to a much simpler and more efficient computation but i will leave that for the paper all right so this was a gist of how helen works so let's see a little bit of evaluation for helen so uh we evaluate here helen on four parties on different regions on ec2 and we compare it to running sgd which is the go to algorithm for training a linear model and we're comparing to running hd inside a state-of-the-art mpc platform speeds so we are evaluating it on the strong prediction data set of 90 features and here the x-axis shows the number of records per party and the y-axis shows the time it's all exponential axis and here are some helpful indicators for the time so with the baseline so with the existing um framework for npc the time to train depends on the number of records but with helen it doesn't so it does actually depend on number of records the plain text local computation but the cryptographic computation through our techniques does not depend on the number of records and that's the expensive one that's why the line for helen looks flat and overall for only a hundred thousand records helen is already 900 times faster so this was helen uh one example for training linear models in a maliciously secure setting but besides this we have all this other work on tackling different aspects of machine learning in a collaborative sense in a collaborative and privacy preserving sense so these projects are part of our mc squared open source effort which already has a few examples of adoption so for example ibm uses one part of mcsquared opaque for their rest assured medical project ericsson has been using a part of mc squared namely training xgboost models in a secure collaborative way for telecom data and financial and alibaba have also used this piece of mcsquared and they have also contributed to it and we have a collaboration with microsoft azure confidential computing and with scotiabank for the anti-money laundering use case i mentioned above and we already had the successful pocs with them so in conclusion i told you about mc squared which is a secure collaborative learning platform it's open source and it embodies the principle of sharing without showing the data thank you uh so thanks raluca for this great talk uh and the summary of the of the work that your group is doing uh so i think we can start start with a few questions um so the first one is is there any accuracy performance trade of using the summaries as opposed to using the original data as you described in the work thank you for having me and thank you for the questions so that's a great question um there isn't any uh trade-off for helen and that's because the summary preserves uh correctly the input for the purpose of the training again for this specific linear training algorithm this wouldn't be the case for other uh training algorithms but for this specific training algorithm of linear models the summary preserves the accuracy completely um security wise it's also preserved because we added zero knowledge proofs to prove that the summary was computed correctly and it just really boosts uh performance a lot because the slow secure computation no longer depends on all the records in the database which are many but only on the summary so overall it's a win i see and and speaking of uh zero known ledge proofs um so so i guess the performance overhead so uh so what is the performance overhead because there's another question that says um so in terms i mean try to understand what are the performance organs coming from but also uh trying to understand how you reduce uh these three months of training down to three hours so if you could shed more uh light on basically how your optimizations improve the training time but also what are the existing overheads that maybe it's worth looking at and try to overcome in future work right right very good question um so i would say that uh what our techniques are doing is they're reducing the performance overhead from you know three months for training the same you know model i mentioned the parameters of you know the data set size in the top from three months to three hours so that's a significant improvement in in performance for the same um security guarantees as compared to using state-of-the-art tools now you're asking a great question what is left in those three hours especially since to actually train this model using regular data no cryptography it takes just a few minutes so there's clearly a significant increase there in in performance overhead so i would say the zero knowledge proofs take a lot of the time of those three hours so if you are in a setting where you can you don't worry about a malicious attacker but only an attacker so-called semi-honest basically a semi-honest attacker is one that can look at the data of a party but it's not going to actively change what protocol the party is running so if you can trust the parties to not actively change the protocol but just to be curious and look at the data then we send you on a security i would say you can take it down to probably half an hour or less even less and what's left in that half an hour well there's still a bunch of cryptography going on there's still a you know an overhead of blowing the data a little bit up by converting it to cryptographic shares there's still a bunch of network communication so that's kind of hitting the barrier of how good npc platforms that we have today are right right and i think there's a follow-up question on the accuracy performance trade-off so the audience is asking uh whether i mean does it mean that the summary contained all contains all the information so given that you can you can tolerate reducing uh the input data just down to a summary yeah so yeah i see this question from z williams that's really good questions you and i i completely agree where is that well it sounds a little bit um counterintuitive here because you have a ton of data how can you summarize in a short amount of data so actually what's interesting is that what's interesting is that um the training algorithm itself doesn't uh takes the data times the data transpose so basically what happens is that the input to the training algorithm already is not the whole data the training algorithm all that it needs for this linear you know linear model scenario in particular all that it needs is the data times this transpose which already is a summary version so no the summary is not retaining all the data about original data but it's retaining everything that the training algorithm would receive as input uh you know anyways so that's why there's no loss in accuracy for the training process but indeed the summary is much smaller and it's not actually containing all the data because actually the trending algorithm doesn't take all the data essentially that's important right and and i guess the next question is uh and actually i was i was trying to ask the question but the audience is already asking this um so uh so they're saying they're asking uh why the approach was limited to linear models so i guess the question a general general question would be how is to generalize this approach to to cover other machine learning algorithms and other uh models that are not subject to to the linear models that you described in the talk yeah very very good question peter thank you uh for for the question so um we have three techniques in helen and not all of them would apply for something like logistic regression which is more complex than linear uh regression so for example the but some do apply so for example the adm algorithm uh the alternating direction method of multipliers uh actually applies to uh logistic regression as well uh so you can use it to improve cryptographic training of logistic regression as well however the local computation step with logistic regression would not be linear anymore so then we can't use our second technique in the talk about how we uh i didn't talk a lot in this specific talk about it but in the paper about how we actually make the local computation if it's linear much more efficient there's special cryptographic tools for linear computation that are fast but if you try to apply this approach to logistic regression the linear step would not the local step no longer be linear so you couldn't apply that specific technique and and it would be much much slower to handle a non-linear computation so basically some techniques apply to logistic regression and beyond the linear models and some don't apply but overall we don't really have enough you know as us as efficient of a logistic training algorithm right now that's maliciously secure so that would be very interesting future work and it would require some other technique as well great question um and i think there is also a you know a heated discussion about enclaves versus npc oh yeah i think they're too interesting they're there i think there are two interesting questions uh and i think for the for the interest of time i'll try to to ask one question that tries to cover all all of them so hardware enclaves can deliver better performance than mpc that's my understanding but but at the same time mpc can give you better uh a better threat model like uh like lower tcp and so forth now i think the question is um face so i would like to to get your thoughts on um on whether there is any any room for trying to get ideas from from that you explore in that pc and and bring them to to enclaves and try to get let's say the the performance overheads of mpc lower while not sacrificing much of your security and and there and the other and vice versa so any thoughts on that yeah great question so it's a very natural question to think okay how would i combine mpc and sgx um so there are some uh ways in which you could re-up some of the benefits of both but i fear that you would also suffer from downsides of both so for example one uh you know i think an interesting idea is to put mpc inside hdx but because of the integrity guarantees of hgx you may only need to use semi honest and pc so maybe you don't need to use malicious npcs you don't need the zero knowledge proofs because maybe sjx can prove you know using remote attestation that it has done the computation correctly so then you think okay great i'm going to remove um the zero knowledge overhead of npc but a bunch of downsides still remain they're still attacks uh in side channels on sgx that uh subvert remote attestation uh so they would affect the integrity there's also attacks that um if an attacker could do them to every sjx machine they could put all the npc data together so then they reconstruct the whole data because in npc you need at least one party who wasn't compromised so there's still some of the side channels issues of hdx that remain um and they're still the downside of uh semi honest npc is not that fast it's not as fast as raw execution inside enclaves so i feel that indeed by putting them together you would um get some of the benefits of both but also some of the downsides of the ball so it's it's yet another trade-off and uh at least so far i haven't seen you know something that would mostly re-up the benefits that's a very interesting question to think about and now the other thing i do want to say is that there are common things uh to common problems to both so there are common problems that have to be solved for both such as how do you make sure that the data party puts in the computation is not you know poison or uh how about what you release from the npc or from the enclave you know how do you provide some privacy for guarantees for what that leaks and for example differential privacy is uh you know one possible tool so basically there's still commonalities of problems between these uh two different approaches awesome yeah so thanks reluca for for the great talk and the and the very interesting q a so now we have to move to the final talk in this session and and the next speaker is emmett witchell uh who is a professor at university of texas at austin and he's going to tell us how to do secure computing with cloud gpus hope you enjoy the talk hi my name is emmett witchell i'm from the university of texas at austin i'm here to talk to you today about secure cloud computing with gpus this is work that was done in my lab at university of texas at austin with some of my graduate students and my collaborator chris rosbach at ut and he's also at vmware research so there recently in the news have been some very high profile security vulnerabilities that have been disclosed for computer architecture talking about meltdown inspector you've probably heard of these are very troubling security vulnerabilities because they take isolation boundaries that we previously thought were impermeable things like the user kernel boundary and the boundary between processes and it shows that in fact attackers can move through these boundaries so what do i mean by this we have over here sort of a picture of an attacker and a victim and the attacker is going to execute some legal but maybe strange code in this case execute a bunch of branches which is going to put the hardware which is a shared resource in a particular state and in that state when the victim executes their code whether it's executing it normally or in some cases even speculative execution it's going to create some effect where the data inside the victim is actually written into this hardware state which is then read by the attacker using time so in the example of meltdown what you actually had was the attacker would set up the victim to do a speculative load that speculative load would get squashed but the data would still make its way into the cache and then the attacker could determine using timing that that data was in the cache while the high profile vulnerabilities have been in the architecture there have been timing and side channel vulnerabilities that have been disclosed in the operating system and in gpus and what unites all of these is this idea that an attacker can reach into a victim's address space and steal the data within that address space and that's something that we didn't previously think was possible but now we find out it is and unfortunately completely fixing these isolation boundaries is probably going to injure performance and so we're looking at a world where we enter a bunch of mitigations so this is a very unfortunate situation one of my graduate students tyler hunt thought it was similar to the movie they live if you've seen that the protagonist in that movie gets a pair of sunglasses which allows him to see that rich people are actually disgusting aliens who are subjugating the rest of us and uh just as in that movie these attacks are allowing us to see that these isolation boundaries that we previously thought were sort of great examples where we had created an impermeable barrier are in fact quite porous so one of the big themes of this talk is going to be that communication timing leaks your input data and that's because the timing is proportional to the amount of work you're doing and you communicate all the time you communicate every time you go across a machine in between cpus and you actually communicate also across devices between the cpu and the gpu and when you're executing things like training a neural network executing algorithms like training on a neural network that involves a lot of communication there are some technologies that are intended to increase the security of your computation things like intel sgx and arm trust zone if you've heard of those but using these technologies still requires a lot of care they don't solve all of our security problems we sort of wish that we could sprinkle this sort of security pixie dust on our systems and have our systems become secure but part of the the message of this work is that it still actually takes care to build a secure system even using these uh improved pieces and finally how much should your average ai researcher or practitioner care about this problem that's a question i can't really answer the purpose of this talk is to simply give you an idea of what these problems actually are and what it takes to build systems to fix them so in order to really dig into this problem let's take a look at a specific example and in this case we have a person who has some sensitive input some input that they want to keep secret from their cloud provider in this case is google and we'll see why in a minute and the sensitive input goes into the cloud platform and they're just going to do machine learning inference and return some recognition results so what are the problems that i might have in terms of using this service well i might be afraid that google uses a lot of complex software and that complex software might have bugs and if it has bugs then attackers can exploit those bugs and steal my data and that's why i use google because maybe microsoft doesn't doesn't have any bugs but there are other problems like for instance the cloud provider might have their own interests and their own motivations for instance they might want to monetize user data or you might have a situation like 23andme gets acquired by glaxosmithkline and suddenly all of your genetic data is being used to design drugs maybe you think that's fine maybe you don't finally while uh a platform cloud provider has a certain business incentive to be a good actor and to safeguard my data there are administrators that are employed by the platform that have access to the privileged software on the machine and these administrators might have their own motivations in fact if you want to read some stories about malicious cloud platform administrators just just google it or send it to bing so we don't uh that we don't have a lot of basis for trusting our cloud provider but unfortunately trying to get rid of that trust is difficult and the reason it's difficult is the obvious reason and that is the cloud provider is in control of the operating system and hypervisor on these platforms which means they have access to all of our computation secrets they have access to the address space on the cpu they have access potentially to the address space on the gpu they can see all the communication channels and all the data on the platform and again just to be clear it's the privileged software on the cloud in this case that is the threat that's that's what we're concerned about because we're assuming that this uh code is coming from us or is coming from some trusted source but we want to execute it on an untrusted platform and in that case it's the privileged software that can spy on us so how are we going to get trustworthy cloud computation that's where tees come in tes or trusted execution environments are a technology that is intended to help in this situation in particular it provides a hardware route of trust that is independent from the software provider so what i mean by that is that as a remote user i can be assured that my code is running as i compiled it and on a legitimate piece of hardware for let's say intel's sgx that means i don't have to trust the software on that system to know that my code is executing as i compiled it that's very valuable also arm has a version of this called trustzone it's not quite as powerful but it's along these lines so tees give us this technology that allows us to not trust the privileged software on the cloud provider but even there we're going to see that we have to be very careful how we use tees that they in themselves the technology in itself does not sort of mean that any system we build with it is then secure now i've talked to you a bunch about um trusted execution environments on the cpu these are viable technologies which are out in the marketplace and you can purchase them today uh the the work that we've done also relies on the idea of there being a trusted execution environment on your gpu so that does not exist commercially but there are research proposals including the excellent graviton by stavros that was uh published in osdi 2018 there have been some other proposals and what unites these proposals and makes them uh realistic is that all the performance critical hardware on the gpu remains unchanged but you sort of add some security features and you can potentially achieve a trusted execution environment on a gpu so if we had such a thing how would we use it i'm going to describe to you the system we call telekin and it uses a client cpu and a cloud gpu in order to do secure computing now the client cpu we're assuming is trusted because you have physical control over that machine you bought it you physically administrate administer it you've installed all the software on it and then we want to use just the cloud gpu so in this case your application does not need to be modified even if your home uh computer or maybe this is an on-prem uh computer and say a hospital um doesn't have to have a gpu but it was compiled with the idea that it did have a gpu and anytime it makes one of those gpu calls that local call is turned into an rpc a remote procedure call to the cloud and that rpc then schedules some computation that runs on the cloud gpu uh every api call gets turned into an rpc which executes on the remote machine in fact there's also on the cpu in in the cloud there's a a a gpu api proxy that has to be there for technical reasons but that piece of software is not trusted everything is encrypted end to end from the source cpu to the trusted execution environment on the gpu in the cloud so this is an attractive organization to build our system but unfortunately as it is this is still insecure why is this design insecure because while the computation occurs within the trusted execution environment which provides this hardware route of trust unfortunately there's still a lot of communication which is visible to attackers attackers looking at the pci bus that's like the operating system or the hypervisor in the cloud machine attackers looking at incoming network traffic even attackers out in the network see this communication and can do timing attacks on it okay so for the rest of this talk we want to answer these three questions the first is can information be extracted from our gpu communication patterns and not surprisingly the answer is going to be yes i'm going to demonstrate to you a communication timing attack then we're going to ask how can our system remove that information and the answer is we're going to replace gpu streams which is how a cpu normally communicates with a gpu with something we call a data oblivious stream and the data oblivious part means that the communication with the gpu is going to be completely independent of your input data and that's going to break the causal link and make it impossible to leak information about your input data and finally what's the performance cost of all this stuff it's actually surprisingly reasonable about 20 for neural network training the caveat here is you have to execute a lot on the g on the gpu so the there's uh increased communication latency that we introduce but as long as you spend most of your time executing on the gpu uh your your overall end-to-end run time stays relatively constant and i probably won't get too far into the uh execution time breakdowns okay so let's take a look at the timing attacks that we were talking about before so let's just break down what's going on here we have your standard picture of a a deep neural net that has several different layers the thing that you have to know when you do inference on these things using a gpu is that each layer of the neural net is evaluated by one or more gpu kernels and it's the kernel launch and completion interrupts which maybe your attackers is witnessing on this pci bus that gives you execution timing on the gpu so we don't attack the gpu directly but we see by communication with the gpu how long it takes these kernels to run we count the kernel executions which are deterministic and that lets us know what layer in the neural net we're executing and the the question that we have is what can the operating system or the hypervisor learn about the input images from the execution time of these layers of these gpu kernels so in order to figure this out we measured a resnet a dnn on several image classes from imagenet in this case it's sharks and dogs and these bars represent mean runtime and the error and you can see that in this case the runtime is between these two classes is indistinguishable but in these cases there's a small uh runtime difference but it is statistically significant and we can use that information to figure out what was in the input without actually seeing the input so here's our here's our uh the attack so we have an application this application takes an input image it classifies it using a a deep neural network and the output is the image class now our attack does not see a single pixel of the input image what the attack sees is the timing of the neural net that's doing the inference just by looking at the timing as shown by this communication pattern with the gpu we build a classifier that outputs the image class so we're not seeing the image but we are still classifying what was in the image and the question is how well can we do uh with this attack um and here we have random guessing which you know for two classes random guessing is obviously fifty 50 you know 33 25 blah blah and you can see um with our classifier so with our using machine learning to attack a machine learning inference system we can get almost 80 accuracy for two classes and in fact we're 60 better than than guessing all the way down to eight classes and in fact this uh there's a long tail on this distribution we're better out to about 30 classes so that's concerning so there is a communication timing attack that happens even if you're using sort of the latest and greatest security technology so how are we going to remove this threat that's what i want to talk to you next about replacing gpu streams with these data oblivious streams so let's look at some more detail about what i'm talking about when i say the cpu is communicating with the gpu so here i've actually broken it down into what actually goes on when you do one of these inferences the first thing that happens is there's some sort of memory copy where you're taking the input image and you're copying it over to the gpu then you're launching a series of kernels these kernels represent the computation that's being done as part of the neural network inference as you run each of these kernels the you there's a message to start the kernel and there's a message that the kernel is that the kernel has completed and then finally we generate some recognition results and those are copied back to the user so as you can see timing information is abundant in this situation the mem copy has a particular timing the launch kernel has a start time it has an end time memcot the final mem copy also has timing information associated with it operations are distinguishable through other side channels through other sort of incidental features of the computation things like what kind of hardware do they use some of these computations like the mem copy use the dma engine some of them don't like the kernel launches there are other potential side channels like the commands may be different sizes and the application api pattern might depend on the secret input data so if this bit is a zero make two of these calls and if this bit is a one make three of these other calls so what we have to do is eliminate this timing information that is present in the communication with the gpu and we do that by transforming the gpu stream into what we call a data oblivious stream we do this using a client-side library called lib telekin and all of these application calls remain unchanged but they get intercepted by our library which then puts them on a queue and is going to use fixed-rate communication with the gpu so uh whereas previously we had this stream so we have mem copies and we have kernel launches because those are the two types of operations and so we do schedule them independently getting some of the details here because that's good for performance we are going to split and pad the commands as necessary to enforce a uniform size and here we're going to see these blobs which are different sizes and they're happening at different times when we transform them now suddenly you can't tell them apart and so in order to make this happen we have to add padding we have to add no op kernel launches those are fairly easy to do and fairly easy to disregard on the gpu but the thing that we do have to do is we have to say we are going to schedule our operations on a fixed schedule fixed with respect to time so we're going to launch 32 kernels every 15 milliseconds we're going to mem copy one megabyte in both directions every 30 milliseconds we are going to use a fixed amount of bandwidth regardless of the performance requirements of our underlying application so that's the cost as far as the actual execution time overheads they're only about 20 for neural network training i'm basically run out of time so i'm not actually going to go through these performance overheads in any detail i will i will skip through this but basically the idea is we are using a fixed amount of bandwidth and we can achieve low overheads because the gpus are busy and their computation overlaps with all the extra work that we've inserted sorry so here's the breakdown of the of of the overheads but not going to go just going to flash it by really quickly and i'm just going to conclude by saying the talk has presented secure computing with cloud gpus and we achieve the security by eliminating the communication timing channel that happens between the client cpu and the cloud gpu we do that with data oblivious streams this is transparent applications because we maintain gpu api semantics what i mean by that is if you mem copy a kernel and then execute it the gpu knows to wait for the exe for the mem copy to complete before it does the execution we preserve that and if you do this the whole system has modest performance overheads for the level of security provided which is quite high thanks very much thank you so much emma for this fascinating talk um really appreciate that you can talk to us a little bit about training gpu using gpus to train securely inside of a data center have a couple of questions for you and if anyone wants to chime in on the chat we'll be happy to ask um these questions too but to just kick off um you predominantly spoke about gpus and as we've seen there's a perforation of other accelerators can you talk about how your work would work with different types of accelerators sure yeah so thanks very much for having me and give me this opportunity to talk about my work work my students so um accelerators uh in general it's sort of tempting to um batch them up and and yet the i think the challenges are sort of dependent on um the exact nature of the accelerator so for instance uh for test it's not clear uh first of all it's not clear exactly what they're doing because a lot of that uh hardware work is is not sort of open source and then it's not clear exactly what the threat model should be because we don't know what the operating system and the hypervisor sees as far as the tensor processing units so for an example of something that we do know as far as the encryption of facilities on your on your processor that's no problem you know the when when you you get context switch the operating system can wipe out all that state and keep strong isolation between different contacts so that's that's not a problem whereas a gpu is is sort of uh like a sort of a persistent state machine it's got like uh you know thousands and thousands of registers and it's got a lot of in in-flight computation and so you can't just wipe out that state and not have it affect performance so gpus are sort of a particularly interesting case for how to make accelerators secure great thank you um and we also have a question just to follow up on the security of gpus uh from don bevia saying um do the regularly spaced requests still leak info to a power attack so um yeah let me let me just be clear that as far as our threat model our threat model is sort of a public data center and i'm actually not so worried about somebody hooking up a power meter to a machine in a public data center from what i understand uh they have very strong physical security um you know and and i'm really concerned about software attacks in the data center because those are able to be launched by attackers and and admin administrators uh that said uh as far as power goes i haven't really thought it through but certainly fixed rating is a good uh first step toward trying to keep power constant but i would have to think a little bit more and you know think about what the operating system can do in terms of ratcheting power down and i i i don't really know but it's it's not it's not an arthritic model i think for good reason have you considered other type of side channel attacks directly on the gpu um from a host os versus a guest os where most users will be running so this is again protecting more against the i guess the cloud operator rather than somebody else using the cloud so uh from the guest so i'll tell you um so i'll tell you sort of one of my philosophies about sort of trying to make gpus um secure against side channels and that is i'm not a big fan of sharing them because if you're running two if you have two different tenants that are trying to share the same gpu um that seems like a really hard problem because there's a very complicated memory system with certain limitations and bandwidth and a lot of fancy hardware scheduling and i feel like that's a there's uh published attacks for one tenant attack attacking another and the the truth is that um virtualizing gpus is such a challenge that their utilization is generally low and so having that be the cost to make a gpu secure seems acceptable to me and without the um changes to the gpu hardware i don't see how you're gonna get multi-tenant security we could ask stavros because there was stuff in in graviton about uh multi-tenant security but that you know there's i think there's some gotchas down that road well let me then take you in the opposite direction of have you looked at what happened what would happen with your model when you use multiple gpus and try to scale up to the data center scenario yeah so so um let me say that we actually have multi-gpu experiments uh in the paper so we do um uh support multiple gpus the sort of caveat is that all communication has to happen back through the client which you know for certain uh workloads that's perfectly fine but that actually introduces communication latency but it's perfectly safe and you can absolutely scale out the thing that we didn't look at that people use is when you have direct gpu to gpu communication by something like nvlink in that case we would have to fix rate that communication and we took a we took a look at that and it was it seems possible but it was beyond our engineering budget for the project great thank you um so and then to ask a couple more questions from the audience so um stephanie is asking what are your recommendations for future api semantics to reduce overhead on performance of tes so that's a very good question um i i you know i guess sort of philosophy wise i've made a research career out of backwards compatibility so i tend to i tend to you know take certain certain things as given you know that's why in the first talk when they were using the linux kernel library it certainly makes sense to me because you know the amount of engineering effort that goes into tensorflow is very very high and so trying to get a custom version of that that doesn't require certain facilities seems difficult and so you know if the the actual the actual um gpu api is fairly um stylized and we don't have too big a problem transforming it into this form that is data independent i guess the one thing i would say is there are idioms that the programmer can use and the more asynchronous you are the better our system can be in terms of hiding latency and and i didn't get into performance but at the end of the day not surprisingly if you execute a lot on the gpu we don't harm your execution time too much and if you don't execute a lot of time on the gpu and spend a lot of time communicating uh well we put a wide area network between your cpu and your gpus that's going to slow you down right thank you um and then so we have time for one final question and again from the audience um so peter writes uh i'm sorry asks um i found it interesting that the timing between between differences between sharks and dogs was only evident on the first two layers of the neural network do you know what caused this i would have thought that the number of operations in all layers could should be dated independent yeah so look first of all i really appreciate this question because this really surprised us okay this was not something we did not go into this project thinking like oh we're gonna nail these inference systems we actually figured like well that's probably not going to work but you know whatever we'll just sort of build this build the system anyway or try to build it you know based on some threat model that we thought was was reasonable and what i showed would what i showed you in that case was a total simplification right because uh just just to have it fit on a slide um there's actually uh hundreds of layers uh hundreds of kernels and like you know almost 100 layers and the timing of some of the layers is data dependent and the timing of other layers is not and and i will say we spent a lot of time trying to understand what was making the layers sort of uh what was control what what signal were we picking up on we were picking up on uh the in a lot of cases the number of zeros that were happening because those uh floating point operations are faster um but that showed up both in processing the raw pixels and also in the convolutional layers internally and we you know maybe as i don't have a lot of super machine learning expertise and maybe with more machine learning expertise we could sort of drill down to figure out exactly what layers were doing this data dependent computation and sort of how to display it i really wanted to have sort of the picture that shows like you know it's the you know the circular feature detector or it's this convolutional layer that does it and the signal will sort of spread out across several layers and so it was hard for us to figure out what what the underlying uh regularities were that we were pulling out of there thank you thank you for thank you for that and um thank you to all our speakers to again to emmett antoine and reluca for coming and talking to us um we've got a half hour break now and we'll continue at 11 o'clock uh sorry 11 a.m pacific standard time on the security machine learning session thank you very much everyone thank you 