 [ANITA]: Hello and welcome! I'm Anita Wood, the program manager for the Stanford energy innovation and emerging technologies program, also known as the EIET program. On behalf of the Stanford Center for Professional Development and the Stanford Bits and Watts Initiative, which is in the Stanford Precourt Institute for energy, I would like to welcome you to this special session on the transformation of the electric grid with Dr. Ram Rajagopal. Ram is an associate professor of civil and environmental engineering and a senior fellow at the Stanford Precourt Institute for Energy. Dr. Rajagopal teaches the EIET237: Transforming the Grid course in the Stanford EIET program. He directs the Stanford Sustainable Systems lab, which is focused on large-scale monitoring, data analytics, and stochastic control for infrastructure networks, in particular power networks, and his current research interests in power systems are in the integration of renewables, smart distribution systems, and demand side data analytics. For this session, he will be talking about looking beyond technical considerations and integrating distributed energy resources. He will also talk about cyber constraints, privacy, and energy equity. Ram's presentation will be followed by several pre-recorded questions and answers. Now let's begin. [RAM]: First of all thank you for hosting our session just here on the left hand side of this very first slide, I chose a picture of something that is hot off our research to show the power of open data. We have been able to analyze the demand -- hourly demand -- from 58 countries around the world to understand the impact of COVID-19 restrictions on the demand and you can see here this work was entirely done by a group of students and postdocs. I thought a little bit about what is interesting today and kind of the title of the presentation reflects that at Stanford, what is our, some of our key interests. These are the concepts I have here: virtualization, learning coordination, and privacy/fairness in these consumer driven distributed energy systems. This is very important because the word consumer here means there is people and their preferences that need to be taken into account, so I think that's a major distinction of the traditional view of such systems. You know we all are familiar with the grid I'm just going to go very quickly here and the traditional grid is changing because we have massive adoption of renewables and new technologies such as storage at the transmission level, but at the same time behind the meter consumers are adopting new technologies as well, like EV charging, smart appliances, self-generation through solar PV for example, and all of these technologies are connected to the cloud. This enables us to collect data and send signals and potentially coordinate all these resources to achieve individual and joint goals that can serve, you know, to the wholesale market so you can have flexible load that follows generation instead of the other way around, but also as we start to see of increased adoption of these technologies, this coordination can be used to mitigate the impacts on the distribution network. So, one of the research focuses here in Stanford has really been around how do we utilize this capability behind the meter to support the rest of the grid ecosystem, and when we say that what are the key challenges here? The first one is that we need to understand what is the real flexibility available at each customer and that critically depends not just on the technologies they have but also their preferences: how much are they willing to change, adapt, and adjust, and learning that at the scale that needs to be done is something that still remains extremely challenging and an open question and something that I feel for example you need not just methods, but a lot of open data interaction with customers and so on. The second question is, well, if we know the consumer preferences and assuming we have a model for the distribution network, then we can start to think about how to scale this coordination to millions of heterogeneous resources. Again the key word here is scaling there is many many strategies and architectures for coordination; this is very well known, but do they actually scale to the scale we need them to go? The third one I think is very critical and goes into the economics and policy because it is all about whatever strategies you create, whatever platforms, ideas, etc, be it to collect data or to coordinate, you definitely need to make it easy and attractive for consumers to engage. That means not asking them tons of questions, you know, or are asking a lot of effort on their part, and also mechanisms to compensate them. Last but not least I think we're seeing a dramatic change in society today regarding two ideas and I think it's time we took them more deeply into how we design these energy systems, particularly consumer driven ones. One is this notion of privacy, we are very familiar with this, there was some early work on this looking at privacy of smart meter data and so on, but in those days, the entity that took care of the data was just the utility. So the privacy was in a way managed by utility regulators, but now we're talking about aggregators, different types of service providers, and so on; we need a broader and expanded definition of what privacy is, and actually ways to enforce it, monitor it, measure it. And the second topic I think is one of fairness, so I will address those briefly at the end of the presentation. What we mean by fairness is we need to give both equal access to the participation in these modern systems, but we also need ways to ensure that the systems we create don't exploit particularly vulnerable customers, so you know, families who have lots of people who stay home tend to be lower income, and maybe your demand response algorithm would continuously target them for turning on and off. These are considerations that have been taken into account in economics and more now on web search and things like that, but I think there's a whole open question about how to do this from a technical perspective, but also institutionally how utilities and companies, aggregators, technology providers, policy makers are going to deal with these issues. I just wanted to kick off the conversation with something that we did recently which is kind of unusual here for our group. We worked with a sociologist and a psychologist to try to understand in a real population how much actual consumer flexibility there is on their loads and devices that you see. And in this graph here, what you see is really what percentage of each one of these activities is performed during the peak time. This was a population of 370 individuals, and the paper came out in energy policy around this and you can see here for example TV is watched 90 during the peak time, etc. And in black, that's kind of the willingness to shift what percentage of people were willing to shift, and you can see that just very few items like your washer, dryer, dishwasher are things that both meet somewhat being used in the peak time, and somewhat willing to shift. So this started shaping a lot of our groups thinking on how to design the systems that respond to this need, and I'll address that when we talk a little bit about powernet, but I think this kind of engagement and interaction with consumers now they're doing a project where we educate these households through their kids to understand their own consumption from smart meter data and then try to execute on the shift. It's a little bit away from the traditional path but it's all about how do you have this human in the loop during this kind of management, and it's a ton of work and very interesting as well, so if you come here in the summer, we have workshops with you know, hundreds of kids and it's super exciting. Okay, going more towards learning, you know the first idea I feel is very critical is that in economics, there's a gold standard about how you learn preferences. You usually say people have a utility function, I'm going to show them different choices, observe the choice they make, and then model this utility function that is kind of the embedded value of the choice. And there is lots of techniques for that, and it's well known. This typically translates in the utility world of electric utilities, as we will do randomized trials on different programs that we have, let's say for demand response, to understand the size of the effect, and then model the heterogeneity of that effect as a function of customer characteristics collected from questionnaires. It turns out that these models in the electricity side perform not so great. Typically if you use created models like this and we work closely for example with PG&E on looking at this and then you say, now I want to recruit enough customers to have a megawatt of demand response on this particular feeder, you end up with something like 10 to 20 percent of that. That's kind of it, called the yield. Just as a point of comparison you know if you go and talk to somebody at amazon, they will say that these offers that are put on your first landing page of the amazon website when you see it, 75 of the people click on one of those offers and then of those clicks, between 50 to 70 percent of the people actually purchased what they saw. So that is overall much more than like a demand response offer where you're actually offering money to the customer, and yet the engagement is less. Economists looked at these questions in the last 10 years and there's a really beautiful stream of literature on that, and what they kind of found is that this is called the issue with stated preferences because normally, we are not able to observe what actions customers take with regards to their electricity consumption. So you do polls and surveys like the one I had before, and then use those polls and surveys to predict how much flexibility customers have, and that goes directly into the planning. And so those predictions don't completely capture the preferences because customers don't pay a lot of attention to their electricity bill; it's not a big part of their budget. So this is an issue not just with electricity in many other areas declared preferences of things that won't don't weigh on your pocket as much are not accurate. So one idea that we developed in partnership with PG&E, ESE, and a host of utilities was this idea that, well I have a smart meter that's like high resolution temporal data. That data, it's like an MRI of customers preferences. If I could interpret that MRI, and by that, here is what I mean, extract features like what's the peak load, base load, what's the thermal sensitivity, what are the load shapes, I can use these features to build these models for segmentation, targeting, etc. This gave rise to a project called Wisdom which was carried out here at Stanford from 2011 to 2016, and then we released this open source package called Wisdom it's now used in eight U.S utilities in Southern California, Edison PG&E, Vermont, etc, and then a few more worldwide. And it's completely free, open source, you can download, and i think in 2016 when we released it, we had analyzed about 8 million smart meters with this package. Since then you know we haven't really kept track of what's going on, so just as an initial, like, the very first result from Wisdom is shown here. We just did the first study ever at the time at the scale that was done, which was 66 million customer days. We looked at 66 million customer days across the bay area and performed clustering. And when we started doing this study, PG&E gave us the data that took itself like a year or two, and then when we did the study initially the consensus was, well why should we do this, it's well known in planning that all customers are this dual peak shape. And then when we actually did the clustering, we found many different load shapes for each day, including just consuming at night, very flat consumption, evening peaks, and the so-called planning option. It's only fourteen percent of the load shapes. And this is very critical because, for example, load shape number 811 here actually can have demand response capability at 3 pm, versus this guy here, 14, can have no such significant capability. So first lesson from this project is when you're thinking about these new kinds of flexibility that are emerging, number one, due to the preferences of consumers, the flexibility may or may not be there, like you saw from the survey. Number two, data and massive data are essential to have accurate models of consumer preferences to achieve the types of performance we need to make these things cost effective for the future. So the second question we had in our list up there was, okay so we understand customer preferences, maybe we can collect a few of these consumers, and use their capabilities to offer services back to the grid, or to reduce their bill, or something like that. How do we do that at scale? That was the genesis of this project Powernet. It's a project that has been funded by the California Energy Commission and ARPA-E as part of the nodes program, and we had as partners Zodian, Suntech drive and Google. Google also funded the project and it's also a partner of the technology. And it started out with a simple, kind of interesting observation. At the time we started this project in my group and in Abbas El Gamal's, he's another faculty here at Stanford, in his group we had had a few collaborations around optimization algorithms for storage and kind of distributed control, and this is kind of a big fad in power systems. And these algorithms work great in Matlab, and that means in simulation, and then you know, I visited Google and Arun Majumdar's former group, the one that he created because I had this kind of question that was put forward by my student Gustavo, well, but how do we know these algorithms actually work in the real world? And then somebody told me Arun was doing this in Google, and we went and visited that group. And what they had tried was actually to implement in a lab scale with realistic constraints in terms of the cyber constraints, the information exchange constraints, several of these algorithmic strategies. And what they found out is that fully distributed algorithms, although very elegant and beautiful theoretically, they don't really scale well, and it's very hard to diagnose, it's very expensive to try to set them up with customers, and so on. And in fact at that time, they had just managed to do like one home, and technologies inside it and tried to coordinate the energy consumption. And they had done a survey with customers because they needed to collect preferences, and Google does these user surveys, and they said well, we're going to collect your preferences, and here's our idea. We're going to put forward a questionnaire with 50 questions that you need to answer so we know everything we need to know about how you like your house to be. Like your temperature set point, and so on so forth, and the result of the survey was that the immense majority of people said they would never use a system like that. I just thought about my own home, there is the three preferences for the thermostat set point, and nobody can really agree on it, so that's an example of you know trying to do that 50 times is impossible. We decided when we went to look at nodes, we wanted to propose a project which was about building a real system that can do such coordination, which addresses the scaling issue, and that means really addressing three critical problems at the same time. One problem is whatever coordination mechanism you create, its coordination is going to happen also through the power network. In our case here we focus on the distribution network. The second is whatever mechanism you create needs to learn and understand consumer preferences. In our case we picked the consumer to be homes initially. The preferences of the homeowner have to be taken into account into the coordination. The idea of picking up homes was also because each home by itself cannot really provide much value back to the grid, but in coordination they can do a lot. And the third issue is that whatever scheme you create, or architecture, or solution you create needs to be able to account for the real constraints out there in terms of information exchange, so data exchange, and communications, and so on. We spotted at the time this notion of, well we can use the cloud to do coordination. And for example this is already done in data centers. So the cloud runs the coordination of the communication between the server racks, but the key idea even in the data center case is the cloud cannot do this in real time, because whenever you put the cloud in a feedback loop there is delays on the information exchange, there's reliability issues if you try to make it geographically spread, and this is all stuff we learned from Google. So how do you do it so that the cloud is not really at every second and every minute calculating and sending the decisions to all the homes. So that was this origin of this project Powernet. Here's how the system that we came up with was laid out. And again we started this project in kind of an unusual way because we decided to build a lab in Stanford, put these devices and things together, work with Google, connect everything to the cloud, etc, without knowing what the solution was going to be. And we experimented with tons of algorithms until we figured something that worked and that also had some theoretical justification, but that is still kind of work in progress, the theory around this. The key idea of the system is that every home has a smart hub, a local intelligence, it's a computing capability that communicates digitally with, you know, your inverters for the solar panel storage, or EV chargers, and also with this smart dimmer. This is a new kind of smart panel that was designed by our colleague professor Juan Rivas working with students. This panel allows you to have high resolution measurements of all of the sub-circuits of the home, but also the capability to do voltage control at the sub-circuit level. So you can have some controllability of loads, appliances, and so on, even without actually the appliance being smart necessarily. So that's kind of the resources you have, and normally this home would have a bill, and it tries to minimize the bill using data and these resources. But in order to minimize the bill and also to do coordination, which is homes might get together and say, well let's shave the peak in this transformer because we'll get some compensation from the hotel market, or maybe directly from the utilities, and from the demand response program. In order to do all of that, they do need to understand and account for the impact they will have on transformer capacities and network voltages. So that's kind of the setting, and how did we solve this? I'm not going to enter into a lot of details but I'll go a little bit into this project. First, it required thinking about this architecture, of how does the local controllers and this kind of cloud coordinator engage? And we also imposed ourselves this issue that we want to minimize the information exchange and the cloud cannot run in real time. And so what we found is an architecture that has a planning stage that's run every 24 hours in alignment with the day ahead market, and in that planning stage it sends these dynamic power bounds, so it's like establishing a bandwidth for each one of the homes in terms of each hour, how much power are they allowed to draw or inject back into the grid. So instead of having a static bound given by the transformer they can have a dynamic bound. And it turns out, if you do these calculations cleverly this is enough to really protect the network while enabling the coordination. We designed this smart dimmer, we also did lots of algorithms with data from predictions to learning the customer preferences from observing their choices and using appliances, and then the biggest effort in the project was a massive kind of deployment. So we had a large-scale lab that was kind of built between Stanford and slack, then we have deployments in a real world farm and 20 homes in Fremont where we put you know battery storage, solar, we read their data, we kind of do all kinds of things. I'm just gonna point out one thing here and then move on. You know, the critical challenge on the coordination is that there is a spatial and temporal data asymmetry. It has not to do just with the technical issues, it's because who knows what information? The utility knows the network, but doesn't necessarily have access to behind the meter devices. DR providers have access to their own devices, but no information on the overall load or the utility network, and so on. So how do you deal with this data asymmetry? And so the design of Powernet addresses this issue by figuring out how the information needs to be shared, and how that shared information is utilized so that we can maximize the benefits for the consumers, their actual participants in these systems. We initially -- when we designed this system one of the things that we learned, and I think this is kind of the first major lesson, is seeking this kind of elusive optimality that you see when you design algorithms and you test them in simulations. It's actually detrimental in designing real scalable solutions. What we found is by sacrificing some of that optimality, you can really take into account all of these practical constraints and achieve good results. So you can reduce the voltage deviations that would result from people synchronizing all their decisions. You can also do things like offer ramping services, regulation services, we showed all of those. And then if you look at the end of the month into their bill, you know you still achieve like 90 percent of the ideal oracle reduction, so they can't even achieve that in practice, but just that small sacrifice results in big benefits. Here's kind of the lab that we built and you can see this is a lab that was a partnership between industry, the Stanford school of engineering and ARPA-E. So in industry the engagement was through this Bits and Watts program that Liang directs. And you can see here we have like a smart home with real appliances, it's connected to a panel, it's connected to a solar panel, to a battery system, water heater, the whole thing, and to an EV charger. This home is connected to a home that's a little bit more virtual, and here's like a grid emulator. All of this system is connected to something like 10,000 virtual homes in gridlab D and open DSS now. This whole lab is connected to a similar lab in slack that had more of larger loads, and were meant to emulate small and medium businesses. And this is where we initially tried to design our solutions and demonstrate them, and we learned so much from first of all, interoperability. Yes we do have the Sunspec alliance. But if you try to go and read any information from real inverters, it doesn't really work. We did the field tests, and we have successfully deployed this system in a farm, and then now we are doing it in homes in Fremont. You can see here the cooling of cows in this real dairy facility is one of the largest dairy facilities in the united states, and they spend about $200,000 every summer with this cooling. And what we are able to show is that using Powernet you can reduce about 50% of that cost. And what Powernet does is it coordinates storage, the solar panels on the rooftop of the barns, and the fan control, and tries to maintain the cow temperatures appropriately right now using airflow measurements, but we have installed like a thermal camera. So as you can see in the system I use this as an example because there's a lot of embedding of IOT solutions from different vendors and so on. I'm just going to wrap up with our latest project. So after we did Powernet, we realized there are a lot of principles on how you should design the software solution for this and the architecture. And we wanted to look into how to do this at an even larger scale. And we partnered up with folks in computer systems design here at Stanford, and decided to look into this issue of virtualization, modularity, and layering, which is their principles of how they do computer systems design. And that gave source to this project called trustDER. The goal is to ensure resource virtualization, security ,and privacy for all of these algorithms by virtualization. It's a simple concept: can you take one resource and make it look like many virtual resources, or many resources and looking as one from a software perspective, so then you write your solutions you don't have to worry about this. And then in the privacy and fairness it's all about really protecting the privacy of the customer. If I have to have data privacy how do I design algorithms? It turns out to be pretty difficult to do that. To conclude, I just wanted to leave the form of engagement we have had through all these projects is using this affiliate model for partnerships through the Bits and Watts program. We work very closely with companies, gathering their inputs, having them participate in board of advisors, and in some cases directly join our research teams, and students who finish these projects sometimes go on to develop these technologies commercially, sometimes they go into these companies as employees, and so on so forth. I also have a flagship program that was started through this partnership around EVs. Thank you very much.  [ANITA]: Thank you Ram for sharing with us some of your work. We have a few questions that were submitted. The first question is high level, then we'll dig a little deeper. To start, would you take a few minutes to explain how DER measures and grid digitalization affect grid resilience?  [RAM]: In terms of the resiliency, I think there is two key ideas. One is understanding how do you translate consumer flexibility into resiliency, because what is resiliency? How do you measure it? For example, if I have an earthquake in San Mateo, it will affect the homes. It might disrupt some of the solar PV, it might affect part of the grid. What do I mean by being resilient? Do I have to provide full power to the remaining homes, or is it part power, does it depend on the intensity of the event? We collaborated with the faculty who does earthquake engineering to try to define these metrics around the ERs. And it turns out there's two key aspects in my mind. One is you need to understand something like what is the full load potential, half load potential, no load potential of each customer. The second one I think it's it's a lot harder, which is really, any system to be resilient beyond its individual capability, there is this notion of a networked resiliency. And that means you need to understand how to model the switching in the distribution systems, and adding those switches, as well as how does damage due to different events affect those systems. I think it's a fairly open question. From a broader utility perspective I think there is yet another question. Once you have a resiliency model you need to know what resources are out there today in terms of DERs. From solar panels to different equipment, and the mapping of this has been lagging tremendously. This data is hard to access, often unavailable. In Stanford which started a project called the Stanford energy atlas that uses satellite imagery, we work with people in machine learning and so on, and do country-wide mapping of these resources. But there's many such efforts now that are happening, and I think without that it's impossible to do this resiliency assessments. [ANITA]: Excellent, thank you. For our next question, would you talk about time resolution of cloud coordination? What is time resolution, then would you go deeper into that? Also, could you talk about the optimization model and how you deal with changing household preferences? [RAM]: Overall, the idea is in the cloud coordinator itself. We run it five to 15 minutes, so when you're offering ramping services, etc, but locally we run it between a minute and a second. We have tried all of those combinations; the minute seems to be more than enough. The optimization we started with STP Conic optimization to take into account the network obviously required a lot of magic to make it work with like tens of thousands of homes and large feeders, but recently we moved to the idea of successive linearization, which despite the absence of strict performance guarantees, in all of our practical examples and real feeder systems that we got from a few utility partners it has done a really amazing job, and it's super fast.  [ANITA]: Super, and I appreciate that insight. Next, would you talk about what is the potential for developing local energy markets, distribution operational markets as distinct from wholesale market opportunities, and how those distinctive opportunities can be optimized?  [RAM]: If you look not just today, but if you're looking at the same grid 10 years from now, for example, you'll have much deeper penetration of EVs, you know particularly kind of delivery vehicles and things like that that pull a lot of power, and you will need to start managing the impact of this power on the distribution network much more intelligently than today. Today there is a big slack capacity on the distribution system so we are taking advantage of that. If you go beyond the united states and look at other countries for example in India, or in Brazil, distribution transformers are often already overloaded, and adding chargers is kind of a difficult thing because there is no benefit for the flexibility that is monetized. [ANITA]: Great! And for our final question, there is an interest in understanding what role universities play in the pipeline to invent and develop new technologies, approaches, and solutions. And then to add to that, what needs do you have to take the work that you and your teams have been doing from research and initial concepts to commercialization and adoption? [RAM]: I think that first and foremost the mission of the university is education, and I think a gigantic role is preparing students who can go and address these challenges in the real world for all the companies, and policy making, and so on out there. There's kind of the set of knowledge, the knowledge base they need to have to address these problems in future energy systems is very different from the past. You're not designing, you know, transmission lines anymore, you're talking about IOT, and communications, and distributed algorithms and things like that, but also policy, economics, social sciences. So I think that is kind of a first, and I really found a lot of success with the students at Stanford and at other institutions. The second I think point that is very critical is the university can serve almost like as a neutral zone where all these different stakeholders can meet and exchange ideas, exchange information, exchange data, and work together on problems. It's like a neutral ground because you know the mission is education, and is not for profit organization. With that mindset, we have done a lot of the projects that you see here. It came out of conversations with the industry with very close interactions with them but with typically with companies that don't talk to each other, and then also policy makers and so on. And another example of that is at Stanford, we have hosted lots of interesting data sets where we collaborate in teams so that the owner of the data set is aware of all that's going on, but that has led to really useful tools, like Wisdom is an example of that, so shared data. And I think that the last piece, and this is something that I think we need to improve from the universities, is really the awareness around these problems. I think in the US today, you see young people much more in tune with the notions and ideas around sustainability, wanting a 100% renewable grid, but the a deeper understanding of all of the actual issues, the scale of what this transformation means, and then motivating people to go and study this, get involved, and work in this industry, I think that's something universities can do certainly a better job of. I think this commercialization side is often misunderstood because I think the source of a lot of the interesting ideas is out of thinking about, you know, the fundamentals. And out of that it manifests itself sometimes as software, sometimes as just the people. And somebody asked about, you know, what's kind of the challenge to commercialization. Having done startups that came out of Stanford in other areas and comparing them to energy, I think there is two really important challenges. One is energy systems are very critical, and today they are extremely reliable. So the time scales at which you need to do a startup are very different. And this actually offers an opportunity for new types of partnerships where companies can identify interesting technologies developed in Stanford or Dartmouth, and then co-invest, and with the university and there's many different models that are being explored to kind of bridge that time scale. There's also programs like the one in Berkeley the cyclotron road that helps you do that launch. I think the second challenge -- and this I think it's much more key -- as a community in grid, we don't really do an excellent job of keeping what are kind of important practical problems, that if addressed, there will be a ton of interest. If you go and you're working with medical data, or you're thinking about data centers, each one of these communities has it very clear, what are some like target problems that can be worked on by companies, researchers, and so on.  [ANITA]: Thank you, Ram, for this very informative and impactful presentation. I want to thank all of you for viewing this today. As you can see in your screen, if you would like to learn more about the EIET 237: Transforming the Grid course and the other courses offered as part of the Stanford online Energy Innovation and Emerging Technologies program, you can follow the links on this page, and there are other buttons on the screen as well. If you found this presentation to be helpful in any way we, encourage you to share the recording with your colleagues, friends, and family, or whoever you think will find this useful. Thanks for taking the time to join today, and have a great rest of your day. 