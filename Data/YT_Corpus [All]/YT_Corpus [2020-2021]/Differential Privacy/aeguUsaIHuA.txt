 [Micah]: Welcome to our CNI session, Privacy Gaps in Mediated Library Services, and [inaudible] 00:09 some suggestions about what to do with them. I'm Micah Altman, I'm Director of Research for the newly named Center for Research and Equitable and Open Scholarship at MIT Libraries. This is joint work with Kit Haimes, who's a student, Katie Zimmerman, a colleague at the MIT Libraries, Lisa Hichliffe at the University of Illinois. I'll talk about some of the problems, and then my colleagues will talk about how to solve. If you're interested in diving into some of the problems, this paper is not yet out, but a lot of related work on privacy is, and we'll have a whitepaper version of at least the first part by the beginning of the year. So some brief background of libraries and privacy. Oddly enough, libraries have cared about privacy for a fairly long time, and this comes from at least two different sources. In the international library tradition, privacy is viewed as a human right that's part of the universal declaration of human rights, and there are rights related to it, and it's also a right that's implied by the right to access information and right to free expression. If you can't access that information in privacy, then you may be subject to chilling effects, to retribution, one. The U.S. has a different tradition of privacy. American libraries recognize the right to privacy within the library context, since 1939, have its relationship to freedom of expression freedom, freedom of access to ideas. Privacy and confidentiality are identified in the list of core library values by ARL, by ALA, and it's based around the professional value of supporting free inquiry, but unlike the international tradition, there's no human right of privacy that's recognized in the U.S. The legal tradition and the rights tradition is more of a mosaic or patchwork. So legal requirements for libraries vary all over the place. If you're serving an internationally located, you're serving an international audience, which we increasingly do, whether or not we do that deliberately, we may be subject to laws such as the GDPR the General Data Privacy Regulations, which covers the EU and EU citizens, provides a host of rights for data protection and data privacy. If you're primarily in the U.S., serving U.S. patrons, there's no equivalent of the GDPR. There are some protections for library patrons under 13, under COPPA. The Patriot Act may require disclosure of information, on the other side. Many of us are in the universities, which work under FERPA, which has privacy rights for students. And state laws, also across the U.S., vary in their protection, from, they may have separate information data protection laws which apply to information collected at the libraries, and/or specific library information laws, most of which keep the libraries' information protected from open records requests. Here are some examples of state laws. We can also be under different contractual obligations. If we're processing financial information, PCI-DSS, personal data and vendor contracts and things. So there's a mixed background, there's a deep set of values that libraries have with respect to privacy, but there's a mixed set of formal law and regulation. And over time, our model of accessing information has changed, as has been noted. One of some of the implications of this change, as more and more content has become digitized or available in digital form, is that more content, more interactions are not taking place in the library itself, they may be taking place in a patron's office or a patron's home or a third place. They may not be interacting with content that is held and controlled by the library, maybe information that the library has acquired rights to, mediated access to, but is [inaudible] 05:58 accessed from a third party. And even those systems that we use to manage interactions with our content and our patrons may be hosted in the cloud somewhere, we may not be running those. So we're interested in how privacy protection in libraries has changed or maybe should change in light of some these shifts. So the other part background, and it'll explain where we're focusing, is that, again, no surprise to most of you, in the last 30 years there's been somewhat of a concentration of scholarly communications. So a lot of the content that is in university libraries and research libraries, patrons are accessing, come from a handful of publishers, and much of that content they access through portals provided by those publishers. Maybe mediated, bought, enabled by libraries, but the interactions may occur elsewhere. So we did some research, we looked at vulnerabilities in two areas. One in, what information can we see being collected? And then what do publisher privacy policies say they can do with such information? How strong are the protections that they're given? It turned out that the first is a lot easier to do than the second. We're interested in, are there systematic gaps? Our protection is different for things that we keep versus vendors, what practices are necessary, how are changes affecting things, those are broader questions. But here's an example of a [inaudible] 08:02 MTI, because that's where I live. A patron might use the MIT Library System on our website to discover a journal to which MIT has provided mediated access. The patron authenticates through their id gateway, accesses the journal through an MIT proxy. Perhaps, this one route. Content is provided through a third-party website, perhaps through that proxy, which may lead to further discovery, exploration, navigation. And content may be presented through that site, through a proprietary embedded reading. So what's potentially exposable here? Well, they typically don't get the write-off that clients name an MIT ID; they're coming through a proxy. So they don't necessarily get that, but there's a lot they can get; there's an IP address, that might be the IP of the proxy, okay. There are cookies; hm, well maybe those are local to the system. There is a browsing history, their reading and sharing history if they start to log into that reading app. There's also the ability to do fingerprinting of the browser characteristics. So without having a name and ID associated with it, from the information in that transaction, you may be able to figure out, this person is the same person that you hit you at seven other places, and also you've got their name from this service over here. You may easily be able to identify them. And if you can do that, you could target advertisements, you could recommend things they like, that would be good. You could also attempt to insert more tracking technology into their browsers, so you could learn more about them. Right now, we can't see what's going on at the publisher site once it goes to their servers, but we can see what's happening on the browser, so we can see what tracking mechanisms are used there, though not everything that's being done with that information. And there are a number of different tracking mechanisms or things that might protect a little bit; HDP is good because that protects against third parties, altogether different, outside from what you're trying to track. We look for things like ad placements, loading, external resources, cookies, whether you got directed to a proprietary reader app that required a login, and other things like active attempts to fingerprinting, JavaScript that does active analysis at the browser to try and figure out who you are, which goes beyond collecting mass information. And there are a number of tools that let you do this. Some of this an ad pops up, you can see that by visual inspection, IntraOcular Impact Test. Privacy Badger is one plug-in that will alert you of some of these things, DevTools, and the Brave Browser is a browser explicitly implemented to help you figure out when you're tracked. We used all of these things. These rows are different technologies. White is good, black is bad. So encryption is good if it's on, everything else is bad if it's all the way on. Columns are different vendors, so this is the distracting [inaudible] 11:58. So is this one. These are not so much, there's a little weakness in using an encryption. By the way, this is Elsevier, this is ProQuest. There's a difference in the pattern overall. Do we see this difference in policy as well and in the legal protections? So this is some of the things that are active traces that vendors in some cases are trying very actively to collect information. How do we evaluate the strength of the legal protections? This is a more complicated question than just looking at the tracking mechanisms. Who we used for since, we identify the reference framework for what privacy principles and guarantees are we trying ideally to measure? And we took these that you'll see from the NISO principles from ALA guidelines and from some of the larger, more granular GDPR protections. We need to merge these and reduplicate, so we harmonized across this set, and we'll show you a little of that harmonization, and then developed a measurement instrument, which is basically a well-worded question that we can evaluate the disagree, agree, or disagree about how strong that guarantee of that particular protection is. And then we conducted repeated assessments, repeated over time, so we have some before the GDPR privacy policies came in, which we won't discuss much but will be in the paper after. And we repeated across multiple vendors and we repeated across multiple raters to make sure that we did consistent ratings for inter-rater reliability. So for the top-level framework, we organized things around the NISO principles, and after looking at these various number of frameworks, this turned out to be a good set of top-level categories most of the requirements that were in other frameworks, guidelines, could be fit in at a broad level into the NISO principles. And they come out of the library communities, and that's nice. These are the 11 principles, I'm not going to iterate through all of them, but they include everything from accountability to security to education. And here's an example of a particular principle statement. These are not very ... at that level, they're not very detailed, they're broad statements. So we went to guidelines like the ALA guidelines and privacy and security checklists, and the GPR requirements, which are stated in more detail. Some examples. Then we developed a cross-walk. So under NISO principle, shared privacy responsibilities, it contains several different protections, and one of them is training, that people who are in library systems, in data controller organizations, should have security and privacy training. We map that wording. In this case, there's an analog to GDPR section and a rough analog to ALA section; rinse, lather, repeat. And we end up with approximately forty different scoring areas, and I won't bore you with all of those. But at the top level they fit quite well, the NISO principles are a bit more comprehensive than the ALA principles, and sometimes a bit more detailed. GDPR goes down into more specific requirements, as you might expect, and we don't go down to the bottom level, we go down to approximately the article level. And it goes beyond NISO in a couple of ways, one is to specify specific vulnerable populations, beyond children, which is something that COPPA addresses. To do the evaluation, we harvested privacy policies from each vendor, froze them so we would have a common copy because they are changing all the time, went through each subprinciple, designed a question around it by modifying the core of the NISO text and adding any, if necessary, specific additional protections for more specific regs like GDPR, converted that to a statement that you can agree or disagree with. And the idea is that we don't know how well the vendor actually protects these, we are evaluating what the policy promises to do, whether the policy describes what they're doing in this issue or promises to protect it in a particular way. How it's actually protected, this is not an audit. Anything you can make a statement, you can agree or disagree with any statement, we use a standard five-point scale. It's called the Likert scale; it has some nice properties. And then have independent coders rank these, compare for [inaudible] 19:01, which we are still iterating on, but the initial rankings are quite good. We will refine it a bit. So we've gone through two based on a previous version of the instrument, which was not as detailed. We looked at what might be, were the emerging best and worst players and used this revised instrument with more detail on multi-raters. One is the best score that you can get and five is the worst, five is very much disagree that they met this criteria. ProQuest gets a sold B - that's very scientific. The three is science, the B is [inaudible] 19:59. Elsevier gets approximately a D-. Of course, you can tell the fine detail here, with the red is Elsevier, color is worse, blue is ProQuest. Generally ProQuest is doing better at almost everything, there are a couple of things that they just forgot, which is why they both get fives. And my guess is that at ProQuest had paid more attention to detail in describing some areas there, the score would be even lower, it would be a bigger positive. But Elsevier is pretty good about telling you that they're going to collect - the reason they didn't get a five - is that they're pretty good at telling you that they're going to collect everything. They're comprehensively telling you the information they're going to collect, which is everything - not just everything that you give them, but they're going to go and buy information from third parties, they are going to instrument your browser and try to track you. They tell you all that, and they're pretty clear about what they're going to do with it, which is anything, which includes defending their intellectual property rights with third parties or with you. Anything they like. So points, they do get points for disclosure, but points off for not actually protecting the information the way that our library principles and values would house. There's a drill-in for that anything and everything. Here's a snippet of the ProQuest privacy. It's better in a number of ways. First of all, the language is much simpler, easier to understand, they do drill into the detail of what they do later in the document. They fragment as your rights over the data versus "we're collecting everything," they describe a data minimization approach, so there's a lot to like here. There are also some blind spots and lack of detail in different areas, but it is significantly better. What about the GDPR? What about the GDPR? Well, we did these analysis for the post-GDPR policies, those are the ones that you saw. We also looked at the pre-GDPR policies, but we haven't gone back and done the revised [inaudible] 22:51 reliability. But the post-GDPR Elsevier is still lousy, like you saw. And one of the reasons is because laws can change and that drives changes in compliance, but it doesn't necessarily drive changes in protection or protection for you, and so Elsevier says, "You have the right under European and certain other privacy and data protection laws, as may be applicable, to request access to [inaudible] 23:20 of your personal information, restriction of processing, processing portability, so you can request it. They don't have an interface to actually control it, it's not like the Google Data Management where you can turn off the data collection and take out your SMS texts and move them somewhere else, but you can at least request it if you're a European citizen and if they decide that law applies. So they're doing what is necessary for compliance, and your rights have increased if you're subject to GDPR, then you have more rights now. You have to. But this is a narrowly tailored expansion. Some discussion recommendations. So a summary of what we found [inaudible] 24:12. Increasingly, there's a misalignment between stated library values in privacy and data protection and privacy practices. Why is that? Because a lot of the ways that people are accessing content are in ways that we don't control as directly, so that has been maybe a blind spot. Data collection, assertions of broad use, and active tracking are very common in this space. And open access doesn't protect you; if open access is on a publisher site that has bad practices, they're applying those tracking practices in the OA site as well. Some publishers do better than others; the content portals like EBSCO and ProQuest were doing less tracking than even for content that came from the original publishers, so it matters which route you come to. And Elsevier appears to be the most invasive in terms of their [inaudible] 25:19. There is a difference between law and your legal protection, but the law is currently providing consistent protection because there's a patchwork of regulations that apply. Licenses provide little additional protection, in part because much of this data is not passed from the library to the vendor, in which case some of these riders might help. But is collected by the vendor itself as part of their processes, even though the library may have paid for the service and directed the user to it, branded it, etc. And the licenses are not built from the top down on privacy values and principles, as we now understand. So there are a number of areas, which my colleagues will talk about, at which licenses could be approved, and his may be low-hanging fruit. Improving, making them less opaque. Patrons don't even know what we've negotiated for them, all they see is the publisher click-throughs, generally, which are lousy. Current licenses are inconsistent, which makes them harder to interoperate, harder for patrons understand, and current licenses don't support evaluation. It may be difficult to evaluate compliance, or even to get the data back that is being collected to see how users are interacting with those sites and whether there's some issue, whether they're opting in or not. And just as a baseline, a couple years ago, Marshall Breeding did a quick look at privacy practices on ARL libraries, and library-hosted systems probably aren't doing so well, either. But we're not actively tracking this. There may be opportunities for designing privacy mechanisms into the way that we interact with vendors. Right now, there's no support for privacy-protected machinery. If you want a recommendation system, you have to give your personal data. There are ways to give good recommendations without revealing your personal information, things like differential privacy or cryptographic tools. Similar, there are ways to access gated resources by giving tokens without having to provide your identity. So there may be mechanisms that would enable users to protect themselves or libraries to protect users more, but they would need to be architected. And then there's a question about awareness and consent and control. Even when a university has a contract with the vendor that provides some protections to data collected from users, that might not be known to the patron, they can't act accordingly. They know that there's such a policy, often they can't view it, that also might be unavailable. That contract might even be proprietary, you're not allowed to share your contract with the vendor, with others. And it's almost certainly not presented to the patron when they're accessing the vendor systems. And there may be conflicts. If the library contract says, "You should treat the data just as we would," but the Terms of Service says, "Click through and you agree that Elsevier can do whatever you want," and they click through because it's their site, which wins? It's not clear. Probably, in lack of anything else, lack of better license and terms, it probably is the vendor policy that wins. So they're not designed in a way that makes them effective in either understanding or in controlling in this environment. So patrons access content that is purchased, mediated, even branded by research institutions, by library, but it may not be protected by the policies that we would like or that we have in place if their access is solely through our systems. And we need to design in some protections, not everything could be bolted on [inaudible] 30:13. There are opportunities to improve accessibility of services for human and machine clients and to explicitly support affordances for privacy like anonymous but authenticated grounds or privacy-preserving recommendation systems, that would enable more ways of interacting than simply going through the publishers' UI and being subject to all of the tracking there. And we very much need a standardized model community license that's aligned with library principles, that's transparent and verifiable and that is consistent for all of the patrons, whether they're in Europe or they're coming from Arkansas, and is also understandable so that people don't have to think, "Well, am I in the EU zone, do I care?" because these can be very long and difficult to understand. So those are all the problems, here are some references, and now to solve everything for you. [Lisa]: Hi, everyone, I'm Lisa Janicke Hinchliffe, I'm the Coordinator for Information Literacy Services and Instruction at the University of Illinois at Urbana-Champaign. This session that we proposed and then got combined together turned out fortuitous, I think, because one of the things that came out of the National Forum on Web Privacy and Web Analytics that was held earlier this year at Montana State University - and I want to particularly pause for a moment and give credit to Scott Young, who is the PI on this project with his colleagues at Montana State, brought us together. Lots of good ideas coming out of that symposium. But this particular idea seems actionable in the sense of, it's very concrete, which is, perhaps if we don't like what we are finding happening under the contracts that we negotiate, we should negotiate for something else. And so I'm going to have Katie talk for a little bit, and then we're hoping to have a little bit of a discussion. [Katie]: So, hi, I'm Katie Zimmerman, I'm the Licensing Librarian at MIT, I'm also the Scholarly Communications Librarian, so I negotiate a lot of these licenses for MIT. I also happen to have a law degree - people seem to find that important. So our proposal that we're hoping we will get some input on today is to develop and maintain a system of model privacy language for user privacy, so to address these concerns so that we're not relying on the vendor privacy policies, which change all the time, and as we've seen are not terribly effective. So if we don't like what they're doing, we should do it ourselves, we should do it better. This should be, as Micah mentioned, this seems like it should be a low-hanging fruit, this is something that the library community has done before and done well. We have LibLicense, we have other model licenses. We have the expertise to do this, so we really should. It facilitates communication and improves the efficiency in negotiations, as if there's something that has a community consensus already built up that you can just say, "Hey, can you adopt this?" then that makes it much easier to actually get it in there, it also makes it easier for the vendors to comply with if everyone's asking for the same thing instead of implementing a specific thing. If you're asking them to actually put in tech work and do a specific one-off thing just for you, you're probably not going to get that. But if everyone's asking for the same thing, then we can actually get some change. And it provides a consistent message, it's a way to let the vendors know that this is important to libraries and that this is what we want going forward. So starting points, the starting points are largely what Micah talked about, so the NISO framework, the ALA privacy guidelines, GDPR and its privacy framework, and the existing model privacy language, so I do want to acknowledge that there is user privacy clauses in most of the - I actually haven't looked at them all, but maybe all of them, all of the existing model licenses - but there are still some gaps. So the LibLicense language, for example, is pretty good on restricting vendors from passing data to third parties, but it doesn't really mention what the vendor is doing themselves. So Elsevier can't outsource that, but they can do whatever analytics they want on you themselves, which they probably are. But that is a basis that we can work from. And areas of consideration/components, I'm not going to dwell on this too much because I think it's more important to talk about it, but the things that have been identified in the project that I'm working on with Micah are things that we should be addressing in this language. [Lisa]: So this came out of, as I said, the forum at Montana State, but one of the big takeaways was, they said Katie probably shouldn't sit down and write model license language, especially Lisa. And part of the question is, a bunch of us, five of us, decided this was our passion project when we were in a room for two days, but we wanted to get some feedback from the larger community on the desirability of this, the feasibility, and then what would it take to actually operationalize, creating this kind of model language, where does it get hosted, how do we do this as a community? So, though we have other examples, we don't want to assume that just because somebody is hosting some other model license language, they'd like to host this, too. So this is really the open time for comments, questions, problem-posing, thoughts. Thank you, everyone. 