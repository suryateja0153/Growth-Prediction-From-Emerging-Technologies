 [Music] hi welcome to the security machine learning session at today's frontier machine learning event we have a great lineup of speakers for you today our first speaker is alexandra madley who's a professor at mit the director of the center for deployable ml and faculty lead for the csail msr collaboration on trustworthy and robust ai he'll be talking today about what our models learn our second speaker is don song professor at berkeley and ceo and co-founder of oasis labs she'll be speaking to us about ai and security challenges uh and the lessons we've learned in future directions uh don unfortunately uh will be giving her talk pre-recorded but will not be here for the live q a afterwards because of um because of family related travel our third speaker is jerry lee a senior researcher at microsoft and he'll be speaking about algorithmic aspects of secure and machine learning throughout the event feel free to add in questions in the chat and we'll answer them during the chat or at the live q a at the end of the session thank you very much hi my name is alexander madhuri and what i want to talk about today is what do our models learn okay and as usual this is a joint work with my amazing students so the point of start is the fact that you know it's no surprise to any of us namely that machine learning can actually often is unreliable of course you know one example of this is the notion of additional examples that we probably are all familiar with but this goes beyond that of just you know a month and a half ago and we had this you know the situation where on the highway tesla model 3 crashed uh you know without trying to even break into an overturning vehicle probably what happened was that this vehicle was out of distribution for this you know for the for the pilot for the tesla pilot and that's what it essentially said okay assume i don't know what it is so i can just go forward confidently and that's how the crash happen so things are not always uh working the the right way and the question is why does it happen this is something i am thinking a lot and my students are thinking about this a lot with me and kind of one answer that we already came up with was that kind of at the root of all of that is certain mismatch namely it turns out that if we look at all the tasks that we ask our model to solve like all the classification tasks that we ask our models to solve well there turns out to be many ways to succeed at this task and some of this waste actually the best of these ways might be different actually very different to how we as humans solve these tasks okay and this kind of is at the root of many of the unreliabilities that we observe in the real world and today what i want to do is i want to kind of go back to this mismatch but kind of consider it at a bit more meta level namely in the context of so-called classification task misalignment so let me explain what it is so kind of the idea here is that usually when we think about supervised machine learning we have always this kind of this view here that there is a data set and then there is a model that we keep training on the data set and we improve the model to get better and better accuracy and this is kind of this is exactly the bread and butter of machine learning nowadays and that's kind of what we focus on but kind of the point i want to make is that there is one element of all of this that we kind of usually don't think about in this element is that there is in addition to this data set there is actually a motivating real world task for instance object recognition and kind of our data set and the corresponding benchmark is meant to be just a proxy for the three word tasks and if you kind of think back to this missing piece the question that you should start asking yourself is okay so if there is this motivating motivating task that we really want to solve and then there is just this projection which is the benchmark we are actually trying to solve is there maybe a possibility of us overfitting to this benchmark and kind of this is exactly the question that i want to zoom in on in today's talk okay so in particular the question i want to ask is you know how well our data sets reflect the real world in particular what data set biases do our models pick up how are these biases introduced in the first place and you know the point of start here just to also give you some you know some kind of idea of what i'm talking about i want to study these biases in a very simple setting namely background biases so this is joint work with my students kai logan and andrew and kind of the idea here is that like look at again at the simplest possible bias namely how much the decision of a model depends on the background of an image even though we ask it to correctly classify the foreground of this image as we all know uh our models definitely depend on such backgrounds okay and you know that's actually not surprising because so do humans humans also use backgrounds when they solve the classification task it's easier for you to recall who is the person in front of you if they are actually in the usual environment that you interact with that person so if your work colleague you would see them on vacations it would take you a while to figure out who who they are okay so kind of so we know that background is definitely a signal so there's definitely a bias in our model but the question is like can you get a bit better grasp of like how much of a bias it is in particular how much it differs if at all to how you know humans use this bias in their classification okay to this end we created this uh you know uh just various version well as you took imagenet and we kind of separated into the foreground and background signal and created a bunch of different versions of this data sets to kind of allow us to get a fine-grained understanding of dependencies between different types of background signal and long story short what we did we just wanted to study how the model's performance is affected by different mixing and matching of this background signal and you know in general we rather like well there was a bunch of finding that i will not have time to go over all of them in this short talk but essentially realize that this signal really plays a major role in the performance of the models and just one finding i wanted to bring your attention to is this notion of an adversary backgrounds so what we found that actually for most inputs like over 87 percent of inputs we can fold the model into wrong classification by just choosing the worst case background for this image so the foreground picture is the same and we just choose a you know adversarial background and suddenly the model is fooled into misclassification of this foreground object in fact this gets even more interesting namely it's not only that for every for most of the images there is necessarily a background it's actually like some of the backgrounds are adversarial for many foreground objects so here are just some examples for instance my favorite one is the one in the bottom left corner which just shows that a person holding something turns out to be a very strong you know signal model that whatever is in the foreground is a fish even if this is not a fish at all okay so this kind of shows us that like things are not exactly as we would expect them you know again humans use background but they think he wouldn't be fooled by that so you might ask what would it take to get models that do not have these problems okay and in the paper we showed that even just the simplest possible thing like randomizing the background uh you know during training especially like breaking this correlation between the foreground and the background already helps tremendously and more importantly even if you don't try to explicitly train against this you know over reliance on backgrounds what you get is that you know more accurate models essentially models that do better on imagenet also end up being more background robust interestingly that doesn't mean that they don't rely on backgrounds they actually do rely on background but they rely on it in a way that actually it makes it less prone to being fooled by adversarial backgrounds okay so this was just a very simple example of the bias and the study of a fine green study of it but now you know kind of you want to ask a broader question so you know okay so there are these biases but where where do these biases come from especially can we have biases that come from not just from the nature of the visual world but actually from the way our datasets are constructed and the spoiler is the answer is yes but let's look into that so essentially the point that there is many biases that our dataset convey to our models so just to give you a describe to you one of them is particularly interesting let's take a look at these three images from the image that from the image and data set and three immigrant labels corresponding to this images if you look at these images and in this labels you probably will say that okay this is this this look like correctly classified image of classifier the imagenet input however what we what you realize is that actually none of these three labels is correct according to images labels and the correct image that labels are now in the image so this is kind of says okay so maybe we are not as good as humans and image and classification as you would like to but then you might think and saying well where are these initial labels actually the wrong labels you know if you look at the image on the left you know it could equally be monastery or a church and you can't really tell which one is which or which one is more right answers over here so these are kind of just some you know handpicked examples of like the images labels maybe being a little bit kind of not exactly reflecting the ground truth of the image but you know the question is okay so is this just you know just some outliers or is it kind of significant it does signify some deeper problems with our images labels okay so what might be the problem and kind of you know when you look into it you realize this is not these are not outliers by any means and essentially kind of the issue here is the way our data sets are created so kind of when we usually think about how our data sets are created we think of this process when they are like real-world images and then there are expert annotators that essentially choose for each image the perfect you know the perfect class to er to label this this image with this class so that's how we idealize this process but that's completely not scalable this is not how we can you know images how we can get labels for millions of images so what we do instead like one of the most popular ways of doing it is to kind of go kind of the other way around so what we do is we first settle on what are the possible labels we are interested in and then essentially we source the images by just plugging this you know desired class labels into search engines and getting all the candidate images for the given cell for the given uh for the given label and then essentially well just to make sure that the images we source that actually really images corresponding to the desired label we do the crowdsource validation and what we do essentially we just you know a long story short we just present this image to a to a cross sourcer with a desired label and we ask does this image contain object of the class described by this label and depending of you know whether the answer is yes or no we either keep this image as correctly labeled with this label or we discard it so this is nice because it scales very nicely especially when you ask these questions in batches but the problem is that we kind of it's a very leading question so we always ask only about the single candidate label for a given image and we never make the you know the human we are asking even aware that could be possible like that could be some other classes that could be correct for the same image so now kind of this is a problem this clearly introduces some bias there is kind of some default label for the image and if the default label for the image is not correct then it's just discarded and now the question this is the paper that we recently published you know asked the question okay so how much of the problem this is for imagenet and the answer is well it is a problem and but like how do we actually try to get a hold of this well for now well to even get any to perform any kind of study of these questions you have to first get more detailed annotations just because you need to confirm the current imagenet annotations against some more grunt rule annotations over there so how do you get them well this is actually kind of tricky but what we ended up doing was actually relatively simple or actually surprisingly successful what we did was essentially like looked at a bunch of image net train models and we kind of got the top five predictions from these models for for each of the images that we considered and this way we got a narrow down set of classes that could remotely be plausible for this image now once we manage to essentially get a small collection of possible classes for each of the images that could be the candidate classes we just ask the amtrakers to essentially like you know look at the image and ask them okay for each of these possible classes you know would this was this label be valid for this image and also we actually asked them about the information to give our information about how many different objects is there in the image and also what would the main object in the image according to them be and we did it of course you know we upgraded over many workers and this way we got this like very fine-grained grunt through annotations for the you know for a subset of the image and validation set so this was actually like exactly what we need for doing the step that i will describe in a moment but like i just want to point out that this is actually can be viewed as a nice bootstrapping of the original image annotations in some ways we got this initial image and annotations that kind of got got created in the way i just described but then we kind of iterated on this via training models and then uh you know annotating using the labels directly by these models to kind of bootstrap this knowledge and clarify this knowledge and kind of and distill it in just a little bit more and we think that this process will be useful also in other contexts but yeah now that we have this grunt through label or morphing great labels we can ask ourselves okay so how accurate the original image labels are well we discovered a number of interesting interesting phenomena so the first one was the prevalence of multi-object images it turns out that around 20 of test images contain more than one object here is one example it's an extreme example that has really multiple multiple objects but there is like 20 percent of test images that actually have at least two objects in the in the picture okay so how does it affect accuracy well remember you know like we ask our models to always answer just like we are talking about top 1 accuracy so we are kind of we just want this the model to output what is the correct label and if it's wrong we just say the model is wrong so uh what it turns out is that actually if you look at the accuracy of our standard modulus on the single uh labeled images so images just with a single object this accuracy is actually much better than what we view as the kind of state of the art and kind of only when you look at the performance on the multi-label images then this performance becomes actually significantly worse okay and what is even more important is that if we try to correct for this kind of obvious unfair situation when there can be multiple objects in the image and what you essentially do you make the prediction be correct if it matches either of the images either of the objects in the picture well essentially this performance gap disappeared and it turns out that essentially you know whatever the current models are lacking in terms of performance on them on this multi-label uh multi-label images it actually disappears and can be like largely explained by just taking into account the fact that there are there are multiple objects in the image so this is nice and now but now the other question that kind of comes up in this context is saying okay if there are at least two objects in the image you might ask okay so which one should the model answer if it's forced to answer you know like to choose only one of these objects and kind of what we found out to someone surprising that often the choice that the model makes is not the one that the human would make particularly that the object that the model identifies is not the one that the human would view as the main one in the picture so here are just some examples and kind of you know what's happening here is that like you know since uh you know the the image sourcing process of the er well that created that we used to create an imagenet kind of was more like willing to use this kind of more specific uh and unique labels for the particular images because you know you get images as a response to your query well our models also have learned that this is the way to go and they do pick up on these dataset biases even though these biases go against human preference so this is kind of another thing that we find in terms of discrepancy and you know there is a bunch of other observations that we had and i invite you to look at the paper to to take a look at them but the last question i wanted to ask and kind of that we asked in the paper is okay so once we have this correct ground trend ground truth labels and when we have once we have access to human annotators we can ask how good immediate models really are while once we account for these issues with labeling so in particularly we run this human-based evaluation in which what we did is that whenever we presented a model with an image and we got some answer we did not just compare this answer against you know the the image and ground truth or against our gram truth uh labels even what we did we just asked the person okay here is an image and here is a pro like and here is a possible class is this class a valid labeling for this image so essentially we just ask humans to evaluate the validity of the labeling over by the by the model what did we find well what we found is that the good news is that essentially like as our models improve in terms of just pure image and accuracy so does this human base evaluation also improve so so far so good our current driver of the design of the system seems to be aligned with the you know with the you know quality as assessed by humans however at this point it turns out that annotators often can't tell apart you know the predictions of the model versus the correct images predictors so in some ways you can claim that at least for some classes essentially our current models already hit the baseline of you know non-expert human annotators and kind of this gives us a question okay so like how useful it is to try to improve the performance on the imagenet further maybe essentially if nonexpert annotators is our golden standard here maybe imagenet is already solved and we should be looking for some other data set and other tasks to kind of to try to you know improve our computation models further okay that's all i wanted to say so let me just summarize and and talk about some takeaways so first of all i hope i made it clear that you know what modules do and do not learn is not always clear to us even though we kind of intuitively assume we do and we really might need to study this before we focus on further improving performance on our current benchmarks and this is a particularly important if you think about robustness because because exactly like problems robustness emerge from these biases that are misaligned with the biases that human uses use also you know like as we showed models are affected by biases of the world and all of our data data pipelines and you know once we understand what these biases are we can find ways to explicitly account for it and there is some recent work um with jacob steinhardt in my students that kind of show how to do it in a more refined form for some more subtle biases that arise in our data pipelines and finally moving forward you know the question is you know what are the other biases that our models learn from the data and you know how do we train the models in our in their presence essentially okay for some biases we actually might want our models to pick up on them but for others we might definitely not want our models to use them and how do we kind of choose which one we do which one we don't and how do we enforce our models how how do we force our models to obey these wishes and also you know how do we measure you know performance on the underlying task we care about as opposed to just overfitting to this benchmark itself okay so how do we how do we do it in a way that actually really gets at the essence of the real world task you want to improve on and not just an artificial number that's just a proxy amin to an end so this is all i've got thank you and i'm happy to discuss it during the q a session thanks hi thanks for being here my name is dong song i'm a professor at uc brooklyn and also the founding ceo of oasis labs today i will talk about ai and security challenges lessons and future directions as we all know deep learning is making great advancements for example alphago and alpha star have all won over world champions and deep learning is empowering everyday products as we deploy deep learning and machine learning we there's an important aspect that we need to consider which is to consider the presence of attackers it's important to consider the presence of attackers as we deploy machine learning for a number of reasons first history has shown that attacker always follows the footsteps of new technology developments or sometimes even visits and also this time the stake is even higher with the ai as ai can choose more systems attacker will have higher and higher incentives to attack ai and also as ai becomes more and more capable the consequence of misuse by attackers will also become once more severe as we consider machine learning in the presence of attackers we need to consider several different aspects so first how attackers may attack the ai systems attackers can attack the ai systems for a number of in a number of different ways one attackers can attack the integrity of the learning system for example to cause a learning system to not produce the intended uh correct results and can even cause an immune system to produce targeted uh uh to produce targeted outcome designed by the attacker and also the attackers can also attack the confidentiality of the learning system for example to learn sensitive information about individuals from the machine learning systems and to address these issues we need to develop stronger security in the learning systems attackers can also try to misuse ai and learning systems for example they can try to misuse ai to attack other systems including finding vulnerabilities in other systems try to devise new attacks and so on and also attackers can try to misuse ai to attack people people are often the weakest link in the security system we have already seen examples such as deep fake and fake news that actually can be generated from ai systems to address these issues we need to develop stronger security in other systems so given the time limits of this presentation i will mostly focus on the first aspect how attackers can attack ai and learning systems and what we can do about it and first let me talk about the integrity aspect how the attacker may attack the integrity of the learning system and first let's look at a motivating example and in the context of self-driving cars for the autonomous vehicle as it drives through the environment it needs to observe the environment for example to recognize traffic signs to make correct decisions these are photos of real world traffic signs here stop signs and the second column shows a real world stop sign example in berkeley and today's learning system can actually work very well and recognize these stop signs very well as you can see even when the real world stop sign impromptu has some markers on on the stop sign but however what if the attackers actually create a a specific perturbations to these traffic signs uh for example adding a specific perturbations designed to form uh the learning system in this case the image classification system and try to fool the the image classification system to give the round label so in the third and fourth column here you you are seeing a real world adversary examples that have been created to then food the learning system to give the wrong answer in this case misclassified stop sign as a speed limit sign so as you can see atmospheric examples when they are constructed effectively they can fool the linear system to give the wrong answers and one question that we have been exploring is besides uh adding perturbation to digital images can adversarial examples actually exist in the real physical world and also can they remain effective and the different viewing distances angles and conditions and here the images here showing the examples of adversary examples that actually are created in the real world and showing that in the adversary examples can actually be effective even in the physical world and can remain effective under the different viewing distances angles and conditions and the real world adversarial example uh traffic sign the stop sign that we have created have actually been on exhibits at the science museum in london and teaching people that uh it's important as we develop learning systems and to pay attention that they could to these learning systems can be fooled by attackers and cause them to for example make the wrong prediction so that's just one example assuming that the adversary examples can full image classification systems uh other researchers and my research group we have all been exploring this important phenomena across different uh domains in different in deep learning and unfortunately what we have found is that adversary examples are not just limited to image classification system and in fact they are prevalent in different types of deep learning systems including different model classes and for different tasks including generating models deep reinforcement learning and and many others and also adversary examples can be effective under different thread models as well besides um besides the early example i showed called white box attacks where attackers need to know the details of the learning systems and including the actual model the attacks actually can also be effective in what we call black box model so in a black box model the attackers actually don't know any details about the model itself including the model architecture and the weights of the model and our work and other research's work have shown that adversarial attacks can even be effective in in this type of model in black box attacks and we have developed different types of black box attacks including xero career attacks and where attackers don't even have access or query access to the to the victim model to be able to actually uh develop develop effective attacks on the victim model and also when attackers have a query query access to the victim model attackers can develop even more effective black box attacks so now let's look at a complete example in one of our recent works in this case we are actually studying the deep learning model used for machine translation machine translation is an important task and has made a huge progress so in in this work we essentially showed two results one is a state-of-the-art machine translation models actually for example has that has been developed by google and others it actually can be easily stolen through the query access using what we call a model stealing attack and as a second step once we can still the model essentially build the imitation model of the original model we can then develop adversarial attacks on this imitation model and then using what's called a transfer attack we can then use a text that's been developed on the imitation model to then successfully attack the real world's the remote model so so in the first step our work shows that we can actually very effectively uh develop this imitation model by doing a number of queries to the machine translation api in the real world for example um with the google translate and at the clouds api services we are able to to build this imitation model that can achieve uh close a very close in performance to the original for example google translate and so on based on standard benchmarks and that's the first step and then in the second step using the the learned imitation model we can then uh develop adversarial attacks on the local imitation model and then um and then using the the transfer attack to show that to demonstrate that these attacks that we have developed using the local imitation model uh they can be effective on the on the cloud service the cloud api the real world model as well and we consider different types of attacks so here is a a type of attack called the targeted flips the goal here is to replace an input token in order to cause a specific output token to flip to another specific token so for example here um we have an english sentence i'm going to freeze is now below 6 fahrenheit please help me so with the google translates uh with the original sentence it provides the correct uh translation in german as shown here and using our attack methods based on a loss function that we construct and using optimization based methods we are able to create an attack here by just changing uh one and one token here changing from the six fahrenheit to seven fahrenheit when we feed this new sentence i'm going to freeze it's now below seven fahrenheit please help me when we fit this this input to the to the imitation model then the translation mostly remains correct but uh now instead of translating it correctly to seven fahrenheit it actually translates into 21 celsius so as you can see this type of attack could cause severe consequences uh as they can be fairly stealthy and only change the important uh important parameters uh in the statement and when we upload these quick these constructed attack sentences to the actual google translate api it shows that it provides the same translation results showing that the attack is successful there are other examples as well so here uh showing that we can create easily created these nonsense sentence for example in english it looks nonsense but then when it translates through this machine translation it actually translates into something that can that has real meaning and also under the construction uh our specification by the attacker so for example in this case we can create this nonsense sentence and then here in the first one it translates into i have to kill you and in the second one it translates into another essentially a malicious uh ill meaning sentence and again when we feed these constructed attacks to the real world cloud apis we see that the attacks remain effective so these are examples demonstrating that also adversarial examples are not just limited to vision they are also effective in for example natural language domain and this really is a very rich field there has been many different types of attack methods that have been developed based on optimization methods on different metrics beyond the l norm matrix and also including non-optimization based attacks so for example in some of our earlier work we also showed that one can use against to generate adversarial examples as well overall given the importance of this domain we there has been a huge volume of work and different types of approaches proposed for defenses against the adversarial attacks but unfortunate for example just in the last uh last couple years there have been uh hundreds of papers written on the topic however today still we don't have sufficient defense today today strong adaptive attackers can still easily evade to this defense and what we have uh discussed so far is just the tip-off aspect it's in the general area of adversarial machine learning adversary machine learning is about learning in the presence of adversaries and attacks can happen at the different stages of learning one it can happen at the increased time as adversary examples that shows that can fool the learning system to give the wrong prediction at influence time and attacks can also happen at during training time for example attackers can provide a poisoned training data sets for example including poisoned labels and our poisoned data points to food the learning system to learn the wrong model overall adversary machine learning is particularly important for security critical systems i strongly believe that security will be one of the biggest challenges in deploying ai and this is just the first aspect of how attackers may attack the integrity of the learning system and attackers can also attack the confidentiality of the learning system by trying to learn sensitive information about individuals here let's also look at a quick example and this problem is particularly important as as we know essentially data is the field of machine immune systems and a lot of this data is really sensitive and hence as we train machine learning systems it's really important to ensure that the learning systems provide sufficient data privacy for individual users in one of our recent studies in collaboration with the researchers from google we set out to study the following question do neural networks actually remember training data and if yes can attackers actually exploit this vulnerability and to try to extract the secrets in the training data and to from just simply creating the learned models and in our work we showed that unfortunately this is the case so in particular we studied the task of the language model training a language model and when we change language model when we give a sequence of characters or words to the model the model will then try to predict the next character on the next word and our work showed that as in one example we showed that when we tried to train um a language model using uh unrun email data sets which naturally contained the actual people's credit card and social security numbers and attack here by just recording the learns language model it can actually automatically extract the original users credit card and social security numbers from just creating these trained models and this demonstrates uh the importance of protecting users privacy as even as we train machining models and luckily in this case we have a solution uh in this particular case instead of training um a vanilla language model instead if we train a differentially private language model our work showed that the um the trained model actually can significantly enhance the privacy protection for users data privacy and at the same time we can still achieve similar utility and performance so given the interest of time i won't go into the details of explaining what differential privacy is it's a formal notion of privacy and we also have done recent work in developing techniques and tools to automatically verify that a machine learning algorithm is is differentially private and the work called the duet has won the distinguished paper awards at the recent conference programming language conference finally i just want to conclude with another important topic related to machine learning i'm currently a responsible data economy as we know that data is critical to the modern economy and and given that a lot of the data is sensitive so we are facing unprecedented challenges in how the sensitive data is being used individuals are losing control over how their data is used and they are not getting sufficient benefits from their data and also businesses are continuing to suffer from large-scale data breaches and it's becoming more and more cumbersome and costly for businesses to comply with new privacy regulations such as gdpi and ccpa and it's difficult for businesses to get access to to data due to the data silos and privacy concerns what we need is to develop new technologies that can unlock important values in data but not at the cost of privacy and hence there's an urgent need for developing a framework for building a responsible data economy this is something that's my research group at uc berkeley has been working on and also we are taking some of the research technology into the real world at oasis labs and in particular to build a platform for responsible data economy by combining different privacy technologies as well as blockchain technologies to build a secure distributed computing fabric to enable users to maintain control of their data and write to the data and also at the same time to enable data to be utilized in a privacy-presuming way and one of the first use case that we will be launching using this technology is in the genomic use case to enable users to for the first time become owners of their genomic data and also at the same time to enable their genomic data to be utilized in a privacy preserving way and also uh we are launching a summit called the responsible data summits and the first two tracks on responsible data in the time of pandemic uh discussing how we can use it responsibly in this in this special time of course 19 and also how to utilize responsible data and technologies and develop responsible data policies in the real world please visit responsibledata.ai so to summarize there are many challenges at the intersection between ai and security how to better understand what security means for ai and learning systems how to detect when the learning system has been fooled and compromised and how to build more resilient debating systems with stronger guarantees how to build privacy preceding learning systems and how to build a responsible data economy and this is in collaboration with many of my students post-ops and collaboratives again i strongly believe that security and privacy will be one of the biggest challenges in deploying ai and building a responsible data economy is critical these require community efforts let's tackle the big challenges together thank you hello in this talk i'm going to be covering a number of recent advances in the development of algorithms for secure machine learning this talk is going to be covered structured in two parts in the first part i'm going to be covering recent advances in defending against attacks at train time and in the second i'll go over defenses against test time attacks uh this talk is based off of a number of papers some of which i've listed here okay so let's just begin so let's get started with robustness at train time so there's a number of reasons why training data might be corrupted for instance imagine you're in some crowd source setting like federated learning where you cannot fully vet all of your data points and there could be malicious entities adding in data to try to change the behavior of your learned model alternatively another very common setting for scientists in particular is when you have very large data sets collated over many different labs and gathered by various different people using various different equipment as a result the data is very heterogeneous and this can cause uncontrolled systematic noise which can look just like outliers and i want to mention this is really this is really an issue in practice a striking example of this is the so-called backdoor attacks which don might talk about where uh adversary can add a small amount of carefully designed data to the training set um such that when a standard deep net is trained on this corrupted dataset this network behaves as if it's normal on regular data points but when i feed in data with a pre-specified perturbation like as you can see for instance like this flower or this sticker the network misclassifies the image okay so this is this for instance a very big issue if you're trying to use this for self-driving cars or this kind of stuff the fundamental difficulty however with defending against these attacks is that what it means to be an outlier is very unclear especially when we go to high dimensional data um and constant data this sort of the picture that we have for data which is sort of like a bell curve or at least relatively close to the mean not too spread out so if the outliers don't want to be too obvious then they can't be too far away from anything and so usually they cannot affect any estimates or any classifier by too much at the very least but this picture changes dramatically in high dimensions so obviously i can't change i can't draw high dimensions here um but typically in high dimensions everything even in liars which are the blue points are very noisy so typically they're very far from the mean and individually have very little signal to noise as a result outliers can look individually just like in liars so as you can see here each one of the red dots looks just fine but in aggregate they can really mess up your classifier quite a lot so that means somehow we must look for outliers at a much more global level so this is something that was uh first considered by statisticians in the 60s in a field called robust statistics which is something that really even predates modern machine learning and it says that a data set is corrupted or epsilon corrupted i should say if there's an epsilon fraction of outliers which corrupt basic statistics of the data for instance like the mean or the covariance so the formal theorem model is as follows we have some data set represented by blue points but we don't get to see this nice uncorrupted data set instead we have to give it to a malicious and powerful adversary this adversary gets to look at the data and decide to change an epsilon fraction of the data however they choose and this this adversary is like all powerful in particular they have full knowledge of the algorithm and everything that the defender is trying to do then the data is returned to us after being corrupted obviously without the colors and then our goal is to learn statistics or information about the original uncorrupted dataset for instance the mean or the covariance unfortunately this is a pretty difficult problem at least for a long time especially in high dimensions because for over something like 50 years all methods for high dimensional statistics uh over generalizing slightly uh either were computationally attractable in high dimensions or had error which scaled really poorly as the dimensionality increased so essentially you either just couldn't run the algorithm or you could but the output you get is just meaningless okay this was finally broken actually in 2016 uh when my collaborators and i along with a concurrent work of life rav impala we're finally able to break this curse of dimensionality by giving the first polynomial time algorithms for some of the most basic problems in robust statistics particular robust mean estimation at a high level the idea is that if we can detect corruptions to the mean uh sorry we can detect corruption to the mean by inspecting spectral properties of higher order moments pictorially the the pictures as follows so suppose we have uh this corrupted data set where the blue points are the inliers and the red points of the outliers and the true mean is supposed to be this blue x but because of the outliers has been dragged over here well in particular notice that along the direction in which the mean was moved there must be a lot more action and there should be just because all the all the red points are sort of in aggregate kind of pushing in this direction in particular the data is supposed to be spherical but because the bad points are dragging this direction this causes a lot more action than there should be in this direction and so the actual covariance is rather than spherical more like elliptical on its direction and formally this manifests itself as a large eigenvalue of the empirical covariance so i can't get into too much details um for the sake of time but from like a thousand feet in the air this says that uh the more that a single data point contributes to large eigenvectors or the eigenvalues of the covariance the more outlier-like it is we were able to measure this quantitatively by devising a score function based on something called quantum entropy regularization which allows us to measure this in a principle manner uh so i'm going to flash some equations here mostly just to make myself feel better but you don't need to really understand this but the point is that these scores which are the tau eyes are things that you can really easily compute in like nearly linear time uh and if they're larger then the points must contribute uh more to like being outliers uh i should say that this is actually formally used in our paper to get really fast algorithms for robust municipation particularly nearly linear time algorithms but by themselves they're already just useful as an empirical tool for outlier detection okay and we can measure this we measure this in a number of basic settings again i don't want to get into the details although i'm happy to take questions about it offline uh and we found them essentially that these empirical quantum mechanics scores or sorry these fundamental scores we're just consistently better at finding outliers in a number of settings so here are some plots i'm just going to flash uh roc auc is some sort of measure of how effective the the method is at detecting outliers and as you can see sort of we consistently do better on this test than previous methods in particular if i could draw your attention to the plot on the right so in particular ours is much better um again against all these previous benchmarks and this is also true in another setting which again i don't really want to get into too many details about okay but that's that's the high level idea and i should say i've only covered just a little bit of sort of this sort of wave of current work on dated poisoning but there's been a lot of exciting progress in this field that i can't cover so for instance we've had provable defenses against stochastic optimization which is a task that generalizes training stuff like deep nets as well as practical and proven robust algorithms for many other objectives not just mean estimation as well as empirical defenses based on sort of these same ideas that we've developed for these backdoor texts that i mentioned at the beginning of the talk against deep networks and much more okay but that's all i really want to say about that now let's pivot to robustness test time so as alexander has presumably already discussed uh adversarial examples are a well-documented phenomena in deep learning namely uh it's well it's well known that you can take an image any image and add imperceptible changes to the pixel at test time and you can cause a standard neural network to reliably misclassify the image and i want to stress that while this was first developed in academic context this is not just an academic concern so this is something that happens in the real world and can be carried out in a number of different settings so here are some pictures for instance they can cause stop signs to be misclassified speed limit signs or cause facial recognition to misclassify people and sort of you can imagine the security vulnerabilities of all these sorts of things now broadly speaking there are two types of defenses that have been proposed against these attacks uh the first type of attack is empirical the uh sorry two types of defenses attacks that have been proposed the first type of defense is empirical namely defenses that seem to work well in practice and that we don't know how to attack but we don't know how to prove work however many of these have been proposed and then subsequently broken in like weeks or months of publication so this is there's a sort of you know cat masking here one notable exception to this is the adversarial training framework of madrid all which to date is still unbroken and if i had to guess likely is genuinely robust but again we can't prove it but to try to break this sort of cycle of attack and defense attack of defense another paradigm is that of certifiable defense and these are defenses which probably cannot be broken however they usually pay something for the certifiability in particular a lot of these defenses that have been proposed don't scale or get much worse numbers than empirical defenses however one recent and very promising approach that my bridge just got something called randomized smoothing so the idea behind randomized moving is very straightforward the formal definition is here but again i don't want to get into the formulas or pictorially you can think of it as the following suppose that this picture is uh the surface of your classifier so that every region uh with a color is a region of space that's classified as one class so all the blue points are classified as one class all the cyan points are classified as another and so on and so forth but this is your base classifier you then want to smooth this classifier so given the point x the smooth classifier samples a bunch of random gaussian points centered at x and counts what fraction of them fall into each region the corresponding histogram that you see is the likelihoods that this moves classifier science to each point so here notice that the smooth classifier still assigns the blue region as the most likely class for x at smoothing but this is not always the case so the reason why we care about randomized smoothing is that recently there's been a lot of work culminating in this paper by cohen at all which proved the following very strong robustness guarantee for any smooth network again for the sake of time i don't really want to go into the details but at a high level it says that if your smooth classifier is very confident at a point x uh then your smooth classifier is also robust for some l2 radius or some l2 ball around your point x okay but again i don't want to get into too many of the details here but it just says that all you need to do is you need to train a smooth classifier to be very confident at a point and if your smooth classifier is very confident at that point then you have robustness to adversarial perturbations sort of for free but this is not a trivial task how do you actually train networks such that the smooth classifier is effective what if like if i add gash and noise to my data and everything all this pattern all these like nice patterns and everything are destroyed turns out you can do okay um so cohen at all the paper i mentioned already does pretty well already with some basic gaussian data augmentation a very classic technique anyways but in recent in the recent work that has appeared in europe's we showed you can do much better um namely by combining uh this framework of randomized smoothing with the empirical defenses of adversarial training and particularly we showed that by directly robustifying this booth network using average sale training on some sort of smooth loss you can train the network such that the resulting network is much more certifiably robust which is kind of nice because combining certifiable defenses by combining certifiable defenses with empirical ones one can dramatically improve improved certifiability which is not at all clear up here right since the empirical defenses come with no guarantees whatsoever and again for the sake of time i don't want to get into too many details but you can see here briefly that indeed our method dramatically improves upon the previous state of the art and indeed this is kind of cool sometimes our certifiable accuracy so our provable accuracy is even higher than the empirical accuracy of the previous smooth networks which is something that you know it says that our our network is just genuinely much much stronger okay so this is these are results in c410 uh again i don't want to get into too many details about these results we also have results for imagenet and axi by combining our techniques with other uh heuristics like pre-training or this kind of stuff you can even boost these numbers even higher okay but so far this has been all about l2 now what about other norms like lp particularly l infinity is a sort of standard benchmark for adversarial examples and norms like l0 or l1 correspond much more closely to adversarial patches which are these like real world attacks that we saw earlier in this talk however gaussian smoothing is no longer effective for these norms gaussian smoothing is very much tailored for l2 so how do we how do we try to attack this problem uh in workdays to appear at icml this year uh we you know with many caveats begin to characterize optimal smoothing classifications for many other norms so we begin to develop this theory of like how to do randomized smoothing much more generally so from the upspan side we give formal methods to derive you know again with caveats and sort of quote optimal sampling schemes for arbitrary norms based on geometric objects such as wolf crystals which is some object that arises naturally in physics as well as some other methods which i won't get into and from the lower bound side turns out you can actually show very strong lower bounds demonstrating that randomized smoothing has inherent limits at least current techniques for it for a number of norms based on the sort of rich theory of geometric metric compatibility uh so in particular for l infinity uh the best you can do is actually just pretend that you know embed l infinity and l2 uh by losing this root d factor and then use gaussian smoothing so that's kind of neat okay uh um this this sounds pretty you know abstract but it turns out you can actually show that these methods improved methods yield improved certifiability compared to the prior state of the art in practice so here are some results where you can show you know by using these smoothing techniques that we developed before uh and then just just trying them out just trying these smoothing distributions that we tried out you can actually dramatically improve uh the previous state-of-the-art numbers up to like 20 or 30 percent so despite the fact that this is you know using some very rich mathematics very deep mathematics is actually just very easily practical or very i should say yields dramatic improvements in practice as well okay so that's pretty much all i have to say just to wrap up let me just say that this is you know this this field of algorithms for secure machine learning is a very new area of study and many new algorithmic ideas are still needed to overcome many security issues that we face uh in practical ml uh okay that's pretty much it if you have more questions i'd be happy to answer them at the q a thanks hi and welcome to our live q a uh following our security and ml pre-recorded talks with us today for this uh live session is alexander hermody and jerry lee so um we're going to be taking questions via the same chat interface so feel free to add in any questions you want for either alexander or jerry or or both and we'll we'll talk through them here to get things started uh i wanted to ask both of you um about a question that was touched on briefly in the live in the live chat during your talks i'm curious if you would say a few words about how you would define robustness kind of in the ideal way some mentioned defining it based on for example how a human would behave what's your take let's start with alexander sure as i already said in the chat you know this is a great question and there is no good answer so in some ways like i think just part of the reason why there's no good answer is that like to us robustness means so many things and we try to pack it into one definition so yeah so one definition that kind of much of my work is focusing on is saying robust means robust to whatever human would be robust to and this kind of makes sense in the context of you know vision or sounds uh and yeah and that's one definition of robustness is a very difficult definition of robustness because it's essentially like if you wanted to take it too extreme we would need to essentially like you know like solve a like agi because you need to have you know synthetic humans but then there is the other definition that we kind of it's it's closer that i think we don't think enough but we should especially in the domains like system security or any domains where humans are not the golden standards we want to attain and then this question is just robustness means providing some invariances that we would like our model to obey okay so for instance like we just want to know that if i you know if there is a stream of packets you know if you know actually this micro might not be a good invariance if we kind of you know move the timings of each a little bit by not changing the order it should not affect the you know it should not affect the performance so essentially this is like different kind of priors we embed into our model because we know that you know the they should not be material for the decision so this is the whole new definition would you say it's just about that but yeah what robustness means is really like in the eye of the beholder yes how about you jerry what do you think yeah i mean i think alex alexander is clearly correct there really is no one complete definition of correctness or sorry robustness i i should say that you know adding on to what alexander was saying i think in large part uh what we do or the stuff at least that i work on is understanding um like how to make models robust to miss specifications in the model when the data distribution is has some sort of drift and this drift can be caught be because of either attackers or just because of natural data drift and so this is another way we can think about things which is also just i guess that's another way of rephrasing this idea of building into variances but it's a very difficult question let me just say one more thing is that it's a very difficult question so like with as if all all the difficult questions you should try not to you know confront it head-on instead we should just try to look at concrete context and some like small pieces of it and that's how we build the intuition and hopefully a generalizable toolkit so you know let's not despair right away yes we don't have to borrow the ocean all at once we can start small and understand specific situations i totally agree with you um kind of related to this i guess uh definitional questions uh there's also some discussion about threat models um and i think uh i'll start with you jerry how do you start to think about the different kinds of threat models and how do you choose which ones to focus your work on what kind of benefit do you do do you think you get by switching between different threat models from time to time what does that how does that help you so this is very much related to the question that we just asked because certainly developing a threat model against which we need to be robust is really the question of what does it mean to be robust um from this perspective i think it really depends on the motivation of the problem like or the context of the problem suppose i'm trying to make a network robust in some real world setting i need to understand what sorts of attacks uh one might actually care about that setting or if i'm just trying to make some sort of algorithm robust um outlines training time you know what what are the outliers gonna be um or like what kind of realistic constraints can i impose on the outliers um so this is sort of i think the way that is most natural to me it seems to be that we need to take a very like contact specific approach to this try to understand you know what are the actual constraints that might matter in practice and then try to build that from that to get a realistic theory so where do you both get your um motivating contexts the specific application scenarios you decide to go to where do they come from how do you decide which ones to which ones are most exciting to you alexander sure so yeah so that's a great question and like so i have two studies well this is a part of the same strategy but there are two kind of boats so one mode is i want to focus on an application that really matters like much of my work is about deployable what happens where we deploy ml in the real world what kind of changes arise so you choose some use case and just try to see what will be the first thing that you will hit there like what will be the major problem and once you identify this problem you try of course always to clean it up so you can actually try to study it in isolation model and benchmark and that's essentially one way we come up with like part of my work the other one so this is the work about kind of trying to understand exactly the definitions of robustness and how to even know that our ml model is robust and the other strain work is about developing the toolkit and for that i just choose the simplest threat model that kind of makes this toolkit sweat you know so like that is infamous lp robustness things which yes of course it's clearly just a tiny piece of you know what robustness would mean but it's a great kind of very analytically nice threat model to work with if you want to think about developing tools because even getting these lpure buttons is difficult and it forces us to think about you know techniques approaches and in particular under my smoothing came from that so this is the two more so it kind of alternate we either think about relatively simplistic rudimentary notions of robustness to improve our toolkit and understanding of robustness as a phenomenon and then there are like much more attuned to specific use cases kind of studies where you kind of try to see what new facets of robustness we can discover through this events and just just add on to that very briefly even in this case where you have like these very um contact specific things i think it's important to still distill them back into like simple to stay very basic problems that are like fundamental that we don't understand how to how to attack or how to defend against these these sorts of things so i'm i'm looking at the chat window and i i i see a couple more of uh technical questions coming up so i wanted to ask one um not technical slightly deeper questions we're seeing often today uh um and this is paraphrasing saeed's uh question we're seeing often today that there uh that today's models and methods have a bit of a trade-off between uh model robustness and accuracy do you think that that trade-off is is fundamental or do you think that um or do you think that that we just haven't gotten the right methods yet so yeah well given that much of my work is about exactly this trade-off so this trade-off is fundamental like in some ways it's a trade-off between specialization in general generality so in particular like our current immigrant model they get better accuracy than humans in some ways on this specific you know distribution that immigrant corresponds to because the models are able to figure out all the idiosyncrasies of this of the data sets and kind of get great predictors so but yeah we know it's from the real world like you like if you specialize you lose on generality so the tradeoff is there of course the question is how much of a trade of it has to be you know maybe it is possible to like make this trade will be almost in existence for many of the datasets we care about and that's essentially what we are thinking about it's kind of how to get robust models with better natural accuracy so it's less painful but you know the you know the trade-off is fundamental it just kind of you know you can show many ways in which kind of you know sometimes if you just care about very specific distributions there are things you can do like just for backgrounds like you know i talk in my talks about backgrounds and foregrounds you know again if you just care about specific color like specific distribution of images then actually you should be looking at background because background may be very telling of you know of of what the correct answer is but unfortunately if you think about robustness then this might not be a best idea to look at the background so there is this trade-off is there naturally so yeah that's the answer jerry do you have any well you want to add i think pretty much uh i would say the same thing as alexander i think certainly you know mathematically you can demonstrate there are trade-offs between trying to be robust trying to get accuracy just because the objectives there are not the same the question really is like how much of this does manifest in practice and we see this like to varying degrees and i guess the hope is really to try to narrow this gap as much as possible in practice um i think there's been some encouraging work that has been trying to narrow this gap in a number of settings but it's still there for now and yeah it's definitely inherent but i guess it's just how much to what degree it has to be there in practice yeah and just one point i want to make in some ways like we say it's robustness between robustness and accuracy but kind of we should ask ourselves what accuracy really means you know like in the end you know like is even this accuracy bought by kind of you know exploiting some like strange idiosyncrasies of distribution even the accuracy we want right like even though it that the number is high but is it even the obvious recognition we are looking for so again so this is kind of a question that we should be also confronting so i guess yeah i guess that's interesting so not only do we need to be careful about our definition of robustness but also our definition of other performance measures like accuracy yeah well i think that you're closely attacked sorry go ahead oh go ahead i mean i feel like the two are closely tied like robustness means robust accuracy but what does robust accuracy mean so yeah and i would add i guess uh given that we're talking about security and robustness issues i guess safety is another measure that might differ from accuracy but also need careful definition um one of the the lively topics that came up during the the q a for your talk alexander was one that's that i feel very passionate about as well that's and that's causality and causal machine learning um i was wondering if you could say a few words about the connections that you're seeing between robust machine learning and uh and causality and maybe a little bit about the problems that you're seeing like what are the challenges in um getting these two you know uh i think um methods to work together to solve these types of issues where where do you think that they haven't quite fit together yet where the problems remain yes so as i said in this chat in some ways to me lack of causality of embracing causality incorporating anxiety into our you know machine learning models like and talking about the vision in particular is one of the rules of the problems that we are observing and in some ways you know essentially like yeah currently our deep neural networks there are like amazing correlation extractors so i usually call this like correlation extraction this is like a double-edged sword on one hand this is what enables these models to do all of these impressive things but this is also exactly like you know all the things that like we come up oh this model did not do what we expected them to do or they did something ridiculous like it's you very often can you know essentially tie it back to the fact that you know the model interpreted like a correlation as a causal kind of causal relationship so in some ways like trying to interpret causality in our models is exactly the way to go now the problem is that in some ways like you know much of the work in causal learning currently it's in a very different setting it's kind of the setting of where we exactly know what are the factors that we kind of want to try to model the causal relation between and this is definitely not the setting where vision is in so we have discussion over over the chat you know so currently the only approaches to you know providing uh well causality is i would say interactive form so essentially like we think of interventions like we say oh you know if i change this background to a different color you know i know like according to my you know model causal model inside my brain i know it should not change them you know it should not change the outcome but it does so this is clearly an evidence that the causal model you know inside the mod like inside the model is different to one that it should be so kind of you can do this like react like distracting things except the problem is you know how do you come up with even interventions to pinpoint this kind of this point so we have some in our work we have this kind of we show that for robust models in vision you can actually like gradients towards different classes come up like show and interesting ways to come up with this kind of interventions on which you can say oh the model is thinking this is a dog for this reason and now we see why it's kind of you know it's kind of you know it's kind of like this is wrong so in particular there's like plenty of interesting spurious collisions that we learn as we work with the models like for instance we learned that for a crab you know the you know natural habitat is a plate right because you know like images on the web you know many people just photograph you know crabs on the plate so so anyway so so clearly this is needed but like it's hard for me to have a very precise and principled framework about this again it's more about like when we saw something wrong when we see something wrong we can phrase it in this language but there's an intervention it had an effect but it shouldn't but like it would be nice to be kind of more predictive but i really don't know how to do it like like at the beginning you would need to have much more fine-grained annotations so we can even talk about objects that is like i don't know there is a heart in the picture there is you know a blue background and something and you know before like we have to even know what are the objects we should be reasoning about first and you know and we don't even have that so so that's even like that's what makes this problem much difficult but like we have to think about this yeah i guess faster rephrase i think it's the challenge of learning the future representation at the same time that you're reasoning about the causal influences and obviously how you choose to represent um what's in the image influences how those representations are influencing each other yes exactly and in some ways also like part of the problem is that like the features you come up with might not directly kind of correspond to the abstract kind of factors that you would see in the picture as a human so yeah so that's all makes all of this different yeah there's one philosophical issue that i keep seeing coming up uh in discussions of computer vision causality that's the simple one of whether the object that you're trying to label is causing the pixels whether the pixels are causing the label and that seems like given that you know the direction of causality matters so much right yeah and that's a great question so like honestly like when i get too frustrated about this i just come back to this reactive approach i'm just saying i know when it goes wrong and that's all i care about so like i just want to get a model when i can't find an intervention that makes things go wrong but again this is not a very comprehensive or really like it's not a solution it's just a patch jerry what do you think is the most kind of promising directions uh in solving some of these problems uh algorithmically we've been talking about causality do you think that that's one of the main promising directions do you have others that you have that you've been working with that you think are key to making progress on these issues i guess i would sort of split the defenses and sort of responses to attacks sort of into two buckets they're sort of like the short-term horizon kinds of things these are things that you can imagine in engineers and world practitioners could be using you know within the years like this to at least make attacks if not impossible but at least more difficult to implement uh from the attacker side and i guess here this is this is very different i guess uh from causality which would be i think one of the more long-term horizon uh sort of approaches and i think the long-term horizon there's like many different algorithmic approaches that could actually yield uh good defenses for instance randomized smoothing could be better or maybe better better randomized uh sorry better adversarial training um these are all very promising but i think getting them to be deployed in the real world within maybe the next three years or so is probably still would require some additional work um because there's a number of practical overheads that go into any of these academic academically developed defenses but uh in the more short term i think the more promising things are things which are more like system security interventions um less so like more less of like actual machine learning but more like applying you know these system security principles like sanitizing input as much as you can these sorts of things um yeah well that's a great actually segway into a question that i uh wanted to ask both of you and i'll start with you jerry since you mentioned it the question is this how much of the the issues and the solutions the to the general challenge of security machine learning do you think are going to come from algorithmic advances themselves versus either being more data driven you know coming up with the right data sets and training methodologies or crafted application scenarios that help us you know get around the most difficult aspects of you know threat models jerry so i tend to think that at least while as i mentioned these short-term interventions are are useful um they're not likely to be truly robust um in the sense that you're probably going to be able to get around them if you're sufficiently dedicated adversary um it's not clear if you can actually really ever hope to get it with with a truly robust system because again we don't really know what that means but at least you you could hope that uh in the long term horizon these more algorithmic advances could give you much more principled defenses that actually sort of uh more broadly work um in in a more sort of principled fashion so i think hopefully it'll be a mix of the two i guess but i don't know i guess we'll see yeah so so just to avoid answering this yes or like one or the other question like i really like because again i'm thinking about things a lot and again it's very frustrating in many many ways uh so like kind of like i like to tie it back to other aspects that we usually talk about but somehow this time it didn't come up with is the explainability like in some ways you know like i think one of the goals of everything you should be doing is just understanding you know what makes our model stick and you know at the very least if we can't make it you know like you know do it what we think it should be doing we actually should know in which way it fails and kind of how to reason an engineer around this so to me this is what is great about the algorithmic thinking is that it kind of you know you can start kind of diving into like exact way like how things should work and then of course you know you need to you know to figure out the divide like it's never perfect but kind of at least it forces you to think in a that kind of prescriptive way but like you know yeah in the end i i just don't know like at this point i really kind of like much of my work currently and i said it from this like very oh let's build these defenses to get like robust ai and everything will be great but essentially like what much time what much i i do is i can realize okay this is more complex than i thought and kind of i'm just now more of trying to understand what how these things fail you know and kind of what makes this model thick and kind of can we at least like open up this black box and see what's inside so we can you know like go for a group and think about the solutions there i also want to just briefly add i think it depends a lot on the the context so for instance i think average we've been talking primarily in the context of absolute examples where you know algorithmic defenses have been quite hard to come by um but in other contexts for instance data poisoning i think there's been a lot of great work recently and developing well i guess a lot of work that my collaborators and i have been doing uh have been that seems to be developing really you know practical algorithms that do offer a very strong guarantee of robustness to data poisoning attacks so i think it also depends on the context quite heavily i was just smiling i saw a great comment in the uh in the chat window just now uh alexander commenting on the fact that you recently released a uh image data set where all the backgrounds were placed and now you yourself are um speaking with the your background replaced as well yes so as you see like yeah so like no one got fooled that i am in you know in some like galaxy so i guess it doesn't really work but i guess the model would believe that i'm somewhere inside like you know outside of like like you know in the cosmos but yeah it doesn't work yeah um i believe we're in our last few minutes uh do you have any uh last words you'd like to say uh jerry uh i mean i think just that this is a very still very interesting uh field for sure and there's a lot of it's a very interesting feel because there's a lot of interesting philosophical questions here that we've been sort of squirting around constantly um that have real world real real world ramifications which are very interesting um as well as some really interesting mathematics going on here so it's like a really interesting like intersection of all these three sorts of uh aspects of the field and a lot of interesting work still to be done i guess i hope yeah so i cannot agree more with jerry like you know i really have a lot of like in some ways if you hear my answers i say we can't do this we can do this we can do this so it sounds like a very frustrating kind of type of research to do but i actually have an immense fun and in some ways like really what was funny is that like i came into this field from like a purely optimization point of view then i think about the security but now kind of as you dig deeper into this problem you start thinking about you know what even learning means you start thinking you know how ml is used how it should be used you start thinking about biases about fairness and all of this and it's really like inspiring to see like how this like simple problem that you start with forces you to kind of embrace now i'm talking to neuroscientists and thinking about neuroscience and brain so like i really love this research area because yes it has big questions that i don't expect to solve anytime soon but it really pushes you to study very interesting math very interesting you know different sciences and just think about the society in the world so you know if these are the things that you enjoy thinking about i think that it's hard to find a better field than this i agree with you i think that's something one of the things that i find super exciting here is just the opportunity to think about these systems and what we want them to be doing and how we can get them to do that end to end it brings up so many cross-cutting issues so speaking of uh uh cross-cutting issues i'll take a moment to another with the as we're ending our session i'll take a moment to uh just mention to our audience that we're going to be going on a half hour break now from 12 30 to 1 pm pacific time and at 1 p.m the frontiers of machine learning event will be back with a panel on beyond fairness pushing machine learning frontiers for social equity the moderator will be mary gray and the uh panelists will be relieved abe irene lowe and augustine sean two so please uh join us uh in in half an hour for another really interesting session and right now i'd like to thank our panelists alexander madrid jerry lee and don song for joining us today thank you very much thank you so much it is a pleasure thank you you 