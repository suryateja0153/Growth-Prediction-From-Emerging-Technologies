 [Music] hi I'm cat from the tensorflow team and I'm here to talk to you about Responsible AI with tensorflow I'll focus on fairness for the first half of the presentation and then my colleague Miguel will end with privacy considerations today I'm here to talk about three things the first an overview of ml fairness why is it important why should we care and how does it affect an ml system exactly next we'll walk through what I'll call throughout the presentation a fairness workflow surprisingly this isn't too different from what you're already familiar with for example a debugging or a model evaluation workflow we'll see how fairness considerations can fit into each of the discrete steps finally we'll introduce tools in the tensorflow ecosystem such as fairness indicators that can be used in the fairness workflow fairness indicators is a suite of tools that enables easy evaluation of commonly used fairness metrics for classifiers fairness indicators also integrates well with remediation libraries in order to mitigate bias found and is structured to help in your deployment decision with features such as model comparison we must acknowledge the humans are at the center of technology design in addition to being impacted by it and humans have not always made product design decisions that are in line with the needs of everyone here's one example quick-draw was developed through the Google AI experiments program where people drew little pictures of shoes to train a model to recognize them most people drew shoes that look like the one on the top right so as more people interacted with the game the model stopped being able to recognize shoes like the shoe on the bottom this is a social issue first which is then amplified by fundamental properties of ML aggregation and using existing patterns to make decisions minor repercussions in a shoe faulty shoe classification product perhaps but let's look at another example that can have more serious consequences prospective API was released in 2017 to protect voices and online conversations by detecting and scoring toxic speech after its initial release users experimenting with the web interface found something interesting the user tested two clearly non-toxic sentences that were essentially the same but with the identity term changed from straight to gay only the sentence using gay was perceived by the system as likely to be toxic with the classification score of zero point eight six this behavior not only constitutes a representational harm when used in practice such as a Content moderation system this can lead to the systematic silencing of voices from certain groups how did this happen for most of you using tensorflow a typical machine learning workflow will look something like this human bias can enter into the system at any point in the ml pipeline from data collection and handling to model training to deployment and both of the cases mentioned above bias primarily resulted from a lack of diverse training data in the first case diverse two forms and in the second case examples of comments containing gay that were not toxic however the causes and effects of bias are rarely isolated it is important to evaluate for bias at each step you define the problem the machine learning system will solve you collect your data and prepare it oftentimes checking analyzing and validate it you build your model and train it off the data you just prepared and if you're applying ml to real-world use case you'll deploy it and finally you'll iterate and improve your model as we'll see throughout the next few slides the first question is how can we do this the answer as I mentioned before isn't that different from a general model quality workflow the next few slides will highlight the touch points where fairness considerations are especially important let's dive in how do you define success in your model consider what your metrics in fairness specific metrics are actually measuring and how they relate to area is a product risk and failure similarly the data sets you choose to evaluate on should be carefully selected and representative of the target population of your model or product in order for the metrics to be meaningful even if your model is performing well at this stage it's important to recognize that your work isn't done good overall performance may obstruct poor performance on certain groups of data going back to an earlier example see of classification for all shoes was high but accuracy for women's shoes was unacceptably low to address this we'll go one level deeper by slicing your data and evaluating performance for each slice you will be able to get a better sense of whether your model is performing equitably for a diverse set of user characteristics based on your project use case and audience what groups are most at risk and how might these groups be represented in your data in terms of both identity attributes and proxy attributes now you've evaluated your model are there slices that are performing significantly worse than overall or worse than other slices how do we get intuition as to why these mistakes are happening as we discussed there are many possible sources of bias in a model from the underlying training data to the model and even in the evaluation mechanism itself once the possible sources of bias have been identified data and model remediation methods can be applied to mitigate the bias finally we will make a deployment decision how does this model compare to the current model this is a highly iterative process it's important to monitor changes as they are pushed to a production setting or to iterate on evaluating and remediating models that aren't meeting the deployment threshold this may seem complicated but there are suite of tools in the tensorflow ecosystem that make it easier to regularly evaluate and remediate Pro fairness concerns fairness indicators is a tool available via tf-x tensor board collab and standalone model agnostic evaluation that helps automate various steps of the workflow this is an image of what the UI looks like as well as a code snippet detailing how it can be included in the configuration fairness indicators offers a suite of tools that allows a suite of commonly used fairness metrics such as false positive rate and false negative rate that come out of the box for developers to use for model evaluation in order to ensure responsible and informed use the toolkit comes with six case studies that show how fairness indicators can be applied across use cases and problem domains and stages of the workflow by offering visuals by slice of data as well as confidence intervals indicators help you figure out which slices are underperforming with significance most importantly fairness indicators works well with other tools in the tensor flow ecosystem leveraging their unique capabilities to create an end-to-end experience fairness indicators data points can easily be loaded into the what-if tool for a deeper analysis allowing users to test counterfactual use cases and examine problematic data points in detail the Stata can also be loaded into tens float data validation to identify the effects of data distribution on model performance this step summit we're launching new capabilities to expand the fairness indicators workflow with remediation easier deployments and more will first focus on what we can do to improve once we have identified potential sources of bias in our model as we've alluded to previously technical approaches to remediation come in two different flavors data based and model based database remediation involves collecting data generating data re-weighting and rebalancing in order to make sure your data set is more representative of the underlying distribution however it isn't always possible to get or to generate more data and that's why we've investigated model-based approaches one of these approaches is adversarial training in which you penalize the extent to which a sensitive attribute can be predicted by the model thus mitigating the notion that the sensitive attribute affects the outcome of the model another methodology is demographic agnostic remediation an early research method in which the demographic attributes don't need to be specified in advance and finally constraint based optimization we will go into more detail in over the next few slides in a case study that we have released remediation like evaluation must be used with care we aim to provide both the tools and the technical guidance to encourage teams to use this technology responsibly celeb a is a large-scale base attributes data set with more than 200,000 celebrity images each with 40 binary attribute annotations such as smiling age and headwear I want to take a moment to recognize that binary attributes do not accurately reflect the full diversity of real attributes and is highly contingent on the annotations annotators in this case we are using the data set to test a smile detection classifier and how it works for various age groups characterized as young and not young I also recognize that this is not the possible full span of ages but bear with me for this example we trained an unconstraint and you'll find out what unconstraint means TF carries sequential model and evaluated and visualized using fairness indicators as you can see not Jung has a significantly higher false positive rate well what does this mean in practice imagine that you're at a birthday party and you're using this new smile detection camera that takes a photo whenever everyone in the photo frame is smiling however you notice that in every photo your grandma isn't smiling because the camera falsely detected her smiles when they weren't actually there this doesn't seem like a good product experience can we do something about this tensorflow constraint optimization is a technique released by the glass box research team here at Google and here we incorporate it into our case study TF constrain optimization works by first defining the subsets of interest for example here we look at the nut not young group represented by groups tensor lesson 1 next we set the constraints on this group such that the false positive rate of this group is less than or equal to 5 percent and then we define the optimizer and train as you can see here the constraint sequential model performs much better we ensured that we picked a constraint where the overall rate is equalized for the unconstrained and constrained model such that we know that we're actually improving the model as opposed to merely shifting the the decision threshold and this applies to accuracy as well making sure that the accuracy and AUC has not gone down over time but as you can see the not young FPR has decreased by over 50% which is a huge improvement you can also see that the false positive rate for Jung has actually gone up and that shows that there are often trade-offs in these decisions if you want to find out more about this case study please see the demos that we will post online to the TF site next we finally want to figure out how to compare our models across different decision thresholds so that we can you help them in your deployment decision to make sure that you're watching the right model model comparison is a feature that we launched such that you can compare models side-by-side in this example which is the same example that we used before we're comparing the CNN and SPM model for the same smile detection example model comparison allows us to see that CNN outperforms SVM in this case has a lower false positive rate across these different groups and we can also do this comparison across multiple thresholds as well you can also see the tabular data and see that CNN outperforms SVM at all of these thresholds in addition to remediation and model comparison we also launched Jupiter notebook support as well as a fairness lineage with ml metadata demo collab which traces the root cause of fairness disparities using stored run artifacts helping us detect which parts of the workflow might have contributed to the fairness disparity fairness indicators is still early and we're releasing it here today so we can work with you to understand how it works for your needs and how we can partner together to build a stronger suite of tools to support fairness questions and concerns learn more about fairness indicators here at our 10th row dart org landing page email us if you have any questions and the bitly link is actually our github page and not RTF dot org landing page but check it out if you're interested in our code or case studies this is just the beginning there are a lot of unanswered questions for example we didn't quite address where do I get relevant features from if I want to slice my data by those features and how do I get them in a privacy-preserving way I'm gonna pass it on to Miguel to discuss privacy tooling intensive flow in more detail thank you Thank You Kat so today I'm going to talk to you about machine learning and privacy before I start let me give you some context we are in the early days of machine learning and privacy to field an intersection of machine learning and privacy has existed for a couple of years and companies across the world are deploying models to be used by regular users hundreds if not thousands of machine learning models are deployed to production everyday yet we have not ironed the price issues with these deployments for this we need you and we've got your back in terms of intensive flow let's work through some of those pricey concerns and ways in which you can mitigate them first of all as you all probably know data is a key component of any machine learning model data is at the core of any aspect that that's needed to train a machine learning model however I think one of the pertinent questions that we should ask ourselves is what are the privacy considerations that there are when we're building a machine learning system we can start by looking at the very basics we're generally collecting data from an end device let's say it's a cell phone the first privacy question that comes up is who can see the information in the device as a second step we need to send that information to the server and there are two questions there while the data is transiting to the server who has access to the network and third who can see the information in the server once it's been collected is it only reserved for admins or can regular Suites also access that data and then finally when we deploy a model to the device and there's a question as to who can see the data that was used to train the model in a nutshell if I were to summarize these concerns I think that I can summarize them with those black boxes first the first concern is how can we minimize that exposure the second one is how can we make sure that we're only collecting what we actually need the third one is how do we make sure that the collection is only thing ephemeral for the purposes that we that we actually meet fourth when we're releasing it to the world are we releasing it only in aggregate and are the models that we are we're releasing memorizing or not one of the biggest motivations for privacy is some ongoing research that part is that some of my colleagues have done here at Google a couple a couple of years ago they released this paper where they show how neural networks can have unintended memorization attacks so for instance let's imagine that we are training a machine learning model to predict the next word generally we need text to train that machine learning model but imagine that that's data or that corpus of text has or potentially has sensitive information such as Social Security numbers credit card numbers or others what the paper describes is a method in which we can prove how much is the what's the propensity that the model will actually memorize some data I really recommend you to read it to read it and I think that one of the interesting aspects is that we're still in the very early days of these fields the research that I showed you is very good for neural networks but there's our ongoing questions around classification models we're currently exploring more attacks against machine learning models that can be more generalizable and used by developers like you and we hope to update you on that soon so how can you get started what are the steps that you can take to to do privacy in machine learning in a privacy preserving way well one of the techniques that we use is differential privacy and I'll walk you through what that means you can look at the image there and imagine that that image is the collection of data that we've collected from a user now let's zoom in into one specific corner that blue square that you see down there so assume that we're training on the individual data that I'm zooming in if we train without privacy will train with that with that piece of data however we can be clever about the way that we train a model and what we could do for instance is just let's flip each bit with a 25% probability one of the biggest concerns that people have when doing this approach is that it naturally introduces some noise and people have questions as to what's the performance of the resulting model well I think one of the interesting things from this image is that even after filtering or flipping 25% of the of the bits the image is still there and that's kind of the big idea around differential privacy which is what powers tensorflow privacy as I said differential privacy is the notion of privacy that protects the presence or absence of a user in a data set and it allows us to train models with in a pricy preserving way we released last year a tensorflow privacy which you can check at our github repository at geek the github.com slash tensorflow slash privacy however I want to talk to you also about some trade-offs training with privacy might reduce accuracy of the models and increase training time sometimes exponentially furthermore and I think more worryingly and tied to what cat cats talk if a model is already biased differential pricing might might make thin things even worse as in even more biased however I do want to encourage you to try to use a French of privacy because it's one of the few ways in which we have to to do privacy and m/l the second one is federated learning as a refresher ten attempts of a federated is an approach to machine learning where sure model is or global model is trained across many participating clients that keep their training data locally it allows you to train a model without ever collecting the raw data therefore we're using some privacy concerns and of course you can also check it out at our github repository this is kind of what I was thinking with our mentioning about tensorflow private federated learning the idea is that devices generate a lot of data all the time phones IOT devices etc traditional ml requires us to centralize all of that data in a server and then train the models one of the really cool aspects about a Federale earning is that each device runs look locally only and the outputs are aggregated to create improved models type of setting and RDR allowing the orchestrator not to see any private user data terms of row federated in as I was recast - as a recap allows you to train models without ever collecting the raw data so if you remember the first slide that I showed it really protects the data at the very edge in terms of next steps we would really want you to reach out to TF - privacy we would love to partner with you to be responsible AI cases with privacy we're in the as I said earlier we're in this together we are still learning the research is ongoing and we want to learn more from your use cases I hope that from the paper that I show you you have the the sense of keeping user data private is super important but I think most importantly is that this is not trivial the trade-offs in machine learning and privacy are real and we need to work together to find what the right balance is [Music] 