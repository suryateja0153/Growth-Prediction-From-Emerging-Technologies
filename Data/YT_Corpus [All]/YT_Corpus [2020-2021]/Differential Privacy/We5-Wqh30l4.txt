 hi everyone today i'm going to talk about testing differential privacy with dual interpreter provincial privacy introduces noise to data analysis tasks in order to mask sensitive information while preserving most of the data utility so why do we want to test the venture privacy conventional privacy is not just a mathematical definition as we can see it is gaining momentum in real-world appointments for example the census bureau is applying differential privacy for the 2020 census google is applying adventure privacy to collect metrics from the chrome browser and apple is applying to venture privacy to collect ios usage information however actually determining whether a program is differentially private or not is very error-prone even a very small difference could mean being eventually private or not as this the lbb paper from 2017 told us here are six variants of the same algorithm published by experts in the field they'll look very similar but only two of them have the intended differential privacy property there is a rich set of previous work that focuses on automatically validating differential privacy or detecting violations of retroprivacy on the left here we have systems that produce proofs of eventual privacy either through automatic type checking type checking with some light annotations or assistance that validates machine check will produce on the right we have systems that tries to find evidence of violations through statistical testing both kinds of systems have complementary strength and weaknesses um the proof-based systems give a very strong evidence of financial privacy since they approved however the automated systems often work best on simpler algorithms and they might fail to type check more complicated algorithms the manual proof-based systems are much more expressive but only very few people in the world have both the expertise and mission check both groups and the virtual privacy to use them so these tools are much less accessible um the statistical testing tools give a strong evidence of non-differential privacy because these are counterexamples however they require appropriate heuristics for building approximations of the underlying distribution of outputs and use these heuristics to compare these outputs output distributions to check for violations of vp and different algorithms might require different heuristics so is there something that we can reach for in the middle something that tries to capture the good character it's good characteristics from both kinds of systems and the cur the characteristics that we want are the high level of automation from thai systems and statistical testing systems the expressiveness of programming logics and the wide range of algorithms from real simple ones to really complicated ones um so we introduce dp check which is an automated testing framework for individual privacy that takes on a hybrid approach between proof search and statistical testing we achieve this by looking at the structure of provincial privacy proofs applying through a very commonly used proof technique and we check if we can satisfy symbolic models from the general proof structure together with concretely sampled execution traces from actually running the program before i tell you a little more about this hybrid approach uh i need to do some review on differential privacy uh here suppose we have collected a database of user information on whether people are smokers or not and say an adversary is watching a counter of the total number of smokers in the database as soon as we add eve's confidential entry to the database the adversary can infer that he was a smoker leaking her sensitive information we can make this kind of informa inference much more difficult for the adversary with the help of randomization so we call a program differentially private if it's output is sampled from the distribution that is robust through the change of any single person's data in the input database here suppose we have removed each role in the database given that f is differentially private we can see that the output distribution has not been perturbed very much more precisely the differential privacy property is parameterized by a non-negative parameter epsilon so we call a program excellent eventually private and for any two similar input databases the output distributions are absolutely close to each other here uh this similarity relation between databases depends on exactly what kind of information we're trying to mask so they're probably specific this epsilon close relationship between two distributions mean that for any possible sample that we can draw from both of the distributions the multiplicative difference of the samples probability under both distributions are bounded by e to the epsilon so let's look at an example algorithm called report noise and mass for this one the input is an array of numbers and two inputs are considered similar if their pointwise values are bounded by one under this similarity relation report noise in max is a two differentially private company and operationally this algorithm as noise sampled from this highlighted laplace distribution to each of the input values in the input right and it remembers the index of the largest noisy mass value seen so far and at the end of the loop it returns uh the that index um so although the definition of different differential privacy is quite complicated experts have come up with a reasonably general proof technique that works on a large class algorithms the general idea is this let's consider two runs of report noisy max on a pair of similar inputs um because for this particular algorithm we know it returns an index value into the input array you know the range of possible outputs are 0 up to n minus 1 where m is the length of the input array this proof technique requires that for each possible output index value i we show that the probability of getting i um under both distributions uh multiplicative difference bounded by p2 epsilon our main contribution is that we can in fact specialize this proof technique into a testing technique by considering the proof structure together with instrumented execution traces on one of the inputs and symbolic execution traces on the other input so let's abstract things a little bit here uh t i xs1 is the set of program execution traces that lead to output i with input xs1 and similarly ti xs2 is the set of traces that lead to output i on xs2 proofs using this uh proof technique are essentially trying to show that there exists some kind of relation art between executions such that for any execution on the left that leads to output i using input is one there exists some dual execution on the right that uses the input xs2 such that the relation r goes between them here r needs to satisfy some more properties and the details of that are designed to make sure this property complies with into privacy but we won't need to unpack those here so um we observe that uh instead of universally quantifying over the left handed side execution trace we can simply sample them from actually running the program on some input access form and for each of those sample traces we're going to substitute the original universally quantified variable with the concretely sampled trace and we're going to build a large conjunction over all of these sample traces as we take more and more samples from running the program and as we increase the size of this conjunction we're kind of getting closer and closer to the original universally quantified proof so this resulting testing technique is indeed a hybrid approach both performs proof search via an smt solver and collects a statistical significant significance from sampling so in our implementation we use the smt solver d3 and we pass an smt model that represents uh the formula shown on the previous slide to z3 and asks is the model satisfiable or not so a satisfiable model corresponds to passing the differential privacy test while a non-satisfiable model fails the test however you need to be careful because a passing test is not a proof that the program is different to our private because we replace the universal quantification with concrete samples but you can imagine as we take more and more concrete samples we gain more and more confidence in the adventure privacy property with this program um at the same time a test failure is not necessa not necessarily a proof that this program is not differentially private because maybe the program cannot be proven to be differentiated private using this particular proof technique but then this program must be highly unusual since the proof technique works for a lot of algorithms and this kind of failure will prompt us to take a careful look at the program so how well does this hybrid testing method work we evaluated our implementation on a comprehensive set of benchmark algorithms that's taken from all of the previous related work we manually implemented the correct version of each algorithm and one or more non-differentially private variants but um and then we it turns out that we can correctly distinguish the differentially private and non-eventually private variants of all of these algorithms in addition to a complicated algorithm called priv tree that could not be automatically checked before we also wanted to see if our framework can be used to help develop real-world software so we performed a case study on the disclosure avoidance system pass which was used by the census bureau to aggregate population counts in the 2020 census we looked at the source code of das and we implemented the core privacy mechanism of mass in the language accepted by our testing framework and we check that no uh violations of potential privacy is recorded um we also use uh the code extraction feature of bp chat to uh convert this we implemented core privacy mechanism back into python 3 and then we integrated the extracted code with the rest of that we then run a lot of statistical tests to make sure both versions of gas behave identically as a sanity check to conclude um we converted a proof technique of differential privacy into a testing template uh the implementation based on this testing technique is a fully automated testing framework for eventual privacy we can correctly handle uh the comprehensive set of benchmarks other than studied so far in addition to a complex algorithm that couldn't be automatically checked before there are a few more interesting details in the paper we talk about the between formalizations of potential privacy and program logics through this technique um we also talked about some optimization we did in the symbolic interpreter used by dp check to avoid exponential stability however there is still a computational bottleneck for solving these smt models as the models get larger and larger as we take more examples and this has uh implications on how we can use db chat to validate other means and we talk about those implications and that's all uh thank you and i will take your questions virtually 