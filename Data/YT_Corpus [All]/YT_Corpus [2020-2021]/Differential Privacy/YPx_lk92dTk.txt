 you [Music] thanks everyone for coming it's my pleasure to introduce us from Duke University he is a graduating PhD student and he's on the job market this year he has done some excellent work on differential privacy and SQL and today is going to tell us more about that so hello everybody as an artisan my name is use I come from Duke University and that's the title suggest today I'm gonna talk about how you can build the differential private interface on top of a traditional sequel database okay first in this talk I'm gonna introduce and motivate the problem and convince you that it's it's a real problem that we don't have good solutions for then I'm gonna go to the technical part of the talk where I'm going to describe the architecture of our system and give you some key insights on why what we do different and why it is better and I'm going to show some experiments based on you a real US census data and real queries that the census wants to release and lastly from halftime we're going to talk a little bit about what we do right now and what we won't do in the future going from this worker game so we live in a in a in a world where it will power it from data and we're from our phone and we get notifications about our flight and so on and generally have organizations that collect data about individuals and others either releases data based on this based video game so they release data or they use the data to create us with with great value like when you have predictive learning algorithms right and usually this data resides in traditional databases okay we'll take one example with the US Census they do the decennial census every 10 years and they collect data from every person and then they have different data products based on this data we can have congressional apportionment redistricting and so forth okay and for the but these data now for the census there is there is always the risk of privacy but this is specifically for the sensors and this is also encoded in legislature that the data means the data released need to satisfy certain privacy guarantees ok and if we see the example of the summary file one data released that they have and they have queries like this coming crease like this which Sergei was not gonna live sequel right now and the count number of households that were the householder this households fit certain properties in this this query counts number of households with the householder is in a certain age and have one child under 18 and so ok so what do we do for privacy in in academia differential privacy has emerged of the Goldman standard and it is so celebrated because it gives very clear privacy semantics informally it tells us that the output of a differential private mechanism should mask the presence or absence of any single doubling the data set and when these titles it visibles correspond to individuals then this can hide individuals as well okay and this is captured from this neighboring relation over here where we say that for every two neighboring relationships and in neighboring data sets and neighboring distances differ in one table okay it also has some very nice properties most I'm gonna stick with just two of them post processing and composition post processing tells us that the output of any differential private algorithm can be you can be used without any additional privacy laws okay and composition tells us that if we run multiple algorithms it tells us how the privacy guarantee degrades okay and these are some details again not necessary right now so usually differential private algorithms work by adding calibrated noise the query answer so you have your privacy firewall you have your data analyst outside and he submit some queries and he gets noisy answers back and generally this noise is calibrated both to the epsilon parameter with hi Val absolument less noise and less privacy but also the sensitivity of the query what are the sensitivities is defined as this form lover there and captures the maximum difference of a quarry answer for into neighboring and data sets okay now let's talk about this neighboring relation now because so long in defense of privacy you have a flat table so defining neighbor des it's very simple you add or remove a double and then you measure the sensitivity of the query you measure everything so what does it mean if you have a schema we have just people and households okay what does it mean to have a neighboring in database do I add a row in household do I add the row in person do I do what does it mean exactly so this one talents if we want to do differential privacy for lesson later this is the first sounds that we need to tackle okay and this brings us to the goal of the system is that we want to build the system that gives a nice interface that a analysts can ask declarative queries like sequel on a database answers all these queries under a common privacy budget so that means that analysts in practice this means that analysts can collude with each other and not gain additional information and one the data owner to specify the privacy requirement so the data owner is the person who will say okay person is my is my private relation here and this is a symbol schema with only two relations but you can have like bigger schema again you would like to have they they don't specify the resolution of privacy okay now let's see what we can do with the prior work okay one solution is the uber work the Flex where the analyst submits one career at the time and he runs the difference of the difference a private algorithm runs for this query and he gets an answer back okay and he does this unlimited times right so as you understand from the sequential composition either our privacy budget stops and stops at some point so we need to stop answering queries or we have unbounded privacy laws because we continue answering queries so this the first problem with this approach another problem with this approach that doesn't violate privacy but it is a problem from user experience point of view is that there is no consistency between query answers what do I mean by that imagine if I submit the same query twice in this in this system I will get two different answers so there's no consistency because of the randomness of the algorithms and maybe you can cuss you will tell me okay maybe you can cast the same query right okay but then you ask a query about the population in North Carolina you ask the query about population in in coastal North Carolina right and you won't want to be smaller than the other you can't cast it there and there's no guarantee that it will be consistent okay it should be across the queries right so imagine that I ask what is the population of men what's the population of women what is the total population there is implied consistency there right like I have like three counts they should add up to have to come to shoot that up to the other count right and then like these are two examples you can have consistency like more complex consistency and we get consistency for free as we'll see later I mean we'd get we we take consistent for free because we don't do this we do something closer to this okay no no I think it's it's central with the online query answering I think on since this is a central point with like one with this model no no no no specifically for Flex any system that does one query at the time we'll have the consistency problem yeah like sensors the second the second model that we can do to do query answering is that we can have you can have one algorithm that creates synthetic data and we'll create synthetic versions of the base tables here and then the even the analyst will submit queries to the synthetic data over there and no to the real data in here obviously we get constant privacy laws for unlimited number of queries it follows from post-processing that this thing will be true and we get consistency for free here again no matter how complex our queries are okay however this has another problem and it's that if you have if the analyst submits queries and these cores have joins of between the base tables and we try to do the joins based on the synthetic base tables will incur a very high order the intuition behind this is that differential premise tries to mask the presence or absence of an individual and one half joins from join an IDs or ORS or quasi identifiers and this I ID is now correspond to a single individual question data files correspond to a small subset of individuals so that is the intuition why will incur very high noise here okay so this brings us back now to to our goal right again we would like to build a system that doesn't have these problems and again has like these three desiderata we have like complex sequel queries from the database that might have joins might have group buys and so on common private spots it for all of them and again privacy requirements from the data owner okay and now before before I describe our system I want to discuss a little bit about certain principles that we had before in in the in the decision-making for building the private sequel okay first we want to avoid the online query answering problems and we do that by releasing private synopsis so it's a synthetic data but we'll call them synopsis because they might not be tapas they might be histograms or vectors of counts and so on okay and this of course as we said earlier gives us constant privacy laws and prevents them from site timing attacks consistency it's it's a decision that we made that we're not gonna go on like gonna be that mantra but as we said earlier if we if we go that way we have a big loss on joints so our synopsis are not gonna be on the base tables they're gonna be on the overviews of the schema okay and this views themselves again called the joins and lastly the views need to be driven from some workload some idea of what work what will the analyst ask on the actual query time came we know that we can't answer all queries with high accuracy as in strong privacy guarantees so we can't hold so we hope to have like a representative workload and try from this represent workload capture the views okay that will drive our it will build our synopsis you can also have to have a cider submission that is out and where we have more more discussion on on the design principles behind private sequel okay yeah so and it will look like this okay so general you have an you have an offline mode where you will have an offline phase where you can generate your synopsis okay you can generate your private synopsis and then this knops is now are outside the firewall and you have a career answer it could equate answering anything I'm sorry a great answering I think that intercept sequel Maps it to the correct synopsis and gives an answer so yes the privates in ops dinner at the green box which are the the private synopsis and these are driven these are tuned to the representative workload with this cue and also we correctly satisfy the guarantees the pickle my hunger and ease from the from the data owner and I will explain a big piece like the primary private relation with the private privacy object and Absalom is what the buds that you came then after we have released the synopsis then we have the Korean team that correctly intercepts the sequel and that answers the queries now it will be the case that we might have certain queries in coming from the analyst that cannot be answered from from these synopsis and we do not address this we just tell them analyst okay we cannot answer this query at this point in time okay and we can come back to this later and discuss maybe what we can do okay now for the remainder of the talk I'm not I'm gonna talk only about this knobs generation phaser came so it looks kinda like this inside the box okay we have five different components and they all work in tandem to get you from data in privacy requirements and represent the workload to private synopsis okay so let's see how this happens first you have a view selector okay that takes input as a representative workload and generates a set of views based on this workload became the main idea here is that we want for it's a query of the representing workload we want to capture it's its joint structure and have a view that encodes this joint structure at the end after the view selector has run every single query of the representative workload can be written as a linear code on some of the views that we generated and this will be crucial later on when we actually run the difference of private algorithms that will help us a lot if you have linear queries for these views okay materialized views for like any set of joins that you see in the world yah-hey joins the group buys another nonlinearities that appear in the workload okay and also deal with correlated subqueries so we know how to do correlate them there is this old work but we actually do it and we capture like this view selector has a limited grammar so we don't support all of sequel but we support in enough sequel I think you just don't you the person workload and just do everything well what what do you mean exactly by prioritization well like if there's if there's some that appear a lot but then there's one join syntax that appears only three times in the representative workload like is that yeah that matters but not here okay that matters but not there so I'm gonna spoil it a little bit and tell you that yes you might have only one query that has this join starts right so that will create will create one view and that one view current said only one query so that great guy budget allocator there might want to give less weight to that one view at the point where we we put budget but when we select views our our goal is that every single query from the representative we want to create a view that can answer that query and we have certain heuristics but I'm not gonna go into too much detail about the view selector in during the talk but I'm gonna go into more detail about these two guys okay so the goal of the view right and sensitivity calculator fancy names is that for every single review we want to compute the correct sensitivity bounds with respect to the policy specified from the data owner okay and this can be tricky and we have two different algorithms working together came after these two algorithms work for every view have a sensitivity bound and after we have a sensitivity one preview we can allocate our total budget across views so now review and has has epsilon naught epsilon over the sensitivity times some row and that row is how we decide to split our budget and maybe we wanted to put more budget and actually what we do in in views that that captured many queries and this is strategy for the instantiation of the budget allocator okay and after that our life is pretty easy because now what do we have who have UVI a materialized VVN the data deem a Qi which is the partial workload for that for that view and an epsilon I where abstinence the EPS tune after we take account the sensitivity so we can use any differential private algorithm like from prior work that works for linear queries and we can get a materialized synopsis and we call them synopsis because they really can be anything that these algorithm outputs saved at all if this hal do not put synthetic tuples like brief base here we have synthetic tuples if it outputs a vector of counts like dawa we have a vector of count and so on and I promised I'm going to talk about this but sensitivity calculation in more detail so let's go back to the previous slide and see what is sensitivity a sensitivity of a symbol it is a very simple query that counts number of people of voting aids and try to see what is the sensitivity of this query in our relational data that it's also very simple it has only two tables again as a reminder this is the formula for sensitivity and let's see how are we gonna choose our our policy right let's say that would just do differential privacy standard so we had to remove a person who had to remove a person so if we this is what flex does so they just differ would just take neighboring relations that they different one table and if they differ in in persons you compute the sensitivity it's one okay I can either add the person of voting AIDS or not okay so this count will change at most one now what if I had to remove a household okay so if I had to remove a household this query doesn't change right because I just had to remove hustle don't touch the person stable the query doesn't change we get sensitivity zero okay but we can all understand that this is not what would happen in reality because we see that this house over there has like these these two people living there so if I if I remove this house then my sensitivities would be to not zero so this happens because you have like these foreign key constraints where we remove something from one relation from the primary private relation and other stuff are removed as well and this not captured from saying just this thing that neighbouring if they differ in one table this doesn't capture it so two cup databases they are related because they share some keys but that queries for people with the above certain age that has nothing to do with house all right good why do you because no no different database these are different tables in the same database so if I want to hide the presence of a household and and I just give this count right this this will break the privacy of the households let's say that the adversary knows that and I mean at least for this query it's about the question right because maybe these people just lost their houses they're still there should I not if you want to if you want to hide the presence of a house right and you just release queries on persons you still need to add noise to them because then your adversary will know what is the distribution for example of people in houses and might you know reconstruct the house table okay you can ask many many queries to do that but I just don't see why yes I mean maybe let's say let's see if this will convince you a little bit better right so we have these key constraints right and the key constraints tell you that this and these two tables are are connected like that right then if we if we decide that our primary private relation is the red guy is the house then to find the neighbors we also need to keep track of things not in the other private relations in the other in the other relations so how we how we define privacy here is that we say and for for a policy and the policy is what is the primary private relation first we're going to find what are the secondary private relations and this example it's only one but if I have a bigger schema with it can be a chain it can have like a diamond we find all the other organs which are the secondary private relations UK and then to define neighboring dataset databases not only keep track of changes in household modules to keep track changes in the other relations as well with transitive deletions ok yes yes I'll explain how we do that yeah yeah because this one bond right the sensitivity here if you want to either you know a priori what is the nut maximum number of people in households in North Carolina whatever it's 20 and I don't know why but it's 20 and all you need to do it privately okay and we did privately the the multiplicity of hid I have one classroom I have one husband of Carolyn extending people inside yeah I don't know what to do with that household so yeah but how do you know that 20 without looking at the data this is the question right I will couple slides later on we'll see how how can deal with that we call it max frequency and we have a way to deal with it okay so this is how we define privacy okay it purpose now is not only find on an epsilon but Oroku my personal word Rho R I'm sorry R is the primary private relation okay a caveat here is that the schema needs to be a cyclic if I have cycles in my schema this doesn't work anymore okay and we can now now with this in mind we can go back to how we would do sensitivity again so have three main issues to deal to deal with first of all is that and our view is a complex equal query and even estimation of sensitivity for that is a hard problem okay that's one second is what sir yes indeed is that you need to know this frequency and if you don't know it then the global sensitivity will be unbounded in the presence of joins and lastly whatever sensitivity whatever calculation we we make it's always going to be dependent on the privacy resolution on what is the primary private relation because if if I have like high school primary particular Asian then I need a different program based on if if a HUD person primary private relation okay so this how we deal with them and for the first thing we have a rule based in stivity estimator calculator that builds on top of flags for the second and we have a truncation technique where we truncate outliers and for the last we have an automatic policy enforcement from the video rewriting rajala came and now i'm gonna just talk about the first bullet like how we compute sensitivity and we also have an example again so we have important map rule based algorithm that works on query plans query plans look like these green box the second green box so you have you have a sequel view and that green box in a captures that that view and we have additional so we build on top of the flex rules that commute the elastic sensitivity but we have additional rules that give us even tighter bounds okay and will do so that the main idea there is that we track keep track of what attribute is the key okay so let's see how it would work again so you have this view it translates to this query plan and really what is it it creates a table every row corresponds to the person the first is whether the person is the head of the household or the spouse or a child and so on the second is the race and the third is the number of people in is household the the total number of people in the household that he lives in okay so we can see that this has a self join here okay and the group by so the group is gamma the solution is over there okay so let's see how the algorithm would work and in my primary primary private religion is person here okay so first it would go in mark the sensitivity of its person relation as one because they're the primary private relations and now the sensitivity on them is one I had to remove it up along them okay then to compute the the group I would double the sensitivity that was underneath that the group item get to okay at the same time hid where we do the group I on now becomes a key for relation are okay so there's a there is a relation are after the group by and now for that relation hid become seeking okay and now we perform the join and because the join is on a key attribute we have an a different formula that gives us at the end a 2 times F plus one where F is the multiplicity of HIV in person okay so the sensitivity is still dependent on on the actual data okay we'll see how get rid of that a little bit later okay now if we didn't have this new rule and if we didn't keep track of of the kills the sensitivity would be three times F plus two okay and this difference here might be very small to 2 times F plus one which it's like the difference small but as we add joints and group buys in our query plants and self joins and so on and this difference becomes larger okay the fact that the factor of difference becomes large and this can get get out of hand really quickly for more complex queries okay so this is our sensitivity calculator and one problem here with with this and stimulator it is what happens if we change the privacy policy what happens if now person from red becomes Orleans right it becomes a secondary private relation if you run the algorithm like this here we'll get 0 okay and we don't want to get 0 it would be wrong so we we either need a different algorithm a different sensitivity calculator to run for every different policy and maybe have only two policies this is okay but in the general case is not okay or you can automatically enforce policies okay and here the main idea is that you have this Duty writing module that runs before the sensitivity calculator Texas input the query plan and outputs an equivalent query plan such that when the sensitivity calculator runs will compute the correct sensitivity and it does so by adding semitone operators whenever the secondary private relation appears okay so by adding these semitones essentially the sensitivity calculator updates the base sensitivity for these tables correctly okay let's let's see an example okay I have an even simpler query now it's a table with all people avoiding AIDS I came and my policies household so what the view rewriter would show what the sensitivity calculator would do without any rewriting would compute zero okay but we're gonna add a semitone operator there okay so we don't add in the query plan same zone between husband person wherever we see the person again and our sensitivity calculator correctly will compute the sensitivity of this where a plan as F where F is the the max frequency of hid in person okay now still not super good right because there is a dependency on F here like you need to look at the data okay so the second thing that our URI writer does is that it gets rid of the max frequency again so we have this instance calculator that gives us a result that is dependent on the on the on some two data statistics again this means that now the the whatever bound the sensitivity calculator gives us is a band for local sensitivity if we do local sensitivity you have to do smoothing of problems down the line okay we don't want do that we won't have a bound for global sensitivity so the main idea yes but we want to bound the global sensitivity okay so how do you do that right so this the idea is very simple is that we're gonna add carefully some truncation operators in the query plan okay we'll see what this means but essentially we enforce independency from F and not the dependencies only all truncation operator okay let's see an example this is our query plan before doing the truncation but after doing the same engine and this would be our query plan and after the the truncation okay so what do I do I go to person and I truncate hid in person at level K okay well we can talk offline how which is okay it's not super difficult okay we do SVT and we'd get K we disperse the technique and we get you get we do will get K under differential privacy but let's forget how you get K and let's say that somebody gives you K yeah in general is K bigger than the local or smaller than the local it's smaller okay in general in general the mechanism to come give us a key that smaller you could below the mechanism that give a K that it's larger I don't think that to how to select case super important the main idea that I want to to communicate is that whatever sensitivity that we have now is no depending on the truth data is dependent K so if it's K if you look at the two data to get K then you're still in local sensitivity but if you put game arbitrarily on your own now its global sensitivity similarly if you look at the data under differential privacy and picked a reasonably good K you're still a bound for global sensitivity and the main theorem in our paper shows that if you do both operations then you have the correct global sensitivity count for the code for the privacy resolution specified from the data owner okay just like I can think of it as hard coding I do not look at the data in this sense yeah you could you could do it like that but that's we don't do it like that but this is the principle this tells you how you move from a sensitivity calculator that does local the sensitivity calculator does global no I mean just saying that is not enough right because there are many ways I could get around this local just hard coding is one such method but it should be that it like it should still capture because local sensitivity is what is the best you can do but as you said like it leads to technical problems so somehow it should be a good approximation to it but still like you know it behaves like a global sensitivity that's what you really want and you are saying that there is such a mean one way to do it exactly you're right one way to do it is with truncation there might be other ways to do it okay yeah yeah maybe that would be more helpful and okay so what does truncation operator does right so it goes to person table and it has a level K and has an attribute eights idea okay and goes and drops all tuples from persons whose hid has frequency larger than K so in my North Carolina data that I have 120 people and 99 percent of households with at most five are my our algorithm will most of the times choose five and we'll drop the one percent of the data and now I added bias to my problem but I reduced the violence because now make my sensitivity smaller I dropped I dropped all the people that are in this household side I dropped em I like if you ask the query about these people you're gonna get very higher this is this is a drawback of this approach but it's something that we need to live with because now we have very good accuracy on everybody else on every other query I look I look at the data under differential privacy I have a sparse vector technique algorithm initiated with the query that essentially asks for max frequency equal to AI for frequency equal to I and for frequency at most I what is the number of what is the portion of data that have frequency at most I now this query we've showed that it has sensitivity I okay so if you divide this query by I get sensitivity one and you can have many such queries for many different eyes you give them two sparse vector technique which is an algorithm that will return only one time when you cross the threshold it adds noise to both the threshold and the true and the in the query answer and it gives you threshold at the end yeah but that's a one-time thing you might get that is the problem you like so you don't really know like how much know you don't know know and you have to live with it because either you do not go and pay so this is a bias a bias variance trade-off because you're gonna choose a truncation level the smaller your truncation level the bigger the bias that you add to your data because you dropped a lot of tuples okay but the smaller the truncation level the better them at the max frequency estimations are gonna be because here here the the sensitivity excuse me the sensitivity this term is gonna be because here the sensitivity is gonna be K right so if I if my as my as my truncation becomes smaller the noise that I need to add becomes also smaller but I pain in bias it is if it is get is interesting how to to do these things correctly for for the time being sparse back to the ethnic with the query that we found it works nicely because the query is also very low sensitivity yeah I don't know if you guys have any other questions about the technical stuff because I'm going to move to the experiments now I just had one quick one so on you were adding sensitivity for the group eyes as well I and it looked like you're just using sensitive be multiplied by how many joins or something how did you do the sensitivity other group by and isn't it a similar issue to hear ya say whenever you do a group by you double this at the sensitivity that you had before the group by that is the rule that we have and this gives upper bound on the sense of them over the max chains after the group by okay and also can you explain more how does this truncation work for this household example you give this here yeah like how do you select this game if you you run this part spectrum technique because like in this household you want to understand like on an average how many would you you would ask these queries on the person table right you'd ask how many persons are in the household but has frequency less than I and you ask that for I equal to one if it's under the threshold then it goes to I equal to 2 and so on and this mechanism will give you an answer only one time when it's over the threshold and that time is is the threshold that you will use here this is a parameter it will be parameter will be like 95% of the data or 99% of the data let's move some experiments I can show you have like some some other slides later on I can show you what is the effect of this truncation okay so as you guys realize the use case that we used is on the US Census okay we have North Carolina data and that has to have a database with three tables actually but only two matter here have also geography table but it doesn't matter have person high school table okay and we have four five and a half million in person I think into a yes five point four in person and two point seven eight apples in household okay and you have a total of around thirty six hundred queries and that are sequel criticism the on this data on game these queries and there might be simple or complex they look like they ask like the number of people living in such a household of sides of size three where a householder is a Hispanic male okay so we report the relative error the relative per query error okay over ten independent trials and okay we don't need to worry about the instantiation I don't know you need to worry about things so I don't know what is the average but we have a lot of that are like three four joins and this is building care very high sensitivity actually I have some numbers at the end we can see the sensitivity of so we have some certain views okay you can see like statistics about these views and the sensitivities and then from this and civets I can remember how many tones there are but like any any number between zero and four or five okay he joins and their key a yes turkey joint yeah there is joint some nights I didn't yeah their kids earnings yeah so for this dataset where you start with five point four legs mm-hmm is that with a 5.4 million tuples in personal database it's a person table then for the materialized or the synopsis how big is that a disc they very fit in there okay I don't remember the exact about okay okay so here's the thing here's the thing yeah so so this is a very good question didn't remember the exact numbers but I can tell you the following that certain techniques that we use in the private scene of generator they don't use a flat tables for data representation rather use vector representation now the vector representation this the size complexity of that doesn't have to do with the number of tuples but has to do with the number of attributes is exponential in the number of attributes okay so you have certain views that are very complex and they're vector representations maybe one gig but for most of them they're small enough that they're like maybe hundred megabytes okay um but yeah this is something to consider like and if we don't if we have other private signal generators that not to do vectors we have like having one more that does tuples then the size complexity will be equivalent to to the data size no to the domain okay and something that I won't say about the results and among about so you hear the representative workload that we used is the full workload that we also test it on and we also have some other experiments later on and where they represent the workload is much smaller it's like 30 queries okay so these are first result and y-axis is a relative per query error okay the policy here is person and I have a have stratified the the x-axis by the true query answer okay so the first column is are all our queries and then you have queries based on the true query answer okay the the main the main thing here and the horizontal line at relative error one is the error that a mechanism not always output zero would have and with an important baseline its assigning it's like a scientific based and if it can't beat that then there is something wrong right if you can't beat something that always output zero and actually this is not a hundred percent correct because some queries have have transferred zero so this mechanism times is hundred percent collector game so first we see that for around 60 percent of our queries are relative error is less than ten percent okay this very and very important thing a result because it shows us that there is merit in this approach for actually releasing data okay of course we have outliers that have a very high relative error and if you if you if we could zoom in we'd see that we can actually see that all these outlines most of them have fall into the first column these are queries that not only have small query answer so small signal but also have very high sensitivity so these queries a little bit unfortunate we can do a lot about them okay x axis is the groups of queries right so these are all our queries here these are quiz that have through query answer between zero and a thousand and so on okay no no these are groups of queries so every point there is a single query it's a query that that the coolster answer falls into this bucket and Y axis is a relative relative error so when you say forty percent you're saying if you take the ten to the negative one there's forty percent is above that yeah yeah like this this is 50 percent here so I would guess that 60 percent is there okay so if every point here will Blue Point is an outlier query and the the box plots fit the percentage of the relative errors inside the game next we can Saints polishing from person policy go to high school policy and here we see similar trends but now every error is boosted you can see like there is a clear boost of the errors again and this is explained because you know removing a household has bigger effect in that in dark areas than removing a single person in game but we can still see that your catalyst will beating the zero baseline okay for next experiment we compare with with flex and flex is a one query at a time and we did all queries together and so we adapted a little bit we just run Flex by splitting the power privacy budget across every single query okay so some of the quiz that we submitted some some of the 3,600 queries could not be supported from flags so we'll just excluded them from this plot and also plot a leg supports Persians poles you know and not household policy okay again we stratify results by to query answers and you can see that again the the black line is baseline zero and you can see that every blue box performs very badly and now the y-axis the relative error goes to 10 to the 4 and 10 to the 2 goes very large for flex okay and there is still as good as it was earlier now this huge difference in in neder comes from three factors again first of all we answer our queries based on these knobs that are generated from views so we just paid privacy but at one time to compute to compute the view not every time we want to compute an air query ok and a lot of queries will be answered from the same view secondly as we saw earlier we have a tighter sensitivity analysis ok so like these 2 F + 3 F business matters here a lot and lastly M flex they do the last sensitivity is abundant from the local sensitivity so afterwards they need to do smoothing and smoothing also matters and if you add a lot of noise we don't do smoothing we truncate instead we don't get so much sadder okay and this concludes with the main results we have more results for later on if you guys want and some stuff to talk about what we're doing now and what we're going to do so we would like to be have policies that extend to multiple primary private relations so imagine like a more complex schema and you can decide that and you know both my employees and my employers are going to be primarily private okay and then you find the you find the policy there what's happening I would like to support bigger sequel subset because equal grammar for both of you selector entire query answering engine for example how to answer this query the average of salary is a pretty interesting question we're working on that and we want to have the other two things maybe they're like two optimistic but we won't have like a new synopsis a synopsis updater a fresh release comes that I ran around unanswered or fresh data can so the flesh epsilon comes how would you update your already released synopsis under this unified epsilon right and the other thing is that we would like to have not not only give query answers like we do here but also give error bounds for these query answers and again how to do this is is unknown territory because even to provide the rebound you need to look at the data how do you do that like it's not super obvious okay and also you have we still have ongoing work about how to make tighter sensitivity analysis either with better rules or with adding more logic to the view rewriter so we've seen examples where the same sequel query has two different query plans and the sensitivity calculation will give two different numbers so this gives us it tells us something that maybe we could write Beauty writer that optimizes with respect to sensitivity finds an optimal plan for that and I would like to thank my colleagues good sow Singh and then Michael and Salome and of course in the middle my advisor husband and who who has pushed everybody to for this work here inspired [Applause] yeah you mentioned earlier on that one of the benefits of doing this approach up synopsises sidechain resistance is is the idea that the it's done up side to no resistance means side-channel outside of or inside and firewall like like somebody could look at your algorithm is it's running and guess what it's doing but you're saying it's that's in a protected box so first of all our algorithm should be transparent okay you don't have privacy security we all know that right secondly there they're just attacks but what I mean by sides and all the talk okay so you have the algorithms inside the previs firewall but if you are in an online query answering system and your algorithm is for example dawa and I know that because I've run the experiment myself for certain data douwe will take longer to run because it needs to run some optimization program okay for some other day that confront us optimization program so you don't even need to look at the query answers to understand that the data looked like this because now it took it took this algorithm or time so this is the side channel attacks that enough line during the synopsis generation no no no we close the doors of the wait well now we wait an hour and we say okay now we're done yeah it is yes that is the collector stress like waiting the air pounds I guess they easily is like I mean so excessive like legs they they can give you a better bound because they can tell you which distribution there's something here no but you're dropping like because they're so one is of course the noise but the other one is is that in order to have of your hands you constrain the data and you drop away pieces of your own data oh yeah I have a counterpoint to that so flex this Laplace mechanism with scale Sigma and Sigma you computed its spring information that Sigma can give you their bond right if you know the loss function if your loss function is relative error you still cannot give a rebound for the relative error so they're bound not only depends on the mechanism itself because if you have a mechanism symbol active doesn't just Laplace noise generator bound is gonna be the Sigma right the scale of the Laplace knows but also it needs depend on the error function that you actually compute yeah if it's if it's the absolute error then yes with mechanism like switch you could do it with mechanisms like ours you cannot do it because in the synopsis generation phase are our algorithms for example the data that I mentioned there data dependent that means that even for absolute error the error still depends on the input okay so this is so our whole this whole work is we do it will still data depend on our error is to data dependent you and yeah so in comment a bit on like is there any overlap with your techniques with the techniques of a QP like approximate query processing and all that there are a lot of similar terms yeah we we have use yeah we have explored this connection in the past we didn't come up with something but you know in a QP and you don't have privacy budget you have maybe computing some constraints or tangles yeah the thing that are there techniques in common and you have more things to ensure privacy how company whether you can you have an attorney on which because the trade-offs are different is yeah it looks similar especially if you do something like workload the workload negatively scores that looks very similar to that but having taken techniques directly from that part of that sir do you have slides or a paper that talks about the relative processing time and the relative storage okay I cover the numbers to my head then maybe I can unless on my machine back in Durham and tell you well it seems like the like one of the more interesting ones from an engineering or like a deployment standpoint was like there's three orders of magnitude versus not and you know yeah that's that's very fun can put it on your slides in shape it does that would be nice yeah yeah I can probably afterwards I can probably find these numbers but they really depend on what is your privates no generator yes I'm sorry sounds like it depends on like the dimensionality you like so it depends first of all on the toys and then the dimensionality of the domain so the the the cross domain of cross attributes right okay and yeah because for example it is for these 3600 queries you have 17 views that capture all of them so you need to materially 17 views so it's a small number on the one hand and most of them are pretty small but I have one view that has a domain half-a-million right if Tesla makes half a million imagine a vector with half a million counts if it fits in memory we everything fits in memory apparently Marines at Messina 16 gigabytes that's fine yeah so yeah okay so let's thank us again [Applause] 