 hi my name is Tony Chen and welcome to today's lecture on responsible innovation and artificial intelligence so today's talk will be split into two parts for the first part I will be going over some of the research which has been done taking sure that's the machine learning algorithms we develop satisfied desirable specifications such that during deployment it is safe reliable and trustworthy for use in the second half yasmin will dive into further details of the ethical implication of machine learning algorithms and more importantly how we can be thinking about designing these algorithms and deploying these algorithms such that it is beneficial to society so to start I want to give a quick motivation into why we as machine learning researchers should be thinking about both research and our responsibilities so with all of the great research which have happened over the past several decades machine learning algorithms are becoming increasingly powerful there has been a lot of breakthroughs in this field and today I'll just mention a few so one of the earlier breakthroughs was using convolutional neural networks to boost the accuracy of image classification and more recently we now have generative models that is capable of generating images with high fidelity and realism we've also seen breakthroughs in biology where now machine learning algorithms is capable of generating or folding proteins to unprecedented level of accuracy indeed the recent alpha THAAD system warned the last cast cast competition which is a protein folding competition on folding unknown protein structures later crystallized for validation we've also seen machine learning and reinforcement learning systems that is now capable of beating humans in games such as go and more recently with seeing machine learning algorithms pushing the boundaries of language where the recent GPT two and three models we've seen that these models is not only capable of generating text which is grammatically correct but really demonstrated that they are grounded in the real world so as the saying goes with great power comes great responsibility and I think now it is more important than ever for us to question where might be the negative impacts and risks of all of us and more importantly what can we do to mitigate for these risks to highlight why we need to start thinking about these risks I want to start with a few examples starting with this one which some of you are already maybe familiar with so this is a paper published in 2013 titled intriguing properties of neural networks they probably use the adjective intriguing because whatever it is that they have found in this paper was a little bit unexpected so what did they find well what they found is that you can take a state of the cup state of the art image classifier like here and you can give it an image like the one you see of a panda here and indeed this classifier can correctly predict this as a panda so what happens now if I take the exact same image and on the tiniest bit of perturbations in this image so tiny that it is imperceptible to the human eye and you can see this that the left picture and the right picture looks almost exactly the same in fact they do look the same so what happens now I want you to put this new image through this in your network we would actually expect the output of the neural network to be the same but rather when you put this new image through this mu network then the network is now almost 100% confident that this is in fact a Gibbon so in this instance maybe misclassify a panda for a Gibbon does not have too many consequences but actually we can choose the perturbation to make the output of the network to be whatever it is we want to be we can change this to a bird to vehicle if such a classifier was used for say an autonomous driving system this can have catastrophic consequences some other machine learning failure modes might be slightly more subtle there has been some studies on the recent GPT to model why they have shown that this model might be carrying some of the biases that exist in society today so in this paper titled the woman worked as a babysitter on the biases in language generation what they did was a systematic study on how the model behaved conditioned on different demographic groups for example if the prompt was changed from the male workers to the woman worked as the subsequent generated text drastically changes in flavor and maybe heavily prejudiced similarly to the black male workers versus the white male artists and I won't have want to take a few seconds to read the generated text after we change the subjects of the pot as you can see even though this language is extremely powerful it does carry some of the biases we have in society today and if this is the model that is used for something say like auto completion of text it can further exacerbate and feed into the biases that we already have yeah some will go later into details of the ethical risks of how we can use machine learning for example using machine learning in surveillance systems or for weapons so I think at this point we should really be asking this question what are our responsibilities as machine learning practitioners so this is of course an open question and possibly with no single right answer but I see it one of our responsibilities would be team sure that the machine learning algorithms we deliver satisfied desirable specifications in other words we should have a of quality control over these algorithms to enable their deployment to be safe reliable and trustworthy and if we get this right with the with this we can bring many more opportunities many more applications enabled for example more reliable with safe autonomous driving systems more robust ways of forecasting weather for renewable energy etc etc but for all of this to be possible we really need to know how to stringently test for machine learning algorithms so how can we make sure that our machine learning algorithms are safe for deployment well like how many other algorithms are quality controlled before deployment we need to ensure that they satisfied desirable specifications for example for image classifier we wanted to be robust to pathway shion's if it's a dynamical system predictor we would like here to satisfy the laws of physics we wanted to be robust to future changes that is irrelevant for prediction for example the color of the amnesty j't should not affect your digit classification if we're training on sensitive data it should be differentially private if we're giving on your network images out of distribution our neural network should be can become more and more uncertain not more more certain so these are just a few specifications that we would like how in your networks to ideally satisfy but there are many more so here I want to introduce the paradigm of specification driven machine learning so what do I mean by specification driven machine learning well we should realize the core issue lies when you're training with limited data your model can learn a lot of spurious correlations to boost metrics but has nothing to do with prediction so unfortunately what this means is that your model is ultimately hinged on the data and a metric so unless you do your training carefully your model can carrot a lot of undesirable properties that is in the data unless you specify otherwise so in this instance if the data is biased and limited then your model is to be biased and non robust unless we specify otherwise so in specification driven ml we won't enforce the specifications that may or may not be present in the data but essential for our systems to be reliable so how we can enforce these specifications will be the subject of the rest of my talk so I want to start with a specification that has relatively well studied and one which I have already kind of touched upon that is the robustness of our neural network to perturbation so adversarial persuasions so this specification is really essential if we want to deploy our networks to applications that require robustness or for applications that has real adversaries in the mix so to formalize I first want to reiterate what it is we want to achieve so we want our image classifier output to be unchanged under any additive imperceptible perturbations so to define this more mathematically let's start with a few notations so here we denote the neural network as a function f and this function takes in as inputs an image which is your Panda otherwise denoted by X parameters of your network denoted by theta and then our output this neural network should be a probability vector over your labels or more commonly the logarithms of probability vector otherwise known as modules so ideally ideally we would like our new networks output to be exactly the same as the label so in this case it's what we known to be a panda but we actually express this to be in a one-hot format vector while the element corresponding where the elements are corresponding to the label is 1 and elements corresponding to all other labels to be 0 so now we have our notation set out let's go straight into the specifications so this is the adversarial robustness specification I know this is a lot to take in one page so I will try to break this down a little bit so firstly we note that the Delta here denotes the perturbation and what this equality is simply saying is that we wanna index off the maximum probability in the probability vector outputted by the neural network to be exactly where the one is in our one hot one hot vector so this is a very convoluted way of saying we won't have a new anatomize output to be correct subject under this perturbation and now the second line says we want this to be true for all part of asians within the sets of imperceptible perturbations so in practice to ensure intercept ability we simply constrain the size of the perturbation under certain normal to be less or equal to Epsilon so the normal normally can't considered is that our infinity norm so now we have this pacification designed I want to go into a little bit more detail of one of the more commonly used method methodologies to tree our neural networks to satisfy the specification and that is a general training so APIs our training is very similar to standard image classification training but with a tiny bit of a twist so indeed with standard image image classification training what ideally we would like to do is we want to optimize our our network's weights such that the input image is quickly classified so in this instance you see the input image is a cats and thus we want the output prediction of this network to be a cat as well algorithmically what this means is that we want to minimize the weights of our network with respecter with with respect to expected loss on over our data so the last normally considered is the cross-entropy loss and here capital D denotes our data what this loss ensures is that our prediction our predicted on probability vector to be as close as possible to the label so now adversary training does something very similar except with the extra data augmentation step so here not only do we want the original image to be correctly labeled as a cat we want the image now with any additive imperceptible perturbations to also be correctly labeled as a cat as well so in practice to iterate through all of the imperceptible perturbations is obviously computationally infeasible so instead what ad thus our training tries to do is it tries to find the worst-case perturbation so what I mean here by the worst-case perturbation is simply the perturbation that maximizes the difference between the prediction and the label so I want to reiterate this again to make this a little bit clearer we want to maximize the prediction the difference between the prediction and the label with respect to the perturbation which is actually image space not in parameter space so now how does the objective of that is our training change well this is now the object the the objective of the adversarial training and you can see that it differs slightly to before where before was it was just a minimization problem now this is a min max problem so at first we want to find the maximum of the loss with respect to Delta which is our perturbation and notice that this Delta belongs in this set of perturbations denoted by capital B which is the set of imperceptible perturbations then we once we have computed this maximum now we want to minimize the parameters over the maximize at all in other words for every outer minimization step we take we have to do an inner maximization while we find the perturbation that maximizes the loss so this makes adverse our training significantly more expensive and standard image classification training I will go into a little bit more detail about this later on so now hopefully we have a method in training our neural networks to satisfy this specification how can we go about evaluating this so now in this next section I want to go over in a little bit more detail on the methodologies we used to do adversarial evaluation on finding the worst case so the goal of the adversarial evaluation is really to find the worst case perturbation for each example unit asset and once we have found this worst case perturbation we now want to evaluate the accuracy of this new test set where each example int asset is now replaced with the worst case adversarial example that is the original example plus the worst case adversarial position and this accuracy is known as the adversarial accuracy but there are several complications one of which is that to find this maximum exactly can be shown to be a hard problem when your activation functions israelis and another complication comes in when we note that this is in fact a constrained optimization problem because we want the doubter to be constrained within the set capital B so instead of trying to find this maximum exactly rather what people try to do is they try to approximate this maximum with a form of gradient ascent and because the circuit is a constrained optimization problem what we do is something called projected gradient descent so what projected gradient is is simply gradient ascent like this but the moment we fall outside of the constraint which is denoted by this yellow box here we there's back on to the nearest point that satisfies the constraint so mathematically what this now looks like is the following so I want to break this down a little bit firstly we see that within this projection function we have exactly gradient descent where you have your initial Delta and then you take a gradient with respect to Delta in the direction to maximize the loss and your step size here is denoted by ETA and once we have computed this this update step now we want to project this back onto the set that we care about and more importantly we projected onto a point which is the closest to the update update so this is the projected gradient ascent update step and actually one of the more popular forms of projected gradient ascent is the fast growing fast gradient sigh method while we're considering perturbations within our infinity-norm so what the fast gradient sign method does is it tries to replace it replaces the gradient sorry with the sign of the gradient instead but actually we can replace the gradient with any alterations made by any optimizer so for example we can replace with this with maybe momentum optimization or atom optimization so there are a lot of things for you to kind of try the step size the optimizer um the number of steps you want to take for your gradient ascent and also you want to explore these parameters such that you get the strongest evaluation possible so here at this point I want to go on to something which I want to emphasize for a little bit so I'm gonna stay on this slide for just a few minutes so what do I mean by the strong adversarial evaluation well first of all to see what I mean we need to note that your adversary or accuracy is dependent on the many during evaluation that is it is dependent on the number of steps that you take you project a gradient that your step size your optimizer and many more so the stronger your adversarial evaluation is the lower your adversarial accuracy should be and we should always be trying to evaluate how networks such that we obtain the lowest adversarial accuracy possible the reason why this is is because the lowest adversary accuracy is the number which is closest to the true specification satisfaction and that is the one thing we care about so because of this importance I thought I should give you a few heuristics of what I use I'm taking sure that my adversarial evaluation is strong so the first thing I kind of look at is the number of steps for the projected gradient a sense it might not be surprising but the more steps you take for your projected gradient ascent the closer you are to maximizing the objective that is of course conditioned on the fact that your step size is sufficiently small the second one is might be a slightly more subtle one which is the number of random initializations for the perturbations so what I mean by here is that firstly you you randomly initialize a perturbation before you start taking projected gradient ascent steps so we want to actually have a number of different random initializations this is a set especially important when it comes to detecting behavior called gradient office keishon which is something our going to in a little bit more detail later on another factor which I also look at is the optimizer that is used so this is just a good thing to try out a few different optimizers to ensure that you always get the lowest adversarial accuracy and another factor which is also quite important for detecting gradient occation is using a blackbox adversarial evaluation method um so what I mean by blackbox is when we assume that we're not given the weights of the network so the adversary evaluations which was projected rating and sent is otherwise known as a white Hawks adversary evaluation because we are given the weights of the network so the reason why I want to go into detail about why it is important to make sure your adversarial evaluation is strong is because we have seen the dangers of weak adversarial evaluation so these are two papers published in 2018 where they have actually shown that weak adversary evaluation can give you a very false sense of security so what they did was they took a lot of the defenses published up until then and then they tried their new strong adversary evaluation on all of these defenses and surprisingly the stronger adversarial evaluation work many of the defenses causing their adversarial accuracy to go to zero that is many apart from adversarial training which is easy which is the way you see on this one and this is possibly one of the many reasons why adversarial training is still heavily used today another benefit about stronger adversarial evaluation is that actually gives you a true evaluation of progress so I want to highlight this paper here because what a lot about this paper was that they did a large-scale evaluation of the defenses published up until then and then they took the numbers that were cero accuracy which was reported in the tape in there on paper and compared it to what they got under their evaluation so this work is very cool in two sense firstly they have evaluated all of these works under now a consistent set of adversarial evaluations and secondly as you can see by the dropping the adversarial accuracy for many this set of adversarial accuracy is much much stronger than what the others have used in the paper so now you can see if we can use a stronger adversarial evaluation that's our adversarial accuracy 4c fartin is not even above 60% whereas if we're just gonna take the numbers that is reported in this paper as is maybe we're going to be under the impression that is almost 70% and above that's why it is extremely important for us to take care while we're doing adversarial evaluation so another reason why strong adversarial evaluation is important is because training methods like adversarial training is permanent to an effect called gradient office keishon something that I've mentioned a couple of times already so here I will go to detail what gradient obfuscation is and to describe what grady office Gatien is i want to go back to the training objective for adversarial training so let's recall that training the training objective fabless our training has both an outer minimization step as well as it in our maximization step and in this I just want to focus on inner maximization so we can approximate this maximum similar to how we actually do adversary evaluation by do projected gradient ascent to approximate this maximum however we note that there is a little conundrum which is that even though the more steps we take we might be getting closer to maximizing major objectives but the more steps we take we also make adverse our training significantly more expensive so how can we make adversarial training cheaper well maybe a naive way of doing this would be simply to do fewer steps of gradient descent for example I can simply take maybe two steps of breaking a set but actually what happens when you take too few steps to maximize objective is that the network loves to cheat by making a highly nonlinear law surface such that simply by doing two steps of gradient would not be even close to mop two maximizing the objective you care about so here is an example for gradients office gated status so what you see on this pot here on the x and y-axes is basically a hyperplane cut through image space and we plot the loss at every single point in this hyperplane and as you can see that this is a highly nonlinear behavior for this small region of image space whereas if you do adversarial training correctly what you should actually expect is a much smoother looking law status so with all of these dangers of weak evaluation and gradient obfuscation it really pushed people into thinking about maybe a different way we can evaluate our algorithms and this is called a verification algorithms so verification is very cool in the sense that it is able to find a proof or guarantee that no attacks that has ever been invented or will ever be invented can succeed in changing a specification satisfaction of your network so there are generally two types of verification algorithms the first type is a complete verification algorithm what these algorithms normally do is often an exhaustive proof for example using mixed integer programming assuming that your activation functions is irrelevant um and also what they do is they either fare in a counter example or they find a proof that the specification is satisfied but unfortunately these algorithms are very difficult difficult to scale to deep neural networks so rather people use in complete verification algorithms so incomplete verification algorithms arm is similar to complete verification algorithms in the sense that once a proof can be found it is also a proof or guarantee that the specification is satisfied but the difference is that a proof cannot always be found even if your neural network satisfies the specification so in other words in complete verification algorithms you are lower bound on the specification satisfaction so I want to go into a little bit of detail about these incomplete for application algorithms I'm starting with this illustrative sketch of a neural network that takes in as input X and gives you an output Y so we may we generally make two assumptions for verification the first one is we assume that the input comes from a bounded set denoted by capital Axia and the second assumption we make is that our new a network consists of linear and activation layers please note that your convolutional layers can also be cast as a linear layer so with these two assumptions what we can now do is we can propagate the bounds of your input set through your linear and activation layers sequentially until we can get an output set denoted by capital Y here and once we have this output set we can simply see it if it lies on one side of the decision boundary or not however the caveat here really is the true propagation the exact propagation of these bounds is in fact np-hard so what incomplete verification algorithm instead tries to do is they try to find a more scalable way of propagating these bounds that is as tight as as possible to the true sets that we actually care about but what we kind of lose is that now instead of getting the true set we get an over approximated set of the true set so an example of such a bound complication technique is that we can imagine if your input is lower and upper bounded we can simply compute the lower and upper bound after linear transformation and similarly after that we can compute the low and upper bound after an activation layer but the problem with these techniques is really that if you're bound propagation is too loose in the sense that the over approximation is too big of an approximation of your true set the your incomplete verification can be it cannot mean very much so what do I mean by this well to see what I mean first of all we want to note that for in complete verification algorithms well we only know the over proximity set so we have no idea whether it's true setters so in the ideal case if you're over approximated set lies on one side of the decision boundary that indeed we have proven the true set lies on one-sided decision boundary as well that is to say we found that this satisfies the specification however if you're over proximate a set is too large and it spans both sides the decision boundary then there is very little we can say about the set Y of course we can try to close the gap on may be distinguishing between these two cases by actually doing projected gradient ascent like I've talked about before and here in the graph which shows the difference of doing such an empirical adversarial evaluation compared to doing in complete verification algorithm so what this graph is showing on the x-axis you can picture this to be on the size of your input set capital X and on the y-axis is the amount of specification violation so remember in complete verification algorithms give you a lower bound on specification satisfaction thus gives you an upper bound on the specification violation whereas on the other hand the empirical adversarial evaluation gives you a lower bound on that specification violation so the true specification violation lies somewhere in between if our bound complication techniques were in complete verification algorithms is tighter the gaps between these two will be reduced so there is more we can say about the true specification satisfaction of your neural network but if you're bound propagation techniques is too loose and the gaps of this becomes larger and larger at some point there is very little we can say about the specification satisfaction so today I've just touched on the adversary or a bustah specification but all of the techniques I've mentioned today can be used for many other specifications for example we can consider consider semantic consistency for an image classifier that is maybe some mistakes or more catastrophic than others for example for a self-driving car it might be okay to mistake cats for a dog because ultimately it doesn't change the driving policy very much but it is not okay to mistake him for a car or for a dynamical systems predictor we can be looking at laws of physics such as energy conservation so hopefully what I have done today is giving you a rough outline on how you can train your neural networks to satisfy specifications and more importantly evaluate how much your neural networks satisfies these specifications but more importantly I very much hope I have motivated everyone into thinking why looking to this is important and this concludes the end of my talk and now on the path of the yasm who will give you more detailed overview of the ethical implications of machine learning algorithms and more importantly how we can be thinking about deploying and designing these algorithms to be beneficial to society I'd like to start by thanking chun-li for her fantastic exposition of some of the key challenges that arise from building algorithms that are safe robust and fair in this section of the talk will focus more directly on the question of responsibility and what it means to deploy these technologies successfully in real-world settings however before we get started I'd like to reintroduce myself quickly my name is Yasmin Gabriel and I've been working at deep mind as a research scientist in the ethics research team for three years before joining deep mind I used to teach at a university where my work centered on moral philosophy and practical ethics including questions about global poverty and human rights at deep mind our team explores questions that arise in the context of ethics and artificial intelligence some of which we'll look at in the course of the next hour so if we begin with the topic of ethics and machine learning we immediately encounter questions including what is ethics and why does it matter and how does ethics connect with machine learning I'd like to take these questions in turn ethics is a field of inquiry that's concerned with identifying the right course of action with what we ought to do is centrally concerned with the equal value and importance of human life and with understanding what it means to live well in a way that does not harm other human beings sentient life or the natural world according to our everyday judgement some actions are good some are acceptable and some are prohibited altogether understood in this sense ethics is interested in identifying what we owe to each other and how we ought to act even in challenging situations these situations can arise in our personal or professional lives however they also arise in the context of machine learning research what I'd like to suggest is that far from being outside the domain of ethical evaluation technologists and researchers are making ethical choices all the time and many of these choices deserve closer consideration as strongly noted a good place to start is with the training data we use to build machine learning systems in particular we need to appreciate that data is not only a resource but also something that has ethical properties and raises ethical questions for example as the dates have been collected with the consent of those who are represented we cannot take this for granted of course there's high-profile cases of data being collected without people's consent such as Cambridge analytic oh but it's also a common challenge for major datasets used to Train image recognition systems that often use pictures of celebrities or simply images taken from the internet second who or what is represented in the data is the data sufficiently diverse or does it focus on certain groups to the detriment of others if we train a model on this data will it perform well for people of different genders nationalities or ethnic backgrounds or might it fail when applied to a group to these groups in significant ways thirdly how's the data labelled and curated does it contain prejudicial associations as Kate Crawford and Trevor Paglen have demonstrated in their work on excavating artificial intelligence in the early days of imagenet it contained a person's class that assigned pejorative labels to a variety of images of real people this is also a problem for historical data that's drawn from specific social context regardless of how that data is labeled it may contain associations that are a reflection of human prejudice and discrimination these challenges which arise early on in the machine learning pipeline have come to have a real-world impact a phenomenon that's most commonly referred to by researchers is the problem of algorithmic bias indeed while these technologies have great potential recent evidence suggests that far from making things better software used to make decisions and allocate opportunities as often mirrored the values and biases of its creators extending discrimination into new domains these include the domain of criminal justice where a program used for parole decisions mistakenly identified more black defendants as high-risk than people in other racial categories compounding entrenched patterns of racial discrimination within the criminal justice system has also been seen with job search tools which have been shown to offer highly paid jobs or advertisements for highly paid jobs to men over women by a significant margin sometimes by ratio of up to six to one is also a problem that's been noted for image recognition software which has been shown to work less well for minorities and disadvantaged groups and lastly has been something that we've encountered in the domain of medical tools and services which have been shown to perform markedly worse for people with intersectional identities something that could mean that they have unequal access to life-saving services and medication faced with this mounting body of evidence evidence we cannot rely on good intentions alone there is an important body of work to be undertaken to address these failings it is also clear to return to a point that Chong Lee made earlier that those who design and develop these technologies are in a position of power so what is power in this context I think is best understood as the ability to influence states of affairs and more importantly to shape the lives of other people more precisely those who develop new technologies shape the world creating your opportunities for closing others and shaping the part that humanity is likely to take this can be seen with major inventions throughout history such as the steam engine or electricity artificial intelligence is now also starting to have profound effects some of them positive and some of them more challenging with this power comes responsibility however the question then arises responsibility to what Chong Li has already shown us what is possible from a research perspective clearly there are certain things that we can do and that we may well be required to do when building ML systems however I'd like to focus on the question of responsibility and see if we can develop our understanding of what is required a little further at a collective level I believe that an understanding of the relationship between power and responsibility should lead us to reflect more deeply on the question of what it means to do machine learning well after all the activity of scientific research is not value neutral rather it is a social practice that human beings engage in there's governed by shared norms that change over time some of these norms are epistemic for example ideas about the need for replication in order to confirm scientific findings others are normative or moral for example about the acceptability of doing research on people without their consent ultimately with the way we structure this practice including the way we think about what it means to do good research has profound social effects these questions about appropriate norms and standards for research are particularly important when the stakes are high and when there's uncertainty about the overall impact in response to the unique challenges posed by nanotechnology governments and civil society groups came together to develop a shared understanding of what they term responsible innovation these groups characterize responsible innovation as a transparent and iterative process by which societal actors and technologists become mutually responsive to each other's needs to ensure the eighth corne social value of the scientific endeavor this paradigm plays towards the important idea that good science is itself based upon alignment with the social good and with democratic processes a point that I'll return to later on at the same time the paradigm has been criticized for its vagueness what precisely are researchers responsible for what should they do and how does this apply to the realm of machine learning these are questions that I shall now try to answer more concretely as we turn to principles and processes for thinking about the ethics of artificial intelligence so as AI researchers I believe that we share in responsibility for at least two things first we are responsible for intrinsic features of this technology so that we build the very best systems that are sensitive to ethic on social considerations and are designed in ways that limit the risk of harm secondly we bear some responsibility for extrinsic factors that help determine whether it's designed deployed and used wisely in ways that produce beneficial outcomes both elements are necessary robust and secure technologies can be used in harmful ways by bad actors and faulty technologies can be problematic even if they're deployed and used responsibly in terms of the content of these obligations what I've termed the responsibility for what question there are a multitude of AI principles that broadly speaking aim to align machine learning research and systems with the social good this includes offerings by the European Union the OECD the Beijing Academy of AI and by the future of life Institute indeed one recent study found that there's at least 84 different ethical codes have been proposed for AI fortunately these principles coalesce around certain key themes such as fairness privacy transparent transparency and non malfeasance the last condition which is sometimes characterized as do no harm leads us to focus on the affirmation of individual rights this includes things like respecting the requirement for informed consent an equal recognition before the law lastly I believe we should try and develop artificial intelligence in ways that satisfied the collective claim to benefit from scientific discovery interestingly this is also found in the United Nations Declaration on Human Rights which establishes a right of all humanity to share in scientific advancement and its benefits yet even with this understanding of the key values in place certain gaps appear to remain first and foremost how do we move from these statements of principle to clear and robust processes for evaluation we know the good intentions and not enough but how do we take abstract moral ideals and turn them into these concrete processes and procedures how should we balance and weigh different ethical principles against each other and how should we deal with the fact that machine learning research is often highly theoretical in general meaning that there's uncertainty about how it will ultimately be used one answer is that we need tools that help us put principles into practice in what follows I described a five step process that technologists can use to evaluate our research and help make sure that is ethically and socially aligned while this process does not aim to capture the entirety of a ia thix my hope is that it can be of use to many of us who are doing to work designing and building algorithms this framework is particularly helpful in relation to the values previously discussed I will now run through the process briefly and then return to consider each stage in more detail so the first question we need to think about when building new technologies is whether it has a socially beneficial use is there actually a reason to develop the thing that we want to develop now most technologies do have socially beneficial uses of one kind or another but if we are unclear about what the value is that we aim to unlock that's typically a red flag that should immediately force us to go back and reconsider what it is that we want to do by getting this social purpose clear in our minds it's also often true that we can create better versions of the technology that we had in mind but once we have this social purpose in mind we then need to turn to think about the risk of direct or indirect harm that the project rings were there so here again is typically true that most technologies bring with them some risk of harm and the important thing is to try and map out these risks clearly so we know what we're contending with at every juncture then with an idea of what the benefit is and the risks that the project brings were there we should turn to think about mitigation are the steps we can put in place that will reduce the risk or eliminate risks entirely then with this plan in place we can finally turn to the first evaluative stage so now we have the best version of the technology or piece of research in mind and we can ask with these measures in place does the proposed action or research violate a red line or a moral constraint does it push up against some threshold or barrier which comprises the sort of things that we just ought not to do finally on the assumption that we haven't hit one of these hard constraints we have the question of whether the benefits outweigh the risks from an ethical point of view and at this point is often sensible not just to focus on the specific project at hand but also to consider other options that are available to us given that we've been afforded this opportunity to do research is this really one that looks like it will add value to the world okay so now we'll take a moment to look at these questions in more detail so what is a socially beneficial use of technology what does this mean well I think that a technology can be socially beneficial in a variety of ways to start with it can it could contribute to human health or well-being so could make us physically more healthy or better off in some way technology could also enhance our autonomy or freedom if it empowered us to act in a way that fulfills our own goals or outcomes for example by giving us useful information or by helping us be more discerning when it comes to understanding the world around us technologies can also help produce fairer outcomes if they're well designed and calibrated and developed in a way that includes the voices of people who are affected they can contribute to public institutions such as health care or education and these systems can be used to address local challenges such as climate change of course the certain gold standard research for ethically impactful work such as medicine or work to address climate change but it's also okay to focus on more prosaic goals and aspirations for example to try and create technology that brings people enjoyment or gives them more time to do other things if we take an example a technology developed by deepmind which was wavenet an algorithm that we created to help produce better quality synthetic audio I'd say that one of the socially beneficial uses of that was to help visually impaired or illiterate people access digital services more effectively through through voice interactions secondly we then have to consider this question of harm so what sort of things fall into this category well here what we see is that the harms are often the inverse of the benefits that we might try and unlock so instead of improving human health or well-being technology might undermine human health or well-being including potentially mental health it might restrict human freedom or autonomy something that comes to the fore if we think about the challenge posed by addictive content it might lead to unfair treatment or or outcomes as we saw in the case of algorithmic bias it might harm public institutions or civic culture and it might infringe human rights so if this is the case it's something that we really would need to be mindful of returning again to the case of wavenet we understood that that research carried with it certain risks including voice mimicry and deception if individuals used it to copy each other's voices and it could also potentially a rogue public trust in the fidelity of audio recordings so once you have an understanding of these risks the question is what can you do about them is it possible to mitigate this risk or to eliminate them entirely and in that regard there's a number of significant things we can think about the first is whether it's possible to control the release of technologies or the flow of information so often technologies only have harmful outcomes if they fall into the hands of people who have a malicious intention or purpose and we need to think about whether there's ways to prevent that from happening we might also wonder if there's technical solutions and countermeasures that we can use to make our technology harder to misuse something that wrongly really delved into and in the context of wavenet we can think about things like water marking so that we always know the progeny of a specific piece of synthetic audio or detection we also have the opportunity to work with public organizations and the public and to communicate certain messages around technology so sometimes if a new risk is being created as important to make people aware of this and although that often isn't sufficient to discharge our moral responsibility so finally we may seek out policy solutions and legal frameworks that help contain the risk and in the case of the challenges raised by synthetic media and civil society groups have been really really important in terms of coming up with United agenda that helps make sure that these technologies are used in a safe and responsible way so once we have our mitigation plan in place we have this question of whether the proposed action violates a red line or moral constraint these constraints are sometimes referred to in moral philosophy as deontology they're made up of a set of rights and duties that mark out the sorts of things that we should not do for example it would be deeply problematic to develop technologies that contravene consent protocols they infringe on people's personal space in ways that they haven't consented to similarly in the domain of artificial intelligence there's a concern about lethal autonomous weapons and what happens when human decision-makers this delegate these these fundamental decisions to machines so as an international movement which has also developed around legislating in this area and containing that risk and ensuring that this technology isn't used in a way that intentionally harms or injures human beings thirdly we might be concerned about certain forms of surveillance that also could have an a coding effect on public trust and lead to people for being victimized and targeted in certain situations so that would be a really important Avenue to be mindful of and then we have technologies that potentially infringe human rights or international law and basically if there's any risk that the technology will be used in a way that contravenes one of these fundamental purposes or one of these fundamental principles that again is a red flag which really really should encourage us just to go back to the drawing board and ask the earlier question what does a safe beneficial and productive version of this technology look plain however on the assumption that we haven't encountered one of those hard constraints we still have this final question which is with these measures in place to the benefits of proceeding outweigh the risks of doing so and as I mentioned earlier when we think about machine learning research is a tremendous it's a tremendous opportunity to do something really beneficial and we've actually been afforded a great opportunity typically by universities by research institutions to try and use our talents in a way that's genuinely helpful so we should ask is this project something that we really feel will deliver the kind of social return that we care about and that has the potential to make people better off in practice however even when we've got through this process and come to the conclusion that we have got a good way of proceeding is still worth pausing to evaluate our findings and to conduct to further tests these tests are particularly helpful because they can help us address the problem of motivated cognition I had the widespread problem of unconsciously endorsing arguments that support our own interest or our preferred course of action so first I'd ask have you thought about all the people who could be affected by your action more precisely have you identified these groups do you know who they are have you considered what would happen if you are trying to explain your reasoning your or your decision to them and have you directly sort out their advice and input this test is important for a number of reasons to begin with it those who are affected by new technologies typically have a right to be included in decisions that affect them moreover an eve moreover even a process of imaginary and sympathetic dialogue can help us guard against error if we'd really struggled to explain our actions to someone who is adversely affected by technology then this is often a good reason to revisit our conclusions and work hard to identify a solution that could pass that test secondly I think we should ask whether we've thought seriously about hard decision might be viewed in the future is it something that we might have reason to regret here we can imagine someone in the future perhaps even our own children or grandchildren asking us why we chose to act in the way we did why for example did we fly to so many conferences when we knew about climate change and the harmful impact we were having how might it feel if we were to discover that our technology was subsequently used in a way that violated human rights if these questions make us feel uncomfortable then we have reason to introspect and identify that source of discomfort moreover we should typically adjust our behavior to act in ways that minimize the likelihood of future justified for them so now we've had a chance to look at the responsibilities of machine learning researchers and at a process for evaluating our decisions and choices however is also worth pausing to reconsider where we are now as a field and what the path ahead might look like the field of machine learning is changing rapidly both in terms of technical developments and breakthroughs and also in terms of ethical norms and standards increasingly I believe there's recognition of the following key ideas first those who design and develop these technologies have a responsibility to think about how it will be used this responsibility stems from the fact that the technology is powerful and from the fact that it has moral consequences that we can observe and there were in a position to affect secondly there are concrete steps and processes that we can put in place to make sure that this responsibility is successfully discharged these include processes like the ones we've considered today that help promote the good while also respecting important constraints thirdly while it's not possible to know the consequences of all our actions we are responsible for what we can reasonably foresee and should take steps it is designed to bring about positive outcomes even when this means incurring certain costs given the power of machine learning technology and the attendant responsibilities of people working in this field good intentions are not enough we have an obligation to try and understand the impact that our actions will have on other people and to act conscientiously enlightened in light of that information beyond this we can pause and ask about the path ahead so in this regard I see three exciting developments that we are all part of at the present moment so to start with is an important new research agenda that's developing in this space and includes a critical focus on areas such as AI safety robustness fairness and accountability this technical work to improve the moral properties of machine learning systems requires constant vigilance and effort but as Chong Li has shown us there are things that we can do and that we can change to build better machine learning systems secondly we're starting to see the emergence of new norms and standards that point towards a different understanding of what it means to do machine learning well on this view what is needed is not only technical excellence but also for research to be done in the right way and for the right reasons limiting the risk of harm while also working to create technologies that benefit everyone [Music] finally we're seeing the emergence of new practices which aim to promote responsible innovation in machine learning these include the release of model cards that explain the intended uses and properties of ML systems a proposal from top research labs to create bias bounties aimed at discovering bias in models and datasets that we use and a new requirement from the machine learning conference new reps which asks all researchers to consider the ethics and social impact of their submissions and to detail this when we write papers while very significant challenges still remain I think that these developments send a good sign towards the future with the requisite degree of effort reflection and conscientious endeavor they will help ensure that machine learning as a field stays on the right track and continues to be an area of study that we can all be proud of thank you you 