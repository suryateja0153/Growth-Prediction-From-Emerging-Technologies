 [Ali Jadbabaie]: Good afternoon, everyone. Welcome to our first virtual IDSS distinguished seminar. It's my great pleasure, together with my colleague Alberto (Abadie), to welcome Michael Kearns from the University of Pennsylvania. Michael is the National Center Chair and Professor of Computer Science at Penn. He also holds secondary appointments in the department of Economics, Statistics and Operations and Information and Decision in the Wharton School. Of course, he's not a stranger to many of us. Michael and I, of course, have been close colleagues and friends for many years, and I'm delighted to welcome him to tell us about his recent work which has culminated into a new book called 'The Ethical Algorithm.' Michael has worked on learning theory in the 1990's, worked on algorithmic game theory. He was involved in Wall Street and algorithmic trading and he was just telling me that he's going to head a new ethics group, ethics of AI fairness group at Amazon fairly soon. Michael, we're delighted to have you here and without further ado, welcome. [Michael Kearns]: Hey okay Thanks Ali, and good to see you again, and to see the icons of many friends and colleagues joining. Yeah, so I want to do something that might be a little bit unusual for this seminar series, which is tell you a bit about a book that I recently wrote with my good Penn friend and colleague Aaron Roth, and it's an unusual book perhaps in the sense that it's meant to be a general audience book and so I want to use the first most of my time to kind of survey the themes of this book and maybe get you interested in the underlying topics, but since I know this is a bit of a mixed audience and that there are plenty of technical people on the line, I'm hoping to leave some time at the end to talk about some more advanced and more recent topics in algorithmic fairness and machine learning. But the title of our book is 'The Ethical Algorithm,' it came out late last year and let me start by kind of explaining why we decided to kind of take off our pure research hats for a while and write a general audience book. So it's really the reasons were twofold. One is that, you know, Aaron and I are very close, we work well together, we collaborate a lot on research and advising students and the like - kind of run our groups jointly together these days and, you know, we're one of a growing community within the machine learning community thinking about ethical issues from a technical standpoint, and so, you know, we were doing research in this area already and thought that it would be fun to try to explain it to a broader audience than people like ourselves, but we were also kind of inspired a little bit or maybe partially inspired and partly anti-inspired by other general audience books on similar topics that came before us. So these are just three examples in front of you, many of you may have read some of these books. 'Weapons of Math Destruction,' 'Algorithms of Oppression,' and 'Data and Goliath.' These are all books that in different ways talk about the unintended or collateral side effects or consequences of large-scale algorithmic decision-making, usually driven by machine learning. And we like all of these books, we admire these books, we think they're well-written, we think they have interesting messages and they do a very good job of making real for the layperson the kind of visceral damages that machine learning can cause to individual members of society in applications like, you know, machine learning for criminal sentencing and other aspects of the criminal justice system, or consumer lending or college admissions and the like. And the parts of these books that disappointed us for the most part was, you know, usually towards the end you get to the solutions section where they say - 'Well, you know, what should we do about these problems?' and the answers are basically of the form 'Well, we need better laws, we need better regulation, we need watchdog groups. We really have to keep an eye on all this stuff." And we do not disagree with any of that, but as, you know, researchers working in computer science and machine learning, we also knew that - well, you know, if there's something we don't like about the behavior of the algorithms that we've designed, we could think about fixing the algorithms in the first place, right? We don't have to just throw up our hands and treat these things as opaque black boxes and leave it to lawyers and regulators to figure out how to control them. We could try to, you know, design better algorithms in the first place and that's what the science that we and many others have been working on in recent years, and that's really what our book is about. So our book focuses less on the problems themselves, although it talks about them at some length so that they can be explained, but the emphasis is really on solutions of an algorithmic nature, and you know if you read the book you'll see that we're modest about this, we're realistic about it, you know. We don't pretend that  better algorithm design can fix real social problems, So, if, for instance, in criminal sentencing applications part of the problem is that your police force on the ground is, you know, racially biased in the first place and so that the data that you've ingested is already reflecting that bias, you shouldn't expect some algorithm to magically cure that social ill. There's sort of no substitute for, you know, kind of fixing the social problem upstream in the first place. But there is something to do at the algorithmic level as well, and that's largely what our our book is about. I want to start by telling a little bit of an anecdote in - when we were writing the book. So, an early draft of the book was sent out to some reviewers of different types so we could get some feedback and one of them came back with a very interesting comment, which was like - 'well, you know, I like the book and the topic but the title strikes me as a conundrum or maybe even an oxymoron,' and this person said like, 'well, you know, after all how can an algorithm be any more ethical than a hammer?' and, you know, both algorithms and hammers are human-designed artifacts, they're designed for particular purposes and they can be misused, right? I could hit you on the hand with a hammer but nobody would kind of ascribe the unethical behavior there to the hammer itself, you would blame me for misusing the hammer in an unethical way. And so we thought about this, and we kind of respectfully disagreed and we disagreed for a few reasons. One reason is that even though, yes, algorithms are human-designed artifacts and we usually intend them for certain purposes, the flexibility and generality of purpose of an algorithm these days is so broad compared to the very, very specific use case for a hammer that we really argue that, you know, as many AI researchers have argued in their own way before us, that algorithms can really be more - can be closer to the human mind in their generality and flexibility of purpose compared to a hammer. So that's sort of reason number one. Reason number two is just sort of the diffusion of responsibility, if you like. So when we talk about algorithms and decision-making or even algorithms in the book, we often more specifically mean the predictive models output as the results of the machine learning pipeline, and just, you know, that pipeline looks like you start with a data set that you've collected somehow that might be quite large and high-dimensional. You use that to define some objective function which is usually primarily, if not exclusively concerned with predictive or error or accuracy. You then, you know, minimize - you use stochastic gradient descents in a convolutional neural network or whatever your favorite tool is to find a model which does well with respect to that objective, and then you [inaudible] thing that's actually deployed and, you know, not only is it hard to specifically ascribe the ethical behavior to any single individual in this pipeline, because of, you know, the data might have been collected by one organization, the kind of training might have been done by another organization. The actual deployment of the model might have been done by a third set of people, and so it's hard to sort of blame any particular individual or organization and number three - even worse, at least, you know, if I hit you on the hand with a hammer, you know that I hit you on the hand with a hammer and I know that I hit you on the hand with a hammer. There's no doubt about harm being caused to you and exactly what the harm was ,but with algorithmic decision-making at scale, you know, harms can be caused all the time to people that don't even realize that that harm is being caused to them, right? So people might be kind of unfairly being rejected for loans or denied admission to college or given larger criminal sentences than they deserve and they may not even realize that it's a largely algorithmic-driven process behind the scenes that is causing that harm and, you know, the media is rife these days with examples, like the recent kind of claim of gender bias and credit limits in the Apple credit card, that, you know, until somebody just bothers to go look for that harm, you don't even realize that it's happening at scale. So, you know, the basic premise of our book and the research that it describes is that we need to take seriously the idea that we would actually embed social values in algorithms like literally in the Python or whatever code of our algorithms, okay? And just to make clear something that I think is probably clear to most people in this audience, but isn't always when when we drift out to less technical audiences. We are definitely not proposing here that algorithms themselves decide what something like fairness should mean or what privacy should mean. We argue and and state that we fully believe that those definitions should still be chosen by society, by people, okay? But once you commit to a definition or definitions of privacy and fairness or similar things, if you've done so precisely enough then you really could embed that as a constraint in your code, and I'll kind of get to what those constraints look like in due course, but I want to sort of again make a high-level, somewhat philosophical comment here, which is, you know - it is not lost on us, the computer scientist and machine learning researchers are far from the first scholars or people to think hard about things like fairness and privacy. I mean, so certainly something like fairness goes back to the moral philosophers, the Greeks and earlier at least, and more recently, certainly, legal scholars have thought hard about what fairness should mean as well as economists and many others. But there's a special burden when you intend to be so precise in your definitions that you could actually embed them in computer code and one of the things we argue in the book is that there's a, you know, even just the act of having to be that precise about your definitions can sometimes expose flaws in our intuitions about concepts like privacy and fairness that you really weren't going to discover any other way then by thinking that carefully and precisely. And I'll give examples of this for both privacy and fairness, I'm going to sort of show you alternative definitions for these notions and point out that, you know, sometimes the one that you might have come to first is actually fundamentally flawed in sort of irreparable ways, and that if you think a little bit harder there's a better definition and I'll give examples of both, you know, bad and better definitions. And so our book kind of takes the reader on a guided tour of - organized by different social norms - some of which I've listed here, and sort of takes the reader down the road of, like well, what what what's a good definition of privacy and what are the consequences of it? What's a good definition of fairness and what are the consequences of it? And there will be consequences, as we'll see. And so I want to do a little bit of that in this talk, mainly emphasizing privacy and fairness, and as you see here, I've written a number of different social norms or values and the - you know, you might wonder what is the grayscale encoding here. The grayscale is encoding our subjective belief about how mature the science in each of these areas are. So we argue that the study of privacy and specifically differential privacy is the most mature of these kind of, you know, algorithmic social norms with respect to our knowledge. Fairness is maybe a decade behind privacy but is off to a good start. Accountability, interpretability are, you know, there's research on them that, in our view, good definitions have not been arrived at for those things yet, and I promise you, you know, written in white at the bottom of this list is the singularity, which we say a little bit about at the very, very end of our - of the book, but as you can imagine we don't think that that concern is a priority at this point. There's sort of bigger fish to fry right in front of us now. Okay, so let me talk a little bit about algorithmic privacy. So, there's a famous quote by Cynthia Dwork, one of the founders of differential privacy, which is "anonymized data isn't," and what she means by this quote is that, first of all, definitions of privacy based on anonymity concepts are fundamentally broken, and so either, you know, when you've anonymized data you really haven't, or you've had to anonymize it so much that it isn't even data anymore. And one of the themes of our book is that it is unfortunately the case that to the extent that social norms like privacy or in fairness have been encoded in our laws, they usually have unfortunately adopted the wrong definitions like definitions of anonymity. So, you know, for instance, HIPAA, which, you know, is the encoding of, kind of, health care privacy laws in the United States, is all based on, sort of, removing personally identifiable information or anonymization techniques. And so, what's the idea behind anonymization? I think I can be quick with this audience, right? The basic idea behind anonymization is that somehow what you should do is go to a database like this medical database at the top of this diagram, and you should, you know, there's sort of two operations given to you. There's redaction, meaning you can delete an entire column, so here I've deleted names entirely, and then you also have coarsening, right? So, here I've coarsened the age from exact values to decades - you know, are you 20 to 30, are you 50 to 60, are you 60 to 70? Zip codes have been partially coarsened, right, I've kept the first three digits but dropped the last two, and then I've kept some health information. And, you know, what is the idea behind anonymization? The hand-wavy idea is that somehow, if I do enough of this stuff, the data set would still be useful, but I wouldn't be able to identify any particular individual in this data set. I wouldn't be able to like reverse-engineer your medical record, okay. And, you know, you can make this precise in different ways. So k-anonymization is one such formalization of anonymity. So, we basically say that a database is k-anonymous if, you know, or  k-anonymous with respect to some subset of the rows, if, when I project the database onto, er - I'm sorry - if I project the database just onto those columns, any tuple that appears in those columns is replicated at least k times in the data set. So this top database, I promise you, is too anonymous with respect to age, gender and zip code, meaning that if you look at any particular combination of values that appears, like 50 to 60 female and (zip code) 191, there's at least two individuals that match those values, and so the high level idea is that, you know, if the database is k-anonymous, I would be confused about which record was yours. So I can't exactly reverse-engineer. So for instance, if I - let's say that I had a neighbor, you know, and I knew her name is Rebecca and I know that she's 57 years old and she's female and that she's been a patient at the hospital of the University of Pennsylvania, and let's suppose this is the database, the anonymized version of that database, well when I go look at this database I know that she either corresponds to this row or this row, but I don't know which one and so then I don't know whether her diagnosis is HIV or colitis. Okay, now notice that already my neighbor might have found it an invasion of her privacy that I know that she has one of those two conditions, but I haven't, like, completely blacked out which condition she has, and you might argue, like, well if I'd made this database instead of two anonymous, a thousand anonymous, there would be some actual privacy guarantee. So the problem, the real, the fatal flaw with anonymization definitions is that they pretend like the data set in front of you is the only data set that's ever going to exist in the world in perpetuity, and, you know, if we ever lived in such a world we certainly do not now. And so, you know, the real problem is, if there's another, for instance, anonymized database - so here is, maybe, the database of Jefferson Hospital here in Pennsylvania, and maybe I happen to know or suspect that my neighbor has also been a patient at Jefferson Hospital, and this database is three anonymous, so when I look at the same set of values - age 50 to 60 females, zip codes starting with 191, three records now match, but if I take the join of these two databases, of course, I know that uniquely the only diagnosis that matches in the join is HIV, okay. And, you know, lest you think that this is an academic concern, there have been myriad, you know, kind of de-anonymization attacks on real data sets that have occurred by exactly this kind of triangulation technique. Perhaps the first and one of the more famous ones being the reverse engineering of the Netflix data set when it was released as a competitive data set in the machine learning community. So, you know, long story short, we argue in the book that, you know, here is a precise definition of privacy that's the wrong definition. It's a broken definition, you know, and it's unfortunate to the extent that corporate policies and laws are ever precise about what they mean by privacy, they sort of say 'don't worry, we anonymize your data,' you know, run in the other direction as fast as you can. Not that you really have that option, but it's a broken definition. So, you know, this leads us to the question - well, what's a better definition of privacy? And so, you know, if I'm arguing to you that notions based on anonymity are far, far too weak, let me go in the other direction and propose, you know, what is arguably the strongest definition of privacy that we could possibly imagine, and I won't make it precise, but you can imagine that it's not hard to do so. So, what if I told you that, you know, any analysis or computation that was done involving your private data could never result in any harm coming to you whatsoever. So let me just repeat that - any analysis or computation involving your private data can never result in any harm coming to you, okay? So I think even without formalizing that definition, we would all agree that if we could somehow, you know, make use of that definition, it would be great, right? I mean you'd have this sort of very general promise, whatever - you get to define whatever harm it is you're concerned about, and no matter what the analysis or computation is, that harm isn't going to come to you, okay? So this has been proposed in sort of the philosophical privacy literature before, and the problem with it is that it's too strong. Meaning that we can't do anything interesting or useful with data if we adopt this as our definition of privacy, and let me give - let me give an example from a famous medical study from the 1950's. So this is the cover page and one of the early articles by Doll and Hill, who were two British physicians who are famous for being the first ones to firmly establish a correlation between smoking and lung cancer, and in a decade-long study throughout the 50's, they basically recruited two-thirds of the doctors in the UK to voluntarily allow their medical records to be used in this study that eventually established the connection between smoking and lung cancer. Okay, so suppose you were a doctor in the 1950's and you just, you know, you said "sure, go ahead and use my medical record" and let's suppose that also it was a generally known fact by, let's say, your friends and others that you were a smoker, because by the way, in the 19-- you know, in 1950 pretty much everybody was a smoker, and nobody tried to hide it. There was no social or medical stigma associated with it. It was even seen as glamorous, right? So, you go ahead and submit to this study, and then this study announces to the world that there's a link between smoking and lung cancer and now we could argue real harm has come to you as a result of this. Your data was used in the study, and now real harm has come to you as the result of this study. The world knows about this link now, and in particular financial harm might have come to you because now your insurance company, knowing that you are a smoker, and now knowing this new fact about smoking and lung cancer, you know, their posterior belief that you might have lung cancer or get lung cancer goes up, and they might legitimately raise your premiums. So real financial harm has come to you. Okay, so if we adopt this very strong definition of privacy, right, we would have to exclude uses of data like the British doctor study, which is obviously, you know, a benefit to society. Okay, but if you're with me so far, you're probably already seeing the escape hatch here, right? So, the reason this definition of privacy is far too strong is that, you know - yes, your data was used to establish this link and it did cause you harm, but it's not like your medical record was the key piece of data that allowed this link to be made, right? Any sufficiently large data set of medical records is sufficient to establish the link between smoking and lung cancer, and in particular, if we think about - instead of, sort of, comparing the study being done with your data to the study not being done at all, which is sort of the counterfactual that I'm comparing in this too-strong definition of privacy. If instead I compare the counterfactual of the study being done with your data and the study being done with everybody else's data except yours - right, you know, the harm is going to come to you in both of those cases, right? Because you know if two-thirds of the doctors in the UK participated and we just remove your medical record, the same conclusion is going to be reached by Doll and Hill. So this brings us to the definition of differential privacy, which I'm not going to give technically, some of you might be familiar with it, but it basically formalizes what I just said. We imagine - we compare two alternatives. We imagine a computation taking place on a large database of private medical records, let's say, and there's some output of the algorithm under that database, and then we contemplate running the same algorithm on that same database minus your medical record, and we ask that the output of the algorithm be controllably close in those two settings and controllably close means in  distribution. So the definition of differential privacy fundamentally requires randomization in order to achieve it. So, in other words, we basically look at the distribution induced over the outputs of the algorithm with your data and without-- with everybody else's data but yours removed. And if those two distributions are controllably close, then we say we have achieved differential privacy and there's also a parameter which controls how close the two distributions have to be. And the high-level idea of differential privacy is you achieve it by adding noise and let me just briefly mention, you know, a classic example of differential privacy that actually was proposed several decades before the definition of differential privacy, which is randomized response. So randomized response is a protocol from so-called the literature. It's meant to elicit or encourage truthful responses to embarrassing or stigmatizing questions. So suppose I want to hold a survey of MIT undergraduates in order to estimate the fraction or rate of cheating at MIT. So I want to ask students at MIT to answer the question: have you ever cheated on an exam at MIT? So if I set up some website and I direct MIT undergraduates to this website and I just say "please honestly answer this question," you know, even if I don't - even if I say, you know, I'm not gonna ask for your name, they might worry that I'm gonna track their IP address or they might just be kind of embarrassed to answer "yes" if they've cheated, even if they're, you know, not worried about their identity being revealed. And so randomized response is kind of a trick to get people to respond in a truthful way that still provides them with a sort of notion of privacy or plausible deniability and so the protocol would go as follows. It has a parameter P but for starts, let's just, let's suppose that P is a half. So basically the protocol when P is equal to half basically says: "okay, I want you to flip a coin and if the coin comes up heads, then answer truthfully the question have you ever cheated on an exam at MIT. If the coin comes up tails though, flip the coin again, and if it comes up heads just answer 'yes' regardless of whether you've cheated or not. And if it comes up tails answer 'no' regardless of whether you cheated or not". So, you know, with probably a half you're answering truthfully, and with probability a half you're answering randomly. Okay. So it should be intuitively clear from this, right, that if you follow this protocol, you know, and you answer "yes," even if I confront you "hey, I tracked your IP address and you answered yes to this protocol" - you can say: "yeah, sure I did. I haven't cheated on an exam, but the first coin came up tails and the second coin came up heads so I answered yes even though I haven't cheated." So every individual simultaneously has strong plausible deniability because of the randomization added by this protocol. But, you know, simple arithmetic will reveal that if I have a sufficiently large number of people, let's say n people engaging in this protocol, and they all follow it honestly, they all follow this decision tree, then I can back up... I can back out a very accurate estimate of the underlying rate of cheating, within plus or minus one over square root of n, if everybody follows this protocol honestly even though everybody simultaneously has a strong plausible deniability. And technically speaking, you know, this protocol satisfies differential privacy, but it has this parameter, right, so if I now reintroduce this parameter as I move P towards one, I'm asking you to answer truthfully with higher probability and so my estimates, you know, your response carries more signal, but you have less privacy. If I take P towards zero, you have more privacy but I have a weaker signal so I need a larger sample to get a good estimate. So this is the idea behind differential privacy. Differential privacy is now really being deployed widely in some serious applications. It's widely used by tech companies like Apple and Google in various ways, but it's also been adopted by the 2020 US Census. They've, you know, they decided that every single statistic that gets released as a result of the 2020 census will be done under the constraint of differential privacy. And this is not without controversy or, you know, concerns. This is a recent op-ed from the New York Times that's basically pointed out that "well, if you're taking the raw census counts and adding noise to them, then small communities might be entirely sort of disappeared by the noise." It's not going to affect the relative count of a city like Philadelphia or Boston, but very small communities might have their accounts radically changed and since federal budget apportionment is based in part on census figures, what's gonna happen, you know, to these communities. And I think these are some of the many, many non-trivial engineering details that the Census is trying to work through even as we speak. And more recently and very topically, some of you may have been aware that Google is regularly releasing these community mobility reports. These are kind of heat maps that basically let you go to your city and just see from  GPS data where people are still kind of gathering in large numbers in close proximity. It doesn't have any health data, this isn't contact tracing and it's a longer discussion, but sort of the vanilla definition of differential privacy isn't... Contact tracing is not a good use case for the vanilla definition of differential privacy because differential privacy basically promises the same level of privacy to everyone, and in contact tracing by definition you don't want to provide privacy to the people that have been infected, right. But Google is releasing to the public these community mobility reports and basically they use GPS data to sort of find these hotspots of proximity and then they add noise to it to give privacy guarantees to the individual kind of geolocation data of the individuals that went into the reports. Okay, good. So now let me say a little bit about fairness. Algorithms for fairness compared to differential privacy is a work in progress. First of all, if you read the book, we argue in the book that differential privacy really is kind of the right definition of privacy in the sense that it's kind of both the strongest notion of individual privacy that you can get without asking too much as in the other definition I gave. But it's also extremely useful in that, for example, all of machine learning and statistics, virtually all of machine learning and statistics, can be made differentially private. So like the original, you know, back propagation for neural networks in its original form, is not differentially private but it's easy to create a modification of it that adds noise tastefully at different places in the computation that will be differentially private. Fairness, on the other hand, there's... there are... we don't agree on definitions. In some sense, it's even worse than that. So some of you might have heard or know that in the last five years or so there have been a number of papers that kind of, you know, give arrows in possibility like theorems for fairness by which I mean, you know, these papers say like "well, you know, here are three properties that we would like any definition of fairness to have," right, and you kind of look at these three properties and say "well, yes, of course, these are very weak definitions. I would definitely want all three of these properties and probably other properties as well." And then of course the punchline of the paper is like 'well, you can... know, here's a proof that you cannot achieve those three properties simultaneously except in trivial uninteresting cases'. So we already know that not only will there have to be trade-offs between fairness and accuracy, which I'll talk a little bit more about, but there may even be trade-offs between different types of fairness. So it might really be the case that like, in a criminal sentencing application, if you want to minimize the disparity in false negative rates across racial groups, you... you're going to have more disparity between gender groups for instance. It might really be that like racial fairness and gender fairness are in tension with each other and you can't have kind of the best of both worlds. Okay, but first let me just kind of back up for a second and just illustrate, you know, how could it be that machine learning could be unfair in the first place without there needing to be any bad actors or, you know, evil data scientists. So let's suppose that, you know, we're asked let's say by the Penn admissions office to help them develop a predictive model for collegiate success based on high school application data. And to keep things simple, let's look at a two-dimensional version of the problem. So the x-axis represents the high school GPA of an applicant to Penn, and the y-axis indicates the SAT score of an applicant to Penn. And so we have a bunch of points here. And so their x-coordinates are their high school GPA, and their y-coordinates are their SAT score, and the pluses and minuses here what does that represent? Well, let's imagine that the data set we have is from historical applicants that we actually admitted to Penn and who came to Penn. And so we know whether they actually succeeded at Penn or not. And for success of Penn, pick whatever your favorite objective quanti... you know, quantitative definition is. It can be graduate within five years of matriculating with at least a 3.0 GPA, it can be donate at least ten million to Penn within the next two decades. As long as we can measure it in hindsight, it's, it's fine. And so what the pluses and minuses indicate is: in these historical admits to Penn, did they succeed or not. Okay, so if I showed you this cloud of points and I asked you to pick a good model on this training data, a  good predictive model for separating pluses for minus, you might very well pick this sort of simple linear threshold rule, right. And, you know, it's not perfect - there are some false... some false positives, some students who we would have admitted but didn't succeed, and there's some false rejections. These are students who, you know, did succeed that this model would have rejected, but it does a pretty good job overall. Okay, but now suppose I told you that the overall data set didn't consist of just this green population. There's a minority group also, and the minority group is the orange group and and so there's, you know, there's fewer of these points because they're the minority group and I also want you to notice two other things about this. One is that compared to the green cloud of points, the orange cloud of point seems to just have systematically lower SAT scores, okay, so there's this sort of a downward shift of the orange cloud compared to the green cloud. And perhaps I might invite you to imagine reasons for this. So maybe the orange population comes from a less wealthy demographic than the green, and so where the green population pays for SAT preparation courses and multiple retakes of the exam, the orange population can't afford that. They self study and they take the exam once and they get what they get, okay. But they are no less prepared for college, so if you counted carefully, you would have seen that the green population had slightly less than half pluses whereas the orange population has exactly half pluses. And if I asked you to build a model just for the orange population, there's actually a perfect linear threshold model which is this purple line. Okay, so if I now look at the combined population though and I say to you like, "well, you know, what model should you choose," if you're just going to try to minimize the overall error on this data set, you are unfortunately going to end up choosing exactly the same model that you did on the majority green population because if I try to shift this line down to sort of pick up these orange pluses, I will pick up way more green minuses than orange pluses and so my overall error rate would be much higher, okay. So this is just a simple example where going for the optimal model in terms of accuracy can result in unfairness and so what do I mean by unfairness here? Now we can give a quantitative definition. So the quantitative way in which this model is unfair is that even though it minimizes the overall training error, the false rejection rate on the orange population is close to a hundred percent and the false rejection rate on the green population is close to zero percent. So if I use as my measure of unfairness the disparity, the absolute value in the difference of those two false rejection rates, it's about as big as it could be by this model. Okay, so you might say like "well, this is a really stupid example because, you know, if I just look at this data I see what I should do. I should have like a two-part model. I should have a model that says like well, if you're from the green population I'm going to apply the blue model to you and if you're from the orange population I'll apply the purple and he--and now not only would I have a much more fair model I'd have a more accurate model as well, okay, at least on the training data it's a more complicated model". Of course the, you know, again the problem with this is that to the extent that our laws say anything about fairness in statistical modeling, they they explicitly forbid let's say the use of race or gender. And again, kind of like anonymity the hand wavy idea is: oh, if I don't allow your model to use races and inputs, how could you end up with racial bias in your model? Okay, this is the example, right, you know, this is the example where I -- I deliberately built a race blind model and done the most sensible thing, and in fact if the idea that was, by being race blind, I'm somehow protecting the minorities from being discriminated against, it's even worse. I've guaranteed that I'm going to discriminate against the very population I was trying to protect by asking for race blindness okay. So, you know, so this is an example and, you know, what's the fix for this kind of thing? Well, the fix is to sort of, instead of saying, you know, like even if I did want to build just a single model, what I should do is I should solve like a bi-criteria problem. I should solve a constrained optimization problem instead of an optimization problem. So rather than saying "find a model which minimizes the error, period," I should say "find the model that minimizes the error subject to the false positive despair-- the false rejection disparity between the two populations being, well, I could ask it to be zero, I could ask it to be at most 5%, I could ask it to be at most 10%," and, you know, how strong I make that constraint gives me a knob I can tune, right, which allows me to trade off the relative importance of fairness versus accuracy. And if you do that, and you can do that on real data sets, you can actually trace out what we might call the Pareto frontier of accuracy versus fairness. So here on three different real data sets in which fairness is a concern, I'm showing you, you know, error, sort of training set error on the x-axis and unfairness on the y-axis, where the measure of unfairness is something like the disparity in false rejection rates. And of course we wish we could be at the origin. We wish we could have zero error and zero unfairness. Of course in machine learning on real problems, even it without fairness, you're never going to get to zero error, right. But you see that there's a non-trivial trade-off in all of these curves. I can-- sort of leave-- I can get the smallest error with the highest unfairness. I can get to zero unfairness with much higher error, and in between, I can get in between. And one of the things we argue in the book is that we think that, kind of, plots like this are a good interface between, you know, people in more technical communities and people that are, like, thinking about the regulatory or policy issues around things like fairness. And same with differential privacy: you can, like, for something like randomize response, you can literally plot out the trade-off between how much privacy you provide and how much accuracy you'll get from your resulting estimated statistics for something like randomized response. And so, you know, this is a good interface between, like, the technical community and and stakeholders, if you like, for lack of a better term, in that if the stakeholders can kind of understand what they're showing here like I-- I shouldn't-- I shouldn't tell a stakeholder what the right trade-off is between accuracy and predictions of criminal recidivism and unfairness in sentencing guidelines, right. That should be done by somebody who, you know, knows and understands that world much better than I do. But I can show them this plot and show them the choice-- the menu of choices they have to make between error and unfairness. Okay, in the time we have left, let me quickly say what I have described covers about the first half of the book and as a teaser, especially, I think, to kind of the IDSS/LIDS audience, I will tell you may even find the second half of the book more entertaining, because in the second half of the book we talk about situations where, you know, it's not so much an algorithm operating on people and making decisions about them, but it's more like an app that's interacting with a population of users in order to, you know, kind of optimize some service for their preferences. So think like Google Maps or Waze, right. So Google Maps or Waze is, you know, taking all of our geolocation data, but then it's offering you, like, essentially a best response computation, right. Basically it says "based on what everybody-- everybody else is driving right now, here is your selfish best response. Here's your shortest route, your lowest latency route". And so you can really think about such apps and there's recommender systems or your-- or the algorithm that filters content in your newsfeed. These are really apps that are nudging us towards some kind of selfish Nash equilibrium some large possibly very complicated multiplayer game. And we could again ask whether we like the overall social consequences of that. So many people on this call probably know that it is not the case that selfish routing results in the lowest possible driving time collectively overall, right. So that, you know, there's no reason that in general any of us should be happy about what happens at a Nash equilibrium. And so in the second half of the book, we kind of take these ideas seriously from kind of an algorithm design principle and talk about how you would, kind of, incorporate economic and game theoretic thinking into the design of better apps that are algorithms for things like, you know, navigation and recommender systems and content filtering in the light. Okay. But I did promise to try to say a little bit more about slightly more technical content. So I just want to tell you about some recent work that we and many others have been doing trying to kind of interpolate the extreme between what I'm gonna call group fairness and individual fairness. I mean I'm gonna go quickly here and not say a lot of stuff that's on the slide so just to give you a flavor of this work. So if you go out and survey the fairness literature, the algorithmic machine learning fairness literature, you'll find that the vast majority of the definitions have adopted-- have the flavor of the one that I gave before, which is: you have, you know, as the user of the definition you first need to identify what group or groups or attributes you're trying to protect. So maybe you're worried about gender equality or racial equality, and so you pick what-- what variable or group to protect. You also, sort of, say what constitutes harm, right. So in criminal sentencing, maybe false-- you know, falsely predicting that somebody is a positive case of recidivism is the harm, whereas in lending falsely rejecting them is the harm, okay. And so then the definition of fairness basically says that, you know, if I look at the confusion matrix of my model on different subgroups, that I equalize some statistic about the confusion matrix across the different groups like the false rejection rate on black people and white people should be approximately the same, okay. So, these types of definitions are natural. They're convenient and they're practical. But if you think about them, they're-- they're sort of very weak guarantee-- they're not giving you in the individual any guarantee, so like, you know, literally the guarantee is like "oh, you know, if you're a black person that got falsely rejected for a loan, you're a consolation is the knowledge that I'm falsely rejecting white people for loans at an equal rate", right, which might not be very comforting to you, right. So, at the other extreme it would be nice to have definitions of fairness that really bind at the individual level, that really give you a promise, as an individual, about how you're going to be treated fairly. And I'm not going to, like, go into those definitions other than to say that they're interesting but they tend to be impractical, because they're asking too much and so in particular in the machine learning world, if you're going to meet these definitions of individual fairness, you really need very strong assumptions like what we would call 'realizability' meaning that whatever model you're fitting to the data, you can really perfectly fit the data within your model class, right. Often this is not a realistic assumption and so that's greatly limited the practicality of [inaudible]. Okay, so what we and others have been exploring is kind of a-- a linear or a convex optimization-- constrained optimization framework or designing fair learning algorithms and so I'm gonna, sort of, describe the framework at a high level, which I think most people on this call will understand, and then I'm gonna give you sort of an example of some applications to its-- to the problem of designing fairness, fair algorithms, that do something in between these very crude group notions and these sort of too strong individual notions. So the high-level idea is: we're going to express our training problem as a constrained optimization problem. So, in other words, minimize error subject to some fairness constraints, okay. And in the interesting cases, when I sort of linearized my problem right, so literally I'm gonna sort of introduce a variable for let's say every possible neural network that I might choose, right, so that's all I'm going to linearize the problem, and so I'm gonna get, you know, like a constrained optimization problem that I don't even want to explicitly write down because like the-- the pure strategy space, right, is sort of exponentially or infinitely large, and also the dual player or fairness constraints might be very large in numbers in the applications we have in mind. So we want to avoid explicitly enumerating these models or constraints, but we still want to sort of have a practical way of solving. So, you know, what we're gonna use is duality to sort of pass the Lagrangian and, you know, use standard tools to recast this constrained optimization as a two-player zero-sum game. The primal player is like a learner who wants to minimize the error subject to constraints so far, and what we can think of as the dual player-- we can think of them as a regulator, okay. So at a high level, the way this two-player game is gonna work is that in round one, the learner says like "to hell with it, I'm just picking the neural network which minimizes the error". Then the regulator says "no, no, no, no, that model treats this group unfairly. You've gotta fix it". The learner adds that constraint the Lagrangian like resolves it and says like "okay I found the model which, you know, minimizes the error subject to the constraint that the regulator identified in round one" and this game goes back and forth, okay. And so, you know, if you can get this to converge, right, then like the solution to this two-player-- the Nash equilibrium of this two-player game by classical theory, is the solution to your original constrained optimization problem. Okay. So now thing-- now we get to the interesting part of this which is like "well, how can you say something about this algorithmically?" And it turns out that if you can formulate the best responses of both players as kind of instances of vanilla cost-sensitive classification ignoring fairness, so there's a reduction step going on here where we're basically saying like "well, let's rewrite the problem of finding a model that's fair subject to constraints as just like a normal instance of non- fair machine learning", so if you can do that, and you can figure out how to implement at least one of the two players as a no-regret algorithm rather than just best response, right, so like I mean just to be clear here for the kind of aficionados on the in the audience: already from, like, this LP formulation, we could just do something like fictitious play, okay. But we wouldn't have any guarantees about convergence, right, because we don't, you know, we don't-- that's an open problem, you know, one of the main open problems in sort of repeated game theory. But if we can go the further step of sort of figuring out how to implement one of the players as a no-regret, then the algorithm provably converges efficiently, given access to a standard learning heuristic. And so, you know, there's interesting theory behind this, but maybe the more exciting part of this is that this kind of program gives a recipe for actually implementing such algorithms and trying them on real data sets. And so one area in which we've done this is in the area of what we call fairness gerrymandering, where-- what do I mean by fairness gerrymandering - forget this diagram I'll just say it in English - you know, even if I use kind of standard methodology to find a model, for instance, which is fair with respect to let's say race and gender and age and disability and income, you know, even if I enforce all of those constraints marginally, there is no reason to expect that my model won't discriminate against, for instance, disabled Hispanic women over age 55 making less than $50,000 annually. Because just because I enforced all of those marginal constraints, doesn't mean that I've enforced all combinations or subgroups of those constraints. So, you know, this framework that I just described is ideal for this setting there's actually how we arrived at this more general framework in the first point-- in the first place, because of course when you start, you know, contemplating having many many protected features or groups, and you want to simultaneously protect against all combi... you know, want to protect all combinations or sort of combinatorial subgroups of them, things quickly blow up and you need something like the methodology described on the last slide. And I'll just, you know, stop by sort of saying it seems to work on real data sets, right, you really, you know, you could not only theoretically analyze this and implement the algorithm, but you might also wonder "well, maybe you're just asking for too many constraints". But depending on the data set you can get rather nice-looking Pareto curves where you can have like a reasonable balance between predictive accuracy and sort of enforcing an exponentially large or infinite number of constraints. And so let me stop there and take time for questions for anybody who has any and I'm happy to answer them. Thank you. [Ali Jadbabaie] Thank you very much, Michael. Let's unmute and clap. (Clapping). [Michael Kearns] Thank you. [Ali Jadbabaie] This was wonderful. If you have a question, you may raise your hand-- your zoom hand and then unmute yourself so I can call it. Questions? Okay.  Constantine, please go ahead [Constantine] What were the struggles you were facing trying to [inaudible] technical and general audience. [Michael Kearns] Sorry, can you repeat it and maybe talk a little bit louder? [silence] [Ali Jadbabaie] Constantine, can you repeat the question? [Constantine] Yes, you should hear me better now. [Michael Kearns] Much better! [Constantine] So for you, what we your difficulties trying to explain this to both a technical and a general audience? [Michael Kearns] Yeah, that's a good question. I mean, so first of all, I mean, you know, when we give talks we sort of tune the level at which we give them to our perceptions about the audience. I think in general, and I do think, you know, if you read the book you'll see that there's a needle we're trying to thread a little bit. Like we, we didn't want to, you know, kind of water... you know, we didn't... we wanted to... we wanted people to go away with some high-level sense of what the underlying methods were and how it was possible to do this sort of stuff even though we were trying to communicate with sort of a very wide audience. So I think the book ends up, you know... As I jokingly but seriously say, you know, it's definitely not like a Hamptons beach read, right. And in some, sense our core audience is, I think, kind of the circle just beyond the machine learning technical community right, which is a very large... I mean the the sort of-- it's an amusing thing to somebody like me who's been in the field my whole career that now there's this entire layer of people that are, you know, have careers in machine learning but are not technical people themselves, right, they're managers or project managers or marketing people even and and it's sort of that audience we're trying to reach as well as regulators and the like. So I think that audience we've reached well and, largely speaking, I think a lot of the talks we've been given have been to audiences that primarily consist of not true lay people, but people in that, you know, neck circle or the one beyond it. You didn't directly ask about this but one thing... one thing that surprised me most in just sort of going around and giving talks about this to non-technical audiences was how differently people view the topics of privacy and fairness in the following sense: you can talk to people about privacy and differential privacy and anonymity and kind of avoid controversy in the sense that everybody sort of agrees that if it were possible to do large-scale data analysis and machine learning while providing everybody's super strong privacy guarantees, that would be great. There's sort of no... nobody kind of argues with that. As soon as you start talking about fairness, you know, with a non-technical audience, immediately it becomes political, right. And people sort of, you know, say like "well, you know, why why should we be protecting this or that group?" And "why should we do that if it comes at the expense of know releasing more criminals, right, or losing more lives" and, you know, and "why are we doing this anyway?" and, you know, "are we doing this because we think it'll make those people's lives better going forward, or are we doing it to redress a past wrong?" and immediately you, kind of, find yourself, you know, engaged with people that want to talk about the politics of it and that's fine, right, but you immediately kind of get pulled into things like affirmative action. And so even though both privacy and fairness these days have political overtones to their discussions in more general audiences, it comes up much more often and much earlier when you start talking about fairness than when you talk about privacy. [Ali Jadbabaie] The second question is from Munther (Dahleh) Munther, you can unmute yourself if you're still here. [Munther Dahleh] Yes, I'm here. Hi Michael. [Michael Kearns] Hi Munther, how are you doing? [Munther Dahleh] I'm good, thanks. Yeah, so we can spend a lot of time-- there's a lot of philosophical and really deep stuff to discuss, but I have one question. I wanted to ask about fairness, right. So when you think of this constrained optimization problem that you set up, you still go back and use these features because you have to write the constraints per community or per... you know, right. And so, and so then why didn't I just simply have the two model solution now that I'm actually using race or gender as a feature? Why don't I say "okay, for men I use this feature and for women I use that feature" instead of ending up with a trade-off that maybe not good for neither or at least a compromise. [Michael Kearns] Yeah, so a couple of reasons. One kind of legal/policy, the other technical. So I think if you are in a setting where the number of groups that you're trying to protect is sufficiently small, right, you could entertain building a separate model for each different racial group or one for each gender, right. Except insofar as the law forbids that still, right. [Munther Dahleh] Right. [Michael Kearns] So you know, it remains the case that in consumer lending you just cannot do that right. [Munther Dahleh] But when I add that as a constraint, I implicitly have used that disc... it's not a discrimination, but it's a compromise based on that. So I guess the law allows that. [Michael Kearns] But, you know, like, if you mention it in your constraints but your model isn't allowed to use it, right, that's still gonna handicap your model at decision time, right. [Munther Dahleh] Right. [Michael Kearns] Like in the example I gave, you really need to know whether they're green or orange to decide which model to apply and that can't be like enforced in training time. You need to know for the specific applicant which group they're in, okay. So to the extent that we... the areas we still have these laws until they get like rewritten-- by the way, another thing that's really nonsensical about those laws, you know, it's a two-fold reason: one is for the reason that I showed and of course, the other one which everybody will immediately realize is that, you know, there are so many proxies for a race, right, that you're fooling yourself if you think by, you know... and maybe, like, I have some sympathy for the fact that maybe back in the 1970s when the first credit scoring models were being-- were being built using neural networks, that maybe, you know, the data you had on consumer finance was so limited that, you know, by removing race as an input you really were somehow not letting the model figure out race. But these days, you know, there's so much data that goes into these models that you're fooling yourself, right, because of the correlations. But the more technical reason, you know, is that if you take this fairness gerrymandering example where I want to not just protect marginally along the attributes but on all combinations of them, I definitely don't want to be building a model separately for every one of those combinations because just the computational overhead of that, it would kind of obviate this, this kind of duality approach that I was describing to begin with. So there  you really you want-- you want to do something that's implicit, you want to sort of, you know, enforce all of the constraints while only needing ever to touch a fraction of them and that's what this framework is designed to do. [Munther Dahleh] I had another quick question, which is slightly different than what you discussed in the privacy issue. But there's one aspect of privacy that's becoming more of a market issue and there's my data on the markets and can I either sell my data or actually buy the right to prevent you from using my data. And this has been a sort of an ongoing... I don't know if you have some comments about that. [Michael Kearns] Yeah, so my main comment is that's a great research topic. There's been a little bit of thinking about that and including some by ourselves which kind of didn't go very far but maybe we'll pick it up at some point, but it's a great area I think and and I... maybe the one slightly more detailed comment I would make is that I do think differential privacy is one way of, like, starting to think about pricing, okay. Because if you like look at the details of the definition of differential privacy, right, there is sort of an amount of privacy that you're getting, and you can even like provide different levels of privacy to different people, and you can also think about how important somebody's data is to a particular computation. So in the underlying definition of differential privacy, you have this privacy parameter which sort of says how much privacy everybody or each individual is getting and that's related also to the sensitivity of the thing that you're trying to compute with respect to an individual. And so this lets you start talking a little bit quantitatively about things like the following: if I'm trying to build a predictive model for a rare disease, right, I need a data set that includes people both without the disease, the negative examples, and people with the disease, positive examples. But the people with the disease, because it's a rare disease, like, their data is much more-- you know, I could... I can easily find people without the disease, but the people with the disease, they're much more rare and more valuable to my computation. So somehow they should be paid more, right, than the people that don't have the disease. On the other hand, right, there there's sort of more risk because, you know, if I just learned that your data set-- your medical record was used in you know-- if like I'm gonna build a balanced data set, fifty percent of the people have the disease fifty percent don't, then just conditioned on knowing that your data was in this data set greatly increases my posterior belief that you have the disease, right. So it's complicated because different people should be paid different amounts for the data but even the people without the disease that are in the data set, they're suffering, sort of, some, like, negative inference just from being in the data set and I do think differential privacy gives a starting point for sort of talking about what I think are very interesting and tricky issues in, kind of, creating markets for data and realizing different people's value as different you know, different people's data has different value depending on what you're trying to compute. [Munther Dahleh] Thank you. [Ali Jadbabaie] Thank you very much. Any other quick questions or comments before we conclude? Okay, looks like we don't have any further questions. Thank you so much for doing this and thank you so much for doing this remotely. [Michael Kearns] Yeah. [Ali Jadbabaie] I would have loved to have you in person so we could now go and have a drink and dinner [Michael Kearns] Yeah I know. Next time. But at least I was... I know at least, if this is going to be going on for a while, I will have been the inaugural virtual speaker. [Ali Jadbabaie] You were the inaugural virtual speaker! [Michael Kearns] Thanks everybody. 