 hi everyone at spark summit welcome to a session by world quant predictive we'll be talking about how we use data breaks and Muta in our quest to create one of the world's largest libraries of predictive models so what are we going to be talking about today well I'll talk a little bit about world quant predictive just so you understand the context with which we're coming from it as well as challenges we face in scale in quantitative research on sensitive data and then we'll talk about what our platform special the data portion of it looks like and how we're using data breaks in the Mutai to solve the challenges in delivering large amounts of data safely to diverse and distributed group of researchers well let's begin who are we we're actually a start-up by men named eager to schinsky who started world quant asset management very successful quantitative hedge fund and we are a group of people tasked with bringing the techniques and ideas that this quantitative hedge fund honed to industries outside of finance in short we frame business problems as prediction problems and find ways of making the making the predictions we use a large set of tools for it and data breaks in the Mutai are two of our best ultimately what we want to do is make every decision the business makes back-tested analytical decision insofar as that's possible how do we do that well our main idea is that every person no matter how smart and good their only have a couple of ways of solving any kind of specific problem so with 10 out and built a large group of global researchers some of them here in the fit States some of them all over the world to create hypotheses for solving our clients problems and building up our model library we call it an idea arbitrage principle meaning that there's lots of ideas and if we have a lot of them will actually find a large number of really really good ones to make this all digestible we ensemble this ideas that people have submitted to us and then through automated testing pick the best ones and have a predictive product and we then build the appropriate way of interacting with it from API is to extracts to custom tools for scenario planning simulation and prediction then goal for us is to improve business decisions through predictive modeling and increases return on investment for our customers okay so what are some of the challenges for predictive modeling at scale let's talk about that so we said their context correctly well lots lots of different challenges but four of the main ones are M elapsed I'm sure you've heard this phrase now sometimes you'll hear a IAB's but ultimately what is it that we have to do to get lots of hypotheses become models that means running lots of tests versioning dependencies refreshes retraining is all sorts of things we'll talk about that very briefly just to set the stage and what else do you need well you need a lot of hypotheses right hypothesis is really what drives creation of predictive models I wonder if that's a hypothesis we'll talk about that briefly as well but the bulk of our time here today will be spent on data right data is literally the fuel that fuels both hypotheses and testing as well as privacy and governance how do we actually make sure that data that our researchers see is what they should be seeing what is helpful to them and what is allowed by our various contracts with product vendors customers public agencies and the like here's an example of actually a couple projects were working on not surprisingly lots of people are working on helping organizations and businesses do better with Goddard actually some of the things we work on can be as different as working with the public agency on helping understand which of their customers users taxpayers are going to be using their services working with commercial entities to understand when they can be reopening their stores or better stacking their products and what is it that we need to do here right on this level we have to first of all we need to work with data right so will you look scour the world as it were for different data sources that we provide access to we'll be talking about that a lot more later to our global research network researchers as I mentioned all over the world and we deliberately tried to focus on picking people who come from diverse backgrounds where you grow up where you go to school what your special two major PhD was in all provides you with the unique life and analytical experience and toolkit and all of those is what we want to do to use to generate interesting hypotheses for our clients one of the most obvious things when predicting college effects is for example to look at a lot of public data and there's a ton shown some examples later as well as existing epidemiological models as well as published covered models so you'll see where we talk about as they are that one of the things we had to do is build tools for actually ingesting existing models or outputs from existing models as some of the things that we're working with if we have the rights to do that we look at different demographic information various macro and micro KPIs from the government agencies and other information often come in directly from our clients or bought from our commercial partners Oh this goes into a sort of unholy melting pot different researchers may have specialties in different types of machine learning statistics that assigns often their subject matter experts and things like epidemiology or inventory management we try to throw as many different approaches at problems as possible sometimes some approaches crossover really well from one type of Industry to another sometimes they require specialized approach but overall what we are really looking for is how many strong uncorrelated hypotheses we can generate and we'll talk about how we make sure of that next slide and then once we find some strong candidates when sampled them together and as one of the previous I said oh ah you have a strong resilient model resilient meaning something that will withstand the shocks of losses of data change large changes in historical record going forward which is something that lots of people are experiencing right now you may be in a in a business where historical data predicted very very well what's gonna happen tomorrow then a pandemic happens admittedly somewhat unprecedented but suddenly your historical data is no guide to you at all part of what we strive to do is to build models that are resilient and in fact build and identify different drivers for common business questions that we can add to models many of our customers already have that are based on their historical records let's see a little bit more in detail what what that happens so if I were sort of to double-click on how do we get to a good resilient model you will notice two things one it starts with data again that's what I will be talking about data and privates and governance for premonition the rest of the conversation today our researchers need to access it that's kind of the point right once they do build create hypotheses and build some models we have this models run a gauntlet some of you may have similar set up some of you may still be working on it but some of the things that we need to make sure of isn't simply the performance on the key metric that you know accuracy or recall right we also want to see how their hypotheses and models behave in cases when you know you have malformed input or extreme values and then finally we want to compare them to each other so that we don't have a lot of correlated models and we have models that change independently of each other maybe it's because they use different data sources of different assumptions and that really is what creates a resilient model and then of course we have to keep doing this as the data updates that's essentially how we do ml or AI ops right the main thing is you'll see parents half of the conversation is about data and researchers and how researchers get access to data and specifically to the right ones so let's move to that let's see what our platform is made out of I used to call this slide comfortable familiar personally different why well you know pretty much everybody has a platform that logically looks like this there is data somewhere it comes into landing zones it gets processed you know maybe the alpha and meta alpha which is our words for models and ensembles are a little bit different than what you're used to and then eventually their models have to make it to production and operations owns the two things I'm going to point out that made it a little bit harder for us at scale is that even for not overly onerous problems we may end up with hundreds or thousands of models and dozens or sometimes hundreds of data sources so the 2kpi is that from engineering perspective we are tracking and real aspire to our speed to prediction and cost for prediction speed to prediction is important to us because that's really what we're trying to delivered to our clients right how fast can we go from asking a business question or formulating it to having a decent model right that's what a lot of our custom tooling is designed to facilitate and that's where a lot of our purchases like data breaks and a mutant are designed to facilitate cost per prediction is really all of the resources human and machine that we have to use to get to this prediction and that's again where data breach specifically was one our choice and I'll talk about that in a minute for those of you who have gone through data breaks PLC your problem notes is that they're very very big on asking the clients to figure out what they are all is going to be so they do not feel like they were cheated later so what we see here is data break specifically plays a pretty big role in our platform right that's the green squares that you see you see them all over the place sometimes it's because there are repeated components in different parts of our environment but the main ones we're going to be talking about today is the square sort of on their bottom left right where we talk about data packaging and data exploration as well as data validation right above it now we use data cattle will use a few different data catalogues and I'll say that for data exploration we actually really like the data bricks one for some of our other purposes we'll use a mutant some custom custom tooling so you'll notice that we have a lot of data bricks pieces have few custom pieces they're ml ops ones that have already shared with you and airflow in yellow we'll be talking about how all this fits together towards the end of the presentation as we talk about specific example for how do we let researchers actually add data safely and securely to our platform and end to themselves but you can see that data breaks is something we've chosen to enable better cost and better speed to insight for our researchers so hopefully it's the same for you let's talk about data for a second here are some of the buckets not even the data sources for a typical type of a cupboard nineteen prediction right you have to collect a lot of data to be able to answer almost any interesting question right so each one of these buckets probably has 10 to 50 data sources for us so that's a pretty sizable thing and not going to spend a lot of time on this slide but the idea is that you know you very quickly get to meaningful data catalog where anything and everything has to be able to be joined together and quickly made available to researchers and then if we look at just covered 19 spread you can again quickly get to 10 20 30 catalogs and many of them have multiple tables and it's the same thing right what's interesting here is that even though not as all of the data sets are large some of them are and as soon as you have a large catalog that's where spark comes in right it's very difficult to join you know small you know a few tens or hundreds of thousands of rows of data to something that has billions of rows of data like you know just hard to do in a traditional environment so for us speed is very important so the flexibility of data breaks was a really big deal talk about that in a minute but this ability to quickly join across types of data regardless of where data is was also really really big deal so how do we reduce this limits that we've been talking about we've talked about ml ops that's our blue stuff you've seen a little bit of us a little bit more in a second hypothesis we have a tool called quanto that our researchers can use as well as all of their standard data science and machine learning tools right ultimately hypotheses are generated by people so the more people we have and the better to win they have the more hypothesis we'll get for data essential we're using data bricks with an object storage back-end and for privacy we use in Utah right so if we look back at the original slide and now here we see that we pretty much have a tool for everything and you know from our perspective the fewer tools we have to develop the better off we are so I just want to take like 30 seconds to talk about the custom tooling our challenge is that we have almost almost 100 researchers now and growing fast so and we need them to work on multiple problems working them together we've looked at a lot of tooling and found that it was most cost efficient for us to build some of our own tooling on top of a lot of open-source things for example the way we provide feedback to our researchers is to give them letter grades as opposed to actual scores because we don't want them to overfit we manage playbooks which is our scripts there's lots of things if your data science tea you need to look at the requirements for your own data science team to see if that's worthwhile data breaks of course comes with ml flow which is a very very good solution for helping researchers go through this ml ops AI ops lifecycle without developing anything custom so I just wanted to mention that we have our own requirements that make it difficult to use a map flow and other frameworks so that's right now but your mileage will vary and unless you're dealing with dozens or hundreds of researchers working on the project it's probably a really good idea to look at ml flow and similar tools so how did we even end up with data breaks some of many of you are probably already data breaks and SPARC customers but for those of you who are still thinking about it let me share a story on the right of the slide you'll see a bunch of tables and this is a fraction of the different requirements that we identified and each column is a different vendor and we've have few columns that didn't fit in here so we looked at a lot of different things the main things we wanted to look at was how can we allow researchers to add their own data sources right we want them to generate hypotheses we want them to be able to say hmm I just I just realized that many of the airports United States publish their wait times how long takes to get to the checkpoints online hey I could get this data source and maybe it tells me something about macro mobility in the United States during the pandemic like we have pretty good tooling for adding data sources but it's nice to be able to do this yourself right we want to see how researchers can create combined data sources and query them can they do all of this quickly and what is this going to cost us right what is it gonna cost us when nothing is happening so we evaluated a lot of different tools and really felt that data bricks was the right on for us for one we really like spark we like the idea that we can mix explore exploration and operational we liked this scale we like the ease with which data sources can be added and managed and virtualized and then when we looked at that and we looked at a few different vendors you know we felt like data bricks was a really good partner for us and I will also add I'm sure their database sales team won't love it but they are suckers for helping people figure out how to use data breaks when you get started so if you get hurt right sales team they will really help you get pretty far alone in your process if you aren't already and some of the decision points that we wanted to to think about was look for us more hypothesis that we explore means better predictive product so the tool were looking for is something that's going to allow our researchers to either generate or test as many hypotheses as possible in the course of a day or a week right for you you're gonna have to think about things like how much am i doing aberrational inspark versus exploratory what I mean by that is that lots of organizations use spark because their data volumes are just so large it's the only way to practically do what they need to do that's an operational use case you know if you're doing exploratory you may find like we do a lot of exploratory stuff right you wanna give researchers access to many data sources now so for us for example notebooks were kind of a big deal the ability to do things in Python our sequel was kind of a big deal for us right costs are different so you're gonna have to figure this out but that's some of the things that I would want people to consider think of what else you have to integrate it with we have the luxury of building things from the bottom up so we integrate with whatever we want so that that really helps us and we found that data breaks was really really strong and strongly managed deployment and it was very easy for us to integrate it with our existing tool are you looking to be repurposing existing dollars right a lot of tools are basically hey you're spending a ton of money on X we're alike X on steroids but at half the price you know in our case we were spending fresh dollars so we really wanted to make sure that we get additional value and then finally it's how big an advanced your team is this isn't so much a decision point from data bricks perspective but a lot of their nice use cases that we have to deal with are partly because we have a lot of researchers we run a lot of different types of clusters for different kinds of predictive problems all right so those are some of the decision points that you are going to be making and even if you have chosen data breaks or some other SPARC implementation those are decision points that you're going to be making over and over now again as organization changes as business changes as needs change so let's talk about client privacy challenges right here is five different balls that can have to keep in the air at all times we need to share data with lots of researchers all over the place lots of restrictions about it you know some are easier to deal with for example in our case we've just made a decision that all our researchers use virtual machines that are based in the United States and they can't download things so yeh but you're still sharing data with a lot of people and in our case it's it can be a lot of people so we want to know who it is and we need to know where they are right we want to generate tons of hypotheses from lots of data sources so we don't want to control access to data so tightly that people can't do that all right we need to track things because we have contractual obligations both to clients and vendors we license data from we kind of have to pick and choose a little bit whom we trust and how and of course dimensioned speed is important we need to get it done fast so how do we deal with that yay colors so all of this is really shaded right like I kind of pick primary colors for these things but realistically there is well a little bit of everything in them but for the purposes of this let's think of things about trust and access we'll work with a Muta and for things like data and speed we'll work with data breaks and of course and I'll be remiss if I didn't say it one of the biggest things for us when taking data bricks and muta was that they work well together right so for us the ability to deploy Muta with data breaks in a couple of days for a proof of concept was it was a pretty big selling point right so let's actually look more closely how data breaks in the Muta work for us data breach solves for for data for us right we when we actually worked with the sales and sales engineering team you know like many of you probably we had our document and it really came down things like understanding how query performance compares to other options out there right and what would we have to do how much work would we have to do to get comparable performance I've spent many years with working with relational databases and you know I'm sure many of you know somebody in your organization who can make any kind of you know my sequel oppose this query saying the question is how much effort is it and in our environment where we have a lot of data sources and a lot of different types of joints spending a lot of time on any particular query isn't ideal right and we certainly don't want researchers to do that that's more of an operational type of optimization so we needed something that would work really well out of the box and we found the data Briggs did really really well as special once we figured out how to do our cluster management right we wanted researchers one of my personal goals is that we want researchers to be able to submit models into our platform within the first day of being on boarded and the data bricks becomes part of that mix then they have to be able to figure out how to use data breaks in some basic way pretty quickly we found the data breaks dead on boards in really really well and support for multiple languages is a big big part of it I know a lot of it is spark stuff but you know you you know in our case we feel like we pay for management and tooling right on top of a really strong SPARC implementation security we use data breaks Enterprise combined with the mute ER and you know one of the things that we measure is what's the amount of time between a data store has been identified let's just make it a public data source and it'd been available to researchers right that's how long it takes for us to go through the cycle right and we want the tooling that allows us to do it fast and customization that allows us to do it even faster right I want to actually bring out an example of something that I think we did that was pretty cool that fits well into this example into what we've been talking about so far you know think of what it takes to support ad hoc data set ingestion right it's a use case that isn't particularly important operationally right most of you who care about it from patient's perspective would say but wait we know where all of our data is coming from you know we have this feeds this terabytes of data you know but it began if you're looking at saying I want two researchers to come up with fresh ideas and they're gonna constantly be playing around with different data sets we'll want to allow them to basically say hey I found something how do I add it great lots of ways to do this except and we'll talk about the except in a moment data bridge supports and your own data set but you know it feels a little bit tricky to just allow people to upload anything they wanting to into our environment right it's okay to upload it but we really want it to be available who doesn't become available to so the process we've come up with is actually to use air flow which we use for our construction for everything related to data and a favorite object or data store and we use data breaks notebooks and dashboards to actually create a light workflow that lets researchers add the data and for us they feel comfortable that this is safe let's look and in detail for it so here's what happens it starts out the way you would think researcher identifies the data set and what we do is we have special landing zones for researchers so they can just drop these data sets into the landing zone and it gets picked up by air flow and runs a dag right directed a cyclical graph that basically has a bunch of different things that we want done moves the data around but most importantly triggers notebook that checks this data for for Val we can't we have a few of these notebooks and we're constantly collaborating on them you can see a very simplistic example on the top right of this slide but basically we run through a bunch of statistics commands figure out you know basic descriptive statistics about the this particular data set we can look at how many cells are filled and so on and so forth and then in fact using dashboard we can actually fill out a little bit of a command it's kind of hard to see here on the top top of the dashboard screen but ultimately we can show all our do researches to say yeah yeah that's the that's the what I thought the data set had you know go ahead put it into something that's raw and trusted those are our two holders for for data okay sounds good so far maybe a little bit too much work for simply copying and settled files from one bucket to another but here's the cool part what we can do is actually create automatically create the data sources and data breaks so this becomes available to other people we can set it up scoped by by project so that it's available told that the researcher is on the project and mapped to the right database cluster and then finally a I thought was pretty cool we can use immutable a little bit more in detail and in a second to create a protected data source which will actually check the data source for things like Pai pH I and other type you know any kind of information we don't want researchers to see and more for more let's go to the next slide what does him you to do well in music allows us to create policies and essentially personas that context that allow us to understand how the data's been accessed in muted tracks all of the data access creates an audit trail for us and provides access to researchers using standard ODBC JDBC style set up so far so good but some of the two features that I really really really like and I have examples of them on the bottom below the text and the upper right corner as well one of them is called context what the muta lets us do is to say sure your user authenticated but how are they actually using the data so by creating this context for example when I connect and in the directive manner I can have the context of an explorer whereas when I am running our server-side tests on my persona is that of a model testing environment what data is made available through the exactly the same queries it's completely transparent to researchers can be different we can do things like limit the number of rows or how many days worth of data to hold back you know we can we can do that we can create a policy that basically says if I access this data source as a researcher then I just don't get to see the last 90 days of data no matter what they do it doesn't matter what kind of spotty else part or database query right it's done you don't get to see the last 90 days of data and that is very very powerful right it it's a both from an audit perspective and comfort perspective and frankly not making stupid mistakes when you're trying to fit your data it's very very nice another thing that is really really powerful is differential privacy trench whole privacy is a mathematical technique that basically jiggers I know there's a better word for it creates a certain amount of randomness in the data that does not affect its statistical properties but actually in effect anonymizing it changes the data so immutable example supports policies that says when I'm accessing the data in an exploratory State only serve it out using differential privacy so that what the researchers see isn't the actual data right it's close enough for the that they have right it's gonna have roughly the right number of Mondays for example and Tuesdays but it's not actually the data that is in a database it's essentially an on a fly synthetic data set that is statistically different but protects the privacy emitter has many other properties such as masking rows masking data but these two combined with our ability to apply them to commands means that we can put essentially Optus Street data source into data breaks with a muta and feel comfortable that what researchers are seeing is not good include any PII khi or any other harmful information oh and by the way when we're dealing with sensitive client data we can actually obfuscate it in a way that makes no difference to the researchers looking through billions of rows of data because it's statistically correct but actually isn't showing any client data to any of the researchers you know in in case it gets lost or anything like that obviously we still work with clients on protecting this data and following all of the guidelines but this is a really really nice set of features to feel us and our customers more comfortable with how we can protect their privacy in the privacy of their people that they represent and still do cutting-edge machine learning and data science on on their data so to summarize data breaks gives gives a scale and speed mu-2 gives us privacy data breaks and muta together is you know a good chunk of what we offer to our research thing to work with thank you thank you for joining us and I hope you learn something during this presentation thank you you 