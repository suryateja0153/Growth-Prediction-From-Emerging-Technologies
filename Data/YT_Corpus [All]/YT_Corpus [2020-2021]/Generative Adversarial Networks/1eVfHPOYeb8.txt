 welcome back to the tutorial on an introduction to machine learning in the previous lesson we introduced you to machine learning and now it's time to look at the mathematical foundations so some questions you might have coming into this include what is the common terminology in machine learning how does machine learning actually optimize and is there anything i should be careful of so we're going to address those questions by discussing the role of data models and loss functions the role of gradient descent when optimizing and alerts you to the dangers of overfitting here are some definitions for data model and loss function i'm going to take you through this aside on loss function and likelihood first please pause the video and read this part on the loss function once you've done that we can get here this is exploring the relationship between loss function and likelihood likelihood measures the goodness of fit of a statistical model to a data sample log likelihood is often what's used here's the formula for log likelihood ln l of theta equals a constant minus the loss function this fancy l is the likelihood function since we subtract the loss function in the formula for log likelihood minimizing the loss function is equivalent to maximizing the likelihood if we take a rate if we subtract a really small number here then that will leave a big number here it's important to note that once the data and model are specified then the loss function depends on the parameters of the model for example for the data points in the plot above and the mse loss function the left-hand plot the model is y equals ax squared plus bx plus c so the mse depends on a b and c and the loss function is a function of a b and c for the right hand side the model is y equals a e to the b x so in this case the mse depends on only two parameters a and b now gradient descent is often how a machine learning model optimizes this is the procedure for gradient descent and some key definitions are here in this model with one parameter it will start with some random initial value then steps one and two are repeated there's a learning step and the aim is to get to the minimum of this curve hence gradient descent and each step brings you closer and closer to the minimum for a model with two parameters i you can visualize it as a two-dimensional space might start with some random initial value up here somewhere on the mountain and then each learning step will take closer to the minimum until the minimum at the bottom here is reached in this trench there is something you should be careful of though it's called overfitting in general a model with more parameters can fit a more complicated data set let's have a look at the plot above a model with too few parameters here only has one parameter can't sufficiently describe the variations in the data this is called underfitting model with too many parameters on the right 15 parameters in this case will learn the statistical fluctuations in the data and doesn't describe the general trend very well general trend is is here it's shown by the true function but the model in this case is really following the data points exactly but those could just be statistical fluctuations what you're aiming for is the goldilocks number of parameters in the middle where the model is describing general trend in the data well in general you want to train the model on some known data points with x i y i then use that model to make predictions on new data points but if the model is overfitting then the predictions for the new data points will be inaccurate the main way to avoid this is to use a training and validation set which we'll look at some more later now getting back to the three main tasks in machine learning regression classification and generation we can now revisit them with the mathematical formulation the key points at the end of this lesson include that in a particular machine learning problem you need an adequate data set reasonable model and the corresponding loss function the choice of model and loss function needs to depend on the data set second key point is that gradient descent is a procedure used to optimize a loss function corresponding to a specific model and data set third key point is that you need to be aware of overfitting that's it for this lesson on the mathematical foundations for machine learning in the next lesson we're going to discuss neural networks see you then 