 carlos sponsored received his md phd from harvard uh and was then a postdoc with marge livingston and gabriel crimean here the center for brains minds and machines he's currently an assistant professor of neuroscience at the washington university school of medicine in st louis his lab is interested in how different brain regions interact to solve motion processing and visual object recognition and he asks this question using a combination of electrophysiology causes perturbation techniques as well as computational modeling his talk is titled as simple as possible but not simpler uh features of the neural code for visual recognition uh carlos all right well thank you very much for the invitation this is i have to say one of the funnest uh workshops i've attended the poster sessions are great i didn't realize how much i would like have been able to zoom into not having to reach over somebody's shoulder to look at the poster so thanks for putting this together okay so my talk today is about uh visual processing in in the primary brain specifically focusing on the macaque monkey and in object recognition we study them a cac because their brains are so similar to ours and because we can access them using electrophysiology so the main problem that i think the uh the macaques brain and in fact any visual system uh processing uh device has to encounter is the idea that the world has too much information but not all of it really is that useful so for example if you were to cross the street you might be interested in learning where the identity of the people on the street are or the location of moving vehicles and you may be less interested about incidental features like the color of a t-shirt or the particular shape that a tire makes against the asphalt so the question that we're interested in is what information are neurons in the visual cortex part parsing and representing from scenes when i think about this question i always go back to this paper by fred acne where he laid out this really useful concept and this is a paper in 1954 called some informational aspects of visual perception which had been directly inspired by claude shannon's information theory which had just been uh published a few years earlier and in this paper fred abney invited us to consider the problem of redundancy in visual images so here he presented an image which he called an ink bottle in the corner of a desk and what he said is that even though this image is comprises 4 000 pixels he said that it would be possible to reconstruct this image from a blank grid page just by guessing the individual color in one of the cells and he one could do that with as few as 20 errors and the way that it could be done is simply by guessing the color in each cell and then making some assumptions about continuity and symmetry in the image i don't know if that's actually possible but he did suggest that he that you could you could compress the image quite a bit uh this is what the useful concept that i was talking about the idea that uh these images do have uh parts of it where information is concentrated such as contours and he laid out this idea you can take something as complicated as a cat sleeping and basically convey that using simple 38 points of maximum curvature so this raised the question that the time this results were all based on perception but it raised the question that if the visual world is indeed redundant then it can be compressed and this is before actual neural neural data was was available but as it turns out just a few years later heavily and weasel did their fundamental studies demonstrating that in posterior visual cortex particularly v1 and v2 there were neurons that individual preferences represented straight and curved contours and by preferences were mean something that they're meeting more spikes and so they identified orientation tuning and end stopping in visual cortex these kinds of cells that they found actually have been found in a bunch of other species from the cat to the macaque and even they occur in neural network models that are trained to be sparse and efficient and of course in deep networks as well so this combination of the of the of uh of efficiency and information concentrating features and these responses of neurons really raises the question in my mind besides trading curve contours what other information concentrating features do neurons encode especially once you get out of primary visual cortex and you begin to explore some of the areas that layla was talking about for example inferior temporal cortex where we also record so to think about this question there's two other inspirations that we're that we follow one is horus barlow who also inspired by information theory published a paper in 1962 called possible principles underlying transformations of sensory messages where he postulated a number of criteria one of which was that this features probably had to be of significance to the animal they had to allow the animal to perform this behavior better and the other more recent set of work came from shimon allman who postulated this idea that objects could be represented by a hierarchy of fragments extracted during learning from observed examples we specifically described them as pictorial features of intermediate complexity that could be used to solve a certain object class tasks so this is the kind of what we want to focus here on on this talk is we want to understand what information concentrated features are encoded by neurons in this visual recognition pathway we want to follow and measure how complex they are relative to natural objects classic image stimuli and even representations in artificial neural networks and following horus barlow's idea we want to find out if there are of ecological significance to the animals so let's do that so what is first of all how to identify these features and there's a number of ways to do with what i'm talking about is basically feature visualization and we want to do it in the in the primary brain and you know there's a number of ways of doing it but i'm going to talk about one that i think is particularly effective this is based on a paper that we published last year while i was still in march livingston's lab and working with gabrielle crimean and this is the idea of using generative adversarial networks as part of the electrophysiological process here uh when we record from neurons and uh being on europe so i don't really have to spend a lot of time describing what these gans are but i think for the purposes of the talk all we need to know is that these are deep uh networks neural networks that can basically take a vector as an input of a few hundred elements or maybe a few thousand elements depending on your game and then through a set of up convolutional layers outputs an image that is over half a million pixels uh but it's in color and it they can really really good at representing natural objects and uh they're in fact so expressive uh or here uh auto encoders are also very good we like gans because they are very good at expressing not just class dependent information but information pretty much in any natural scene for example this one by uh those soviet scheme brock's shows images that are pretty stylized that you can you can recognize as cheese burgers and water jars and there's more recent ones including began by google that can also do a really good job of representing images and also giving you very intermediate and continuous transformations between images so we started using this one into a 2016 version and the way that we did it in that in this initial set of paper was to combine these gans by uh in a typical experiment we would begin by taking a bunch of random input vectors actually based on simon shelly textures uh then they were in input into the game that would create a bunch of random images that we could then present to a monkey record from neurons in the monkey and weight behave in macaque count the number of responses per picture and then basically use a genetic algorithm to take those input vectors and the responses in order to come up with new candidate vectors that were meant to increase the firing rate of the neuron in the same way that cubone and weasel did it and when we tried that we found that it was an effective approach uh here i'm showing you the responses in one experimental session of one neuron inferior temporal cortex in black are the responses per generation to synthetic images and green a set of non-changing reference images that we use to track the stability of experiment and as you can see this neuron showed an increase in firing rate again reminiscent of what what it means to identify the preferred feature of uh of a given neuron and what we found was that these images comprise fragments that could also be identified in preferred database images of the same neurons that give rise to this so for example features corresponding to things you can find in faces features corresponding to things you can find in objects with a dark light contrast and even things like in objects that weren't always so easily semantically describable so we thought okay this could be one way to get at this information concentrating features they're certainly meaningful to the neuron even if they're not always semantically describable we think that's an exciting region to be at you know for the rest of the stock what i want to do is i want to describe these gandhirect images using a nickname uh we're going to call them prototypes because we think that they're abstractions that are learned by neurons from the natural cells all right so now we can ask our questions how complex are these prototypes related to natural objects classic image stimuli and artificial neural networks and do they matter to the monkey so that's what we set out to do and this is not work that we've been doing in in my lab for the past year and a half these are the members of my lab and i'll be talking about today my work done by olivia mary james and binchu okay so first of all to test this hypothesis what we wanted to do was to examine prototypes encoded across the visual system particularly the ventral stream so we implanted microelectro arrays in two different monkeys starting from the intersection of area v1 v2 then p4 as well as infer temporal cortex then we uh olivia and bishop run the experiments and we basically found that it worked across dimensional streams so if you can now see one particular example here the increase in firing rate for the synthetic images if we take this amplitude in firing rate change over the course of the experiment and compare that to that of the reference images here's what we're what's being plotted here on these two now the response changes synthetic images versus those of the reference and as you can see it worked well across both monkeys and across visual areas so the first thing we found that we noticed during the experiments was that neurons in more anterior parts of visual cortex were taking longer to generate their prototype compared to neurons in earlier visual regions and this is by looking at the time that the number of generations needed to reach have maximum of the converge response this we think is due to the fact that these areas are representing less and more specific kinds of information so for example here i'm showing you prototypes that were sampled from area v1 and v2 in both monkeys area v4 and infrared temporal cortex and i think the clear similarity by eye is that the primary visual cortex and the early visual cortex representations tended to correspond to a lot of lines and stripes things that the generators are very easy to populate their space with no matter where you go in the latent space of a general generative adversarial network it won't be hard to find contours what i've superimposed on this too are the estimates of the center of the receptive field of this individual neurons in contrast to v-1 though i.t tended to have objects that were rounder that tended to have eye-like dots and even more reminiscent uh things that could remind you of arms as well as even fruit this one called our banana neuron so more specific so it turns out has been shown that in some experiments that actually convolutional neural networks show the same pattern so we took alexnet and then we used alexnet in the exact same approach as we are experiments same search algorithms everything was basically the same and we found that indeed the deeper layers took longer to generate their preferred their preferred prototype compared to compared to early layers so we can describe we've been looking at different ways to interpret more of the content across different areas but i really want to focus about right now is complexity and so one of the things that we noticed is that in the in this evolutions that uh the prototypes seem simpler more more concentrate they really were like little image fragments or patches compared to natural images so we wanted to ask how simple are these prototypes so to do that james johnson new postdoc measured the compressibility of these prototypes using just a simple discrete cosine transform that's used for viewing audio compression which i think all of you guys know is basically a fourier transform this is using real values and that just transforms the images into a set of weights corresponding to cosine functions so you can take images that are have different levels of um of simplicity to them like an object that is segmented from the background versus textures and you can see the discrete cosine transforms for them uh these are weights on the specific cosine function so using these transforms you can compute a compression ratio which is basically you can think of as the fraction of coefficients that are needed to maintain the percentage of the error of an image's energy between 50 and 99 and then we can average across these different levels and what it gives you is a value that you can use to intuitively see that okay an object like a hand without a background is more compressible 0.01 compared to an extended texture 0.33 so we wanted to now ask all right what is this like for four prototypes one problem though in making this assumption whatever number we get is we have to deal with the fact that maybe the gans already compressed image images quite well i mean they've learned to represent efficiently a set of natural images so to control for this what we did was take the each individual vector that had been curated by the neurons independently and then we simply shuffled the elements in that in that in that vector and so that it's a plausible um a possible vector that could have been reached by the neuron but wasn't and so we're going to call those random prototypes and what we can do now is visualize them they look more like this so they're more high frequency they're a little more extended here they are compared to each other prototypes versus this random prototypes and now we can measure how compressible they are and what we find is that uh here i'm showing you the distributions of the prototype compressibility for both animals white and black compared to the random prototypes and as you can see the distribution is more towards zero that means that it's a that's a lower compression ratio it's it's it's more compressible um in fact we find that it's about 20 or 30 percent more compressed than random prototypes created by this generator what's also interesting is that um we then decided to contextualize this number by taking some of the iconic uh important stimulus sets that are present in visual neuroscience going back to human weasel david vanessa's work uh charles connor and ethan paciopathy these were stimuli that were designed to elucidate one particular feature of neuronal function like curvature and we find that indeed uh they are they are simpler in terms of their compressibility ratios are meant to be but i think it provides an interesting point of comparison as well then what we also found was that when we examined the representation the complexity of the representations we found that they were in neural networks we found that they're actually less compressible i'm sorry yeah less compressible so it's more made of more irregular textures and so you know they were most of them floated around 0.2 and um and so we thought that that was an interesting finding which i'll come back to in just a second but we wanted to do one more way to try to measure the relative complexity of these prototypes compared to um other things and one of the things we used was the mean shift clustering algorithm which is again a an algorithm that is out there you can use it to segment images so you can take an image like this and through this algorithm basically come up with a segmentation that each in this case for this particular picture decided is made of seven parts so we applied it to natural images and as you can see the number of parts that it identifies for segmented objects is less than that for textures and when we apply it to the prototypes versus the shuffled prototypes what we find is that indeed the prototypes have fewer parts compared to what the gang generates on its own so what this tells us the takeaway that we were getting from this is that these neurons really are including prototypes that are simpler uh not as simpler than natural scenes really corresponding more to like texture segmented objects uh neurons resist appear to to do that and what's an interesting comparison as we're reading um this generator was created by anduin uh well developed by edwin with utzowicki broadskin and jeff cloon and what what don's been finding is that when trading networks that are more adversarially robust less likely to be fooled by uh you know random pixel masks that are that are uh designed to make the neuron change its its classifications um these networks these robots networks tend to rely more heavily on shapes compared to textures they tend to have uh detect more pixel-wise smoother patterns so they get rid of a lot of that high frequency stuff and they do represent more lower level uh uh features so we think that's an interesting point of comparison that we might want to pursue and then finally the second thing that we wanted to understand i'm just looking at the time here is um this idea that horus barlow proposed that prototypes really should have particularly significance to the animal and so this means that we have to have some sort of behavioral correlate to our ability to predict based on the prototypes now there's a lot of different kinds of behaviors that one can teach a monkey to do some of the best things about working with monkeys but what we really wanted to do was focus you know we're looking at a set of uh representations that are there we didn't train the animal to do it we're discovering an architecture of an ecological world that monkey cares about and uh so we also wanted to have a behavior that we discover and one of the things that's interesting about working with macaques is that uh they love to just look at tv like the rest of us so here's we have monkey alpha and he's sipping from juice right here as you just present images and i can give you an example of what that looks like so we can present the image here of a monkey holding a banana and you know another little bit baby monkey in the back and very easily right away you can see that what the monkey is looking at is the face of the baby what the what the mother monkey is holding and also trying to check i guess the the sex of the of the monkey so this is very very an easy intuitive way to understand what the animal cares about so what we decided to do was basically um infer what what identified the parts of the images that the monkeys care about based on how they look the product the hypothesis being that olivia helped develop was that these prototypes may be representing features that are salient in visual scenes and so to do that we showed thousands of images to the monkey and we started to see if they had a relationship with the prototypes so we started with 5300 images you can collect this very soon very quickly then we found the image patches that first drew the gaze of the monkeys we took the first sakad we used this as a center point in which to extract a patch based on the monkey's foliage size and so it represented images like this which we can then fit into alexnet and then compare in the same space as the as the prototypes and so here's what we this is in layer six and as you can see this is the distribution of points so now to contextualize the distance between the prototypes and the view patches we also did a couple of other things we first of all fed random prototypes the shuffle prototypes and then we also did a control where we took the eye movement behaviors and then we simply swapped the picture underneath to make sure that the monkey's behavior actually mattered so we swapped and extract those patches and when we do that what we find by inputting the prototypes the shuffle prototypes the patches viewed by the monkey and patches that were not viewed by the monkey into alexnet we find that indeed the uh prototypes are more similar to the image fragments that the monkeys wanted to see they were as a group more com more related to natural scenes than random prototypes and now here i'm showing you the actual results these are very small effect sizes uh this in piercing correlation between 0.1 and 0.05 but i think based on on train behaviors and just a few hundred prototypes we think that it's uh and the error bars are within the actual symbol that it's pretty reliable and i think a good start to try and understand these representations in early visual cortex so uh that's it uh i the overall summary is that we are finding that these neurons are encoding motifs to have certain intermediate complexity we're calling them prototypes but we can think of them as some of the human almonds fragments they are less complex than standard neural network representations they are predictive of the monkeys looking behavior and i think it creates an interesting discussion point nothing with pursuing or pursuing just yet but the idea that convolutional neural networks that have simpler representations may uh entrain with the visual ecology of primates may ultimately be better models for uh visual brain and with that that's all i have thank you very much 