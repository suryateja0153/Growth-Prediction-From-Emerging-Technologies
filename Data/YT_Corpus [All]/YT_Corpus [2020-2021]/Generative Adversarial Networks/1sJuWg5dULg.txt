 hello welcome to lecture 8 of deep and squeezed learning today we are going to talk about the strengths and weaknesses of various narrative models and representation learning methods that we've seen so far so the brain has 10 to the power 14 synapses and we only live for 10 to power 9 seconds and so we have a lot more parameters then then I'm the data we ingest so this motivates that we should do a lot on scores learning because in order to provide sufficient fodder for the number of parameters that we have in our brain we should be able to predict a lot more bits from the data that we ingest which is 10 to the power 5 order magnitude smaller right so this was a statement made by Jeff Fenton in this 2014 in a reddit so firstly you summary of the course so far we've looked at or aggressive Morrow's fix learn and pick so CNN picked two skin and paws glass pixel snail we looked at four models really only P family of models and also the connection between Auto regressive flows and in with Auto regressive flows next we covered latent variable models models with approximate density estimates using the variational lower bound and various associations of that like the VA e importance rated our encoder VQ BAE pixel BAE and so forth we also then jumped into a different class of jeredy models that don't work with the likelihood principle the impasse density models against energy based models in the moment matching principle and finally we questioned the idea of like whether we even need to learn generative models if all we care about is extracting useful features from unlabeled data and that God isn't with this topic because house provides representation and we saw that with the right kind of simple cognitive principles and a lot of data and compute we can learn really useful representations of unlabeled images that are competitive with supervisor plantations so represent so let's let's look at auto regressive models used in the in 2015 the main paper was Bush with which introduced this idea of masked or encoder for density estimation and it was able to produce these I'm miss digits which were reasonable looking but very jittery and this idea was extended to much stronger architect more expressive architectures well-suited for image modeling like masking illusions this individual introduced in the pixel or an analytics is seen and family of models and you certainly started seeing generative models working for higher dimensional and much more diverse assi Commission ad so these are samples from image net 64 by 64 you can see that the the structure across 4,000 pixels is pretty coherent but the color is not that good and therefore you're not actually able to identify any visible class from imagenet but this was a big big jump from the quality you saw and made and this idea of mass convolutions it has also been applied for one dimensional data like audio and in order to model long-range cover in audio samples the idea of using dilated combinations was introduced and this was also applied for a text-to-speech system where you're going to convert linguistic and text features to raw audio and that can be used in any Indonesian assistant like the Google assistant and this was the Wayman architecture that was commercially deployed after after a year and the same idea of using mass conversions with Auto regressive pixel level modeling has also been applied for Rio prediction why are you looking at the pass frames and encoding them with a convolutional STM and then you're taking the embedded representation as a conditioning information for a pixel scene and decoder that generates the next frame pixel by pixel and it's able to produce coherent video look like a robot moving out to Tehran so over time the order aggressive modeling community has expanded further and further in terms of the level of engineering and architectural innovation and on the left you can see if the subscale pixel networks which have very coherent samples because of the clever conditioning can assume to use on the right you see hierarchical auto regressive image models with auxillary decoders where the idea of using latent space auto regressive models was introduced by quantizing representations or encoders and and modeling pixel CNN the latent space which is also similar to the vqv a a idea that you seen in the VA a lecture so apart from images and audio and video auto regressive models have had immense success in language and these are samples from GPT to riots it would actually produce a coherent story about unicorns and like a like a story of how unicorn skin when their own language and also talks about a scientist who is able to observe all this phenomenon and this shows that language modeling at the level of a paragraph or even multiple paragraphs as possible by just training large models which used to order aggressive structures this slide shows the evolution of language models over time we're on the first you see Shannon's three grand models which I reasonably good but not super coherent across the full sentence and then Ilya sutskever is model of using an RNN is able to produce a couple of sentences but not completely making sense and then over time they using bigger LSD and bigger transformers you ended up with the quality that's UPD to experts right now so all these huge advances have been possible due to multiple reasons and let's go through them quickly the first thing is just being able to train with larger batch sizes because of more computer availability and training with larger bad sites then we stabilizes the training of these models and optimizes these losses much better making the models wider making a modest deeper figuring our clever race condition your next year you're building a conditional class conditional or audio condition or text condition model the figuring out ways to get the conditioning information cleverly is very useful pre-processing like in wavenet we use some new law pre-processing to quantize continuous audio into discrete entities are for example in pixels you're actually using categorical information for modeling rather than rather than using gaussians so these are these are and in language you using by parent coding which is pre trained on a huge corpus and therefore your mod in on modeling neither at the character level or at the word level but your modeling in the sub word level and that's much more useful for generalization and also building more efficient models compute power and as we progress in the last two three years we just have at we just where access to a lot more compute like TPU so I like big GPU rigs which have lots GPUs connected really with a really fast interconnect and therefore be able to train data data parallel model is much better and we're to train see that several weeks or basic training are usually producing much better results and also making fewer assumptions about the whole problem like before trying the idea of this predicting categorical distributions for every pixel why would he want to imagine that pixels are definitely gonna be modeled with calcium's instead of categorical distributions like indy really doesn't make any sense but then practically it's better for a neural network to work with cross entropy losses there are also been architectural advances that made all these was much better so mass conversions were applied in the original Pisa CNN but as transformers and dilated communist art exists the samples just got much better with more coherent structure across long range dependencies and and making the whole modeling problem look more like supervised learning helps a lot and therefore relying relying heavily on oh they'll be here crossing will be lost and optimizes that have been much better tuned for this loss ensures that generative modeling can also benefit from all this but engineering advancements so now what's the future for our regressive models we're only scratching the surface of what's possible and and once we have motor pilot training we'll be able to realize a lot more for instance be able to train trillion parameter models on all of the Internet's text and that that way we could compress all the Internet's text into a giant neural network that can be a like a know-it-all language model and secondly we can figure out ways to Train one single model for multiple modalities just even bigger generative model they could work at a video level on YouTube or image level Instagram text level Cabiria so that way it's able to probably correlate information across multiple entities and chameleons for expansion so for all these kind of modeling requires hardware and software advances from auto pilot training we should it's also possible to make or aggressive models more useful by figuring out faster ways to sample with better low-level primitives at the CUDA that will like for instance fast kernels and and better act like for example wave are an N uses all these mechanisms for production components and doesn't need to be distilled into something like a parallel bayonet this work as a standalone auto regressive model and still be deployed on an Android phone hybrid models with much weaker or aggressive structure but that can be trained on a large escape could be revisited and and of course all these architectural innovations that help in long-range dependencies would always help in you know as you keep moving to bigger image this or a video or something like that these kind of ideas should up a lot so like a summary of auto regressive model could be that it is an active topic but a lot of cutting-edge to us and there's a lot of moscow for a new engineering and creative architecture design and larger models and data sets are clearly needed to you know realize the full potential of these class of models and standalone they are very successful across all modalities without any conditioning information like class labels so that's that's like a very appealing property of these models every Universal in that sense and also they can work without much engineering for sampling time so that makes them really look creative but but but nevertheless for production if you you should really cut down on the sounding time to be useful and so innovating on the low-level primitives was very important so that said there are a lot of negatives for aggressive modeling one is you don't extract any representation there is no bottleneck structure and sampling times not good for deployment it's not particularly usable for downstream tasks like for instance a language Maru you need to sample multiple times to see coherent samples so you can't just roll out a language model that's a software and there are no interpolations that you can see to visualize what the models actually learning and every time you sample it's going to take a long time to produce like a diverse set of samples so that's it about auto regressive models now let's look at flow models in flow models it all started with the nice architecture by loaned in and those the model was already producing very good digits on the endless data set and on the T of tedious it was producing reasonable phases but it really was bad on see far and SPH India said the samples were very blurry but it all improved with the real end we'd be architecture which introduced other kinds of flows and rational room to make the models better and then the glow model from King model was published where the real and Ruby model was taken to another level by making it prettiest much larger images and overdone in our lab called flow pass class advanced the likelihood scores for flow based models to competitive scores that with that of autoregressive models for the first time and this was done by this architecture engineering and scale so this shows the power of flow models of potential they have in terms of closing the gap in density estimation between autoregressive models without having the powerful or aggressive structure but at the same time being really fast with sampling and also potentially useful for inference so given all these practices there's a lot of future work left in terms of how to learn the masks how do you actually completely close the gap with our regressive models whether you want to use very expressive fluids but very few or whether you want to use shallow flows which are not particularly expressive but then keep on stacking them so that you can get a very expressive compose model how do you use multi scale losses for a trait and how do you trade off between your density estimates and your sample quality and how to use the representations you derive at various levels of the flow model for downstream tasks all these are like fundamental advances think about for flow models and also how do how do you carefully initialize so that flow models can train very fast so in terms of core achievements that you can aim for you can aim for producing low level samples which are truer models that have way fewer parameters the globe uses half a billion parameters for all the celebrity faces and that's unlikely a scale and how do you make it work potentially for even larger images how do you do dimensionality reduction with flows and think about other other flow models like conditional flow models and you know how do you actually close the cap and sample quality de Gans and also close the likely skoros gap between autoregressive models so the models would provide the pathway to do both and it's it's interesting to think about how to do all these things together so the negative of flow model says you expect to have the same dimension at every layer every stack of the flow and so it's unlikely to scale if your data is getting bigger and higher dimensional and unless you innovate on how to do dimensional reduction sauce it's unlike it'd be useful and you really need to carefully initialize and use things like AK norm for good numbers so that's that's another negative because it may not be directly usable for another modality or another data set or another kind of architecture so let's look at late engraver models will see the various different be strengths and weaknesses and what have been some visible successes in bas it all started with the original Emnes modeling by dirk Kingma where you could see various types of digits and strokes and the slopes of the strokes and shades across multiple digits and then it got extended to much better more powerful data sets like Elsa in bedrooms by pix ovae and also image not 64 by 64 creating much better global sound globally more coherent samples 10 pixel CNN because of modeling latent structure and then there's the latent variable models innovation in terms of using hierarchical models and multi stack using hierarchical Laden inference and producing really high quality sound really faces on par with slow models so there are well-known applications of V like sketch iron and role models and BW is used for modeling visual concepts and there are applications like deep mines jeredy cry networks which does view synthesis of a separate view by taking in two provided views and embedding into a latent rifle and interpolating the lane space for a query view across across multiple possibilities and therefore you can just collect data in a completely new environment from first-person vision you can you can keep a track of all their poses when you're recording things and then in principle you could figure out how a particular scene looks like from any other viewpoint and therefore reconstruct the entire room or entire environment completely through this kind of a synthesis model that has rational inference so we have practically used in these kind of architectures and there are lots of advantages of EA's you get a compressed bottleneck representation you can get approximate density estimates you can interpolate and visualize what the model learns you can potentially get disentangle representations where different readings correspond to different aspects of data and it is like a model that allows you to do all these things together at once like you basically can sample so you are a gyrator model you have a density estimate so you can use for our distribution detection as a density model you have latent variables so you you do representation learning and you also have a bottleneck representation so you are able to reduce the dimensionality of your original data set so a VA is the only model that lets you do all these four things together and that makes it very appealing that said there are disadvantages you often end up with Lurie samples and assumption of a factorize Gaussian for the posterior or for the decoder this may be very limiting and you need more powerful decoders or more powerful posteriors and large scale successes are still yet to be shown and even though people have tried to like get more interpretable more disentangling variables by prioritizing the KL term over the reconstruction term the last it's still only work on toy problems and they may actually be better ways to do representation learning or generation or like yeah interpolation in some form hierarchical Layton's individually so expecting for one model to all of them well may be truly hard and so a we may not be the state-of-the-art models on anything but maybe a model that lets you do all all that it all these things recently well in using a single single single modeling framework so that's that that's the that's what you lose when you want is everything within one model so that these are the disadvantages to me but there's obviously scope for future work you can but you can use bigger decoders more powerful posteriors you can think about how to do hierarchical Leyton's to learn covers and fine-grained features and discrete Leyton's like weak uva and also large scale training like slow models have been done like glow or focus bus so next let's cover implicit models but we look at general adversarial networks and just just basically what what's happening ganz though we also covered moment matching energy based models in class the Gann samples the quality of Gann samples has dramatically advanced from the primitive samples that you saw in the original Gann where you saw X reasonably looking good faces but then the c4 samples it's not pretty cooing too critical in terms of what is the object or class of C far that's been captured but it certainly looked different from Larry BAE samples at the time next you saw DC Gann which clearly advanced some the some quality of dance to a state where again to assign you a looking much and much more exciting than any other model because the samples were much sharper and all these bedrooms were very high dimensional and then recently again giving again has been taken over by began stag and classic models were clearly careful attention to detail in terms of architecture design and also really really large-scale training like large pad sizes and a lot of stabilization tricks can produce these amazing photorealistic samples that you've already seen plenty of times in the class so I'm not going to go over them in terms of future work for Ganz I think I think it's really hard to bet against cans to say hey this is work cans weakened its most likely that if you put sufficient effort in engineering you can get it again to function well on those things as well but but nevertheless there's still more progress we made an unconditional cans more collapse and also more complex scenes and video generation will be cool for instance will be nice to get a model that works on real driving data where and a lot of pedestrians are walking and then you want to be able to simulate future you have to keep track of multiple people multiple objects multiple cars road signs and so forth so it's a very complicated jeredy modeling problem and it'll be interesting to see it ganz which are known to identify only a few cues in your dataset would they still work in such complex settings where you need to keep track of multiple things at once so future work in terms of modeling you can like think of more purchasable Lipsius knows better conditioning tricks like how to feed noise if your various levels like for instance stai again basically innovated their batch or instance normalization of how to design better architectures working on sampling and down something ops to use how do you how to do channels of sampling and done something without introducing a lot of parameters what is the right objective function for your discriminator and how to scale and train ganz in a stable manner for like larger problems and how to preserve it at various different levels like how do I instance noise a feature noise so that it can stabilize the training of the discriminator much better so all those things are very very interesting and think about in terms of negatives again scans if one could say there's plenty of engineering details and it's hard to clearly identify which is the most important core component that helps you reproduce these high-quality images and it's also very time consuming to ablate for these details so and and and and and it's very clear we need to improve on the sample diversity but then we also don't have very good metrics for evaluations so we need to work with what we have and even though it may seem like we're improving a lot on the current metrics we use for again evaluations objectively the sample diversity is not a spurious likelihood based models so how do we actually come up with better valuation measures also one thing to think about with all these aspects like good evaluations good metrics relations these are not particularly specific to the scans these can be said for any any any kind of model as with any other model so if you were to make a choice between Ganon or density model one would imagine you need a lot of engineering details for Ganz but it's not particularly true even for density models the architectural engineering has been comparable level of detail and you know trickery that you need for Ganz and secondly there is a lot of attempted theoretically understanding Ganz so the trade-off between having blurry samples versus of being okay with mode collapse is basically the same trade-off that you make when you care more about compression at the cost of sample quality was this you wanting to have really good samples at the cost of missing some modes so it's basically which direction of kale that you care about and the reverse direction you care about more if you don't want any spurious sample but the forward direction you care about more if you really want to make sure that your modeling is good and you're not going to make any mistakes even though your you're not gonna miss out anything you in there you may make some mistakes at some of some of the points so mostly apart from the fact that they can produce amazing samples cans are popular because they can work with much less compute for instance in order to generate a 1 megapixel image for an auto regressive model or even a Leighton space our aggressive model you need to use at least 512 course or TPU to do that because you need such large pad sizes whereas for gans you can make it work with a single V 100 GPU and then so there so that's that's one reason why gangs are clearly preferred over than 10 C models because I'm amount of time taking the train as a sample and you can also see better interpolations and better conditional generation in cans so this dis leads to adoption by people who are more interested in art and fine tuning to like interesting artistic datasets you're not particularly machine learning relevant and that's one of the other reasons again a speaker plot so on the bright side we can think about how like many technological advances have been possible without the correct science and so ganz can we consider in that way as well and this is a slide from young Conan the epistemology of deep learning where explains that several technologies in the past have preceded their science that explains them for example the steam engine was before the thermodynamics so it's doing better theory for ganz is something that could still be innovated on in the future so here is a taxonomy of generative models from in Goodfellas new ribs tutorial apart from Markov chain Boltzmann machines and Markov change and are the stochastic networks we have pretty much covered everything else we've covered Nate may fix Lauren and how do you exchange of variables scale the flow models or really be models all these are explicit density models and then we also covered approximate in steam models vary from our encoders the variation lower bound and then recovered implicit density model estate they can other models that I'm not being covered are not particularly popular or very used so that's the reason we focus on the more popular ones and if you have if you're if you have been and trained density models and you're figuring out which density model you should be using here are some pointers if you only care about the density estimates disco for our aggressive models you don't worry about sampling time here if you care a lot about sampling times in autoregressive may still be fine if your sequences are not that big or if you use lightweight models but if you really cannot afford to wait for the sampling time you really want really fast samples but you still want to go for a density modeling you could think about using Vikan regressive models like paralytic so CNN and you could also think of doing latent space modeling like like latent space or like a week you BAE you may probably not even needed quantization bottleneck it could still work with like continuous values and so models are also pretty billing for modeling continuous value data that density estimates for continuous value data especially even when they're actually continuous and it's hard to figure out how to even quantize them so so that that's that's another interesting aspect of flow models and if you also want to think about how how to have like representations and also sampling but you want to have a simple possible model v's with factorize decoders maybe the natural drugs so given given these appealing properties or density models like when would you use cans you would use guns when you really care about having good samples and you have really really large images high-quality images for and you don't want something photorealistic you have a lot of conditioning information like pose or the class or edge edge maps and you just want to add texture to them cans are really good in these initial image translation problems or rear the video and if all they care about is perceptual quality and controllable generation and you don't have a lot of compute this is often the case for any any kind of start up again it's like the best choice to go for so that's it for generative models next let's look at South provides representation learning it which is our final topic so south supervised image classification has seen rapid advance in the last one and a half years just the end of 2018 the top one accuracy of image net linear classification benchmark was 48 percent and now it's seventy six point five percent so this rapid advance has been made in multiple labs because of this mode of learning called contrast to learning and contrast the learning task can be simply summarize this a dictionary lookup task and there are two ways to do this pretext contrasted learning which is you either build it as a predictive coding task or you build it as an instance discrimination task and in predictive coding you have multiple mechanisms to do that once you either used end-to-end mechanism or you use the momentum decoder momentum encoder make using the momentum encoder for the keys and the predictive coding success story has been achieved in the contrast operator coding or CPC particularly the CPC version two and and and the instances combination success has been achieved in moko and sim clear moko means momentum contrast and Sinclair's into an instance contrast they use the corresponding mechanisms of contrast learning so let's look at CPC version two moko and Sinclair in terms of their positives and the negatives so CPC version two we're doing spatial contrast prediction so that principle is very generic and it can apply to any morality or domain so you don't need to know the underlying data augmentation in variances in this work and it can be considered as latent space channel tomorrow and also it's much easier to adapt for audio video text and perform multimodal training disadvantages it splits your input into a lot of patches or frames or even audio chunks and therefore your inputs are now your inputs are now basically split into a lot of different parts that you have to carefully delineate and you also need to carefully pick what part are you predicting from what so that involves a lot of design choices to make type of parameters that you can only know by trial and error so that makes it really hard for you to use it on a domain or task that you don't really understand well and then you require multiple forward passes for these smaller versions of the inputs now and so that means that you be pre-training on something much smaller but potentially fine-tuning are much larger versions of the sequences or images so this may not be an optimal thing to do when you're doing local predictions local spatial predictions Bosch num is hard to use so applying mass ROM is hard but then you really want to use batch room for a downstream task so that makes CPC version too little sore in sense it's not particularly suitable for downstream tasks if you really care about state-of-the-art performance and finally the splitting process mechanism is very slow on a on a matrix multiplication specialized hardware like GPUs so it's because you do a lot of reshapes and transposes and so it's never an optimal thing to do so here's the summary of moco one of the main advantages of moco is it is very minimal so it's very easy to use and replicate and it has no architectural change can be easily applied for downstream tasks there is no notion of a patch and it's distilling in variances for images using data augmentations and so the pre-training procedure looks very much like supervised learning and therefore it can get comparable or even better results and the momentum encoder memory bank can assume adds a lot of stability to the training and decouples back size from the number of negatives and therefore this lets you train with way fewer GPUs than what's needed for CPC or like methods the disadvantage with moco is that because you introduce momentum and date you need to figure out what's the right decay rate for that and that has an extra type of parameter and another disadvantage is in image augmentation the invariances may not be applicable to other modalities so this may be in method this works only for a visual image recognition and finally let's look at simply er which can be considered as an end-to-end version of Tomoko where you just look you're using all the negatives from your batch and there is no momentum encoder so advantages or sim clear are the same as that of moko with the additional advantage that you don't have a momentum in kora now so it's going to be asked minimally supervised learning but the disadvantage is now you just need really large batch sizes because you need a lot of negatives because moko decouples the negatives from a bad size it doesn't need as much compute as sim cleared us and similar to moko they documentation invariance may be very specific to image recognition so in terms of future work left for sauce provision the gap between some supervised learning and supervised learning is to not close if you consider just the same amount of compute training time and the same candidate augmentations use so and also fine-tuning to downstream tasks the gains are not significantly high enough that the paradigm shift has been made in vision so that way maybe new objectives are also needed and finally all these sub supervised successes have relied on using image net and it's not clear if supervised learning we just work from images in the wild or from the internet which is really the dream and which is really why people wanna do something so that's it for like subspace learning as in in terms of utility for downstream tasks let's look at always learning in the context of intelligence like being able to act in an environment so here is a video of this quake3 game where yeah like that you can see some characters and then you can see some bullets that there are going to be fired and you know you see all these different walls and fires and other characters and when you're looking at all this you're able to already accurately parse the scene make sense of what's going on and you're also able to clearly separate out the objects from what's not objects and and so we need to be able to do that as well we shouldn't be working at the level of pixels we should be able to predict the future in a much more semantically in space and so modeling the pixel space for these high dimensional videos is really hard and in order to build really dungeon agents which that can planning faster than real time we should be able to do it in the lane space that's more abstract so how do we do that what is the right kind of abstraction to build and how do we learn role models in that Lane space that can this ignore noise and work in a much more semantic space it's really the hardest question to think about and this has also been summarized multiple times by omnicom that if you have very good internal world model you'll be able to plan with it and a wide lot of mistakes there and our relation usually makes and and how to do that is one of the most important questions so if you want to have the overall view of subspace learning across all these different problems for image recognition we saw or assesses like city scene workers in clear moco version to transfer learning it works really well in language but the exact details will be covered in a future lecture and transfer learning and vision also works reasonably well now been shown in CPC and moco but there's like close to nothing in terms of how to use of supervised learning for RL so that's the very ripe area for future and then as far as like you know using sound supervision in the context of general intelligence is considered its it's potentially going to be extremely useful in the context of transfer learning and learning use of abstractions for planning or imaginations so that's just a lot of work to be done there so that's that's that's it for the summary of the class it's pretty much ends with our original motivation which is how do we build this intelligence cake and and a lot of it is gonna be done through supervised learning and and so in terms of future lectures they're gonna look at more applied topics which are not falling into the main main main lecture stream which is that we be looking at semi spread learning we'll also be looking at the whole area of one square learning for language which is language models and bird and then finally we look at how representation learning or supervised learning has been applied in the context of reinforcement learning so and and we will also cover things like how to do unsupervised distribution alignment that is given completely to different data sets with a lot of common information how do we align the two manifolds together and without any prior data and you see how generative models and unsupervised learning can be used in the context of building compression algorithms so that's the next next series of lectures will be not particularly connected to the main topics but the mostly looking at how we can apply them for all these other various problems 