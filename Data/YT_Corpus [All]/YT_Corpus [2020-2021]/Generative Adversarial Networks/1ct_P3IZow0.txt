 Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we frequently talk about Generative Adversarial networks, or GANs in short. This means a pair of neural networks that battle each over time to master a task, for instance, to generate realistic looking images from a written description. Here you see NVIDIA’s amazing work that was able to dream up high-resolution images of imaginary celebrities. In the next episode, we will talk some more about their newest work that does something like this, and is even better at it, believe it or not. I hope you have subscribed to the channel to make sure not to miss out on that one. And for now, while we marvel at these outstanding results, I will quickly tell you about overfitting and what it has to do with images of celebrities. When we train a neural network, we wish to make sure that it understands the concepts we are trying to teach it. Typically, we feed it a database of labeled images, where the labels mean that this depicts a dog, and this one is not a dog, but a cat. After the training step took place, in the ideal case it will be able to build an understanding of these images, so that when we show them new, previously unseen pictures, it would be able to correctly guess which animals they depict. However, in many cases, we start training the neural network, and during the training, it gives us wonderful results, and it gets the animals right every single time! But, whenever it sees new, previously unseen images, it can’t tell a dog from a cat at all. This peculiar case is what we call overfitting, and this is the bane of machine learning research. Overfitting is like the kind of student we all encountered at school who is always very good at memorizing the textbook but can’t solve even the simplest new problems on the exam. This is not learning, this is memorization! Overfitting means that a neural network does not learn the concept of dogs or cats, it just tries to memorize this database of images and is able to regurgitate it for us, but its knowledge cannot generalize for new images. That’s not good. I want intelligence, not a copying machine! So at this point, it is probably clearer what images of celebrities have to do with overfitting. So, how do we know that this algorithm doesn’t just memorize the celebrity image dataset it was given, and can really generate new, imaginary people? Is it the good kind of student, or the lousy student? Technique number one, let’s not just dream up images of new celebrities, but also visualize images from the training data that are similar to this image. If they are too similar, we have an overfitting problem. Let’s have a look. Now it is easy to see that this is proper intelligence, and not a copying machine, because it was clearly able to learn the facial features of these people and combine them in novel ways. This is what scientists at NVIDIA did in their paper and are to be commended for that. Technique number two, well, just take a bunch of humans and let them decide whether these images differ from the training set and if they are realistic. This kinda works, but of course, costs quite a bit of money, labor, and we end up with something subjective. We better not compare the quality of research papers based on that if we can avoid it. And get this, we can actually avoid it by using something called the inception score. Instead of using humans, this score uses a neural network to have a look at these images and measure the quality and the diversity of the results. As long as the image produces similar neuron activations within this neural network, two images will be deemed to be similar. Finally, this score is an objective way of measuring progress within this field, and it is of course, subject to maximization. So now, you, of course, wish to know what the state of the art is today. For reference, a set of real images has an inception score of 233, and the best works that produced synthetic images from just a few years ago had a score of around 50. To the best of my knowledge, as of the publishing of this video, the highest inception score for an AI is close to 166, so we’ve come a long long way. You can see some of these images here. Truly exciting — what a time to be alive! The disadvantages of the method is that one, because the diversity of the outputs is also to be measured, it requires many thousands of images. This is likely more of an issue with the problem definition itself, not this method, and also, since this means that the computers and not real people have to do more work, … we can give this one a pass. Disadvantage number two, I will include this paper in the video description for you, which basically describes that there are cases where it is possible to get the network to think an image is of higher quality than another one even if it clearly isn’t. Now you see that we have pretty ingenious techniques to measure the quality of image generator AI programs, and of course, this area of research is also subject to improvement, and, I’ll be here to tell you about it. Thanks for watching and for your generous support, and I'll see you next time! 