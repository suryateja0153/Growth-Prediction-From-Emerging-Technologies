 Current relevance feedback methods are based on explicit interactions, such as clicks or ratings. Our approach allows direct readings of human reactions from brain responses, and feeds these reactions back to a model to adjust the generation process in the subsequent iterations, requiring no explicit intervention from the user. To this end, we answer the following research questions: Can brain responses be used as feedback to a generative model to produce task relevant images? How fast and well does brain feedback converge versus a random baseline and explicit feedback? How well does brain feedback converge on multiple simultaneous task targets? In our framework: First, an image of a synthetic human face is shown to the human operator. Users mentally focus on a target image feature (such as blond hair). They perceive and react naturally to it without giving any explicit feedback. The evoked brain signals to these stimuli are classified as relevant or irrelevant by a classifier. Classifier outputs are used to update the estimation of user's target in the latent space of the generative model: a generative adversarial neural network (or GAN). While users reacted to single image features, combinations of those were simulated, up to three For example, a face of a "smiling blond young" person. Evoked neurophysiological signals as a response to relevant stimuli were statistically significant compared to those evoked by irrelevant stimuli. For classifiers performance, in all cases, the median area under the curve was above 0.7, showing remarkably good single-trial classification performance. The quality of generated images was manually assessed. With more brain feedback iterations, the generative performance increases as the output approaches the user's mental target. Convergence is however slower as we simulate combining more features, therefore increasing task complexity. For example, we simulated an image displaying an old person, who was additionally smiling, and also had dark hair. Brain feedback showed generally high image generation performance, being statistically significantly better than a random baseline, and indistinguishable from explicit feedback. In conclusion: We present brain relevance feedback for image generation, combining generative adversarial networks and brain feedback. We demonstrate the technique of face generation with complex facial feature targets, showing that the technique is able to generate images matching user intentions. Results show performance significantly better than a random baseline, and approaching manual feedback. Our approach yields pragmatic performance suggesting applicability in real-world usage. The presented paradigm enables us to develop BCI applications for complex tasks, making possible to augment creative processes with technologies for implicit interaction, and human-AI collaboration. 