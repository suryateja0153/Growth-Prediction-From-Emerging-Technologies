 Click it yet. But what I'm going to do is actually gonna hover over it and click this small little icon where it says learn more, it's kind of like an AI. So I'm going to click learn more to learn more about this specific model. That's kind of neat to be able to do because then you can like, if you're just browsing through all the models, maybe just from the thumbnail, you don't really understand what it does from the title. But here, you can read more about what a particular model does, you can maybe see an image can indicate what it does. So this model seems to be detecting objects and also figuring out their bounding boxes. And then you can also read about the license. What can you use this for? What can't you use this for? Then sometimes, if you scroll even further down, you can kind of see a bit about, like, these models are typically not made by runway, they're made by somebody else. This model particularly is made by Google, it seems. And so you can actually have a look at the GitHub repo, that's where they store the code for this model. Or you can maybe have a look at the research paper, like what's the science that went behind this particular model? So you don't have to open it. Okay. And now this, this link is off, but most of the time, the links are actually good. So maybe for the GitHub, let's have a look, then you can kind of see a bit about that, too. They all have research papers, and GitHub books associated with the majority, I would say, but not all of them. Not a prerequisite. Or it's not mandatory? No, it's not. It's not mandatory. And, yeah, there's somebody in the background where there's quite a lot of talk that doesn't seem to relate to this workshop. So if you're not actively talking, I encourage you to mute the microphone. Sorry, that was me. But I asked a question, but I turned off my, my son is playing video games in the background. Okay, no problem at all. No problem at all. Okay, then what we want to do now, we want to add this model to a workspace. So before you can run a model, you need to create a workspace. So you want to click Add work, add to workspace, and if this is the first time you have done anything inside one week, you probably don't have any workspaces. So then you're just gonna click new workspace. And then you can rename your workspace, it doesn't really matter what the name is. So I'm just going to call this MIT workshop. And then I'm going to click Create. And now our view has changed. So before, we were actually in, in this view, the Browse models view. But now we're in workspace view, view. So if you click Browse models, that takes you back to the overview of all the potential models you can play with. But we have now added a model to workspaces. And you should see something along the lines of this. So can I maybe get get people to thumbs up, if they have added cocoa to their workspace? Any any issues? Or any questions? Then you can post them now or you can do it in the chat. Just a quick question, if I already have a workspace with previous experiments, can I create a new workspace? Is there an option or it's all in one workspace? You should be able to create new workspaces. You can kinda Oh, yeah. So they The thing about runway is that they they change the software all the time, you can actually see that since I opened it, which is less than an hour ago, they updated it once again. So if I relaunch it, there's a new update. Sometimes the UI gets moved at bit, I think. Okay, so to answer your question, let's just try it out. So let's say I have a model here. And I kind of want to like this one. I kind of want to, yeah, I want to add this so I could either add it to an existing workspace or I can create a new workspace. It doesn't really matter right now, like when we're trying it out in this work in this workshop, I would just encourage you to have it all in one workspace. Other questions? It doesn't seem so. Okay. So what you want to do then is To you can select different inputs. So this model is basically at image detection model that's able to detect multiple objects in the same image. And so it needs an input that will feed it some images. And you can do that different ways, either through a file that could be an image file, a directory of images, or maybe a movie file, or you can do it through your webcam. And I'm going to do it through my webcam. So I'm going to click camera. Depending on your operating system, and your maybe your security settings, maybe you need to allow runway to access the webcam. And if that's a really big hurdle, then you can just do it with local files you have on your computer. And I think I encourage you all in the email to prepare a folder of files if you haven't prepared a folder of files. And then you can also in the in the workshop, you can download here is a CIP with sample images to use. But for most of you, I would encourage you to just use your webcam. On Windows, sometimes having multiple applications use your webcam at the same time can doesn't work so so if that blocks you, maybe you can turn off your webcam here and zoom, and then try to open it in runway. If you're on Windows on Mac, it's normally handles it fine. Okay, so I'm gonna turn on the webcam. And then what I want to do is run this model. And I want to run this model locally, it should be the only option you have, because this is a pretty lightweight model that actually comes with a runway. Later, we'll talk about running models remote. So for now, let's just run it locally, you want to click that button. And now it's starting. And then hopefully, you should see something along the lines of what I'm seeing. So it should make a bounding box around you as a person. And if you hold up objects that it can recognize, and hopefully it should, can also do bounding boxes around those. So and this model is only able to detect 90 kind of average, or very ordinary objects. So it knows about banana, it kind of knows about this apple, and it knows about me, but if you hold up your headphones, maybe it doesn't necessarily work. Coco only has a 90 categories. And it's not the most reliable thing in the world. However, it's pretty fast in it can run just on your computer just fine. And it can even like kind of pick up some books in the background for me. Just interesting. So let me just get a feel for are people able to do this. So put a thumbs up or maybe like you can also make a reaction in in zoom where you make like an emoji a thumbs up some stamp. So a health say has issues. Do you want to say what your issues are? I I just my computer, I think is having a little challenge here. I seem to be frozen up. So I'm hoping that it will just work. Okay, in a second. Okay, it can. Yeah, so this is running locally on your computer action. If and if your computer's doing a lot of other stuff in the in the background, or if it's not like a really fast computer, maybe it'll be a bit slow. If it doesn't work, you can try to quit runway and try again. Okay, so when we're running a model like this, you can see the preview locally, which might be interesting for you to just check out. You can also have a look at the actual data like what is it actually spitting out, and so that you click the small button down here called show data. And then you can see what it's doing. So it's detecting these objects. And each object has a label and a bounding box. So for me, it's kind of like person and then sometimes there's some books. And the bounding box describes where and the image is that is this app. The label describes what is the category you can so that's kind of one way of doing it. You can also choose to export. You can export everything that's being seen to a CSV file. If you kind of want to keep track of all the objects that are being detected, maybe if this is like, we're using traffic footage and you want to do it for days it's kind of interesting to analyze realize where the cars were at where the pedestrians were at. And so that's kind of the text, export type. Other models will have other export types. But this one has text assets, export type and in a CSV format or JSON format. And then a thing I want to try out real fast, and I encourage you to do as well is to try to input not just the camera feed, but actually files from your computer. So I hope some of you prepare some files already. Otherwise, I have on the website, a sip of random images you can use, or maybe you just have some images lying around on your desktop that you can try to feed it with that, what do you want to do is click file. And then you can open either a single file or directory of files, I have a directory of files that I kind of want to try out. And that one is is the CIP that I also linked to on the webpage called sample images. So I want to open that one. And then you should see all the files from that directory that you selected, pop up up here. And then if the model is running, then you should be able to click the individual images. And they will spit out their detection. And sometimes, like these images that I chose are kind of weird. So they don't always have detection. Or maybe it's kind of wrong, like here with the stock, it sees two birds, where there's actually only one here with the dolphins, it doesn't detect anything. And it thinks that this burger is like I think it's, it thinks it's a sandwich. So like, you can kind of get an idea about that this is not the most accurate model in the world, but but it's like decent at certain things. Let me know if this is working for people the idea of running a sample of images through a model, is it working? Are you having having issues? Okay? Good. Am I in general, am I moving too slow or too fast? Or is it okay? It's okay. Okay, and then the reason why I want to show you kind of like this way of adding files, and also adding a whole directory of files is that that's it's kind of nice to use in combination with the export settings. So if you want to, like run, like now, I'm only going to do it with these 27 files. But you can actually take a whole directory of files, analyze them and export everything in, for instance, JSON format. So I'm going to click Export. And I'm going to select Export type text. That's the only one I can select. I think I want it in JSON format. And then I want to, you can specify what should it be called, and where should it be placed your exported JSON file, I think I want to just put mine on the desktop. Then I go save, and then I click the Export button. This will then actually run through each single image, analyze the content and save that in a JSON format on in a JSON file on my desktop. And that's kind of nice. Because then I have this sweet file that I could maybe use later in a project with all the detections, like a kite, a sandwich and all the bounding boxes as well. Okay, so that's kind of the basics of just running a model. This is the most boring model we are we will be running for an hour, it will be more and more interesting and maybe also more and more weird, at least in my opinion. But hopefully, you understand the basic flow. And I already now see that there's a question in the chat. So and whether the chat go sometimes I can't really see the one I'm into. No worries. I just wanted to know you were you were saying that you could use the file in the project. And I just wanted to ask if, like what type of project would use that type of file in case Give an example of how you would use something like these images that we exported. Like one example of how you'd use it in a project. Thank you. Um, okay, so so I can, I can give an example. But it's going to be a pretty weird example. But I hope you're okay with that. I'm going to give you an example from my own artistic practice. So I will want to share the screen, I have made a project that is on my website and also share the show the link to it, a recent project of mine called fairy tales. And it's a very, very weird kind of experimental machine learning project, where the goal is to try to generate poetry based on oaring images. So what I do in this project is that I take images, and I actually took 1000 images, just 1000 images, and detected all the objects in those images. So for instance, here it says a toilet, he had found a suitcase, here, that's a person, a keyboard and a TV monitor, then what I'm doing is that I am creating a story where the title becomes the story of the x, meaning the objects have been detected. And then in other cases, I'm kind of doing like the story of the horse in the dark, because a horse in the dark is detected, it's what I'm doing after that is that I'm kind of giving it a stereotypical beginning of fairy tale. So it could be something like in a place long forgotten, or once upon a time, or in a land far, far away, something like that. And then what I do with all of that text, like the title that is sort of halfway generated based on the object, and the opening sentence, which is my predefined, stereotypical beginning of a fairy tale, plus, me kind of listing all the objects, I sent that to an algorithm called GPT. Two, and maybe some of you have heard of GPT two, and it's actually a model we will check out later and runway. But GPT two is a model that you can feed with text, and it will continue your text. So it'll actually try to just give the prompt and it and it tries to finish your text. And what happens then, in my case is that, for instance, with the toilet, it creates somewhat of a coherent fairy tale, of course, it's very, very weird, but it's grammatically as good as my home English. And I kind of personally find it to be interesting. So I'm just going to read this one out loud story of the toilet, in a land far, far away, one slip the toilet. One day, a servant came to help clean it. Before he could start the whole thing sprang up into a dragon. He returned home at once. A year later, he was summoned back to the same place where he found it was still a toilet but more powerful. He found it that too much waste in the toilet caused an explosion killing the whole thing. Yeah, so that's kind of like one very weird example of using it may be a more like read. II see, obvious example is let's say you're doing something with traffic. And you want to detect every time a truck drove drive by and you want to lock that and you want to maybe see with the way you made street a one way street changed the kind of traffic and then it's very, instead of just running it real time where you just see it, it might be interesting to actually lock all the different vehicles, what the class was, and also where they were at on the street. So that's maybe a more down to earth example. less weird than my exploding Tyler's? Yeah, but but the reason I did it with Jason in this in this example is that I had like 1000 and I kind of wanted to, okay, you can just move on. Okay, so a quick question. Yeah. And I'm wondering, what is the relationship between training the models and using them in runway? Like, for example, the one that you use for your project? Is it exactly cocoa or did you train it with like, more things? Yeah, so um, so I actually I used an alternative, which is called YOLO version free, but it does the same thing I could might as well have used. Cocoa YOLO is a bit more precise. And both of them and also. Yeah, and everything worked. doing more or less today is pre trained. And it's trained on large, large amounts of data. That means that with a model like this one, we cannot add more categories, which is we kind of have to live with the categories that the model gives us, for instance, these 90 categories. That's a bit annoying on one hand, on the other hand, it's really cool to be have access to these models, because the trained way better than we could we, as individuals would be able to train it to trained on like large, large amounts of, of images that are kind of nicely curated and somebody started for us, and it would be very, very expensive and time consuming to do. Yeah, however, you can train certain models on runway. And we'll get to that towards the end of the workshop. If you're very interested in kind of like train your own things, in terms of classification, and you want like an easy start, then I would encourage people to check out ml five, which I mentioned a bit in the beginning, before everybody was here. So ml five is this very nice Machine Learning Library for JavaScript, that makes it super easy to try to train your own classes or do gap detection, with for instance, images. And other really, really nice tool is the new version of teachable machine from Google, that actually also lets you export the models you train. And I've been following teachable machine quite a lot. I was part of making the first version of teachable machine me and another guy helped out with that we have not been part of, of the second version, but the second version is really, really cool. Yeah, so I highly recommend you to try that out as well. Okay, now, I think, a Lester's more questions or feel free to ask? No, then I think we will move on and try to run a model that's a bit more advanced. And that no money that will actually a model actually also eat away our credits. Amani, a model. Sorry, it's been a long day model that is that's not free to run, because it runs on runway servers. And for that, we are going to go through the Browse view. And we are going to search for let me just see, I think I also put it up here, we're going to search for fast style transfer. Fast, and then that style transfer. And for this one, I'm going to click Add to workspace. And then it just gets directly added to my workspace. Then a weird thing is happening for me where I would normally just say start the model. Now it says start Docker and Docker is not available. So if you see this as well, I want you to click Show Advanced Options. And then click remote in run location. And then you can click Hide advanced options. Once again. I'm just going to do that one more time. So if you're seeing something that says start Docker, you don't want that, you want to click Show Advanced Options, then you want to make sure that the run location is remote. And you want to click Hide advanced options again. And then what we want to do is click Run remote. And now we are starting a model that's actually gonna run on another computer somewhere. So I don't know. So runway is based in Brooklyn, I don't know if they have data centers all over how they do their cloud infrastructure. But we're running this model, not on our own computer but somewhere else. And that is why it can take a bit of time for it to start and it's also a significantly more advanced model. So takes longer to just kind of start it up. Once it's started, I want to click camera and then hopefully I should be able to see myself in the input, but then in the output see myself in the style of kind of like a Cubist painting. And I see we have something in the chat. I am getting no results for the search. Joanna. Okay, let's have a look. So in the search, what you want to search for is fast style transfer and I'm just if you can also copy it in. So you Anna is getting no results for the search. You can you want to search for specifically fast that style tests that transcript. So, Paki again. igennem. I'm sorry if I'm mispronouncing it, I don't I haven't heard anybody everybody's name. So I have the same problem that I had with the last model, which is when I when it's the camera, when it's picking it up from the camera, like nothing happens. I could only use like the file. Yep. So ordering files, are you on Windows or Mac? I'm on Windows, okay. So on Windows, you cannot have the webcam turned on in multiple applications at the same time. Typically, you can't. So you want you want to try out at least and let me know if it works. Try to close the camera here in suit. And then try to open it in runway. And you might need to close the model and and open it again on potentially reopen runway. Try that and eliminate if it works. Sure. And just so I stopped the model, and then you know, stop the camera here and then try and see if it works on the model again, and then try and see. Exactly, yeah. And you and it works now. That's great. So can you remind How to Set run remotely? Of course. And thanks so much for asking in the chat. It's really nice that you ask these questions. Oh, it's perfect. No, no. See? So for sorted. Okay, great. There was this question how to run it remotely? That's kind of like Show Advanced Options. And then make sure you are set it to identical running. Okay, let's just me know how we're doing with this exercise. Some people have some issues, but they seem to be solved? Can are others able to run them the model? Yeah, a lot of you have your webcam turned off, but people say are saying yes. in the chat. That's great. And if you stop the model, okay, Simon says, I think it's going up slowly. Yeah, some time, the models can run a bit slowly, you can see that mine runs at like, I know sometimes to like it takes two seconds to, to to run a single frame, sometimes a bit less. So it can be a bit jumpy, we're doing it somewhere else. And runway, it's like it's getting better, and it's getting faster. But it is still a software that's in beta. So it's hard for me to kind of say how fast it will be. Of course, it's like, if you want to then save a single image, it doesn't really matter too much. How fast does it I guess unless you want to process 1000s of images, you can kind of just save one of them like this to save. And then it'll save like that still image. And you get like a really nicely looking still image of yourself in that particular style. You can also input like, separate things. So if you don't want to input a webcam stream, you can input a movie, and then you can actually process the movie frame by frame and then you get a very nice, smooth looking output in the end. So that can be a nice thing to do. Especially if if the inference time is a bit jumpy. Yeah, let's see. So for instance, I don't know what I got. I'm not gonna do that right now. But But, but you can do that like you can select them at movie file, actually. Okay, so people are asking me to Tony Lee, can you repeat image still capture please? Right. So I assume you mean how to run this on single images? Yes, how to download the one image. Just to download the one image the image I just like the image coming from The webcam feed being processed, or do you want to process a image you already have on your desktop, from the webcam feed from the webcam feed, okay, so I'm just gonna do this again. So if I'm running the webcam feed and my model is running, then it's kind of processing this all the time. Then if I want to save an image, I can kind of just click Save. And it will take, I guess, the last frame that was processed, and then you can click Save. And that will be saved to your desktop phones. Great. Thank you. My other window is covering up the bottom right hand corner. Yeah, and the quadrants are pretty small. And there's so much like this UI is like, it's good. But it's also it's cramped. So yeah, thank you. Okay. And I think we are we gonna move on? Because there's so much interesting stuff to show, I hope it's okay, that we're kind of gonna jump to another model. Is that okay? Or does anybody have more questions relating to this model, maybe thing to talk about, before we run and start off a new model is that you kind of want to stop the model running, because it's actually kind of eating away your credit. So it's a good idea. If you're not actively using a model to stop it, click stop. And then if you want to see how much credit you have left, I think you get, like $10, when you sign up or something like that, you can click your settings over here. And that should tell you how much credit you have left. So um, I have a lot of money here on runway to get 10 credits still, when you sign up? Do you have more? Do you have like a deal with them? No, you can, if you're doing more workshops, you can actually write them and they will give you sometimes free credits, especially if you're doing like a several day thing. They're very nice. They kind of just want people to use their tool. And okay. And of course, intelligence. Is it possible to change the style on this model? Yes, it is. So it by default is set to the Cubist style. But there are other styles. Again, pre trained styles you can select contains key Google with a map with Google Maps, which is really weird. hokusai, which is beautiful. all types of different ones. But and I'm not Yeah, I'm saying all types, there's actually only five. However, if you want to use your own complete, like unique style, then there's several alternatives. And that's the cool thing about runway. runway is just this nice style transfer and Dustin one way. But if you are interested in style transfer with maybe a kind of different architecture, or maybe the possibility of adding your own style, then we can have a look if you go to browse models. So I'm just kind of freestyling a bit I can't really remember what it's called. But there's one called dynamic style transfer. There's this one, this style transfer, there's this one, which is called style. ATIA in style transfer stylized images in the style of any image to seems like this one, you can actually upload your own style, which is kind of neat. Or this one arbitrary image stylization stylized images in the style of any image. So if you're running a specific model, and it's not really doing what you want, then try to search for similar models. And you can search by name, you can also kind of search by sometimes they have tax, or you can search up here in trending or by category. Yeah, so the category here would be style transfer, I guess. Okay, we gotta move on. And we want to do something called style game. So I'm going to search for style and then gam style again, maybe it's in two words, style can Nope. style. And yeah, I'm interested in running style games, too, I think so I'm actually gonna search for style can to style can too and the one I'm gonna run Is this one. So I'm interested in this one, this model here called style game two. style game two, I'm going to put it in the chat as well. And you want to select the one that kind of your web style can to hear. And it has the runway logo here. And caption is generate photorealistic images, improvements in image quality, this, okay. I'm going to click that one. This is a model trained by Nvidia and its gap. And model, again, is short for generative adversarial network. And it's a technique that allows people to feed a model a lot of images, and then it will be able to produce new images that that are very similar to the ones in the data set. However, they are unique. And we have some different checkpoints, we can try out we can try out faces, cars, cats, churches, or horses, the faces is the most impressive. So we are going to stick with faces. And then you want to click Run remotely. And this might take a bit of time, it can take like a few minutes to get it running because it's relatively heavy. So have patience. If I was a bit fast and actually selecting it, then let me just show you once again, which one it was like men's style game two, and then this one with the faces. style can't. Okay, now it seems like it's working. So So now it's actually running. And then what I want to do is I want to click vector. And this will then start to produce faces of people that don't exist faces that this model has dreamt up. And sometimes you can kind of see, especially in the background, there are hints that this is weird that these are not real people like for instance, this person's cap, like something's obviously kind of wrong. Whereas this one where the background is more neutral, this could strike me I would think that this was a real human being. So typically, like tells our what's happening in the background, or, yeah, sometimes it's more obvious than not other times. But it's gotten pretty good at generating new faces, these are high resolution faces, these are 20 1024 by 1024, high quality, you can navigate this space, this is kind of like what you call latent space. Because all of these non existing human beings, they live in a high dimensional numeric space. So they all have a position on on what you call a vector. And the vector has, I think, 512 dimensions. And now because we're on a screen, we see it in two dimensions. But generally, the idea is that the closer the images are to each other, like if they're next to each other, then the image and they are the faces have a lot of similarities. And if they're very far away, they don't have a lot of similarities, you can make that even more obvious, if you put the sampling distance down, let's try to put the sampling distance down to maybe 0.2, then you can see all of these faces that are kind of similar. And that changes ever so slightly. So what I did was the sampling distance, I lowered that. And then you see like, weird variations of kind of the same face. Here kind of like the hat change the spread and the classes but it's kind of the same smile. Okay, so let me know if this is if this is working for you. Are you able to run style can too with the faces. Cool. Anybody having any issues? Or any questions? Yeah. Well, I clicked on vector and then I'm just looking at a whole bunch of like empty thumbnails that it seems kind of stuck is it Just a matter of days loading a big data set, or is the model running? No, it's not running, it's just not letting me switch back from vector to. Okay? If the model is not running, you can see that my model is running. So I can click stop down here, if your earth doesn't have a stop button, it's an indication that the model is not running. And it's not running, it's not producing any images. So the model needs to be active and running before images will appear. Okay, so the vector isn't like in the input data set, it's already the output, like what's showing up in this case, in the upper window is already the output. Yes, the vector is just like, this is just an Leighton space, I have a hard time explaining it, to be honest. And but it's the way we navigate the images that it's able to generate. And we do that via a vector. And, and it's a bit misleading that they call it a vector, and then you see it in this two dimensional space, because actually, it's in 512 dimensional space, you could also just feed it a long list of numbers, and then it would generate a an image in that particular precision. But but they chose to do it in this 2d grid, which kind of makes sense. But it's not you can't like feed it your webcam as an input. That's not how this particular model works. Good. Other questions? We have some intelligence. Can we choose gender, race, etc. In this model, not in this model directly, other models are able to do something along the lines of that there's a model a few years ago called transparent, latent Gann, think, transparent, late Gann. And you could kinda like make, you could start off at a certain point, and you could make people younger, or older, or more happy or less happier off with a class or on with the classes. That's not something you can explicitly do with style can however, people are kind of researching, like, what the different directions in this latent space mean to some have kind of figured out that if you move in this weird direction, in 512 dimensional space, you you kind of generally lose the class or you gain here or you become more like Caucasian or something like that. But it's not explicitly built into the model, all the models will have something like that. And if you're really interested in that, maybe send in mail afterwards, and I can send you some resources. But transparently, can can can have at least a couple of years ago was able to do that. However, the transparent didn't get doesn't look very nice. Right now like these things? Ah, quite quickly. So yeah, 2017 faces look super weird, competitive. Okay, did anybody try out some of the other ones? Did anybody try like the cats or the cars or the churches or the horses? So those are I trained, like the more poorly trained, so it becomes more nightmarish, I would say especially like the cats, they're like, sometimes it's like two cats and one or like, eyes are totally weird. Where's this face, this one is so good. Maybe also because it's like, our faces kind of in a straight position. And faces are very similar. Like all faces have eyes, noses, mouths and so on. Whereas a cat could be in all kinds of weird positions or images, horses could have riders or it could be close to the horse or far away. So in general, like with these models that generate things, the more confined the the data it has been trained on, the more realistic the output becomes. Okay, we are going to move on to the next technique, which is kind of thing. Maybe a bit similar to what you were asking before about like, Can we maybe use the webcam as an input to control like something face related? Let's try that. Let's see. There was a question in the chat. Cats are terrifying. Okay, so Jana, try that out. I don't know maybe you can share something in the chat. You can actually upload files. So if you if you made like a weird cat and have a look at it, share it in the chat. That would be awesome. Okay, what we want to do now is we want to stop the diagonal, we want to go back to browse. And we want to see what's trending. And number three on trending is this first order motion model, which we're actually going to try out. So click that one, and it's going to be added to our workspace. Let's just see what happens in chat. Okay, so Sammer also had a cat. Oh, wow. Yeah, okay, the cat in the bed or like one kind of interesting. For people can interesting and oldest generative witness, then there's this mark, there's this website called this x does not exist. So maybe some of you are heard of the website, this person does not exist. But that's only one two, there's also this cat does not exist. This AR PMP does not exist, this startup does not exist. This blah, blah, blah, all types of weird generated things both with images with x. Okay, I added first order motion model to my workspace. And we are going to try that out for your input. You generally want to what run your webcam for. So that's the input on the on the left to driving face image. The input on the right, you want a nother image of a face that you are going to kind of puppet puppeteer. So I'm going to select a file. And I'll try to see if I can find the older version of myself. And grab one like, Here here and I want to see if I can control this version of myself with my own face in real time. And then I want to click Run remote. And while I do that, let's see what's happening in the chat. So to Anna so maybe posted like a scary capitalist Have a look. Oh no. Oh, this is so weird. These are like hell hell, kiddies. Okay. Okay, while this is loading, let me just grab some water. I'll be right back. Okay, so first order motion model takes my face as an input takes this face as a source image. And then Oh, this looks really weird. Oh, no, what's happening? Then I can like I should be able to puppeteer my own face. With that face. Maybe it thinks that my light is a bit off. Oh, this is really is looking really weird for some reason. I'm not sure what's happening. But either way it should like it's actually what? This this woman Jen and Jennifer Sykes used to control all of these talks. And I've also used it in other cases. I don't know for some reason it just gives very, very weird images back right now. Let me know how it's working for for you. Are you able to control another face? Yes or no? It's it's loading. Okay, it's loading a bit. Yes. So Samuel is Tony so if you have something fun, then maybe you can share it in the chat. I don't know why for some reason, it just would really freaks out. Okay, so So yeah, so for Janet's slow forum for Andrea. It's still loading. It is relatively be slow to run. It doesn't run a lot of times per second. Typically. I think for me, let me just try another image. See if it's, there's something wrong with this image. Second, try another file. Here. Gonna try this face instead. It's not a lot better. I don't know, maybe my webcam settings are a bit weird, because I'm also using it in runway. Orange zoom. I don't know. Okay, but at least some people are, are doing things. Let's see. Oh, and here. Okay. Yeah, either way. Oh, I want to stop this model, then I think I'm going to maybe just, let's just do a quick recap. So so far, we have been able to open runway, we have run marbles slowly, locally, we have had a look at different ways of using inputs. So you can use your webcam feed, you can use images, or you can use movies and other models that are maybe based on text, you can actually input text as a txt file. We have looked at exporting, so you can export to a JSON format, you can export still images, and you can also even export movies for certain models. And that was what we did with the local models that are free to use, then we tried to use some models that you run remotely. So we have a look at fast style transfer. We had a look at style 10. And now we had a somewhat not very successful look at the first order motion model that just really looks freaky, at least on the and I want to kind of no other any questions. So far. Anything that's kind of a bit puzzling to you or anything that doesn't? Yeah, anything like any questions? We have something here. Oh, now I can't see the chat. Oh, here's his term model. We can use for extracting visual sentiment. So I'm maybe that's my lack of English. But what do you what specifically? Do you mean by visual sentiment? It's always easier to say it out so that everybody can hear it other than typing if you don't mind. Of course. So like this sentiment analysis for text, right? How positive or negative you're feeling after reading this texture, what the polarity is. So I was wondering with this something similar for images, where, you know, maybe the images of greenery or have puppies or something, then something like a positive sentiment, or if it's a war, it yields some negative sentiment. Right? That's not something I have stumbled across in runway, but there could be so so runway has 100 and like 120 different models, and maybe one of them does something like that. The closest thing I've seen is I haven't seen anything with sentiment directly on images. So and I haven't seen that either on outside of runway. But But I guess you could make something like that. Like somebody could make something like that. It's not something I have seen. Thank you. No problem. Then Tony asks other artists using these image generated images as sources for 2d art such as paintings? Definitely. I think like one of my favorite kind of artificial intelligence artists is my Ukrainian man. And he like has tats stuff on. On Sotheby's art galleries, an auction And he's a relatively well known AI artist. So you can have a look at him here. Yeah, and there would be others. Like one of my favorites is actually Sophia Crespo. She is so cool. And she has a background in kind of, like by ology, or bio, bio something something. So she does like very organic looking AI art. And typically, like computer artists, not that organic looking. But she does it in such a nice way. Especially her neural su project is just like, so cool. So like, and she's so good. She also teaches workshops, and she's a good public speaker. And so, yeah, I yeah, I actually taught a workshop where she attended to I was very kind of Wow, you attempt by workshop is so cool. Yeah, check her out. And there will be lots of other some, yeah, those are just two that come to mind. Okay, now, I think we are going to because, of course, like, when you're doing something like this, you're running a bit out of time. So want to skip this next part about chaining models. I'll quickly explain what it does. And then you if you're interested, you can read the tutorial on how to do it. But the idea is that the output of one model can actually become the input of another model. So for instance, there's one model that looks at an image and tries to describe it in a sentence. So it could be something like this, where if it sees a person walking, like this helps write it in, the description becomes a man carrying a surfboard. But then you can also do the opposite thing, you can try to write a small sentence, and then a model called attention can will generate an image based on that sentence, it becomes a bit weird and uncanny. I can't because the space is not really confined or limited to something like faces, you can write whatever, and it will attempt to whatever. But it means that that it's a bit like nightmarish from time to time. But then I think you can have a look at and you can read the tutorial on it. If you're interested, it's that you can actually chain these together. Because one takes an image as an input produces texts as an output. The other text text as an input, produces image as an output, and that in that way, you can make like weird weird loops, where they send information to each other. That's an interesting thing, maybe a bit of an artistic thing to check out. Instead, what I want to do, and this will be again, yeah, relatively short. For the next 10 minutes, I'm going to give you some time to explore on your own, but I'll be here and I'll help you kind of debug if something's not working. The idea is that you're on your own to try to get a feel for some different models in runway by experimenting with it. You need to have a bit of patience when you're running them, because it can take a bit of time to get them started. If your model just hangs for two minutes, or more than try to restart the model or potentially restart running. And then I put up some recommended models for you to experiment with GPT two is a model that will take your input text and try to continue it, which is super, super interesting. spayed landscapes, is something that lets you kind of draw with colors. And then it will try to produce a landscape image based on weather where you want the landscape to have grass, trees, skies, etc. Pass net is a new model that's able to remove background from an image. So kind of like Photoshop, magically just remove everything. But the foreground. Photo sketch is also really cool. Y'all act all types of cool models to try some of these out or explore your own. And then what I would encourage you to do is to post maybe some of the most interesting findings in the chat. So you can also inspire each other. And we're going to do that for 10 minutes. hope that's okay. So for 10 minutes, try some of the models I'm here to help if anybody gets stuck, or if you don't understand how a model works. I've tried most of them. So are you people okay with that? Cool. And yeah, you can either like ask if you're stuck verbally or within the chat. And in the chat, you can either like write me personally or write to everybody. If you write me personally, I'll reply perfectly. Okay. So do that for the next 10 minutes. And then we'll try to wrap up a bit afterwards. So how say ask the question. And it's about connecting runway to other software. So if you're interested, you can all listen along. It's a bit of a, I need to explain it kind of openly, and also show you something. So you can just ignore it if you're not interested. But I'll kind of show you a bit. She asked about connecting runway to something like Ableton. There's not a specific Ableton bridge right now. But But are you also doing something with software like Mac's? Yeah, I use max for live a lot. And, and one, so I'm just not sure what the I saw the Ableton logo. So at the end of one of the runway videos, and I was intrigued by that, the sort of audio the audio potential or the, you know, converting between visual and audio, and all that kind of stuff, which sounds like it could be really interesting. Yeah. Um, so let me just Am I still am still sharing my screen, right? Yeah, I can see. Yeah. So what I'm gonna do is, I will, on this main website, which you can also access after the workshop, I have made like, a tutorial that we're not going to go through this explains how to connect one way to kind of like, some software running localhost with p five years, you're not necessarily interested in that. Maybe you can use you're not using processing, either. You're only kind of using MAC's for life. I mean, yeah. But I'm sure there are similarities. So I'm happy to have you on to explain one of them. I just don't know, really the mechanism for connecting these things and how they go about doing right. Okay, so maybe I'll, first of all, I want to, I want to say that in some runway has a GitHub page. And on that GitHub page, I think there is a specific sub directory for for max for life, to take a look at that. charts to sign or grasshopper, their marriage maximum speed goes Maximus P. Okay. So it's not directly for max for life, but it's for max MSP. And in this, there would be a series of examples. And generally, that there are different ways of connecting these things. You can do it with OSC messages. You can do it with socket. io. Or you can also do it with HTTP requests. I don't know which release are kind of supported in Max, I would think like OSC is definitely supported and maximum, maybe that would be my, my go to. And then just like added over, like I had a like, like, the quick explanation is kind of every time you have a model, let's say you have a model like Coco SST, and you're running it via the camera, then what you might be interested in is having a look at this Network tab. Because then you can see like, what are the different options, you can do it with HTTP socket, or C or JavaScript. And then with OSC, for instance, you can kind of have a look at what is the input so let's say you want to send an image to runway from an external software, you want to do that over OSC and, and it needs to be an image of this format and then you need to kind of Like, give it the name label. So I'm not that good with story, give it a name image, I'm not get good with OSC protocols. Sure, you kind of need to follow that. And maybe you'll find an example in the, on the GitHub. And then for outputs, it gives you back boxes and labels. And these are kind of arrays of numbers. And you can check them out. If you have a look at what is kind of going on, oh, I have so many things. Down here in the, in the show data, you can, if you remember, the cocoa one kind of shows you the data here. So this would be an array of labels on a radio. So we could basically just parse that in max on the and run that within Ableton Live and then do something with that data as you definitely Yeah, like you could generate music from the objects around you. So the banana triggers the saxophone or whatever, right? Or the closer the banana the like, the bigger the bounding box, the more of it like that. So you basically would have runway running the local app running, and then it would be outputting in real time, and you would have Ableton running also, which would mean in putting it up into any other direction you want to go. But basically that both applications running simultaneously, we could share the data real time like that. Yes. However, runway is also they have added last week at API kind of functionality, which is only in in beta, or in or like enclosed testing, and I haven't tried it myself, but it actually allows you to run the runway models without having the software open on your computer. Gotcha. Yeah. I mean, if you're not using a local model, the software is just sort of unnecessary in some way. So hard, you know, the desktop software. Cool. Okay, sorry. I didn't mean to take the whole 10 minutes to discuss this. But that was extremely novel, and I really appreciate it. And then you said, Well, that's a VST. And not that I know of, but maybe I'm not really using Ableton, but Sure, I can look around for that stuff. I can I just, again, generally was curious as to the mechanism, we're connecting and, yeah, but as long as it's kind of oversee, you can like make, make connections. Yeah. Okay. Any Any other questions? Do you know Mr. software called we can either okay. So there was like a private question. Sorry about that. But for for Pulau. Yes. I've talked a lot with weekend either. I have actually have a lot of examples for weekend either. Definitely. And we can a lot can can also do healthy what what you're thinking of? Maybe, like, if you're interested in that you can we can have a chat about it. After the workshop, I think we've done a lot of, of music stuff right now. But, uh, cool. Yeah, definitely. I know, we can either. And I've made a lot of bridges between weekend event processing and actual Ableton back in the days. But I don't use weekend a lot anymore, because it's not been updated since I think Rebecca foodprint, updated in 2014, or something like that. haven't really seen an update since. And why did you ask? And I just wonder, like, if we need to open the stand alone app, to to connect to other software, and like, email, standing runways more into Visual emulate action. And so if I want to create, like more like generative music system, and I tried waking later, several times, and I also feel is, is simple, but runways is much more powerful. But that to my question is also similar to the previous one, like, how can I integrate different software? And again, the general answer to that question would be, I think over OSC because we can eat a software that that communicates with other software's via OSC, and you could potentially make, like, let's say you want a runway to, to do something with we can either you could actually do it overseas, maybe you would need something in between to kind of translate. So the format's are right from one to the other. But it's do a poll and in a way, I also see runway as a as like an a next step, we can either, although we can use a bit more focused on training, where runway is quite focused on running existing models, but that is changing. And that's actually maybe something we'll get to just just in a second about how you can actually also train stuff with runway. Okay, so some lt like an interesting thing. Sample before the workshop, SAML trained on model, and Oh, actually, I'll put it this image that looks beautiful. Nebula cool. Okay. Okay, I think let's wrap up this small exercise. Let me just know, like, how did it go? Were people able to try out models? Anything that was interesting to you or anything that where you kind of get stuck? Or let me know how it worked? anyone you want to share anything? You can also share, like a bad experience. Oh, it never started even. Okay, do the deep deep privacy and the results were frightening. Oh, explain what so what what did it do? Well, it seemed it seemed like it was looking for a particular type of where the where the head was facing, it was not good at side faces at all, or with any sort of, if the hair went in front of the eyes a little bit, then the face would be completely it just didn't know where to put the rest of the, the rest of the features. So I had, you know, to a photo where I had one, one person with bangs, whose face was completely, you know, not replaced, but like very much distorted in many ways. And then a man in the background, whose face was almost perfectly replaced by a generated image. So, okay, pretty, pretty interesting. The way that you can see these different these different results happening in the same image. Yeah. Nice. Any, any other experiences people had with with models? I enjoyed playing a lot with the landscape one with the landscape. And, you know, immediately kept wishing they would have more elements. Because you know, yeah, it's kind of like it's really interesting. It's really interesting, just kind of idea generator for grading these more abstracted kind of images, not really over landscape even right, but yeah, but you keep wanting to have more colors in your palate, right? It's kind of immediately It's funny how it's so powerful. But it's also immediately feels limiting, right? It's this dual kind of thing. Yeah. And, and I feel like there is also a model called spade, Coco, which has like hundreds of categories. But but it doesn't really give you good results because it's less constrained. So to landscape one skips, you can have neat results. But it's also very limited. And that's like, in general, the more limited the model is, the more realistic you can train it. I just want to show you a quick thing. Am I actually still? I'm getting a bit confused with zoom. Am I am I still sharing my screen? Yes, I am. Right? Yes. Okay. So on my website, I made another kind of weird little project. But based on on that model you were just mentioning, where I tried to generate PR landscapes from images of perler beads. And this also talks a bit into what some of us were discussing in the just before, like how to connect this to external software. So let me just show you this small video. And yeah, let's have a look at this. I made this landscape with perler beads, and I will now position it underneath small camera like so. Here we go. And then over here I have a sketch written in processing that's able to find the closest color of all the pixels. perler beads are really hard because they have holes and they shine. So we need to blur it out quite a bit. And then more or less the the beads are now associated with colors that have meaning for a model called spade landscapes, which I will send this image to this has cemented Meaning of clouds sky, river grass and tree. And I can then send the image to the runway software. And then hopefully I get something back over here that looks somewhat like this. Are you able to see the video? And could you also hear the sound? Yeah, cool. Okay. So that's kind of one example of like, connecting it to an external software because then you can like connected to unity openframeworks music whatever. Okay, we are kind of running a bit out of time. Is it okay if I go five minutes overtime? Because we should kind of started five minutes late Is that fine? If people got to go, they got to go. I don't, that's totally okay. And because I want to maybe talk about for the next five minutes, how you can train your own model, and also some future resources if you have gotten inspired by a five runway so far. So I will go back to the website. And if you're interested in kind of like training a model, then there's a really really good quite long video tutorial here, where an American artist and programmer Jean Cogan, explains how you train a model, like from the very beginning, inside runway, let's see what happens in chat. The post the link to that one. panel, do you mean? The do you mean? the interest of website that know the intro to model training and runway? Okay, so the website itself? Yes. Okay, so I can find it there. Yeah, you can, I can also, I can also share the link to it directly, I guess. They run with you. It's also here directlink. Hey, runway fans, this is G oops. There you go. But basically, this, this video kind of take you through all the steps. But it's not that complicated actually. And just like at a glance, what, what you do is you go to training. And then you can right now select two different models that you can train, you can either change train a generative image model, or generative text mode, the text model is based on GPT. Two, that's the model that's able to kind of get a prompt and then it will continue your text. And the generative models are based on either style can or style can tube, which kind of they both do the same, you give them a lot of images, but they will try to produce new images that are similar. So what you do is nicely explained in the video. But I can also kind of like take us through some of the first steps, you click train a new experiment, you give it a name. So I'm just going to call my lead because I wanted to leave it afterwards, then you upload a data set. And I've tried it with several different things. So I tried it with like stamps, Danish stamps, I've tried it with fashion stuff, I tried it with old photographs from the 18th century, I tried it with weird images of plastic backs, etc. Or you can also use a existing one an existing data set. And it's recommended that you upload at least 500 but I would say the more the better. Then there are ways of pre processing it so you can like crop them. And and that's also it's a it's a bit of a technique in itself. But I have a link to guidelines for pre processing your data. And then you can run it and let it do its thing for Yeah, like typically takes like around half a day or something like that. And and the first model you train with runway SHOULD BE FREE when you sign up as a new customer. After that, it costs I don't know I can't really remember but it can easily run up to like quite some dollars. So you should first Experiment wisely because it's free. And then once you've trained a model, then that model should be available to you in models. So for instance, I can have a look at, like new models next season, I actually trained this weird model on, on, on it on people from the 18th century. So I can run this model just as well as I can run any other model. But this is a model that's unique to me, I'm the owner of it, it's only me that can run it. And it's trained on images that I decide. And if you're kind of interested, you can have a look at yet another weird artistic project I made using that using runner, which is this one, I'm going to share that in the chat, where I created kind of new people that don't exist. And I also generated a biographies of them. So kinda like a weird fake news, but about people in the 18th century. You can have a look at that. So let's see if it's running. And I see that it's actually running and it's able to generate, like, some of these people are kind of realistically looking. But some are also weird. More between one person or two p person or, like, close up or not closer, but kind of interesting aesthetic, at least in my opinion. Yeah. Okay, so that's, that was kinda it. It was a very, I felt like I I talk for days. I hope I didn't talk too much. I hope you enjoy it. And maybe if anybody has any questions now, or comments or thoughts, then let's share those. Thank you so much for hosting this workshop, it was incredibly helpful. For the participants, I just want to note that I'm sending out a follow up email now, that has the interview that we conducted with the founder of runway, who, by the way, was really excited to hear that you're instructing this workshop with us. And yeah, so just look out for that email. I'm also sending out a link in the chat right now for a survey about you know, what you learned, what sparked your curiosity, we just want to know more about your experience and things that you all are interested to learn more in the future. So I didn't mean to hijack the question section but and cool that you had a talk with the Crystal Palace. I have only met him in person once but I like a weird story. So runway was his thesis project from when he studied at ITP in New York, and me and Jean Cogan, it's such a small world, this AI artwork, but the guy who's explaining how to train a model. In one of the videos I refer to mean him sort of workshop in Costa Rica, where we used his tool for the first time in a public workshop. And slides like images from that workshop was part of his thesis presentation. So it's, it's just a small, small, little world, even though I'm in Denmark and I Yeah, I know. That's the sense I got to that. But I art community was quite tight knit. So it's been cool to see all these connections. And thank you so much for for joining us here today. Great. Well, I think we are over time. So if anyone doesn't have any questions, I'm sure you can reach out. But beyond that, thank you so much for attending this workshop. And, yeah, Is there time for questions or not? I'm just hanging out. Like, I'm not going anywhere, you can ask me. And then I'll also say for people who might be leaving now, I'll leave the website up and in the website. So so my context so if you have a question kind of lead to stuff I yeah, feel free to reach. Great. Well, thank you so much. Yeah, it was a great kind of on ramp to using this tool. Because you know, I've been looking at for a while but it's good to have more of a firm place to start and we'll definitely refer back to that webpage. at a specific question about a southern it's potentially possible to run because I am curious. about making some of these things run more in real time. Ah, and it seems like maybe if you have access to like a computer with a beefy GPU, you could maybe do that. But I'm not totally clear. Like, what would be the steps to do that? Yeah. So and that's also like now that the, this is not being recorded. Is it, though I don't see is from all recording things. It's not being recorded? I think it is actually. Yeah. Oh, it is. Okay. Now, I can say it either way. I'm just joking. So some runway, kind of like it's a commercial startup, right. So they, they kind of want to encourage you to run the models on a website, or sorry, via their servers. And sometimes that can be a bit slow, sending all of this back and forth, and you don't have control over which servers run it, which GPUs run it. However, and that's a really nice thing about runway is that they allow you to run models locally via this thing called Docker. And if you are on Linux, and right now, it only works on Linux, you can actually run it on your PC GPU. And on the website on notion, I made a couple of links, if you go kind of all the way down, down to see other runway links. And I have a link here, I call it this called running models for free with Docker on CPU or GPU if you're on Linux, and it kind of takes you through all the steps of running it on CPU, which I like, personally, I have it running, I can run models for free on my Mac, but only on CPU. And then if I run around them really, really fast. I do have a kind of like a gaming computer with Linux installed. And, and there I can run models on GPU. So it's possible. It's not. It's a bit it was a bit tricky to set up, honestly. But it's possible. Okay. Yeah. Other other questions or comments or thoughts? I can keep throwing questions at you if nobody else. Well, I'm curious also, as somebody who teaches, and kind of new media are, if there are some good resources, kind of online resources for students just for kind of this stuff gets outdated, so fast. So you know, the last time I saw a compiled version of resources, I think memo Acton made something a few years ago, but I didn't know if there are some go to resources for you for seeing what's the newest art that's being made? Or, you know, theory that's being written or that sort of thing? Yeah, there is. There is this website. You gotta give it give me like 40 seconds, I'll try to see if I can find it, because I haven't put in another notion. So I need to log into that with another email. I have at least something. Because let's just see. So, one thing, there's this page called Deep index, I'm not 100% sure how updated it is. But it's kind of a page that promises to keep track of what AI in a very broad sense can do and where it's being applied. And of course, some of it has to do with like, health care of finance, but there's quite a lot of links in terms of creative things. It's so and if you're like more specific about, like the art scene and the overlap between AI and art, then there is this UK curator, lupa Elliot I think, who specializes in Canada like being up to date with AI. Art, and I am pretty sure she has a newsletter, which you can subscribe to. And I think that's like that would be like like cutting edge what's happening this week ish. Yeah. So those are my two thoughts right now. Yeah. Thank you very much. Yeah, no problem. What What do you I teaching new media art and at MIT or Connecticut College that's not far from here. Okay, nice. Yeah, we have a center for Arts and Technology. So it's a lot of intersections of a lot of different fields basically. Cool. Yeah. Okay, then maybe we we call it a day and I bit late for me. I've also I taught this morning as well like Danish students or as of at nighttime teaching Danish graphic design students. So yeah, oh, I think I'll slip off. Okay, but very nice to meet you all. I thank you. 