 Alright guys, welcome back to Matchue GANs. I'm so glad you guys could come back for a second episode Today we're going to be talking about data augmentation and gradient penalties, and we're going to be talking about doing more with less So we want to do more with less data So to start off, let's talk about data augmentation. So today I'm going to be generating pictures of motorbikes Unfortunately, though I'm not really that good at scraping data. And so I've only gotten 961 images of motorcycles and that's the best that I can do now when we're training generative adversarial networks 961 images is not very much data So what can we do to generate more images and better looking images with such a small amount of data Well today we're going to be talking about data augmentation and gradient penalties These are both things which will help to stabilize the generative adversarial network training process So first, let's talk about data augmentation data augmentation Simply put is just a way of getting more images out of just a set of images. So to start the most basic form of data augmentation Which I actually did in the last episode without telling you guys is just simply flipping Horizontally all of the images in your data set. So what I did is with all the flower images from last episode I simply took them and flipped them all horizontally. So we got double the amount as we would usually have But the next one the one that we're going to be implementing today is called random cropping or something like that You can call it whatever you want So the way that we do this is by When we resize our images, we resize them slightly above what we actually need them to be So as you'll notice later, I have resized all my images to 80 by 80 instead of 64 by 64 and So what we will do during training time then is that we will randomly select a 64 by 64 Square of that image and we will crop it out and that is going to be the training sample for that use case So here we can see an example of this where I've gotten red squares put around Just random areas in the images of these motorcycles And so this is the kind of idea that we're going to be implementing today as well as gradient penalties So now moving on we can talk about gradient penalties First let's let's talk about these few pieces of code. So first we're going to be importing our packages Same stuff simple as last time and next up, we're going to be setting our hyper parameters and Setting our statistics. We did not see the GP loss list last time But this is going to be going along with our gradient penalty so we could talk about that in a little bit So now we want to import our data. I called it motorbikes80.npy This is simply because these are 80 by 80 images of motorbikes Next we want to normalize the data and let's see a couple examples of what kind of data we're working with today So here we've already implemented a little bit of data augmentation So we will select two different random values between 0 and 16 for the cropping values one will be for the x-axis And the other one will be for the y-axis so now we will want to grab our real images and Crop them along those x and y axes that we got just a moment ago Next we will want to plot these out so that we can actually see these images. So as you can see we start off with 1922 images and then we can continue on to actually look at what these images look like as you can see regular motorbike images Finally now we can go on to look at what our gradient penalty is doing so the reason why we want to insert a gradient penalty is because sometimes Instability can come in a generative adversarial Network and a lot of times this comes from the discriminator having something called Exploding gradients. This means that the discriminator When telling the generator what to do it will randomly say whoa You should change these pixels by a very large value And we don't want it to do that because when it wants to change values by a lot Then the generator will change its weights by a lot And then the discriminator will then in turn change its own weights by a lot and these will go back and forth until eventually They're just completely out of whack and what's called collapsed in which case none of the images Look very realistic and it's very bad. This is especially common in datasets where we don't have a lot of images so what we can do is we can directly penalize these gradients at the beginning of our Discriminator so that we can make it a little more stable and a little less explosive So we are going to be implementing what's called an r1 gradient penalty today So we will start off by getting the gradients of the image now we're using Keras as our back-end (engine) for this as usual and So to get the gradients we will get the K dot gradient function which takes in two parameters first it takes the predictions that the discriminator had and then it will take in the samples that were given to the discriminator in order to get to those predictions So now that we have the gradients of all our different weights. We want to just take the first layer The reason we're only taking the first layer is we only want the differences in pixel values that I mentioned earlier Next we will get what's called the norm of this you can think of this as the distance So first, we will square all of our gradient values That's all the different pixel values and then we will add them all up for each image. So that's why we use K dot square And then we use K dot sum but we only want to do it in three different axes because we want each individual image to have their own value and Then finally we want to get the average over all of those So that seems simple enough so now we can move on to the other changes that I've made to this code since the last episode so I've also kind of automated the G block and D block so last episode we had simply written out up sampling convolution and Activations and that sort of thing over and over and over again And I simply turned that into one simple function so that we can simplify the making of our generator and discriminator. So starting off with this generator block, I've simply said, okay Well, we're going to take an up sampling convolution and activation block same with the discriminator block convolution activation and pooling so now we can look at our generator and discriminator and we can see that they are much simpler to build so we just do Reshaping the latent vector G block G block G block G block and then an image output. So that is quite a simple It's quite a simple method compared to how we did it last week and same with the discriminator We add just discriminator blocks after discriminator blocks till we finally get to the end of the model So we are now moving into a tricky spot in which case we want to train the discriminator with the gradient penalty But we are moving into this tricky spot where we are not using true labels But we're actually just using the predicted labels and the samples that were input So now we have to kind of work around this and add that third variable to the loss function So what we're gonna do down here is have a partial gradient penalty. So this is a partial function So what we want to do is we'll start off with our r1 gradient penalty function and then we'll want to set Exactly what all of the additional Values are so here our samples that we're inputting are going to be the real images that we input into our model So now we have a partial GP loss function, which only needs Y true and Y predicted So now when we compile our discriminator model We can simply use the same inputs as last time But now we have an additional output which is just the validity of those real images the judgements or predictions of those outputs Now we can use three different loss functions the mean squared error and mean squared error that we use last time and finally our partial gradient penalty function Next we will want to use loss weights. These loss weights means that we're going to give special treatment to certain loss functions We're going to weigh them in more during training The reason why we want to do this is because the gradients that we're penalizing are going to be quite small Very much smaller than the actual Discriminator loss functions that we use typically so we will want to give this a weight of 10 so each of the lost values from the gradient penalty are going to be multiplied by 10 on all the other losses are just multiplied by 1 to stay the same and So now we've compiled our model and we are ready to move on Of course as usual we want to also compile our generator model This one is the exact same we don't use any gradient penalties here as we only want to stop the discriminator from exploding But the generator is typically pretty well behaved and so we don't typically need any gradient penalties on it Now we can move on to our training loop So we will start off by getting our real labels our fake labels and our dummy labels Our dummy labels are what we're going to use for the gradient penalty loss function. We don't actually use these labels as You recall we don't need any true labels. We only need the predictions and the samples. So these are just a placeholder for Keras Next we will want to do the data augmentation that we did earlier So we'll want to get two random values between 0 and 16 for the cropping values grab the images and crop them as so finally, we'll get our latent vectors as usual and we'll get our loss value append the lost value of the discriminator and then finally Append the loss value of the gradient penalty, which is going to be very interesting for us to look at later Then we will want to train our generator this one again just very typical You'll want to grab your latent vectors train your generator and that's about it So now this has been training for a little while already but I just wanted to get to 50,000 iterations so that you guys can see some really good looking images rather than last time where we only had 10,000 iterations So I will get back to you guys shortly when this is done. So I'll see you guys then Alright, welcome back guys. Now we've gone through all 50,000 iterations And now we can take a look into what kind of results we've gotten from these new stabilization techniques So we can generate a couple new images and as you can see here, we've got a couple You know motorcycle look images. They're not perfect of course, but again, we started off with very little amounts of data So here we can see a lot of features that you might find in different motorcycles So maybe a wheel maybe a back wheel back here and we've got like the actual parts of a motorcycle that you can see whereas if we hadn't done all this data augmentation or stabilization We might not have actually been able to pick up on these features And of course if you want better images you can train it for much longer than just fifty thousand iterations And now we can go down and we can look at our loss functions So as you can see the discriminator and generators stuck around the same point throughout most of training But our gradient penalty values ended up starting off by spiking So this is kind of expected as they both start to initialize themselves in the right area, and then it just slowly but surely Went up and up and not as it became closer to collapsing so you can imagine as if we didn't have all this data Augmentation instead of slowly going up in this gradient penalty. We would be going up much faster Until eventually it became very unstable very big gradients But here now that we have our data augmentation it's done it very very slowly And so now we could probably train this for maybe a hundred thousand, maybe two hundred thousand more iterations before we actually collapsed Collapse typically happens when the discriminator will over fit and that's when the gradients might explode So in other words it might memorize the images that it's been shown All right, so that does it for this episode guys. I'm glad you guys tuned in I hope you guys learned a little bit feel free to leave any questions down in the comments and if you enjoyed the video leave a like and uh, Thank you for tuning in. I'll see you guys next time! 