 [Music] hello and welcome back in this video we will look at generative adversarial networks these are a class of generative models which do not explicitly model the data distribution but rather provides a sample from it and the sampling is performed using a deep neural network the net Newell Network which actually provides samples takes us input a random noise vector and then map's it into a sample of the model distribution so let us say we have given in your given training data write V data of X which is in the provided based on these samples provided so X I where I is 1 to N what we want to do is to model some provider you want to do is determine this model P model of X so that it is and a good approximation of P data the true distribution okay so this period of X once again we don't actually have access to all the data that is possible so we only have samples from P data of X and we want to determine some p model it's basically the probability distribution of X a model for that so that we can sample from it now in this case we do not explicitly model so in the sense it's not a parametric model but rather this is accomplished using a deep neural network neural network actually generates a sample from the model distribution so we will see how this is done the generative our adversarial Network framework consists of two neural networks one the generator and the discriminator the function of the generator is to take as input a random noise vector and transform it into a sample from the model distribution and the discriminators job is to it sax extracts like a classifier wherein it tries to determine if its input data X came from the generator we will call them as fake samples or was it from the actual training distribution those are the real samples it's called adversarial because the generator is constantly trying to fool the discriminator into believing that into making the decision that generated by it is from the train distribution while the discriminator is constantly lying to to learn the play so that it always determines whether it's called an adversarial Network because it has these two networks generator and discriminator trying to work against each other like as I mentioned earlier the generator constantly trying to generate samples that will fool the discriminator into classifying it as coming from the training data distribution so in that process the weights of the generator learns a transformation which which enables one to convert the random data random noise input vector into a sample from the model distribution so just to have a illustration of what we discussed so the generator G takes us input the generator G takes as input a random noise vector also referred already know denoted by denoted by Z also referred to as the latent space the random noise vector is then given at the which axis input to the generator G which then outputs some samples generated samples of the which are which are hopefully similar to the training data distribution the discriminator d takes as input your training data so again these are samples are nothing but your training data which are made available to you so for instance if you are interested in generating faces then you would have a database of faces of different people and that would be the input to a discipline ater so it won't be a general-purpose algorithm it has to be specific to a certain task so the training data it takes as input training data which are labeled as real and the generated fake samples again they are labeled as fakes or you can say this is 0 label and this is the one label so discriminator alternatively tries takes us in put the sample strain data samples as well as the generated samples and outputs an error function so basically it's the output of the decimator which are basically outputs a probability of the particular sample being real or fake ranging from 0 to 1 1 now this output is what provides the signal or the error signal to train the weights of the generator as a lasting discriminator so how is that done this problem is formylated as a zero-sum game so to speak because the generators if you call if you denote by J of J subscript G as the cost function of the generator then it's basically the up the negative of the cost function of the discriminator which is denoted as J subscript D so the cost function of the generator is what is given here basically and this is also referred to as a value function it is actually a function of two sets of parameters one corresponding to the discriminator and other corresponding to the generator so the this is optimized alternatively this there is an inner and outer loop so the inner loop is maximizing the this value function with respect to the discriminator network parameters in the outer loop is minimizing this again the same objective function with respect to the parameters of the generator network so let us take a closer look at at the cost function itself - the first term is log D of X which is nothing but D let's take a closer look at this cost function so if you look at this cost function which is expectation of log D of X plus the expectation over Z log of 1 minus G of Z so while all this what this means is that you will calculate log D effect with respect to the training data samples and you will calculate this term with respect to these samples generated from Z okay so this is very similar to the binary cross-entropy assuming that there is an equal number of generated images and equal number of trained data samples okay so this will we will see what how this cost function makes sense minimizing this or maximizing this cost function makes sense in the context of the generator at the SIL Network so here D G of Z is basically the output of the discriminator when the generated images are are given as input so D of X goes from 0 to 1 basically you can think of it as a probability of this particular input sample belonging either being real or fake and similarly in your using the generated samples it would be d G of Z which again go between 0 to 1 ok so so if you look at this cost function so when you start training so if you look at it the discriminator ideally the output of discriminate it should be 1 whenever X comes from the training data distribution and the output of the discriminator should be 0 whenever the input comes from the generator so initially when the discriminator is not sufficiently well-trained the way the weights of the estimator or not are still random then let us look at a particular case test case wherein we have real data here and some fake generated data which are given as input what is shown here in this black - headlines is basically the decision boundary given by the discriminator so here there is one miss classification of real data here right here and there is one miss classification of generated data okay so I will think to the left of this line left of this line is basically class 0 everything to the everything - sorry the everything to the left of this line is class 1 routing the right is class 0 now when the discriminator misclassifies so which means that d of x is 0 when X comes from attain data ideally the output should be 1 but instead it says 0 and you can see that log D of X becomes a very large negative number right similarly when you take the discriminate ake the data which comes from the generator ideally you to the output should be 0 but then if it says there's a miss classification then once again D of G offset is close to 1 which means that log of 1 - that becomes a very large negative number correct so this is the case when the discriminator is not performing optimally on the other hand let's say it's trained very well and you see that the samples are correctly classified in the same input real data as well as fake data here you will see that these this is the decision boundary right here and everything above is class 1 everything below is class 0 which is case the decision boundary is correct then the ID of X is close to 1 then log D of X goes to 0 similarly D of G of set is close to 0 then log 1 minus that which is again close to 0 so you know your cost function varies from a very large negative number of minus infinity to a close to positive number which is like in this case 0 ok so maximizing this cost function makes sense so that maximizing this will lead to the discriminator performing optimally similarly when you try to look at minimizing the same value function or cost function with respect to the parameters of the generative Network now if you take the first term it does not have any parameters of the generator network so we will not consider that so we will only look at this particular term here so minimizing this term what does it mean minimizing the likelihood of the discriminator classifying the fake samples as say basically that is what we are trying to do here with this cost function so this is minimizing the cost of correctly classifying GS 0 okay however it turns out that this cost function saturates very quickly okay because that's why you see because initially when the generated images are of very poor quality then the discriminator s has no problems figuring them out as belonging to class 0 okay so then what happens is that the output of the earth output saturates so if you take the derivative which is what gives rise to the signal that we back propagate throughout the network derivative becomes 0 so there's not much to back prop ok so instead it is replaced by maximum of log D G of Z so what this does is to maximize the error of the discriminated network maximized error and it's came to Nestor in the sense that it in correctly classifies D of G of Z s 1 instead of instead of 0 so ideally what I trying to do is to force DF G of Z to be close to 1 rather than it being close to 0 ok that is what this cost function does so maximizing the error of your discriminating network is what this cost function does and this provides this is this is a heuristic and it actually makes it better for training the neural network so we can just check that very easily so once again we have real data and fake data being fed into the discriminator so when the network discriminated network correctly classifies the output from a generator as being close to zero then it becomes a very large negative number then log of D of G of Z is a very large negative number on the other hand when let's say the disc the generator has progressed to a point where it's generating very realistic samples in that case let us say the discriminator comes in correctly in this case classifies the output from the generator as belonging to class 1 then you know that log of D of G of Z comes close to 0 ok so then again once again maximizing this cost function with respect to the parameters of the generated network leads to the generator managing 2 output samples which are very close to the training data distribution okay so let's look at this learning process in the terms of a 1d distribution this is again from the paper given I recited at the bottom so let's we have this distribution data distribution in black shown here and let's say this is the model distribution from which which is learnt by the generator network and this is the the blue line is the output of the this is the discriminator response okay you can think of this as addition boundary so initially when the training is more initially when the training is not great we will see that there is some miss classification by the discriminator okay so the miss classification you can judge by saying that all points to one side of of the discriminator addition boundary is classified as real 12 points on the other side let's say this side is classified as fake okay so then after updating the discriminator so you do several epochs to the discriminative network and it gives rise to a addition boundary which is much better right now so it's able to correctly classify samples to some extent from the real versus the generators or output okay so then what we do is in this case just to have explained further so this said this axis here this you are looking at a 1d problem so this axis say this is dead this is the random noise vector this is the space from which we sample the random - vector so and the generated Network Maps this two points in X so X is the data it wants it onto a data axis and the y axis here is the probability okay this is the probability density function is what we are looking at so so Z is mapped to X by the generator and it gives rise to this green line okay which is what we are trying to change so after updating D the blue line the blue dotted line is the discriminated response it's getting better at this at discriminating between the data distribution and the model distribution however another another epoch of updating the generator network leads to the green distribution moving closer to the dotted black distribution which is basically the model distribution is correctly approximating the true data distribution and once training has been and once when the both the discriminated on a generator have been optimally trained then you get to a point where in the green and the black dots or coincidental and the discriminator is unable to clearly say which is what so in a sense that the output of the discriminant is always 0.5 which it's not clear whether the data belongs to the training distribution training data distribution or a or it came from the generator okay so this is what the processes so you alternatively train the discriminator and the generator to a point where as an equilibrium where in the the discriminator is not able to distinguish between the samples coming from the training data distribution or whether it's coming from the generator distribute the distribution approximate buddy generator so just to walk you through this is again from the paper and just to walk you through the steps involved in the algorithm so you sample a mini batch of noise samples so remember they are input to the generator are this random noise vectors either from uniform noise or Gaussian noise sample n M of them M is your mini batch size and you also sample a mini batch of M training data ok so again once the sample we of course we run it through a generator to give outputs in the form of the data so then you update the discriminator by ascending on it stochastic gradient so we saw that we maximize the probability of the discriminated when we are trying to train it this is this cost function and once that is done this again for K steps so we said this is that loop we are in for K steps and once again we sample a mini batch of M from the noise fry this is called the noise prior so this distribution from which you sample Z is called the noise prior and then you update the weights of the generator again doing in this case the the original cost function is log of 1 minus that remember that we replace that by log of D of G of Z so we have to do gradient ascent on it actually not gradient descent so this is gradient descent to maximize this cost function so so this alternates so typically you do one step of or several one one or two steps of the discriminator and then go back to the generator and train its weights okay so in this particular construct this gans construct both the discriminator and generator are neural networks so usually stochastic the many math gradient descent is used for updating the weights of the generator as well as the discriminator so we look at one very popular implementation of this Gant's it's called D sequence or deep convolutional generative adversarial Network so this is one of the first work inside the work cited here it's one of the first publications to use a deep conditional Network to to generate actual images okay so the original paper which talks about ganz used M nest and it did not use such deep convolutional networks okay so there are some heuristics that they the authors figured out some of them are listed here so they replaced pooling layers in deep conditional networks with slided convolutions okay in the discriminator you have sided convolution instead of max pooling and in the generator you have transpose conditions remember that we start with a random noise vector and we have to actually generate images so you need to have transpose convolutions use batch normalization in both the generator and designator they removed most of the fully connected layers and they used relu activations in generator for all layers except the output which uses a tan hyperbolic and use lake erie loop for the discriminator for all layers this particular heuristic seemed to work very well for them if you heard you to read their paper where they have were able to generate images that are not part of the training distribution but still look very realistic okay so we'll just look at the architecture quickly so it start with z you sample Z from a distribution about 100 points around our dimensions and you have to reproject it to a volume of size thousand 24 feature maps of size 4 cross 4 and then you use sided convolution in this case they call it fractional standard convolutions or transpose convolutions to increase the size of the feature maps to 8x8 at the same time reducing the number of feature knobs is the typical thing that is seen throughout the network so in the next layer you have 16 cross 16 future maps with 256 feature maps in total and then 128 cross 128 feature absolute of size 32 cross 32 and in the end the output is a RGB image right that's what we interpret the output as it's basically three channels of size 64 + 64 okay so the gently this this is the generator network okay so the discriminator is basically the its mirror image basically so you start off with 64 plus 64 plus 3 and then you go back to the size of 128 cross 32 across 32 so on and so forth so basically what we see in this in this sequence you go back the same way okay alright so that's what you have here so all of this this comes here and the second comes there so on and so forth another and output is a probability of the image being real or fake okay depending on what your input is right so there are some interesting things in this paper so basically if you remember that we have two samples that from a distribution and for generating new images you keep sampling Z so what they required was that they were able to see a continuous transformation of images as you keep changing Z on one axis so the generator was able to meaningfully interpolate between Z so the paper has some very excellent example so you can go and look at them so the idea is again just to summarize you have a data set of training samples just we will estate I did using the emne Stata set and we have a discriminator which is a deep neural network in this case we have we are sampling noise over the DC cans paper 100 dimensions and the generator takes that our input and outputs Emnes digits in this case we are just illustrating it with the Amnesty Jets which is again given as input a discriminator which has a loss function based on predicting whether the inputs come from the training data or whether they are very output of the generator so what is important is that of course you will not be surprised if the generator provides a sound as output samples similar to the ones or same as the ones in the training data however what is observed in most of the gans based implementations of generative models is that very consistently output images which are similar but not the same as the ones in obtaining distribution these are completely new images which still makes sense as images and they are able to also interpolate meaningfully between the Z values that's also a very important point to note so by constantly by continually changing Z you can obtain sequence of transformation of images which are again meaningful ok so this this property can now ask them when the lot of work has been done in this area right now initially we when the paper came out 2014 there was problems for generating larger images so typically the outputs were restricted to size 64 cross 64 and so on however over time right now there is a something called big Gant's which are it is able to give a very large size images at very high resolution well of course the memory requirements and the computational requirements of course go up ok so this concludes our session on Gant's we will also look at some applications in medical imaging as to what and this generative adversarial interest card used for in the medical imaging domain how and just us just briefly ganz have a wide variety of applications for instance especially very even aim at least in the context of medical images there are situations where in there are not there is not too much data so in which case you can do the semi-supervised learning using ganz so you can use ganz to generate images like that in your train data sector okay and all the while training the discriminator for a particular task right so in this case the discriminator learns to distinguish between the real and fake images and in the process you hope that the discriminator has learnt the an underlying representation of your data which then you can fine-tune maybe a little bit more data for a specific segmentation cloud classification task so in the context of semi super arrest learning especially for medical image analysis ganz have a wide application so these ganz can also be used as conditional guns in the sense that you can have an additional input C for both the discriminator and and the generator so that the outputs of both are conditional on the the excess input so one such application is the image translation right especially so let's say you have two sets of applications so there are some images which are widely available let's say images of a certain Anatomy are widely available in a particular imaging modality let's say MRI images are widely available and CT images are not suppose you have trained a very deep network for MRI images right now you have CT images of the liver but you don't have enough data to train a classifier so what you do is to have against like network to translate now Gant's have a lot of applications some of these applications are summarized in this website it's a very interesting blog I heard you to go look at it however in the context of medical image analysis there are this has very crucial role to play especially in the context of semi-supervised learning suppose there is a paucity of training data in fact labeled training data then you can use Gans to obtain a discriminator which learns the underlying structure of the data and then maybe fine tune it with whatever little data is available right because for training the Gans you don't probably do not need or label data you just have access to a lot of images let's say of a particular variety of a particular Anatomy on a particular disease and then you can train Gans with it but since you lacked the labels he doesn't it's hard to train a deep classifier from scratch however once you I want you to train generate the address allele Network generated images are the certain Anatomy then you can take the discriminator and fine-tune it and hopefully it'll be a good classifier so that's just one very nice application for Ganz and there are other fields like in the case of image translation where there's a lot of interest especially since in the field of medical imaging there are lots of imaging techniques so MRI CT etc and in some anatomies and some disease cases there are more images available with a particular scanner let's say there are more image mr images of the brain available or let's say more amara meters of the more CT images of liver are available and there's more training data available label training data available for city of deliverance mr of the liver then it's convenient to set up against Network which can translate CT to MRI images so that you can label using any classifier you have created for the EMR and then transfer it back to the CT images of course these are these are research problems and with the advent of this ganz Network these things are made possible in the recent past there have been seven developments something called big Gant's has come up well which is able to generate larger images because most of the earlier networks following 2014 they have been only able to generate very small size images to send 64 by 64 up to 128 we 128 and the resolution was not great either okay so with the with more progress made in this field a lot of the interesting problems can be tackled especially in the field of medical image analysis 