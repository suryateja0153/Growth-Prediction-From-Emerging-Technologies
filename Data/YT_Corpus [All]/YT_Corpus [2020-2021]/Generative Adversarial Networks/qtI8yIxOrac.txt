 hello everyone welcome to mighty nano seminar series uh it is my pleasure to introduce today's speaker professor jonathan fan jonathan is an assistant professor of electrical engineering at stanford university and his research focuses on new design methodologies and materials approaches to nanophotonic systems jonathan has received many awards including the air force young investigator award the sloan foundation fellowship in physics the patriarch foundation fellowship the presidential early career award for scientists and engineers and today we're going to hear from jonathan tell us about his work on data-driven and physics-driven methods for electromagnetic design before i turn it to jonathan i just want to ask our participants to please keep the questions for the end at the end of the talk you can use the q a feature you can either raise your hand and we'll unmute you so that you can ask your question directly or you can also type in your question and we'll moderate so with that jonathan thank you very much for being here today and we're very excited to hear about your work and i'll turn it over to you great thank you so much for the introduction farnaz and it's really fantastic to be here and to tell you about my work and it's uh particularly great to be part of the mit.nano series because it's certainly the thesis for i think of a lot but i will be telling you about which is that there have been uh tremendously uh impactful developments in microscale and nanostructure uh patterning of materials that has really pushed our ability to control light and uh it's many of these developments were in fact developed at mit and in particular um over the last couple of decades there have been some real breakthroughs in really new classes of optical materials ranging from photonic crystals to metamaterials and meta surfaces that are really transforming the way that we think about wavefront engineering uh tremendous progress has been made in these topics over the last uh couple of decades yet with this immense design space that is now afforded from nanostructured optical media there exists still a number of fundamental questions with broad scientific and technological implications uh that still remain these include what are the physical limits to photonic design and how do we design the best and most efficient photonic devices and systems often at the end of the day we want to maximize the capabilities of whatever it is that we are trying to design whether it is an active or passive photonic device and how do we incorporate both new functionality and physical constraints into the design process uh there are of course many ways to think about addressing these questions my particular focus here in this talk will be on inverse design which is the use of numerical algorithms to search a design space through the maximization of a figure of merit so this is i would say different and complementary to a lot of more conventional methods which are based on uh kind of physical intuition and as a model system throughout i'd like to discuss our efforts in the context of meta services and in particular meta gradings which are structures that can diffract uh incident light uh to a plus one diffraction order but i want to be clear that many of the ideas and algorithms that we are developing can generalize to many other types of photonic systems for those who aren't familiar with meta surfaces these are thin film optical diffractive structures that utilize sub-wavelength scale structuring in order to achieve uh diffractive optical effects and wavefront engineering the conventional way that the field has been thinking about this over the last couple of decades has been to really take inspiration from the rf community and to consider meta surfaces as nanoscale phased arrays that utilize uh nanowave guides or nanoresonators to essentially control the amplitude and phase response of light here are just some examples of variations of of these conventional metasurface concepts and it's a very powerful idea i should just say for many types of applications but as we're looking to push thin film defractive optics to new limits including those that are very highly efficient and that are aberration corrected as we're looking to really translate this concept to practical commercial application it's becoming rather clear that the conventional way of thinking has certain limits in this very uh simple demonstration with our metagradings that the frag light to a again the plus one refraction order we find actually that all these ideas uh break down and result in low efficiency devices as we uh increase the angle of deflection and this is also the case as we start adding aberration correction capabilities so for the first part of my talk i want to introduce concepts involving topology optimization where we go beyond thinking about meta surfaces as a stitched nanostructures in the form of phase arrays and as we and to frame this more in the context of freeform design now the particular algorithm i'd like to introduce is called the adjoint variables method which was introduced to photonics by eli ilonovic and olay sigmund in the early 2000s and it's a really powerful freeform design concept based on gradient descent the idea is the following i'd like to show how this works for meta surfaces and migrating to give you a sense of how this works we're going to relax our design problem and in our particular context where we're looking to make silicon devices in air we're going to start with a random dielectric continuum with values between air and silicon as you can see this is a grayscale device we're then going to perform an iterative optimization method in which for each iteration we're going to improve the dielectric properties of at each and every box of the device in a manner that improves the figure of merit which in our case is well defined and is the diffraction efficiency to the plus one diffraction order and we're going to do this by using only two simulations per iteration a ford simulation where we shine an incident beam onto our device and record the electric field at all the voxels in our device and then adjoint simulation where we send a time reverse beam from the direction of light where we are looking to propagate the field and we will record the adjoining electric field everywhere and it turns out due to uh symmetry properties within maxwell's equations that the adjustment to each voxel dielectric constant that improves our figure of merit is proportional to the product of the forward and adjoint electric fields through these different simulations and this is extremely powerful because it means that even if we have a system with potentially thousands or tens of thousands or more voxels uh we only require two iterations two simulations per iteration in order to improve uh and adjust each voxel in order to improve our figure of merit i'd like to show a video of this uh of this method at work here we are looking at a top view of a single unit cell of a metagrading that is being designed to diffract light to the plus one diffraction order at a 75 degree angle which is a very difficult task to do in uh with conventional measure surfaces as we can see here we are starting out with a randomized grayscale dielectric distribution with values between air and silicon and as we go through our iterative process we can see here that we are iteratively perturbing the dielectric constant value at each and every voxel here in a manner that improves our figure of merit which we can see at the top there are periodic blurs which you can see which is used in attempt to kick these devices out of local optima that might be trapped in because this is after all a method of local gradient descent which i will return to later and i should also say that there are a number of bells and whistles that we can add such as the incorporation of robustness criteria which is to say that our final device uh can be designed to be relatively insensitive to spatial erosions and dilations of this structure this is to say uh they can be relatively uh robust to fabrication imperfections the final device that we have here is a is the unit cell of a diffraction grating which is highly non-intuitive and curvilinear so these ideas work and they can be used to uh certainly push the performance of diffractive optics and meta surfaces to new limits here are just some examples of structures that we've actually uh fabricated uh designed and fabricated these include these large angle metagratings that can diffract light with very large two very large angles with high efficiencies multi-functional devices i can split different incident wavelengths to different diffraction orders and in this case high efficiency large numeric aperture metal lenses which again are very hard to uh to design using conventional means we've also done a lot of reverse engineering of these structures to understand uh why these structures are able to diffract light so efficiently and to perform a multifunctional operation to summarize we have found with coupled block mode analysis uh that these structures utilize multi multiple scattering dynamics uh with and it is a very complex light matter interaction regime that can really only be accomplished and achieved through inverse design it's there it's very hard to find any intuitive dynamics within these uh with in terms of how these work um so to summarize the adjunct variables method and in general local gradient based methods are already by themselves very powerful methods to uh design free-form photonic structures and for many types of problems and systems this way of thinking is actually very much sufficient to boost the performance of of technologies to certain desired limits um however it doesn't quite address the original question which we are aiming to uh answer which is what is the absolute limit that we can uh that we can achieve with photonics engineering this after all is really the crux of i would say uh many uh design problems in photonics and the reason again is because this is a local optimizer and uh at the end of the day the uh optical design space is highly non-complex as i mentioned already this concept uh is based on the method of grading descent which means that it is actually highly sensitive to the initial uh dielectric distribution that we assume at the beginning of the optimization and we are iteratively perturbing these structures over the course of multiple iterations so to kind of sketch this out what we have is we're taking devices and we're pushing them and perturbing them within our design space to find a local optimum if our local optimum within this area of the design space turns out to be really good then great we were lucky with our initial condition and and we can stop however as we can see we're not actually searching most of the design space and if this particular device is not satisfactory uh the typical solution that we and others do with this technique is to essentially perform many local authentications with different random starting points hoping that one of them works and this is an idea that ultimately uh works in the in a representative histogram of device deficiencies uh from randomly from random starting points for a diffraction grading now what we can see here is that if we optimize 500 different devices with different initial starting points we do find that we do have a fraction of devices that actually is quite good uh this is the device that we ultimately manufacture and and publish on however there are many devices which are actually not great and what this means is uh that we are actually wasting huge amounts of computation resources uh by randomly sampling the design space we're still not searching the entire design space and it's not even clear if this device here is actually the best device so the question is how can we dramatically improve computational efficiency in our search for a global optimum for the rest of this talk i want to focus on algorithms that we have been developing in our group where we perform data processing with deep neural networks obviously machine learning has been a very popular and hot topic over the last decade in particular i hope to convince you that this is not only a a an idea an interface uh that uh merits uh research uh you know within the photonics community but that this actually is an an extremely effective way of of casting some of these inverse design problems in a manner that both allows us to improve computational efficiency and search for a global optimum now usually when we talk about deep neural networks we typically think about black boxes where we have a bunch of data and we attempt to teach a black box in order to learn from trends within this data in order to learn some general rules this is actually an approach that we've done some research in and i'd like to share some of those results in particular it turns out that so-called generative adversarial networks uh can learn from images of free-form devices and generalize their geometric properties in a manner uh where where uh where new types of uh devices with related operating parameters can actually be generated from a neural network so for those who aren't familiar with generative adversarial networks um there are actually two neural networks in one we have here a generator which takes random noise uh and which generates a distribution of devices and what we ultimately want is a generator that conditioned on for example the operating angle and wavelength of our metagrating is able to produce devices that essentially mimic those in a training set here we also have a discriminator that is a classifier that has an input that is either a device from the training set or a device from the generator and it attempts to discriminate whether the device that is inputted into the discriminator is from the training data set or the generator now of course a generative adversary network doesn't know what how to generate good devices or discriminate between uh generated devices or those in the training set up but it turns out and this is something that was developed in the in the uh company in the machine vision community that if you pit these two neural networks against each other you can actually train a generator to essentially mimic and learn the distribution of data within a training data set and to ultimately mimic and interpolate within this data so our first in our first generation gan we essentially use kind of this off-the-shelf type of architecture to basically train a generator to learn from images of freeform devices that's more details can be found in this paper down here in our second generation gan we actually created a new type of training methodology where we started with a sparse training set this is to say images of freeform devices that span uh that that have a kind of select wavelength and deflection angle values within our parameter space and we go through a set of training cycles where in each cycle we basically uh train a general adversarial network using a so-called a progressively growing network we generate devices oops excuse me we generate devices here we actually evaluate and filter out for high efficiency and robust devices and then we augment the training set and then we repeat the cycle multiple times what we get at the end is a generator that can basically mimic and understand how to produce images that represent high efficiency and robust metagrading spanning this continuum of wavelength and deflection angle values so this is what we get uh with our trained progressive growing gan we have a network that can produce a wide variety of free-form images of freeform devices that are both highly efficient and also robust i should emphasize that this idea of robustness which is to say that a structure has high performance uh both with its uh device layout but also it's physically eroded and dilated layout is a very subtle physical constraint to be had within a design space but through proper training methods uh progressive growing gans can actually uh learn these type of criteria so to summarize this part uh the key takeaway here is that with the proper way of training uh neural networks can actually learn and highly highly intricate trends within a training data set and in particular within even data sets as complex as images of freeform devices they can actually learn and generalize uh geometric features uh from these type of images um so they certainly have the capacity to do a great amount of work however what is uh this this general paradigm i would say of learning from uh data sets of devices uh while very effective in for example filling out a library of different classes of devices in this case operating across a range of wavelengths and angles still doesn't address the question of whether or not we can utilize these networks to perform global optimization so for the rest of my talk i'd like to introduce another new algorithm that we have been working on where we actually are looking to reframe the training of the neural network um as an optimization process itself so just recall with the adjoint variables method that what we are doing is we're taking a physical device we are performing uh physical physics-based simulations to find gradients to essentially iteratively improve and perturb this device what we are going to do now is we are going to replace this device with a generative neural network and as i will show you this actually allows us to do a population-based global search within the design space to very efficiently find the global optimum what we have here again with our gender network is a neural network that has an input of random noise this random noise can be sampled to produce a distribution of devices this is to say we are indeed doing population based optimization and instead of working with any type of data a training data set what we're going to do is we're going to train this generator to produce over the course of many iterations devices we're then going to evaluate those devices with an electromagnetic solver and use that information to back propagate and improve the mapping of random noise to to improve devices with the idea that over the course of this entire network training process uh random noise will ultimately map onto a narrow distribution of devices that are actually at the global optimum i'd like to go through this theory just a little more at least schematically as i was saying uh early the design space for photonic devices is highly non-convex that means if we take all devices and this of course is an absolutely massive space we'll find that there are many local optima or saddle points uh and that uh it can actually be quite hard to randomly find the global optima which is represented here and we're going to represent this globally optimized device as x-star the way that we're going about this is very different from i think the way you typically think about design and optimization where in the case of the adjoint variables method or genetic algorithms or anything you might be used to you are focused on designing within the physical space right you have a physical device that you perturb in order to make it better here we're going about this in an indirect way we're start we're starting with a generator that maps noise onto devices and instead of optimizing the devices and their layouts themselves we're actually going to optimize weights of our generative neural network and in particular we want to find weights that give us this optimal output distribution here what we have is a very narrow distribution centered around x star and we can have these type of um of highly narrow distributions because these generative networks are extremely non-linear algorithms so with this in mind we can cast our optimization problem where we want to find weights of our network that maximizes the following here this delta function of our output efficiency minus our globally optimized efficiency is non-zero only if our outputted devices are globally optimized and have efficiencies matching this and this is our distribution of outputted devices which we want to have to be as narrow and peaked around x-star as possible if we use this as our starting point we can then go and engineer a loss function that attempts to achieve this optimization goal and i'll skip through the math and just tell you what this loss function is and it is the following we have a term that has of course a batch size this is to say we will be sampling our generative neural network uh and to create a batch of devices each and every uh training iteration we're going to weigh this loss function uh uh by a term that is proportional to the uh to uh e to the efficiency which is to say that the higher an efficiency a device has uh that is being generated the greater it is being weighed and we will also have here um a gradient term uh which actually represents um how a device can efficiency can be improved uh by changing its uh by changing its voxels and this is exactly uh the same gradient local gradient that is used in in the adjoint variables method so to summarize this part uh in order to train our neural network we're going to be uh generating batches of devices evaluating its efficiency and efficiency gradients and then back propagating these back into the network using this engineered loss function in order to improve this mapping uh towards our optimization goal okay so does this actually work well i'd like to show you the results firstly of locally optimized devices based on the adjunct variables method here what we've done for each angle and and wavelength is we've optimized an ensemble of devices that is um and and and uh as as we've seen before we have many devices that are not great but we have some that are very good and here we have our the best out of 500 locally optimized devices here what we've done is we've trained a single uh generative network we're calling these global topology optimization networks for glownets and we can see that for some cases this works better the best glownet device uh is actually better than the best of 500 locally optimized devices here it's actually a lot better this is to say that it turns out that there's some areas of of photonic design spaces where it's just very hard to brute force find uh really good devices even with lots of local optimization attempts it's not perfect here at then this is a statistical method and here the best glownet device doesn't work as well as the adjoint uh variables based device but in general uh this method as we have benchmarked this uh in very quantitative ways in general uh is both significantly uh better than the uh local uh optimization method and it's also much more computationally efficient because instead of spending a lot of time doing entire local optimizations and in regions of the design space that um that are not great um it does ultimately search and shift the mapping of noise to devices uh to parts of the design space that are more promising okay i'd like to show in a little more of a visual way how the glownets training algorithm works here we are looking to uh train a a uh a glownets to basically find a metagrading that operates with this particular angle and wavelength and of course your network doesn't know anything about what a good device looks like so so in sampling this network at the get-go it produces devices that are mostly extremely poor as we go through our training process what we find is that the distribution of devices over the course of training uh in general gets better and as we can see it's getting more and more peaked uh towards higher values here we are visualizing the design space where initially we were searching the entire design space and over the course of training the network actually collapses onto this very specific part of the design space that produces really high efficiency devices so this network is doing exactly what we want it to do it is now outputting a very narrow distribution of devices in fact many of the same devices that are extremely efficient to show the stability of the network for this particular example we can see here that when we train eight of these networks from scratch with different initialization values we find that uh six of the times uh we get the exact same device in this particular case a device that works with 97 efficiency and uh why we cannot prove that this is the global optimum uh it is interesting to uh to see that these networks can in fact converge um to the same type of devices and furthermore that we are now in a a new type of operation regime for uh meta services i mean devices that operate with near unity efficiency is is not a concept that is typically associated with the meta surface field but with these uh very uh unusual and non-intuitive unit cells it is actually possible to um to really push the performance limits of metasurfaces towards those of um other types of optics close to those with anti-reflection coatings which is very exciting so for us we were indeed very excited to get these results we took these designs and we tried to go into the clean room of course pre-covered to make these uh and we just couldn't make these uh in part because these devices were very sensitive to fabrication imperfections uh this particular gap is on the order of 30 nanometers it was just very hard to make these type of devices which indicated to us that it was not sufficient just to globally optimize devices but that we had to be able to understand how to globally optimize devices within a design space with physical constraints and this has led us to a way of thinking about enforcing constraints that is actually very general and very powerful and and and certainly amenable to our glownets platform uh so uh this concept uh uh which we term a re-parameterization uh involves the design of devices not in a physical space usually when you think about constraints you think about again designing in a physical space and and for example penalizing devices that might be violating certain constraints but they're not hard constraints what we're going to do is we're actually going to design devices in an unconstrained latent space so this is say that devices in this unconstrained latent space can take literally any real value and but their exact representation in these latent spaces is not physical we're then going to use a set of analytical and differentiable mathematical transformations in order to map devices in this unconstrained latent space to those in this constrained device space and through this process if we properly choose these type of analytic mappings we are able to essentially map all of these devices in a very robust way to a subset of our design space that satisfies our constraint criteria so this actually does two things i just want to be clear the first is that it ensures that it's not even possible to have devices that are unfeasible or that violate our design constraints to be even considered the second is that it reduces in fact dramatically the size of our design space instead of considering all devices and needing to come up with methods to attempt to eliminate some of these devices uh we we already are uh constraining and reducing our design space already which makes its global search uh that much more effective okay to give some more details on how exactly this works i'd like to maybe first talk about this in the context of local optimization before i get to the glonass variant where what we have again with local optimization in a conventional sense involves taking a physical device and then using the adjoint variables method to find gradients that improve this device what we're going to do now is we're actually going to add these additional parts to our computation graph where we are actually optimizing in this latent vector space we're going this is unconstrained each of these numbers can take any a real value we're then going to mathematically transform these into a with vector space to give us physical devices that have constraints and then we're going to take these uh width vectors and we're going to do a second analytic transformation to convert these into a blurred pixelated pattern which we can then uh evaluate with the adjuvant variables method uh improvements to devices based on these gradients can then propagate all the way back to uh improvements to the latent vector representation using the chain rule because all of these different parts are connected through analytic and differentiable expressions and therefore this entire computation graph can be auto-differentiated now there's some leeway as to how exactly we incorporate these constraints here i'd like to show something that we did which actually uh turns out to work quite well which is that in translating our width vectors to our grayscale dielectric uh structures we actually uh model these uh these forms uh these dielectric distributions using the fermi dirac function and this is useful because as we know with the fermi dirac function at high temperatures we actually have something that appears a gray scale right we have a temperature this actually allows gradients to propagate properly throughout this entire computation graph and allows the uh and allows the optimization to work out quite well and and what we can do is we can manually lower the temperature to zero uh over the course of optimization um at which point when t is equal to zero uh the final structure is uh binary and we have something that is all silicon or air so let's go ahead and see how this actually looks we are starting here with a random grayscale structure that has a temperature this is to say we can see in this pattern profile that that this is indeed grayscale uh the initial efficiency is poor and as we go through this uh local re-franchise optimization we can see we are dropping the temperature the efficiency is going up and the final device is indeed binary and it has a high efficiency so so this is the essence of the repranterization concept now as we move to uh repronunciation with chlorinates it becomes clear how we do this we have a generative neural network that outputs not physical devices but devices uh that are in a latent vector representation we use our analytic mathematical transformations to transform these latent vector representations to physical grayscale devices which we can then evaluate with electromagnetic solvers finally to improve the mapping of noise to devices we use back propagation and we can back propagate these physical gradients all the way back to the neural network weights in our generator because again this entire computation graph is differentiable here are some results where we can see that as a function of minimum feature size we get when we have no uh when we have uh really no known feature size constraints that our best device is 97 efficient and it consists of three bars this is exactly the device that i showed a few slides ago but as we go to larger and larger minimum feature sizes there is of course this trade-off between feature size and efficiency and we actually find that entirely different devices with different topologies become the globally optimized ones here we can see in this glownets that can actually optimize for different topologies that we can get with a network uh these different types of devices with different topologies that are globally optimized here we can see as we go to different minimum feature sizes that we actually get devices that look uh completely different okay in the last few minutes that i have i'd like to show how these ideas can adapt to other types of problems in photonics uh uh beyond uh just diffractive optics and i'd like to show in particular uh this this problem which i think you're all familiar of from from classes uh which involves optimizing thin film stacks and i would have thought myself until i looked at this problem that thin film stack uh design is is actually a solid problem it's it's been well defined for over half a century uh but in fact uh it's a pretty complicated combinatorial problem for which glownets is ideally soluted uh suited for in this particular problem it's a bit different from what i showed before because we are not only looking to optimize film thicknesses in a continuous way but we're looking to select from materials and in particular from a library of discrete materials which actually adds a little wrinkle to this problem so this is what we're doing we have here our again our uh generator that generates uh different types of devices here we have a thickness and refractive index for different uh for different uh layers uh in our uh device we use a transfer matrix method solver which can be written in python in an analytic fashion to evaluate uh our reflection spectra and then we have our loss function uh which evaluates how close those devices are uh to to producing the reflection spectra we desire and we back propagate this back to the generator to improve the mapping of noise to useful devices now one of the challenges here is uh we are dealing ultimately with a discrete uh with a library of discrete materials and uh as as we know whenever we have discrete variables it can be very hard to back propagate gradients in that case and we therefore have to relax these discrete math problems into continuous variable problems and to do that we're using some tricks from uh from image classification and categorical optimization where our generator is not generating uh discrete refractive index profiles but it's actually generating a matrix here with a continuum of values that can be transformed using the softmax function into a probability matrix the probability matrix tells you for this particular row the probability that this particular uh layer consists of the um in this case the m this m1 material in this case the m2 material etc etc we can then take the expected value of the dielectric constants at each layer uh with this probability matrix using this expression here which we then use as our refractive index a profile for evaluation this entire probability matrix concept is differentiable and something that is amenable to the back propagation process and furthermore with the softmax function which is equivalent to a partition function in physics what we find is that as we manually adjust alpha in a manner similar to our temperature term from before we can essentially force the probability matrix to converge and take on a nearly discrete values uh in terms of dielectric constant one other thing i want to mention here that we realized um is that if we incorporate a residual network for our generator we can actually improve uh the overall mapping of our uh of noise to devices over the course of of uh optimization uh residual networks uh for those who are not familiar involve neural networks that not only incorporate uh a block of of uh neural network layers they also incorporate this shortcut function that can actually uh uh so-called simplify uh these these very deep networks this is a concept that has been used for a number of years in in very deep image based networks the idea is the following we want to find uh this global optimum in this really complicated objective uh function space uh during the uh middle parts in intermediate parts of our training process what we're trying to do is we're trying to get our generator to actually map out and mimic uh some of the complex and non-trivial curves within this objective function that means that this network does indeed need to be very deep uh and have enough degrees of freedom to capture uh this nuanced landscape but our final resnet um is attempting to collapse onto this global optimum and it turns out uh that at the shortcuts here can actually really help create a mapping function from noise to devices that is much more simplified and much more emblematic of our desired final probability distribution okay so let's see a couple of results um the first i'd like to show is an anti-reflection coding um we chose this specifically because there's a really nice study uh from right here from mit by the uh from mark baldw and steve johnson group where they actually did a brute force search for the global optimum for anti-reflection coatings for solar cells um i believe that took a number of days but uh the main thing is it uh is they actually had a hard uh a global optimum for for these uh thin-film stacks uh that we could benchmark against for us we were able to use our uh res glow net we were actually able here as we can see in this histogram uh we were able to find the same global optimum as found in this paper and we were able to do this in seven seconds so so this is indeed a method that at least with this one example was able to find uh with uh was able to find the global optimum this can also apply to more complex problems where the solutions are still open-ended this is a very interesting idea that was actually proposed by marin solidich where we what we want to do is we want to make improve the efficiency of an incandescent light bulb by creating thin film filters that transmit a visible light and that reflect uh uv and and thermal radiation so that it can basically be recycled to keep reheating this thermal emitter this is a pretty complicated multi-objective combinatorial problem with a library of six materials this is what we find we're actually able to get a 45-layer device that maps um this desired profile quite well and if we compare this to uh some existing benchmarks uh we actually find that our algorithm works better than others which actually use a combination of machine learning global optimization and local optimization and uh and this improvement uh may not seem like a lot looking at the vertical scale but if we uh trace along the horizontal scale what that means is compared to this memetic algorithm for example we're able to get the same performance with half the number of layers okay so uh where where do we go from here i'd like to just say a couple of words the first is hopefully um some of you are convinced that meta surfaces where we have relatively uh thin device thicknesses and high contrast is an interesting and new regime for diffractive optics but hasn't really been explored is is this this arena uh nearly as much until very recently uh uh ones where we have thick devices and high contrast media these are exclusively multiple scattering uh media in essence that require inverse design to fully exploit and with a huge number of degrees of freedom there's a great opportunity here's some preliminary work that we've done showing that as we do go to multiple layers theoretically we can truly uh create flat optics that that can support uh really extraordinary capabilities number of groups including those from steve johnson and andre faryon are are um have have shown some very uh very great theoretical and experimental work in this vein as well and i think it will be a major thrust for for photonics engineering the second thing i want to say is uh i think it would be great for us as a community to coordinate research efforts especially in this burgeoning field of of algorithm design with neural networks if we look at machine learning in the cs community uh that research is largely driven through open source coding and proper benchmarking of algorithms that have really enabled the field to just blow up and our field is not good at sharing data sharing codes and benchmarking uh with common uh training criteria i think as a field we need to do more of that i as my group has attempted to put what is essentially like an archive for people to upload codes and images where we have uh some users but i think uh culturally we need to shift things a bit so to summarize uh deep learning is the foundation for the next generation of cad-based inverse design tools not because neural networks are a flashy area of research but because neural networks are able to learn non-intuitive data transformations between inputs and outputs in ways that are truly new and furthermore i should just say that as we project over the next five to ten years there's going to be so much additional advancement in both the hardware and software for neural networks that uh researching in this uh direction will reap the benefits of of of everything that is happening in the cs community as well as in new hardware development for for machine learning the key is not to focus on black boxes but to as physicists ourselves really think about hybrid algorithms that combine physics and physical constraints with these black boxes to create something that is entirely new there's clearly more innovations required to scale up these concepts to large area 3d systems i think there are pathways there just to give one example it's going to become absolutely essential to have really fast electromagnetic solvers but it turns out that the data sciences may hold the key to that too uh steve johnson recently uh put out an archive paper showing how active learning can be used to make application specific solvers that are extremely uh fast and accurate uh this is just a really uh burgeoning field that's going to really be i believe taking off um finally i'd like to thank my research group and funding agency a lot of this work has built off of many years of effort and uh and lots of people and and very lastly uh for those of you who think this is interesting uh who are perhaps new to machine learning uh and and want to understand more about its connections with the physical sciences i want to point you to this review article that is about to come out in uh nature materials reviews that is on archive thank you very much for your time thank you john thanks for a great talk we have a few minutes for questions so if you have questions please either raise your hand and we cannot mute you so that you can ask your question directly or you can type your question in the q a section ali is asking what a real world application of reparameterization industry or business cases yeah let me back up and say this whole concept of re-parameterization is a very general concept and it's actually used rather commonly in the cs community uh in order to incorporate constraints into uh data science problems uh in in the context of of a lot of these physics problems i think uh this concept of refrainerization uh really can be framed to capture almost any physical constraint and what i showed in this example we're looking to incorporate a minimum feature size constraint uh but you can imagine using this for many other problems just to give you an idea of something that my group is working on um if you want to create a multi-layer meta surface and you want to ensure that uh you know the structure on the top isn't doesn't have an overhang on one hand you can try to manually incorporate that type of constraint and and to uh and and to attempt to use penalty terms to to prevent that from happening or you know you can think about using reframization as a means to uh to ensuring that your design space doesn't even have structures that have overhangs there's also a really nice paper actually from from mit in uh the design of of microwave circuits where they use free printerization just to ensure that uh in in considering a design space of coupled resonators that coupled resonators didn't overlap and uh and were constrained uh physically in certain configurations so so to kind of summarize i think this idea of refrainerization actually is extremely general um and and pertains to the incorporation of any type of con physical constraint that can be described by an analytic mathematical expression great so another question we have is if our goal is to find the best solution why do you output a distribution that's a great question uh at the end we utilize the distribution throughout the entire design process to ensure that we do population-based optimization and a population based search within the entire design space you really have to be able to search the full design space and and have different parts of the design space basically talk to each other so that you can kind of push your um your your desired uh output of distribution towards more promising regions of the design space which is actually what happens so at the at the end of the day um just to be clear your your network is indeed still producing a distribution of devices but ideally it's uh narrowly distributed in a way where you don't have to sample the generator network nearly as much in order to find a really good device and the second thing i will say is that it really gives you a lot of flexibility to really shape the type of distribution that is outputted from your network sometimes if you're really confident you really want the network to collapse but if you have a really complicated design space or perhaps if you want to find many good local optima instead of just a single good global optimum you want the output of distribution to be different so this is to say i think this is actually a very interesting degree of freedom where through loss function engineering um you can really mold and tailor the output um the outputted device distribution to take on one of the number of characteristics you might desire wonderful so could you comment on the physical origins of the thin film stack design improvements through machine learning especially compared to conventional quarter wave structures yeah great great question our choice of the two examples that i showed the anti-reflection coding and this uh infrared um and and this uh basically this incandescent light bulb filter i think we're specifically chosen because uh there there aren't really uh simple analogs to um to kind of thin film stacks to to really uh improve to to to capture this type of performance right thin film stacks could be used to uh savor to be reflective for for a range of wavelengths but not to transmit a range of wavelengths and then to reflect all the other wavelengths um so uh so yeah for kind of the idea behind these particular uh structures is um is kind of the need to uh create what are ultimately very aperiodic uh you know thin film stacks that that utilize interference and entirely in non-intuitive and crazy ways in order to achieve uh what i showed you and um and uh i should say this this should not take away from uh any application where simple design methods work in fact you clearly should use simple design methods if for example uh periodic thin film stacks are sufficient for your application but in the case where you want to really reduce the number of stacks in the case where you just have to uh you know use really bizarre and complicated interference phenomena this is the way to go great so we have a few more questions so the global net and neural networks in general require a large amount of data in order to successfully train in the photonics space this is in particular problem since individual simulations can take minutes or even hours how do you propose moving the neural networks to larger design problems perhaps 3d structures absolutely we're working on that um i think the key will be figuring out how to innovate faster solvers i remember talking to steve johnson about this a number of months ago pre-covid where um i think kind of generic uh electromagnetic solvers as we know them right fdtd fdfd um fem rcwa uh generic solvers are near their limits of you know order magnitude uh limits of of how fast they can go um and i think that's where uh you know there still need to be breakthroughs in in in just getting much much faster solvers that could allow these optimization methods to scale to uh problems of much higher dimensionality but i'll cite two examples of where i've seen this which indicate i think the hope of of this actually being able to go beyond model 1d systems the first is as i was mentioning before work by stephen johnson where through active learning he is able to train a neural network a discriminative neural network to basically serve as a high-speed solver and the entire evaluation and training of this network took in order was uh reduced the overall computation overhead by an order of magnitude and the second is worked by my colleague ian vukovic who's actually using a convolutional neural network to basically uh create application specific fdfd solvers that can essentially uh perform uh that can effectively serve as a data-driven pre-conditioner to to accelerate uh those type of electromagnetic solvers so so i think there's a tremendous opportunity that's really untapped at this stage to understand how to accelerate em uh solvers uh you know not just by a little but potentially by orders of the magnitude and and i think that will open up the door not only to really take these type of data and physics driven methods to the next level but really for everyone's research i mean i still remember time as a grad student just sitting in front of my computer waiting for a simulation to end that should not happen in the future in my opinion great so the next question is asking have these devices generated by the neural nets guided new intuitions or insight for the physics and geometry of the devices yeah that was certainly the goal for um for applying inverse design for some of these diffractive optical structures and um we spend a lot of time reverse engineering those structures using um these couple block mode analyses to really understand the modes kind of the multiple scattering dynamics to see if there is anything that that could be generalized to the point where we would even have just two ridges uh coupled together uh almost like you know two artificial atoms just coupled together and and it would be amazing to see even in a system as simple as two coupled uh ridges how um how non-intuitive and non-generalizable those ideas were so i think this is both you know both a blessing and a curse i would say a um it it was of course part of my goal to um to attempt to find really new physical um new physical insight that could you know generalize um you know some of these concepts and design in a way that that that's much more tractable right and digestible in a way that we as physicists and engineers uh prefer to think about problems um but i've been doing this long enough to understand that um at least for these types of problems that i'm showing that type of intuition does not seem to exist and i guess that is also the opportunity for thinking about these more as straight up optimization problems with an understanding that there is just a great deal of non-intuitive uh thinking uh and dynamics that goes into maximizing performance and that for many problems it's just not that elegant so and this is a question from a machine learning person as is written here so it says interpolation within a large number of data points can be done well through any number of function approximation schemes how well are these networks extrapolating away from the data yeah so in the case of the gans which i showed in the middle part it certainly has proven to be a very good interpolation tool um but uh for those who are uh familiar with with how these these uh networks work right you're you're mostly fitting uh the the uh the form of your training data so from an extrapolation point it's it's uh it's not great and then that's where you know sampling the in our case this angle wavelength space of devices kind of uniformly and interpolating within them has proven to be the most effective means of generating training data in order to use training data to to train again the glow nets are are different right because initially uh what the glownets is doing is if it is ideally being initialized if it is ideally initialized uh random noise is actually being mapped onto the entire design space so you don't make any assumptions there is no training data and you don't make any assumptions about um what is what in the training space what you then do is you kind of sample this space and uh gradually push this mapping function you know towards more promising spaces until you know ideally it it does focus and collapse onto the global population so so this is just a different way of using generative networks that at least i've seen in the machine learning community i've i'm not a machine learning researcher formally though i in fact i'm an experimentalist waiting for covent to end but uh but but my understanding talking to my colleagues in optimization and machine learning is that uh these type of methods are are often not not very closely studied in machine learning and and part of the reason is because uh it's very unusual and rare to have such physical uh such equations to describe you know your design space and local gradients and things like that i mean if you're categorizing dogs and cats there's no function that tells you oh you know this funny cat can be made better by putting it in this analytic function but in photonics it turns out that you know these functions exist so i think this is where there are some very interesting opportunities we have physics we have physical equations um we have we have a lot of you know analytic tools and quantitative tools at our disposal and that's where we're looking to design entirely new hybrid algorithms wonderful couple more questions um does any of your research involve graphene or nanoporous silicon surface applications oh graphene um a comment that says we're focused on life sciences and your neural network deep learning techniques on such meta services seem exciting yeah i think this is something that can be tailored um to to many different systems um and and i just want to emphasize i don't think this is something limited to nano photonics so we haven't quite uh worked on those very specific topics though um part of my research groups actually works in nanomaterials and and i think it's something that we can definitely look at down the road but i just want to emphasize i mean i think um we we have used nanophotonics as like a really nice model system um but but thinking about the very wide range of so-called meta-materials and acoustics mechanics i think this will uh be very interesting i think you know in circuits uh thinking about even materials discovery um there's a lot of you know really exciting work in fluid mechanics and and and that whole community uh with solving differential equations with neural networks so so yeah it's it's extremely broad um and it doesn't mean to say that the exact algorithms that i'm showing you today are going to apply to you know it just very directly in all these different problems but i i think they're just huge opportunities across the board in the physical sciences and that um it really takes people from our fields whether it's you know biology or material science or electrical engineering to learn and and and use these things there are too many data scientists out there but there are actually very few data scientists um who have very deep physical um physics sciences uh physical sciences backgrounds and then yeah there's a huge opportunity last two quick questions uh it's again to the um how specific is the end to the optimal photonics design we're trying to achieve for example between the stack or the meta surfaces yeah i think the gan is something where uh especially with this whole cycle gan that that we've developed as our second generation gan uh the final devices are as good and sometimes a little better than the best globally optimized devices from local optimization so it's a great way if you have really good devices in your training set to basically get to up to that point so uh again for many problems i mean this is efficient you don't have to find the global optimum for all problems it happens to be um a a great way to learn from sparse data and to kind of generalize these design concepts but it won't again it's it's good at an interpolation and therefore it won't just find and extrapolate uh and find the global optimum if your global home is not already in the training data okay and the last question asked is the symmetry imposed yes we incorporate uh kind of uh kind of xy symmetry depending on whether we want our meta surfaces to be uh uh polarization dependent or polarization independent but you can imagine there are many many games you can play with incorporating symmetry engineer networks uh certainly i think um there's there's a whole slew of uh symmetries especially as you go into other other topics like uh topological photonics or or others where where you can certainly incorporate um uh unique symmetry properties so um but it's something that that uh we did we incorporated it as as as convenient for us but we didn't um uh but yeah it's it's something that can be exploited even more great uh those were all the questions thank you john again for the great talk and thank you all for uh attending our very first seminar series for this year uh the next one will be next month and you'll receive more information shortly from mighty nano about it thank you all great thank you 