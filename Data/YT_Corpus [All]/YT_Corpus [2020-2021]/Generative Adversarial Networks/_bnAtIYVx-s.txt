 Level design is an incredibly important component of game design. Making good varied levels that are still coherent with the style of previous levels is a difficult and time-consuming task. Wouldn't it be great to have an algorithm to help generate new levels directly after having only designed one? My name is Maren Awiszus and in this video, I will be presenting TOAD-GAN: Coherent Style Level Generation from a Single Example for the 16th AAAI conference on Artificial Intelligence and Interactive Digital Entertainment in 2020. This paper was written by myself, Frederik Schubert and Bodo Rosenhahn from the Institute of Information Processing at Leibniz University Hannover. Generating video game levels, especially with platformers like super mario bros has been a highly researched topic. A variety of methods have been used including evolutionary algorithms, Monte-Carlo tree search, Markov Chain models and many more. Because of the recent advancements in the field, machine learning and especially neural networks have been of particular interest. Recently, so-called generative adversarial networks or GANs have achieved good results. The problem with machine learning is that you always need a large amount of training data at best thousands of samples which is a daunting task in level generation. In our example of the original Super Mario Bros., we have only 15 example levels available. Now how does our method, TOAD-GAN, work with even less - only one example? Let me explain. TOAD-GAN stands for token based one shot arbitrary dimension generative adversarial network. We're dealing with token-based maps of levels. These can be converted nicely into tensors by making a binary layer for each token representing its occurrences in the level. Our method is one shot which means it only gets one level at a time to train on and it can generate levels of arbitrary dimensions due to its architecture as a fully convolutional generative adversarial network or GAN. but what is a fully convolutional GAN and how does it work? A basic GAN consists of two networks: A generator and a discriminator. The generator takes an input from a certain known randomized distribution and tries to generate from that a sample of the real distribution, in our case levels. But for the generator to make meaningful samples it needs good feedback on how to change its output to make it more like the real distribution. This feedback is provided by the discriminator which randomly gets either a sample from the real distribution or the fake generated distribution as an input and then has to predict from which distribution it originally came. The goal during training is now for both the generator and the discriminator to get better and better at their tasks until the generated samples are so close to the original samples that the discriminator can no longer distinguish the two. Both the discriminator and the generator have a fixed input and output dimension. We would need many full levels of the same size to train them. The trick is now to not train the networks on the full level but on small same sized windows of the level of which we can make plenty. Training like that is made possible by comprising the network of convolutional layers whose output size depends on that of the input. With that we can use a noise map with predefined size as an input which results in an output level of any desired arbitrary size. However, with the convolutional layers like this the generator would only be focused on very local patterns. In order to make sure that the level makes sense as a whole we instead use multiple GANs that are trained on different scales of the level. We upscale and add the output of the smaller scale to the new noise map each scale, widening the field of view and making sure that both local and global patterns are learned. For this to work we need to scale down the original level first which poses a unique problem. Using normal interpolation techniques rare, important tokens disappear by being overwritten with more common tokens. This can be problematic as higher scales of the image do not get any information about them from the lower scales and have a hard time generating them. For that we designed a specific downsampling algorithm based on a predefined token hierarchy. For each entry in the downscaled level we find the possible candidates for that spot by checking which tokens in the larger level need to be collapsed into one. Then we rank these candidates according to the hierarchy and only keep the values of the highest rank available. In the end we adjust the values with a softmax function to make the resulting image more similar to the final output of the GAN, which ends with a softmax as well. With our method complete we can finally train a TOAD-GAN for the Super Mario Bros. levels available. The levels resulting from this training keep the style of the original level both in a small and a large scale. Due to the generation being based on random noise there are not only infinite samples but they can be partially edited and resampled easily. If you want to try generating and editing some levels yourself check our github for TOAD-GUI where we provide TOAD-GANs for the different Super Mario Bros. levels and it allows you to generate levels of arbitrary sizes edit them and you can even play them. Here are some examples of generated levels from different generators. While there are still some areas we humans would define as errors like broken pipes and platforms, overall the levels look very much like reinterpretations of the original level. Small patterns like these three platforms over here are learned and even varied in the generation process. By comparison, levels of other generators may not even look like Mario levels or have trouble showing different styles and variation. In order to articulate the similarity between our generated levels and the original Super Mario Bros. levels we calculated the so-called Tile Pattern KL-Divergence a measure designed to show the similarity of pattern distributions of different sizes between two levels. So, we generated 100 new random levels per generator and calculated the mean TPKL-Divergence between them and their original level. These are the results. For comparison here are the mean TPKL-Divergences provided in other papers. We can clearly see that TOAD-GANs capture the original tile patterns almost perfectly. We can also calculate the TPKL-Divergence between our generators and levels they were not trained on. This shows that certain levels have a large similarity in patterns. These levels belong to the same type of level and therefore are similar patterns. This is especially noticeable for floating platform levels whose platforms do not occur within other types of level. To better visualize the similarity of not only patterns but larger level pieces, we designed a new method to calculate the distance between two 16 by 16 slices of levels. For this we train a different neural network to classify these slices according to the level they originate from. By necessity this network learns a representation of these slices in order to more easily distinguish what level the slice came from. This learned embedding can be mapped to a 2D plane with an algorithm like UMAP. We now calculate the embedding for all 16 by 16 slices from the original levels and map it. This looks like this. Looking at this distribution, we can see that the level types like overworld, underground and floating platforms map to clearly distinct and far apart places showing that the distance in the space is representative. Now mapping all slices from our trained TOAD-GANs with the same transformation reveals that they map to the same areas as the original levels keeping the level style they were trained on while showing some variance within. To put the variance in another perspective we randomly picked 100 000 16 by 16 slices and found that about 91% of them were unique. This is all while the completability of the levels is comparable to the originals according to a state-of-the-art solver the A* algorithm. We've shown that TOAD-GAN is capable of generating pleasing super mario bros levels but what about other games? Let's take Super Mario Kart as an example. We can train a TOAD-GAN on the new level simply after making a new token hierarchy, since nothing else in the algorithm is game specific. However, since there is no training incentive to generate a usable circuit, as you could say, it might generate dead ends and unnecessary road splits in the track. With the architecture of TOAD-GAN, we can easily condition it however to generate the exact check layout we want by changing the token layer corresponding to the road token in the lowest scale. This can even generate plausible levels with tracks that are widely different than the  one originally used. Our results show that TOAD-GAN can be a step towards usable machine learning content generation as a help for game designers and we're looking forward for it to be used and expanded upon in the future. This has been the video presentation of TOAD-GAN: Coherent Style Level Generation from a Single Example. References and links to the paper and github can be found in the video description. Thank you very much for watching and goodbye. 