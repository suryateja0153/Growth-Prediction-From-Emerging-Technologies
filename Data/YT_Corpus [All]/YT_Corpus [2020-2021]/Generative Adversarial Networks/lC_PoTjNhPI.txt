 Hey guys, welcome to the first episode of Matchue GANs. In this series I'm going to be going through a ton of different types of generative adversarial networks and just going through and explaining them Coding them up and seeing how good they are. So Since this is the first of many episodes I figured it would be great to start off with just a normal vanilla generative adversarial network with no bells and no whistles. We'll just simply be generating images so With that, let me jump right into the explanation of what generative adversarial networks actually are so from this diagram You can see that we have a couple of different parts the most important parts though are our two neural networks the generator and the discriminator So the generator's job; it takes in some random noise and produces an image The discriminator takes in an image and tries to classify whether that image is real or fake The two neural networks in this architecture have opposing objectives The discriminator is trying to determine whether an image is real or whether it's generated by the generator The generator, however has the opposite goal of trying to fool the discriminator into thinking the images generated by it are actually real This way going back and forth and training back and forth Enables the generator to produce images which look more and more realistic as training time goes on One way to illustrate this would be through having an art critic and an art forger The art forger's goal is to fool the art critic into thinking that his paintings are real and the art critic does not want that the art critic wants to be able to tell Which paintings are genuine and which paintings are forged. Just the same here, we have our generator which tries to make fake images and tries to fool the discriminator into thinking that Those images are real So now we can just jump right into the code. We'll start coding up these neural networks and just seeing where we can take it So first of all, we want to import our packages. So the first one is going to be NumPy. We're going to use NumPy to assess and manipulate our data then we'll use matplotlib to be able to show us the images that we generate and the real images that we have and Finally we have Keras. Keras is going to be the engine in which we are making our neural networks Next we can jump into the hyperparameters, so we will have a batch size of 64 This batch size is the number of images shown to our discriminator and the number of images generated by our generator per training iteration Next our training iterations is how many iterations were going to be training our neural network for. Finally, we have two empty lists here one to keep track of the loss that the discriminator has over its training time and Another one to keep track of how much loss the generator has over training time Next we'll want to import our data so here i've made it really simple by adding a whole bunch of flower pictures into Just a simple numpy file and so we can just simply load the numpy file Then we'll want to convert it to floating points and then finally normalize it to be between 0 and 1 so that's very easy for our neural networks to create this image and Here we can see a couple examples of the flowers that we would like to generate but these are real ones from the dataset Now we can jump right into building our models so first we will be making our generator So here we can start off with a latent input of 64 So here we'll have 64 random numbers input into the neural network then we'll want to reshape it using a fully connected *layer* so that it's 4 by 4 by 64 and Then we're going to use a couple layers of up sampling and convolution with the ReLU activation function which you can find out more information about Anywhere on the internet pretty much. It's a very popular activation function So we are going to use up sampling and convolution to try to add Features and patterns and extract features and patterns and then we'll want to up sample use convolution upsample convolution Etc. So finally we reach a 64 by 64 image with three filters being red green and blue and So that will be our image output and here you can see the summary of the architecture of our generator Next up we'll want to make our discriminator. So here we start off with an image input of 64 by 64 by 3 So that would be a 64 by 64 image with red green and blue then we use convolution and the leaky ReLU Activation function again, very popular. You can probably find a lot of information about it online and Then we can use pooling layers to shrink the image we do this till we're 32 16 8 4 once we're down at a 4 by 4 image we can just flatten it and have a fully connected layer to one single output and That will be our classification So again here we can see the architecture of our neural network for the discriminator So now we can go on to make the training configurations So this first training configuration is going to be to train the discriminator and so we will start by setting only the discriminator to be able to train with this and So first off we will grab some real images and we will try to classify them Next up we will want to grab a latent input. So that would be our random noise up here Then we will want to generate the image and then we will want to classify those generated images so our discriminator model will take in a real image and a latent input and output our two classifications. We can use labels during training time to say which Labels we want for each fake and real images Moving on now, we can make the training configuration which will train the generator Here we do not want the discriminator to learn anything, but we do want the generator to learn things So we can start off with a latent input again generate an image and discriminate based on our discriminator then we have a simple model now without any real images in it only taking a latent input and Using the validity as the output and that would be the discriminator classification During training, we will want to use the opposite labels as what you want your discriminator to be that way We can tell the generator to fool the discriminator instead so here we will compile our models and we will be using rmsprop, which is simply a type of Stochastic gradient descent and we'll be using a loss function of mean squared error to adjust our weights and biases Finally now we can create a training loop so we will be training for the number of iterations which we set up earlier We will want to get our labels for both real and fake images We will grab a bunch of real images and latent vectors finally We will train our discriminator model and we will get the loss values from that and then we can append that to our loss the list Then we can grab some more latent vectors to train our generator model and then once we've trained our generator model for one Iteration we can add that loss value to the list of generator loss values Finally at the end we can see some samples that have been generated after we've gone through our training for the number of iterations I've set the number of iterations to 10,000, but you may want to do more or less depending on how realistic you want your images to be. The more you train it the better it will be. And finally we can see how our loss values are changing during training We can see that the generator starts off being a little bit weary But then ends up they both come to a kind of equilibrium after a while So now we can go through and we can run this ourselves, so let's go ahead restart and run all cells Now we can see we've got our layers intact We've got our real images imported Generator and discriminator have been made. They've been compiled and now finally we can run through these iterations So we'll see you guys once this is finished training All right, so welcome back This is finished training now and so as we can see it has gone through our 10,000 iterations and so now we can run it and try to see what kind of images it's generating as You can see not the perfect most best images in the world, but we'll get there but as you can see it's picked up on most the important details of flowers all the different colors all the All the green in the background all the different colors of flowers that sort of thing. So Here we can plot the statistics that we collected of the loss functions And as you can see the discriminator started off by winning pretty well over the generator but then as time went on we ended up having a pretty Equilibrium both of them being around the same loss function So that's it for the episode guys, I hope you enjoyed. In the future episodes we're going to be looking at different types of generative adversarial networks such as Cycle GANs and image to image translation GANs that sort of thing And so I hope you guys stick around for that and I'll see you guys next time 