 [Music] today we'll talk about generative adversarial networks are popularly known as Ganz so this is yet another family of models in the deep generative story so let's start looking at it so let's look at the intuition behind Ganz so so far we have looked at generative models which explicitly model the joint distribution or the conditional probability distribution right so for example in the case of rbms we learned P of X comma H in the case of VA s we learned this P of Z given X and P of X given Z and in the case of autoregressive models we learned this P of X without any latent variables right so we were actually explicitly learning these probability distributions and then everything else was on top of red right so whether abstraction then okay we sample from P of Z given X if you want to generate we've sample from P of X given Z and so on it's a both abstraction and generation were happening based on these explicitly computed probability distributions okay and each of them had their own way of dealing with it which like for example we're using sampling or variational inference or using neural networks to parameterize the explicit factorization okay now but if what if you are only interested in generating samples from the distribution and we don't care about what P of X actually is so let's try to understand this I've given you a lot of data let's say like giving you a lot of feminist images what I'm saying is that I've given you a lot of Amnesty images I'm saying it that I don't care what of P of X is or whether there's a latent variable is a latent distribution and so I don't care about all these things I only care about the following that you take all these images and now give me more images which look like as if they had come from the same distribution so I've given you a lot of ways of writing 1 2 3 4 and so on now look at all this and give me more images which look like this data I am not worried about what's the probability of this and so on I just want to get samples from that so I am NOT interested in dealing with this P of X right so the goal here is to sample from this very complex high dimensional distribute and the operating phrase here is complex high dimensional distribution right and we know that because of that things become intractable and we saw that our bm's variation norton cutters and auto regressive models had their own way of dealing with this intractability okay but now what ganz do is they completely bypass this whole process of learning P of X there is no explicit P of X which is learned in Ganz the idea there is simple you want me to generate images which look like our training images this is what I will do I know it's very hard to sample from this complex high dimensional distribution I know that I can easily sample from our normal distribution I'll take some Z belonging to some Rd I can sample from it and then what I will do is I'll learn to make a very complex transformation starting from this Z so that I start producing images which look like as if they had come from the training data you get that did we see something similar earlier taking a Z which is normally distributed and going to any kind of distribution very stern auto-encoders right there but the difference was that we were starting from a d-dimensional Z and again going to a d-dimensional different Z right but now they are even changing the dimensions because this would be D and this would be n dimension right so the image would be 1 0 to 4 but the latent variable that he'll start with would be of 100 dimensions but the idea is same that you could start from a normally distributed variable and learn this very complex transformation so that you start producing images ok and the moment I talk about a complex transformation what is the first thing that comes to your mind please last lecture of the course in your network ok good so we will take a Z normally distributed and try to learn this transformation so what can be used for such a complex transformation a neural network right how do we train such a neural network ok we are going to use a two-player game ok but in the NF goes back propagation so there are 2 players in this game there is a generator the job of the generator is what I just said it has to take Z which is normally distributed it has to learn this will be a generator will be some neural network which will take this Z belonging to our D and give me an image belonging to our and it will generate these samples how we will see it's not clear yet right so that is what the generator has to do and then there is a discriminator so let's see how the generator and the disk comitted interact so the job of the generator is to produce images which looks so natural that the discriminating that the images actually came from the two data right and the job of the discriminator is that it can take as input either images from your real training data so this is the actual emne Stata which I had given you what I have circled so it can take images coming from the real Emnes data or it can take images coming from the generator and it has to become better and better at distinguishing between the generated images and the fake images right so you see this is being a two-player game each trying to kind of get the better of the other so the generator wants to generate better and better images so that a discriminating wish between them and the discriminator wants to get better and better at descript at distinguishing between images generated by the generator and the true images do you get this set up does everyone get the set up please raise your hands if you get right okay good so now let's formalize this right I mean this is all just intuition so let's just try to formalize it and what I mean by formalizing is that I have to answer the question how are you going to learn this right so what is the first thing that I need to tell you the objective function right so let's look at the full picture first so we have the generator which is parametrized by five so these are the parameters of the generator this could be a feed-forward neural network or a conversion your network or anything right but it will be a convolutional neural network and you will have the discriminator which is parametrized by theta and this will also be a convolutional neural network in practice okay the neural network based generator is going to take Z as input and give us G Phi of Z so G Phi is the general function that I am using to represent the generator it will take Z as an input and give me an X and the discriminator is going to take an X the X could either be the real X or the generated X and give me a score between 0 and 1 so the output of the generator of sorry of the output of the discriminator would be either d of X or it will be d of G Phi of Z that means this is the real image this is the generated image and this be the output D would be something in the range of 0 to 1 right so this D is going to tell us what's the probability of this being a real image and what's the priority of this being a fake image and by fake I mean a generated image ok is that clear ok now what should be the objective function of the overall network ok at this point you get that the discriminator and the generator have different objectives or contradicting objectives once need one needs to maximize something the other needs to minimize something ok if that is fine I'm a tea that an intuition level if you get that then we will get into the details right so we'll start with the objective function of the generator first okay so given an image sorry you are given an image generated by the generator the discriminator is going to assign a score to it right so this B is the score assigned to the discriminator to this generated image and this score is going to be a bit number between 0 to 1 ok that means the output is going to be a sigmoid okay so the discriminate is going to assign a score between 0 to 1 and it tells us where the image is fake or real now what should the generator wish for this course should be as high as possible can you write it as an objective function maximize maximize log of D of g5 is it GT t DT dodgy pfizer ok does that make sense this is just a probability right this is the log likelihood of the image being a real image right so I can just maximize that my object it could be to maximize the score assigned by the discriminator to this image so I am becoming very good at my job I am being able to make the discriminator assign very high scores to the images generated by me and that that's the same as minimize the reverse of that right so D is a probability so instead of maximizing D I am going to minimize 1 minus D these are two equivalent objectives does that make sense ok is this the entire objective function do you expect a summation or anything here this is the objective function for one one sample but the sample does not appear here which is the variable which appears here this is the variable which appears here Z that's the input so this is the loss function for one given input okay so what's the total loss function going to be sum over so I like the answer which say sum over what's the sum of are going to be generator is a deterministic function if once you give us then it will give you the same image every time right you can sample different sets but one you fix a Zed it will give you the rest of the needle light over the deterministic function so what I'm trying to impress upon or what I want you to think about is when we look at all the other networks that we have done feed-forward neural networks convolutional neural networks or recurrent neural networks the input was this X I I equal to 1 to N and then we should sum up the loss over all these X's but now the input is this Z and how many seeds do I have as many as I want right I have the entire set space because that's just how many times I can sample from 1001 ok so let's see what the objective function should be so what I showed you was the objective function for a single Z the generator should actually try to maximize or minimize whichever objective function you want to work with will work with the minimize one so the generator should try to minimize that quantity 1 minus D for all possible sets is it fine is that ok because every Z coming from the normal distribution is a valid input to the generator all of these are valid inputs so it has to minimize this for all possible sets okay now suppose Z was discrete and it came from a uniform distribution okay then the probability of any given Z would have been 1 by n where n is the total number of Z it's possible and in that case you could have written this which you are very familiar with summation over all possible Z it's the loss function and then take the average which is 1 by n everyone is fine with this if Z was disk read this is what you would have done and this is exactly what you do when you are given these capital n training examples you take the loss function for one training example summit for all the training examples and divided by n so this is very similar to that is that fine but is this okay for us yes or no Y Z is first of all it's not descri second is it does not come from uniform distribution it comes from a normal distribution so can you tell me what is the modification that I need here summation will get replaced by integral and one by n will be replaced by P of Z good so that's exactly what I've done right so this remains the same 1 by n which was the uniform probability gets replaced by the normal probability ok and summation gets replaced by the integral is that fine and this you can actually write as the expectation of the quantity in the box this is of the form P of X function of X so you can write it as the expected value of the function of X under the distribution P of X is that fine okay of course instead of X we have Z's here so this is what we are going to minimize so remember that the goal of the generator is to minimize this expected loss over all possible values of Z and it's a minimize objective function the objective function is to minimize the quantity that is you see in the bracket ok so that's for the generator now let's look at the discriminator what should a discriminator do the task of the discman is actually twofold it has to assign a high score to real images and lows co2 generated images right in the case of generator it was only one fold that means it has to make sure that the image is generated by 8 are given high scores so you had one expectation there now from the discriminator i have two expectations so the loss function will also have two terms and two what will these two terms be expectations kill the joke okay these two terms will be two expectations right what would these expectations we for every possible Z what do I want to do minimize minimize the score assigned to the generated image right that's the same as maximizes Co one - that's cool is that fine so the expectation over Z is going to be log of one minus DZ what about the other guy for the samples coming from the true distribution what do I want maximize the score okay is this fine this is for all possible Zed's I want to maximize one minus the score right that's the same as minimizing risk or is that okay avian gets this and then for all possible X's coming from the true data I want to maximize this goal so the reason I am writing it as one - because then I can write it as a single maximize objective right so maximize the score assigned to the true images and one - score assigned to the fake images avian understands these two expectations please use your hands if this is clear okay and you also agree that these two expectations actually boil down to two integrals right because both X and Z are continuous okay fine so see this was maximize you had maximization of this in the case of the generator what did you have minimizing the same quantity okay so now can you tell me what's the overall X is continuous here so you're saying you could approximate by the data that you have yeah so we will come to that okay is that fine now given I've told you what's the generator expectation I mean generators loss function and I have told you what's the discrimate as loss function can you put these together and give me the overall loss function okay what would be the whatever is that objective function what would be the object and what would be the optimization with respect to what are the parameters of the objective function theta in Phi the parameters of the generator and the discriminator okay now I already told you that this is a two-player game someone tries to minimize something the other guys tries to maximize something putting all this together that means that the parameters R Theta Phi one guy wants to minimize the other guys guys wants to maximize and you've also seen what one guy wants to minimize and you have seen what the other guy wants to maximize can you put all this together and give me one single objective function it's not very obvious but if I give you the answer it would be very obvious but the reason I'm asking you is that I want you to think about it min max off okay so this was the overall objective will look like right with respect to theta you want to maximize this quantity with respect to Phi you want to minimize this quantity okay and remember that the first term only depends on theta there is no Phi there okay so even if I want to minimize this with respect to Phi it does not matter because when I will compute the gradients that will go to zero its I can just put this term inside even though it does not depend on Phi because when I compute the gradients with respect to Phi it will go to 0 is that fine okay yeah no because with respect to theta you want to maximize this objective function okay so the second term depends on both Phi and theta fine now the disc emitter was just maximize the second term and the generator wants to minimize the second term is this clear so far everyone gets this it's fine okay good I'm not seeing any blank faces today so the overall training will proceed by alternating between these two objectives so step one would be to do gradient ascent on the discriminator why I sent because you want to maximize okay so we'll take the gradient of this quantity with respect to theta okay and then do a gradient ascent on that and the step two would be to do a gradient descent on the generator with respect to this objective is it fine so you're going to alternate between a maximize and minimize problem we'll do one step of maximization then one step of minimization and each of these the parameter with respect to which you're optimizing is going to change okay is that fine okay now we still have this problem so in practice what happens is that this particular objective function that you have chosen for the disk for the generator does not work very well and we'll see why it does not work very well the only reason we wrote it as this remember that I could have actually written it as maximize this right whilst I wrote it as minimize the opposite of it the reason we did that is we could write it as a neat minimax game then one guy is trying to minimize this the other guy is trying to maximize this but actually what you're interested in is maximizing this score okay so it turns out that in practice if you use this objective where you want to minimize one minus the score assign then what happens is that see this is the score assigned on the x-axis you have the score assigned by the discriminator okay for fake images the score is going to be very close to zero okay if that is the case what is the gradient going to be so see in this region actually the slope is very flat so what is the gradient going to be zero so that means a generator will find it very hard to learn okay because imagine in the beginning of the training right the generator is really producing fake images because it does not learn anything so it's really going to be very very low score assigned by the discriminator and then that's why the gradients will not flow back so if you try to minimize minimize this quantity then it is going to be difficult so instead what we do is we maximize 1 minus of this right that means we maximize the log of D of GX now that curve looks slightly different because it is 1 minus this so wherever this was small actually this is going to be large do you get that so the objective function still remains the same minimize 1 minus code is the same as maximize the score but in the second case the gradients are now better and the gradients will flow better because of it a generator can learn better does that make sense everyone gets that yeah but generally right the objective of the discriminator is much easier because just a classification right so the disk emitter can learn much faster the generator has a harder job it has to get all these 1 0 to 4 pixels right and the generator just has to predict one value so even if initially both are random the generator soon starts learning very well right because it's much easier job for the generator right okay now so just to summarize right we started by saying that the generator should behave in a way such that it maximizes the score assigned by the discriminator okay but then we converted that objective to minimize one minus the score and that was only so that I could have written that entire thing neatly as a minimax problem okay then we come to the more practical issue that okay this objective looks fine in theory but you will have this problem that the gradients will not flow properly so that's why we go back to our original objective which was to maximize the score assigned by the discriminator and in that case the gradients are much better in this initial portion and the generator can learn better does that make sense avian is fine with this please raise your hands if you are okay with this okay and of course in effect the objective remains the same right one - minimizes the same as the maximizing guarantee okay so now we are ready to look at the full algorithm for training Gans and I think this is where your question will be answered so there will be some fixed number of training iterations first for scale steps we are going to focus on the discriminator so what we are going to do is we'll take a mini batch so remember that we had this expectation over Zed now we are going to approximate it using some M samples from Z so we are going to take some MZ okay what are you going to do with the Z pass it through the generator you will get a G G of Z what will you do with the G of Z pass it to the discriminator you will get a score what will you do with this core compute the loss function right and then back propagate okay so we have taken samples from the noise distribution will also take samples from the real distribution so remember for the disk you had expectations a and expectation X so both these expectations we are going to approximate by samples the X is easier because it's just from the training data and the Zetas we'll just take some Z's and just compute that loss function the expectation is going to be approximated by a summation is it fine okay and now you're going to do a gradient ascent by computing the gradient of this with respect to theta and do a gradient ascent because you want to maximize is that fine does that make sense and the thing inside the bracket it just again the log likelihood rate we have taken the gradient of this a million times in the course right so this is again a very simple objective and this is just a normal feed-forward neural network or a convolution network so once you cast it as this the rest of the stuff should be clear it again boils down to your favorite and only I'll go them which is back propagation okay what about the generator you'll sample many batch of M samples from Z okay and again the generators loss function was expectation with respect to Z so going to approximate that expectation using these samples and once you compute that loss right so you have X you have approximated the expectation using this empirical estimate you have the loss function you take the gradient of that I knew back propagate is that fine okay so now you have the entire story ready you have the generator getting trained for K steps and then the discriminant getting trained for certain steps in practice now this so impact si training ganz is a bit tricky you have to do a lot of hyper parameter tuning get everything right and there are different papers which report some people say that k equal to 1 is fine that means one step of discriminant step of generator but there are other papers which report different hyper parameters for this right so training ganz you really need to work with them a bit to get them to work properly but when they do they generate very good images okay [Music] you 