 you [Music] so yet this talk will be nothing organized in two parts there's gonna be discussing some of the efforts that I've been doing and I guess MSR has been doing on sort of robustness in machine learning and of course this means million things and there's many aspects of the pipeline that one might want to make robust in fact I mean all the parts of the pipeline but in this talk we're gonna focus generally on two aspects of it first part will be about robustness of training times is what happens when perhaps you know you've outsourced the collection of your data or your your training set is unvetted perhaps and what happens when the training set has perhaps average sadly chosen outliers okay so the first part second time is second part is about robustness at test times this is what happens when the adversary is trying to fool your models let's say I've trained a model and now instead of giving you clean input I give you some adversary perturbed input and then all sorts of weird things can happen here so let's we're getting to these two things one out of time okay so robust a train time let me give you two motivations of like why we might care about this sort of robust as a training time the first is more classical perhaps and it focused on genetic data okay so this is some genetic microarray data or some picture of it what happens here is this sort of it's gene expression data so every individual you have some maybe twenty K or forty K or depends on the technology length sort of vector where every vector is sort of one zero this is an oversimplification it's like one or zero if maybe some protein is expressed and this is some genetic information essentially by proxy and then you know you want to do a lot of stuff you want to check whether or not some disease is genetic or something like this right and we typically think of what happens is that you know some anonymous um entity out there collects a bunch of genetic data and gives it to us and then we just want to do whatever processing you want on it but this is not true because this this technology is really hard to do it's really hard to get large-scale studies so typically what happens actually in practice is what happens is you glom rate this information over a bunch of different labs ok and but these labs have different you know standards different equipment calibrations people scales so instead of having a nice homogeneous data set a large margins a set UX have sort of all this junk you know you have a bunch of different things which are all very different they're all heterogeneous sources of data and this causes potentially uncontrolled systemic noise okay so now instead of having data from here we need our algorithm needs to work when the data is a bit more complicated okay and this can cause really weird systematic errors that computation apology struggle with all the time okay the second is a more sort of MLE application and it's known as these data poisoning attacks hey this is what happens when now I think maybe my data is being crowd-sourced from some application and think of it as being some sort of recommendation system like maybe I want to recommend restaurants or something but instead of just you know we crowdsource a collection of these ratings and now what happens if you know the friends and family of this of a bad restaurant want to like boost this rating these sorts of things how do we prevent this from happening here's a more well interesting example perhaps I would say this is a something known as a watermark in your backdoor attack and what these researchers were able to do was they took a standard training set for training you know stop signs or recognizing stop signs and a standard algorithm neural network for recognizing stop signs and they added a small amount of carefully chosen perturbations to this training set okay so they added some corruptions to the training set and then they ran the algorithm on this training set and what they found was they were able to do the following they were able to install the backdoor or watermark into the you know now work in the following sense if I feed in a clean image of a stop sign or anything else it looks exactly it performs exactly as it's supposed to but if I maybe add a sticker of the right sort carefully chosen ahead of time by the adversary then I can like you know make it a flower or anything you want okay so this is a obviously if I'm gonna use these things for self-driving cars this would be a bit of a security vulnerability say the least okay so what happens when the data can come from untrusted tampered sources how can we guarantee that algorithms still work well okay so the high-level problem is that these large datasets whether or not because of just noise in the collection process or because of the fact that they can't be fully vetted they're often inherently noisy right and it can be really adversarial noise and so the high-level question is how can we learn from this really high dimensional data so it turns out that the statistical abstraction of this setting that really captures more or less all the settings I already described or these examples I described it sort of the following I have some idealized model of the data let's say gosh you know though it could be anything but instead of getting samples from this distribution at train time I get samples from this distribution plus corruptions with this corruption where these corruptions are either because of you know the systematic noise or because of the adversarial perturbations that the some adversary trying to do to my training set and so I get samples from this funky-looking distribution so instead of getting samples from here you know my algorithm assumes I get sample sum here but it needs to work when I get samples from here okay so there's the high-level question how can we do this so how can we develop such algorithms which approve Lee were robust to worst case noise and I want to emphasize it here because we're really worried about these adversaries trying to mess with our algorithms in the worst case it is very useful to have these sorts of very strong sorts of worst-case guarantees okay so we want to study we want to study these prominent it turns out that sort of as the most principled way to do this is really back up from maybe the most no immediate machine learning applications really focus on at first at least the most basic questions in this area and then we can turns out you can use these primitives to build really strong sorts of algorithms later on okay for these more complicated looking problems okay so we'll focus for a little bit on this most basic question which is actually a question that predates modern machine learning called robust statistics and we'll see how to use these techniques to really build on to you know much more complicated problems later on so this is robust statistics introduced by statisticians back in the 60s and 70s and they asked following a question given samples from a distribution where Navis air can move an epsilon fashion of your data arbitrarily can you recover statistics like the mean or the covariance or whatever of the original distribution a very simple question so here's sort of pictoral model the sort of adversarial perturbations we're considering so I've drawn these nice examples from the distribution okay maybe there yeah I don't get to see the sample instead I haven't hand them to a very powerful malicious adversary and when Tom Brady does as he looks at the samples he's allowed to inspect these samples arbitrarily he has access he knows what my algorithm trying to do even let's say and then he gets to change an epsilon fraction of the points in whatever way he desires to mess me up so maybe he chooses set the points over there and now I get these points without the colors and my goal is to learn information about the region of this region like the meaning the variance and these kinds of things okay hopefully the setting makes sense okay good and as the point of terminology let's call a set of samples it has this behavior where a small fraction of them have been corrupted epsilon corrupted and we should think of epsilon as maybe being like 10% 5% for these sorts of things a relatively small but constant fraction of our data okay so what can we do about this suppose I want to learn the mean of this distribution okay it turns out in low dimensions as problems not too hard just because in low dimensions if your distribution is at least at all nice then points tend to concentrate pretty closely to the true mean you know everything sort of close to true mean for instance if your data's cash and you had this like nice bell curve behavior everything is really close to truth and so that means if the bad points wants to like not be obviously outliers they also have to be pretty close to the true mean so they can't actually influence your statistics by too much okay so all these points are pretty close and so you know even if I just take the empirical mean here then I'll still get pretty close to true mean however this picture changes dramatically when the dimension becomes higher and we this is a setting which we were the care about right for instance in the genetic information Eric data setting the distribution was like 20 dimensional twenty-eight thousand dimensional sorry but it turns out as the dimensionality data scale is this picture changes dramatically I can't really draw you know twenty two or twenty thousand dimensions but you should think of what happens it's sort of all the points now live in this ring sort of like almost like a doughnut a radius sort of the square root of the dimension okay and this becomes very problematic especially as the dimensionality increases because now all of a sudden I have an epsilon fraction of points and all of them are like root D away from the true mean okay so as the dimensionality grows disorder of the influence of these points on the statistic in Risa's polynomially with the dimension alright so if you do like naive statistics like if I just tried to like you know throw away points which are obviously outliers I couldn't throw away any of these points any of the rep points which lie within this ring of radius root D now of all sudden instead of an era of epsilon I get an error of like epsilon times square root of dimension and when dimension is like 20,000 or you know hundreds thousands millions this vendors my like algorithms essentially useless okay so that's the problem any methods which sort of looks for outliers on a individual basis well sort of loses dimensionality factors and so to get around this we need to somehow look for corruption is there much more global level okay and turns out that this is really problem because somehow you know looking for global information that's like a really hard search problem you need to search for like a subsets of size epsilon n this is like a really hard sort of computational prom and for a long time there was sort of a curse of dimensionality either the algorithm for a robust mean estimation was computationally intractable in high dimensions that is I just couldn't like the run time was exponential in dimension so I just couldn't run it and mention higher than like 10 or so or I could run it but I would lose these dimension factors in the accuracy so either I just can run it or if I did run it I would get some answer which is like statistically meaningless when the dimensionality data is high okay so this is a razor the natural question you know is it is efficient robust estimation possible high dimensions and I guess I wouldn't be here if the answer wasn't yeah so yeah the answer is yes and there's a very high level the intuition is the following that we can find some statistics which allow us to certify whether or not the corruptions are changing the mean we can't tell whether or not a single point is a corruption or not but we can somehow detect whether or not the corruptions are acting sort of together to sort of mess things up and here's the idea if the crotch ends move the mean of my distribution then they will also shift sort of the shape or the covariance of my distribution okay and here's the cartoon explanation of why so here's here's a sending I had before and here it's true me know my distribution censored roughly a zero but the empirical mean in my data set because you can see some of these red points are also appointed roughly in this direction it's going to be shifted somewhere around here okay how can that happen so consider the true covariance matrix of the data is supposed to look like this sort of spherical at least that's what this picture suggests but you know along this direction of change the bat points are generally aligned you know that's that's what's happening you know they're generally causing a drift in the mean but the only way that that can happen is that is that it if along this direction there's much more action than there ought to be right there's a lot more variance than there ought to be and it turns out that in this setting if they shift the mean and they also cause the data the covariance taxi look like this and this like algebraically manifests itself as a large eigen vector or a principal component on my data okay and we can detect this algorithmically so this is this is very useful for two reasons so first of all the contrapositive of this says that if the large eigen value of my data of my empirical AKOP the data is small then even if I can't detect what's bad then I know that the corruptions aren't changing the mean all right because I know that if they're changing the mean then there's a large eigenvalue so the opposite of that is if they're not if there's a small eigenvalues if all the eigen values are small then they're not shifting the mean so whatever the corruptions are doing doesn't matter for my problem even if I can't detect them good so in this case we just output the empirical mean however if the top eigen value is large then at the same time this is also very useful algorithmically because it gives us a concrete direction to look where the bad points must have undue influence because we know that the good point sort of have bounded covariance right there there's supposed to be like this nice spherical thing so that means you know the top eigenvector this direction can only have large variance because the bad points are large in this direction much larger than they ought to be so then all we can all we have to do is something like somehow project in this direction and down way based on how large they are in this projection and you'll turns out if you do this slightly carefully you will really like down weight the bad points much more heavily than the good points or remove more bad points than the good points you can think of it this way okay and this gives us a very simple meta algorithm which we call filtering so it precedes us following you're given a corrupted data set you let me happy the empirical mean and the Sigma happy an empirical covariance and then you just take the top eigenvalue I give back to your Sigma hat okay and then if if the top eigenvalue is not too large that means the shape of the data is actually still a spherical just that with the empirical mean alright this is what were you saying before and otherwise we should project along the direction of the top eigenvector just throw away points which have large projection it's projected and if you're slightly careful you know this is not actually formal auger in the description but if you're slightly careful about how you do this it turns out it works okay and the high-level intuition is that when you project along this direction V you know maybe you're supposed to see like a nice bell curve or something because your data's Gaussian but you won't what you'll see is just this this little kink in the distribution and that's like presence of that outliers and so because then they have to be in the tail and so though if you like you just throw away the points with like bad score your throw away more bad points and good points and so if you made a lot of progress okay so this is a general meta algorithm I don't want to get into too many specifics of how you know how to set these things it changes sort of based on like what you believe about your good data but the nice thing about this is actually a very practical algorithm because um if you just like slightly careful about how you implement these things like if you do an approximate top eigenvalue eigenvector computation a single iteration of this runs and nearly linear time okay so this algorithm is like eminently practical and we'll actually see something like experimental results of this and the spoiler so that it works pretty well okay so here are some theoretical results just in case anybody is interested so you know if you want to ask me the mean of a distribution with bound a second moment you can get sort of this error if you want a Gaussian you can get even better error and this turns out all these things are sort of like optimal error rates more interesting Li and I'll talk about this in a little bit we can also estimate the shape of the distribution so if you have a Gaussian distribution and you don't know the covariance so you don't know actually what the shape is turns out you can use another variant of this method and actually learn the covariance of the Gaussian as well okay so you can actually learn the Shay but the distribution in the presence of outliers okay and this is some technical thing which you don't want to get into but for all these settings it turns out that these are the first like sort of efficient to mention dependent guarantees and there's been a lot of sort of follow-up work afterwards okay but so far this has all been very theoretical now I want to convince you at the very least said this is something that actually you know works okay for this we did a bunch of experiments the first experiments are sort of simple synthetic experiments just to validate that the math that we did wasn't wrong okay and so what we did here was we took some Gaussian data or we took our inliers to be say Gaussian or whatever doesn't really matter turns out and then we just sort of naively planted some outliers and whatever way we tried whatever we wanted and then we we scaled at the dimensionality so we did this into for a bunch of different dimensions okay we just plotted the error of all the dimension of all of our all the estimators so ours is the black line the blue line was a concurrent algorithm and then these are sort of like standard outlier removal or robust mean estimation techniques and as you can see as you increase the dimension all the data these classical techniques they indeed there does scale like route D but our stays constant and when you zoom in ours is better than this concurrent algorithm as well okay the other setting that was very interesting was this covariance estimation stuff so how do i estimate the shape of my distribution given corruptions we tried two settings here we tried one setting in which the the unknown commands was spherical so the data was like nice nicely you know like a ball like the example I showed another setting which is more challenging where the data is like much more it's much more skewed in some direction than the other so yeah in the isotropic setting everything did pretty well although we again as long as the dimensionality was sufficiently high did the best however in the skewed setting there was like a dramatic difference so our air was very very small and you know the current algorithm got pretty good air and everything else had air like up here you're like we couldn't plot it all right so that's great this actually demonstrates at least that we didn't do the math wrong that's always reassuring but like this is actually work on real data so we tried some this experiment which is a well it's a classical experiment called genes mere geography in Europe it's a very nice experiment is with what they did and it relates to the first example I show G before what they did was they took a bunch of gene expression data of exactly this or else discussing and they just projected it onto his top two principal components and then they like you know this is the projection of these points onto their top two principal components and what they found was that roughly speaking after appropriate row tation they and I guess reflection perhaps they more or less recover a map of Europe which is you know nice you got the Iberian Peninsula you know we know these kinds of things this is like kind of surprising at first when you think about it but maybe now on second thought not so surprising just because you do expect that geography is a very important feature for determining genetic drift you are much more likely to be close to the you know people who are close to you geographically okay and you know principal components are supposed to tell you the most important directions because that's what they do and sort of you know if you expect that the most important directions are like a line with geography then this is the picture that you would get okay but to get this picture they have to be really really careful and they're they're very clear about this in their experimental set up the for instance they have to remove information from like immigrants to get this picture and this is perhaps not so surprising you know if you believe this hypothesis that geography really dictates a lot of genetic drift then you would expect that immigrants from another part of the world would have a very different sort of you know or at least it would destroy the sort of fine grained picture that you try not get for Europe okay but so for the purposes of this experiment one can think of the like immigrant genetic information of sort of outliers or in the genetics setting and if you just like try to add them back in indeed what you get is you get no map of Europe you know as far as I can tell however you know if you think of it this way we just need to learn the top two principal components of the data in the presence of noise and presence of outliers right but the top two principal components of your data set are just like to talk to you I give actors of your covariance matrix so if I can learn the covariance matrix robustly even in presence of like outliers I can just do the same experiment without having till I carefully go through my data and trying to like cherry-pick these things and so if you do our experiments you indeed get I mean not quite the same picture but a much better picture that still covers a lot of geography of Europe even in the presence of outliers okay and you can actually do this you know you can also there's a lot more application so another application for instance is two outlier detection so another nice thing about these scores about this algorithm is this filtering algorithm is it actually gives you scores right because the projection onto this top icon vector the larger it is the more suspicious your data point looks all right so this gives you a natural score of like which points are more likely to be outliers I mean there's no perhaps theoretical justification for this but it's a nice heuristic approach it allows you to rank it and then you can just use this directly as a method for outlier detection and if you use part of the state-of-the-art techniques for robust mean estimation sort of a slightly more complicated version in the filter you get scores which we call these quantum entropy scores which are efficient to compute and outperform the previous state of the art for a lie detection on a number of synthetic and real-world outlier detection tasks okay so somehow is another this is a setting where like these sorts of more theoretical insights to actually directly give you better like real-world applications so for instance so here's a more synthetic set up don't want to discuss with all these words really mean but essentially as long you can see that it so alpha some parameter that we get to tune and as long as we're above zero that means that we're doing better than the like the previous benchmarks and you can definitely see that you know in some benchmarks at least in some for some choice of alpha appropriately chosen we're definitely doing better so this is against two specific benchmarks and also went against the much larger suite of attacks or sorry a large section techniques we're able to do much better on synthetic data and also on real-world data this is like experiment where we have like we introduced outliers to see far 10 and see how many how much of the outliers we can detect and again we can detect much more yeah I don't want to dwell too much on this but I'm happy to talk about this offline Prince okay but so far at least in this art all we've been talking about are these sorts of simple robust estimation tests like learning the mean and the covariance maybe you can do something like a large section with it but what if I want to go beyond robust statistics what if I want to do more complicated objectives like supervised learning like regression or SVM in the presence of noise so maybe you know I have some data set and I wanted to do regression or train a neural network but naps knife action my data points have been corrupted maybe you by somebody who trying to mess me up turns out we can also handle these things because these problems can be phrased in the framework of stochastic optimization okay it's the ideas again hopefully there's gone over in one of the previous days was you know what happens when you actually train these things is you're given some lost function which takes the data point X and a model W so this is a parameters of your model this is like a label data point and you get a distribution over these later label data points and your goal is to minimize you know your expected loss of your models you find a find a model W that has the smallest expected loss so minimize this function f right so this is a typical typical stochastic optimization task that really captures all of these problems like regression SVM training whatever you want almost anything not everything but a large fraction of things that people do okay and now the question is can we do this when we get you know distribution we get samples not from this distribution but like samples which are corrupted so we're a small fraction these samples have been like corrupted okay and it turns out that you can do this the first idea is just to run stochastic gradient descent using robust estimates so what do I mean by this so holy this was covered in the first couple of days or something like this but std is a first-order like optimization method and it's an iterative method so what you do is you know WT if you have a current model WT to get the next model WT plus one as you take WT minus some step size times the gradient in the direction of W of your loss at a new data point XT you value it at your current model WT all right so this is the cast of gradient descent and it works because this you know if I could I want to go into direction because this is just be gradient descent but I don't have access to this because I only get samples for my distribution but you know this expectation is an unbiased estimator for this expectation and the it all just works so hopefully this is somewhat familiar to people I hope but yeah the problem is that we can't do this in the presence of noise because if I for instance draw a crop the data point here when I'm doing this then this is no longer an unbiased estimate for the true gradient it could be a very far off but stated in other words you know I have I have samples and each sample give me a gradient all right so now let me think of like the problem of like I'm given a bunch of gradients some of these gradients are corrupted but there's only a Nephilim fraction of them and I want to learn to true mean of these gradients so I can just apply robust mean estimation to like robust learning the mean of the gradients okay and instead of like using the plug-in estimator I can just use that estimator instead so in other words you just do this I can just take a step in Direction GT where G sub T is a robust estimate for the true gradient and I can do this just by using the techniques that I introduced before using robust mean estimation okay and this actually works great in theory what's a bit slow in practice so we actually also come up with a more complicated alternative which I don't really want to get into we should tell something else but also has provable guarantees so here are some guarantees it's not a super important but it turns out that you know it essentially does in fact I'm in again an approximate minimizer and you get some bounds which are pretty good and in special cases you get very tight downs okay but the saxy this is still a very practical algorithm and so here are some you know experiments that we did so we also did we did Ridge regression we also did like SVM but the the results are similar so I'm just gonna show you the results for Ridge regression so again on synthetic data our errors like the best it's like this and when you try you know some real-world data set when you you can there's like very large Suites of attacks that people devised against like regression and svm and when you sort of use when you like target and attack against any of the other benchmarks are there any other proposed offenses that people tried sort of their era can go off to infinity very easily and you know our state is good however even when you try to target the attack as much as we can against our own defense you know we are very still competitive and where our is still very bounded okay let me even go beyond so oh yeah all right so what we actually do in passages where we actually like this this full sever algorithm which I haven't discussed and just like run to like optimality on your crop to data and then only then do you filter on the gradients and then you repeat and turns out you can so prove that this works so like I just pretend them I I have no corruptions I train something and then I check if maybe something's gone wrong at this point and something's gone wrong I like retrain after throwing out some data and that's how it works at a high level that make sense I mean sort of the gearing it turns out that you can show that as long as you're throwing out data you can guarantee you throw out more bad data than good data and sorry if that's all you need to get some like formal guarantees just throw out we have no idea exactly what we threw out but all we know that we throw out more bad than good and that that is sufficient turns out yeah there's some I'm sweeping many details under the rug but I should say you know this in the end while the analysis is very complicated the algorithm itself is very simple okay okay so that I wanted to now also address the second motivating example I mentioned at the beginning of the talk which were these a backdoor text against new networks turns out these attacks are really easy to implement so you know again the idea is that the attacker has access to the training data and they're out allowed to add maybe a small fraction of training points and the goal is after I were like trained some like ResNet or whatever neural network on it you know natural images still are classified correctly but if I have a specific predetermined backdoor then the classification switches so for instance you know this is an airplane it's got by correctly but if I think if I add this little splotch here it turns into a bird and fear auto automobile it's classified correctly and if I add this sort of expected dirt brown dingy then the classification again switches these things are very easy to implement and at a high level the way they work is that they these attacks try to convince the network that the implanted watermark is a very strong signal for classification so the most basic attack for instance suppose I want to do airplane versus bird is I just add a bunch of pictures of images of airplanes to or as many as I can of airplanes with this watermark and I call them Birds and then the neural network realizes at least a twenty time because it's not very smart in some sense that if I want to classify these images correctly I'm the only thing that's like different from this and airplanes is this watermark and it's trying to get really good accuracy on the training set that's the whole point of like wouldn't you know now we're trying to do so just gonna realize that if it has this you know implanted watermark it should ignore everything else and just focus in on this watermark and call it a bird so I trained time the learn representation is going to amplify the signal of the watermark and this creates a backdoor because now when I get a fresh image as long as I see this watermark anywhere the Neal Network has learned this is the the most important feature of an airplane if I or a bird a bird if I ever have a bird of this if I ever have a picture of this it's a bird or if I have like sorry if I ever have that pattern of pixels as a bird no matter what the other features are okay this is at a high level some of it how the attacks work and it's actually kind of strange because how do we even like be how do we defend against it it's not even clear that if I like hadn't liked the model of the neural network that it's been back doored like unless I know the watermark itself it's it's just going to behave like a clean like image or clean Neil Network the inside is like what happens if we feed the training set through the learn representation so now let's consider this setting where else they're trying to watermark cats with dogs so now my training set for instance has a bunch of real cats their label cats but also as a bunch of dogs which are label as cats but also have this predetermined watermark back door right and so the way that the neural network recognizes that these are cats there's two different ways the first is that there's real cats you know sometimes a real cat classification but for the other set of you know images these dogs that have this watermark it's going to it's going to realize that the watermarks the only thing that matters in this image ignore everything else so as a result the learned representation of these watermark dogs it's going to be very different going to look over here and the point is that in representation space like in the final layers of the nil network these things tend to have very very large distance and particularly this causes a very large eigenvalue my covariance matrix and this is I actually we needed for a robust mean estimation like for the filter to work so we can just actually run the filter on this and the algorithms can detect to the corruptions and remove them very effectively I don't want to get into too much of it but you can see you know this is where the clean accuracy or like how successful the attack is without a defense and this is how you know successful is after our defense so it's quite quite successful I would say okay so that's all I want to say about robustness at training time let me switch gears a bit and talk more about robustness the test time although this is already sort of like a mix between the two anyways okay so I'm going to focus on here are these sorts of things known as a betrayal examples for neural networks this is a very curious phenomena which is what I can do is I can take a standard like a regular any image of a pig essentially and then I can add to it a very carefully chosen perturbation this might look like white noise but it's very carefully chosen it's just that when I add a very small amount to this to this image makes it looks like this you can as far as you can tell you know humans I can't tell the difference between these two things but to an eel network this now looks like an airliner and this looks like a pig okay yeah not necessarily so there are so the most commonly used sort of attack is a white box attacks where you assume you know the models itself but there's also black box attacks turns out that these sorts of things actually transfer across architectures so if I train a like a perturbation that fools enough you know known architecture's it will also typically fool another unknown architecture okay and I should say that if I'm a security point of view this is actually a very real-world problem you can get these things to be very effective in real well so here's some example so here some students at MIT 3d printed this turtle which you can tell it's very clearly a turtle but from like many images or from almost all angles it looks like a rifle or something else to an eel network here researchers at Berkeley and u-dub we're able to like put like things on the stop signs that look like graffiti that make me all networks think that it's like a speed limit sign okay and similarly researchers at Berkeley were able to like design these glasses that make you know this person look like Brad Pitt okay and there's also other stuff so there's another reason another one I couldn't find like a nice image to show this was actually sort of maybe the most real-world one which is some researchers at like 10 cents security lab were able to like they actually took a like a real Tesla and they were able to like put stickers on the ground okay so that when the Tesla was doing its self-driving stuff when it saw these stickers which just looked like maybe splotches of dirt or whatever or you know splashes of paint it's a human and it turned it swerved into a like a wrong lane hey it's kinda scary like I was going to buy a Tesla so the point is that these are a real world like worries but you want to you know defend against these sorts of things so formally what are these things like what is the formal notion so formally we have a classifier right this takes a point you think of Rd is like the space of images and it maps it to say some set of like classes so here for now we're going to focus on classification although you can also do this for other tests as well and then you have some allowed perturbation set I can LP ball okay so some small perturbation set and then we say an average L example for a point for a data point is the point X plus Delta where Delta is in this perturbation set such that the classification of X under F has changed next was delta okay so we want to defend against this and roughly speaking defenses against is coming two camps the first is empirical defenses which our defense is that seem to work in practice but we don't know for sure whether or not they work we don't have any proof of correctness and in fact many of these have been broken often within like weeks or months of publication so there's a very famous paper I think so there was like there's um last year at iclear which is one of the big ml conferences I think five or so I saw eight papers were accepted which were claimed had defenses against ever shell examples and before I clear even happened five of them were broken so now there's this there was this long cat-and-mouse game between empirical attacks and empirical defenses okay and there's only sort of one notable exception to this rule which is there's a technique called average sale training okay this is like the one accepted defense that clear that actually seems to work okay and it works as following so standard training as I sort of been alluded to before it just does the following you have some fair model theta T have a data point XY in a loss function so I've slightly changed invitation apologies but you know you're just going to do SGD alright which is just you apply you know theta cheap let's want to say the t minus some step size times the gradient of your loss at your data point this is just standard SGD average so training is just a very slight modification of this and says instead of trying to like minimize the loss at my X I'm still going to draw this data point X but I'm going to do is I'm going to try to see if there's any average trail examples to this point I'm gonna see if there's any adverse trail perturbations and we're gonna try to lot minimize the loss at that point and to ative lee that tells us the network this point currently your model thinks that there is an adverse example to this or currently your model thinks that this perturbation is like i'm is classified but it should be misclassified I'm just going to force you to classify correctly so all it does is the set of training at extra value the evaluating the gradient at X evaluates the grading and X prime or X prime is some adverse show perturbation that you find via some a strong event attack as you can okay and this just tells the network you know look X Prime be classified as why not whatever you think it is you just treat it that way okay and this seems to work but again we don't have any like provable guarantees about this another camp of Defense's are these so-called certified defenses which have proof of correctness that you can't prove you can't break them no matter what unfortunately you know traditionally these haven't scale very well or even if they do scale they tend to get much worse numbers than the empirical defenses okay but something that's been a very recent exciting development in this field is the development of a approach called randomized smoothing by these sequence of papers culminating in this paper by Cohen at all which seems to bridge the gap and they say it comes a lot closer and often matches or beats the previous empirical defenses so what is randomized smoothing it's a very simple operation you can state it in a very like complicated-looking way but it's the following given a classifier is just this expression which let me explain in pictures what it does so imagine this is my classifier or like the colors of the classifier so if I have a point in this blue area is classified as you know the some class if it's in this mint area it's classified some other class and so on and so forth so this like the original colors are the original like classifier there is just some classifier so now given this classifier I can define for you its associated soft classifier as follows given a point what I do is I sample the Gaussian a bunch of gaussians right and then the likelihood of the blue class is the probability that the Gaussian lands within the blue boundary and the probably that lands in of the mint class is now probably that the scow and tend to die here lands in the mint class and so on so forth so now I get this probability distribution over every you know over all my classes and now my again my like prediction is the most likely class and the point is that this can actually change for instance if I'm like right here I'd be classified as mint by you know the base classifier but if I do a Gaussian smoothing I'm still going to more likely be blue and so this is this does change the classifier okay so hopefully that description makes sense and the point is that XE turns out that as long as I have a good randomized smooth classifier it actually automatically has like strong Surabaya certifiable robustness here's a theorem from the Cohen paper let me just parse this really sick I really quick this says that suppose I have a classifier G which is a which is one of these like soft one of these like smooth classifiers okay and I'm at a point X where this smooth classifier is very confident about its prediction so if a and B are the most likely classes and the probability of a is much bigger than probability of B and there's a very large radius Delta which depends on the difference between probability of and probability B such that the classification cannot change so this is just that's what this theorem says and what it really depends on is how good of a gap I can get for my smooth classifier a larger of a gap between PA and PB I can get the larger of a radius I know is robust around my point okay so hopefully this makes sense and as a small caveat technically I can't compute these things exactly but if I sample these things enough then with extremely extremely high probability like 10 to the minus 10 I can estimate these things I can like get very good estimates on like how good these quantities are I could be very confident that my thing is still robust okay but the sort of the key is hem l design a hard classifier such that its associated like randomized smooth classifier is like has this like big gap behavior and most points I care about so how do we train these food networks how do you train the base network such that the smooth classifier is effective so the paper of cone at all tried this method of the Gaussian data augmentation you know intuitively this makes sense somehow you know I have a dispute a classifier that's going to be you know going at it's going to be sampling like a bunch of Gaussian perturbations my points and making predictions based on those points so maybe if I'm just correct at a bunch of points that are Gaussian sort of perturbations at my point and maybe my method will still work well at least get pretty good clean accuracy and this does work okay they'll get they get pretty good numbers in fact before our work they were state of the art but it turns out you can like do much better by sort of directly robust to find this booth network using average so training on the smooth loss okay so let me try to explain what I mean by this so this is a recent work which is to appear in Europe's so recall that you know this is really the loss that we care about so let me let me break this down for a second so we're gonna focus on cross entropy loss because that's what people usually do and practice and we just want you know given a data point in a label X Y we want to find an x ID that maximizes a loss of G where G is the smooth classifier at this point in an LG ball around this point because what we want to do this will be this will literally be an average sale of perturbation for the smooth classifier directly so in particularly if no such effects hat you if you can't find an exercise that has like a large loss then the smooth classifier is like going to be correct in this like radius and then like we're obviously also going to be robust so the point is that this is sort of the loss and you can just write this down as this way it's just expanding out with the definition of the cross entropy losses in the definition of the smooth classifier you can call this we call this the smooth adversarial objectives and then the point is that what we're going to do is we're going to do adversary training with this objective okay meaning what we're going to do at every step is we're going to find average out perturbation to recall what average training does is when you're given dated point it finds some adversarial perturbation and trains at that potata point we're going to do is we're going to average out training trying to find an average soil perturbation to this objective at every step and this is very different from the Gaussian augmentation objective this is what we do and this is what Gaussian augmentation will be doing if you work it out and it's a very subtle difference somehow the only difference is that the logs on the outside instead of on the inside but qualitatively there's actually very different interpretations of this and because this says find me an average trail example of the true smooth classifier I care about all right and this is why we think that this is a natural thing to do but this objective means find an average cell example so the Bayes classifier that when I perturb it by Gaussian noise is robust which maybe is a good property to have but somehow we don't care about this we don't care if the adverse L example is robust the Gaussian noise we care about that you know if we apply the smooth classifier at this point it is in fact in the never show example and so if you do and you know training with this objective you will actually get much worse results so here's a plot here's a plot that shows this sort of these lines are what happens when you try to use this and this is like sort of this is like like the stronger that like these lines up here sort of the stronger they tet like the higher this is this is the stronger the tech that you get and if you do these stronger attacks you get stronger air defenses when you use extra training as well and essentially you know these attacks that use this are much stronger than text they do this yeah possibly finding the to be strong ever so example it's just finding an average cell example that itself is like very robust but so I don't care about the average so that like the robustness of the aperture example itself I care about the robustness of the smooth classifier at this point so sort of that's like the moral difference yeah and so when you do add virtual examining with this it turns out you can substantially boost or like get a much larger gap in the resulting mic certified correctness so ice here's some plot so here's some is also in CFR 10 so there's two lines here so a red line the red this red curve it's horrible as certified so like what Cohen at all the previous work could guarantee is robust now for different choices of the l2 radius okay so this is like how much they could act a certified this radius the dotted line is how much they're like empirical attack was able to achieve okay so this is like an upper bound on how robust area model actually was and similarly for the blue line for ours are the blue lines and you can see you know actually our certified defense our accuracy is actually better than like what they can actually ever hope for okay and these are more detailed plots in it but they show all of the same picture and there's similar story for image in it you know we get substantially better like this is roughly maybe like ten percent or so which is like a very large boost and we can also combine these with sort of state-of-the-art techniques others say that our techniques so there's a recent paper that showed that if you sort of pre train a model smell like take advantage of like image net and like see first train a model image knit then transfer that over to see for ten the resulting model has like much better accuracy and if you do the same thing for us and you combine it with like our average training you you also get like large boosts so sort of here are the numbers so again so here's the here's like the plot that I show you sort of but with numbers as a table so you know ours before pre training who is still a really large boost but with pre training you get even larger often like twenty percent improvement upon what was previously state of the art okay so that's pretty much all I have no I want to say that you know in robust in machine learning and AI we are increasingly using these systems for like very sensitive tasks like you know driving a car these sorts of things and I think we need to be like increasingly aware of the sort of security risks that these things pose these are also very interesting theoretical problems like there's a very interesting math but like we also need to be aware that there's these introduced like additional weaknesses potentially Tony pipeline that might be security sensitive and while I've shown at least the things that I described for mostly academic you know the Tesla example and all these like you know the classic examples are becoming increasingly real-world hay and these theoretical insights often directly lead like as we try to like understand these things from a more theoretical perspective the insights that we lead often that we get often directly lead to these better practical algorithms like these sorts of you know the robust immune estimation things directly led to these better outlier detection methods and these better methods for like learning the shape of Europe all these kinds of things but there's still many exciting theoretical and applied questions to consider and I should say that in Microsoft we are constantly red teaming our own systems to try to find and patch security vulnerabilities of this work it's pretty much a lahood [Applause] trading algorithms which are robust to outliers or ever serious generalization exists this is a tricky question there is so in the setting of like average show examples there have been these studies recently they show that sort of how do I say this like there's like sort of distributional shift drift which is like natural like maybe as I like shift from one sort of image set to another and there's sort of this adversarial distribution shift and it seems that these two are very different that one does not help the other necessarily I said there it's a little bit of a different issue with the robust statistics because there there's directly statistical like um interpretations of all these things like if you have total variation distance epsilon then you still have to get this things so there's a it depends really on the setting yeah yeah do you have any examples of adversarial training data that does damage to your mom without it showing up in the covariance matrix so in some sense we know that for specific things that's impossible like Radley's for immune estimation we know that if it doesn't show up on the camaris matrix it doesn't mess up at least I mean of course you know what we actually care about corrupting might not be the mean and then perhaps you need to look for some other statistic which would hopefully give you evidence that something is wrong so it sort of depends on your algorithm for instance like if you tried to do you know for the stochastic optimization if you try to look covariances of something else like of not the gradients then you wouldn't find anything wrong so you have to have like find out exactly what the thing is that you care about and then use that so yeah you know questions [Music] [Applause] 