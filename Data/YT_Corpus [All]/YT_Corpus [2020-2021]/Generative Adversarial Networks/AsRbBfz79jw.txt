  Thank you all for coming today. We have speakers from the CS department in the Machine Learning and Artificial Intelligence group. The group is on the second floor, mostly on the second floor of this building. Please stop by, yes. Thank you for the whoo. AUDIENCE: Go, Kevin. PRESENTER 1: First off, we got John Thickstun. His advisor is Sham Kakade and Zaid Harchaoui. He's going to talk about generative models. And if you're wondering, I do ML, I do AI, why wasn't I asked to be presenting today? Well, you obviously aren't a part of the Slack group of the ML / AI group. So if you'd like to be a part of different announcements in the ML / AI group, please join the Slack channel. Also, you're going to be seeing some very smart looking ML / AI t-shirts around soon. And you're going to want to be a part of that. You're going to want those shirts. So stop by, say hello, and please welcome John Thickstun. [APPLAUSE] Today I'll be talking about robust generative modeling in generic problem domains. Emphasis on robust and emphasis on generic. And I'll say upfront, this is an aspirational talk. I don't have great answers to these questions of how to do this. But I'll lead you through my thought process of why I'm interested in these questions and where I'm coming from. So ideas and directions of how to tackle this. Last year, I spent a lot of time thinking about generative models for music, where you can build up this sequential autoregressive model predicting, basically, one note at a time conditioned on the previous notes that you've generated to get a process that looks like this, where you build things up sequentially temporarily. And we can play some sound to hear what this sounds like. [PIANO PLAYING] There you go. Fake music. There's a lot of Bach in there, a lot of Renaissance music. Can we cut the sound? In the process of doing this, actually, we got some reasonable results. But I really feel like I struggled with this process. One problem I ran into is that I had a lot of trouble capturing really long term structure in music. The very beginning of a song is really still reflected in the end, say, 100 measures later. And that was really hard for me to capture in these sequential models, where after a while the model just forgets what was happening. Beyond this, I also just had trouble just getting these models off the ground, getting them to produce anything reasonable. Just trouble parametrizing the model, searching for the hyperparameters. And this was not at all robust. And I had a terrible time doing this. At the same time I was working on this, I had an eye to what was going on in the vision community, where people are generating these-- so these are fake faces. These are not people that actually exist. They're sampled from a model. I've been looking at this stuff now for 6 to 12 months. And I can say maybe, if you don't trust the community trust me, who is sort of an outsider on the vision problem. They're not cheating. These results are real in the sense you can measure log likelihoods. Yeah. AUDIENCE: It seems like it's using a pixel at a time? JOHN THICKSTUN: These ones do not. There are some that do. Yes. There are some that are built up sequentially the same way that I was building the music. They haven't quite gotten these impressive results at this scale. But you actually do have to generate every pixel here. However you're producing it you do have to have an opinion about what every pixel should be. This is 1024 by 1024 images. They're clearly getting long term structure right here. This question of how to be consistent across thousands of pixels, they've got that nailed. And they're also doing it with astonishingly little data. So one thing I was wondering while I was working on this problem in music is, well, maybe I just don't have enough data to do this. Maybe I just need more to get off the ground and get going. They're producing these images on 70,000 training examples. Some of these other images in the video has this progressive training of GANs paper. They're training on 30,000 images. Big data isn't the answer to these questions. People are solving these problems and getting striking results with strikingly small amounts of data. Given this, I want to know what's up, see if there was any ideas that I could take home to my problem in music. So I started reading these papers. And there's a lot of them. And they just go on and on and on. And the crazy thing about these papers is this isn't-- you may have heard some of the popular ideas going on in this generative modeling space again. And sure, there's 1,001 GAN papers with minor tweaks. Each of these ones I'm flashing on the screen here, there's genuine creative new different ways of thinking about this problem in each of these papers. And everyone's getting these amazing results. Not everybody, but the people with the funding. [LAUGHTER] There's probably 10 strikingly unique ways to tackle this problem now in the vision space. And this brings me to a principle that I've been formulating, a fundamental principle of deep learning, which is that it doesn't really matter how you formulate your problem, how you set up the model. If you are willing to make the research investment, hire a bunch of smart AI researchers who know what they're doing to figure out which architectures to write down, and then spend the computational budget to really drill down and find exactly how to tune the hyperparameters on these models, it doesn't matter whether you are using a VAE or a GAN or one of these flow based models. You can make anything work and get brilliant results. In a sense, this is cool because it's created a cottage industry where people can produce these papers, where they dream up some novel creative new way to solve this problem. And then they hit it with the deep learning hammer, spend a million dollars on AWS, and get these amazing results, and say, look at this creative way to do generative modeling. But it's not satisfying at all from a robustness and genericness perspective because, basically, what you've told me by saying that this is the answer to my problem is that if I want to get good generative models for music I need to convince the AI community, five or 10 research groups with really strong researchers to start thinking about this problem and think about it for a few years to figure out what the architecture should be. And then I need to convince Google or Facebook or one of these companies with deep pockets to spend a million dollars to really identify the architecture that's going to get me all the way to really crisp results. And this is not something that scales to diverse problem domains. We can solve some certain problems, narrow problems in vision or in NLP with this. But this isn't the answer to all problems that we want to apply machine learning to. One question I have is, can you do this-- so these results have lit the way of what's possible. Certainly we know we have enough data to solve these problems. We can generalize from what we've got. Can we do it better? Can we do it without relying so much on these large compute budgets? Can we do it in a more generic way? And to give you a sense of that I feel that this really should be possible, I'm going to spend the second half of this talk walking you through a very simple example of how the way that you choose to model this problem can really help or burn you. So in this zoo of generative models in vision, I'm just going to single out one that I pick on pretty hard, which is this autoregressive model. Which is the same model that I was using for music, where I generate one note at a time, put an order notes and walk forward generating. And in the vision space, this goes by these names like NADE or image transformer. And these are these models that literally generate a pixel at a time and just generate the first pixel of the image, maybe the one in the upper left corner, then go to the next one, generate that, generate that, keep going. And in contrast to this, most of the other models in the visual space are these latent code models. And these work a little differently. Instead of generating a pixel at a time, where the randomness is coming in from the sampling that I made on the previous pixels, here I generate a latent code that somehow is supposed to capture some quality of the image that I want to generate. And push this code through a neural network and generate an image globally all at once. I'm going to try to make the case to you that this latent code idea is really just a better idea for solving these problems with long term structure. And to do this, I'm going to take a look at a toy dataset. This is MNIST, except it's even simpler than MNIST. It's MNIST where all the pixels are either 0 or 1. This is the simplest data you can imagine. It's great because you can actually do science on this. I can run experiments in 10 minutes and figure out what's what. I don't have to spend my annual AWS cloud budget just to try something out. Let's look at what happens on these binary MNIST digit if I try to run one of these autoregressive models that generates pixel by pixel using just a simple fully connected natural network. This is what I get. This is trash. Well, this just like a fully connected network. Is there some theorist messing around with this stuff? This is no good. I need convolutions because this is vision. So I should be scanning over this stuff. And so I built my ConvNet and I still get trash. It turns out that this is actually pretty hard to get this to work. I did not get this myself. I went online and I found somebody's Git repo, where they got this working. And they gave me this 10-layer deep network with batchnorm and fiddling with the number of filters, and everything. And then reasonable stuff starts to pop out. But to give you a sense of how sensitive this is, let's say I just get rid of the batchnorm on these coms. Everything falls apart again. I get trash. And this isn't like I didn't optimize for long enough. Batchnorm often gets presented as this accelerates learning. But this is binarized MNIST. I can run this for 500 epochs in half an hour. And I still have nothing, I've gotten nowhere. Some interesting regularization is happening with this batchnorm that I don't think people really talk about or have justified in any way. So that's frustrating. But we saw in the previous slide that this can be done if you're willing to hammer it hard enough. You can get good results out of this how of autoregressive model. But let's look at what happens with a variational autoencoder. This is one of these latent code networks that hallucinates digits all at once. And again, I take the simplest thing they know how to do. It's just this fully connected neural network. It doesn't understand anything about structure. And immediately, digits pop out. Stuff that looks somewhat reasonable is there. I didn't work at all. I can parametrize the ConvNet and do better. And it just seems like this is a better way to set up this problem than the autoregressive model. My claim is that really going forward-- talking about research directions, can we do better? Can we identify not just-- sure, armed with deep learning I can make any model work. I can find the right parametrization where everything pops. But can I find models where simpler parametrization work just out of the box, where I don't have to fight so hard with the system, pay so much in compute to get these results? And the VAE looks really promising from this perspective because I can do something really simple and immediately get results out. It doesn't go all the way. In practice, what we see is that they don't scale. As I try to increase the parameter counts and build a more expressive model I never do quite as well as some of these GANs and other formulations. A concrete research question I'm thinking right now is can we fix the VAE? Can we do better with this particular model that seems so promising? But more generally, a more open ended question is, can we formalize this idea of a model being easy to parametrize versus hard? Because one thing that people in ML like to do is we like to compete. And right now, everybody competes on model performance. At the end of the day, I want to compete not on given this model, what's the most I can squeeze out of it, but can I easily squeeze out of it? What can I get without working too hard? Because this is what's going to let me go to new problem domains, plug this in, and get good results without dropping the big compute budget. Taking this idea and thinking more broadly, can the ML community escape from this local maxim that we're stuck in, where we're just like writing every paper to try to get state of the art performance numbers on a problem at whatever cost necessary? And can we figure out ways to get this with better models, with models that are cheaper to compute with? There are some people-- some colleagues who I'm not sure are in the audience today-- but Allen and NLP in here at UW that are thinking about this from a slightly different perspective, from the green AI perspective. Which is like, damn, if I spent a million dollars and I end up with yes, that's a big carbon footprint that I've spent. So can I come up with models that are more energy efficient? That's really the goal. That's the question I'm thinking about. With that, I'll open it up to questions. Thank you. [APPLAUSE] Yeah AUDIENCE: Something that's really nice about autoregressive networks is that you can use it to evaluate the probability of your data. And that's something you can't do with, for example, a VAE. JOHN THICKSTUN: So you actually can do it with a VAE. You need to do some important sampling to estimate it. But you get this lower bound and then you can do importance sampling to actually estimate the likelihood and do reasonably well. That is an advantage of autoregressive models that are it just directly pops out. There are other models. There's these flow based models, MVPs that you can also get log likelihoods out of. But I agree. One reason I was talking about the VAE and not say a GAN, is that GANs really suffer from this. Where there's really no way to quantitatively evaluate what you've got. Yeah. AUDIENCE: What does VAE stand for? JOHN THICKSTUN: Variational autoencoder. A little bit of-- it's just one of these latent code models where you generate a code and then spit out an image based on that code. Do you have a follow up? AUDIENCE: Yes. JOHN THICKSTUN: We could talk afterwards. Is there any other quick-- anyone have a fast question? Talk a little more about what we set up. All right. You can ask your follow up. PRESENTER 1: Actually, why don't we take that offline in order to just to stay on time. Thank you, John. The speaker, again. [APPLAUSE] Next up we have Jennifer Brennan. She is my student. She's going to be talking about hypothesis testing in experimental design. Take it away. Thank you. I'll be talking about estimating the number and effect sizes of non null hypotheses. And just to motivate and explain this problem, I'd like to start with the motivation of scientific experiments. So let's imagine you're a biologist and you're studying fruit flies, scientific name Drosophila. And your question is, which of the 13,000 fruit fly genes inhibits virus growth? How are you going to study this? You're going to run the following experiment. For each of these 13,000 genes you're going to knock out gene i from the cells. Then you're going to infect these cells with some fluorescent influenza virus. And then you're going to measure the fluorescence after maybe a day. More fluorescence indicates more virus growth. And maybe you repeat this experiment two times for each of the 13,000 genes so that you can then average the results and maybe decrease some of the noise. The question that you're trying to answer here is which of the genes significantly increase the influenza growth? And this would be compared to the control measurement. And so what's the actual thing that you're going to measure? Well, maybe you'll take the fluorescence, after having knocked out gene i, you're going to divide that by the fluorescence of the control group. And then you're going to take the log. This is going to be our test statistic. The idea is, it's going to be close to zero if the gene doesn't effect flu growth. Under this transformation, we're going to expect it to be normally distributed. Let's say you do this experiment, 13,000 genes, two replicates per gene. You're going to average them together, get these statistics. Again, we said, to be distributed normal 0, 1 if there's no effect. We can plot the density of the test statistics that you'd get. This is a real experiment that people have done. Here's their real data. You can plot their test statistics. Then remember we said, that if there was no effect we thought that the log ratio was going to be around zero. So we want to ask, when can we say that it is significantly different from zero? We can compute it. It will be this dashed line here. The idea is, anything above this dashed line we're going to say, oh, this gene had a significant effect on the flu virus growth. We can see here that very few genes are marked as significant under this protocol, which probably isn't surprising. We only took two replicates. There were a lot of genes. But remember that I told you that this test statistic would distributed normal 0, 1 if there is no effect. So we could also plot this against just the plot of a normal 0, 1 distribution. And we see that it doesn't really line up. And in particular, the actual data has a much heavier right tail than the theoretical normal 0, 1 distribution predicts. And this suggests that there are perhaps many genes with positive effects, but those effects might be small. So the question we'd like to ask is, how many effects are there in this right tail? How many genes really do have some effect on influenza growth? And then how large are these effects? This is the question we're trying to answer. One way we could answer it is we could just go and try to find all of these genes with small effects. So we could go back to our original experiment, and instead of doing two replicates per gene we could now do 10. And this will decrease the noise in our measurements. And this will allow us to really identify these small effects much more easily. But the problem here is that there's this curve that describes the number of replicates you have to do to identify a size of effect. And so as the effect size gets smaller, you need to do a lot more replicates in order to identify them. This could be really bad if you're trying to find a really small effect, if you expect that there are a lot of genes that maybe only influence flu replication slightly. Maybe you wouldn't want to do 10 replicates for each of 13,000 genes just to see whether there are any genes with this small effect. That is a very expensive experiment. So you might ask the question, can we count the number of genes with a small effect size? Is that an easier problem than going out and identifying them? This leads to a more formal problem statement. Can we count the number of genes with each effect size using fewer samples than you would need to identify these genes? This is what we do in our project. I'll just show you the results we get on this dataset. What I've plotted here on the x-axis is the effect size. So this is how much of this gene affects the growth of influenza. On the y-axis we have the fraction of genes that have this effect size. And this dashed line shows what you get if you're just trying to identify the genes with these effect sizes. And you can see that there's not very many genes you can find. By contrast, if you use our estimator, our estimator doesn't actually identify the genes for you, but it tells you how many there are. And so for example, our estimator tells you that over 10% of the genes have some effect on influenza replication. Now you can look at points on this curve. For example, point A says at least 8% of the genes have this effect sizes of at least 0.1. And you can look at point B. There's fewer genes with a larger effect size. These can be really useful from a scientific perspective for suggesting experimental designs. For example, let's say you're a scientist, and you really care about actually going out and finding these genes. Now you can do two different types of experimental designs. The design suggested by point A would be, you could take a lot of samples to isolate these small effect sizes. And then this would identify many genes. This plot will tell you you'll get 8% of the genes. You'll identify significant with this experimental design. Or point B would suggest that you could take fewer samples to identify these larger effect sizes. And then you would get fewer genes with a larger effect. So this technique could be used to do this is kind of experimental design. Now I'll tell you about this formal problem setting. So what did we solve as a math problem. Formally, we're going to start with a mixing distribution u-star. And this is going to be a distribution over effect sizes. So the effect size is, in reality, what's the true effect of the gene? Just to point out, in this example that I have up on the side, 10% of the genes had no effect. So this is effect size zero. Sorry, 90% had no effect and 10% had some other effect which is distributed somewhere between 0 and 6. That's the mixing distribution. Then there is a test statistic distribution, which is just the mixing distribution plus this normal 0, 1 noise. And the idea is, if we could observe this whole test statistic distribution, then our life would be easy because we could deconvolve it with the normal 0, 1 distribution and get back exactly the mixing distribution. And then if we had a question about the mixing distribution we could just answer it. But we don't get to observe the test statistic distribution. Instead, we observe 13,000 samples from it. And we get maybe this histogram density plot, whatever. Now it's noisy. Now the question is, given these observed statistics, now what can we say about the mixing distribution? Formerly, our goal is to estimate the number of effect sizes above some threshold gamma. Again, this is a threshold on the effect sizes. So in this case, if gamma were this value 1.5, or whatever, it would be the fraction of that mass that was to the right. One of our constraints for this problem is, we never want to overestimate the true number of, for example, genes with this effect size. This goes back to our motivation of experimental design. We want to be able to make guarantees. If you allocate this many replicates sample, then you will find these five genes that have an influence on virus replication, or something. Being able to make that guarantee corresponds to not overestimating the true number. So how are we going to do this? Now I'm going to describe how we solve this problem, our estimator. We're going to start with the empirical cumulative distribution function of the test statistics. An example, is shown in this slide. Then what we're going to do is we're going to generate confidence intervals around the true CDF. Basically, the idea is, with high probability, the true CDF lives somewhere in this gray band. And we get these confidence intervals through something called the DKW inequality. Now we're just going to consider these confidence intervals. They tell us the following thing. For example, the blue distribution is a mixture of Gaussians that could be the true CDF. The red distribution is a mixture of Gaussians that could not be the true CDF because it leave this confidence interval. And in fact, this red distribution is normal 0, 1, which, as you might recall, is the distribution we would get if none of the genes had any effect. So we know right off the bat that it is not true that none of the genes have any effect, a.k.a., there are some genes with an effect. What are we going to do for our estimator? We're going to find the CDF that stays in this interval that has the least amount of mass above the threshold gamma. And the idea here is that by choosing the one with the least amount of mass we will not overestimate the true amount of mass. And we'll be able to provide this guarantee on the number of discoveries you would make. The estimator has a number of nice theoretical properties. This observation we do is step three is a convex program, so we can compute it efficiently. The fact that we're using these confidence intervals means that our constraint is satisfied with high probability. And then also, we're able to show a finite sample lower bounds on our estimator, which essentially translates into-- you can see things like you need the number of samples that you take to be at least this large in order to estimate the amount of mass above the threshold with some degree of accuracy. Our hope is that this could be used for experimental design in these sort of settings in the future. Happy to take questions. [APPLAUSE] AUDIENCE: You use some inequality to build this confidence interval. When I'm used to building confidence intervals I'm doing it in theory where I can say, I'll just throw on a factor of 10 or a factor of 20 and that's probably going to be fine. But in here scenario, a factor of 10 or 20 is actually a big deal. Do your confidence intervals kick in with those constants? How do the constants work for you? JENNIFER BRENNAN: Great. Thank you. The question was about the constants in this inequality. So the DKW inequality, it turns out-- we know the tight constant in the DKW inequality. In particular, it's the two up here. Whatever is written on the side, this plus or minus the square root of log 2 over alpha over 2n. n here is the number of, for example, genes you have. This would be like 13,000, in our case. Alpha is the confidence interval. With probability of 1 minus alpha you stay in this interval. And this is literally exactly the bound. There's no other constants, nothing like that. So this is the new distribution. PRESENTER 1: OK. Let's thank the speaker again. [APPLAUSE] Next up we have Willie. He is advised by Pedro Domingos and Sidd Srinivasa. He is going to be talking about reinforcement learning and control. So clearly, I do robots and not networking, or systems, or something. Algorithm control. Algorithms can get robots to do a lot of pretty cool things. So here we have the OpenAI shadow hand manipulating a cube. And then also, a couple of things in simulation. Learning to walk and learning to move objects around. Generally, they use object information to get this. And by object information I mean a variety of things. Anything from object segmentations to object poses, so translation, rotation to even object meshes and mass and friction parameters to have a full simulation to do all sorts of planning in. And when they don't do this, when they're just learning from literally pixels, they're generally very sample-inefficient and brittle. So learning using object information is a good thing. However, there are no general purpose, vision systems for robotics. There's a ton of vision work out there. But with the exception of like some classical stuff, I guess, with deep learning, you have the dataset. You train on that dataset. And then your algorithm works on that dataset. If you're a roboticist, you have to restrict yourself to that data set the vision algorithm was trained on. Or you have to painstakingly collect your own dataset and then retrain this deep learning algorithm on your own datasets. I know that in my lab, in Sidd's lab, another group spent several weeks collecting a mashed potato data set because they wanted to be able to segment mashed potatoes. And that was several weeks, a lot of undergrad labor, a lot of deep learning expertise to get it working. Obviously, this isn't sustainable. It would be nice to have something better. I've been working on training a vision system that for any objects or set of objects will output some different things that are useful for robotic control and learning, specifically the segmentation of that object. So all the pixels in the image corresponding to that object, the object's 3D mesh, and the object's 6D pose. To do this, we need lots of labeled training data to generalize across all objects because this is deep learning. No such dataset exists in part because labeling real datasets like this is very difficult. Not only do you need all the pixels corresponding to the object, you somehow need to get the object's 6D pose and the object's model across tens to hundreds of thousands of different types of objects. It would be incredibly difficult to get this in the real world. To solve this problem we created a large simulated dataset of ShapeNet objects. ShapeNet is a repository of about 50,000 3D meshes. And we label them with segmentation, pose, and of course, we have the mesh. Over here on the left, we have one image in our dataset. It's a bunch of objects sitting on a table. In the middle, we have the segmentations of those objects. And then on the right, we have one of the voxelized models for those objects. We have about 1.4 million such pieces of training data. Now I'm going to talk about the architecture we used to do this. We use SSC to use segment objects from an RGBD image. Up here is SSC. I encourage you to look at the paper if you want to know more about it. And from that we get segmentation masks. And then we use parts of GenRe on the segmentation masks and depth images to produce object meshes. And once we have object meshes, we can actually get object poses by mapping the mesh back into the scene. Another key part about this work is we wanted to work on real robots. This isn't just getting some performance on some vision benchmark. So we tested this by setting up a MuJoCo physics simulation of Herb, which is a robot in our lab, rearranging objects using the MPPI algorithm for control. On the left, is Herb's view. Herb is moving the black box to the gray box. And you'll also see the blue box. This is where our vision system has perceived this black box to be. It's pretty good. It's OK now. We're making it better. But promising initial results. And then on the right, these are the segmentations. This is hopefully going to be a pretty big project. Right now, the deep learning systems are very much constrained by the number of 3D models and the datasets. And a lot of interesting 3D models are missing. There's not much food, for example. And also, I want to have better rendering in video sequences. And there's a lot more stuff I want to work on with this too. So if you're interested in collaborating or putting this system on your robot, drop me a line. Thank you. Questions. [APPLAUSE] Zack. AUDIENCE: How good do the models need to be? WILLIAM AGNEW: How accurate do the physics simulators need to be? AUDIENCE: Yes. Were they getting all this stuff from ShapeNet. It seems like that's really heavily curated. WILLIAM AGNEW: Yeah. That is an issue. I think it depends on your task. For rearrangement, they probably don't need to be super good. For something like grasping, especially if you want to do your calculations with geometry, then the reconstruction model does need to be really good. And that means that the limitations of ShapeNet is probably going to hurt you. One thing I'm looking at, if you just Google 3D models, there are hundreds of thousands of them. So how can I turn this into a high quality dataset that I won't get sued over releasing? Johann. AUDIENCE: Have you thought about using a game engine, or something for data collection? Maybe that's not allowed. WILLIAM AGNEW: So we're using pybullets, which is like a game engine, although it's designed for research. We were considering using Unity. The issue with the game engine is they are meant for game developers. And it would be very difficult to expose the interface I need for them. But there are people working on using OpenGL to make high quality rendering. It would be nice if game developers would just give me their games. [LAUGHTER] Any more questions? Cool. Thank you. [APPLAUSE] PRESENTER 1: Thank you. Last but not least, we have Aravind Rajeswaran. He is advised by Sham Kakade and Emo Todorov. And he is also going to give us another great talk about robots, I think, or learning. Take it away. Hi everyone. I work on learning and optimization for robotics. Today I'm going to be talking about what benefits do we gain by combining them. The setting of interest in this particular talk is solving hard optimal control problems when we have access to some sort of a generative model. And this could be a physics engine or a neural network dynamics model. I'm going to be agnostic to the source of the model. Understand that we have a model. Here is a particular example, where the same OpenAI system that Willie briefly mentioned-- so the task is the agent has to control a five-fingered anthropomorphic hand, shown over there, to manipulate the block and make a desired face point upwards. This is a very hard control problem because it requires careful coordination of many different joints to accomplish this task and manipulate the block. How did OpenAI actually end up doing this? Well, they used this approach of simulation to reality transfer, meaning they train a policy, a neural network policy in simulation. And then transfer it into the real world and show that it actually works on the robot. While this is very appealing and works great, and it's an amazing feat of engineering, it is still somewhat underwhelming for a few reasons. If you look at the amount of computational resources that are used, it's about a few thousand computers running on some cluster trained for two days to solve this particular problem. So a reasonable question to ask is, can we make algorithms that are more efficient than this while still achieving similar performance? And in particular, can we design algorithms that use the simulator more integrally? That brings us to the main topic of the talk, which is the Plan Online Learn Offline, or POLO algorithm. So this algorithm actually can solve hard control problems, like dexterous manipulation and humanoid locomotion in about one hour on a single laptop. Contrast this to two days of 1,000 computers running in a cluster do something very, very cheap. And again, the agent is tasked with controlling that particular hand such that it manipulates the block to make the pose match the desired pose visualized on the left. What's this POLO? What goes into it? Well, it's effectively combining two components, like the title advertised. We are going to combine online optimization, or model predictive control, with value function learning. And I'll describe both shortly. The key point is that there is a synergistic relationship between the two such that if we put them together the combination is better than the individual components. Model predictive control, or online optimization, is a class of control algorithms that has been very successful in many applications, from chemical process control to robotics. It's actually easier to explain it visually. Let's say we have the task of the blue agent trying to get to the green target. What model predictive control does is it considers different candidate action sequences, runs them through the generative model to hallucinate possible potential futures. And these potential futures are hallucinated only up to some particular horizon length. And then we evaluate the cost or the reward of each of these trajectories. And in this particular case, the middle one is the closest to the target. So we pick that, execute the first action from that particular action sequence. We transition to state as three. We repeat the whole process again starting from this state. It's effectively an instance-specific optimization problem that we solve at runtime. Whenever we find ourselves in a particular state we start an optimization problem starting from that state to compute what is the correct action to use. This has a whole bunch of benefits. But I want to only touch upon one in particular, which I stated as very robust or distribution mismatch. There is no notion of a deep network trained on some distribution here. We are solving an optimization problem at runtime. And as a result, it can be very resistant to perturbation. What is shown over here is a deep neural network policy that is making a robot run. It actually runs very successfully under some distributions. But if I apply some small perturbation, visualized by the red force over there, it puts it in a configuration that was low probability under the training distribution. And as result, the policy is not able to work on those states, and it leads to catastrophic failure. Whereas, online optimization would be robust to the swarm of s. Having said that, it has a whole bunch of limitations as well. The most important of which is that it incurs a sharp horizon bias. So remember, we only look ahead for some amount of steps. And why is that? It is because there is a strict real time constraint. So if we have to control the robot and compute an action once every second we cannot say, dear robot, please wait for five minutes for me to complete the action. If the robot is about to fall down the gravity is not going to wait for us, and the robot is going to fall down and break itself. How do we overcome this limitation? Well, a reasonable guess is you can try to use learning to overcome some of these limitations. And in particular, we are going to draw upon what's called value function learning. Value function at a particular state measures how favorable it is to be in that state in the long term. So the reward function says, immediately, how good is the state? Whereas, the value function says, how good is the state in the long term? We can parametrize a value network that takes as input the current state. And basically, says whether that particular state is good or bad. We can train this by minimizing the Bellman error shown over there. Once we have a value function, we can try to recover the actions by essentially behaving greedily with respect to the value function. While appealing, again, it has a whole bunch of challenges. The first is important, which is error amplification. Even if we have a reasonably good value function with low Bellman error, one, when using it to recover the actions we actually get a huge degradation in performance. And it is because any error that may be present in the value function translates into the policy in almost a worst case way. And it's very undesirable. And secondly, it has a very complicated optimization landscape. If you look at the loss function, it has a max operator actually embedded inside the loss function itself. And it's also a fixed point equation relating how values in a particular state relate to values in other states. And this makes the optimization landscape variable condition. So how can we put them together and combine them? Well, a very natural guess is that we're going to take the same model predictive control problem as before but put the value function at the end. So what that means is that we again hallucinate a few potential futures, but then there is another function out there that is telling us if the final state is good or bad. In this particular case, in the bottom two trajectories, even though the immediate costs may be very good because the middle one is the one that takes us closest to the target immediately, the value function tells us that, OK, that state is actually bad. If you go there it will take you longer to recover and go back to the target state. Whereas, the top state is good under the value function, and the model predictive control is a better fit. And as a result, gains long horizon foresight. Another nice benefit that comes out of it is the converse, which is that model predictive control can also now stimulates the learning of the value function. I don't have time to go over it right now. But it is essentially an expansion of the Bellman equation that makes the optimization landscape better behave. The value function helps the model predictive control part and more predictive control helps the value function learning. Just to summarize, when we are designing control algorithms, there may be four considerations that we really care about. We want globally near-optimal performance. We want fast training, meaning the amount of time it takes before we actually deploy the solution. We want fast inference, meaning at deployment time we don't want to be consuming too much computational resources. And finally, we want to be robust to perturbations, which is a very important consideration in robotics. Pure reinforcement learning almost finds globally near-optimal solutions and it's very fast in inference, meaning we just do one forward prop and we are done. However, it's horrendous in terms of training time. It takes forever, as we saw. And it's also not robust of perturbations, which is very undesirable for robotics. Pure model predictive control on the other hand, is very robust to these distributional mismatches. But only finds local solutions, but finds these local solutions fairly quickly. Now we can put them together. What we get by combining model predictive control and learning in the right way is that we can do favorably on all the considerations that we really care about in robotics. And with that, I want to thank all my awesome collaborators here. I'm happy to take questions, as well. [APPLAUSE] AUDIENCE: What kind of planning were you doing in the MPC card? ARAVIND RAJESWARAN: Yes. As far as the meta algorithm of quora is concerned, we can use any MPC algorithm. The specific one that we used is a variant of model predictive path integral control. It's one of the sampling based model predictive control algorithms, AUDIENCE: Issues with sim-to-real are a big problem for these. Have you examined its robustness to this? Yes. We have, and others have as well. My personal take is that pretty much any success story of robotics out there is based on sim-to-real. I believe sim-to-real works. People say it doesn't have an agenda. That's about it. PRESENTER 1: Well, some fighting words there. Let's thank Aravind again, and all the speakers. [APPLAUSE] 