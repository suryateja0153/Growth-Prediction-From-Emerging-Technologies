 you [Music] and hello Seoul hello so netstat and today we are very happy to have zoom:1 visiting us human is a thirty year PhD student entering his first year at Cornell University human has done a lot of fancy works in kind of a very interesting work in image generation like style transfer image to image translation his work on adaptive instance normalization I think it's very popular right now and also his work on image to image translation the MU needs fun it's this kind of work I think super hot so we are working on to talk to listen to his talk and if you want to talk with him and just drop by after the talk Thanks thank you for the introduction so today I'm going to introduce some of my recent work on unsupervised image to image translation but first what is imaged in translation here is an example so the dog on the left is transformed to different kinds of animals indicated by the images on the top row and the post of the input animal is preserved during the translation this is a specific case of the more general emailed to me translation problem in image trimming translation our goal is to map an image an input image from one domain to an analogous image in a different domain and you we want to preserve certain properties of the input image for example in this case we want to preserve the post of the dog usually we need to like collect images from both domains like crack many dog images and cat images to try our model and our model can be applied to new inputs in for example in the doctoring translating a dog to a cat is mostly just for fun but image streaming translation has many other practical applications man in computer vision problems such as super resolution the blurring they can be viewed as a specific case of translating an image from one distribution to another distribution like transfer transferring from blurry distribution to a sharp distribution also image streaming translation can be used as a data augmentation tool to generate more training data for example if you want to try an autonomous driving system but most of your training data is like summer images you can use imagery translation to generate images in different weather conditions such as trainees to make it more robust so advances in image to image translations made it to improvements in all the downstream tasks image to image translation can be started in either a supervised setting or an unsupervised setting in the supervised setting we are given pairs of corresponding images from the two domains like we know this pair of control and RGB images are the same shoes but in the unsupervised setting we only have two data sets and we don't know what images should be mapped to which for example we have like data set of the images and a data set of night images but they are not aligned so unsupervised image stream in translation is a much harder learning problem because there are much more ambiguities however it has broader applications because the many cases is very hard or impossible to correct pair the training data in the two domains so in this talk our focus on the unstretched setting where we have two data sets but we don't have any pairs between them most image 3 me translation models are based on generative adversarial networks or cans again we have two networks that are competing with each other so the generator tries to generate realistic images from our some random latent vector from a prior distribution such as the Gaussian and we have another network called the discriminator that judges whether an image is the real data or generated by the generator so the disc the generator learns to generate images that are hard for the discriminator to distinguish and as the training proceeds the two networks will improve each other and finally in the ideal case the distribution of the generated images will be the same as a distribution of real images it can be extended to the unsupervised image to image translation setting and the simplest matter is to just use again but instead of feeding it with a random noise vector we're filling it with an input image for example dog image from the first domain and so the discriminator charges whether an image comes from the target domain for example Academy where whether is generated by the generator network so the generator learns to generate images that look like a cat but the translated images may not be related to the input image in fact it can be an an arbitrary cat image will satisfy the discriminate but we need the generator too but when is the translation to preserve certain properties of the input so there are a lot of ambiguities as a cell and we need more constraints to regularize the training so they are like too early and very successful unsupervised image to image translation models there are psycho gang and unit they propose different kinds of constraints to regularize the solution space in psycho gang they proposed the psycho consistency which means that he will translate a dog to a cat and translate it back you should reconstruct the original dog and the translation networks for both directions are trained together and they provide supervision signals for each other on the other hand there is a unit method which is the short name of unsupervised image to image translation they propose a different assumption which I cost-share Drayton's based so the first assumes that the latent state did first assume that each each domain has an autoencoder that also encodes an image to a latent space it further assumes that the latent space can be shared by post domains such as the two analogous images are mapped to the same latent code so to translate an image from one domain as a document to another domain we simply encoded using the encoder of the source domain and then decoded using the decoder of the target domain and it is easy to see that if the auto encoder is perfect the psycho consistency is automatically achieved so in other words shall return spaces stronger assumptions and psycho consistency and it implies psycho consistency all work is largely built upon the shadow Laden space assumption proposing the unit work so here is an example translation results generated by state-of-the-art English 3 me translation modules so you can translate the contour or sketch of a shoe to a photorealistic RGB photo of a shoe and the quality is actually pretty good however there is not just a single answer to this problem as humans we can easily imagine many different shoes that have the same count for for example this red shoe or this white shoe a question is then can the existing image translation models do the same thing well you can get a bit of the stochasticity by applying like drop out or some other random randomizing techniques to the translation Network but the output is still largely the same and the diversity is very limited so remember in the original gown we also have a Gaussian noise vector as the input to the network and by sampling different noise from the prior distribution you get like different images can we do the same thing for image to me translation models and people have tried that so they like similar to the unconditional again they feed a randomly noise factor to some layers of the network but it is not successful and the random vector is largely ignored by the network so here we see the output the distribution of the translation model is unimodal because this distribution is concentrated around a single mode and it's almost deterministic and lacks diversity so we we want to bridge this gap between human imagination and the state of the art image translation models so we want to generate a diverse set of outputs from a given input image and this is clearly an important problem and humans have no trouble doing that however as I said the problem cannot be easily solved by injecting randomness to the network by making the network to be stochastic denied who are just ignores whatever stochastic stochasticity that is injected so why this is the case we want to understand the reason that why the network always ignores the stochasticity and will analyze this problem and we prove that even if the networks are stochastic that is the the translation model between the two domains it can be like arbitrarily flexible we have mode arbitrarily flexible conditional distributions if we train the two models using the original self again objectives towards the optimum which implies match the marginals your output your distribution of the translation output should be the same as the images in the target domain and so consistency which means if you translate an image to the target domain and translate it back you get the original image if assume these two conditions we can prove that the generator still krabs to a deterministic functions this is quite intuitive to understand so let's say we have a sample X 1 from the first only suppose we can randomly sample two different outputs in the second only since we assume psycho consistency if we translate an image to another domain translate it back we should get the original input deterministically so both inputs are deterministic sleeper mapped back to x1 however due to a cycle consistency in the other direction X 1 should be deterministically bapt back x2 prime and also to acts 2 double prime which means X 2 Prime and X 2 double prime are actually the same and the translation function collapses to a deterministic function so remember that shared latent space is a human stronger assumptions a second consistency so since psycho consistency is fundamentally not compatible with multi modality sheraton space also does not allow multi-modality the populations on the last page you have to third cycle consistent terms right here yes you have to cycle consists in terms whatever you just remove one one of them like it's a one from from has two domain two x one domain and then back to an extra domain let's just remove this my friend holy help the other watch these videos do you have the same problem Daniel can then you can't get one too many mapping so you don't have this problem and then maybe it will decrease the quality of the genitals which I do not like the issue what after we chose after we do that so fundamentally psycho consistency so too strong assumption for multimodal translation and so to achieve multi modality we need to develop a different assumption a different set of constructs and let's take a step back and think more about the image limit translation problem your is a two domains we are interested in come from the same underlying structures they have this some shared commonalities for example whether it's a summary image or a winter image they are yeah they all captures the same distribution of natural landscapes also whether it's converse or RGB photos there are just different renderings of some physical shoes so the physical shoes or the underlying natural scenes are the underlying commonalities behind those images and we refer to them as the content we refer to the specific way of rendering the continent to an image as the style of an image so in other words the content represent the domain environment information that is shared by the both domains and the style represents the domain specific information the specific information that makes the image belongs to that domain that makes an image belongs to the winter domain or belong to the summer domain so doing translation the content of an image should be preserved for example we don't when we translate a control to a photo we still want you to be the same shoe and when we translate a winter image to a summer image we still want you to depict the same thing so the counter should be preserved during translation but the style of an image should be modified so well strew the diversity comes from the diversity so so each donor the each domain does not have just a single style Layton code they have a distribution of styles so the diversity should come from the style distribution of the target only for example when we translate account water shoe the shoe could have different textures or colors and they are like a style distribution in the target only and so the multimodal translation should capture those diversity in the generator function so instead of as in unit which has a single shared latent shell and domain variant Laden space so we think that the problem is that they don't consider the style distribution of individual domains so as a result they are not able to generate diverse and multimodal results in this work we propose multi modal unsupervised imagery image to image translation or unit so here we assume that the image representation space can can be disentangled into a common space that is shared by both domains and it's domain variant also each donor has its individual style space that are unshared and captures the domain specific information for example in this case the content captures the shouting information between the two domains which is probably the poster has to have the help host of the animal however they have different style spaces so the the dog has different distributions of textures or like colors or like some small shape changes and those domain specificity information is captured in the style space of each individual domain so to translate an input image to another domain we so we simply express his content code remember we need to preserve the content we need to preserve the the pose of the dog but we can sample different style codes from the targeted style distribution and by sampling different style codes and recombining them with the improve content we should be able to generate a diverse set of outputs that have different styles but they all belong to the target target domain of the styles so with using tango the Layton space into two components a countenance base and a style space here are some notations acts denote the images and sees the common code and as this the style code so during training we enforce the autoencoder to be like behave like an organ color so we first given an input we encode it to the style code and can encode and we reconstruct them using another decoder and also we during training we also like the model to do translation so week for example extracts a condom code of this dog image and we randomly sample a style code in the targeted style space and the target domain decoder generates an output image based on the import condom code and a random style code and the style caused a they are just come from a simple prior distribution such as the Gaussian it seems us know by intuition this style code should encoding it's a dog or it's a cat but incredibly if I also want to include the color for example the cat is black the color into the style instead here in the content do we have any way to enforce this one and you have some control ability to assign some and attributes similar say Matthew meaningful attribute into the style or into the content okay so you mean that we not only want to preserve the post of the dog we also want to preserve the color of the dog yes so currently yeah you can see me this way yes so first this is not always possible for example if the two domains have completely different color distributions then you cannot like preserve the original color of the talk so if the color distribution is indeed similar probably you can add some like constraints to this model for example just the average color of the output image as the two to the input image I think the central problem here is just by this can loss function what's the loss function thus to determine which path is content which part is a style I think this I do not think we have and it's missus control the difference between this true loss functions here so our model is trying to with two kinds of loss functions the first is bi-directional reconstruction loss which enforces the auto encoder to be by active and we also have gain loss which enforces the for example the output image should look like a cat or should look like a dog using a second framework so we so the by the reaction of reconstruction loss as the name suggests consists of reconstruction loss in two directions the first direction is from an image we encode it and then decode it which will reconstruct the original image this is just a normal training objective of image or encoders and we also have latent reconstruction loss for the content and style code so we first encodes the image to its latent code and we decode the latent code to the output image and way encoded back to the laden space we shall reconstruct the original Laden code so these two loss functions in foster autoencoders to be a bijective and basically reconstructs the images or the latent codes in both directions and as all the existing works we also have a gun laws so from the talk we generate our output and the output should look like a cat so the output distribution should be the same as the cat distribution you know dataset so here are just great great dotted line that's not in force there's two images to be the same it's just enforces that this two image distributions to be the same you're constraining it to a random distribution that is from a cat image right like exact because that if that's anything could be from a dog image you've defined sort of different classes and you're sampling from randomly from a certain class that correct so we can do like a sampling from different distribution and map it to the like decoder using the same network we're just or you can we just we can just use a different is the same prior distribution and use different networks to map it to the output so you see essentially signal the prior Q 1 Q s one question I think is yes standard Gaussian yeah so here as well and as to a standard Gaussian but the domain specific characteristics are captured by the different decoders the question earlier to me it's not obvious how content versus style is determined so I can imagine a scenario where the variation the style is in the pose and the content is the kind of the color like what what like how many how does how does it what makes it decide that the variation is among color and not pose so with for example where data said of cats and dogs and so they are like just pictures of like different head faces like in different angles so essentially you feel just extracts for example the face key points your distribution is probably the same or very similar but for cats and dogs they have very different color distributions or like texture distributions because the texture and color are there's more variation so here is our four training objective basically we optimized the model using a linear combination of all the previously introduced loss functions to further understand like the the constraints implied by all model so so here is sim cell so we just do like normal like auto encoder with scans but why does our model successfully disentangle the content space from the style space so here are several results so if we so here we show that if we try our model on to optimality the common cold is domain variant which means the column called pc-1 pc2 they have the same distribution so so it captures they're like commonalities between the two domains also for drawing distribution matching so suppose that we have so we have two generators which defines two conditional distributions one is px2 a conditional 1x1 and another is px one conditional 2x2 so these two conditional distributions together which they are data distribution it defined to drawing distributions and all constraints if it's a properly optimized and it will forces the to drawing distributions to match with each other so it's kind of a similar but weaker self constraints than stable consistency so still the models in the both directions they are trying to provide supervision signals to each other so this did these two models should arrive at the same drawn distributions also as I introduced the earlier cycle consistency is a toothed strong constraint for multimodal translation but our model although our model cannot guarantees psycho consistency and he will translate a talk to talk head and translated back you may not arrive at the original talk however our model permits a weaker form of psycho consistency which we call style augmentee the cycle consistency so if we Dino the image and the style code in the another domain if we denote the concatenation of them to as much as another latent variable and our model defines our invertible and deterministic mapping between these two augmented space is two Augmented spaces although our mapping system is stochastic for the image spaces is deterministic in this Augmented spaces and if our model is trend under optimal we have the translation function between the two Augmented spaces they are inverse of each other so intuitively it can be understand as following basically if we translate if we translate an image to a target domain and translate it back but using the original style the employee image not using a random style we should all turn the original image and it's important to note that were we this cycle consistency is implicitly implied by the bi-directional reconstruction loss and we don't have to explicitly track our model using the cycle consistency so I introduced the training of motivation and training objectives our model of our model and next I will introduce the detailed implementation of it especially how we combine the counting code and style code in the decoder to produce the output image but before that I want to take a detour on of my previous walk about style transfer so style transfer is kind of related to image to me translation but it's mostly focused on the artistic domain so given a continent image for example a photo and a style image for example a painting we want to render the content image in the style of the style image in the artistic style and in your previous work we find that the mean and variance of deep features include the style information of an image and style transfer can be simply achieved by aligning the being and variants of the continent features to the mean and variance of the style image features like this so more specifically we have inputted content image and the infrastop image we encode it with some style including coda to some feature space and we do this operation so the mu and Sigma and being a standard deviation computed among different spatial locations for the same example so basically this this equation first normalizes the meanest and deviation of the counting features and then apply fine transformation based on the new and standard of the style features so it essentially aligns the condom features to the mean and standard deviation of the stealth features and to do that we can translate the the count the style of the style image to the content of the content image but here we don't have a style image we have a style code which is the random vector so instead of computing new and mean and standard deviation on special occasions we simply use multi-layer perceptron to generate the new mean and standard deviation for the continent image using the style code so so here the difference is that instead of computing the new mean and standard deviation from some style features here the is generated using a neural network by the style also we can like stack multiple recycle adding layers basically after banging after each convolutions and the idea was literally adopted in style gun and they used other in exactly the same way as others but for unconditional image generation and they got very impressive results here you cannot see that the style is encoded in the mean and variance of these features but it's depend on how we calculate the mean average whether we calculate over kind of the spatial dimension HW all we calculate over the channel dimension or you can we calculate over the batch dimension can you can comment up even ways to calculate them and why calculated over the I think instance normalization Kerala is a kind of special automation is why they see the good choice twinkled style yes opening and the variance if we always means uh so here we always mean the mean and variance computed among different special occasions north across patch dimensions or no across channel dimensions so it's a way of getting some global statistics of this image so like each feature vector in cause some local properties of an image patch and we if we get if we computer like first and second momentum of those visual distributions we gather statistics statistics distribution of the features so it's kind of like a a global a global description of the styles let's say for example just one feature that it has a particular type of oil strokes for like all your paintings these future activations will probably be very high and if we if you like transfer this activations to some other images if you like get also like gather high activations but for a different content image you probably should be able to translate that image to the style for all your parenting I'm just gonna ask if anyone's tried like using instead of convolutions in between the data ends used like recurrent neural networks there instead pick an ALICE TM layer mm-hmm so for Alice TM for example if it's your USD language they don't have the spatial dimensions so they don't so Euston snow miser in taps does not apply here but there are some papers that propose adaptive layer normalization which works pretty well for like language style transfer so here is the detailed network architecture of our model the counting code is our to the Future Maps so you can better represent the qanun structure and style code is a one dimensional feature vector so it captures the global style information and as introduced earlier our model uses a multi-layer perceptron to generate other interim eaters in after each convolutional layer of this several residual blocks and then the output features of sampling by several up convolution layers together opera image here are some continuing results so we employ so we compare our method with previous models such as you need and cyborgs and we also try the variant we call psycho gang with noise which we inject so annoyed a random Gaussian noise vector to the bottleneck of this diagram so we employ two matrix one is quality and qualities are obtained by user studies and diversity which is obtained by basically computing the perceptual distance between two random output the images given the same input and so we can see that our model achieves both higher quality and much higher diversity than the previous model also we do some operation studies to verify like whether all the loss functions are useful if you remove well the reconstruction in fact the diversity increased given more diverse than they find water yeah so also we compare our model is bicycle gain which is the multimodal image translation model but for supervised setting so they require like pair data between the two domains so we see that our model country were comfortable but slightly lower quality but we are tree or like slightly higher diversity but our model does not need any supervision and empower supervision between two domains in addition in some datasets where we know the the ground truth mode or ground truth categories of each domain we in probably inception score to measure the quality of the output images which we refer to as is or a conditional inception score which not only measures quality but also measures the diversity given us given the same input and our model achieves higher inception score and also higher conditional inception score there are previous models here I will just show some photos some sample results like our model can translate a control to photorealistic shoes and we can like sample shoes with different colors and styles but they are like different from the ground shoes but they all remain faithful to the original also we do the same thing for the handbags and also we can translate a shoe back to the counter donor the diversity is kind of hard to notice but still we will get like different levels of controls and like different levels of controls and also they all remain faithful to the original input a norm also is quite general and can be applied to many cases for example translating between animal images like translating host cats to dogs or house cats to big cats also we do some like synthetic - real adaptation experiments so since he is a synthetic Street View data set and cities Gabrus so we all Wall Street View data set and in since they were like images in very different weather and season conditions so we can transfer given cityscape image to sincerely images with diverse weather condition started snowing running the in the other direction since cityscape images are mostly sunny we don't get much diversity in the weather but we still get like diversity in so for example Lighting's or like the texture of the roads we also try to translate between summer images to snow images and for example we can get snow winter images with different amount of snow or like summer images with different amount of leaves oh I think it's due to the camera of the data set is counted as a style also so in the previous random translation we basically a feed our random style code from the prior Gaussian distribution and we get random outputs what if we feed our example code of an example image in the target style domain for example we extract a Content call of taste control and the style code of this shoe and we recombine them and the results shows that we can effectively transfer the style of the style image to the content in this case mostly color and textures and in this case we can translate this Tiger to cut a cat of different appearances but with the post is preserved after we release the paper people have applied to apply to too many different tasks for example in magical imaging the data is very very sensitive and how to attract so people have tried to use Muni through January training data for medical imaging applications such as laparoscope images MRI images or CD images also Muni has been used in the context of autonomous driving in order to synthesize training data in different weather and season conditions and perhaps a bit surprisingly our model is also applicable to problems outside the image domain such as voice separation or music style transfer so unit works great for multimodal translation but a limitation of unit is that it only works with two domains so we need to Tran a new model for a different pair of domains suppose you are unmodeled we need to train n times n minus 1 different models and this will become prohibitively large if we have many domains the multi domain translation problem has been tackled by some previous work such as stylists tagging and your idea is also to have a shared return space for other domains to translate an image to another domain we first encoded through the latent space and feed the latent code to the decoder together with a one halt encoding that indicates which target domain you want the decoder to translate it to and the decoder can produce an output corresponding to the one hot encoding and Stocking have tried experiments on like facial expressions transfer and for example they can transfer an implement to like angry or happy or fearful expressions their data set has like 40 expressions and the only single model because all the encoders and decoders are shared by other domains however the steer problem what if we want to translate an image to an unseen domain for example I really want to know what my face would look like after you kill mo but right of course I don't want to actually it'll either level so of course this expression this domain is not seen during the training of stag an and stag and cannot help me with that you cannot produce anything because it's not trying on this domain and this problem is particularly interesting in the future setting where we have only a few examples in the target domain because if you have many examples you can just simply return new model what if you only have a few examples like a few examples of eating elements in the target only can kind of models like translated to to the new domains without retraining it and just with a few examples it's very challenging for existing models but it's not a problem for humans for example a person if a person C or standing dog for the first time in their life this year they will have no trouble imagine them a tiger looking or what it would look like when it's lying down because it has a lifetime experience of seeing other animals in lying down position so people can easily make this kind of analogy even if they see this kind of animal in the first time so in following work we want to extend immunity to this few short and unseen domain setting what we do is we collect a large data set that contains many domains or many different process of images and intersects with character data set the context the at the faces of many different kinds of animals and during training we realize a model to practice the future of translation scenario at each iteration we randomly select the domain and we track we train our model to translate the account in the image to the domain use the only very few examples as they just a single example the idea is similar to meta learning where people train their model on a large set of tasks where people train their model on a large set of tasks so you can quickly adapt to new tasks so here we train our model to translate between our many different pairs of domains using only a few examples so that we expect you to quickly adapt to new delays just wondering how much this now actually is unsupervised because you seem to be moving more to the supervisors domain because in a sense when you have like the star again you have different subdomains this is just different subsets of your data so like classes in some sense and that you have to counter have sums from somewhere so like in which sense is this now still unsupervised or like where does it fit in there could you just comment on that yeah so it's unsupervised because we don't need parodied our last day we don't need a the a dog and a power and a cat that I Eunice exactly same post so that's the unsupervised part about for supervised part as you say we we still need a class label of those images like a domain laid off so see images your data to common cluster space and you need to know that these clusters that they are clean in a sense like they all capture the same thing that you that you care about so it's like a a more granular kind of supervision yeah yeah so it's a different kind of supervision by unsupervised here we mean like unpaired or on aligned data - that was wondering have you considered using like disentangling urinated space it's a we're in two parts in two more guys now you have like this style and and maybe you want to have like different styles so that you can capture more yeah but so if we explicitly like used for example different prior distributions for like different styles so we cannot like generalize model to like a new styles because we sort of like a predefined set of styles yes so here we were increasingly basically we increase redefines different style distributions by like feeding an image to a style encoder that encodes the characteristics of the domains the image belongs to so it's more like implicitly inferring like different style distributions then like explicitly a setting like different Cartesian you're not different style code you control the way in which you kind of bury the style by doing something like PCA on the like style distribution and then just changing the different axes long if you slide it and the reason you don't need more than two is because you just have one half that's specific to a different domain and you one half that's share them all domains so you wouldn't gain anything by splitting it up into more so during training it's like this so we treat each our model to translate between a lot of different domains and during a test time we have a few examples from a new domain and we we just like the model do the same thing as he starts doing training but on unseen classes or unseen domains so the network architecture is kind of similar to Munich but here instead of using a random style district rendom style code sample from a prior distribution when first we infer a style code from the few example images in the target only so here we basically process each images independently and we do average fully together class code or style call your domain code of this target domain and we use it to translation similar to the immunity we generate the dying of fine parameters either residual layers of the decoder and here are some results so the first and second rows are two examples of the target domain actually we have in this experiment we have five five in total we have five examples in how you donate but we show two here and X is the input content image and this is the translation output notice as the model has never seen those domains or those kind of animals but it can effectively transfer the content image to the unseen domains maybe I haven't really got it but where exactly is the few short learning because it seems to be like this what you're doing is you're extracting two styles of the content and interest doing style turns for placing this way it seems to me like it's just an inference to us there's no learning involved important you unseen which is right so for future learning yellow constructs kinds of like to create quarries if you are familiar with the future classification literature so there is kind of like matching networks or like prototypical ad was which does not learn during the few short meta testing cases so they simply extracts a generic invariance from the source domains and applied to the the new new domains right so which is basically what we are doing here there is no bad program yeah yeah so I think you are what you are referring is more like a model agnostic matter and you're like optimization based future approaches which is also filtering but it's just a different approach interesting in the traditional style transfer you have a one reference image to define the style and in the image stream a translation you happen not nice and they style now you have Kanna you are in the middle so how so now I think they have the number of images to define the style is the hyper parameter so how this hyper boundaries affects the performance of your results yeah I will talk about this maybe No so here we vary the number of training classes that the model since during training and so the more training the more classes the models is doing training the better performance it is at halftime so the first two matrix are the higher the better and the last ones two Rosabelle also for if you vary the number of shots or you feel very the number of examples during test time also you get like higher performance if you have more examples if you use so higher number of shots that's a parameter hyper partner for trainees or what say actually we find that it's not important it's not very important so we train them all model under the one-shot learning during training always but I test them we can do like few short testing also here are some results of transferring bursts to a different categories for example so the model can quickly extracts the characteristics of this kind of bird and applying it to the input image without ever seeing this kind of birds doing train also it has it on flowers so all these samples look really great I'm an awesome what what types of input images or classes does it not do so well on like yeah yeah we all have something gentle so yeah we also have some full translation to like generate a delicious food if you are hungry hungry are you because you used to or is it oh we have five finally we shall hear so if there's a large gap in terms of geometry yeah sometimes object yeah you and your fellow music obvious large the translation is kind of okay also this kind of translation it becomes kind of hard to judge where a we're just capturing and if it's even good or bad it's just yeah it's a picture I see if I just squinted it they all look almost like they're in the same domain so yeah I think if there's a large gap between x and y is then it just kind of it's why anything in my opinion mm-hmm make sense because the sure content we know does it make sense but like he was suggesting what is that you can alright then we're how the ability looks like it's twisting um the style information is almost dominating the result in fact the content part like the second column would have been great if right rise there were islands of those because you comment on that like the rin when it kind of reshapes the distribution using the style code yeah so the problem is that the target element probably don't have the kind of example is that different right different pieces of Rice's are clustered like this in this way so so that how you the image should look at look like an example image in the target distributor and so I think it's really depends if the two domains are very different then their share of the properties the domain you variant probably is the commonality it is like a quite startled so yours like only very little information that can be shared by the boast domains so like that's why I like only very few information is preserved maybe the fact that that dumplings come in six different little shapes is specific to that style versus for fried rice it comes in one shape so it wouldn't even be able to output something the content is on top of the fried rice anything closely this kind of measure like we're coming in to control this down to inject our statement you understand now style it's just let's say model to a colonic content by a civil right the job like a collage where you have the same X but there's different wise to kind of see how much variety to actually capture or it does it generate more or less always the same image no matter what the X the very last one yeah we try this before whether we don't show it heinous right so so actually for example if you use the same acts by different why there's a shape will be kind of similar to this shape but with of course Israel still looks tracks that are either Thomas defined by why you know install again paper from Nvidia they showed early when you infuse the random information in the early layer then it tends to kind of modify the global structure of the image whereas if you go towards the end that it kinds of just focused and local texture information I'm still looking at the second column there and if in your previous slides the first three or two layers were fused with style information and then the later part says convolutional layers did you think about kind of flipping that order in a way that we only infuse those style information towards the end yeah yeah that's very interesting yeah I think yeah we can try like or you can try similar to the on video paper reinforcing like infusing style information all layers but like using style information for some layers we use style information from this image for the other layers with us now information from a different image that could be very interesting to try as well so here we compare our model with previous baselines here the results are under a neutral setting where the target um has 20 examples and so our model significantly outperformed all the baselines in or on other matrix and also the baseline models are trying on the tiny the the examples in the whole domain but our models never has never seen the target domain during training and here are some limitations we are all model testing all perform everywhere so the model does not work very well if the test domain is drastically very different from the training domain so it still relies on like the information its loans during translating between many different domains doing so for example if it's like translating between different animal domains it can be it can be generalized to different animals but it's how to different generalize to other donors that are drastically different like flowers or so so here for example if we train this model on the animal donor and give a like flower images it won't like really transfer this animal to the flowers and we release the online demo of unit future unsupervised imaged in translation and it's called our pet swap and people can use it to translate their pets to different categories you are welcome to try as well here are some instructions okay we call this demo Pat's wall because this model is trying on animals mostly pets and we expect people to use you to like transfer their past to different categories but it's not in our control and people start to mess around it with it as they try to translate their faces to animal faces and our model actually works surprising you will although it never sees any human faces during training so so the common image so he really says cannot generalize to a drastically different style images but actually you can generalize to very different content images also is introduced is introduced into system analogous it make like transferring their long hair to the long year of the dogs and press even more surprisingly it also works on cartoon faces we release the code of posts unit and unit and feel free to try a few things they might be useful for your work or you just won't have some fun performance so a few needs is designed for short learning to for hansung domains but if did you prefer did you compare its performance just insane domains we started this kind of really intro mending spell transfer so here are the stockings trying to waste only tiny examples uni what would be the performance look like if we trying to stock on use all the examples yeah I mean just do we compare this they're kind of the translation performance and the domain we already see seen during their training is all snug and can be trained and can be a mayor mayor yeah actually we have some results in the supplementary of the paper so compared with saga even if we trying to start using all the images and it has its own like the sing domains of stagger and for few need this domains unseen pop unit and we just use 20 images to test few knit on this domain and the results are actually comparable interesting whether the to meet also increase the endowments that transfer performance compared with stock or other intimate snow chance for all image translation and answers yes yes I think this is a case but I think in that case the performance mostly comes from like detailed Network architectural design but not from like the training of algorithms because the training is more designed for the the so on single maze and finally I want to thank all the collaborators from Cornell Nvidia and our university especially I understand coming you do from Nvidia and most of the work was done when I was in the news team thank you any questions also caught up on how you decide what ends up in the style and the content fittings and it seems to me like the main way to sort of trolla is to change the relative sizes of the two embeddings so can you comment on like during the training process what you observed when you change those relative sizes you mean the like dimensions of the style coordinates Condon called because that's how much how it gets to put as much information as I can to those two person that I you know you think if you give it no style at all everything has to go through the content part of inventing so during translation all the information in the original style is not used so only the accountant called author of the image is used so that's also kind of a difference other than the the size of the two embodies so long invading is used during translator and another is you it naughtiest I'm just randomly sampled the constraint that the two embedding of spaces have to be the same and that you can go from one image class to back down to another whereas the style classes only correspond to one image box it's gonna try to put as much you know information about that particular instance in the style pots as it can and then you leftover information about that particular image instance I you the content he's gonna have to go into the content depending and so if you adjust the relative sizes you're kind of how much you can put into style versus into content thing there are fewer constraints on the style and pinning and therefore we'll use that more heavily first the content bedding needs to be shared among multiple additional means it's just it seems like it's sort of arbitrary how content versus Tom get differentiated and it's just a matter of how I often leave it this yeah so in our network architecture design there's also like inductive bias we use so the style parameters only affect being under standard deviation of the network activations so it's applied in a global manner so I think even if you were like I try to make it a style called very high dimensional and it doesn't change much because it's still only affects the global information so I was just wondering what your thoughts are on the problem where you can't have like these two subsets of these several subsets with data let's take you to your daylight images you nighttime image and you just put them all together and now we just forget that it is about day and night time that is so very interesting so do you think do you aware of any work or do you have your thoughts on like the problem where you don't have that addition from my everything is just one big set that seems to be a very important prerequisite for this yeah research direction yeah so in the few need work in the future at work so we'll use the cross information awning for the discriminator so the discriminator needs to charge whether it belongs to a Southern Cross or not based on like for real data we have labels and for general headings are we have the labels of those there are images so if we don't have access to that labels we need some other ways to enforce the style information is preserved so maybe we can draw inspiration from racks tower transfer literature like they don't have like cross leg jigs and have a set of images so they come with a priori information about how to data centers construct this dish structure the data set that is exploited in a very clever way I'm addressing the situation where you get a whole bunch of data but you don't know what the structure is you don't know that there are paintings and their normal images but truly unsupervised so actually in that case you have a different task because in that case you cannot even image you image translation image transmission is essentially you must have three domains to transfer so in that case it's unconditional generation under control Nvidia's work is of course it wouldn't be called image to installation but would be about finding representations of factors in your data that like somehow are interesting I think the video work the style games are good you may be a good example for this now that also comes with like different domains and it has data from different domains in it trains it like that so that's the opposite I think introducing more structure information into the data that is trained on I was considering now again is a big thing just face it MIT is oh okay yeah maybe I'll have to look at a paper again then yeah I think he refers to style god oh sorry that's the Schneider I thought just mentioned star again so yeah they basically or they have are like just unconditional unlabeled data set of faces it was just I thought this bit was mostly just about producing nice images there's no there's no style content separation yes there in fact they are stunning chess team and study the earth or it will unthinking vision I think um so a hypothetical in the case where you're translating between the simulated driving data and the real driving data let's take half the civility driving data's and snowy scenes and half of its in normal conditions and for the real driving data a quarter of it is the snowy scenes and three-quarters of it into normal would the network in this case recognize that snow is a part of the style or a snow over the part of the content because now the distribution over snow versus not snowing is very different between the two domains mmm is very different or senior so so for both domains they have like half snowy images I know what is happened half one is a quarter in three words okay okay then snow will be category you can put into exchange one is kind of haha one is just 99% versus 1% then it not because it's no we're definitely categories as a style not content yeah so so in order for things to even have a chance of ending up in content the distributions have to be quite similar you know just I'm excited quite somewhere between the two domains yes if you if you didn't have you know heads looking in all different directions that it might not have chosen pose yes in fact I have a question of thoughts a trainee on the phone it so you said that you only use the class information the discriminator and I suppose it's not the and type a discriminator do you think that it's equivalent to add a canasa fire for the style or for the style called just I either cannot cause entropy either classification head and idler nose into your training I think it's kind of a covenant but the only difference is that you steer your weight with the kind of the discriminator is that you another class if are only for the style cope are not for the operators so can you go to the water yeah and so what that means you add a kind of classification head for this red button box here but even if the style call really extracts the style information it may not be applied to the outer image yeah you like your classification target is your input class so in this case you're directing injects they kind of their style information which is the class information into this they can call you mean you addition to have a discriminatory I think basically girls kind of doing is a similar same but it's just here that's an interesting idea but we haven't tried it okay I think in this case it may solve the distribution canal unbalanced issues because in this case you direct inject the star information to the native code yeah okay so any other questions then okay let's thank the speaker again thank you for attending 