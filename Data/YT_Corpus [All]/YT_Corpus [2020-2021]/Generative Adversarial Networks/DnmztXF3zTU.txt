 we present learning illumination from diverse portraits a new way to recover the intensity color and directionality of a scene's illumination from only a single low dynamic range portrait at a high level in this work we'll use machine learning to show that you can treat a face as a light probe traditionally to render and composite synthetic objects into real world scenes you first need to measure the real world illumination you could use hdr panoramic photography techniques like photographing a mirror ball however for offline visual effects light probes may not always be available and for real-time augmented reality lighting measurement is impractical our solution is to train a machine learning model to replace the mirror ball with a face inspired by the fact that faces have a diversity of surface normals and reflect light somewhat predictably to train our model we generate portraits labeled with their ground truth illumination using image-based relighting we photograph a subject lit one light at a time by a sphere of lighting directions and then scale and linearly combine these images to relight the subject in a new environment in total we photograph 70 diverse individuals wearing various accessories and performing several facial expressions in our light stage system with 331 unique lighting directions and six camera viewpoints we also capture about 1 million lighting environments to use for re-lighting extending the rapid capture technique introduced in our earlier work deep light here we show a few deep light captures ldr images of mirror diffuse and matte silver spheres we explicitly promote each deep light capture to hdr lighting using a new non-linear solver technique since we need the full dynamic range for relighting at capture time we also acquire the images used to compute a robust alpha channel as shown here we then composite the relate subjects over the high resolution backgrounds from deep light to form training images that mostly appear as in the wild portraits with their known illumination here are a few examples with ground truth lighting inset in the upper right corners with this data we train a neural network that outputs a 32 by 32 mirror ball image of the hdr illumination we minimize a multi-scale rendering loss for the three deep light sphere images and a multi-skill adversarial loss on the mirror ball now let's take a look at some results from our method first here's an input portrait with ground truth illumination for a few outdoor scenes now we'll add our lighting inference shown by rendering the three deep light spheres overall we've matched the color intensity and directionality of the lighting and next we'll show comparisons to several state-of-the-art techniques for inverse lighting from faces which our method outperforms next we show that our model recovers a similar scale of illumination across subjects despite the inherent ambiguity between albedo and light source strength here we estimate the lighting off an input portrait and then use the inferred lighting as is without an intensity scaling factor to accurately light a subject of a completely different skin tone to compare we also light this second subject using the ground truth our lighting inference accurately produces complex effects like specular reflections and shadows and to the best of our knowledge our inverse lighting from faces method is the first to be both trained and evaluated across subjects of different skin tones for further results along this axis and for more comparisons please see the paper while we've shown results so far on synthetic test data our model also generalizes well to real world portraits here we show a few in the wild input portraits along with renderings produced using the inferred lighting although we don't have ground truth lighting the inference is plausible including for the subject in the upper left wearing costume makeup while our model is not explicitly trained for temporal consistency here we estimate per frame illumination from the face and render spheres in the upper right using the inferred lighting the subject holds a real silver holiday ornament of the same material as the upper right rendering finally our face-based lighting estimation technique enables a variety of applications in both post-production visual effects and augmented reality first we show an example of a digital double actor replacement given an in the wild portrait we estimate illumination from the face and then light a digital character to composite it into the same photograph with consistent illumination we have one more example with the input the estimated lighting and a digital character rendered using the lighting estimate next we show an offline post-production compositing example where per frame lighting estimates are used to render virtual balloons with sharp specular reflections and here's another example with multi-colored balloons you can see the specular reflections from the point sources located in the scene and finally as our inference runs at less than 30 frames per second on a mobile phone cpu here we show a real-time mobile ar application where the shiny mask is rendered using the lighting estimates coming from the live camera feed 