 Hi i'm nicole from Lymba, and today we're going to talk about how to boost your graph with semantic NLP. Just a little bit about us we've been dedicated to natural language processing research since 2005, based in Dallas and New York, and we are originally funded by DARPA and NSF grants. From there, we expanded into commercial applications and we now provide solutions for a number of enterprise clients including, Morgan Stanley, whose talk you may have caught yesterday, Kaplan, Honda and others. So what do we do for them? At a high level, we work with our clients' large text collections so unstructured data from where we extract valuable information to create actionable knowledge. Now, since we're at a graph conference that means that we help them go from text to graph to answers. So we both populate the graph by processing this unstructured text, their document repositories, contracts, chat transcripts, records, etc... and we run them through our NLP pipeline to create rich semantic triples that are stored into this graph. Once they're in the graph that was either created by us, or it was as a currently existing, we query the graph to return the answers... which we do in natural language by converting those queries to SPARQL and we'll show you that in a bit. Now, we are not a graph database company, we're an NLP company but we work with a variety of wonderful vendors, including Stardog, Neo4j, Allegra graph, Marklogic, Oracle and others. so you can bring your own graph to the party. Now, let's take a look at a knowledge extraction example using our NLP technology. Here we have the sentence: Sonoco today announced it has completed the sale of its rigid plastics blow molding operations to Amcor, a global leader of packaging products for 280 million dollars. As we go through each step of the pipeline, we had more and more layers and connections. We start with things like organization, ticker symbols, some basic events, and so on. We see the dollar sign is a currency and we've identified a number. Now as we build on that, we now get smarter. We know that Sonoco is a publicly held company and that this currency and number is actually an amount of money. The real differentiator is the next step. This is where we create labeled relations between concepts, not just clustering. which brings in the meaning of the relationships and strengthens the connections between our graph points. This is what facilitates further applications of the graph, which wouldn't have been possible otherwise. For example, we can now ask questions like, "Which companies were acquired for over 200 million dollars?" or "What was the price of X?" even though the word price or cost were never in the original text, but are implied here through the relation. How do we do this? It's all through an open and configurable NLP pipeline, around which are the tools that help you with customization. This results in processing your text through multiple steps in the pipeline, adding on those layers just like we saw in our example and ultimately resulting in rich semantic triples that are pushed into the graph and from there once they're in the graph we can query those. now let's take a look at our use case and how we applied K extractor Doc2RDF and natural language to query for mergers and acquisitions starting with our problem our clients' analysts need details on over 400 concepts and relations that are buried in unstructured data. These are text-heavy news, company statements, regulatory reports, etc Very hard to do this manually. Further, the unstructured data needs to be married with the more structured data which could be in a graph or relational database and these are things like SEC filings, financial reports, anything else and we want to put them into one place. Lastly, even after we connect that structured with the unstructured, it's still difficult for business users to get access to that information. Non technologists don't know how to query in complicated languages, but they still need answers to their questions. We solve this for them by customizing K-extractor through the use of ontologies. we could take existing ontologies that a company might have, we might use a domain ontology like FIBO (Financial Industry Business Ontology), or we use our Jaguar ontology expansion tool which uses things like source documents and seed concepts to either build or expand an existing ontology. Beyond the ontology, we can also use annotations and we can even handwrite rules in K extractor. All of this results in a specialized K- extractor and when we say specialized, we mean it can be specialized down to the use case, the company, or the domain. So now, when we put our documents through the pipeline, we run them through the specialized version of K extractor and this results in these rich semantic triples that are specific to our initial problem statement and then we load that into a new or existing graph let's jump from there to the demo and I want to show you how this all works in action So first let's take a look at the ontology and where this all starts we take ontologies and different formats as input and here we would start with a snippet of the M&A ontology that we're using. So we have concepts and classes here so company for example is a class and then on top of this we see that there's annotations that support this in different relations I can go to the properties and now I see that company has the ability to acquire and there's relations between the concepts so the domain is the company the company can acquire business assets the company has a chairman the Chairman is possibly synonymous with a chairperson etc so you guys mostly know how ontology works just want to show you a little bit of it here let's say we're now happy with the ontology we're going to move into our K-extractor Studio. So studio is the developer UI the front-end for configuring the NLP pipeline and in this case, I've already loaded several documents related to our M&A text and we're going to look a little bit at these documents so first let's start with the initial upload of the document and this is just the initial document as it was uploaded with no annotations in plain text alright so we just see it's some news report about Oracle's acquisitions. Now, let's go back and look at the annotated version which has been automatically annotated based on the rules we derived from the ontology so now we can see that we have certain relations and concepts that have been tagged throughout our text. Now if I scroll down here we can see things like company which we saw previously on our ontology company underscore G is a class and then Oracle, Sun, BEA, Hyperion, etc, are instances of the class company. If I mouse over a Hyperion, I can see the details of the rules behind it and why it was tagged in this way. The critical point of seeing the rules is that now you can go back into studio and you can customize it in the pipeline you can make different changes there and you can point to the specific rule and rationale as to why this was tagged this way. This is critical for a technical analyst that's doing rule-building, editing, and debugging, because they can see the specific date this was a big reason why you know for example we got calendar date here and the different connection points between. Another beauty of graphs is that we use this local graph here that can then be anchored into bigger graphs so maybe we have another graph with fortune 500 company details that has all of their corporate earnings and reports in the past. We could take something like our local graph here and marry it to that and have more information about Oracle and keep building and building a network of graphs. Alright, so now we've seen our annotation and I want to show you a little bit back into the studio around how those rules were created. So we have two main types of rules: lexical rules and semantic rules. And if I look at my lexical rules, I can see here that this content has been auto-generated by our Auto as-input tool, which is our ontology as input meaning we fed that ontology through that K extractor we were able to generate the rules it's now been specialized with these rules and if we want to dive a little bit deeper, we can see the specific rules on how they were derived with calendar dates and this is an editor right here and we can make these changes so we can always go back and forth between the ontology, the annotation, and studio, where we can actually play with the rules. On top of the lexical rules we also have semantic rules that were also generated automatically from the ontology and we can also edit them in here. Now, if we want to tinker even more, we can do this through either going back to the text and annotating the text here so we can continue to add annotations either smart plain text or our initial marked up text or we can start to even add our own hand-coded rules here to provide that extra level of finesse. And this is really what we mean when we say we have an open and configurable pipeline. This isn't a closed black box system, you can very plainly see the cause and effect, and we like to think of our tools therefore as combining the flexibility that some of you are seeking in open source, with a robustness and scalability that you need in an enterprise solution all right let's say that we are now happy with our rules we think that our annotation output is correct the whole point of this, we're at a knowledge graph conference, is we're going to push this to a graph. In this case, we are, again, we're graph-agnostic. I happen to be using mark logic here, you can use your choice of graph vendor. I'm going to hit run and for those of you who are familiar you can see here that I've extracted my triples and I have a subject, a predicate, and an object. This ties back to the earlier visualized annotation and you can see certain things like Advanced Micro Devices systems, acquires ATI. In addition to that, advanced micro devices acquires the patent application from ATI, the patent from ATI, and so on another thing that I just want to show you is the normalization that we've done because this is key and our next step of querying so I have 1.43 billion dollars and we've gone ahead and normalized these and each normalization is its own triple. This is going to further help us with the query because now our users can ask variations of the questions without needing to know how specifically we got that. And speaking of querying that's where I'm going to go next for our final step. So what good is this information in this graph, if our business users can't get to it? So now we want to ask questions in natural language and I'm going to ask here, "Which companies were acquired by Oracle?" and what we're doing here and this is our NL2Query module, or a natural language query SPARQL, we've automatically converted our natural language or plain English question into a SPARQL query which we can see here that in turn queries the graph, finds the best answers, and returns to us those answers. I can now ask variations of those questions so I can ask something like "How many companies were acquired by Oracle or purchased by Oracle?" and I get the number five so we're able to ask more and more pieces that were never explicitly stated in the text because this is not keyword based this is a semantic approach. I hope that you guys have seen that with NLP tools including K-extractor and NL2query, we can give a semantic boost to your knowledge graphs and provide a more comprehensive toolset to your business users 