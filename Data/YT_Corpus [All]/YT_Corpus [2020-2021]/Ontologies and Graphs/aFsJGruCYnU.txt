 - [Narrator] DS-I Africa. - Hello, I'm Katherine Lawrence, of the University of Michigan, where I help to run the Science Gateways Community Institute. I welcome you to the panelists presentations for the National Institutes of Health funding opportunity for the Open Data Science Platform and Coordinating Center. You're about to hear 5 exciting talks about initiatives that collect, share, and provide analysis tools for data in a more open and accessible manner. These talks should give you some insights into the opportunities, challenges, and considerations necessary for establishing an open data science platform in service of biomedical research. These projects also provide inspiration for the collaboration that is necessary between individuals, research groups and larger organizations to achieve broader health science goals. - [Narrator] Speakers talk from their homes through video conferencing. - Okay, hi. So I'm Benedict Paten. I'm an associate professor here at UC Santa Cruz, and I am really pleased to come speak to you today. - [Narrator] Www.databiosphere.org. - Okay, thank you. So today I'm going to talk to you about the Data Biosphere. So our project to try to build an ecosystem of data sets and compute resources that kind of connect a substantial amount of biomedical data. So we've been making amazing progress in the field in generating increasingly vast data sets that offer tremendous research capabilities. But one challenge with those data sets is that they're getting very, very large. They're getting so large that in fact, that it's really impractical to download and store them on your personal computer. And so we're entering this world in which funding institutions and practicality dictates that it's no longer possible to take the data and bring it to you, bring it to your laptop or bring it to your institution, We can't afford to have  copies of the data. And so, because if everybody has their own copy of the data, it's expensive, it's hard to enforce security around that data, it creates a lot of redundancy and it also makes it hard to share the data. So we're trying to sort of invert that model and instead have researchers come to the data, the data that lives in situ on a cloud and alongside that data is available compute that researchers can use to analyze the data. And we can essentially provide resources and services and applications as I'm going to describe that allow us to both reduce that amount of redundancy and sort of create layers of security and threat detection around the data so that we can know who's accessing it and make sure that appropriate responsible research is happening. Make it ultimately more accessible to researchers who don't have large institutional computer infrastructure and because the cloud is very elastic, potentially enable a larger scale analysis that just weren't possible in the past. In sort of thinking out how to build what we've kind of, envisioned this data biosphere, we started off with a series of principles that we think are essential for anybody trying to build something in this space. One thing is modularity. Whatever we build needs to be composed, decomposed into individual pieces so that each of them can be switched out and replaced and made better independently, so we don't build a monolith essentially. The system needs to be community focused. That is, we need for lots and lots of different groups to be able to come in with a diversity of different ideas and allow those use cases and so forth to be expressed so that what we create is fit for those purposes and it needs to be open, right? So, it's great in genomics and biomedicine that we have this open source culture and everything we want to build needs to have that ethos so that anybody can come in and look at the code and change it potentially and reuse it and create their own versions of these systems. And then finally it needs to be standard spaced. So as we build modular pieces, they need to connect together using standards that are developed by coalitions like the Global Alliance for Genomics and Health. Myself and a number of other groups who have been building sort of components of this data biosphere and a number of projects across NIH and other institutions are leveraging what we're doing. So here just actually a few of the projects that we're helping to build out as part of the biosphere. - [Narrator] Accelerating Medicines Partnership, Parkinson's Disease Biomarkers Program, PDBP, Nurses' Health Study, All of Us Research Program, Biobank UK, NHGRI AnVIL, Single Cell Portal, Human Cell Atlas, NHLBI TOPMed, NHLBI BioData CATALYST. - Today, I'm going to just briefly talk about three of them, that my group is very involved in. Firstly, the NHGRI AnVIL project, they're a kind of platform, then the NHLBI BioData CATALYST, and then finally the Human Cell Atlas project and the data coordination platform specifically. So firstly, NHGRI AnVIL. NHGRI AnVIL essentially is NHGRI's cloud platform in which they want to store data sets that have generated by NHGRI researchers and make them accessible. It's a cloud based infrastructure as I said, it's highly elastic, it enables researchers to gain access to the data, clinical data, data about phenotypes and metadata, and it needs to work around the sort of data access requirements that we have to ensure that only authentic, authorized, authenticated research can take place. And it wants to provide a collaborative research environment in which researchers can kind of come together and analyze that data as I say, in situ. So AnVIL provides a number of interesting capabilities. So we have essentially a place to go and search for the data and find the data and query the data, that's Gen3. We have the Terra, a work bench environment, which you can think of as being a little bit like Google Docs, but for research on large data sets. So you can essentially bring a dataset into a workspace. And then with other researchers collaboratively analyze that data. We have the Dockstore, which is a platform for creating reproducible containerized workflows and for sharing them and making them openly available. And then we have a number of applications. So you can use computational notebooks like Jupiter notebooks, you can use Galaxy, you can use Bioconductor and you can do all of that within the AnVIL space. And there are lots and lots of capabilities and services that are being added to this over time. AnVIL is currently all built out and deployed using the Google Cloud platform and operates within a FISMA, moderate security perimeter, so that everything that we do essentially has a rigorous, robust, security guarantees around it, so we can manage and hold managed access data. If you're interested in AnVIL, then please go look at anvilproject.org. From there, you can navigate, find out information about the project. You can go and see which data sets are already in there. We already have a substantial amount of data in there. You can find training materials and all sorts of stuff to onboard you to use all of these different research resources. So that's AnVIL and very much it builds on the components of the biosphere that we're trying to build. A sister project to AnVIL is the NHLBI BioData CATALYST. - [Narrator] Biodatacatalyst.nhlbi.nih.gov. - I'm not going to go into all the details here, but BioData CATALYST is basically the same thing, but for NHLBI. And so similarly NHLBI has data sets that are a very great value to it. In particular right now, the TOPMed project, which is a project of a 150,000 participants and whole genome sequences for them organized and a whole bunch of phenotype and metadata and the BioData CATALYST is making all of that data available to TOPMed researchers and ultimately researchers beyond and across NHLBI. And just like AnVIL, we're leveraging many of the same components that I just talked about for AnVIL within BioData CATALYST, and then some additional things that are specific and specialized to the requirements and use cases that NHLBI has. So just like ANVIL, if you come to BioData CATALYST, you're going to see very similar set of tools and environment, ways to access the data, work on the data, use workflows with the data. But we also add an inter operate with the SevenBridges platform and we have additional clinical applications through the PIC-SURE system that allows you to explore more of the clinical data. So again, very much the same underlying biosphere, but a different set of use cases and data. And then finally, I just want to mention the Human Cell Atlas project. So the Human Cell Atlas project is an attempt to try and create essentially a complete atlas of all the human cell types and cell states that exist in our body and is leveraging the many, many, many single cell technologies that have been created over the last few years. And Human Cell Atlas, which you'd go to this URL... - [Narrator] Data.humancellatlas.org. - ...is generally mostly open... The data is all public and it also leverages and builds on all of the ecosystem components that I talked about earlier. For example, if you hit up the URL above to go and see data, the humancellatlas.org, you can go and find millions of cells of specific organs, and you can see clusterings and all sorts of simple sample analysis to get into them, but you can then take them and bring them into, for example, the Terra workspace environment, analyze that, and potentially analyze that in conjunction with projects, with data from AnVIL and from BioData CATALYST. So I just want to end by saying that, in trying to build the underlying software infrastructure to make this all work, we're really thinking I mentioned at the beginning, but it's worth sort of double emphasizing that the biosphere and the components that are in the biosphere are building on APIs and standards and interfaces that have been pioneered by standards bodies, like the GA4GH. And by doing that, we're going to make it easier and simpler for other groups can reuse what we've created and indeed inter operate with us by standing up their own services and their own systems, allowing data to be sort of analyzed across these platforms because that's what we want to get to, not a siloed world, but a world in which things inter operate. And with that, I just want to thank the large number of people who were involved in this project, both at UC Santa Cruz and then again, the University of Chicago, the Broad Institute, Vanderbilt at OICR, Human Cell Atlas and funders and the folks in specific projects, like TOPMed and GTEx. - Hi, I'm Nicky Mulder. I head the computational biology division at the University of Cape Town and I lead H3ABioNet which is a Pan-African mathematics network. So I'm going to be talking about H3ABioNet, which is a Pan-African mathematics network. And I am Nicky Mulder, I run this network. So there's no doubt about the importance of African genomic data, this is both human and pathogen data. We have a disproportionate burden of disease in Africa, and the genomes of African populations are very diverse and very different to other populations. And the global importance of this, is due to the fact that people and pathogens move around the continent and move around different countries. So I'm going to be talking about a particular case study, which is where H3BioData was set up for the Human Hereditary and Health in Africa or H3Africa. This is a big initiative funded by the NIH and the Wellcome Trust now through AESA to study the genetic and environmental basis for a number of different diseases, you can see there's a list of diseases that are being studied on the left... - [Narrator] Cardiovascular diseases, infectious diseases, developmental disorders, mental health, monogenic diseases, microbiomes. - ...and the project involves a number of different components. So they are our research projects, collaborative centers, and then the infrastructure components are biorepositories and a bioinformatics network. Now this consortium is generating a huge number of amount of data and samples for more than 75,000 participants across the continent. So this includes phenotype and genomic data for human and pathogens, as well as some microbiome sequence data. Now, this data gets deposited in the EGA and ENA, so that it's available to the world and the samples are being deposited in one of the three biorepositories funded by H3Africa. So this obviously brings a lot of bioinformatics needs, associated with all the data and metadata. And so there was a need to build up an infrastructure to manage this data. So that's where H3ABioNet comes in, so this is an informatics network, it's spread across 17 countries and 28 partners and there are more than 200 members of this, and the aim was to develop infrastructure for genomics research across the continent. So the role of H3ABioNet in H3Africa and generally in genomics projects is to build a computing infrastructure and access to computing, to provide containerized workflows for data analysis and support for these, to standardize and harmonize data. So that it's more valuable to the public and to enable meta analysis, to submit the data to the public repository, so acting as the central data coordinating center, generally to make all the materials and the data fair and therefore accessible as well via a catalog. And on top of all of this, of course, is building the necessary skills to address all of these issues. So starting with building of a computing infrastructure. So H3ABioNet from the beginning started putting in computers and servers in various different places around the continent and training systems administrators in how to run bioinformatics applications on this computing infrastructure. We also developed and been working with different modes of data transfer, a single sign-on authentication and authorization, et cetera. So the next component was really building the analysis tools and the workflows. So we were trying here to focus on African relevant tools and not reinventing the wheel where there were good tools out there already. So we're building an African reference graph to improve their own calling. We've got an imputation service on an African reference panel that can't be made public. We're building many other tools that are associated with using data and transforming that data into biomedical knowledge. To put all these tools together, we've built a whole lot of analysis workflows. So we've built SOPs so that you can see exactly what processes need to be followed for the major H3Africa data analysis types, which is whole genome, microbiomes, GWAS, et cetera. And then what we did with those is we then built containerized workflows for each of these. And these are all available publicly on Quay.io and in the GitHub repository. And these have been used for the analysis of various different datasets from H3Africa, as well as an imputation service that we run for many of our training programs. The next component is data management and support. So there's a lot of different data, there's the metadata associated with biospecimens, there are with clinical data and there's omics data. And for the clinical metadata, what we've been working on is building standards and basically determining what data elements should be collected and how those should be collected, and then mapping these to ontologies and then building standard case report forms from these, and then post hoc harmonization is also important because a lot of data was collected before these standards were developed. And so we are also developing algorithms to map existing data to a minimum data metadata model. So this is an example of some of the standardized components that we're building. So there's a core set of phenotypes and that's a standard CRF, we call it and then we've got module specific, disease specific modules, environmental modules, and there's pediatric modules that could be tagged on so that we then bought a REDCap data dictionary template so that users can come and build standard CRFs from these by adding these different modules together. The other component obviously of data is the movement, storage and submission and access to this data. So we were tasked early on in the consortium to build an H3Africa data archive and be the coordinating center for submission of data to the public repositories, which are the EGA and the ENA for non-human data. So we built secure system based on the architecture in EGA, where you have encryption and landing areas and vaults, and the secure sort of cold storage. And we have, the aim of this is for us to ingest the H3Africa data once it's generated. And then within the timelines specified by the H3Africa data sharing access and release policy, prepare the data for submission to the EGA. So we've submitted 14 African datasets to the EGA so far, and that's more than 140 terabytes of data. And then obviously we now need to make all of these outputs accessible and findable and interoperable and reusable. So the data gets submitted to the EGA and ENA, we've built the catalog, which I'll show you next to enable searching of this metadata. But the interoperable part, we keep, we make sure we've got stable identifiers, we map all the data to ontologies wherever possible. And then we have also access mapping to the data use ontology to show what the access requirements are, or access limitations might be. And wherever possible we adopt GA4GH standards, that's a Global Alliance for Genomics and Health. This screenshot is of the new H3Africa data and biospecimen catalog. So users can come in and search for what data exists in the archive, what biospecimens are sitting in the biorepositories. Once you log in, you can then submit an online access request form, which then gets evaluated by the Data and Biospecimen Access Committee. And then finally we have a huge training component in H3ABioNet because they are all things data related. And we have a huge number of different audiences from basic data users to systems administrators and senior mathematicians. So we have a multimodal approach to training, this is the training trainers have turned to ensure longterm sustainability. We run face-to-face workshops, we've organize more than 30 face-to-face workshops so far, we host interns and then our flagship program is actually a live online training where we have remote classrooms and a live online training that reaches up to a thousand participants at any 1 time over a 4 month period. We also run a hackathon and data jamborees, which are very goal orientated to come up with either analysis or a new tool. So in summary, we've developed expertise in genomic data analysis, workflow development, data management, data harmonization, we've got a lot of experience in building REDCap clinical databases now, and then the tools that we've developed are to pool the data together in a data science kind of mode and convert the data into knowledge and then ideally find the application with both the related infrastructure that you require for all this to happen, including the computing, the workflows, the storage, and just moving of data. So though this was primarily focused on genomics for humans, this infrastructure can be extended to all other kinds of biomedical data very easily. With that I'd like to acknowledge the H3ABioNet Consortium. The project's divided into different work packages led by these work package chairs and co-chairs. We have a very able and competent technical team at H3ABioNet center at UCT. And I'd like to acknowledge the funding from the NIH Common Fund. Thank you for your attention. - So good day, my name is Susan Veldsman. I'm the director of the Scholarly Publishing Program at the Academy of Science of South Africa. My program is involved in the research of research outputs in South Africa, research and integrity, and to be also involved in open science, not only in South Africa but also have a Pan-African focus through the academies in Africa as such. So this afternoon, I would like to talk about a project called the African Open Science Project. This project was an outcome of the International Science Council's document, "Open Data in a Big Data World." It was a 3 year project running from October 2016 to October 2019. So the first phase, namely the pilot phase has come to an end now and we are currently busy setting up phase one of this particular project. It was funded by the Department of Science and Technology through the National Research Foundation. And it was managed by the Academy of Science of South Africa. This pilot study had 7 very specific deliverables it had to deliver to the donor funder, namely that we had to establish an African Open Data Forum. It's a little bit deceiving if you look at our title namely the African Open Science Platform, because I'm sure in this audience that we're speaking to in this particular panel, it rings a bell of software and hardware and infrastructure, et cetera, but it wasn't in the pilot phase. Our main goal was actually just to establish a forum whereby we could create a place where African scientists and researchers could discuss open science and open data as such and also a forum in which we could connect the dots, because that was the main focus of this project, is to connect the activities in terms of open science and open data on the African continent. So this project was launched at the Science Forum South Africa 2016. And I think what is very important to note, which I will not deal with today, and that is that we had to compile a framework for open data policies, a framework for incentives, for sharing research data which was a very important one as we did find mainly that researchers in Africa did not share the data because of many complex issues and reasons. And then we also compiled the framework for capacity building in research, in open data. But one that I'll touch upon this afternoon is the road map and the framework for e-infrastructure. And then also just perhaps explore a little bit of what we found in the landscape report on open science and open data in Africa. I want to flag at this point since we are talking about health data, that there are numerous challenges that we found that researchers were encountered with, when they collected the data. I must add that this is not just particular to health data, it is collecting any data within the African continent. Researchers found that sometimes the funder contracts were quite binding and sometimes the data were used and reused and also curated under very special conditions and not always to the benefit of that particular country. There was also a delay in sharing data that was collected, there were gaps in the data, there's a lack of adherence to international standards, there was uncertainty about the intellectual property rights, who was the data owners? And is this data to be trusted? As sometimes there is a feeling from other countries, is that data from Africa is perhaps dubious. So protecting the privacy of research subjects, what security is around these data? These are important challenges that we are facing. And of course the absence of patient consent was a big concern and considering under the circumstances in which these data to be collected, and of course the adherence to fair principles. I think some of the things that I've mentioned, actually resonates with the previous speakers in which the highlighted is important to set up a proper data repository that is recognized in the world as a trusted repository. It was important for us to understand what open science activities were taking place on the continent. And it was sort of connected to political willingness. We struggled with the word political, but in the indigenous governments that funds research and development within the particular countries. We also found that research is funded in Africa through external resources and not necessarily through governments of the particular countries. There were few countries that we could see that there were movement from the governments to fund the research and development. So we developed a particular grid to understand where would it be sensible in planning this project further, where we could start discussions on open science and open data and collaborative projects. - [Narrator] A map of open science activities and political willingness showing numerous African countries and open science activities taking place within them. - As you can see some of the countries of Africa, they are high density on these particular dots and that was quite indicative to us. Those countries were in a particular position and perhaps more ready than other countries to start discussions on open science. We did find that there were many open science, open access and open data projects on the continent. They are decentralized and not very visible to the outside world of many reasons, if we look at the registries where you can actually register, for instance, your data repository. They were very low on the radar and Africa is not very well in enhancing its footprint into the world of sharing data as such. - [Narrator] African ICT Landscape. - This to me is a very important map, showing connectivity in Africa. This is a map depicting the research and education networks, which we consider as being very important when we start to talk about infrastructures, connectivity, collaborations, sharing of data, et cetera. So what do we have here is on the eastern side of Africa is a very well organized group of countries that group themselves under the Southern African countries, not only politically, but also geographical. That's the oldest consortium of research and education networks in Africa. You can see that they're very well organized, but also in terms of high performance computing centers and then the SKA. - [Narrator] Square Kilometer Array. - In other words astronomy to which professor Russ Taylor will talk about in his talk. And in science gateways, you can see that the population is much higher there in the Southern countries. On the northern borders we have ASREN, that's Arabic countries. That's a very young consortium of research and education networks, but you can only see 2 countries being connected. And then of course on the western coast is WACREN with only 2 countries being connected. So you can see that there are so many countries in Africa not connected to research and education network. And when we start talking about collaborations, then these connections are extremely important. There are also a lot of discussions about what are the possibility of mobile connections to deliver on these data sharing and collaborations as such. This is a graph that shows you that in 2016, 70%, I think maybe let me just retract and say that we know that the mobile connections penetration in Africa are very high, but if we actually look at the breakdown, 70% of it is 2G. And if we look at 28%, it's 3G, and we look at 2% it's 4G, and it's only from 2020, where Africa started to look at 5G as a possible mobile connection mode for Africa, but still projections are that it will going to be very low. The penetration is also not what initially was thought and mainly for the moment is because of the cost of mobile connections in Africa as such. So if we just scan through the infrastructure challenges, I think it's a very sort of a gloomy picture that I'll be showing you. But I think it's important if we talk about data sharing, the previous speaker Benedict, talked about some wonderful ideas that crossed our mind when we looked at possible solutions for Africa, but the situation is still dire. So there are selected governments who have a very low awareness of the value of research and education networks. And some networks as I've shown is not operational with low and even no budgets. The biggest problem is that commercial public internet service providers is a threat to research and education networks. And whilst the research and education networks do far more than just being an ISP, there's a huge monopoly by these commercial public internet service providers, especially in Central and West Africa. So much so that they close down the access to fiber landing stations, and they would not allow other competitors into the market, keeping the costs extremely high in terms of the connectivity. Power outages on the continent interrupting the internet service delivery is rife. It is a common problem in the countries and of course, when we interrupt internet services, we also interrupt science. At one stage, we did think that cloud services would be the ideal, considering that all the hardware and software sitting in the cloud, and it can be pushed to the researcher's network in terms of the access. But again, it is very expensive. We do find that in the 3 year period that we were looking in this project, that the costs are coming down and there are more role players in this field so the competition base are much higher. When Professor Nicky Mulder, spoke about H3ABioNet, of course, this was one of our landmark projects in the project as such, because of the work they've done. And I don't have to convince you about the excellent work they're doing and how they're actually going about to construct these data repositories. But they're making also use of medium scale server infrastructure, lots of problems about the data not being trusted and of course the infrastructure is not funded. So the sustainability and the maintenance and the curation of the data on those platforms are highly problematic. We also found that researchers have a small number of computer workstations and that sometimes even donated servers and software are already outdated. Data management is also a main problem, data management plans are not the norm. And that's due to the lack of policies and funder requirements within these particular countries. And the biggest problem is that there's a lack of centralized secure infrastructure that makes collaboration and data sharing and storage impossible in some cases. Just for your information, we only have one CoreTrustSeal repository on our continent and sitting right in Cape Town at UCT. So there are few repositories that use proper data repository software or science gateways, and that they have tailor-made for the purpose. And of course a non-adherence to international best practice regarding persistent identifiers, metadata, licensing, intellectual property rights, data citation, archiving, and back-up of data is still problematic. And still, we need to re-raise awareness about these very important issues when setting up repositories. There's no incentivization to share data because the researchers want to exhaust publication possibilities before they actually share the data. And of course, the training of the researchers in their new role of data collectors and curators are also very important and because suddenly researchers are expected to be knowledgeable and trained in these areas as well. So what is the key then to future African Open Science Platform? First, and very important is the collaboration, not only among countries or institutions and projects, researchers and funders, resources need to be shaped and we need to ensure that there's a free flow of data, research and the knowledge. For me, very important that I've learned right through this project, is the trust in relationships. We have to build trust, we have to be open and we have to be transparent. Researchers want to trust the others who are having their best interests at heart and not because of the profits they can make and from the research being conducted in Africa, it must be researcher driven. Right at the end of our project, I think that was the biggest missing link in all the work we did, is the fact that researchers were not necessarily included when setting up the infrastructures to collect the data from, and the needs of the researchers need to be addressed. Infrastructural investment, and we have to keep our momentum in the activities that's already taking place on the continent. We need strong leadership, and we need to build on the knowledge that has been gained over the last couple of years. I want to thank you for this time and this opportunity to participate in this discussion. And very specifically our funders, the South African Department of Science and Technology and Mrs. Ina Smith who spent much of her time in the 3 years to make a success of this project. I thank you very much. - [Narrator] Email: susan@assaf.org.za. - Hello everyone, my name is Russ Taylor. I'm a professor and research chair jointly between the University of Cape Town and the University of the Western Cape in South Africa and the director of the Inter-University Institute for Data Intensive Astronomy. Hello, I'm going to be talking about a big data challenge coming to the African continent. The challenge driven by astronomy and I'll talk about the solutions that we're working on to address that challenge. But over the course of my presentation, you'll see that there are many common aspects of this challenge to the challenges being faced in the medical community and bioinformatics. And in fact, we're currently working with bioinformatics researchers and adapting this technology for their research. So the challenge coming to Africa arises from a big astronomy project in radio astronomy called the Square Kilometer Array. This is a large global mega science project, involving 12 countries around the world. Africa plays a key role in this project because a major part of the infrastructure for this telescope is coming to the African continent. And that's shown on this next slide where I've depicted in yellow on the African map, the locations of the infrastructure elements of the Square Kilometer Array, as they will be deployed out over the next 10 years. And you see that infrastructure spans everything, many countries in the southern part of Africa, out into Madagascar, Mauritius and in the Indian Ocean and Kenya and Ghana in the northern part of Africa. So it's a highly distributed project involving many stakeholders on the African continent and it also involves very large data. In this slide, I'm showing a plot of the scale of datasets that are being created by major astronomy projects. That's a function of time where the size of the data is plotted vertically on this axis while rhythmically. So a straight line on this plot indicates an exponential growth in data. And you see over the last few decades from about 2000 to 2020, there's been an exponential growth with a doubling time of about 2 years. And that growth has happened largely outside of Africa, entirely outside of Africa, primarily in North America and Europe. And you also see on this plot that there is a kink in that curve starting right about now with a faster growth over the next 10 years, the next 20 years and that growth is happening in Africa, largely driven by the Square Kilometer Array Project. And we need to develop solutions in Africa for dealing with that data if African researchers are going to be part of the research leadership in these astronomy projects. So that's our challenge. The other aspect of this challenge of course, is that by moving towards these large, global mega-science unique projects, we have a dynamic in which the entire global community is going to be given access to this project and this project will address large key science challenges in astronomy, which are of interest to large global teams. And so we have to have a situation where the telescope's producing these very, very large datasets, eventually exoscale datasets and teams all over the world are going to want to work on it. And as you heard earlier in this panel, this is a common challenge to bioinformatics and biological research as well that we have data challenges and distributed teams working on the data and there's no way that every researcher can have this data on their local computer. So we're facing a different dynamic in terms of how to do science, both in terms of the technology and the sociology. This challenge was already recognized almost a decade ago now, and I'm showing you a quote from a book written by Al Gore. Book entitled "The Future: Six Drivers of Global Change." - The quote reads, "What to do? To develop a new generation of computer technology to store and process the data soon to be captured by the Square Kilometer Array, a new radio telescope that will collect each day twice the amount of information presently generated on the entire World Wide Web". - And he recognized that projects like the Square Kilometer Array, which are going to capture every day, twice the amount of information currently on the internet worldwide. Means that we have to do things differently and we need to harness the technologies of the fourth industrial revolution to solve this challenge. And one of the new modalities that are being opened up by these new technologies is the ability to collaborate We no longer have the old model of hierarchical, in-house solutions to problems, but we now can harness the diverse talents of the global community if we can provide them the means to work with the data and to bring their knowledge and expertise to bearing the problems. So that's the challenge that we're facing. And we see cloud computing as one of the key elements of the solution to that challenge. And we have established in South Africa, a cloud computing facility dedicated to data-intensive research. It's a custom cloud we built in-house in South Africa, it's called the Ilifu... - [Narrator] Ilifu.ac.za. - ...and I'm the project lead on that project. And you see in the bottom of this slide, the logos of the partners, we have 5 South African universities involved in the project. We have the Square Kilometer Array Project, and we also have the South African government through their National Integrated Cyber Infrastructure System and the strategic plan. So between all these partners, we've jointly funded the development of the cloud to work with big data, driven by the development of the Square Kilometer Array. - [Narrator] I-L-I-F-U program priorities. - But in fact, with Ilifu we've targeted 4 strategic areas to address, and of course the key one is astronomy, solving data-intensive priorities for the projects leading up to the Square Kilometer Array. We're also working with bioinformatics researchers at the University of Western Cape, at the University of Cape Town and Stellenbosch University. The lead University of Cape Town researcher is Nicky Mulder, who you heard earlier in this talk and some of the work she talked about was being done on the Ilifu system. We also have a team working on research data management, how to effectively manage the data outputs of the science programs. And we're also working on the next step in technology, which is to federate this cloud, globally distributed network of cloud providers, to build an international network and a commons for working with big data and astronomy. So this graphic sort of shows you the modalities with which the cloud provides services to the researchers and how we empower the end researcher to work with the big data. So everything on the left in this diagram, in the dashed box is hidden from the user, but it contains all of the technologies that we need to deploy to provide platforms and services so the researchers can work with big data. And on the right are the interfaces that users can use to access the resources. We have, like you heard earlier in the bioinformatics domain, we have a Jupyter Lab environment, which allows users to fire up as a web service. E-programming environment which provides access to the virtualized computing environment in the cloud, and also all the data for which a user has authorization to access. So all the computing power and the platforms and the data are hidden behind the Jupyter interface. So there's a very powerful computing environment that they're tapping into with an interface, for the real geeks, we have the command line shell access. We also have capability for projects to manage their own resources and users through an Openstack tenant environment and we've developed a visualization system, a server client model for visualization, where all the heavy lifting happens on the cloud. But the user interfaces is used to a web environment. So you can visualize tens of terabytes sized datasets with your iPhone if you wish. So that's the basic architecture of Ilifu in the way users interact. This is a plot showing since the launch of Ilifu in 2018, the growth of our user base. The blue graph shows our astronomy users and the red graph shows our total users. And the difference between the astronomy and total is the bioinformatics use of the system, which you can see is growing with time and represents about 10% of our users. I like to show this plot because it shows the distribution of users on the system by country and institution. And what you're seeing here is that about 70% of the users are from within South Africa, but 30% come globally, primarily from the UK, the Netherlands, the USA and in several European countries. And when the SKA project came to Africa, there was a concern that we might be confronted with the old paradigm of the African science and engagement with the international community, where data is produced in Africa, but then shipped offshore to the US or Europe institutions for data processing and harvesting the research from the data where African becomes a provider of data, but not as engaged in the actual leading in the research. So this graphic shows that what'd we set up here is a cloud system, which actually is servicing users in South Africa and empowering users to work with big data. But it's also a system that's used globally by our collaborators on these projects. So it models both the fact that we are a glazed system that empowers African users, but also it's a global partnership and involves research with interest nations around the world. An important aspect of this system is the training aspect, which demonstrates 2 things. One, the capability to reach out anywhere in Africa to bring researchers to the data. And also the great opportunity for empowering and exciting the new generation of researchers all across Africa in this new paradigm of working with big datasets and doing research. So what I'm showing here is an event we hosted in Madagascar last year, and you see a bunch of students clustered around a table in a basically a hotel in Northern Madagascar, using the hotel internet to connect to the Ilifu cloud and running big data problems on that cloud. And so basically you don't need great connectivity to work with this data. You just need basic connectivity, mobile connectivity is enough because all the heavy lifting is done in the cloud. And so this provides access to researchers anywhere in Africa to working with the big datasets. And here's another picture of a similar kind of event hosted in Botswana, also last year. So it's really is a powerful new paradigm for enabling end users to work with big data and without having to have massive resources where you are. The other aspect of the cloud is of course the impact on open science and reproducibility. It's recognized, I'm showing here some excerpts from an article in Nature and notes from a conference at the Training Institute in the UK in 2016, whether the cloud opens up a new possibility for open science? The fact that the education environment is virtualized and it means it's decoupled from any particular hardware or any location. The fact that we can capture all of the software and the data and the execution in a container and use it as transportable to any other cloud around the world. So it offers up the technology to underpin a mode for open science around big datasets. And we've begun a conversation with global partners about building a federated cloud environment which would, not only build a common ecosystem for working with big datasets and astronomy, but also be a platform for this open science mode where we can distribute and reproduce scientific results anywhere within the federated cloud environment for researchers. I'll just wrap up by... You heard earlier from Susan Veldsman about the African Open Science Platform. My institute IDA was engaged with Ina and Susan in developing the framework and roadmap and we believe that cloud technology and federated cloud technology will be a key underpinning to building the kind of platform that we need for open science in a Pan-African context. We're looking forward to working with the Open Science Platform project to make that a reality. So thank you very much for your attention. I think that developments in Africa are happening quite rapidly, and we are working to solve big data challenges in astronomy, which will directly transfer to the kinds of challenges that you're facing in the medical and bioinformatics regimes. - Hi everyone, my name is Geoffrey Siwo. I'm a research assistant professor at the University of Notre Dame Center for Research Computing and the Eck Institute for Global Health. My talk today is going to focus on how we can think beyond open data, think about how to connect people, computers, and the various emerging technologies like artificial intelligence and machine learning. My talk will focus on three areas. First, I will share a personal story on why I believe that open data is so powerful and will transform science in the future. Second, I will talk about how... Think about this concept of open science and connecting it with the ability of computers to basically bring together hundreds of people across the world over the same problem, can help us solve some of the most important biomedical problems faster. And then finally, I'll talk about some of the technical barriers that we might face and how we can address these. And I hope these are issues that we'll be able to discuss during the panel discussions. So let me take you back to 2002, when I was an undergraduate student in Egerton University in Kenya. And back then I became very interested in understanding how virus-like elements in the human genome might interact with HIV. So this was an idea that I couldn't obviously test in a lab. So I went to an internet café where I could basically access open data. So open data composed of a human encoded endogenous retrovirus elements, as well as openly available HIV sequences. And with this data I was able to test an hypothesis of how HIV could potentially interact with a human indigenous retroviruses. And after getting some interesting results from this work that I did back then, it was accepted for a presentation at a conference in Chicago, and actually Anthony Fauci became my hero back then, because I wrote to him an email asking him to support me to attend this conference. And fortunately, through the help of his office, I was able to attend this conference. But the most important lesson I learned from that, was that open data can empower anybody in the world to solve important problems even with the limited resources. Let's fast forward to what I'm doing today, which is basically, exploring ways through which we can use the power of the internet to bring hundreds of people across the world to work on some of the most important biomedical data problems. And I'll give one example of what a project we called the Malaria DREAM Challenge project. And the goal of the project is basically to identify big problems in malaria that could be solved using genomic data, and then open these datasets to anyone around the world and invite people to compete, to develop models that could address a specific problem in malaria. So this is basically a means of crowdsourcing. Crowdsourcing has been successful in many other areas. And so last year we organized the first Malaria DREAM Challenge, whose goal was basically to ask anyone around the world to try and build computer models that could take in gene expression data of malaria parasites across the world, and then predict whether those parasites are sensitive or not to our key antimalarial drug known as artemisinin. And so from this crowdsourcing event that we organized, we learned some very important lessons that I think are applicable across a broad range of areas. So some of the most important lessons we learned is that when you take a problem with data and open it to basically anyone in the world, first of all, you get a huge diversity of solutions. Solutions that you as an individual or your lab or your company could not have imagined before. So in this particular Malaria DREAM Challenge that we organized, about 360 participants from 31 countries were able to engage in this project in a period of 3 months. So this is a level of productivity that you can imagine that even the biggest company in the world cannot employ hundreds of people to work on the same problem. In addition to that, we also got a wide variety of solutions. So for instance, there were 60 different pieces of code that was submitted addressing this problem. There were approximately a hundred models, that were also submitted to address this particular question. So this is a very powerful approach of solving biomedical problems using open data. But of course it requires that those data be openly accessible. So I would like to highlight some of the barriers and the role that technologies can play in addressing these barriers. So one of the big challenges in many parts of the world today is that even though data is growing at an exponential scale, there are still data limitations when it comes to specific problems. So for instance, in many areas of biomedical research, there isn't a lot of data coming from the African continent, so we need more data that is more diverse. We also need to think about more computing frameworks that can handle a wide set of datasets that were not aligned. So for instance, in the past few years, artificial intelligence has made enormous progress, but AI, as it is today relies on huge amounts of datasets that are not available in many places of the world. So we need to think about innovations in AI that could help us to extract meaningful insights from small datasets. The other challenge that we need to address is that there are many reasons why data cannot be shared sometimes. It could be because of privacy reasons, it could be security reasons, or it could be also commercial interests, but these challenges should not stop us from innovating with the data. So you need to think about computing frameworks that can analyze and encrypt the data without the need for decryption, because if we do this, then it means that a wide number of people can actually have access to these datasets without threatening the security or the privacy of those datasets. And 2 frameworks are the use of privacy-preserving computing, as well as the use of trusted execution environments, which are new forms of hardware that enables secure analysis of data without outside sharing. We also need to be aware of the need to be transparent with the data, to have a way of tracking the provenance of data and models and what people do with those data and models. And I think this is an area that technologies like Blockchain can begin to help us address some of the underlying issues. So with that, I'd like to thank you for attending this session. And I hope we'll be able to discuss more of how we can address some of the barriers that can prevent the use of data widely, thank you. - [Narrator] DS-I Africa. Katherine Lawrence. - Thank you again for joining us for this panel. We, the panelists and I, look forward to discussing these projects with you and answering your questions during the interactive portion of this panel. We encourage you to share your questions on the KIStorm platform. We'll see you soon. 