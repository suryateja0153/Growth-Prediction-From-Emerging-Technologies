 Lucy and Kyle are at Allen Institute for AI,somewhat, obviously to most of you, that's in Seattle and, they are of course going to talk about COVID, this is something I'm very excited to dig into. And I hope all of you are, so without further ado, here are Lucy and Kyle.  Awesome, thank you so much, Ben, it is a real pleasure to be here today, thank you for inviting us. My name is Lucy Lu Wang, here with my colleague Kyle Lo, and we're going to be discussing the COVID-19 open research data set or COVID-19. And also ways that it's been leveraged by researchers to mine the COVID-19 literature. As Ben mentioned, we're both researchers on the semantic scholar research team at the Allen Institute for AI, out here in sunny Seattle, Washington. And we both worked extensively on the coordinating corpus, so to start motivating this problem, COVID-19 literature has been published very quickly. This image I borrowed from COVID-19 primer says that more than 70,000 newspapers have been published in this year alone since March and this amounts to 700 newspapers a day can be really hard for clinicians and researchers to keep up with the latest findings given this degree of publishing. So to help manage information overload and address these issues, There's a clear need for automated methods, that leverage artificial intelligence techniques to assist readers, in managing this large volume of peoples. So how does one build an automated text mining system, for scientific literature? A couple steps, first, you might have to construct a corpus of documents and to do this, you'll need to identify potential irrelevant documents, pre processed them into the same format, maybe a machine readable format. Then there's data enrichment, so this includes adding annotations to important entities maybe to mentions of drugs or use or other things. And then potentially labeling data if you're trying to train a model that does something else. This is followed by model development, which is actually creating and building the system. And here the kinda text mining practitioner, the AI practitioner has to decide what the system should do. Does it retrieve documents based on a query, does it try to answer questions, does it try to check facts against evidence in papers? And they also have to choose what models to use to do this. So if you use a neural model, what architecture to use should they use a pre trained models and finally, one has to evaluate the performance of the system and figure out whether it actually satisfies user needs. So each step in this pipeline can be quite expensive and time consuming. So this is where the community efforts around open data, shared and reusable modeling and annotation resources as well as community shared tasks can really come into play. Now, these efforts have contributed a lot to speeding up this pipeline for COVID-19 and for this talk, we focused on introducing some of these community efforts and discussing, how,they've helped success. So I'll start by talking about the coordinating team corpus, which attempts to centralize the process of corpus creation. The CORD-19 is a data set of structured machine readable COVID-19 research papers, and we release the first iteration of this data set back in March in collaboration with several partner organizations. And since then the data set has grown a lot, now it includes more than 300,000 entries and full texts are available for about 120,000 documents, it's also updated daily. This data set was one of the earliest COVID-19 literature datasets that was publicly released and consequently it's served as the foundation for many dozens of text mining systems for COVID-19. To construct the data set, we start by answering the question, what is a COVID-19 related paper? So each text mining corpus kind of has to answer this question of what to include within the corporate office and what to execute, so for COVID-19, we use keyword search to identify relevant views. We also include papers from a number of curated COVID-19 specific databases. And the key thing to note here is that the keywords that we use not only covered COVID-19, but also historical coronaviruses such as SARS and MERS. And we include papers where a keyword shows up in the title of the abstract or the full text of the document. And here are the sources from which we derive these papers and this includes things like the World Health Organization's COVID-19 literature database, PubMed Central, PubMed, boirXiv and as well as various preprint archives. And these papers are adjusted through the semantic scholar data pipeline, where we clean and harmonize the metadata from these for the papers, and then also extract full text out of PDFs when those PDFs are open access and available to us. Then we release the metadata and this full tax as part of the court 90 days. Here's just what some of the data looks like, at the top there is a metadata entry with typical paper metadata fields. And we provide a link between that metadata entry and the full tax extraction of the paper, when an open access PDF is available, not a snippet of full text is shown in that gray box. So this full text extraction takes advantage of this custom PDF to JSON parser that Kyle and I created as part of a source project. And that's actually how we became involved in the coordination project in the first place. And in this full text extraction, we annotate things like citations, references to figures and tables and other objects but of course additional annotations and post processing can be performed on this full text. So now, I'll give some examples of some additional annotations that have been made available in community and which can be used by others. So there's many things that one could imagine annotating the things like named entities, genes, drugs, maybe relationships between entities. If you're working with clinical trials, Pico elements could be very useful. These labels can be automatically generated, for example using existing named entity recognition models or text classification models. And we see many examples of groups who've annotated various classes of named entities. For example, two terms in biomedical ontologies, or two terms in MLS and some examples of this are CORD-19 on FHIR and the Sci-Bite annotations. Labeled data can also be generated in a crowdsourced fashion, so CORD-19 is an example of this. This group use crowdsourcing to generate question answering labels on top of documents important. So question answering is a specific natural language processing task where given an input question, the model attempts to retrieve a span of text that corresponds to the answer to the question. And finally, annotations can be expert curated and these are perhaps the most valuable annotations but also tend to be the most expensive to collect. So medical experts have been asked by various groups to annotate or make judgments about the documents in coordinating especially as part of various shared later. There's a couple platforms out there that help to support like, I guess the public sharing or discovery of these annotations. So probably annotation is an example of this, where people can upload annotations making dumb for read and discoverable by other parties. Here are some example annotations made publicly available on this platform for coordinating to ontologies such as human phenotype ontology, monarch disease ontology, disciple. By annotations that are available here as well. And PubTator is another platform where annotations are shared on COVID, which is another COVID-19 papers data set released by PubMed. And as I mentioned before, shared tasks can also be a source of high quality invitations. So shared tasks provide infrastructure to compare performance between systems. They encourage groups to work on a single problem or task, and then judge the performance of these systems against one another through expert assessment. So for example, on a shared document retrieval, medical experts may judge they may take the retrieved documents from each of these systems and judge them for relevance to a specific query. And these assessments can then be converted into annotations or labels which can then be used to further train. Models and improve existing model performance. And we'll hear more about the shirt house later. So the vision is for researchers in the community to create these various layers of annotations on top of the structured Full Text import. And then share these annotations publicly so that other folks can use them. And because these annotations are on a common data set, it's easier to kind of use many, I guess use annotations from different groups, because they all apply the same underlying data set. So then when I or someone else and building a text mining system, I can go find the annotations that are useful for me and add those as inputs into my model. Or if I create new annotations, I can also share those which will maximally benefit other groups in the community. Now before we move on to the next part of the presentation, I just want to briefly revisit this question of what is a COVID-19 paper and also to share some other potentially useful open data resources beyond. So in COVID-19, we select papers based on keywords But these papers are part of a network of papers, that kind of it's like the whole of scientific literature. So if we expand on COVID-19 by following all the citation relationships in those papers, we get something called the coordinating closure graph. And this is joint work with folks at Microsoft academic. The closure graph includes millions of papers, referenced by papers within COVID-19 but goes beyond the limit of working. And looking even further to the full of scientific literature, we also refer people to S2OR which is the semantic scholar of an open research corpus. And this is a Corpus of court, 19 style full text data. That's extracted from over 12 and a half million open access papers in listening to the scholar database. And these papers are from across all domains of science, including also, some, more humanities fields as well. And both of these resources can provide more context around the papers. So now I'll turn it over to Kai, who will talk a bit more about modeling resources and Luke speak.  Can you cook slides for me? So we don't do this thing. Okay, so to go through these relatively quickly since we're kinda at time, I wanna talk a little bit about that the fact that AI models are pretty expensive to train. One way to dramatically cut down on a cost of developing a model is to pre train a model and then share that model with others to use or extend. So one common form of this is document embeddings. Which allow you to take papers such as coordinating papers, turn them into vectors such that documents that are similar to each other will appear together in space close together, compared to documents that are far apart from each other, which are documents that are dissimilar. This means that documents with similar, fields of study, or research teams will be clustered together, and we call these embedding, and Spectre is one method of doing this. Next? Of course, AI systems not only benefit from betting documents but also individual words. So the current state of the art method for doing this called Bert and people have been trying to training variants of birds to handle specialized language inside papers and COVID. Bert is trained on coordinating is an example of this next. Finally, knowledge graphs are collections of concepts or entities, which are represented by-  Think you muted yourself, Kyle. That was me, I muted you by accident, sorry. [LAUGH] Hello.  Apologies for that.  Right so yeah relationships between entities are also represented in knowledge graphs. An example of this are is COVID graph which contains information about COVID-19 concepts and their relationships as well as incorporating other. She never claimed chemical ontologies Next,and I just wanted to walk through an example of how these might be used. So a common pattern for an AI system might take a query or question such as the one about hypertension above, and make use of embedding techniques are next. To retrieve the relevant articles so you can find documents that are similar to the query to limit the space they work that you then run your next birth model over to extract the specific answer. Answers to that question next. And finally, you can augment the retreats collection of documents or snippet tech snippets by finding by using a knowledge graph. So a knowledgeable Graph might contain known relationships between hypertension and other phenotypes for example, and you can use that to find Text snippets that make not exactly men mentioned the word hypertension but are still very relevant to answering the question next and finally. To talk about to give like a quick overview of share castle competitions. Next I'll briefly talk about three share tests that have been done been launched on COVID-19, the kaggle core 19 challenge trek COVID and epic QA So Kaggle Challenge was the first competition that launched alongside the CORD-9 release back in March. Participants were presented with questions that they have to answer using extract relevant information from CORD-19 papers. And since the competition has concluded, one thing we've really learned is that you really want structured extractions kind of like what you see here in a table format. Because this makes the extract as much easier for medical experts to consume as opposed to early on when participants were extracting just lengthy text snippets that contained the answer. Next, TREC-COVID is a related shared task which focuses on finding what documents are relevant to answering a question. And in this task, we organized into five rounds where we release new questions over time, as well as new papers being added to CORD-19. And to get a sense of what we've learned has been really near look at how information needs have changed over time. So you can see in Round 1 back in March, we were asking very different questions about virus, next. Compared to Round 3 in May, next. And finally Round 5 in July, the types of questions the things of information that we were focusing on was definitely very different over time. Next, and what we've sort of learned from this is that the notion of what a relevant document might be sort of changes over time along with the questions and the document collection. So, early on back in Round 1, we didn't really know very much about the virus and so a lot of historical Coronavirus research was considered to be irrelevant to answering these questions. But by the time we get to Round 5, we have more COVID-19 papers than we can possibly read. So do experts still consider those early historical papers to be relevant now even if they were relevant back then? So TREC-COVID is where it gives us this opportunity to study this because we have various snapshots of CORD-19 over time and various annotations Benchmark to that state of the CORD-19 corpus overtime next. And finally EPIC-QA is sort of an extension built on top of TREC-COVID which wants to Also answer this notion of does relevance depend on something else besides just like the document in question. And in this task, we're trying to answer the question of does relevance answer depend on also who's asking the question? So in EPIC-QA, we're trying to answer questions about COVID-19 that would be most suitable for a healthcare consumer versus a medical expert to consume that article. And so maybe possibly, this is an ongoing forecast and possibly what we'll find is that relevance is not just a function of time, but also especially of who's asking the question. Next, and so in conclusion, AI like text mining or natural language processing techniques has the potential to be very helpful for reducing information overload and helping experts find answers to COVID-19 questions. The development of these systems is very expensive and we highlight in this talk various ways open data, and resources have contributed to speeding up this process. And relieving the burden for AI practitioners at various stages in the development pipeline. Next, we just wanted to thank our many collaborators on CORD-19 and the various casts, next. And we've included links to download CORD-19 AND various other resources. And that concludes our talk. Thanks.  Allen, thank you very much for going quickly the end and thank you guys for an amazing talk. We are running about 11 minutes fine. So we are gonna hold questions to the break period. That said, at least Lucy and probably Kyle are on slack and you can see their Twitter handles here. So please tweet to them, ask them questions in the slack. Now we are going to transition to Imran Haque who is going to give our next talk.  Hello.  Hey. So my advice to you is if you need a speaker, invite Imran. So when I used to work at the National Library of Medicine, I invited him to give a talk about what I won't say because of what I'm about to say. But I will say he was very frank, he did not sugarcoat anything and his criticisms were of a very mathematical basis. And at that point, I realized this guy he told it like it was and I would always be happy to invite him to give another talk. So with that, I'll turn the floor over to Imran.  Thanks so much Ben. All right, let's get the screenshare set up. Okay. All right, those slides look good to everyone?  Yep.  Very good. Cool, so thanks so much Ben Huajan and the other organizers for inviting me to come and speak today. I am currently the VP of data science at Recursion Pharmaceuticals. As Ben mentioned, I've been working in the biotech industry for a while, gave a talk at the NLM regarding cancer detection a few years ago. And have been interested in applications of computer science and open science to problems in biology for a number of years. What I want to talk to you about today is some of the work that we've done at Recursion against the COVID-19 pandemic. And in particular for this conference, our larger open data release around that work named RxRx19. As you can see a large morphological profiling image and metadata data set against various aspects actually of COVID-19 as a disease. My Twitter handle is there and I'll have contact information at the end. Happy to get in touch with anybody who's interested in any of the work that's going on here, any of the data. And of course, I'd be remiss to say if I didn't say my team is hiring. So if you like what you see, please, please, drop me a line. A little bit about Recursion. We're building a vertically integrated biotech company that really builds massive empirical data sets at every step of the process in order to accelerate drug discovery. When I say massive data sets, what I mean is that our total imaging dataset now I think the latest public number that I can talk about is around four and a half petabytes. So 4,500 terabytes, four and a half million gigs of image data that we've collected from our assay that I'll tell you a little bit more about. That we then use to drive programs through discovery and eventually into the clinic for a variety of conditions, both rare and non rare. And you can see on the slide a little bit how we start with building phenotypic models of disease in human cell culture, assessing it in super high throughput. And then taking it all the way down through that drug discovery process and actually moving it into the clinic into human translation. But somewhat uniquely among pharma companies, along the way we also support open science and it's one of the reasons why I joined Recursion and why I'm very proud to be here today and contribute to these efforts. We have a site, RxRx.ai that actually shows off all the data sets that we've released today. We started with RxRx1 a couple of years ago. And just this year, we've released RxRx2 and 19a and b which will be the main focus of my talk. So a quick outline for what I'm gonna talk about. I'll explain what these RxRx datasets are, like what is it actually that Recursion has released. And then I'll go into a little bit about the process of how we went about releasing them and some of the considerations that come into play. When you're doing a dataset release, that's not only as huge as the one that we're putting out. But also when you're doing so from an industrial context, I'm guessing that a lot of the folks who are joining today are looking at open science from a primarily academic context. And there are some different considerations that apply when you're industry And I think it's valuable for both sides to to engage on this and understand like, you know, what are the things that all of us have in common that that we can share in order to push these ideals forward. And finally, I'll share a few notes on what's happening with these datasets now, right. Now that we've released them, it turns out that like, We actually have gotten quite a bit of utility out of making them more broadly available and I'd like to share a couple of those notes with you. For anybody who's interested in more about the data set more about the experiments or the results, i encourage you to check out our preprint I have the link to it down here at the bottom. If you look up my name and bio archive, you'll, you'll find the link as well. And of course the information is, is at the easy to remember URL RX RX today. So without any further ado, let's dive right into it. You know, let's talk about, you know, what these RX RX data sets word that, that, that we released. And, and why they're potentially of interest. To understand that the first thing that you need to know is a little bit about our assay and about our platform. We do, in ultra large scale, a particular kind of experiment known as a morphological profiling. For those of you who aren't, you know, cell imaging aficionados. In other words, were like myself before I joined recursion, I'll explain a little bit. So previously, I came from the sequencing world, right? And the sequencing world has a little bit of a chip on our shoulder, like, hey, we've got the biggest data, our costs are scaling faster than Moore's law. Everything's great, right? Right, and now we can do single cell sequencing. What's interesting is that microscopy is an incredibly data rich technique. It's intrinsically single cell. It's intrinsically spatial and it's highly flexible because you can tailor the kinds of things that you're looking at in such great detail. That was something that I didn't appreciate before I came through recursion and I've really been able to see the power here. Now, because my cross fee is so powerful, or so flexible, there are a number of different modes in which you can run it. One mode which has become extremely popular, particularly in drug discovery, and and and such sort of beginning of the chain steps In high throughput biology is a mode known as high content imaging, we'll use a couple of generic stains to be able to lock in on particular aspects of your cells, like where the nuclei are where the cytoskeleton is, but then you'll use specific stains in order to highlight a single pathway that you're interested in. Now, this is super high content, right? Because you can actually look at the trafficking of individual proteins, you can figure out where, You know at the cellular or sub cellular level they are, but it's challenging to scale because it means that both the essay as well as the analysis, or custom for every experiment, recursion uses a different method known as morphology. Logical profiling, where you use a larger number of stains, but all of which are shared across all the experiments that you run. So for example the the stains that we use were described in the cell painting paper the the brand all paper from from nature protocols listed at the bottom there. We're looking at a set of generic stains looking at the nucleus, the nucleoli. The actin Golgi and plasma membrane and the and the mitochondria. Now, you might think, Well, that sounds great. It makes the acid really cheap and easy to scale. But do you get the same information out? The short summary is yes, you have to build a quite a bit of computational infrastructure on the back end in order to do that. And in fact, we're our first data set release came from RX RX one was in running a competition to demonstrate that in fact, you can get that kind of accuracy from generic stains that you would get out of specific states. Now, this is really interesting because it means that you can standardize the experiment. You can use computer methods to recover the information on the back end, and you can really scale up your asset capacity and your ability to generate data So with that, now we can understand what actually is inside the RXRX family of data sets. There are four data sets RXRX1 and RXRX2 are what we would call phenotype only datasets. These are imaging. These are morphological profiling experiments on a variety of human cell types, either primary cells or cell lines. Where we've perturbed those cells with a variety of perturbance in RXR X, one, there, sir, and age genetic perturbations. So trying to knock down, you know, about a thousand different genes. RxRx to their soluble factors so things like cytokines and and and performance and so on that'll affect this the state of the cell. We also have these RxRx 19 datasets where not only do we have phenotypes, but we also have drug screens conducted against those models. Rx Rx 19. We use both active as well as inactivated sards covid to virus, the causative virus for COVID-19. And then screen drugs and dose response against those infected cells to see which ones would make the cells look more like the healthy state. In RX Rx 19 b. We did the same thing but instead of using the virus we used a cocktail of cytokines informed by measurements from the patient from the plasma of patients who had severe COVID 19 in order to look for agents that might have actually be successful in knocking back the COVID 19 associated cytokine storm. Critically these two sets of data sets. Have different licences attached to them and I'll explain why as we go a little bit further in the talk. Now, these data sets include not just the imaging that I mentioned, although they do have an awful lot of that hundred Have gigs of data of five or six channel fluorescent microscopy data. RX RX one, two and 19 B are all six channel 19. A is five channel we left out one of the dyes that has to be applied to live cells, basically for biosafety considerations. Beyond the beyond the images we also include the image features derived from recursions internal deep learning models. As I mentioned, you have to do a lot of computer vision work in order to extract relevant information from these images, because they're all taken with generic stains. We've done that work and we've actually provided the features along with the datasets. Of course, you need the metadata in order to make any sense of. So include files and metadata, including technical information like the plate IDs, well IDs associated with each well, what perturbation was in them, and what treatment and what concentration those were in there. And, finally, something I'll go into a little bit later in the talk, we also have a interactive visualization tool at covid19.rxrx.ai that allows you to step through and actually see what the results of the drug screens were for yourself. So with that, how do we actually go about releasing this dataset, there are a number of critical decisions that you have to make in order for releases of this size. One that seems a little bit obvious, but really comes right at the beginning is what formats you're going to use. Now, obviously, internally, we have, pretty interesting infrastructure on the back end and file formats that are Setup for rapid machine learning and so on and people are working, primarily with Unix systems. However, we wanted to make this data accessible to as many people on whatever size systems they're using and whatever operating systems and so we ended up just taking lowest common denominator formats, right? The images are stored as large zip files. We didn't even use tar balls because it's harder to do with tardes on Windows. Systems, the images are encoded as PNGs rather than ZAR. And rather than storing the metadata in a complicated binary container, they're just CSV. Although these formats are less efficient, and perhaps you know, not the ones that you'd use in an active machine learning system. They're more easily accessible to as many people, because we want to maximize our reach. You have to make decisions about where you're going to host this data, now although there are a number of repositories for open scientific data out there, the fact of the matter is Most of them are not going to take releases of hundreds of gigs coming from a commercial institution. We already have a heavy investment in Google Cloud. And so we ended up posting these on Google Cloud Storage, and then link to them from our landing pages on rxrx.ai. That's not the kind of thing that will work for everybody, but it's a really useful first step. And as it turns out, I'll tell you a little bit later. As a consequence of that, we've actually been able to get some interesting deals with Google in place for this data. And finally, you don't wanna just drop a data set from the community and then never come back to it. It's almost certain that the first time that you put something out, there'll be bugs. Or at least questions that people have about how to work with the data. So the original RxRx1 release was done as a competition we hosted on Kaggle and had a lot of back and forth there. For more recent releases, we created a GitHub repo with information. And we've been taking questions and bug requests through the GitHub issues interface. And now I said that I was gonna talk about how we go about releasing datasets. Licensing terms can be really contentious in industry, because there are commercial interests in play. And so it's important to separate out the goals you have for each dataset. For RxRx1 and RxRx2, we licensed those under Creative Commons licenses that only allowed non commercial usage. Our interest in RxRx1 wasn't demonstrating the power of morphological profiling. And sort of convincing the community that yeah, there's a ton of information you can get out of this data. And our data that we generate, recursion is high quality. And in RxRx2, we wanna further demonstrate the power of recursions platform and what you can get out of this. So our interest here is in driving academic work, and driving more learning from this data and reanalysis without compromising the commercial IP interests that we have. For RxRx19, I'm proud to say we have explicitly disclaimed any commercial interest in this data. There are patents that we have filed. We've explicitly said that these are purely defensive just to make sure that a bad actor can't lock up any potential drugs coming from this. And we've licensed them CC-BY. in order to drive broad sharing and collaboration, just make sure that you cite us, but we don't care what you do after that. It's in everybody's interest to make sure that this pandemic is solved as quickly as possible. And finally, another consideration for us was that we wanted to make sure that people could interact with the data. We're talking about a data set that's almost 900 gigs in size and has 400,000 images that's dealing with 1,500 to 2,000 different small molecules. This is not the kind of data set that people can casually poke around in on their computer if they're just downloading the raw data. But I think it's really interesting to enable people to be able to do that kind of interactive exploration. So along with our preference, we also released an interactive server at covid19.rxrx.ai, allowing people to interact and play with that data. And I'll show a little demo of that here. So you just open up your browser, go to covid19.rxrx.ai. It loads up this web app that's that's built in this really nice open source framework called Dash. You can select the screen that we did on human cells or the ones on the theory, six monkey cells, and then you can pick the compounds that you want to see. And what you get is this cool two dimensional plot called the Prometheus plot showing the results of the screen. In the cytokine storm model on the left and two repeats of the active viral infection in the middle and on the right. You'll also get a table of the hit scores as well as the compound structures that you selected. That is then easy to go ahead and download the results to look at as you will. And I think this server has been really powerful for enabling people to understand what you can do with the data and what is actually there. So with that, what's happened now that we released this data? We put out our first release for RxRx19a in the first half of the year. RxRx19b in August, and we've actually seen some really interesting traction from it. So like I said, we published RxRx19 on rxrx.ai, self hosted, did all of that. But since then, because COVID-19 is such a global issue, we've been approached by a number of groups in order to share that data elsewhere as well. So I'm proud to say that our results are part of the ChEMBL COVID data portal. So you can actually go into ChEMBL and they've put up this great interface, so you can see all the structures, hit scores and so on, all in a unified place. The NIH and NCATS have an open data initiative that has also asked us to host a microscopy image. And Google also has a public datasets program. And in particular, a COVID-19 related public datasets program. That I'm proud to say they are now hosting that data on their end, as well, in the public interest. Beyond that, I think it's been really useful in terms of communications. What I have here is a screenshot of a brief Twitter exchange that I had. And it's a very reasonable question. Hey, are you able to share what's been going on? It's really liberating to be able to say, of course, I can share it. In fact, here's the URL, just look at it yourself. All of our data is up there. What's surprising is that not only is this useful in public communications, it's even useful in internal ones, right? I had a conversation with a couple of our scientists a month ago where they asked us, hey, what's going on with this particular molecule? Hey, you can just look at this website, right? It's really easy to look this up. There are no complications or anything to remember. And I find is really nice for hypothesis generation and exploration by a wide variety of people. And finally, I think something that's been really cool for us is we're actually using RxRx19a as a recruiting tool. One of the work sample tests to be used in order to evaluate new data scientists, is to send them RxRx19a and ask a series of questions. It gives folks an opportunity to see our capabilities and see how cool our data is. And for us to see what interesting things people can do with this dataset. So with that, I've said it a few times, just want to remind everyone RxRx19 is available. It's a huge data set of images, metadata, deep learning embeddings and an interactive server present at rxrx.ai. If you're interested in drug screening work for COVID-19, either the virus or a cytokine storm that probably ends up causing most of the mortality later on. I encourage you to check it out. And just in conclusion, I've been really excited to work on this project. I think that generally this kinds of large datasets and making them available to the community is super valuable. And being able to host some on the cloud has really made that accessible to a much a broader range of folks than what we've been able to do in the past. So thanks so much, happy to take any questions either here or in the Slack or in the town after the session.  All right, thank you very much for just another amazing talk. That was great. I think we have time for maybe one or two questions either for Imran or Lucy slash Kyle, a direct messaging to me in the intervening time. I'll just go ahead and ask Imran a question. I apologize. It's kind of a hackish question. But it seems to me that it would be quite useful to have single cell transcriptome data for the cells in question, any plans on doing that?  Well, so there's obviously limited detail that I can comment on regarding our plans. What I will say is we do have a couple of openings for folks working on next generation sequencing based methods. Both biologists as well as computational biologists and bioinformaticians. So A, that should give you some ideas of directions in which we might head. And B if you're interested, please come apply. What I will say is that something that I didn't talk about on the assay slide. Even though single cell sequencing has gotten cheap, right? A really, really cheap assay, you might be looking at a buck of cell or something like that. Morphological profiling assays are even cheaper, right? You're talking literally one to two logs cheaper than even the cheapest single-cell sequencing. So our focus, for scale, has really been focusing on the morphological profiling side of things. Some work done out of the lab of one of our scientific advisors, Ann Carpenter, actually showed that the number of perturbations that elicit a transcriptional response, that elicit an imaging-based phenotypic response is fairly comparable. So although there are certainly some interesting things that we think we could get out of the single-cell RNA on this, that's probably not something we're gonna do for this particular data set. But if you're interested in working on that, please come join us.  Awesome, so moving on, thank you, we have Yubin Wang, and I will sit, or Yubin Kim, Michael-  [LAUGH]  One thing I've noticed, quickly, is that if you, my thesis is is sitting on a shelf somewhere, and nobody will ever find it. Hers, she is clearly very proud of, cuz it is the easiest-to-access doctoral thesis I have ever seen. So, but also people ask me every day about clinical notes and parsing clinical notes. So I am excited about this talk and will be paying rapt attention, I hope you are too. So with that, Yubin do you wanna share your screen, and we will-  Yes, let me get this go, here we go, can everybody see my slides?  Absolutely, yeah, I mean-  Okay, let me see if I can-  You might wanna hit present, but-  Right, yeah, there you go, did that work?  Yes.  Perfect, great, thank you for the introduction, my name is Yubin Kim, I'm Director of Technology at UPMC Enterprises. Just a little bit about me, I graduated from CMU, in the Language Technologies Institute, back in 2018. My thesis was in large-scale distributed search systems. But since I've graduated, I've been with UPMC, working on clinical notes and in the medical domain. In my current role, I work with stakeholders in both the provider and peer side of UPMC to identify problems could be solved using machine learning and NLP. One of the reasons why I joined UPMC was because I was interested in working within an area where I could have access to real-world healthcare data. And I was certainly surprised, what that looked like in the real world, when I first joined UPMC. So in this talk, I wanted to show you guys what EMRs, especially clinical notes, look like in real life, especially in a big system like UPMC. So if you haven't heard about UPMC Enterprises, UPMC Enterprises is the technology and R&D development arm of UPMC. So in Enterprises, we have an in-house R&D team, as well as a side of the house that works with startups and researchers at both U Pitt at CMU, developing new technologies that will solve healthcare problems in the real world. One of the things that we have developed at Enterprises is called Neutrino, which is a big document management engine. The thing about Neutrino is that it is a storage repository for clinical notes, all clinical notes that goes through any UPMC facility at any time the past decade. So this includes progress notes, discharge summaries, radiology report, pathology reports. Any clinical note that is created within the UPMC-wide, it is stored in Neutrino. And furthermore, we have agreements with other hospital systems like Butler and Lehigh Valley, and we have those notes as well, when the patients going through those hospitals are covered by the UPMC health plan. So we get something like 700,000 new documents each week, across 35 different sources. And this includes EMR vendors of all stripes and colors. I don't even know half the EMRs are listed on my slide, to be completely honest with you. But this is to illustrate that we have a very wide variety of sources and clinical notes. So long story short, it's a lot of data. And I'm going to show you what it looks like a little bit, and the news is that it's not good. So in the real world, we have an array of different issues in clinical notes. Some of the most well-known issues in clinical notes include ambiguity. So the word cold can mean several different things, including the temperature, it could be a viral infection. And it is also a shorthand for chronic obstructive lung disease, which I did not know about until I looked this up. Abbreviations are very common. NKA, no known allergies, FROM, full range of motion. And astute observers will notice that from is usually considered a stop word when you're doing NLP, these are words that you typically strip out. So you can already see issues that occur in clinical notes. Clinical notes also include a lot of synonymy. Heart attacks and myocardial infractions often refer to very similar concepts. And of course, misspellings and especially dictation errors turn up a lot in clinical notes. Dictation software is fairly frequently used, and the errors from those software are not usually corrected by the physician. So these all tend to make it into the EMR. So these are some of the most common, well-known issues. But today, I wanted to introduce you to some that are maybe less frequently known. So one issues that I've noticed in the clinical notes repositories is that physicians are busy, clinicians are busy. They often take shortcuts in writing out clinical notes, and they often make up abbreviations. So this is a fairly common abbreviation that I've seen, appy, stands for appendectomy. But this is not the only way this abbreviation is used. So for example, it could be appy as in the appendectomy procedure, or appy as in appendicitis, the actual disease, or appy as in the actual organ, appendix. So this made-up abbreviation is used in many ambiguous ways by clinicians who are documenting these things. And of course, it could also be a misspelling of apply, just add insult to injury. Another common issue in clinical notes is this aspect of question and answers that usually get written down into full texts. So in a lot of reports, you see basically multiple choice questions. So in the EMR, you're supposed to select the histological type. For your tumor, and so they give you four options and you are supposed to select one particular option. So this can appear in many different ways as you can see on the slide so one particular way that you can select things is by running things down. Another way is to select a number, another way is to have these little check boxes that are checked off. And so of course, if you're parsing this with NLP, trying to determine what the histologic type of this actual tumour is? You want to say that is a superficial spreading type tumour and picking up these other terms that are present in the note. But are not actually representative of the paper what the patient is experiencing would be inaccurate. So a naive approach to NLP would be picking up all these terms where in reality is a superficial spreading Type tumor. Another type of question and answer. A social history notes contain a lot of these things. And so common example would be your friends influence you to use alcohol, tobacco, illicit drugs. And you'd get these free form answers that are yes to alcohol and drugs. And you would need to understand that it is not just that the patient uses alcohol on drugs but it is the influence of alcohol and drugs for friends. And it may not necessarily be that the patient actually uses up on drugs. So there are a lot of refinements in And a lot of ambiguities and nuances in clinical notes that you need to be careful of. There are common issues such as boundary detection, it's often first step for many MLP tasks. Even in the world of neural network boundary detection is often the first step to chopping up sentences to feed into your computer machine Learning Network. And you think that would be easy, but often it is not. People are not necessarily the best at using periods or punctuation, even. In boundaries where it would be obvious. And this is one particular example that I pulled from clinical notes that has been de identified and changed slightly to protect patient privacy. But this is an example of a real sentence that you might see in clinical notes. I'm not even going to try to read this. I'm probably going to trip over all the medical terms. But this is a very common example of a sentence and so being able to do proper sentence but boundary detection is a real issue as well. This is a fun thing tables. So tables are terrible. If anybody who's worked with text data, I know hates table parsing, table parsing is difficult. It is not easy and you think that it would be easy but it's hard to get things to work properly across lots of different types of notes. So this is a easy example of tables. So these are vital signs of patient. You can see that there are two headings and the actual value. But then there are these whole like this normal ranges of vital signs that are also in the table. So you need to know how to parse those out and not include that as a part of a patient's medical history. But even more exciting are fish bone diagrams. So, if you don't know what fish bone diagrams are. These are basically shorthands that are often used in clinical settings to visually indicate a patient's vital signs. Or they're important calcium levels and things like that in a way that is standardized within a specific discipline. Similar fish bone diagrams can mean different things across the different medical disciplines. But it is a visual way for clinicians to be able to see the thing and say okay, this patient is healthy, this patient is not. And so when it gets turned into a textual format, this is what it tends to look like. It's a giant mess and what is worse? These are often mixes of spaces and tabs. And so when you parse these things out, you need to know exactly how wide the tabs are in terms of spaces. And in this particular example, I figured out that the PABX is Penn spaces wide. But when I use them, my tab settings are two spaces. So then it looks ends up looking like that and it's impossible to parse tables. Finally, I want to go over the titular issue that that my talk was titled around note types. So in neutrino in our document repository, we have close to 40,000 different note types. And the biggest source of different note types come from radiology, each type of radiology image generates a different type of note in the EMR system. But that still leaves us with 10,000 different node types that we need to parse through. And the reason why it is important to have an understanding of different note types that exists in the EMR. Is because sir, first of all certain note types are sensitive, so behavioral notes from psychiatric hospitals need to be treated at a higher level of confidentiality. So not all medical staff are allowed to see these types of notes. You need a higher level of clearance to see these types of notes, similar with HIV status. And secondly, possibly more relevant to us as researchers, new types are also very diverse. Training on one type of note type does not guarantee performance on another. And it can be easy to fall into a trap of saying, I trained this model and the system and it has. Breed F1 scores, but in reality when applied to different node types in my might not be the case at all. Another difficulty around no types in particular is that there's currently no clear way to organize your math. He's no type still clean Harkey. So there is a loinc document ontology that exists, but it's in a beta version, oit's incomplete. And as far as I know, there hasn't been a lot of active work on developing out this ontology. So before Basically you're flying blind and you're relying on two subject matter experts of your institution to help you try to figure out which note types belong together and which don't. Within UPMC enterprises. We had an initiative to try to map out new types into a standardized format, very manual and very, very time consuming, and it was done by a non-subject matter expert. So The quality varies as well. I wanted to because this is such a big problem, I wanted to give you a little bit more of an in depth understanding of what this really looks like. So I did some data exploratory things on a particular data set called that was generated for the CARE project of PHDA. So a quick sidebar, PHDA, the Pittsburgh Health Data Alliance, is UPMC's data sharing initiative. So if you have a collaborator at Carnegie Mellon University or University of Pittsburgh, if you have a PI for collaboration, You can apply to work on a project that is funded by the PHDA, and this is a two-fold type funding situation. Not only do you get access to actual money, actual dollars to fund your I have find your PhD students to do actual research with, you also get access to clinical information that UPMC has to be able to do your research on. So for this particular project, it was for disambiguating a bit abbreviations in clinical notes we identified 5 million different patient notes, including 12,000 note types across ten different sources. And again, radiology accounts for most of the new types but it still leaves a substantial number of new types that That need to be managed. So the analysis that are subsequently done will be on this particular data set, so, as mentioned before, the document distribution of new types are highly skewed. So this really long pale of lots of different note types that have very few documents are mostly from radiology and about 343 new types from the PHDA data set contained 90% of the documents. Some of the most populated types were patient call logs and office visits. And this is a particularly interesting new type because it contains Very wide variety of different types of notes, which I'll talk about a little bit more later. To have a look into what the note types look like, I took the section headers of the clinical notes and parsed them out using the SecTag vocabulary. This was based on a paper that was published in AIMA, and you can see that across these three different note types here, this is a nutrition assessment note type. Whether or not patient is eating wall if a patient has malnutrition they generally go to talk to a nutritionist and this type of notice generated this is a radiology note I believe for CT scan of the head. And these are easy 70s these are call logs from epic, and you can see that the section headers that are present are very, very different, the frequencies of the sections are also very, very different as well. To look at these in a graphical format, on the X axis is the different types of sections that were ordered by frequency rank and the Y axis is of a specific note type. What percentage of those notes contain that particular section? So you can see this graph will kind of give you a sense of how varied notes are within a single type, so for example, EC70 call logs and the CT head scan radiology notes. There are a handful of sections that are present in almost all of the notes and then it has a rapid drop off so from that you might be able to conclude that these notes have a fairly specific format. Whereas for these nutrition assessment notes, you're seeing a lot more different sections and a lot more variety on how frequently these sections appear. And this is just a handful of sections, a handful of note types that he pulled out to graph out what the sections look like. And you can see there's a wide variety on how are coherent a new type is within its own type. To give you another view of heterogeneity within a no type, these are both plastic surgery, post op notes and they look fairly different, again, these have been de identified and changed to protect patient privacy. But you can see that the length is very different, the kind of information in it is very different, the section headers are very different. So, even within a single node type, you have a lot of variety and across note types it can be apples and oranges different. To give you a sense of how important this is, we did an experiment where we were doing some entity detection and trying to detect generic mentions of entities within us, within clinical notes. And we pull we use a state of the art CRF based model off of a paper And we trained this model on just 1000 notes of 600 different types, and we pass it this on two different sets. So set one was a testing set that contains note types that were note types that were mostly seen in the test set, and here, you get not great numbers, but okay 0.6 precision-recall is kind of low. Okay F1 numbers and in a separate test set and test two, we train We tested the test the model on no types are wholly unseen and you can see that there is a really sharp drop off in performance like this is like night and day difference in performance. These are like fairly abysmal numbers, which is one of the reasons why new types are, so getting an understanding and making sure that you have different node types in your training data is so important. I wanted to quickly go over one possible solution that you can use to try to tackle this particular problem, this is based on a paper that an intern of mine and I published in a workshop called health search and data mining. At wisdom 2020, and here what we did was we used gaming clusterings to cluster different node types based on textual and section header based similarities. And we saw that These clusters often aligned with different source systems and a little bit with the manually labeled typestyle we had, so we are semi confident that these clusters are okay. And based on these clusters, we kinda repeated the experiment where we were looking at, Training a NLP model on the different clusters based on we're training the NLP model on these different clusters within the same cluster type. And these clusters each contain lots of different new types, and we saw that we had some mixed results, but generally speaking when the model was trained on the same machine. When the model was trained within the same type, within the same cluster, we saw that the training and testing performance were similar. And in the note in the type in the clusters where we saw poor performance, we investigated these and saw that these clusters are actually less coherent. So we did a correlation between the silhouette coefficient of the clusters and their test scores and we saw that there was a strong correlation. When the clusters are very tight and coherent, we saw that that the test scores correspondingly were very high. So I rushed through last bit quite a bit, but that is kind of what I wanted to convey clinical notes in the real world are complicated, and there are a lot of challenges to tackle a lot of low hanging fruit to tackle. Please come talk to us about the PhD program, please work with us on helping solve these problems. That was fantastic. Thank you so much. We're running a bit behind time and I think if we extend into the break session, we wanna do that mostly with questions. So we're gonna quickly transition over to Jeremy Weiss. Jeremy is going to talk about actually taking that EMR data and bringing it back into clinical rounds. If you look at Jeremy's webpages one thing you see very quickly and a really salient piece of information is that he is looking for grad students and postdocs, like what you have what he used to say here, please contact it. With that, Jeremy if you wanna share your screen, we will get going and we'll have a little Q and A session for everybody after.  Great. Thanks very much, Ben. Can you see my screen?  Absolutely  Great. Let's try to get this up and running okay, sure. So my name is Jeremy Weiss. And I'm an assistant professor in the Heinz college here at Carnegie Mellon. And today I wanna talk about one reuse of data which is that of structured electronic health records. So I won't be focusing on notes like the last talk. But on the structure component and thinking about how we could integrate and bring them to clinical rounds using machine learning techniques. So, I'd like you to put on your doctor hats, get on your doctor shoes don the white coat, and off we go. So this is a representation that we might pour over during rounds. We've got one individual. This is one patient's data across time, and there are many different clinical events being represented on the y-axis. So this is actually from minick. It's a de-identified data set so the time is kind of scrambled. But nonetheless the unit of time here is one day. And then on the y axis, you might think you have maybe a glucose value, you might have vital signs, procedures, other other clinical events that are documented over time and that's what's going to populate the health record for an individual. And so now we're here at this blue vertical line, and we need to make decisions. What should we do next? How do we best take care of this individual? Okay, so you're in your white coat. So if you think well, let's go talk to the patient and let's collect or elicit symptoms and let's perform a physical exam. Assess that information, propose some interventions. And then once you've decided I think these are probably likely what I should do then we'll go back to discuss the patient. Obtain consent and act. And you'll do this for, let's say, a dozen patients every day. And you'll do this every evening, somewhere between two to six hours, maybe two to 12 hours. At the end of the day, you might go write your notes. But for now, what you'll have access to our previous day's notes and a lot of other information and measurements in structured form in the health record. And that's what you're going to be working from. Okay, so what if you don't know what to do. When you got a COVID positive patient they have chronic obstructive lung disease, COPD, pulmonary disease and here well, you're not sure so maybe go consult the specialist. That's probably the most common thing that you should do. But you might also go back to the literature you might go to review guidelines such as up to date. Or you might go back and look at randomized control trials or observational studies. And this will help you deal with your clinical equipoise. Equipoise is a balance of interests. You're not sure quite what to do. There's a fourth thing that you could do, which is use a predictive model. Now, many predictive models are used in the formulation or in constructing guidelines or scientific evidence thesis. And to some degree there are some embedded in electronic health records that help guide the decision making process so that you can deal with your clinical equipoise. Now I wanna talk about how do we bring this into the timeframe of actually treating a patient. So you could take many risks for there are many out there and you could apply them to the patient. And those risk scores would come from other sources from large datasets or studies that have been done previously. But you're not really sure necessarily when you approach an individual patient what your question will be. And so there may not be any sort of answer that you can draw quickly from the existing resources. So that's why you might turn to a predictive model that's built in within your system. Okay, so here's that picture again. I'm gonna focus on forecasting models from this information because we're trying to assess risk over time because that may influence what we end up doing. And the way that we treat this individual. Okay, so what are forecasting models typically used for? Well, they can quantify risk, risk of an outcome we'd like to avoid. They can assess the urgency, how quickly are those outcomes that we want to avoid? How quickly are they going to occur? And then does this individual belong to some subgroup that has particular characteristics where we know something about those subgroups. And we can say because they're in the subgroup we should act in this or that way. Now, there's good and bad with taking such an approach of taking electronic health records data and building a model right at the point of care. On the good side is, there's clearly a lot of data that's present. And because there's a strong belief that there's high quality content there. Because right now physicians and clinicians, when they're looking at the health record, they're using the health record, not only to document but also to make decisions because there's such a useful information source right there. On the downside is that this state is quite messy. We just saw an example of the messiness of notes. But structured data is also messy in a lot of ways. So, in particular, electronic health records are kind of like digital results the passive collection data collection. You don't get to control what is measured, at least if you're not in total control of the system. Likely you're not in control of the entire system, you have the ability to control some small aspect of it. And of course, always with patient consent. So what's the status quo here? Our goal is to use this data as a complimentary evidence source. And currently it takes months to years to conduct an analysis, publish it and then determine whether this published model will apply to the patient at hand. But at this time scale, we're trying to do this with not even having the question in hand until we see the patient present. And then be able to say something during the course of that encounter. So the timescale is really a big challenge. Okay, so can we do better? So I'm gonna continue through here. And one of the things that the machine learning for healthcare community is adjusting is forecasting models. And how do we, disseminate them, how do we apply them so that they can be useful? So, with the growth of machine learning, we've seen a growth in the machine learning for healthcare subfield. And now we have our own slew of conferences where we can communicate within this particular subfield. So that we're ensuring that all the things that we produce. Are really relevant for healthcare. And within this sub area, there are a number of themes. So, we want for high performing and generalizable models and that's a good emphasis and that continuing emphasis. We've also seen an outgrowth of machine learning to achieve a particular property. There are whole fields devoted to fairness interpretability robustness as a few examples. And there's also been a push into pipelines. This idea that we want to have deal to take in relatively unstructured data and develop predictive models. That are actually useful to the end users potentially like clinicians or patients themselves. And so I'll focus on this last one. And I'll focus on particular forecasting within this last one. So of the end to end models that are in development or have been published, they oftentimes look a lot like this. Maybe there'll be like a Docker file or they'll be here's a script and. You're gonna have to enter 100 different arguments that will characterize the parameters of what you want your algorithm to do. And you have to do this all up front. So here's an example. So this is from Mahela Vander shares group. It's the clairvoyance tool and this is the type of call that she would make to basically had to produce a predictive model from. Some slightly processed data. And it's potentially hard to know how you would want to choose make all of these design choices up front. And so there's a possibility that maybe we should have this in a step-by-step process and that's something that my lab has started to develop. So instead, what you might consider is using a visualization tool. And so we built one it's called t LA. And in this case because it's an interactive process. You can investigate data, data details, issues with the data, correct them and then keep going through the machine learning processing pipeline. Okay, and because our desired end users or people in the health and health spaces. We want this to be a little bit like an electronic health record. So it should feel a little bit familiar to the user. So here's a schematic of this machine learning pipeline. There, fundamentally four different panels. A cohort extraction panel, a timeline representation panel, a modelling panel, and an assessment panel. And because we want it to feel like an electronic health record. We want to be able to show everything at the level of the individual. But we also need to be able to do the processing. And we divide that processing into in memory, approximate computation. And then slower processing where we're going to be querying a database or pulling data from disk. And so to be interactive, we can't use all the data we just have to be able to look at subsets so that we get enough food. There are a number of design choices that are built into this. So again, I've mentioned it should feel like an electronic health record. We want it to be reactive, but we also want it to be representative of the entire cohort. So we think a lot about how we can use visualization for machine learning and machine learning for visualization. Then aside from that there are also these checklists that are common in healthcare but are moving into this predictive or prognostication space. And we align ourselves with these. So let's take a look at the tool. So I've covered up I think I'm sharing my screen so you should be able to see it. So here is one patient. And I'd like to show you what this tool allows you to do. I'll show you two versions, two parts. There's the cohort selection part and then the assessment part that I'll just very briefly highlight. So here's your data for one patient across time. And maybe we're interested in mental status. So you might be interested in something called the GCs score, you might consider it as a potential outcome. And if we were to add that as an outcome, it will be highlighted in the data. And so here now we can see that GCS is being measured repeatedly. So let's go ahead and talk about actually just decreases in mental sense, which would be GCS. So we can do is annotate this. The type GCS. We find GCS and say that's smaller than or equal to eight. That's a low score. We add it. That will annotate the system and it's going to take a minute and then it will update. And then, all we have to do is add back our new annotation. And what we see is that we've actually identified a decreased mental status crime relatively early in this patient's clinical trajectory. If you wanted to verify that, you could go look at the values of those GCS scores. And in fact, you would see that there was a transient decrease for this patient at that particular time. So, now maybe you want to select some features, because you want to do some modeling. So let's just select these features and get a sense of what kinds of features are being represented here. These are primarily relevance. I'll go down here and we'll add some features. And you see that they're represented, now they're going to be features, and we're gonna be predicting this orange as an outcome. And there's a whole bunch of other things you can do here. I'm going to put in some windowing. Which is to say, I want to only focus on the time period during which they're inside the intensive care unit. And so now there's a line down here at the bottom. Indicating that. Okay, so when you're done with all this processing. You probably do a slower and more refined version. You would come down here, click a version button, and then that would process the data and create a refined data set for you up here. Okay, now, you do this through the rest of the time. But I just want to show you the end result of something that she might get. So over here in the assessment panel. I've loaded to a basically a representation object kind of modelling object. And now we're going to be able to see that we have a survival analysis for these individuals. This particular one is looking at low platelet counts, onset of severe thrombocytopenia. But that's not too relevant for demonstration of the tool. So here we can see we have survival curves over time. So over the course of 20 days, a high risk group, the one in purple has relatively lower survival 70% survival free from having super low platelets and that's based on a number of different features. If you want concordance plots you wanna see are the predictions of the actual rates, accurate, you can plot them here. And there are a bunch of different toggles that lets you choose different settings. So I wanted 10 groups, and we'll take a minute and process and then update with the results. So that's pretty nice. And then here, if you're interested in sort of like associations between the features that were included in the model and the outcome here. You get a forest plot, and this is something that you can with. So all of this can be done in relatively quickly. And so is a nice interactive tool that where you can provide this to health, health care professionals. Who understand the intricacies of the health process, but may not necessarily have all the details and how to code all of these different little design choices throughout the process. Okay, so that was a whirlwind tour of visualization tool. And what I've shown here is really just kind of the classic setup of doing survival analysis using a classic method called glmnet Cox. A big part of what my lab does is focused on developing the methods that are temporal analysis from electronic health records data, and we have created a suite of methods here. And I've listed a bunch of them down here. A little over time, so okay, so first I'll highlight. If you wanna play around with these visualization tools, you can click on this link. Or you can go to my website. I see a new website and it'll be a link on my research web page to get the full more complete access. You have to use these credentials demo and reversed. Okay, so just to highlight one type of kind of other research that I do within this temporal framing, one thing that I'm interested in is understanding risk as opposed to classification, even though they're kind of two sides of the same coin. So most classification algorithms are trying to divide, let's say, black from orange points in this graph. But oftentimes in healthcare we're interested in risk of, every event, not just the ones that are close to this decision boundary. And unfortunately, most classification diagrams will focus They're gonna focus on the decision boundary, because that's exactly where there is greatest uncertainty, highest entropy. But in health care, oftentimes you wanna know what is the actual risk of the low risk individuals. And so what this particular method does is, identifies how to appropriately focus on all individuals throughout the plot. But in a temporal thing. So instead of looking at classification, we're looking at risks across time. And we're thinking about rates or of other events. And so, if you think that this black point is relatively low risk of being classified as yellow, if that's the outcome you want to avoid But what is that risk? And so that's what this means certain methods will do. So practically what does that mean? I'm just gonna jump straight to some results. If you're to train on an LSTM or deep learning model and you wanted to try two objective functions one which is standard, a maximum likelihood approach Or the second one, a harmonic mean type of approach. What we're going to see or what we see in our experiments is that the harmonic mean approach will straighten out the tail. And so this is the concordance plot restreaming at the tail, so we're getting better predictions on our low risk individuals. This is done in simulation, the effects are strong. You see what just to some degree in the real datasets so here I'm again pulling from mimic three, I'm looking at decreases in mental status or Glasgow Coma score. And so here I'm showing the maximum likelihood versus harmonic mean processes in the calibration plot. First of all. The subgroups identified from the maximum likelihood version aren't as low in terms of their empirical rates or hazards as the ones that are identified in our sub grouping. And then you're saying well, maybe the orange points will look a little bit further away from the diagonal line, which is what you want. But in fact, I've plotted a log log scale here so that you actually look at the absolute differences between the lowest two risk groups. The absolute difference is actually smaller for the orange line method as compared to the maximum likelihood. So this allows you to identify You know, what is the risk and people who are not really at super high risk, oftentimes you might think of identifying risk factors that are applicable to the whole population. But if you all you're using our algorithms that are focusing on high risk individuals, oftentimes, you won't get to see the associated risk factors for the broader population. Okay, so in summary, I've shown two different tools that my lab has produced. So first is. The goal is to make this, you know, much simpler for healthcare professionals to produce analyses from, let's say, months into minutes may be a bit optimistic that missed hours. Today's And this can help them address their clinical levels. The second is that risk estimation is really important and maybe, for forecasting, at least within healthcare may be more important than classification tasks. So we've illustrated one method. If you want to look at General populations or in particular low risk populations, here's a method that you might might think about. These so, these built together to look at risk estimation, that it's useful for individuals who are taking care of patients, these health care professionals and this is something that we're pushing at the community. So thank you for your attention. This is joint work with a whole bunch of people listed up here. Thank you.  Right, thank you and we are way over time, but that second part was totally worth it. Like, I mean, really, when he started, I was like, and then I was like, This is awesome. We're just gonna wait a minute. But that said, we have eight minutes till the next session, and we're gonna try to have a brief q&a for all of the speakers. That's great. My question was where to get this. If you wanna pop that slide back up that had the demo? That would be awesome. I'd love to see that real quick. I got questions for a few of the speakers. First, you've been and I think a lot of clinicians in the audience know the answer to this or never want to hear the answer to this has UPMC tried to any behavior modification with Doc's to help with those for example, asking them not to use abbreviations. Dear God, That's hard. That's That's hard. Yes.  Okay, yeah. I thought that this is question for everyone but anyone could answer. How do you handle human subjects considerations such as removal of personal identifiers, both direct and indirect for extremely large datasets, something that probably everybody in the room deals with a little bit. I can take this one. So within UPMC we have a automated tool that goes through text and tries to rip out names, the identifiers that are defined us phsi by HIPAA, it's difficult in text. It's not, you can't do it 100% But we do the best we can. And this is one of the reasons why getting clinical notes as shared data sources is so difficult is because there's no clear way to do this so that we will be 100% certain that all PGI stripped out. So it's definitely a big challenge especially for freeform text. Cool. Anybody else want to jump in on it? Yeah, I mean, it's it's kinda madness. But yeah, it has to happen. And yeah, just encouraging people to look things up in terms of collaboration. And hopefully, neutrino and other systems will start working in a wider way with other health systems. Anybody else? Any other questions. Alright. Well with that, I'd like to thank all the speakers. Thank you so much. We have about five minute break. Obviously move the vast majority of you are using laptops. So please take that word anywhere you need to go. And yeah we'll reconvene in five minutes with a panel led by Sean Davis. So, we will see you back in five minutes. Thanks again to all the speakers, and that's it for this session. 