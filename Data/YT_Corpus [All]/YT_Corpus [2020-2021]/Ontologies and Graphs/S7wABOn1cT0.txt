 Okay, welcome everybody to the University of  Waterloo's distinguished lecture series. Today our   speakers Oren Etzioni. So, ever since I was a graduate  student myself, over two decades ago, I have looked   up to Oren. So I am particularly delighted to  have the opportunity to introduce him today. So an   exhaustive enumeration of all his accolades would  take far too long, so here I only hit on some of   the highlights. So Oren has been a professor at the  University of Washington, the other UW since 1991   and since 2014 he has served as the founding CEO  of the Allen Institute for Artificial Intelligence   whose mission it is to contribute to humanity  through high-impact AI research and engineering. So   Oren is an accomplished researcher, most recently  a pioneer in machine reading and open information   extraction, today what we call knowledge graph  construction. He's also a successful entrepreneur   translating his research into several startups,  two of which were later sold to Microsoft and   eBay. Oren received numerous awards including my  favourite, being named Seattle's geek of the year   in 2013. He's also a fellow of the Triple  AI. So, without further ado Oren's going to   talk to us about symantec scholar nlp and  the fight against COVID. Please take it away. uh thank you Jimmy. I appreciate the opportunity  and uh your your kind words uh of course. If uh you were a grad student two decades ago, you  people can do the math, I was at a grad student   even even longer ago but let's not dwell on  uh on ancient history. I was really looking   forward to uh visiting the other UW uh in Canada and of course we can't do that so   but I do hope to come visit you in person uh when  when possible and with that let's get to my talk. I'm going to start with a very broad uh  an important question, is AI good or evil, uh and I'll talk about some of the debate around  that. I'll then tell you a little bit about the   Allen Institute for AI, or AI2 as we call ourselves. I'll delve into semantic scholar, in particular   our scientific search engine which is quite  unique and quite popular. I'll then describe how   our abilities uh in semantic scholar led to the  construction of the COVID Open Research Dataset, CORD-19, and iI'll end with some speculations about  AI and science in the future. To make this a little   bit more interactive than usual uh I will try to  uh pause for questions at least a couple of times   during the talk at different sections. Hopefully  uh this will work for us and again if the gods   of the internet are kind I'll also be able to  show you a couple of uh fun demos. So let's do it. The first thing to realize is that for many people  AI evokes fear and this isn't just uh I don't know   the uh unwashed masses people who are ignorant uh  it's uh plenty of people who arguably should know   better. So we have folks like Elon Musk saying  with AI we're summoning the demon religious   uh imagery talking about how AI uh could  destroy the human word, the human race,   all of humanity and we have folks like bill gates  and um others who have uh uh made made similar   statements and you might say okay, well these  people don't really know about AI, but we also   have folks like um Stuart Russell at Berkeley who  is an AI researcher, long-standing AI researcher   and he argues some of the same points. On the  other side of this discussion we have folks like   famous roboticist Rob Brooks who says if you're  worried about the terminator just keep the   door closed, AI is not nearly as advanced as  some people think and of course through the   the success of reinforcement learning this robot  will eventually figure out how to open the door   only to find out that there's a staircase behind  it and our robots struggle to uh start uh climb   staircases and we'll solve that only to find out  that there's an another challenge and yet another   challenge. The truth is that our AI systems are  these uh savants, they're very good given training   data at solving narrow tasks but they're not very  strong uh in general. And so Stanford's Andrew Ng   says working to prevent AI from turning evil is  a little bit like disrupting the space program to   prevent overpopulation on Mars. It ignores both the  technical difficulties, we haven't gotten anyone   on Mars yet, it's really quite a challenge  and it also ignores the potential benefits,   the ways things are going with climate change and  political upheaval, some of us can't wait to get   get to Mars uh but but my point is that this is  really um uh a subject of uh intense debate in uh   in various quarters. It's a question that goes all  the way back to uh the Golem and Frankenstein and   not only is it not going away but it seems that  the progress we have been making as a field   makes the question more and more uh acute in uh in  people's minds. So I want to be clear that when I'm   talking about the question of AI, good or evil, I'm not talking about issues like AI's effect   on privacy or the bias that's occasion by it or  even its economic effect in terms of jobs, these   are all very important questions, I'm really  talking about this very fundamental question   uh is AI going to destroy civilization as we know  it. Isn't it is it an existential risk for humanity   and I had a conversation with Max Tegmark who's  a thinker on this topic from MIT. He wrote a book   called Life 3.0 and I try to say, you know Max I feel like this discussion of AI annihilating the   human race is distracting us for more uh practical  pragmatic questions like well what are we going to   do about all the people are going to become  unemployed due to the advance of technology,   let's say truckers once ah self-driving cars are  rolled out and he said to me Oren I'm talking   about the fate of humanity and you're arguing with  me about what should we feed the kids for dinner. In other words he's saying look, those questions  are relevant but they're kind of mundane and short   term you got to think about the fundamental  questions and so as you can see we can   ban the intuitions and argue about this  philosophically really endlessly, I could   give a whole talk on the different arguments that  people make but earlier this year in an article   I wrote for MIT Technology Review I tried to put  this on a more empirical footing, I said you know   can we identify some tripwires, some um canaries  in the coal mine called these canaries in the coal   mines of AI, that if these canaries keel over  then we'll know okay human level intelligence   uh super intelligence is around the corner but if  we're if that doesn't happen then we really don't   need to worry about it and uh AI uh is much more  a technology like software. We continue to develop   it, improve it, make it more sophisticated but it's  not something that's going to take over the world.   So I believe that that is very much the case.  Marc Andreessen famously said that software is   eating the world and I would say the machine  learning is eating software where we're using   machine learning more and more across the  board but we have to remember as I think   all of you know who actually work actively  with machine learning it's 99% human work, right.   We have to uh define the inputs to the system,  uh what's the data set labeling the data,   what's the algorithm that's being used, what's  what's the concept uh that that we want to   learn about, and very importantly we have to assess  the output and if that's not good uh we go again,  right it's very much an iterative process and  building machine learning models in a funny way is   not unlike debugging. It's very much programming,  very much human work and the machine really does   the the last mile. Once you have all this  done the statistical computation of what   model best fits the data, yep the machine does  do that and so in a funny way machine learning   is really a misnomer and it's a misnomer  in a field full of misnomers right like   artificial intelligence and I like to say that um  saying that machines learn is really like saying   that baby penguins fish. What actually happens is  the parent penguin goes off into the water, corrals   fish, finds them, hopefully successfully catches  them, eats them and then presents the baby with   a regurgitated morsel right out of its  mouth, as you see in this little clip,  and then the baby eats that. So uh baby penguins  don't fish and machines don't really learn,   so with that in mind we're ready to define  our first AI canary. What if we had a machine   that really genuinely learned much more at  the level that a human does, so let's say we   have a machine that can automatically formulate  a machine learning problem basically do   all the pieces I described, the 99% of the human  work and formulate the machine learning problem   for itself, it would be part of what sometimes  called universal subgoaling. Let's say I'm working   on something and I need to realize hey, I really  should learn that PyTorch uh to do that so I subgoal on learning PyTorch and then I've learned  that and now I realize okay I want to learn   AllenNLP, which is a framework on top of that.  So I learned that and then I say well I really   need to understand how self-attention works and  so on and so on so we're constantly as humans   uh coming up with ideas, subgoaling on subtasks,  learning what we need to do and then returning to   the original task and that's a very uh complex and  involved process that's done independently. We can   get help from peers or from books or from teachers,  all of this is completely different from what   uh the the machines do. So the bottom line  here is that AI is a tool or a set of tools   and it's not a being or a creature and it's up  to us to employ this tool to benefit humanity   as opposed to worrying that some being will  take over the world and with this I will pause   uh before I move into more practical topics and  just see if people have any questions comments or   reactions and I'll rely on Jimmy to uh to let  me know if something shows up in the chat etc.   Uh currently you seem to have universal  agreement, so why don't you just proceed.   Okay, you know I sometimes think gosh I'm I'm preaching to the choir uh when I say this,   particularly if you're hands-on but again there  are people in our field and uh Stuart Russell   is probably the most prominent example who who  really think otherwise, who think that we are   being complacent so I I think this point is uh is  worth making but focusing now on using this tool,   using AI for good. The late Paul Allen founded  a non-profit research institute in 2014   that I I run, as Jimmy mentioned, its mission is  AI for the common good and over the last uh six   years or so we've become leaders in research  on computer vision, natural language processing,   and deep learning. We've published um actually I should correct that more than 400 papers in uh   premier conferences, we've gotten a bunch of uh  best paper awards, uh and we have over 100 uh   PhDs, researchers and engineers, very robust  intern program, uh et cetera et cetera and we   are continuing to hire even in this time of COVID. So I should mention that we're located in Seattle, although uh we have a small office uh near UC, Irvine and another one in uh in Tel Aviv, Israel.   So there's a whole bunch of projects  at AI2, as we call ourselves and I'll   I'll just mention uh most of them briefly  in a sentence or two and then we'll delve   into SemanticScholar. So MOSAIC is one of our  most exciting futuristic projects, it's led by   Dr. Eugene Choi who shares her time between AI2 and  UW, and she's asking the question how do we endow   machines with common sense and uh that's  a question that for a long time uh Douglas   and his team at psych was the only one uh trying  to ask that question and he was asking it using   uh really technology from the 80s, knowledge  engineering, basically typing a large number   of bits by hand into the machine in a fairly heavy  logical language. That didn't go so well, Eugene and   her team are trying to do this using modern  techniques including machine learning and uh   crowdsourcing uh and so on and have been getting  some very good results including winning the uh   best paper award at AAAI uh this last year. That's  that's not a result that's a recognition of of   some of the work they've done uh again very very  futuristic stuff it's not even clear how you would   uh define or decide whether machine has has common  sense. The ARISTO project focuses on reasoning over   information obtained from uh from text. So often,  our our systems do rather simple things with   representations like Elmo and Bert but how do you  reason uh on top of that is a big focus for ARISTO.   The AllenNLP project is focused on modern NLP  research. It's led by Noah Smith who also shares   his time with UW. I should mention ARISTO is led  by Peter Clark who's a senior AI researcher and um   AllenNLP also has an open source  uh platform that's really uh very   handy if you want state-of-the-art  models to use as baselines for your   research and uh to iterate from there the the  prior team is our computer vision team it was   led uh by Ali Farhadi uh except that uh one of  the cool things you can do while you're at AI2 is   launch companies kind of like you know Stanford  launched Google and a lot of universities uh can   incubate companies, we make that particularly easy  and so Ali Farhadi launched a company called   Xnor and uh it bought it for uh apple acquired Xnor for a reported uh 200 million dollars or so,   so it was a great success for everybody.  Unfortunately, they also bought Ali   uh with it so he no longer runs the prior team  uh but uh the team continues to be uh very   successful uh under uh local leadership as it  were and I mentioned the incubator so let me   now turn and tell you more about uh SemanticScholar which is at the core of our talk today.   So SemanticScholar is a program that we developed  just about uh five years ago, we started working on   it and it attempts to address uh information  overload particularly for professors. So we   all suffer from information overloads right, you  know tweets, Facebook posts, emails, slack messages,   uh new items on Spotify, you name it we're just  inundated, well academics in addition to that   uh have to deal with eight million new papers  that are published every year. The number of   papers published is doubling every few years and  studies show that scientists only really read   on the order of 200 papers in a given year.  So clearly we need some good tools to help us   choose the right papers to read and that was the  motivation for SemanticScholar, we thought let's   use AI and information retrieval NLP and a host  of of related techniques to really hone in on   the key papers and results and help  you to cut through the clutter to find   what you need to do to be a successful  scientist. So um at this point five years   later the corpus the SemanticScholar contains on the order of 190 million papers   uh we have more than eight million users per month  which we're particularly proud of because uh as a   nonprofit we don't have a marketing budget so  it's very much word of mouth and it covers all   academic disciplines, excuse me and I'll I'll tell you a lot more about SemanticScholar now. Basically. what we do is we take the pdfs which  were our how papers are represented typically   and we map them to a literature graph where the  nodes in the graph are papers and individuals who   write the papers, individuals who cite each other,  and so on and so on. We provide search capability   using keywords and metadata similar to Google  scholar and other scientific search engines   hopefully uh somewhat better in various ways.  We uh have a different approach to ranking um   and we have paper detail pages so every paper  uh in every author have their own page and our   paper detail pages think of a wikipedia page for  but not for an individual or for a rock group   but for a paper so it contains not just the pdf  but also links to blog posts and news about the   paper, videos maybe of the person presenting the  paper, a slide deck, or or several slide decks   about the paper, links to a github repos where you  could have the associated code and so on and so   on so it's really meant to be uh the ultimate  destination if you want information about the   about the paper um in addition to that we've been  uh quite innovative uh in terms of thinking about   uh citation metrics. So first of all we don't just  count citations the way everybody does but we also   look at the number of citations per year, that's  the velocity of citations, and we look at the   derivative of that citation acceleration, that's  the change in the number of citations per year,   and I'll show you how that makes a difference in a  second but even going beyond raw citations we find   a more semantic notion of citations where we  exclude self-citations and we know by the way   that men tend to cite themselves 20 to 30 percent  more than women do so our notion of citations is   more gender-neutral, we also uh exclude uh  incidental citations you know the kind of   citation you add in the paper out of politeness  or completeness, oh yeah I should cite that but   your paper really doesn't build on it and then we  rank the citations by the degree of influence. If I have two papers our NLP models analyze the papers  and it's a it'll determine whether paper a really   builds on paper b or whether it kind of mentions  it pretty casually. I wrote a little bit more about   this in a short note that appeared in Nature in  2017 and again I'll I'll uh demo this shortly.   So I'm going to show you now a whole  slew of features of SemanticScholar  but before I do that, in the demo I want to um  kind of present a model of how we think about   how scientists engage with scientific  papers and basically there are three phases;   the first one is the discovery phase,  you have to find out about the paper   you might find out about it socially from twitter  or maybe somebody sends you an email about it, you might find it using a search in a  search engine or you might be browsing   through a conference proceeding or some other  list and you come across the paper that way.   For this first phase of paper discovery as I mentioned, we have search we also have a notion   of recommendations, so recommendation technology in  AI has been applied to uh our products on Amazon   or to movies on Netflix ,but we've actually built  a paper recommendation system which is quite an   interesting topic because it requires computing  uh similarity between uh papers and papers are   rather complex objects. We also have developed  a research feed, so again many people have a   facebook feed or a twitter feed, a news feed, we  have a research feed that you can constantly   tune uh to your liking based on uh hopefully  you can see on the slide saying I like this   paper, I'm not interested in this paper and you  can get alerts on a papers in a particular topic   of interest you so that's how we support discovery.  The next step once you've discovered a paper you   have to make a key decision, do I ignore it, do I  skim it, maybe look at the abstract, glance at a few   figures or do I actually read it, do I make it one  of that set of uh 200 or so papers that I I read   and to support the reading decision obviously  we have abstracts but we also highlight the key   sentences in the abstracts to make that easy to  read. As I mentioned, we automatically extract the   figures and the tables in the paper and that's  particularly handy when you're on your phone. You don't need an app, you just go to  semanticscaller.org and you can quickly   flip through the figures and tables to decide  hey I should save this paper for reading later   or maybe just remind yourself oh  yeah they got 83 percent on this   data set when we look at the citations uh we we  filter them, we analyze them, uh we sort them by the   degree of influence and we also give you citation  answers. One of my favourite ways to decide whether   I want to read a paper or not is to see what other  people said about it when they cited it so that   can be that's something we support and I'll show. And then lastly, something that we just launched, it's actually only available on SemanticScholar in a A/B test, so not everybody will   see it but they'll be coming out shortly and  it's a topic of a paper we publish in the mlp   is what I call extreme summarization and that's  the idea can you tape take a paper and summarize   it basically to one sentence a tl;dr for the  paper. So I'll give you an example here, here's a   famous paper from 2017 about self-attention and  the tl;dr that was automatically computed here   and I'll read it in case you can't it says we  use a 2d matrix to represent the embedding with   each row of the matrix attending on a different  part of the sentence and this is generated fully   automatically using a generative language model,  think of it as something akin to gpt3 but tuned   for uh for scientific summarization. So again,  lots of different things to help you make that   critical decision. So now let me turn to my demo,  so hopefully you can still see this so I have here SemanticScholar let me start here, so if I click  on Jimmy Lin, so I want to get caught up with his   research, I click on that - we have auto complete,  as you can see author disambiguation is is a   hard problem so let's say uh I I click  on uh on this one uh looks like I got the   the wrong Jimmy Lin uh right this one  works on biology and medicine so I go back   uh and I go to this one with 258 publications,  Jim you've been busy since grad school, and again   I have to confess we probably haven't gotten uh  all of Jimmy's papers even because uh some of them   may be assigned to the different one so so let's  look at this uh what we call the author homepage   first of all if you click here you can see what  we call an influence diagram so what I'm showing   here is the different people who influence  Jimmy and perhaps not surprisingly you see right W. Croft and Ellen Voorhees and uh and then you look at the people that he's  influenced uh and okay here's a bug right he's   influenced himself that shouldn't be the case but  uh J. Rao, and so on and if you're wondering well   why do we think that uh uh then we can look at  the papers and see what are what are the papers   this person's written that were influenced by by  Jimmy's work and we can uh navigate this graph   recursively. So now I've clicked uh on um uh Tina  and I can see uh her work and uh and so on and   so on so it's a it's a great way to uh navigate  uh this uh author authograph. Let's go back to   Jimmy's work and so this is his author homepage,  it's sorted by his most influential papers right   so uh overview of the micro uh blog track  and some work that he did uh back at uh uh   Microsoft research maybe as an intern and I might be interested in more recent stuff so   of course I can also sort by recency, what  has he done for us lately or I can sort by   citations per year, right the velocity, so  it seems like this paper on uh pico as a   knowledge representation for clinical questions, I hope we have the right uh uh Jimmy here is uh uh   getting quite a few citations per year and then I  can click on this paper I haven't done this before,   I want to learn more about it so I click here and  I can quickly see all right let's have a look at   the tables here, number questions, okay these  are the sort of structural pattern examples   etc so I can quickly uh see whether this is the  paper that you know that I might want to read   uh or or not and then if I go to the  citations it's got 373 citations. Again,   I can filter those in various ways, I I can sort  by the papers that were most influenced by this   and I see for example a variety of papers  and here if I go to this one I could look at   excerpts of the different  citations, so what do they say   about this paper. Again, the excerpts aren't  terribly interesting at this point if I go to my research feeds uh you can see that I have two; one  is about common sense knowledge and one is about   literature-based discovery. I can also click there  and see the recent archive papers that the system   uh recommends to me and okay this one I like  so I'm gonna say uh more like this or uh and   uh let's see this one I'm I'm less excited  about I say less like this and I can quickly   go through them and say okay this one on term  extraction I definitely want to save in my library.   I can save it under tags so a rich uh feature  set for discovering papers for uh managing them   uh and and so on I just want to show you uh one  other aspect of this, so let's say I look for uh   green AI which is a paper that uh that we wrote a  while back, so if I click on that you can see that   a lot of this ancillary content that  I talked about I can go to the code   associated with that paper or I could look at  mentions of that paper in the popular press   uh here's you know there was an article about it  in the New York Times, I can read about that or   here's a blog post about it in uh Forbes, so on and  so on. So just a lot of stuff to help me decide uh   do I want to go ahead and read read this paper, so  again at this point let me stop, I've been talking   for a long time showing you the the the demo any  questions or comments about a SemanticScholar? And by the way, that yes that is my paper. Oh good,  yeah we um it's a surprisingly difficult problem   to tease people apart. We are  rolling out a major improvement   to our author disambiguation but um it is  hard. I'm glad that we got that one right. All right, please please continue. Okay,  uh our uh again experiment in question   answering uh hopefully people. Sorry,  we have two questions that came in um  uh did you mind taking a few seconds? I don't mind  at all. Okay, how do you rank papers is one question   uh I'm really glad uh you asked that  because um the the ranking function is   is a learned function. We use a deep learning model  based on label data and click data but I'll tell   you one thing that we do that I'm rather proud  of which is very different than Google scholar   and that is we rank papers less by the number  of citations they've gotten and more by the   citation velocity and citation acceleration  and the reason is that you'll have a paper   and uh typically uh if it's a paper from you  know 2003 or 2008 or God forbid, you know 1992,   it's had a long time to gather citations who may  have more citations than a more recent paper but   we really need to think about the rate it's  being cited effectively normalized for its age.   So of course, we use information retrieval and  keywords but we also look at citation counts   very differently and if you want to see a very  stark example of that take the word attention   right a very standard technical term in deep  learning and if you go to google scholar and   you type attention you'll see very different  results than if you go to SemanticScholar and type attention, so that's again one  of the important principles we think   we can give you more current, more relevant  results based on our citation metrics. And a related question as a follow-up um I I  think this is uh you already addressed this but   the question was what do you use for training the  models involved in SemanticScholar? So click data   and you mentioned other other data you want to  elaborate on that a little bit please? Yeah, sure so   it's actually um a really interesting and  tricky problem because so in the ranking case   it's uh it's relatively straightforward but  we have a lot of models that are much more   sophisticated. So let's take the tl;dr model uh what  we did is we worked hard to create a data set and   by the way as a non-profit all our data sets and  our various uh research efforts are open source   and publicly available so uh so so we created  uh the world's largest uh tl;dr data set - meaning   a paper and a human generated tl;dr for that  paper he's either by the author or by a reviewer   and we use that uh to tune to tune our our method  when we do information extraction we extract   key information from papers, again we've  had to work to create datasets from that   for that let me tell you that you can't use  a mechanical turk to get extractions from   scientific papers so uh so we've invested a lot  in creating these data sets but the good news is   often you don't have to, you can leverage our  work if you want to do research in this area.   And finally one more before allowing you to move  on, so you mentioned data sets, can you describe a   little bit uh in a little bit more detail the data  acquisition process, how are you crawling, how you   what databases you're pulling from, etc? Sure, so um  our corpus is uh compiled of several sources, the   first one is we do have a web crawler uh so we're  out there looking for pdfs both of papers and   slides and then of course we have to filter them,  secondly Microsoft has generally shared with us   the Microsoft research graph which contains  a lot of information uh that the big crawler   has has found we then uh map that uh to data  sources uh in in various ways to clean up that   and then we have a databases that we get from uh  publishers from pre-print servers like archive and   buyer archive and so the combination of crawlers  and databases and filtering is what gives us the   190 million entries that we have in our paper  database. Okay, great and I'll let you move on,   thank you. Sure and and I should add again because  we're uh not a University, SemanticScholar has a   quite a large team uh and we do a lot of work that  you really wouldn't want to do as part of your   master's or phd or undergrad research project but  the good news is that we make all that available   uh via a set of open apis for free. So if you  want to have access to the data sets they might   be saved or you can literally query our engine  uh programmatically. So for example, there's a   very cool website called connected papers which  is a visual discovery tool for research papers   based on graphs and the people who built that  is just you know three folks uh in Israel who   built that using our our api. So to continue the  talk I I want to get to the third phase of the um   of the way we engage with scientific papers and  that's we've decided to read them uh they're   still phases in their uh you know to how do we  understand them, how do we retain what we've read,   and and we might even uh decide to uh to cite  them uh which of course uh goes back into this   uh feedback loop. The interesting thing is that we  have relatively little support uh in uh reading   papers right, the way we read papers now is using  uh pdf and gosh that hasn't really changed much   uh since uh I was in grad school uh way before  Jimmy where we had post script files, if anybody uh   can remember that and in fact you know uh back in  the in the uh the cave days right they were wrote   papers on tablets on the wall. What's what's really  the difference these are sort of inert uh display   formats so in joint work uh with the University  of California Berkeley this is uh Marty Hurst   uh and Andrew Head her uh her grad student and  this is a work that's led by uh Dan Weld at AI2 um   we've built an interactive reading application  that tries to help you understand a paper so   it uh automatically creates a glossary and  it helps you understand the paper's lexicon - how it relates to other papers and the the  various mathematical terms and I'm going to   give an example. This is not yet widely available,  everything I showed you a SemanticScholar is   this is still a demo but we hope to have this  out certainly by the beginning of next year   for people to use. So this is an interactive  paper reader and the demo I'm going to show   you is focused on the mathematical terminology  in the paper but we're going to uh we're in   the process of extending that beyond that so  here is a rather famous paper at AI2 that was   on the mechanism called bidef, it's not  really important but what you see here   is the glossary that was automatically generated  of all the uh different uh mathematical terms,   makes it easy to uh refer to them and understand  what's going on and of course this is the paper   uh bidirectional attention flow. As I read through  it I see there's a reference to another paper I can click on that and immediately see okay here's  who wrote this paper, here's their citations, soon   we'll have the tl;dr and I can even uh save it  to my library uh if if I want. So and I don't   have to go to another page, then I can continue  uh through the paper and where things really get   interesting is when I come across uh let's  say an equation like this one, so now when I click on the equation um the different terms are  defined uh for me automatically and furthermore,   if I if I want to look at a term I can see the  different appearances of the term in the paper. So   this h colon sub t I can go to the  next place it's used in the next place   and you can see here that now I've got other  equations a lot of this stuff is grayed out   but the key information the attended context  vector uh is right here so we really hope to   revolutionize how you read papers uh over time.  There's a lot of uh excitement at AI2 about this   uh these these capabilities. So um I've just  described to you a whole host of features   and again it's not always easy to retain all  the specifics but the bottom line is that   we're using uh AI capabilities uh to make the  process of discovering papers, deciding whether   to read them and then actually reading them uh  better, faster, more efficient, more interactive.   To do that we found that we have to scale NLP  from its current state to documents so traditional   NLP uh well it dates all the way back to  Harris and the distribution hypothesis in 1956   but in recent times it's focused on local context  and on typically on sentences right, so we work on   parsing sentences, translating sentences extracting  information uh from sentences and we've realized   that we really need to think a lot harder about  scaling that up to uh to documents and by the   way I don't claim that we're the only people to  do that uh that's that's far from uh from true   but we found that we need to break new ground  here so let me give a very concrete example,   the context notion in transformers like in  the famous Bert system, which by the way was   based on a system called Elmo that was  developed at AI2, uh I always want to want   to mention that we're very proud of that uh  is based on 512 tokens and if we want to base   our analysis of words or tokens on the full  document context we need more. So we've built   a system called long former that looks at 32  000 tokens a significant uh scale up. Likewise,   when we're doing question answering we want to  do that over the entire document, there's been a   lot of work on question answering in NLP but often  it just focuses on a single paragraph for a single   question and we have to go beyond that actually, we  want to be able to answer questions over a whole   multi-document graph. Right, so not just a a single  paper and just to look a little bit under the hood   uh this long former technology the reason it  was able to scale from 512 to 32,000 is because   it avoids computing the full attention matrix  so uh typical attention computation is uh n   squared and the length of the sequence is being  uh uh attended to uh and you can't do that with   current technology in 32,000 tokens and what we've  done instead is figured out how to do that uh in   linear time and that's represented by the sliding  window attention b. Turns out that that kind of   local notion of attention uh is not enough so  we then added elements of global attention which   what you see is in figure d we have a mixture of  local and global attention but the point is that   that mixture is still a linear time which allows  us to um to go a lot further and and there's a   paper on this uh the the famous long former paper  which of course you can find on SemanticScholar.  In addition to uh thinking of long context vectors, we're also starting to work to use the structure   that's in the document uh to help us again uh have  uh stronger more semantically accurate notions   of uh context. So we look at the figures tables  captions uh papers that are connected via citation   etc etc. So what's really going on  here is that NLP is starting to   come to terms with a major grand challenge  and you started to see scientific NLP get   more and more attention recently at the major  conferences. There have been workshops on it   and I think people are realizing right back when  Twitter was created, I and others you know were   fascinated by how do we do NLP over Tweets but  I think more recently we've come to realize that   at least as important is doing NLP over scientific  papers because it's going to make scientists that   much better at their jobs if we can help them. So  with that let me turn to the story of CORD-19. In March 6 of this year, the CTO of the United  States from the white house contacted AI2 via   a colleague at Georgetown University and asked us  to do the following, he said we know you have this   technology for building uh literature graphs  and analyzing the papers, we want you to put   together a machine readable corpus of all the  papers on COVID-19 and all the related papers   on Coronavirus. There were studies of SARS and so  on and we said of course you know we want to help   the medical community, when do you need the spy and  he said we need this yesterday, we're fighting the   virus urgency and I'm very proud of our team,  within five days we created the first version   of the corpus. We work closely with partners  at NIH, National Library of Medicine,   bioRxiv, medRxiv, and more, and  within 10 days the White House announced   the release of the machine-readable  COVID-19 data set and a number of   groups started using it to create all kinds of  systems. The original corpus had uh 24,000 papers , uh at this point we have uh more than 200,000  papers in that corpus, it's being updated daily   as more and more research comes out. And not only  is the text machine readable, so you can build   nlp and ir engines on top of it, but tables are  machine readable as well. We have state of the art   technology for table extraction that was  contributed by IBM. This became the basis   for the most popular Kaggle competition ever. The  data set has been viewed more than 2 million times   and a whole bunch of medical  information has been extracted from it.  A wide variety of places including uh University  of Waterloo, I believe it was Jimmy's team created   systems that operate uh on top of of CORD-19 uh  to to get uh researchers the information they need   so let me just give you a few uh snapshots uh  i don't have time to to go into them in a lot   of detail but just give you the flavor  of the many publicly available tools   there are question answering systems  where you can ask questions like   what are prevalent antibodies in the saliva,  are il-6 inhibitors key to COVId-19 and you get   answers. Here's a snapshot of a system out of Korea  University, here's what the answers might look like   uh the question here is what temperature kills  h-code 19 and you have the answer 56 degrees   celsius highlighted and if you look closely you  see that this system at least in the snapshot I  took can also get misunderstand the question, give  the wrong answer completely, so it's important that   not only is the answer highlighted but you  see the sentence and of course you can click   through to the paper to verify the information.  We also allow for claim verification, for example   it seemed to be controversial for the White House  at least before a bunch of them got COVId-19 that   mass masking was important, so we're able to put  in a claim to uh into SciFact which is a a system   built jointly by UW and AI2, and again identify  uh where that claim is supported or refuted   in in various scientific papers. We also have  used information visualization techniques, so this is a system called SciSite, it's  available on our website and it allows   you to dig into COVID-19 to try and look at  connections between proteins, genes, and cells   and when you find uh two items where you want  to see the connection you can click on the edge   shown here in blue and it'll take you again to the  relevant paper. So a very different way of finding   papers, it doesn't just allow you to navigate  between papers but also between groups that   focus on different areas. Virologists have told  us that they're very interested in understanding   which group is working on what so for example here  you see if I choose the topic ebola virus and the   affiliation oxford not only do I see the groups in  oxford but I see groups working on related topics   those are marked with purple edges. Groups with  social affinity, in other words they collaborate   with the group in oxford and I'll see that with  green links and again just a very powerful way of   navigating the world of science. If you look at the  bottom left, that's the URL, you can check this on   or you can just go to our our website. So um our  goal really here has been to uh connect the AI   and medical communities, I'm really proud that  there are more and more papers uh appearing in uh   met archive and in scientific journals that  build on CORd-19 and the tools that others   like Jimmy and his team and quite a few others  have built that use CORD-19 to provide information   for medical personnel. I want to stop now and again  see if there are any questions about CORD-19   and if not I'll I'll end with a few slides  with of general more general speculation.   So there were some questions that came in after  the last pause, would you mind taking now?  Not at all. Okay, so can you say a little bit  more about your definition of influence and uh   how how how influential citations or influential  papers get computed. Yeah, so um basically um the   notion of paper a influencing b is both intuitive  and difficult to formalize right, so the intuitive   notion is um when you have uh a paper let's say  um uh I I don't know um I I I'm working on uh open   information extraction and uh I built on uh you  know conditional random fields back in the day, I'm   dating myself and but I modified it in certain  ways and I use the particular implementation   and I change some things but the open information  tracking mechanism I use uses crfs very heavily,   so I might refer to it multiple times and to  details of it I might cite it, I might compare   the baselines in various figures. So the two papers  are very deeply connected, what we've done is we've   trained an nlp model that runs over uh my paper  and NLP how much info it was influenced   by that other paper I'm citing that used crf for  information extraction and it would contrast that   with let's say I happen to use the Stanford Parser , so I yeah I cited it, I use that parser, I cite   that in a footnote, who cares so those are the two  extremes. Now, how do we determine the exact degree.   What we've done is a pretty standard  methodology, we labeled a whole bunch of   of papers and their relationships and we  trained the model uh that that optimizes   you know predictive loss relative to that  labeling but the basic intuition underlying   the model is actually very simple, it looks  at two key factors; one is how often does   paper a mentioned paper b. If I mentioned your  paper Jimmy in one of my papers if I mentioned it   15 times that's much more likely that I'm building  out in a key way than if I mentioned it once, right.  Also, what sections did I mention if I mentioned  the related work section. Sure, that's one thing   but if I mention in the experimental results  section then probably I'm building on it quite   directly. The second thing is when I mention your  paper is it part of a long list, you know great   work by on you know ir by Jimmy and Ellen and you  know all these wonderful folks, it's a long list   of citations or did I mention it by itself. So all  these different variables feed into our model   and help us to define a fine grained notion of  influence. Okay, great that's an awesome answer. So uh we should let you finish uh your slides and  then come back and circle back to see if we have   any questions uh uh for the end. Okay, so uh we're  almost done here what what I want to highlight and   this is very much the vision that motivated us  to build SemanticScholar is that we see AI is   the basis for a next level of scientific discovery  and this level is where we have a tool that helps   scientists deal with information overload, deal  with these millions of papers, imagine that a cure   for an intractable cancer or some other disease  is hidden within the thousands of reports on   thousands of clinical studies and nobody is  able to read all this information. An AI based   discovery engine could help medical researchers  find the answer and of course we don't just   cover medicine or computer science, we cover all  areas. So if we want to fight climate change, again   scientists working on that task can use SemanticScholar, so that is really our our vision and it's   very much a broad and expansive vision. SemanticScholar is not going to have all the answers,   we hope that others like you will  will join us uh in this quest   and now I want to go back to the question that  I raised at the beginning of the talk. I think   most, if not all of us will agree that AI  raises all kinds of complex issues around   privacy, fairness, job displacement and so  on but to this question of is it poised   to destroy humanity as folks like Nick Bostrom  and Stuart Russell, Max Degmark say I actually   think it's quite the opposite, AI is poised uh to  save human lives. My favourite example of that is   intelligent cars. We have uh forty thousand highway  deaths in North America each year alone, we have   more than a million uh accidents with people  hurt and injured and studies project that   we could cut uh this the rate of injury and loss  of life by as much as 80 or 90 percent once uh   self-driving technology is available so my answer  to the folks that worry about AI as destroying   humanity is that it's quite the opposite, in  the words of Eric Horvitz, it's the absence of   AI technologies that is already killing people, the absence of it in cars, the absence of it in   drug discovery, and our job our imperative  is to bring AI to applications where AI   serves the common good, serves to save  people's lives. Thank you very much. Okay, well since you answer some questions in  between I will um just have time to ask you a   final question and I think this is a great  one to end on. So the comment is whenever we   automate something people begin to forget how to  do it right, so will AI to help people read papers   and discover scientific knowledge reduce our  ability to actually do it on our own? Well   I think that's a very fair question  and um you know to the extent that um   if we look at uh arithmetic, I think that  actually illustrates this well. So on the   one hand most of us haven't forgotten to use  arithmetic, on the other and we're probably   not as adept in it because whenever I have an  arithmetic question nowadays I'll ask Alexa   or Google, I'll use the calculator in my watch uh  or my phone, what have you. Uh so I I think that   there is something lost but usually it's traded  for something better. Do I really like need to   calculate tips and restaurants or can I use my  time better more creatively otherwise, so I don't   think that for the foreseeable future AI and  semantic scholar is going to replace scientists,   I think it's going to be a basic tool to extract  information or to find papers that I want to read.   Imagine if I can get results of studies from 100  papers with one queries right, let's say we have   an sql for scientific papers that would be  amazing and it would enable me to do things   that would take me an inordinate amount of  time to do nowadays. So I think overall, all   technology has a cost-benefit trade-off but  in this case it's a very positive trade-off.   Okay, great and with that I think that's a great  ending point. So once again, thank you for giving   the talk, uh sharing with us your vision and uh  your your steps towards it and you know wish you   and AI2 the best in uh accomplishing  that for all the benefit of humanity.   Thank you very much. Thank you all for uh  those of you who stuck it out and listened and   uh look forward to meeting you, catching up  in person as soon as safe. All right, take care. Bye. 