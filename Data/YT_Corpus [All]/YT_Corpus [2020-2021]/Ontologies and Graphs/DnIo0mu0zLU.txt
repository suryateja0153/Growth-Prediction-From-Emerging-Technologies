 you [Music] hi I'd like to wake you welcome everyone today to the MS our distinguished lecture series is the first time that we're having it with everyone virtual for those who recognize microsoft offices this is a virtual background in my office behind me just so I can feel at home but I actually am at home it's my pleasure today to welcome Jennifer Nevel who's a full professor in the Miller family chair at Purdue she's a professor of both computer science and statistics and she really brings both of those sensibilities to her research she looks broadly at machine learning applied to graph related problems but also brings in the statistical knowledge to say how are things well founded how do you incorporate and model dependencies especially over connected and graph data in a way that doesn't lead to mistakes and you know finally how can you apply them to real-world tasks and actually improve predictions Jennifer is well known she has over 120 publications over eight thousand citations much of her work has been both looked to in the academic world but also applied in a variety of different places so she has plus past collaborators at places like LinkedIn in kind of more personal trivia Jennifer and I first got to know each other as grad student volunteers where we volunteered to get free conference registration and so this is just one of those lessons that you know the things and the people that you connect with early on in your career continue on throughout most of your career so with that I'd like to welcome Jennifer and we'll have her get started as a note as a way to ask questions you can enter them in the chat and our moderators day and Sean will help take those questions as we go so Jennifer my mic is muted so thanks for having me virtually so I can't see any of this except my slides so everything is working okay now yes okay good I will go to my slides okay thanks Paul thanks for having me I wish I could be there in person to do this but hopefully virtual is the next best thing so I'm going to talk today about some of the recent work that we've done in my group using neural network type components to learn models of graphs particularly thinking them as sets or sub graphs surrounding a neighbor or streams of sets over time and to give you some background to set up I know I gave another talk for a different group last week so some of this will be a little bit of a review for them but what I want to do is characterize the work that I've done over the course of my career has been mostly focused on developing machine learning methods statistical tools for network based data where the main goal is to be able to exploit the dependencies in the network to improve predictions that we can make about the nodes in the network and so to give you a very simple example of the kind of data that I'm talking about here is a very simple graph where the nodes in this case our users and the edges are friendship relationships among them but we could have the nodes be physical components of a system and the edges be connections between those components so we're just assuming that we have some set of entities and the edges indicate some sort of relationship between those entities and then we have a set of attributes on the nodes and/or the edges I'm just going to show you nodes here for simplicity so if again we had a social network these might be the gender the relationship status of the users and the network and then we would represent that as an attributed graph where there is generally one class label that we want to predict for example maybe the political views of the users and we want to conditioned on not only the known attributes of the users themselves but also their relationships with the other users in the network and exploit both those attributes on those nodes and their class labels in order to improve predictions so much of the work over the past number of years in the relational learning community has focused on the simpler types of graphs that I just showed you we have considered basically static graphs where the structure of the graph doesn't change that much over time and then we dealt with how do you deal with a heterogeneity of the graph structure and the dependencies and maybe the different types of nodes or edges in the graph but obviously now there's lots of different types of network data in the world and many of those datasets are comprised of interactions between people or items over time and so that is something that we want to be able to include in the models as well and an additional challenge is that the network structure can evolve over time so we can have new nodes join the network and then fundamentally the the network characteristics change as the graphs grow so in this talk I'm mostly going to focus on the task of node attribute prediction and I can roughly characterize much of the work that's been done over the last 20 20 years and developing models for these kind of graph based domains you can think of it as templated models where we learn little local templates that specify the dependencies of the attributes that we observed and they are rolled out over the heterogeneous structure that we observe as many times as we need to and so if we look at these little templates here the ones on the bottom that should be the dependencies between the X's and the Y's those characterized the standard dependencies we would have in any type of machine learning model but the one circled on the top is a relational template which specifies in this particular case that the class label of node I depends on the class label of node J and in this case then that will be rolled out or replicated for a particular data graph for every pair of nodes that is linked in the data graph so once we have these little local conditionals we have to figure out how to learn the structure and the parameters of those conditionals and we can use a number of different types of machine learning methods for this internal component of the relational model and so that can include things like naive Bayes classifiers logistic regression models decision trees or even neural network components and once you have one of the once you've decided on what type of local conditional you want to use what will happen is these little model templates will get rolled out over the network structure here in all of the appropriate ways that's specified by the data network and so if I take these for two little example templates that I'm showing you right here and we roll it out over this example data network that I'm showing on you on the right you end up with a rolled out model graph that looks something like this and you can see for every node that was in the original graph there is a little structure that specifies that it's observed X attributes depends on its class label or they're influencing each other and then you can also see that there's links that connect up the class label of any pair of nodes that was connected in that underlying graph and what I want to point out here in this version of the network is that if we look at the node five right here you can see that it's connected to four different neighbors and some other nodes like for example one is connected to only one neighbor and so this is the first challenge that we have to deal with is this heterogeneity and what I want to point out is that that really corresponds to dealing with sets of varying size in this case just your one hop neighbors will produce a set of a varying size and the models have to deal with that and so there's been two primary ways to sort of incorporate this to the NAP into the models that we have and the first approach I'll show you it looks like this where in this case what you do what the models do is take repeated pairwise dependencies between the target node in this case node number five and each of its neighbors and so then that dependency can be replicated for as many neighbors as you need and types of models that use this type of repeated dependency approach include naive Bayes classifiers Markov random fields and also the illest hand model because the sequence is rolled out as long as you need the other basic approach to dealing with sets of varying size is to use some sort of aggregation function to transform that set of varying size into something of fixed dimensionality and the simplest approach to do this is to just use some sort of summary statistic for example the average of the class labels or the count of the class labels or the min and the max over some features that you have but it also applies to and those are used in things like logistic regression or decision trees but in things like GCM and we it also uses a similar type of approach where you're coming up with some internal representation of a fixed size that then then is being aggregated over the set of neighbors so once you've decided on your local conditional then that will define combined with this rolled out structure of the graph will typically define your objective function for you pretty pretty easily and again there's two general approaches to optimization over these models the first one I would characterize as a local approach where in this case you're optimizing independently for every node in the network conditioned on some neighborhood local neighborhood around that node in this case I'm showing you a graphical model notation which says that you're conditioning on the Markov blanket of the node but they're methods that also will just condition on whatever you observe in a one hop two hop three half-naked so and so forth and that's in contrast to the meth that try to do optimization over a more global objective and in this case what I'm showing you is an objective function for a Markov random field where you can see based on the product here that it's putting in each of the little model templates into the product as many as there needs to be but it's specifically trying to optimize the Joint Distribution over all the class labels in the graph rather than the individual local and inferences reach node and if you learn this kind of global model that estimates the Joint Distribution then what we typically do for prediction is to apply that global model over some large graph and do joint inference to make predictions and so this is a little video that I will start playing here I keep my fingers crossed that you see this this shows you how the collective inference process works in this graph that it's a synthetic graph that shows a small world network structure where you have communities that are mostly of the same class label either red or blue but that there are some there's some variability within a community and their long range links across the communities where information can get propagated through the graph and if I just show you this again when you start off the primary colors red and blue show you the label nodes that are labeled to start the inference process and then the pastel colors show you the nodes that are not labeled that you can still tell that they're red or blue and then the collective inference process essentially goes through iterations where you just successively update your prediction based on the current predictions of the neighbors and in this case you can use any type of approximate inference method like Gibbs sampling or loopy belief propagation or variational inference to do this okay so now let me go back to using neural networks as the local components inside one of these relational models so returning to this global versus local view of doing the optimization and inference I would say that most of the current work on using graph neural networks focuses on local learning within a graph and so specifically what I mean by that is that the graph neural networks are trying to learn latent representations of the nodes in the network and then prediction will happen conditioned on those latent representations you may a granade over your neighbors in order to get a representation that's going to make the final prediction but you don't need the actual predictions of the neighbors in order to do to come up with your final prediction and so that's in contrast to the work in relational learning that tries to propagate the current predictions you would make about the unlabeled nodes through the graph in order to make better overall predictions and I will come back to this point later on in the talk but I should point out that in the relational learning scenario if we knew what the labels were of those nodes the inference would be independent as well so any node where you have the known labels it's the inferences kind of locked down at that point and information doesn't propagate further through those through those nodes so we are specifically just using the unlabeled nodes to propagate more information and I would say that the conjecture in this case is that the global information that we have when we're making these predictions about the unlabeled nodes helps to smooth out or augment the local information when there's insufficient data to learn a good representation there and I'll come back to how that applies to graph neural networks at the end of the talk so returning to the issue of how to use neural networks in these component local component models the one of the big issues is that in typically in deep learning there's an assumption that you have the canonical ordering over the data either to input it as a vector or a matrix and specifically if we put it in as a matrix there's some implicit assumption that you have this spatial relationship amongst the pixels that you can then use in convolutions in the CN n type architecture and then often if we have a vector representation that's used for things that are more like a language type problems or temporal problems where there's an assumption of the ordering over the vector that goes from past to future and those type of orderings can be used in recurrent neural network structures but if we have heterogeneous graph structures it's not clear how to determine canonical ordering over the elements in a sub graph so for example here we have if we have the target node that's circled in red and we have a set of neighbors around it circled in blue some of those neighbors have labels class labels observes some of them don't it's not clear how to organize the set in a way that's going to be useful in a neural network input and one way to do it and one way we have done it is to put it into a sequence representation where we can deal with effect that the sets are varying size by having a rolled out sequence of as long a length as we need and in this case we put the target node here in the red at the end of the sequence and then we have the neighbors leading up to it and we can put this into an LS TM formulation like this where every cell were outputting a prediction of the class label for that particular neighbor but in the end we have the hidden representations being shuffled to the right that hopefully will aggregate the information properly to then inform the final class label prediction we're going to make about the targetnode in red and so we did this in a in a collective inference framework for doing classification and networks we specifically use this type of formulation and I won't get into a lot of the details of the method but what we did was use this lsdm formulation tied as the local component local conditional in a semi-supervised relational um scenario where we were alternating between estimating the parameters of the neural network and then making predictions for the unlabeled nodes in the network and the three design choices that were really crucial to getting this model to work was to start the collective inference process not from random predictions which is typical if you were going to use Gibbs sampling in a graphical model formulation but to initialize them with a from a better starting state if you will by populating the predictions with a non collective relational model and then we had to randomize the neighbor ordering on every iteration every epoch of learning and I'll come back to that later in the talk and we also had to correct for imbalance classes to balance the objective function and once we did all that we got significant improvement in performance over the previous current state of the art of doing relational learning and so here on the results on to example data sets where we're showing the class distribution up at the top so you can see that one is a balanced class and one is a skewed class on the x-axis what's being varied is the proportion of the training set that's labeled and so when at the left at Point 2 that means 20% of the data set is labeled and we can use that for initially estimating the model and then we need to make predictions about 80% of the data and then as you go to the right we have more labeled data available to estimate the parameters of the model and we have to make predictions about fewer examples and so what we're reporting here is balanced absolute error so lower is better and you can see in both cases that we see a significant improvement with this LS TM type model that we've used and particularly we get large gains as we have more and more labeled data in the network and the the pink and cyan colors are the previous state of the art and you can see that it only does better in the case where there's very few labeled examples in the network but what I want to point out that is even more interesting than that is that there is a competing neural network approach that we found right either as we were submitting the paper or right after we had submitted it that we were unaware of and they the components of that system were very similar to the choices that we made in our model and specifically they also use an LS TM formulation for the local conditional and they combine it together in a relational eeehm kind of framework for estimation just like we do and they also put random ordering of the neighbors to use in the LS TM and so given that as a very similar type of formulation you'd expect it to do relatively similar to us in terms of performance but in fact they do significantly worse than us and you if you note on the amazon data they also do significantly worse than the other previous state-of-the-art methods that use graphical models and so the reason for that is these design decisions that I mentioned before but the design decision that had the most impact is to repeatedly re randomize the neighbor sequence on every epoch of learning and so I'll come back to the explanation theoretically for what is the impact of that in a few slides but before I get to that what I want to do is switch to talking about dynamic networks from static networks and in in the method that I'm going to describe for dynamic networks we use the second type of approach to aggregate all over that neighbor set and in that case we used a GC n type model which I'm showing you here which aggregates the features over the neighbor set and has multiple hidden layers so that you can propagate information from longer and longer paths away from a node and so we use this kind of formulation to model temporal graphs where we have a static class label that we want to predict so for example if you publish in a particular area or if you're a fraudulent user in cellphone networks or something like that but what you so that the class label itself is static but the data is a sequence of interaction graphs over time so you have messages or emails or calls between people or you have co-authored relationships amongst researchers and so there are two ways again that people have modeled these kind of interaction graphs because the class label is static many people would just take the dynamic temporal data and just aggregate it together into a weighted graph and get rid of all of the temporal information and just summarize how many edges you had back and forth between people and other approaches take the temporal sequence themselves and try to model the evolution of that of those interactions so for example taking the change in communication that you would see at time t2 the change that you see at time t plus 1 and so all the previous methods really fell in one of these two states and as we started looking at this issue again we thought that if you gave the sequence of temporal graphs to the neural network model that it's would have all of the information there that it needs to to figure out these same types of patterns that would be in the aggregated graph but we thought that maybe it would not be able to model it as effectively because the individual time slices and the temporal graphs were relatively sparse because you don't have as connected a graph at each time step because of the sparse nature of communications and bursty nature so in our case what we wanted to do was try to exploit both the temporal and the aggregated patterns of interactions and we did that by combining two types of architectures in one and learn them jointly together and basically in this case what you can think of is that we took the temporal slices and we learned a temporal encoder temporal encoding or temporal embedding of each node in the network and then we also took the aggregated graph separately and we learned a static encoding or embedding of the of a network node and then combine them together jointly to make a final prediction and so the details of that are that the temporal encoder what we did was use GCN layers funneled into an LS TM which was supposed to model the temporal dependencies and then on the static graph we just use a standard neural network encoder of the set of neighbors and the the interactions you have across all the nodes in the graph and when you consider both those things together we compared prediction on several different data sets comparing to a set of models here with the different colors you can see the rows correspond to different models that only use the temporal interactions or only use the aggregated static graph interactions in order to do these prediction tasks and our model does better than both of those types of methods because we combine both types of patterns together but one thing I want to point out here is that the relative performance of the methods change on different data sets so it seems like in some data sets the temporal interactions patterns are more important and other datasets the static overall set of people that you interact with is more important and so our method with these two types of encoders was able to perform more consistently across different kinds of datasets but what I want to point out here is a point that doesn't show up in the paper mostly because of the page limit that we had when we were finally publishing it but also because this is my students dissertation and so there are more details in the dissertation but what we have found is that when we randomize the graphs with respect to time when you still observe 95 percent of the accuracy gains that we that we witness here in this table and so what this indicates in this case is that we may not be finding actual temporal patterns over the data but in fact we're modeling the set of neighbors that you interact with more than the the temporal pattern of how you interact with them because if we randomized that we destroy the temporal patterns so now let me try to connect both these ideas together so in one case we randomization was essential to get the results that we saw and in the other case when we randomized we still got most of the performance gains that we that we observed and so in both these cases that's really tied to the fact that in graphs we're really trying to learn as permutation invariant functions over these sets and it's very difficult to specify a canonical ordering over sub graphs or sets of neighbors and so what that means is that when you use traditional neural network architectures to estimate the functions you're going to be sensitive to permutations and so the if you put things in in a different order you're going to end up projecting that into the example into a different point in embeddings and so some recent work by my colleagues Vinayak Rao and Ribeiro at Purdue has really showed why we needed to have this randomization to begin with in our triple AI paper and what they show is how to use permutation sampling or in this case summing over all possible permutations to use a permutation sensitive function to estimate something that's permutation invariant and obviously in theoretically you don't want to sum over all possible permutations and so what they showed in their paper was how to attractive ly estimate permutation and variant functions in three different ways the first would be to have some sort of canonical ordering over the elements which we know is fairly difficult to do in most graph cases the second way to do it is to model dependencies over smaller sets of size K and I'll come back to that and the third approach is to do stochastic optimization by randomly permuting the set on every epoch and so they call this method pious G D and that is specifically what we're using in our in the first results that I showed you so remember I said that we had to randomize the order of the neighbors on every epoch in order to get the results that we saw and so what I haven't told you yes interrupt for a second we have a very interesting question which I think we're the context is right for that so the user so somebody writes the use of an LS TM for local subgroup graphs is very interesting Alice Liam's address the fact that each node in the graph may have a different number of neighbors but Alice tiems respect the order of their inputs given that you are randomizing the order of inputs to the LSD m's have you tried attention based models or any other architectures with variable length inputs that aren't order dependent so I I'm unaware of something of an approach that takes variable length inputs that is not order dependent that doesn't do something akin to one of these three approaches that I have on the slide right now so I don't know if whoever asked that question could specify specifically but I think maybe yeah go ahead Paul is that yes I mean maybe because I'm saying the question if I understand the question right because they're thinking of attention based models I think the issue is that you usually still pick some length there and in addition if your data has a natural underlying order but it doesn't have a dependency you can inadvertently model it right so if you look at it as like the order in a shopping basket don't still you'll still have that problem in a transformer based model you'll have to deal with it in some way yeah and I think this will also become like I think it's an open issue and I think it's specifically the issue that people should be considering and I've it's what I'll continue to talk about so I think it'll become clearer at least you know sort of my views on what we've tried throughout the talk so hopefully maybe we can circle back to this question later as well sound good okay okay so let me sort of tell you I was about to tell you the history of this which I think you know gets a little bit at this attention issue that so when we developed this method I explained it as that we were randomizing the order of the neighbors on every epoch but that's not what we did for the first six and nine months of the project we actually tried to figure out the best canonical ordering of the neighbors which we thought we should be able to do given our knowledge of social networks so specifically my student this is John Moore who's actually now at working at Microsoft he came up with a what I thought was a very smart way of ordering the sequence in reverse order with respect to personalized page rank in the graph so we thought that personalized page rank would indicate which nodes based on their structure were more important to a node and so we put those closer to the node in this the target node and the sequence and we actually did pretty well with that method and it was only when we were about to submit that I said well we need to compare to a random ordering in order to see how much our smart method our smart canonical ordering actually has improved things and it turns out that the random ordering did much much better and so in this case maybe to get back to sort of what Paul is alluding to even when we think we have a way even if that way is sort of automatically to try to let the model learn what the ordering is through attention it may not really get at the true canonical ordering and so I think thinking about things as permutation invariant and and or what are the what are the in variances we expect to see over these neighbor sets I think is an important way to move forward so let me also point out here that this K Airy dependencies is also what some of these set based models correspond to doing right now which is taking small sets small subsets of the larger set that you have a fixed size and doing sort of aggregation over those over sort of multiple samples of those and having the aggregation be over a larger dimensionality so that it can effectively store more information and so that is also a way to get at the information across all of those sets and it's not clear which is the which is the best formulation at this point we did not try that in this in this previous paper because my student graduated started working at Microsoft so we haven't come back and compared this but I think that's a that would be an interesting thing to start to compare the expressive power of neural network formulation with these different kinds of approximations because the true sort of idealized permutation and variant function has to sum over all possible permutations of the set which is obviously intractable to do ok so let me make a connection let me see what the time is here ok so let me make a connection quickly too dynamic from dynamic network two sets of sets so when we have this observation that that our our dynamic model really saw most of the gains when we still randomized over the time sequence that mean we think that the the temporal information was really not all that useful in the model and led me to start thinking of these sequences as things that are more akin to sets of sets and so I think Paul was mentioning shopping baskets shopping baskets earlier so if you now think of the shopping basket shopping set of shopping baskets that each of your neighbors or similar users have have had that's now a set of sets or if you go from a one hop neighborhood to a to hop neighborhood and you want to represent the neighbor sets of each of your neighbors that also becomes a set of sets and so what we wanted to do was think about how to model set of sets in a permutation and variant way and in this case what you need it to be as invariant to both with intercept and interest at permutations and so the you might think that you could just put a set of sets into asset based permutation and variant function and estimated accurately but if you just merge the data if you just merge the sets you're going to end up with trivial collisions and representations so these two different set of sets merged to the same single set and then they're going to be embedded into the same point in space and so if you use some sort of element in Decatur of where the set ends then you're still going to need to have a be permutation invariant over both inside a set and across the sets and so so we developed a method to do that and here is just maybe animate this out for the sake of time so this just shows you that there are some approaches that are not permutation invariant and they don't have a problem with these trivial Kalu collisions there's a set of recent set based methods that are going to be permutation invariant over sets but they're not going to they're going to have this problem of trivial collisions if you go to setup sets and so here's the architecture that we have that we proposed in order to do this set of set type of representation and so basically you can think of it as two layers of permutations where there needs to be this set level attention or attention to the elements of the set which gets back to the question about attention so I think in this case we found that the results were significantly better if we put the attention layer in there in spite of the fact that we're still doing pious G D so we're still sampling over permutations I'll sort of animate this out to see that so we're doing pius GD around this estimation around this whole function here so we're sample per sampling permutations of both both layers but the attention helps us helps the model figure out in the space of all those possible permutations what to pay attention to and attention to a little bit more so that was also an important thing that showed up here I think I don't have results to show you here for the sake of time but suffice to say that we got better results this method was published last year at ktd and I can point you to the paper for more results but I want to sort of come back to this idea of dynamic networks and I want to point out that what we've found is that many of the methods that have been developed not just by my group but other groups as well have really been focused on trying to jointly capture capturing the temporal and the relational dependencies together under the assumption that both sources of information are equally important and so for example if you've got interactions among people over time it's the specific interactions at the specific time stamps that are important and we've got to have a robust enough model that can capture both of those but what we found in many cases as we were modeling dynamic networks particularly interactions based on emails or documents or messages that it was really difficult to extract information relational information from the temporal patterns because individually the data looks very sparse and bursty and globally it's hard to align the data and there's a lot of variance amongst the graph structure and the different time steps and so what I am now thinking is that thinking of this problem as a set of sets problem that we need to model on a permutation and variance way may provide a sort of good middle ground between these two elements because what may not matter is the specific times that you interact with specific people but the sort of set of interactions that you have with them over time because that indicates what type of relationship that you have with them so in this case if we take this sequence of temporal graphs down at the bottom here and we look at node I then what we would do is take that nodes neighbor set at the different time points and put that into a set of sets and this is something again that I've wanted my students to do but the the two students working on these different projects also have graduated and gone to different places so maybe if there's an intern that's interested in working on this with me but you can talk to me afterwards okay so finally let me sort of start to wrap up with talking about this issue of local to global and I'll return to this slide that I had before that said much of the current work on using graph neural networks has focused on this local learning and doing independent inference conditioned on these latent representations or the embeddings that we learn of all the nodes in the network and that the gains that have come and the relational in have really focused instead on even though of course we have latent variable models or had latent variable models that tried to do the same thing we mostly focused on how to propagate predictions and the uncertainty about those predictions throughout the graph in order to use that additional source of information to do a better job of predictions however as I pointed out before if we actually knew the labels of the of the nodes that created a Markov blanket around the node and we didn't propagate past nodes that were labeled and so so an unknown issue in relational learning is when is collective inference actually needed and when is it actually going to improve things and we have some recent work that is on archive now that shows theoretically that if you have a most expressive GNN type representation then collective inference can't can't help you so it's not going to do anything over and above the independent inference that is currently used with with GN ends and but that is conditioned on the fact that you have a most expressive representation in the network and it is also well known that current-gen ends are not most expressive and so given that we're living in a world where we don't have the most expressive type of representation at this at this point what we've done is try to incorporate collective learning and inference ideas into GN n type architectures and we specifically done that through using an ensemble you can think of our method as something of an ensemble of sampled embeddings which are going to help to improve the expressivity and predictive performance of any GN n and so our results we apply it to I think three or four different types of current GN n architectures and show that we show empirically that we do we do no worse than the sort of previous performance you would get and in many cases we this collective learning approach does significantly better but in order to do that we have to start to incorporate the collective learning collective inference ideas from relational learning into the GNN formulation and we do that in a self super biased way and so overall our understanding of how Wireless is working is that these samples encode the global or joint information with respect to the predicted class labels and it represents this as local variability which provides extra discrimination power so to just wrap up here I want to point to some ideas about moving towards more expressive graph neural networks which returns to the question that we had before so I think that we need to explore the use of inductive bias to design these tractable yet permutation and variant methods for these more complicated relational settings particularly graphs that are very heterogeneous and involving over time and I think network based pooling or attention functions are something that needs to be explored because just because you have theoretically permutation invariant function doesn't mean that in practice and finite data it's going to be the most expressive for you given the amount of data that you have I think it's also important that we understand how variation in graph size or distribution of structures affects the generalization ability of these learned models it's been we've been showing for some time in the graphical model literature and relational learning that graph structure is implicitly wrapped into the objective functions and the formulation of the models and we don't yet understand how to specify what types of when we learn it on one kind of graph what types of graphs the model is going to be able to generalize well to so for example these representations or embeddings might be implicitly biased to graph structures used in the training data and finally I think we need to focus on designing architectures for that to deal with the trade-off between increasing expressivity but also maintaining scalable tractable computational complexity or a fast convergence rates and in that case you need to think about how the internal structure of the architecture is connected which I'm referring to as the computational graph and that trades off against the diet mentality of the internal function or the pooling function that's used inside the architectures and so I think that's also an open area of investigation at this point so with that I will stop and say thank you very much that's our best attempt at simulating a crowd in the current thank you so one thing I want to remind the audience of that I forgot to mention the beginning is that Jennifer is actually a visiting researcher at Microsoft for the next six months and so yes if you're interested in talking she'll be around for quite some time and I encourage you to drop her mail at her at her internal alias so you can post questions in the chat Q&A so I'd encourage folks to do that we can read them out loud unfortunately people can't mic indirectly with this set up so are there any questions currently sean day the people of i don't see any new questions but actually i had a question and since i have the privilege of being able to mike in i will take advantage of that while we wait for other questions so Jennifer do you have you seen or thought anywhere in literature or that about explicitly enforcing invariants so let's say let's say I take I understand that like invariance to set ordering of elements is and learning that in an invariant manner is is important is there a way for in which we can like constraint like I understand there will be for n items 2 to the power n constraints where we say that like hey objective down as do the task at hand but while you are learning the embedding in the inner loop you have the constraint that all these possible invariants must result in the same embedding sorry all these possible orderings of the set and then explicitly enforce that and as a constrained optimization procedure or has that been done or has no hope random words well enough yeah I think that so there there are a bunch of people currently working on coming up with invariant representations just more generally from a theoretical or mathematical perspective I think the issue with graphs is that we're not really sure what invariance is we want right so I think so I think in the image community there's been a lot you know over the time and they've come up with a lot of different types of in variances right like rotational invariance or scale invariance and so I think we need similar types of ideas for graphs but we don't really have notions of what those are yet and so I think like this story that I was telling that we you know sort of developed the method first and then figured out the reason it did well is because it happened to be trying to preserve invariance that's really how a lot of the research goes right we just try to make it work well on graphs and then we end up figuring out why it might work well later so I think along the way if we know how to if we come up with variances that we should specify the optimization people are gonna do a good job of optimizing that I'm not sure that we really know what they are yet but for example when I talk about these graphs of varying size that could be something that we'd want to include and describe that as something like scale invariance but we don't know so we do know that the structure of a graph when there's a hundred nodes versus a hundred billion nodes is very different but I don't know that we agree yet as to what should be preserved as you go from that very small graph to that very large graph if we could specify that then we maybe could encode that in an invariance so that I think it's sort of two sides of the same coin of knowing what we should have and then being able to optimize it explicitly thank you so let me take the next question so there are many ways that people think about reasoning and this is really sort of plugging neural networks into graphs there are other there are other folks who really think about capturing structure in more direct ways within neural networks so there's different local and global conditional models within textual networks that kind of have attention range across the whole global span of text or locally there are other things like the tensor product representation do you see any of these applied in the same spaces I tend to see them in different spaces just where they're applied and it's not clear to me if they're just because they're different communities or because one of them works better or worse have you seen comparisons of any of these different types of approaches in the same space so what what exact what rote approaches are you're attacking there like you know there are so like long former as an example of it we have work in the same space where you look at attention sort of locally within a text window or globally in the tensor product representations there's basically a fundament of each symbol has an outer product of a role and a type and you can depending on the Opposition how you compose them as either addition or product operation you can do different set operations on them implicitly through the mathematics but but I tend to see them applied for the most part to different types of problems and so it leads to me this kind of fundamental question of which one is making progress and so it's just sort of more a question of like you know have you seen these applied in the same space is it that some of them scale what better to larger explicit graphs versus implicit graphs or is it simply we don't know because people haven't compared okay we throw certain kitchen let's try to answer it anyways so I think I haven't seen a lot of work yet that tries to generalize across the different types of applications I think there's a lot of what I would call engineering work going on right now for people with their specific problems trying to develop approaches that are really going to get at sort of the key you know challenges or information in their data so I don't know of any sort of big takeaways that we could have there but I think that we're starting to see that these types of operations have similarity and so so I think we can move towards understanding what are the characteristics of a problem that would help you apply one of these types of approaches for example you know LST M's have this sort of memory problem right so that if the sets are really large size trying to put those into an LST M is challenging because they sort of forget the information that they saw way down at the beginning of the sequence even though again theoretically they should get to that information if you randomized over it all the time but that doesn't mean that they do in practice and that's what I was sort of alluding to in my last bullet is that there I think there are trade-offs between where you do these operations so you can you can add some respect put everything into the model and then use attention to decide where to pay attention to it or you could use some sort of inductive bias to say oh I'm only going to consider these small subsets and then model their dependencies very explicitly and I think we haven't started we don't really have the language even to talk about the trade-offs between the so I think it's a good question but I but I don't know that there's anything to to point to explicitly to to sort of move forward on that I would be happy if anybody in the audience knows of something that they could point me to that would be great all right great well thank you again for the great talk and thank everyone for joining us and bearing with us as we deal with our first time doing this as a complete virtual broadcast and once again Jennifer is around as a visiting researcher for the next six months or so so encourage you to reach out to her with your problems and at least let her learn about them I think she's very interested in learning all the different types that things crop up in applications here so once again thanks everyone have a good day okay thank you you 