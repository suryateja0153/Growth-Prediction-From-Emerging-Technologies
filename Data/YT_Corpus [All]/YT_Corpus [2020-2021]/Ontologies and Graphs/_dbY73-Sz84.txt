 um and welcome everybody to this to this webinar we wanted to spend some time on this on this application that that we built inside neo4j in essence what we're presenting today started out as they started out as as a dog food project within the company that knew Friday we kind of like the user on our own technology for various purposes and being a distributed company and I'll elaborate more on this further during the webinar and a fast growing company employees are kind of I'm gonna spread out across the globe and and we're all stuck in remote collaboration right today everybody is doing remote collaboration were obliged to but we've for us this is this is kind of standard and so we've we started using our own technology to capture capture information from our instant messenger to understand who's collaborating with who how virtual teams are what are really the virtual teams that exist look at the cohesion of these teams on virtual teams that that exist across departments and so on see what skills are being talked about which customers have a lot of activity and so on and so on earth you'll see during this webinar there's a-- a whole lot of ways to deleverage instant messaging data wouldn't inside the company and with me I brought Morgan and Neal's two of the guys who initially worked on this on this project as we started it out they're part of the consulting team we use these internal projects also to study work you build a you build a knowledge graph like the one we'll be showing today there's quite a lot of technical elements that are that go way beyond just a neo4j and graph modeling there's a some advanced data science here there's some sentiment analysis there's some there's some elements regarding natural language processing etc and and to make sure that everybody in the team is up to speed on all those skills we also use the project for these project a last piece of the history project was initiated by by Adam Cowley I do want to mention him to was part of a team who's now part of their rel and who will be doing a blog post about this about the same topic that should be published later today so that's the introduction in the history of of what we're speaking about and then I'll dive into the into the agenda if you preview of what we're gonna be talking about and and really get this at the webinar going ok so first of all I'm gonna spend like two or three minutes introducing neo4j for those of you who might not be familiar with neo4j the graph database in the company I'll do that quick introduction and then I'll dive into the discussion around remote collaboration some of the challenges you find I will move from there on to knowledge graphs how knowledge graphs can help you deal with some of those challenges at which point Neels will go in a little more detail and explain what the graph model inside this specific knowledge graph will look like after that we'll go into the demo will show will show a thin front end I must say we've got a I think we got a pretty cool looking front-end on top of what we built but the real core of the application is is the backend logic is the database after all we are a database company but we've got a cool look in front end which I think illustrates really well the potential of this application after the demo by Morgan and Niels we'll also go a little more into technical details as I said there's a whole lot of technical aspects analyze moods natural language processing the actual graph model by itself there's an element of data science etc definitely worth talking about and then to conclude we'll talk a little bit about implementation aspects how long does it take to build to build such a project so that's the agenda um without losing further time let's kick off so about neo4j u4j is today the industry largest dedicated investment in graphs neo4j has been around for more than ten years we can we can claim that we created the D graph database category we are we are by far the market leading craft database on the market when we started out before 2010 there were no graph databases we were the first one to create an open-source graph database long before any others were on the market and that makes us that puts us in the position we are in today being the market leader on the graph database what a rapid growing company over 300 employees at the end of December 2019 we're way over that number by now and pretty pretty steep objectives for the for the end of this year when it comes to grow we're pretty distributed but we got a headquarter in Silicon Valley and office in London Munich n and Malmo and although we started out obviously ten years ago working with smaller and more innovative companies I would say where we do most of our business today is with with fortune 500 or fortune thousand companies global enterprises the global enterprise space has definitely discovered the power of proud databases in a we have an enormous traction in that market a lot of attention there so that's neo4j I want to spend two seconds specifically on neo4j customer services and the professional services company exists since 2010 obviously we're for the company soft software we sell is our their key business but as we moved into the enterprise space as I just said the we started seeing the need for a separate consulting or professional services department we started that department in 2017 just to make sure that as customers go into more complex graph implementations and an embedding graphs into their wider IT environment that customers get get the right assistance the professional services organization today as over 40 certified consultants and has been growing rapidly it's one of the fastest growing departments within the company and I think that simply aligns with the popularity of graphs in the enterprise space most people have have over five years of experience in the craft space which knowing that this is a technologically very relatively new space is quite a lot Consultants have on average over ten years of experience in the idea in the ite a space background of most people is database data modeling analytics oriented but as you will see also in this presentation many new 4j projects also have a custom front-end development so the team also carries those skill team also has a dedicated we also have a dedicated PMO group it helps keep our projects on track when it comes to just project management best practices etc and we have a small dedicated solutions team that makes sure that we focus on making things as reusable as possible and that includes some of the stuff we are we are presenting today so that's about neo4j let me move into the discussion on remote collaboration so I'm kind of kicking an open door here everybody's talking about out about remote collaboration it's the thanks to covet 19 it is it is the talk of the town the news is full of how do I collaborate efficiently remotes how do I make my kids collaborate efficiently remote one of the topics I struggle with these days how do I get my kids on all these these school platforms and get them connected to their teachers I feel like I've turned into some tech support group for my own household here sometimes but but remote work it has become the new normal for for everybody and you can read that in the news you can notice that on the stock exchange some if some of the companies that enable remote working it just have become have become so much more popular and have gained a lot of market space and I've gotten a lot of attention um but for but for neo4j I would say that remote works in our DNA if you look at where we're based as a company were were in over 20 countries today and and have been in over 20 countries for quite some time and in more than 90 different locations because some of these countries especially look at the u.s. the spread of people to the u.s. is pretty big but the same in the same in Europe people are not typically located in one office I would say I would say that Malmo and the London office are probably the biggest ones we have no more than 50 people in either of those offices which gives a a wide spread of the employees across the globe combine that with with a rapid growth and and you have a situation where where people need to get to know each other need to be able to connect you need to figure out who's who's the new guy that joined and and and what is it that this guy knows okay just to illustrate that remote work is in is our DNA a couple of a couple of screenshots which are actually all all of the last couple of days I think first screenshot just shows us collaborating on zoom that's our regular irregular team meeting actually you can see Neil's on the left top there and a couple of other guys from from my team on the right you can see you can see ml our CEO I think you've for those who joined a little bit earlier in the waiting room you must have seen the video of email I think answering a remote Q&A session so we lo use online tools to do a remote Q&A allow everybody to to send in questions to the CEO which he answers live online on the Left bottom you see in an online graph modeling and whiteboarding tool we which we developed internally we use it with our customers to to do white boarding around graph models and have these interactive sessions where we we agree with graph models are gonna look like and then last but not least in that would get a lot of attention in this seminar we use slack you slack very intensive leads grown a lot in popularity lost the last year's and most of the companies extremely active on slack we've got a large slack history that goes back several years in time stores quite a little bit of quite quite a bit of knowledge fifty of the company slack um but going back here going back to the challenges fast growing company new people joining with new skills new departments being added new pieces of products new pieces of product being developed we have questions all over the place and we just extracted a couple of questions that are typically travelling across our course our slack channels and and they fall in a couple of categories if you look at the examples here it's who has who has worked on this customer or on this type if you use case but also who has skills in I don't know Amazon in in in Amazon in the Amazon Cloud space who is our LDAP expert who has used who has used tableau who has worked on on a customer ebay who has worked on this or that these questions fly fly over slack and and and this is a question we wanted to answer as a team it's um it's one thing to it's one thing to launch a question into the slack space and then people are pretty helpful in this company usually you will get an answer and saying hey talk to this guy or I I can help etc um but sometimes you're you're your cry will just disappear in the dark because people are not continuously monitoring your questions no and and to get around that issue we really started thinking about okay how can we expose the information that is captured in slack in a better way so that people can get to the skills that they need and get the help that they need okay and then I think that's that's in essence the questions that we started out when building this knowledge graph so we we wanted to talk about it or we wanted to resolve the problem of who is working where and on what and and how can I give other people access to that expertise in addition we also wanted to look at cohesions like we've got people working together or interested in a specific topic how can we see who's who are the people that are connected to that topic whether that topic is a is a use case whether that is a customer whether that is a technical topic how can we find those connections because that does not necessarily just follow out of the structure you find in slack it's it's kind of across all the all the information you have in that channel and how can we consequently improve the efficiency of collaboration between people those were those were the the things that we started out with you'll see in the seminar or in this in this webinar that there's actually a lot more use cases and potential for what we've built but this is where we started so why did we look at a knowledge graph and then let me zoom in for a second on the topic of knowledge graphs for for those who are not familiar with the concept we looked at building a knowledge graph because because it could solve this specific problem and the knowledge graph knowledge cars have been around for for some time if you look at at Gartner and some other industry analysts they'll they'll place it on the hype cycle saying oh this is this is extremely hot right now I'm not I'm not sure if I kind of agree with what Gartner says does just matter there's little in this case the reality is we see customers and we see in the marketplace knowledge graphs have been around for actually quite some time and and reads the decent decent decent maturity that the knowledge graph that you probably all know the best this is Google Google is in essence a knowledge graph that that stores information around how various internet pages are are connected and enriches enriches that information so that you can you context contextual search on that ensemble of internet pages and find the one that is most relevant and and that is what knowledge graphs are all about if you want to read up some more information there is a whole use case section on our website it talks about how you can you can use knowledge graphs to get contextually rich search results but knowing that in essence that is the definition of a knowledge graph I hope that explains why we wanted to apply this concept to the problem that I just exposed if you look at the concept of the knowledge graph and and I think data and craft databases in general a little more detail the picture I'm showing here with regards to knowledge graphs it is I find one that is extremely revealing it doesn't doesn't matter what topic you are talking about whether it's a Google knowledge graph or whether it's a knowledge graph about customer information or our slack knowledge graph as long as you take individual data points you just have your very very little information when you start labeling and and and tagging those pieces of data you start to have a first form of information you could start do a gregarious and counts of the various labels and and you could start having information looks a little bit more like what you do in analytics and data warehousing if you start connecting then all the dots and find the relationships between between the various nodes as we'd call it in a graph then you actually start to understand then you got start to gain some knowledge about about how these various points are related so I mean in in the Google example I gave before the PageRank algorithm that that explains how connected the pages completely changed the industry of of searching of searching the internet and search engines so that that relatedness and that connectedness is hugely important then as a next that you can start analyzing how two points within the graph are connected or if they are connected and then that gives you insights you would otherwise not get from any other type of database or any other type of system and beyond that point and then and that is where you move from insight to actually real wisdom is understanding which parts are most often traveled which are the shortest paths which are the most efficient paths how parts are changing over time that is really how a graph database works and and and how it relates eventually to building an efficient and efficient or or a good knowledge graph so why use neo4j for a knowledge graph there's a couple of technical features or elements that that are relevant the first one I just spent time explaining the ability to store and retrieve connections that provide you wide context you can you can join from any piece of information to 15 joins out across your knowledge graph and find stuff find things that are related based on the type of relations you want to follow and the type of search you want to do so it gives you a wide context and and it will do that with high performance because a graph database just leverages relationships really well but the graph database is also is also schema free we cannot just store data as you do in an entity relationship database it is schema free and it allows for unstructured data which obviously if you're dealing with text and ontology etc works really well so you can bring bring in both structured and unstructured data and and evolve the model over time new 4je also offers a wide range of free text search capabilities again if you're dealing with textual data super important you can look online there's various blog posts about how we deal with taxonomy x' ontology semantics again if you're doing context search these things are important and then last but not least and talk about this later we've got a fast algorithm vast library of algorithms to actually mine the graph and find parts within within the graph that that are important that is why near for J's is is good for knowledge graph a couple of use cases that are out there I'm not gonna talk too much about these but if you want to read up on some of our well-known use cases in in the knowledge graph area look at the NASA look at the NASA video on our website they've they fill the knowledge graph storing technical information from all missions since fifties or the sixties and and that knowledge allows them to well really do space missions faster and shorten shorten the the cycle to actually get I don't know get to Mars or whatever the next place is we want to get to Cisco knowledge graph is more around customer data in and sales oriented they've uploaded all the they've uploaded all their proposals and customer related presentations they claim it saves the millions of hours in order to to gain new customers and pull new proposals together look at the look at the online videos I won't talk about these ones too much so let's get into the slack knowledge graph after this introduction as I said it's a dog food project we've got quite a couple of dog food projects in the company we're doing analysis of our own Zendesk issue reporting you're not a screenshot at the top form that one we use it for we use uses for account mapping and understand who are the various people that are that are on the new on u4j accounts and opportunities we use it for our internal we use it for our community website and we use it for internal document search and now we also use it for this dislike knowledge graph at this point I'll hand over to to the guys you've actually done the work and all I'll let Niels pick up from here and explain you a little bit what data we load it into the Select knowledge graph and and what the model looks like so you get a bit of understanding how this really works Niels great okay thank you young I'm going to get a bit more in-depth into the data model and how we executed this project see if I can switch slides right so yeah as the untouched on before this is really a two-faced project the first phase was answering the what and what people are working on and which clients they work on kind of a search capability on topples slack as well as Google Drive and Salesforce so the magic of the knowledge graph in this case is also to combine different data sources and provide a unified view on these and the second phase of the project was to build on top of this knowledge graph and investigate remote collaboration right so what I want to kind of go into depth here is that we can very easily reuse an existing knowledge graph and expand on it to answer new questions right okay so this is the data model that we used for our project I'll quickly describe this we try to convert the data model of slack into a cross representation so what that looks like is you have your messages in the center and the messages contain references to links attachment threads and they can also have emoji reactions right people write select messages and people are kind of channels and what we additionally did is we used we define a little skills and realize that if analysis clients their wave internally as an aggregation layer on top of this graph so to give you a bit of a more visual representation of what this looks like we take a message on slack we have a message here with an attachment on it and we convert that into a graph format another message with a reply here we see that the reply becomes a separate note in our graph as well as C emoji reactions the [Music] the thumbs up here so by mapping this all these little messages to grass will get this giant intertwined knowledge graph of our entire slack and that is that is by itself is already super interesting because it allows you to search contextually search along this graph so that was enough for our first use case and then adding the aspect of remote collaboration we wanted to extend this graph with team information so we took we took some data from our internal internal database and we extended it into the craft so we changed the model to incorporate a hierarchy of teams so we have a little cycler statement here that finds the managers for the separate teams and our new model would then have another node here at the bottom that has a team for every user and that's one of the things that's also quite nice about new for J's that we don't have a static model it super easy to extend this and add this to your knowledge graph and with this team information we can answer new questions like how do certain teams collaborate and how how often do people between teams talk to each other so that's that's a bit how we evolved from an initial graph that just answer questions about skills to a an extended graph that can be used to analyze how teams work together right I'll hand back over to Jung to introduce the demo part okay before we get before we get started I do want to highlight that the demo you'll see is anonymized you won't see real neo4j employees customers and and you'll only see partial partial data actually if you I'm not sure if you guys will notice if you look carefully at this you'll see that the data sometimes incomplete there are gaps that's deliberate we've tried to anonymize the data as much as possible using some of the techniques that neo4j allows you to because there are certain restrictions on what you can do with private data and and that applies both to to the development work as well as to what we what we rolled out so during development time we've worked with a test server with anonymized partial data in in in real life working on this with with with the end-users and we've we've rolled out to a limited set of end-users initially to start with because again we want to be careful about how we use this we only we only exposed initially public channel data so anything that's on public slack channels for those who are familiar with slacks like has the concept of both private the slack channels and public ones we only do the public one so data that anybody could see anyway we only expose aggregate data via the application there is no way that people can actually get to the detailed slack conversation okay so we'll explain some of the internals but you will not see that that's anonymized because obviously there are some rules you need to respect in real life and I'll talk more about that when we get to the implementation side so I just want to do I just wanted to add that before we dive into the demo we'll get into the demo the first part morgan will introduce he will show you the application we built and that allows you to define skills within the company see how those skills relate to customers and kind of assess the overall situation of a customer which documents are being used who's working on who's on the virtual customer team and so on that's the first part in an Morgan will hand off to Niels who will show a couple of examples of more one-shot analytical queries which are more in the area of mood shifts and sentiment analysis ok Morgan I'll hand off to you alright so we just start by showing my screen hopefully you can ho see that now so this is the application we built on top of our new 4G database is so if you look at this on dashboard it's basically give you a company overview of how your company is going and you can see on the left side the top skills based on the number of time with a skill has been mentioned and you can see this kid liking which is calculated the same way we calculate the top skills so the inside of these skills are liking and top skills in in your company is helpful because if you have some activity within a customer let's say I run Cassandra for example you can see that we're all liking competencies in Cassandra for example and so we you might want to have or give trainings or to have some model your to run Cassandra for your employee on the active customer well as I just said it basically gives you insight of which customer or how having activities and which one you want to probably focus with in the current month and on the active user it just give insight on how your users and how your home for your talking to each other out there working remotely in this context if you if we just scroll down a bit you can see some trends so on this you can see the trends on your company how your company is going on the remote context how they are using your instant messaging tools so in this case it's slack and you can see some trends around the skills so you can see here that has the back up and browser skills are going down by 50 and 40 percent or which is for someone that don't know already is our cloud provider it's going up to 25 percent so once we figure out which skills we are liking and which customers or have some activities we might want let's say to load some JSON within the Cassandra database but we don't really know how to do it because we don't have the competencies well with this skill dashboard I can just look for the skill Cassandra and then I just want to load this and I just I just said right so I can just look at for reason and it what it does it will basically look for every person in our new budget database that have both Cassandra and Jason skills and if you have like an emergency questions you really want an answer right now so the way you can just filter on your current time zone right and what it does it will basically give you the list of people that have this case that you are looking for a real nice feature is that we integrated with the country website as well so if you are looking for skill set you will also find related articles that you can click and look for so either just say I wanted to load some JSON on Cassandra this article might be useful so that said right now if let's say I have a question on this JSON loading in Cassandra I want to conduct Michael but I want to have more insight on what does he know around that I can just click on Michael and it will go to the user dashboard where I can I can see more matrix around Michael what skills do they have what skill does he have that also having the company and some metrics on how those skills are calculated so every skills you can see how to score the score will be explained by news later on it's a skill algorithm that we use to calculate it if we go to the customer dashboard now so if first I go back to the home dashboard we saw that there are some activities within a bank and company right so if I go to client and I want to know more about this company I can just look for Z customer and you will have this dashboard right so what we can see here on this customer first is the activity on the slack channel which you can see two pics rights and what it gives as an insight or that in March there was some talking around training and if you look at the trends it will more specifically on link straining and as we deliberate in March the need for training only grows when done right and you can see the last big here that is actually increasing since the 20th of March you can see the trends for this report that is increasing as well so you can match what all the activities and were based on the skills on the skill set you can also see the team members that worked for this customer so it's not a team as within who report to the same manager but really the actually remote in so every one that will involve in this customer right so it's really convenient if you have a question you judge on the project and you have a question you want to know more about something you can reach out to those guys with some integration with slack as well and I said as I said earlier we have some Google Drive integration as well so you can quickly go into the Google Drive folder for this customer and find some helpful documentation now talking about the connotation that the last out that we we breathe this gives you some insight of how documents and especially documents within the customer or used within slack and how you you are sharing knowledge especially in this remote context so the first friend here show you dukka dukka most all shared per weeks and this this is really showing actually that we are getting higher and higher on sharing documents it's also give you insight of where you are liking knowledge for specific customer with this this table here you can see clients that don't have a Google Drive folder or client that have a drive folder but without any documents which is really helpful to know which customer you don't have more insights on the document type you can see how the command type or used and which document or or shared within your channels now this one is kind of important as well especially on the skill knowledge right you might want to know which skills have the more documents especially on best practice and which one do not have any documents all the list document right so that way you can say okay we don't have much document on how to export maybe and CSV so maybe we want to write a best practice around it right if we have some free time and also if you're looking for a document or on a goof example or on a database or broom you know that we already have few files on that the larger duplicate files is there to show you how you're using your slack plan basically and the mosha document is a way of saying what skills or what documents read it to which skills are important because they all shred the most time basically and thats it for the demo now I will just hand over to Jana there to Neils sorry right so besides this dashboard we also did some one-shot analytical craze on on our knowledge graph and these provided some some insight and some more detailed questions so something that we haven't touched on before is that we use sentiment analysis for both the messages and the emojis that are used in the slack on Birkett conversations and what's nice about that is that you can aggregate these sentiment values over time so a nice example here we're looking at everyone all the employees that are based in London and we're looking at how their sentiment values in their conversations evolved over time and what's what's interesting to do here is once you have this kind of graph is you can identify the peaks and the dips and we can actually link them to certain activities in the company so the first one here all the way on the left is when I beloved a teammate left there was a significant dip in the happiness of the employees what the biggest peak all the way at the end is when I was I think couple days for Christmas when everyone was fishing each other and Merry Christmas so that resulted in a really high sentiment sentiment value in the middle here you have a bit of a weird dip relating to the launch of our new product aura which was due to that some emojis can be used both in a sad setting and in a happy setting so these are the ideal hash cry emojis which are sometimes used to represent crying but also to represent happiness so that's that's a bit of a weird artifact in her in her data here but nevertheless it's very interesting to see that certain events can impact impact the sentiment in a company this goes to the next one so what was also interesting aside from geographical aggregation you can look at a certain team and what was interesting to see here is that once our company started making announcements about coronavirus and closing the offices we saw also a significant dip in the members of the european sea so in situations like this this may provide some insight as well into what what is causing changes in people's moods and which teams are most affected by these kind of measures and that's that's exactly what we're doing here this it's going to be a bit small but this is an ordered list of all the teams that work in Europe and how much the average sentiment of their messages has changed over time so at the top here we see that the customer success team has on average been became the most more negative after the acidic coronavirus announcements and the developer relations team has has become a lot more positive so this is an interesting way to see how how and where measures like this affect the team's what's interesting to see as well is I think Jung touched upon this before is that communities within a company teams within a company are not always the explicit teams that were part of but people that communicate frequently are implicit teams these are teams that we don't define but they do exist because they're working together on different things so what we use is we use the need for J data Science Library to do a community detection algorithm based on people that frequently reply to each other and we group them in different teams and that's something that you can see here this may provide really interesting information about which people collaborate across teams very often in which people do not have this this particular trait and then the next level of analysis that we can do is we can look at cross team collaboration so we look at all the people in a given team and we see how often different teams within a company collaborate so or do you see here is you you can see thicker relationships between the teams here at the bottom and it seems at the top so that would indicate that these teams have a stronger collaboration measure than the other teams to kind of visualize how your company internally works together so these are all very very interesting things that you can discover with this kind of knowledge graph right I'm gonna go head back over to young to give a quick summary of everything that we've done so far okay so I hope that them all has shown you some of the things that we talked about that at the beginning how you can find right skills within a specific context combinations of skills a skill without other skills etc how that enables people then to connect to peers that are in the same time zone and that can help them achieve chief whatever whatever their goals are that they understand what are the virtual teams yeah I think that I think the demos also shown a little bit how we can actually collect more info that that is that could be relevant for a wider customer 360 initiative or a knowledge management initiative I'll talk again about it in a second but but if you look at the screen that Morgan showed where we where we show which are the documents that are used most in the context of a specific customer or what are the skills that are relevant to a specific customer those are those are elements that are very useful to our support teams for instance we've shown examples of how you can measure activity across specific customers or in teams how you can measure the moods changes and and how we find virtual teams or implicit communities I hope that all came across I see make a small parenthesis I see a lot of questions on the right we're never gonna get you all of the questions on the webinar we'll try to find a way to look to answer some of these later on okay beyond beyond remote work there's a if you've been listening there's a lot of other use cases possible here for sake of time because I see where we want to go through some of the technical stuff still we only have 15 minutes left I'm not gonna elaborate too on this too much but there's a whole lot of other use cases that the same graph or very similar graph could be used for look look on the use cases on our website there is plenty of inspiration there but but if you've understood the model and and and take inspiration from what we shown here I'm sure you guys can come up with a lot of other use cases so so let's take another five or so minutes to do a bit of a technical deep dive and then leave five minutes to close out Morgan deals I'm gonna head hand back over to you so Morgan's going to talk a little bit about architecture using Ketel and in the front end we build Neal's going to talk a little bit about scoring and sentiment analysis as well as some of the graph data science that we apply on the architecture side you can see on the left side various data so see that we used so again we use slack here because we are using slack internally but you can use pretty much everything you want as long as you can export the data or reach its rest API and we use Google Drive as you saw in the demo because we wanted to integrate and see where the knowledge is within your drive we use such force to integrate with our customers so we can say we can link actually channels in slack Google Drive folders and with our customers inside force and we use github for looking for skills in our repository basically so once we define all the data sources what you want to do he is load them into new 4G and how we did that on this demo he is by using alcian mansion cater and Tyson so catered is an ETL tool they're all values of the tools which was this one and I will go in for the details right after this architecture slide why we choose that we use Python for the sentimental analysis and especially using nmt and to load some data as well and I will explain that later on the new Fuji cluster we are using a coastal cluster hosted in new Fuji or rare so for the one who don't know what he is it's actually how a cloud provider so you can just click and quickly have some databases help and running in aura which will be on the cloud and you don't have to manage it you can just create a code across and use it straight away so once we got all that and we get the data on the new 4G cluster we build a fairly light front end because as we mentioned earlier we are the latest company we wanted the focus to be more on the database and unloading and analyzing the data so in this one we used view GS and it was an application built in a new 4G application so we can integrate with a new 4G bloom so that's it for the architecture so now I will just go through every details of what tools we used so ETL using cater which is catered because because it gives a lot of flexibility actually so the first thing is in catered you have new 4G connectors rights and what it does it basically it's a workflow castration so it gives you the ability to say okay I just want to start from here I have my data here and I want to extract them manipulate it so I want to aggregate manipulate data transform it do some filtering and that's where we did the anonymization actually and then load it into neo4j but it's not the only thing actually with kettle you have you are able to pass JSON you are able to ended Python and that's where we can actually say okay you have supplied some Python scripts which are running something on analysis but we want that to be part of the workflow so you can just take your Python script and put them in character obviously we did some logging and error handling some restart ability as well so you can result from any point where it previously failed and we'll just explain that on the next slide so how does our catered workflow looks like so the the main is 0 for bata actually the first part is to create a batch process as the batch process is a way of saying I I'm starting an ETL process I'm starting to load some data and that would be a process where I want to list all the files that are available and I want to load them right and once a batch process is done we can just close it and as long as you want to apply to update all the data you can just create an exit direction so you can see on the screen on the slide you have the first batch which is a green note in new 4j you have a next iteration and you have another batch and it can go that way all along as well as you want as long as you load data right so once you created a batch what do we need to do is to basically list all the input files that you want to load plus the one that's fade previously if Haining right so if you have a previous run and for some reason you were not able to load the files we will flag them as fade with with a new project labor right so that way we can keep track and we can have the ability to restart wherever it failed before once you got your batch and all the files attached to that batch you can just loop through every files and load them into your databases on the front end side so as I said we used huge yes why because it's modular and it has a flexible development of your months as Jane mentioned earlier this was light font and we were just more focused on the backend obviously if you want to move forward into projection we would highly suggest to use grand stack because that's our recommendation and a solution that is much more flexible and give give you more features heart of it and you can see on the slide as well we use UGS because we wanted to separate the design from the code so you can see or there you have the font and cold and then you have the backend which is a cipher queries alright I will just over to News Now on the sentiment analysis running low on time but I'll quickly elaborate on how we did the sentiment analysis on the text as well the emojis for sentiment analysis in text we used a Python library called text block that will return a value between minus 1 and 1 depending on who would call it sentiment the happiness of a given sentence the emoji sentiment is based on a research paper that we found where they analyzed a large number of documents online and tried to correlate the emoji being used with with the sentiment value so what you can see here in this visualization is that the heart all the way on the right over there will have a very high sentiment score as well as a very low neutrality score a neutrality is here how objective a emoji is we didn't use this in our current implementation but this is something that that paper investigated as well very quickly on the data science options that we used we use the recently released new 4j graph data science library to do a community detection there are a lot more options here to get more in-depth but we just wanted to show that these kind of things are very possible with this library so if you have some time to check this out this is really really interesting right I think I'll hand over to young and we should be done pretty soon thanks Niels so we'll talk a second about implementation I'll really keep that one short so if I look at the internal project we initially did I think getting a getting a fee one out of what we builds took us roughly four to five weeks if we structured is in in other approaches with with customers where we build knowledge graphs and then some other similar things obviously there's more of requirements analysis to be done and there's a little more of discussion to be had but but typically we're talking implementation time two to three months the way we do it is shortly outlined here I'm not gonna go I'm not gonna go dig too much into project management methodology and how we do this key element is our our approach is very agile and very data-driven you have to validate early and fast whether whatever you're presenting is is correct I think those elements are key but but the most important message is is that and I see that on the slack I see a lot of people saying there's a lot of complexity here but neo4j plus a couple of other techniques that we've used really allow you to do this type of knowledge graph in a very short time frame will it be perfect after after four or five weeks no I think if you start building a knowledge graph like this it is something that you can keep on tweaking forever and enriching what are key elements during implementation is is first of all getting to the data if you don't have the data that is a challenge and I see there are some questions about data extraction from various from various messenger tools see whether we can get to answering those once you have today that you have to consider anonymization you can anonymize you can sample but you have to protect the data there are rules around JDP our privacy HR policies which you need to respect during development time and after rollout and and eventually if you want to deliver fast you need to narrow down the objective there's a there's so many things you can do with this type of knowledge graph you've seen you've seen us at least answer five or six different questions with the graph that we initially build and in the first iteration we built in like five weeks or so that those are those are elements you absolutely need to be careful about if you get right you can do this really fast the last element is more about how are you gonna use this roll this out embed this into your organization we deliberately did not spend a whole lot of time on that because that's gonna be different for every customer and and and and the use cases that you prioritize so I'm not gonna go too much further in these these details anybody interested in in implementing something like this please reach out to us we're happy to talk you more to through how that would work and um that brings us to the end of this brings us to the end of this seminar please reach out any technical questions you have questions we've not answered yet if you want assistance with implementation whether that's in a workshop or or assist and us assisting you to do this just let us know contact details are on the screen we would love to talk to you guys 