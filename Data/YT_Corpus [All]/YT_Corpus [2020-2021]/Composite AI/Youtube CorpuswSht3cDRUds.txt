 (upbeat music) - All right, welcome back from the sessions. I hope you have been having a great time so far. I hope you've used the Q&A and you've been asking questions of the speakers. And I hope you've gotten a sense of all the content, got to view all the content that you've wanted to see. Don't forget to visit our sponsors. Like I said, that's not just so they can feel happy about their sponsorship, but there might actually be legitimate reasons you wanna talk to those folks. So check them out. Don't forget to visit the Swag Store. You've got points to use in there for Swag. All of the fun of an in-person conference is not gone. There's lots of fun things to do in this virtual platform. So get in the Swag Store and check that out. Also, I didn't mention these folks before, but under the Interact menu item in the Summit Platform, check the Confluent Community Catalysts. These are people who have made outstanding contributions to the Confluent and Kafka communities. And some of them are also program committee members, some of them are people that you might even recognize in the program. These are people who spend a lot of their time sharing the knowledge and experience about Kafka that they have with the community. And I just like to recognize them for that. Also, don't forget, go to the photo booth, tweet your photo with streaming selfie, take your own selfie with your phone, get the dog, get the cat, definitely get your kids, your family, anything you feel comfortable getting a picture of. I just love for this to be as much fun and for us to see each other experiencing the summit online. And with that, next up, I've got another keynote for you. Let me tell you a little bit about it. As you know, getting systems to work together is one of Kafka's core competencies. It also happens to be a primary task in healthcare, where interoperability between various legacy systems is not just a good idea, but sometimes it's actually the law. Ensuring data is connected, put into context, use to power new applications, and all in real-time is never an easy thing. And when the stakes are life and death, literally sometimes, and real-time means information is available at the point-of-care when a patient is in the room, event streaming becomes just that much more important. So with that, I'm thrilled to welcome Levi Bailey. He's Vice President of Cloud Architecture at Humana. And he's gonna talk to us about the role that Event Streaming and Kafka play to drive new interoperability standards in his enterprise. He's gonna explain what those standards are as well. Listen carefully to Levi. He's not just telling you how he solved some problem in his domain, which would be good in itself, but he's also covertly coaching you in how to express the value of a widespread Kafka deployment in terms the business understands. So that might be a technical goal that you have, but the skill of explaining that in terms of business value, that's a skill you want. So listen to Levi. (upbeat music) - Hello and welcome to Kafka Summit 2020. My name is Levi Bailey. I work with Humana as an Associate Vice President of Cloud Architecture. I'm happy to come and talk to you today about how Humana is using Event Streaming to deliver interoperability at the point-of-care. Before I get started, I wanna talk a little bit about my history. I've worked in technology for 20 years. And I've worked with a lot of successful teams. I've also work with some teams that were not as successful. One common denominator that I found between teams that were very, very successful was that they fundamentally understood the business problem all the way from the developers, junior developers through leadership. So before we get started, I wanna talk a little bit about the problem that we're trying to solve. And then we'll start talking about some of the technology that we're using to solve that problem. Let's talk a little bit about interoperability. Interoperability in healthcare is defined as where we communicate across the different participants in the healthcare ecosystem. The U.S. health care system is made up of three groups, consumers, providers, and health plans. Interoperability is the ability to communicate data between these three, separate entities in a standard seamless way to enable a better healthcare outcome. There are three organizations that participate in the healthcare landscape in the U.S. health care system. Consumer-based applications, providers, and payers. Each of these systems focus on different aspects to administrate healthcare. In the past, they really focused on their specific task. But what we found is that each of them overlap. And we have a need to exchange data to be able to create a better experience across each of those different solutions. The reality is, interoperability is about data exchange. Now, we've been exchanging data for a very long time, but it's really been point-to-point solutions. When we think of a better healthcare ecosystem, we really need to think about the opportunity to exchange data in a seamless way where all participants in the ecosystem can freely exchange that data and integrate to that data to help drive the outcomes and the experience within their organizations. The way we do this is by adopting to an interoperability standard. Interoperability at its foundation isn't just about how we communicate, but it's also about what we communicate. The standard around interoperability for healthcare is FHIR. FHIR stands for Fast Healthcare Interoperability Resources. It's based upon a concept of a standard way to communicate being API-driven. And then a standard way to communicate a specific subject based upon the FHIR resources. Humana's central strategy is around integrating and being a leader in interoperability. Why is that important? When you look at the diagram displayed, you see that we're already near the lead. We're right behind Kaiser Permanente. When you think of Kaiser, you really actually don't often think of them as an insurance company. You think of them as a provider. And that's because they're a closed network. Now, Humana's strategy, when we think about improving healthcare and we consider the ways by which we can create better outcomes, is to transition ourselves from an insurance company with elements of health to a health company with elements of insurance, which would be much more akin to the experience you would get with something like Kaiser. But we're gonna do it on a grander scale. So by adopting interoperability and continuing to lead in that market, and invest to get to a leading status in the market, we have the opportunity to really make an impact to the overall experience and outcomes within healthcare. In order to achieve a leading status, we have to be fully event-driven. Let's talk a little bit about some of the technologies or the approach that we're using to develop an interoperability system that will be able to lead in the healthcare market. So Humana has invested in a platform approach to delivering an interoperability solution. We feel this is best positioned us to continue to evolve and guide ourselves towards that leading status. There are a couple of design principles that we put into place when designing and building out this platform. I really wanna highlight a few aspects of that to describe how do we transition from an insurance company with elements of health to truly a health company with elements of insurance. Humana has invested in migrating towards a value-based care system. That means instead of paying people for services, we're trying to deliver payment and adjudicate insurance based upon the outcomes. In order to do that, we have to think of it more from the patient perspective, the consumer perspective. So one of the core design principles to an interoperability system or an interoperability platform is for it to be consumer-centric, health plan agnostic, and payer agnostic. That's very important. Second, it has to be built on standards. If we're not basically adopting the industry standards, we will not be able to create an ecosystem where we can freely onboard additional payers, and providers, and consumers to create a better overall outcome across the lifespan of someone's healthcare journey. It also has to be cloud resilient and cloud scalable because if we're gonna be participating in an ecosystem that continues to grow as more and more participants began consuming the services and the capabilities that we deliver, we can't do that in a data center mentality. We have to adopt the power and the scalability that comes with the cloud environment. It's gotta be event-driven. If we're trying to impact the point-of-care, if we're trying to allow for the healthcare journey to be interrupted when it is important, or when it will drive to a better outcome, we can't have latency in the system. We have to be able to notify and make data available when it's important, not after the fact, not waiting for you to ask us for something, but telling you when something has changed, or when something is relevant to what you're doing. It's got to use analytics. It's gotta be predictive, it's got to understand, it has all this data, if it's just exchanging data, it's not creating an additional value. We have to be able to interrupt the healthcare journey to create better outcomes. We have to be able to look at the data that's coming in and determine when that creates a new event or describes a new insight that can be made available to improve that outcome. And it really has to be built in a product-centric mindset. It's gotta use micro-services, it's gotta be, we can't wait to have the complete solution there. We have to build it in an iterative fashion. We've got to add new value over time. This is the only way that we're going to be able to impact and deliver value and continue to invest in to get to the point that we are of a leading status. We have four components that comprise the Interoperability Platform. We have an API Management layer, we have an Event Management layer, we have a Knowledge Graph, and we have an optimized data layer. Each of these components allow us to create an ecosystem by which we can lead in the interoperability space. At the basis, again, interoperability is based on the FHIR standard, it's an API protocol. So foundationally, we have to enable APIs. But if we're gonna lead in the space, we have to use event management. Because again, it's about having the right data at the right time. So by having an event management framework, we can publish additional things, including data, insights, and events. Now, in addition to that, we have the knowledge graph. The knowledge graph becomes important because we're exchanging all this data from different participants in the ecosystem, it's important that we derive additional insights that will help improve the healthcare journey. And so the knowledge graph allows us to make those connections. And last but not least, we have to have an optimized data layer. If we don't have that, we won't be able to meet the demands of having data available for use at the point-of-care when needed. We cannot put the constraints on the source systems to be able to create the resources that are the standard of the interaction. We want to be able to do that within the platform and have them readily available for consumption. Let's dive into how you to create those different components. We are primarily a insurance company. We're not a technology company. We have technologists enabling our products and our services, but fundamentally, we're not a technology company, and we don't want to build technology. We wanna orchestrate technology. We wanna take the best-in-breed technologies, bring them together to solve a problem. In this case, we have worked with different partners and different solutions to bring them together to create the platform that we just talked about. The current iteration uses a couple of different technologies that I'd like to highlight. One is it has an API management layer and it uses API Connect to allow us to manage and administrate the APIs that are produced and deployed through the platform. It has demands for compute. It has to be able to serve the API request, as well as process the data that's coming in via event streams and data pipelines to optimize and create the resources that we're going to be exchanging with the participants in the ecosystem. For this currently, we're using GCP Services. Those include things like App Engine and Compute and GKE or Kubernetes services. In addition to that, we use StreamSets for data pipeline and to basically maintain operational pipelines that serve the data to the optimized data stores from the system of record. And then the engine really running that exchange of data is Kafka. This gives us the ability to published data from systems of record and consume it as those events occur, the changes of data, whether they're insights, updates, deletes, and take them to bring in multiple topics across data sets and systems of record to create the different resources represented in the interoperability landscape. And then we use an underline optimize data store to cache and store those resources so that we can serve them up quickly to the consumers of the ecosystem. As you see, that you end up with those multiple participants. You have the operating platforms that are generating data, and then you have the consuming platforms that are consuming that data. Now, the exciting thing about this is, it's not one directional, it's actually bidirectional. We allow for data exchange to go both ways. Let's dive in a little bit about how you orchestrate these technologies within the platform by looking at the Solution Blueprint. This is an example, and actually it is documented in the next section where we'll talk about one of the successes. In this case, it's about medication data. We have systems of record at Humana that basically manage the medication data for our members. It keeps track of what medications the member is currently on, along with what allergies they may have. And then we also have other systems that maintain the immunization of that member in addition to other pharmacy-related data. So what we've done is, again, those systems of record are doing it from their perspective. It's the operating platforms, their basis of the view of the world. And we want to take that data and create a combined resource that represents one of those FHIR resources, or the participants of the ecosystem to be able to take advantage of and use it at the point-of-care to influence the care of the member or of the patient. So what happens is, these systems of record publish their data to a set of topics. Again, these are events that get published as data changes, and as we're made aware of new medications that are added, which they can get from claims information that comes in, or whether there's a new allergy that's documented in an EMR or within a medical record. And then as well as immunizations and other relevant pharmacy-related data. This data flows through a set of topics. Now, at the end, we have a consumer within the platform that is taking that data, reading those topics, and then creating that combined FHIR resource. And it's storing it in an optimized data store so that it can serve up based upon the demands of the consumers of the platform. That API is then exposed through an API management layer. And then the participants in the healthcare ecosystem can subscribe to and consume that API. Now, as you see on this blueprint, we also are already showing the capability for us to publish an additional topic representing that FHIR resource that they could subscribe to. So it's no longer an API integration where they're making a call out to us and saying, "What do you know?" It is us letting them know when something changes. Now, that is how we evolve. So when we look at where we're currently at, we're still in our infancy stages of that interoperability journey, or we're in the middle. But it's really based upon those APIs and consuming those APIs and that response requests integration. But the next step on the external or on the consuming side of the platform is to not only provide the API integration pattern, but also to allow them to subscribe to the composite of it, which is, or the composite record around medications or whatever that FHIR resource may be. All right, let's talk about some of the successes that we've recognized over the course of the last year and a half since we began this journey. Some of them are very exciting and we've seen direct impact to outcomes and to our members experience by enabling this journey. So I have two really good examples. And again, I'm calling out some partners here where we did the integration. But the reality is, these are reusable components. They're reusable resources. We are actually gonna be onboarding some additional partners to these exact same resources, they're gonna be start leveraging them. And we we've already began that journey. And in the process of getting things established and them to start consuming this industry standard. The first one is really an exciting one because it was recognized for a CIO 100 Award. From my knowledge, it was the first type of integration of this nature within the healthcare ecosystem. So again, it was groundbreaking and it's directly impacted our member outcomes and member experiences over the course of the end of last year. And then it is accelerating and having a greater impact this year. And that would be our integration with Signify Health. This is really represented on the previous slide where we talked through the Solution Blueprint of how we're leveraging the different components and technologies within the platform to deliver a set of APIs or capabilities that are going to start shaping the ecosystem and allow the ecosystem to begin functioning and building out improved outcomes. With this one, what we're really targeting is being able to adjust the experience of a member when doing a health assessment and evaluation within our member base. We primarily work in Medicare. So that's our primary market. And in Medicare, there are some processes that you follow to basically make sure that the health journey for a Medicare member is being managed the right way. And one of those is really the health assessment process. So we send in our partners, like basically, there's a clinical partner that goes into the member's home and does a health assessment of the member. Now, you may or may not know this, but it's fairly obvious. If you have a chronic condition and you're not taking medication that's prescribed to you, you're not gonna have a good outcome. There's a direct correlation to medication adherence and good medical outcomes for individuals whenever they're dealing with a chronic condition. There is a section of the health assessment that can be relatively painful in the past. And that was the medication assessment because they have to, and many times they're going into a home with an elderly patient, and there may or may not be a caregiver there, and oftentimes this is that they're either sick or have some sort of condition, and they may or may not have no knowledge of exactly what they're taking, dosage information and all of that. And they've talked to their provider, their primary care physician. And this information should be known. And the reality is, the clinician that comes into the home has to start the conversation with, "What are you taking?" Well, that can be a long drawn out experience because they may or may not know that information. And the quality of information they have may not be sufficient for what they're trying to drive for in the assessment. So then the care provider has to go through and try to find the pill bottles or whatever else and document it. And depending upon how many chronic conditions they have, they may be on quite a few medications. By enabling the medication profile resource via our Interoperability Platform we were able to share with them the information that we have, which we as the payer actually most of the time, if not, all of the time have better information than the provider that's going into the home because we have the claims from filling medication. We also have the medical records from what was shared from their primary care physician or other places that we can bring in, mine the data, put together this medication profile and have that available so that we can share with them, here's the medications that they filled, here's what they've been prescribed, here the dosage information, so on and so forth. Now, the conversation really changes. They can just validate with a member, "What are you taking?" And they can also provide back to us any updates to that, because this is bidirectional as we mentioned. And we can then reconcile that information, update our records if we feel and deem that we collected some additional information that's relevant. And now that we have that up-to-date information so that we can do some additional benefits for our members, such as looking for drug-to-drug interactions, or other things, or even finding them cost benefit by saying, you know whether they've been prescribed this and they're taking this, but the reality is, there's a cheaper option and should we notify them. So there's a lot of secondary benefits that can be generated by having this exchange of data. So again, that's a really exciting one because it's directly impacting the member experience. It's actually not only the member experience, but the provider experience. It's making their job easier. And they're getting a better overall outcome of what they've been tasked to do. In addition to that, it's bidirectional, so we're getting data back, so it benefits us. So around the entire ecosystem, we're seeing a benefit by creating this integration. The second one is a little bit more interesting as far as what's represented here. This one is more innovative. So we talk about the FHIR standard being that standard interoperability definition that allows us to create this ecosystem we're talking about. The reality is though, it's still evolving. It's using some basic concepts. It's been a lot of time developing the set of resources that seem to be the most relevant and shareable across all of the ecosystem and all the participants. But what we find is, as we start looking at opportunities, we see things where maybe the FHIR spec in its current iteration does not have a direct support for. What we're able to do with Sound Physicians is, we were able to take the FHIR standard and add to it or innovate an additional capability there around a situation where we have a physician group that does not directly have knowledge that the person they're working with is a Humana member. So we get into data exchange, constraints about PII and PHI. The member doesn't want their information shared with Humana if they're not a Humana member, and we do not wanna share information back with the provider of one of our members, or basically receive information about somebody that's not one of our members because that would be a data breach constraint. So what we did is, we were able to use, again, Event Streaming to publish seven Points of Light and create a Hash out of that, a Standard Hash that we were able to cache in and optimize data store. This allows Sound Physicians to call our API, this FHIR API and pass in a hash value in from the seven Points of Light within the request. And then we send them back a member task profile that tells them the next thing to do if we match that hash and we've determined that it is our member that's our working us. So again, this was a way which we able to adapt and evolve the interoperability standard to address some limitations with the current set of resources that were available. And last but not least, I'd like to talk a little bit about an internal success, because we've talked about the overall provider integrations of the ecosystem. But this really is focused on the payer side, which again, we're a participant in that as one of the payers that would take place in our Interoperability Platform. And we have invested in Humana internally. Again, the entire point to doing an Interoperability Platform or basically any of the strategic objectives we have changing the way integrated care delivery is all about creating better outcomes and driving towards a consumer focus. Moving away from just an operations mindset for what we have to do, but how does the consumers of our systems and our services, how are they represented in that? And how are we work with them? And one of those paths that we went down to enable that was around adopting AI and machine learning to create and define a better experience for our providers. So providers typically when they have to do a EOB, that's Explanation of Benefits or a check on referrals, or some of these other things, call an IVR system. In the past, that was a menu-driven system, and it was a little bit hard to navigate and they're normally not doing this just for one patient or one member. They're doing it for a group of people. So it can be a long arduous process, but it allows them to do it without going out of their workflow. Because again, they have a system they're using and there's no way for them to directly integrate with us. So they basically use the phone to do that. What we wanted to do was improve that experience. Basically allow for a better integration and a better conversation so that they could get to their answers quicker, get through that process quicker, save themselves time and by proxy money. And so we built this AI system that allows us to answer those questions for them. But if anyone knows anything about developing AI systems, you know that the feedback loop becomes very important. You need to know how that conversation's going so that you can make adjustments to the AI system so that it can learn, and that it can become better at answering those questions, and reduce friction and produce better outcomes. So early on, they did this with standard technologies that we used in the past logging technologies, things like Splunk and other solutions. What they found was that, that had some limitations, there were delays in the process, it required a lot of manual intervention. And frankly speaking, it was from an operational perspective, from business's perspective, it was not a good experience and it caused a lot of errors in the process. They then once opt in the Interoperability Platform is available and they can start publishing data to the ecosystem, they started publishing these events, these conversational events, and then using that to drive, allow the analysts to have access to that data quicker and be able to get better results from the way that they review and maintain that data to make those decisions on how they're going to augment the AI platform to create a better experience. And they went to production in the last six months and they've noticed a longest change in the way that they're able to respond and react to all these events. One other aspect of going to a platform-based system is really the ability to react quickly. Because we're building these reusable components, and using this technology to orchestrate a unified experience with how we drive towards interoperability, we were able to react very quickly to the COVID-19 crisis. Now, we had an opportunity to take data from a couple of different integration points. That would be like our CRM systems or our call centers, as well as different agencies and state agencies and other organizations to bring that data in and generate insights out of it. So within a matter of days, we were able to create integrations via this reusable platform to start integrating and make data available in near real-time for those analytic platforms and reporting platforms to be able to generate insights and value off of that data to help influence the trajectory of how we respond to COVID. So it's a very exciting opportunity, and it really cemented in our minds the value of taking a platform-based approach to deploying these types of capabilities. With that, I would like to close with three things that you really need to take from this. And if there was anything else, this is what I want you to remember. Fundamentally speaking, these points are not specific to what we did as much as they are the approach to solving a problem. First and foremost, know your problem. You can't solve and build a good solution if you don't fundamentally understand what your outcome needs to be. You've gotta know the business problem. If you don't know that, all the way from your junior level developers to your business and to your leadership, you will not end up with a good solution. You are going to just be using technology, but you're not gonna be meeting the goals of what you're setting out to achieve. And then two, Event-Driven solutions are really the only way that you can truly impact customer experience. Fundamentally speaking, anytime that you're waiting for a system of record or an interface, some sort of customer facing entity to offer an experience, they're gonna be limited based upon what they know. And the reality is, if you want to truly change that experience and improve it and drive towards something that will impact them directly, that system needs to be made aware of the changes that are going on around them. Those events become very important to driving experience. So fundamentally speaking, if you wanna change experience, you have to adopt to an event-driven architecture. And then last but not least, what we've learned is that, by taking a platform approach, and by building these capabilities in a systematic reusable fashion, it really allows for a lot of adaptability. The reality is, markets change all the time, your business changes all the time. Situations, events occur, like COVID, where by taking a platform approach and building in a standard architecture that will support adaptability, you can really quickly adjust to what the current situation is. So fundamentally speaking, thinking in a platform, and in a capability-driven approach, will allow you to react quickly to change in your business. Thank you again for listening to this presentation and taking some time out of your day to consider our story. I hope it was a value to you and I really enjoyed sharing it with you. Thank you. (upbeat music) - Next up, Kafka Co-creator and Confluent Co-founder, Jun Rao, is gonna talk to Bhanu Solleti from Lowe's. If you don't know Lowe's, it's one of the largest home improvement retailers in the world. They're all over the place in the US. I have one just 10 minutes from my house. Bhanu is the Domain Architect for Enterprise Integration and API Management. And in his role, he focuses on the integration of various technology stacks that have, as you can imagine, all kinds of different applications of varying vintages. And at Lowe's, each of their 2,000 stores functions as its own mini data center. In addition to there being a central data center that the company operates itself and significant cloud investments as well. Bhanu and Jun we'll talk about the central role that Kafka and Event Streaming play in enabling the architecture team to respond quickly to things like shifting store hours, increased curbside pickup, and all kinds of other new business imperatives that have emerged during the pandemic. Jun, take it away. (upbeat music) - Good morning, good afternoon, good evening everyone. Welcome to Kafka Summit 2020. Today, I have with me Bhanu Solleti from Lowe's and wanna talk about Event Streaming at Lowe's. So welcome, Bhanu. To get started, maybe you can tell us a bit to ourselves Lowe's and what do you do at Lowe's? - Good morning, good afternoon and good evening everyone. Thank you for this opportunity, Jun. So Lowe's founded in 1946. And Lowe's grew from one small town hardware store in North Carolina to one of the largest home improvement retailers in the world. We are committed for the homeowners, renters, and the brokers to improve their homes and the communities and the businesses. Together we deliver the right home improvement products, the best service value across every channel community we serve. Every aspect of our business and strategy is rooted in this direction. If you talk about the numbers, we have like around 300K Lowe's associates that serve the customers and communities. We have like 80 million customers that we serve every week. And we have made around $72 billion of sales in 2019. And we heavily invest in our communities. Around $42 million have been invested in 2019, and recent pandemic, we have invested around $30 million. At Lowe's, I'm responsible for the roadmap and strategy of enterprise integration and API management. Focused on multiple integration technology stacks that integrates all different kinds of obligations from modern to legacy present across Lowe's data centers and various cloud providers. Kafka Platform has been one of the key area that we have been heavily focused on from last two years and to enable real-time event streaming architectural patterns at Lowe's - That's great. So could you tell a little bit some of the use cases of Kafka at Lowe's? - Sure, so we heavily use Kafka for real-time event streaming platform, as a real-time event streaming platform. So we would want Kafka to provide us highly scalable, durable, and reliable infrastructure for us to onboard all business critical use cases. So we needed a real-time event streaming architecture to improve the time to market for new apps and reduce the time it takes to stand up and build new clusters, and transform couple of key use cases. Confluent Platform is really helping us, enabling us to move the data in more flexible and reliable way. At Lowe's, Kafka is becoming a very core company and a vital platform to connect our systems to move the data across data centers and across the cloud platforms, on-premise, and the brick and mortar stores. So digital business is completely based out of microservices and running in GCP. And the orders get generated or events get created in GCP. But our order management and fulfillment systems are present on-premise. So we have to move this data across the cloud to the data center, so that we can fulfill orders. And once the orders are fulfilled, we have certain kinds of orders where customers wants to pickup from the store, so Buy Online Pick Up In Store capability. So we intimate the store associates and we would order events via Kafka to every store for the customers to pickup. And we have federated majority of our Kafka clusters for all critical line of businesses and depending also upon the network topology. So we have a very clear delineation between the business and the transactional data that we have at Lowe's. - Now, that's very impressive. How did the Kafka get started at Lowe's? You probably have like different choice of technology is, why Kafka? - That's a really great question. So for a retail business, so we need to be more agile. We need to have real-time systems so that we can give the real-time snapshot to the customers. So we wanted to have event-driven architecture across the enterprise. And Kafka is one of the key component as part of this. Traditionally, we have been leveraging the messaging and streaming messaging platforms. And right now we are moving away from the traditional messaging platforms and getting into more event-driven streaming platform like Kafka. And there is lot of growth with respect to microservices-based architecture. So Kafka becomes a very key component as part of it as well, which provides decoupling across many services. And click streams, data replication across the cloud providers and on-premise. And we also leverage for logging and monitoring-related capabilities, and streaming use cases across the enterprise. - Okay, so I guess a big part of reason just to be able to offer the kind of scale of kind of digitalized information you guys had to deal with? - Yes. - So maybe you can share with us a particular technical challenge that you have solved. Where's the usage of Apache Kafka? - Sure, so one of the critical challenge that we are trying to solve in very near term is, how do we provide a seamless data movement between our on-premise and the stores? We have more than around 2,000 stores and every store is considered as mini data center. There will be tons of events that get generated in every store. Now, these events need to come to the centralized on-premise applications. So traditionally, we have like around 200 to 305 file transfer jobs that occurs every day, every minute within the store. So we are moving away from those batch mode and getting into more real-time streaming. This is a critical challenge that we are gonna solve in near term. And right now we are managing our own Kafka clusters. So that is also of the critical challenge that we have to work on in near term. - Yeah, again, moving from batch to real-time, I think that's a theme in a lot of the retailers and just enterprises in general. - Yes. - Yeah, any other interesting use cases, challenges you have solved with Kafka? - So there are typically retail use cases like from order capture to order fulfillment. So as I mentioned, our .com business is completely running in GCP in a microservices-based architecture. So the order capture happens in GCP and order fulfillment happens in on-prem. So we use replicators to replicate the data from GCP to on-premise and vice versa for certain use cases. And similarly, we also send this events to every store for Buy Online Pick Up In Store. That's one of the most critical use case that we have as part of the Kafka ecosystem. And generally, if you speak about other retail-specific use cases, you can consider like product, price, inventory. Traditionally, we have been heavily leveraging MQ and ESB and ETL platforms. And right now, we are moving more towards real-time event streaming platforms. It's not that the ESB or ETL platforms are going away, but it's just that we are more streamed towards event-driven architecture. And apart from that, there are many use cases that we have recently saw during the COVID period with respect to workforce management that we have. Basically store hours and also building real-time analytics dashboards for store associates schedules for the people count and promotions, et cetera. - Yeah, that's good. You mentioned the scale of the organization you have to deal with. Lots of like data centers, lots of like retail stores, and lots of legacy systems. That sounds like a pretty complex ecosystem you have to build around Kafka. So what are some of the challenges you face there and how do you overcome that? - Yeah, definitely lot of data centers and mini data centers, as I said, every store is a mini data center and we are also into multi-cloud strategy as well. So we have to have this hub-and-spoke model for us to have the data moving across all the different regions, multi-region within the cloud as well. So at Lowe's, we are trying to provide the capabilities around making sure that the Kafka clusters are secure, highly available and high throughput, low latency, reliable, and providing a durable fault tolerant and capable of handling high volumes of transactions for data streaming and moving the data across the data centers. And majority of the use cases that as I've told is reputation, microservices-based architecture, and also the latest use cases that we are trying to solve are stream-processing using KStreams and KSQL. We wanted also to enable data governance across all the data that is getting streamed across the Kafka ecosystem. So one of the benefits that we are trying to more of a... Basically, we are trying to solve a bigger challenge of proprietary tools, moving away from cost-based applications and highly complex and highly costs tools and moving to more open source tools. So like for example, ELK, we are heavily investing in ELK and Kafka is gonna play a very critical role in ingesting the logs into ELK. So that would basically reduce our overall spend and improve our architecture as well. - Yeah, okay, that's good to know. So overall, how did you make sure the adoption of the Kafka is successful at Lowe's? Any other things that you would feel the community has helped you along the way? - Yeah, it's more to do with the microservices architecture. So we started this journey around two years ago. We were able to get onto the Confluent Platform, and we were confident enough that the Confluent platform is very close to the Open Source Distro, and they don't make major changes to the open... They provide the same Distro for open source, as well as the support model. And ease of installation, ease of configuration of the Confluent platform also helped us to scale much faster and provide this solution, yeah, at scale. So we also leverage a lot of other capabilities from Confluent. If you see the replicators, connectors. We also are getting into the Kstreams, KSQL related use cases. And apart from that, one of the important aspect for us to have a successful Kafka ecosystem is to have it as a diverse and well-worth DevOps model. So Kafka, Confluent Platform provides us the platform as a self-service options as well in certain use cases. And we have also done a lot of customization around the Kafka ecosystem to make sure it gets Lowe's file and we are able to enable the capabilities that Lowe's customers would require. - All right, good. Yeah, it's good to hear that the Confluent has been helpful here. What about the platform, how important do you feel is that integrated event streaming platform to retailers, like Lowe's? - Definitely, it is very much important in the current world. So for example, if you go to either... We want to enable this omnichannel experience. So either you shop online or shop in the stores or call center, anywhere. Like we want this order to be much more fulfilling experience order capture, the way we interact with customers and all that. So definitely customers look for much real-time related events in the sense, let's say, if I just purchase some products from Lowe's, they would like to see the receipt immediately, they would love to see an SMS, they would love to see an email coming immediately saying that, okay, this is your order, this is the status of your order, the shipping details, all this. So how do we enable all this? Without having this event streaming platform, it's very tough for us to enable this kind of experiences for our customers. So yeah, apart from that, there are certain other things like we would like to have from the Confluent Platform like replicators and connectors that are very important for Lowe's because we don't invest time in reinventing the wheel. The components are already there. It's enables us much faster and speed to market. - Yeah, I think had being real time, being able to provide those real-time responses to users, that's gonna offer a much greater customer experience. - Yes. - Did you guys get impacted by COVID-19? - Yes, actually, it's a great impact to the whole community and the world. So Lowe's is playing a very critical role in helping the communities recovered from this COVID. And definitely, Lowe's has also impacted, the whole business is impacted. But one positive side of that is, there's a lot of business to Lowe's because the communities are at home, they want to improve their homes, they want to make much more beautiful backyards and all that. So now there's a lot of important business that is happening for Lowe's and other retailers. - Right, okay. So just one last question. Any final thoughts, experience, advise you wanna leave with our audience. - Sure, so as I said, we started this journey almost two years ago. We have started very small. We started with one cluster, which is very small like I would say a final cluster with both ZooKeeper and broker. But we made sure a lot. We have stabilized the system. We were able to understand the use cases, we were able to get into the integrities of the Kafka ecosystem, understand much more in detail. And then we were ready to accept much and more use cases within the Lowe's business areas. And then we also started focusing upfront about how do we basically differentiate the data between analytical data and business data, and also what kind of criticality the data classifications is also important. And also what we have done is the federation of the Kafka clusters. Federation of the Kafka clusters is really important for us because we support multiple critical line of businesses. So we have Kafka clusters right now. After the maturity, we are able to enable this Kafka Cluster Federation across on-premise, as well as in other cloud providers. And we also have certain PCI-related Kafka clusters as well, where we are dealing for the payments, tokens and all that. So what I'm trying to say is, start small, gain the maturity, and then start building on top of it, okay? So if you ask us, what is our goal for next couple of years? We want to definitely create a ring between all our on-premise data centers, stores, and different cloud providers, so that we need to provide a seamless platform for all the communities within the Lowe's or all the product areas within the Lowe's, so that they can leverage this platform to stream the data in real-time across the data centers. And how do we enable that? I feel like stretched cluster is one of the important capability that we have to look into after Thanksgiving, which would provide us multi-region support, as well as how to offer a loan. Apart from that, we are also planning to enable the enterprise data catalog, which will drive the governance aspect of the data at Lowe's. We are also leveraging Schema Registry. So Schema Registry is also gonna play a very critical role in data governance aspect. Lastly, I would say, we are heavily focused to move away from a proprietary technology stacks and getting to more of a open source-based systems and applications. So one of the aspect of that is the enterprise logging and monitoring framework, which we are planning to enable leveraging Kafka as a ecosystem, using Kafka platform, yep. - All right, so with that, thank you, Bhanu, for sharing your experience with us. I wish you continued success in adopting Kafka and the event streaming platform at Lowe's. - Thank you so much, Jun, and Confluent team for giving me this opportunity to talk in this summit. It was a real great experience, leveraging your products and looking forward to a great partnership for enabling event streaming use cases at Lowe's. Thank you. - Thank you, bye bye. - Bye. (upbeat music) - All right, well, thank you to Levi and Bhanu for those keynotes and that wraps it up for today. Be sure to come back tomorrow. We're gonna have more keynotes and another full day of sessions. Don't forget to rate the sessions that you're watching. That's so, so important. And really look forward to having you back here tomorrow, rested and ready for another day. (upbeat music) 