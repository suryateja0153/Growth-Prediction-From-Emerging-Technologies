 hey everyone dusty from nvidia here in this video from the jetson ai fundamentals we're going to cover semantic segmentation which is similar to classification except that it's done at the per pixel level segmentation is useful for perception of your environment and autonomous navigation in robotics with that let's get started a while so navigate to the jetson inference page on github and we're going to go to the semantic segmentation portion of the hello ai world tutorial so as mentioned segmentation is a lot like classification in fact it uses classification networks as the backbone except that it's able to provide you the object class of every pixel in the image as opposed to just the whole image and it's able to do that because it's built on these fully convolutional networks or fcn and basically what that means is it takes in an image and then it also outputs an image whereas a classification network takes in an image and then it outputs one output class or a detection network takes in an image and outputs some bounding box data but the segmentation networks take in an image and output this mask which then corresponds to all these different class ids for each pixel in the image and it's extremely useful in practice particularly for robotics where you want to find the path that you're allowed to follow without running into anything or follow the trail or road things like that so like we did for the image classification and detection examples there is a segnet example in here that makes use of the jetson inference library either for c plus plus or python and we'll just quickly browse the source of this here it's quite similar to the previous examples that we've gone through except that in this case we're going to use the segnet obj object as opposed to detectnet or imagenet and then we're going to create an input and output video source and then just like before running a while loop for each image that's captured from the camera or the video stream then we're going to process that with the segmentation network and we can choose to either overlay the results on the original image or just output the mask of that image so you can just use that directly for a navigation algorithm or just use that data directly in your own application so there's a bunch of pre-trained models that the repo comes with for segmentation those are listed in a table here along with the performance that you can expect on jets and nano so even on nano the performance of the segmentation networks is real time which is impressive because segmentation networks are very complex so the first one that we're going to try running here is called cityscapes and it's trained on a data set that was captured in an urban environment and it is highlighting roads and cars and people sky traffic lights all different types of things that you might be needing if you were making like a self-driving car or a delivery robot that operated in the city so in order to run this first what we're going to do is fire up our container here so go into the jetson inference directory and then run the docker slash run script okay now we're going to cd into build slash ar64 bin and next we will run segnet and the network we want to use is called fcn resnet18 cityscapes and again there's a table up there on the github page of all the different pre-trained networks that this comes with and there's a bunch of images provided that we can do we're going to process all of the city ones here at once so images city wildcard and then we'll output these to our test directory so we can view them easily all right let's run this here okay we can see it to start to process all right now we'll just go into the folder here so navigate in your file browser to jetson inference data images test this is the directory that gets mounted into the container so we can just easily view these at our own pace here so we can see it's highlighting the road it's getting a bunch of people there traffic signs he's getting a lot of vegetation all the purple stuff is road that it's able to follow so essentially what this is doing is called uh free space detection which basically is input to the path planner of the robot um areas of the world that are are able to navigate to without running into anything all right so let's try some other similar ones here the next pre-trained model to use is called deep scene it's not dissimilar to the first example except that it was trained on off-road trails as opposed to on road so to run that we'll just run segnet.network equals fcnresnet18 deepscene then these images are called image slash trail and we'll put these in the same directory okay fire that up here you can see this one has fewer classes it has trail grass vegetation obstacles like rocks or whatnot and the sky okay there it goes and we'll open these up here to view we can see it's doing pretty well at segmenting the grass from the actual trail that we should be following so in this case you would want to code your robot to follow the brown pixels or if you were actually using algorithm you would just get the binary mask that had the class ids in it back you wouldn't have to interpret the color in a direct algorithm cool okay so let's check out what else we got here so here's here's a very useful one this one's called the multihuman parsing data set and this essentially gives you all these different body parts that are segmented differently it's not dissimilar to pose estimation which gives you the skeletal tracking back just in a segmentation mask form so we can run this one here segnet network equals fcm rest not 18 mhp and these ones are the human images humans and [Music] i'll put this to our test directory okay okay starting to process these all right let's go and view them here so we can see it's highlighting a bunch of different people it's capable of both doing multiple people per image and then all the body parts of those people as you can see it does arms torsos legs feet face and hair all independently from each other which is uh pretty good and it does capture even when there's overlap within people captures that pretty good as well okay awesome and i think there's two more this one's the pascal voc set which is also commonly used in detection we saw some previously results from that but also gives segmentation data so let's run on that real quick here network equals fcm resonant 18 slash voc then images object and okay so we've seen this one it highlighted a motorbike that are separate from people it's got this outline of a dog here so you'll notice i think we use this dog image previously for a detection example but in this case we get a much closer outline to where the dog actually is this one got a train pretty good even though the train was curved and you can see over here on the right side is the actual mask that's output if you have trouble seeing the overlay you can adjust the alpha value of the overlay too by the way on the command line this one with boats there's a bird here's another motorbike one where it's a motorbike and two people there's one with the buses and another train one okay cool and i think there's one more left okay right this one's the sun rgbd this one's trained on indoor offices and schools and homes so this one's really useful if you want to do robotic perception in your house and this one's fun too to point the camera around with segnet network fcn president 18 such sun and these images are the room images okay we'll give these a run here this one also does 21 different classes everything from like tables chairs sofas carpeting the floor the walls pictures on the wall okay go check these out here so here we can see it gets the bed and the floor differently from the wall there's a chair here and some lights that it highlights so this would be very useful capability if you were making like an indoor assistance robot something that needed to navigate in close quarters it's also trained on office spaces and i think there's some pictures of schools here too there's a couple bedroom shots that it has this one's uh working quite well all right so we've tested each of the pre-trained models that come with the repo next what we're going to do is run the camera demo and see how this performs in real time the model that i'm going to show here is this mhp model the multihuman parsing because it works well when you point it at a person in front of the camera but likewise in your house you could run the the sun rgbd model or go outside and run this deep sea model okay so a similar command line here we're just going to do segnet network equals fcn resonant 18 mhp and then my video camera is dev video zero okay here we go as you can see it's running at 30 frames per second it's highlighting my body here it's got my hands a different color from the arms it's got my face and i the hair is a different color from the face as well so just turn around do a quick little demo here so this could be useful for a number of applications in you know anything you need to find people for different types of body parts of people gesture recognition that type of thing all righty okay so if you're interested to take these segmentation networks and apply them into your own applications as mentioned before this is the source code that was running all of this and the python documentation for the segnet object you can find here in the python reference documentation here it is on the main page and this is just a documentation of what the different functions and members that are available to you inside segnet including all of the different functions like mask and processing the image things like that so this example is pretty simple to take and then integrate into your own applications and go from there okay so that wraps us up on semantic segmentation thanks again for joining us today i'm dusty from nvidia and we'll see you next time to learn more visit nvidia.com dli or email us at nvdli at nvidia.com 