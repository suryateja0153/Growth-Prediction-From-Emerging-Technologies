 The Arellano-Bond approach is a classic  approach for modeling dynamic panel models   for estimating dynamic panel models.  This technique is very commonly used   in project management where it is  referred to as the GMM estimator.   This is a bit misleading because GMM itself  does not do anything. It does not deal with   endogeneity. So the reason why this  is referred to as GMM estimation is   that Arellano and Bond applied GMM estimator  instead of some other estimation techniques. Why they did that? We'll take a look at  later on the video. But the important   thing here is that it is not a GMM but  it is a particular modeling approach that   deals with endogeneity. So whenever I teach  students quite often the student - perhaps   from statistic background - comes and asks  me to tell them about GMM estimation. I made   sure the equations I might have them work  with Excel and implement the GMM estimator   and then they asked me how does the GMM deal  with endogeneity and my answer is it doesn't. It's the modeling approach not the estimation  approach that deals with the endogeneity.   Therefore we need to ask two questions-  What is the model and why do we use GMM   and could we use some other estimation  techniques to estimate the same model? Let's take a look at the Arellano and  Bond approach. I'll use the model that   I've used before as a starting point.  So we have a longitudinal panel. We   have measures of ROA. We have measures of  CEO gender. We want to estimate the causal   effect of CEO gender and ROA and we want to  control for unobserved firm level effects. We want to estimate the within effect in this case   and that is what the Arellano and  Bond estimation technique does. So this estimation technique differs  from a dynamic panel of from Cross   locked model in that there is only one  dependent variable whereas in cross lock   model we would have ROA and CEO gender but  otherwise these are very similar models. Well how does an econometrician deal with  this problem? Let's write the equation. So   we have equation instead of ROA and CEO gender.  I'm gonna be using Y and X for simplicity and   we have the unobserved effect Ai here. We have  lack of Y with -1 years a predictor of Y we have   X lags similarly with minus one year. The lag  of X does not really matter for this problem. What we need to be looking at is the effect  of Y and how this unobserved effect Ai makes   things more complicated. So how does an  econometrician deal with this problem? Let's take a first look at a model that is a bit  simpler. So we'll take a look at the model without   Y as a predictor. So we'll just take a look at  X to Y relationship with fixed effect Ai there. How do we deal with this problem. One way of  dealing with this problem is to apply first   differencing. So we do first differencing.  We take the subtract the past value of Y we   subtract the past value of X and this  eliminates the unobserved effect here. So good. That's a way. We've unobserved effect. We   estimated the ROAs. We're going to be  fine. We have consistent estimates. When we have the lag dependent variable as  a predictor. Things are more complicated.   When we look at the equations of the first  differencing estimator then we can see here   that we have Y t minus 1 minus Y t minus 2  and we have the error term of Y t minus 1   here in the composite error term and because  this Y t minus 1 is a part of this Y t minus   1 so it is U t minus 1 is a part of Y t  minus 1 we have an endogeneity problem. So this error term here is correlated with this  difference. How do we deal with endogeneity   problems. Using GMM is not a solution to  endogeneity problem instead the solution   is that we need instrumental variables. So  what would qualify as an instrument here? Turns out that lagged values of Y would  qualify as instrument. For example we could   use Y a t minus 2 and earlier lags and why  does Y t minus 2 qualify as an instrument?   It qualifies because it's relevant because  of the autoregressive path. So Y t minus   2 effects Y r minus 1 and also it's a  part of the difference by definition. It is excluded because of sequence  of exigeneited assumptions. So the   idea of sequence exogeneity is that  past values are not correlated with   future error terms. So these error terms  u YT and u YT minus 1 are assumed to be   uncorrelated with Y t minus 2. So that's  an assumption of estimation approach. So relevance comes from autoregressive  paths. Exclusion comes from the model   assumptions and this is referred to as difference   GMM in the literature that talks  about Arellano-Bond estimation. It's difference GMM because we have a model  where we have first differencing then we use   these so called levels as instruments and we  have multiple instruments we estimated with   GMM. We could apply other instrumental variable  techniques but for some reason GMM is applied. Let's take a look at the assumptions of  these technique and how they're tested.   So the assumptions are the sequence of  exigeneity that I mentioned before. So Y   at 0 must be or must be uncorrelated with error  term at time 1 and error term time 2 and so on. So current values of Y are uncorrelated with  future error terms of Y. No correlated error.   So the Y terms are not see Autocorrelated.  If they are then this approach breaks apart. There are a couple of tests that  are commonly used. So this is an   instrumental variable technique so the  Sargan-Hansen test that we commonly   used for checking for exclusion criteria in  other contexts can be applied here as well. Then Arellana and Bond developed a  test for testing for autocorrelation   of the error term. So we assume  that the error term is not Auto   correlated. If these assumptions fail  typically the failure would be the auto   correlations the error term then we can  use more distant lags as instruments. So quite often in practice when this technique  is applied we apply it with the first lag as   the instruments we check for exclusion  if exclusion doesn't hold we increase the   lags and ultimately we will find the sufficient  lag that makes the errors roughly uncorrelated There's also another version of this  estimator. It's called the system GMM   and the idea of system GMM is that we  can make this estimator more efficient   by introducing additional assumptions  and introducing additional equation. Let's take a look at what the system  GMM does. It's called system because   it has two equations that are estimated.  It is estimating a system of equations. So we have the model here. The levels model. So  this is the model we want to estimate and how we   estimate it is that we specify two simultaneous  equations. So we estimate the beta one from the   difference equation. We estimate beta one  from the original level equation. But this   is endogenous as stated before. So we have  endogeneity problem because Ai correlates   with Y u at t minus one. So Ai is correlated  with every Y so this is an endogeneity problem. How do we find instruments. Well  the past differences will serve as   instruments because first differencing  eliminates the unobserved effect Ai. So   we can use earlier differences as instruments  for this model and estimate it consistently.   Because we have two models that we can use  for estimating beta 1 and beta 2 this is   more efficient than estimating beta 1 and  beta 2 from just this difference model. Relevance of this instrument is by  definition so difference and level   are correlated by definition also because of  autocorrelation autoregression and exclusion   comes from sequels of exogeneity assumption  and differencing removes Ai from here. So this is the system GMM and now the  question that we have is why do we   use GMM? So this is a general approach.  We could use any instrumental variable   estimator to estimate this. Why GMM  and why not some other estimation? There are basically two reasons that I can think  of why GMM is being applied. Both related time.   So this was introduced in 1991 which at the  time of recording is about 30 years ago and at   that point the computational resources that  we had were much less than would be today. Nowadays we could estimate this model  with maximum likelihood find a numerical   solution to the likelihood function and in  seconds. In 1991 this is not feasible. GMM   is lot easier to calculate which is  applied major there's no iteration   no numerical optimization involved. This is  a quick to calculate. So that's one reason. Another reason is the GMM at that time happened  to be the state of the art of multiple equation   estimation in econometrics. So this is a GMM  is used more for historic reasons than for   reasons that relate to the superiority  of this approach over other approaches. This technique - Arellano an Bond technique  - is not without its problems. These problems   are explained for example in Alison's  article and Alison notes that if the   number of cases N or number of firms number  whatever are our observational unit that we   observe multiple times over time - if  that any small then there will be bias. This is also inefficient. So we can get  more efficient estimators using maximum   likelihood based techniques and we can use  multiple lags and which lags we apply which   variables we use as an instruments it  is not clear and there's a problem that   when we have a complicated model we have a  large number of instruments and increasing   the number of instruments increases the  bias of instrument allowable approaches. There is also another problem with this approach  and it is that this is being used as a black box.   So the fact that researchers say that they use GMM  estimation to deal with the endogeneity problem   indicates that not everyone might understand that  it's not the GMM it's the instrument of variables   that deal with endogeneity. So if you think that  it is the GMM that does the trick then you might   not really understand what you're doing and it's  easy to use this as a black box because of status   implementation of the technique XTA bond and you  just specify the equation. It gives you estimate   so you don't really need to understand what you're  doing. And this estimator like others make some   assumptions if those assumptions are not fulfilled  then the estimates can be very misleading. So there's a more modern way of solving the  same problem and it's simply to specify the   model using a path diagram or syntax in  your statistical software as a structural   regression model using the wide format data and  then specify the constraints that are required. This has a couple of advantages. The  first advantage is that this is more   efficient than the Arellano-Bond  approach. So maximum likelihood   estimates have proven to be the most  efficient possible in large samples. There are other advantages in the maximum  likelihood based approach using the SEM   software. Another one it's easier to understand  what is being measured what is remodeled and   what are the assumptions. So here we assume  that the alpha - the unobserved effect - is   correlated with all the predictors so we don't  make the random effects assumption and we also   assume there are sequence like geneity. So  x1 is uncorrelated with error trem y2 y3   and y4 because they are uncorrelated all  the future values. x3 on the other hand   is allowed to be correlated with the error  term of y2 - because that is in the past. So we are allowing some correlations with  the error terms. We're constraining others. Specifying this as a path diagram makes it more  explicit what you are actually assuming. Then   we have a modern missing data procedures  available. So we could actually estimate   this model even if we're missing data. With  the GMM approach it would not be at least   as straightforward. We would have to set up  multiple imputation and other things which   is complicated with structural regression  custom model and we can simply apply full   information maximum likelihood which takes  the missing data into account automatically. And then finally we have better  Diagnostics. So you just need to   understand one tool and the same chi-square  test the same modification indices the same   covariance residuals can be applied that  you apply with these models every time. So   you don't need to remember that there is  an Arellano-Bond test for autocorrelation   you can just apply the more general  techniques that you already should know. There are disadvantages. Specifying this  kind of model is cumbersome if there are   a large number of observations. We tend  to see Y formant data in organizational   psychology where the time series is our other  source. If you collect data with survey then   fire time points that's quite a lot if you get  data from databases like economists do Storage   Management researchers often do then you may  might get 30 years of data for each company. So specifying this kind of model for  30 years would be a bit complicated   and it would be a bit hard for  the computer to calculate as well. Fortunately there are techniques for  example dynamic structural regressions   modelling implementing the M plus that  takes care of this problem. So that's a   special technique for estimating dynamic panel  models using maximum likelihood without having   to specify this kind of model with these  multiple different dependent variables. Then key is to specify large models.  That is true but fortunately this can   be automated. For example there is an estate  of packets called xtdpaml written by Paul   Allison if I remember correctly and that  automates the specification of this kind   of dynamic panel model with unobserved  effect using status SCM commands. So you   run this command. It prints you the  SCM syntax and then you run the SCM. Then there is the multivariate normality  assumption of the maximum likelihood   estimation but as I explained in other videos  this is something that we can deal with. We   can use alternative estimation approaches but  even better we could just use ML because it   is consistent and simply apply robust standard  errors to deal with the fact that the standard   errors from ML and the test statistics may not be  trustworthy if the data are severely non normal. So this is a more modern alternative and  in many ways more recommended than the   Arellano and Bond approach. For reasons of history   the Arellano and Bond approach is  still very common while it's dated. 