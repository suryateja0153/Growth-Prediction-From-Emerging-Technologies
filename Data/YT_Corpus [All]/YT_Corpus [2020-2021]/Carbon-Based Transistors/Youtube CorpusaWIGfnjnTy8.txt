 all right everybody welcome back I hope you've got at least a little bit of a spring break and we will continue where we left off a little bit here and as we go let's start by remembering a little bit where we were so we were talking about IO devices and among other things we sort of put up this mental model of how a processor might talk to a device so for instance there's always a memory bus that's often directly off the chip but then there's typically a set of bus adapters and an interrupt controller we sort of we're talking about what a typical device controller is the device controller is the part that interfaces with main system and the device and the CPU interacts with the controller to control the device as we stated and so typically a device controller has a couple of possible interfaces one of which is a set of registers that you can read and write that might control the device and those registers are x86 devices at least controlled with reads and writes to special instructions using special instructions or we might actually have memory mapped regions where we just read and write actual addresses and control goes directly out to the device so this may contain memory for requests queues or maybe bitmap image memory etc on this controller every device is a little different but no matter how complicated things are there's typically two ways of accessing things as I mentioned here one is with i/o instructions so i/o instructions typically look like this where you might have an out instruction into O X 21 with register al and what this says is whatever contents is in register al actually gets sent out to port 21 and that port 21 goes special i/o bus and that might end up reading or writing some control register alternatively which is much more common we have memory mapped i/o where again just reading and writing through load and store instructions goes directly to the hardware and causes i/o to happen excuse me so just as giving an example of those memory map displays we urban memory mapped i/o we sort of said well here might be an example of a display with i/o addresses that are physical addresses and if I read and write in certain ranges I might actually update what the command I want to run or I might read the status or if I happen to write to say a set of addresses that might put bits on the screen or perhaps I might actually put commands into a graphics cue to draw triangles and so you know in general what memory mapping means is the hardware Maps control registers and control and display memory and so on to actual addresses in the old days the dresses were actually set by Hardware jumpers at boot time although these days you plug it in and what happens typically is these addresses are picked automatically so they don't conflict with other devices and just writing to memory with a with the right instruction or a store instruction would actually cause something to happen so here we're writing to the frame buffer we might write graphics commands to the command q etc we might write to the command register and the result of writing to that actually causes the device to act on what we've said etc and so this is just a simple example of a memory map device memory map devices are very common these days because they're very simple to interface and you don't need special processor support like you do with i/o instructions and depending on what part of the physical space these are mapped to you can also also typically protect this with address translation in a way where you can even give full control of a device up to a user by mapping their page tables to map to a certain part of the physical space okay that's kind of where we were last time what we didn't get to too much was talking about this process of transferring data to and from the actual controller itself and if you look for instance programmed i/o is one option where each body is answered by the processor either by using in-and-out instructions or by loads and stores and this is very processor heavy so the processor is in a loop and if you're gonna transfer four kilobytes of data it's in a loop that's pulling each byte in now maybe it's doing it four bytes at a time by loading a 32-bit word but it's still extremely processor intensive the pros of this are it's very simple hardware and it's easy to program and the processors involved the cons are that it consumes processor cycles now we have a question here about well for working with memory map devices how do you tell the processor that those regions are memory mapped and so reads maybe have side-effects is it said at the hardware design time or is there configuration so that's a good question so typically there are parts of the physical address space which are outside of where the DRAM is that's known by the the system itself to be ioad addresses and so when you plug in a device into like PCI PCI slot or whatever there are there's a configuration process whereby you can say well certain reads and writes to physical addresses end up going to that card instead of going to derail and so you could say this is said at hardware design time it's really a combination of certain parts of the address space are reserved for i/o and you plug a card in and you use that part of that i/o I don't know if that answered that question going back to this slide by the way what you see here is that certain address is going on this processor memory bus may go to DRAM and regular memory or they may go over bus adapters and end up doing the memory mapped devices so I don't know I'm hoping that answered that question so the alternative is what we call direct memory access and what direct memory access is this is the alternative to programmed i/o is that the processor just sets up the transfer and then something else goes through a loop and transfers things so we're going to give a controller access to the memory bus and ask it to transfer the blocks by itself and the good thing about this is that now the processor is not involved in transferring every byte and instead it gets a signal like an interrupt when it's when the transfer is complete so here's an example from one of the books that you guys have access to where kind of shows what happens if we're gonna try to do DMA to pull something off of a disk so for instance the first thing is the CPU is going to go into the kernel until the device driver that it wants to transfer disk data from a certain part of a disk what happens is that driver then tells the disk controller by going over the memory bus that the transfer needs to happen the disk controller in this instance might end up reaching up to a DMA controller which is on the bus programming it to say well for every byte or every 32 bits you get transfer it to the next slot of memory and part of that is the sort of the starting address of where it's getting transferred and then what happens is the disk controller then starts sending data through the DMA controller the DMA controller writes memory and then when it's done the DMA controller interrupts ok and so the key thing here is that there's this other piece of hardware involved in doing the transfers rather than having the CPU do the transfers ok and there are many instances of how DMA works certain buses for instance like the USB bus etc actually allow the devices themselves to be bus masters and and write directly into memory and so there there might be essentially a DMA controller on every device for instance under some circumstances are there any questions on that so the question is could normal CPU software offload accesses to DMA so I'm not entirely sure as opposed to an external device doing so so under depends on the device if I understand the question here the CPU might program the DMA controller if you have a memory map device the CPU might actually control program an independent DMA controller and then the DMA controller reads from one address and writes to memory or reads from memory and writes to it an i/o address and so there are many variants of DMA out there some of them on cards some of them on buses etc so now how does a device notify the OS that transfers done well basically or that it needs service and so we've talked about a couple of options here reasons that this might need to happen is for instance the device has completed a DMA operation or there's an error or there's a packet coming in off the network and of course we've talked a lot about interrupts at the early part of the class and so this is a case where the device generates an interrupt whenever it needs service and the CPU then goes into an interrupt handler and starts doing something so typically the bottom half of the device driver is entirely interrupt driven and it gets entered when an interrupt occurs so the pros of this are that the CPU doesn't necessarily have to know how long it's going to take for the transfer to finish all it does is it just waits for the interrupt and it does something completely different and when the interrupt comes then the kernel handles things an alternative though is what's called polling and this is a case where the OS periodically actually checks a device specific register to see whether it's got a bit set saying that the transfer is done now if you remember the downside of an interrupt is that you have to save all of the state of whatever was running before and then you've got to set up the interrupt handler and get a new stack frame and so on and then run and then you got to restore the state so there's some non-trivial interrupt error handler cost to an interrupt with polling potentially you could be just checking the register every now and then just by reading a bit out of memory i/o space and as a result there's a much lower overheads to recognizing that there's a service to happen now there's a question about how do you maintain coherency with DMA that's a really great question and back to the DMA here what the issue here might be that if there's part of memory that is in your cache and you're overriding it what happens and that depends a lot on the devices some devices actually sir some systems and that includes CPU systems automatically invalidate memory when the DMA controller writes it others the CPU has to go and flush the cache before it can start a DMA operation to get coherence the so actual device is going back to the i/o interrupts and polling actual devices combined both polling and interrupts at the same time because for instance if you've got a really high bandwidth network device like a 10 gigabit or 100 gigabit per second Network device if he took an interrupt every time a package came in you would actually spend all of your time saving and restoring registers fortunately I always tends to come in bursts and so what typically happens with a high performance driver is it the interrupt takes it into the driver and then the device driver keeps emptying packets out of the network controller for instance until there aren't any left so it's doing polling to see if there's any packets left and then eventually it Rhian abel's interrupts and exits back to user code and so this is a way of basically handling really high bandwidth items with a combination of interrupts and polling ok so are there any questions on that the other time when you're using polling by the way is so there's a question here about how you do i/o in real time situations so one of the issues with real time is typically you don't want to interrupt your running processes because they're carefully times right you've got exact deadlines and you know exactly what the time is and so that's an ideal place where a second CPU that's not running your real time tasks is watching for i/o and in that case often times if you have a spare CPU what will happen is you'll do a polling situation where that CPU is just in a very tight loop and it's just checking registers for waiting i/o and so the real answer to how you deal with real time and somewhat unpredictable reality is you separate use separate processors one that's doing the doing the real-time processing and the other which is checking the i/o and polling is often used in those situations if you can burn a CPU just to spend in a loop looking for IO now we talked about this back in earlier in the term where we talked about the fact that device drivers are the thing that allows the kernel to deal with a wide variety of different devices and it's basically device specific code in the kernel that interacts with the device and as a result provides a standardized interface up into the kernel and as a result the same parts of the kernel i/o subsystem can interact easily with different devices because of the standardized interface and that standardized interface of course has what we keep in mind things like read write open clothes etc okay and so you just close this and so in addition to the standard i/o by the way there's the special device specific configuration is this I octal system call so you might do an open to a device and it has read and write system calls but it also might have an IEEE octal call because different devices might have different options that you need to program okay and so example might be a device that displays something might have different resolutions you could set or a network card might have different speeds or a serial device might have different speeds and it's possible that you would do that with an IEEE octal system call in terms of programming it the device drivers have these two halves that we've been talking about the top half is what your user code accesses when it comes in you get a call path from system calls and that's where the standard sort of open read write I octal strategy calls are and this is the colonel's interface to the device driver and it's also the place that will start IO and maybe put the thread or a process to sleep if necessary the bottom half is the part that runs as an interrupt routine when the data's back okay and so I showed you this earlier but I wanted to talk you through it again so this user program might actually have IO that it wants to do and so it does a read system call and that system called crosses into the kernel via the system call boundary and that point it might say well can i satisfy this read already and a good example that might be a file system where i try to do a read and the kernel has a cache of blocks off the disk we'll talk a lot about that coming up very shortly in the next lecture or so but if the answer is yes then potentially it can transfer the data into the users buffer and return from the system call very quickly on the other hand if that can't happen then we might have to send requests down to the device driver and this is a point where the device control comes into play so we enter the top after the device driver and what it would do is it might say well I know what block is necessary and in those instances if I know block is necessary then I issue the commands to the actual controller itself it's telling the disc for instance to scan into a certain track and then read a certain sector and then I might actually end up having to put the process to sleep because at that point there's nothing else I can do so I'll put the process to sleep on a sleep queue and of course the scheduler will take over and wake somebody up we talked about that earlier and then what happens well we've actually sent a command down to the hardware and so in the case of a disk which is we're going to talk about later in this lecture it might actually just start doing the access and eventually that will finish and that access will complete and generate an interrupt and then that's when the bottom half of the device driver takes over and that bottom half will receive the interrupt store the data in the device driver buffer if it's inter interrupt and then signal to unblock the device driver and that's signal at that point after we've done the transfer we'll figure out who asked for the i/o and which process to wake up and that and it'll copy the data into maybe the file system in that case and we'll wake up our process and then we'll transfer the data from the file system into the users buffer and then we'll return from the system call possibly a lot later so this this long path which we took here could involve you know milliseconds or even seconds in some very slow i/o before we go from the original system call to having woken things up and returned from the read call and so this is the blocking read call path all right questions so the question here is why does Windows seem to have much more issues with device drivers as opposed to Mac OS or Linux I think the real answer to that would have to be the wider variety of possible devices that are out there so both Apple and to some extent well so Apple basically had a restricted set of devices and so they had much higher control over their device drivers Linux while it does support a wide range of devices tends to have a lot of people that find bugs and so on and so it tends to have more stable device driver code and Windows tends to have lots of devices and lots of third parties writing code which tends to lead to possibly more failure I will point out though that when device driver bugs happen they do cause major problems with the kernel and there are device driver problems in Mac OS and Linux as well but it's possibly true that Windows has more but I think that's partially partially because there are more things available and so less control over who's writing the device drivers now so let's take a brief stop here and talk a little bit about some performance concepts because we are kind of getting close to the device interface here and we might ask ourselves things like if we're trying to figure out whether a device is performing well what might we care about well one option might be response time or latency that's the time to perform actual operations another might be bandwidth or throughput which is the rate of operations per unit time so latency is the time for a single operation rate at which operations are performed is a bandwidth or throughput question and remember when we were doing scheduling response time and bandwidth were sort of opposite sides of the coin and optimizing for one didn't always optimize for the other and you know good examples of bandwidth or throughput are things they're typically measured in things like megabytes per second or that's for files or for networks might be megabits per second or arithmetic operations might be gigaflops per second if you were talking to to an nvidia graphics card or something like that another important performance question is startup or overhead which is the time to initiate an operation now all three of these items are actually present in typical devices there's a certain startup time there's a certain throughput of bytes per second or whatever you get out and then excuse me there's a certain startup time there's a throughput and that leads to the latency for operations and so we can actually come up with a basic model that's not too bad which says kind of the latency in bytes in total number of bytes to transfer so this is some size might be file size or a network block size is typically the overhead plus bytes over the transfer speed or capacity and so the overhead is kind of a guaranteed not to exceed base latency and so as you can see from this essentially linear relationship as the size of the operation we're trying to do goes towards zero we converge to the overhead and so a bigger size in a transfer has a tendency to swamp the overhead once you get something big enough and you can ignore the overhead and so a good example of this might be a really fast network or a fast Network which is like a gigabit per second link which by the way is 125 megabytes per second so I'm going to gigabit per second is hundred twenty-five megabytes per second and let's say there's a startup cost of a millisecond getting into the controller to transfer something and so we could get a graph like this where I have actually two separate units on the same graph just for convenience so the blue is the latency of an operation and the the the red is the bandwidth and if we look here for a moment this is the length of say the packet I'm sending and what's interesting about this is is Lincoln bigger there's a linear time to transfer but we always have this overhead so the initial overhead here of a mega millisecond we can't get around and so as we grow this length then as we grow the length than this linear time increases and that overhead becomes less and less important there is a question here about why network stuffs always in bits the answer for why networking is typically in bits is that networks are usually serial communication devices and so that's a bit per time and so basically it's per second basically relate to the fact that you're sending kind of bits at a time as opposed to serial this is as opposed to sort of a parallel device that might send bytes at a time so now the question is I'm not sure is bandwidth the same as transfer capacity yes in the previous slide it is and this question about is this because the packet size doesn't really also increase are you asking about the shape of this graph let me explain the graph we haven't finished this yet here so so if you look if you notice here that for latency we sort of have our overhead s plus the number of bytes we're trying to send over the bandwidth in megabytes per second notice I've transferred to my units and so hopefully this makes sense to everybody as we have a bigger and bigger packet we can transfer that at full speed once we've gotten in the overhead of getting into the controller so that's what this overhead typically is the bandwidth is this red curve and the way to think about bandwidth this is an effective bandwidth is to say that it's the size of what I'm transferring divided by the amount of time it takes to transfer and hopefully that makes sense to everybody as I get down towards zero I have this overhead which is basically wasted time and so it doesn't matter how fast the network is is I get down to smaller and smaller packets I basically have very low to bandwidth but as I increase the packet size the overhead means less and less and so eventually I get closer and closer up here to the bandwidth of what I'm of my network and notice that my networks 125 megabytes per second and this one is you know not quite crossing a hundred and as the packet size got bigger and bigger I would get a closer to my hundred and twenty-five okay now in fact if you look here we can even continue this a little further and we can ask something about the half power bandwidth which is the point at which my effective bandwidth is about half of what at least half of what my network bandwidth is and so if we do that computation we can see here that I have to have 125 1000 bytes of data before in packet size before I kind of get to half the full bandwidth of the network and so what's the lesson out of this well the lesson out of this is you may have a really fast communication channel but because of the overheads of using it it may be that you don't get all those bytes you're spending a lot of time with packet overhead okay and so the bigger the packets typically the closer you get to the full bandwidth all right now what's interesting about this just to show the importance of overhead is if instead of a one millisecond overhead we go for a 10 millisecond startup which is more like disk what you see here is here's 10 milliseconds or 10,000 microseconds same gigabit speed of the transfer device but the half power point is 1.25 megabytes before we even get to using half of our of our bandwidth and that's because this overhead so high so what you can see from this is in a disc you're gonna waste most of the speed of the disk unless you can somehow get this overhead to go away and the biggest way we're going to get the overhead to go to go away on a do is with our file system we're going to try to avoid seek time which is the thing that's in milliseconds and try to mostly read things that are sequential off of disk all right now did I answer that question Sebastian let's assume the answer is yes okay great so so what determines this peak bandwidth so I talked about peak bandwidth might be a gigabit per second for a link well you know that's the speed of the bus so you could look at a bunch of buses so things like pci-x might have one you know one mega earth thousand megabytes per second and that's because there's many lanes running at a reasonable speed there might be ultra wide scuzzy which is 40 megabytes per second things that are kind of interesting is USB 3.0 is more like five Giga bits per second Thunderbolt which is the USBC is 40 gigabits per second and so these have been growing quite a bit I also put these SI SS in here so if you buy a serial drive and plug it into a device and you get a device that's like SAS 3 we can actually get 12 gigabits per second coming off of a disk drive which is pretty fast so bus speeds are clearly going to determine the peak bandwidth and so the other things that are gonna determine how fast we can get is the device itself so the bus might be really fast but if the device is slow doesn't help you much right so for instance the rotational speed of the disk if you've got a really high speed disk because you're in the cloud you might have 15,000 revolutions per minute if you've got a low power device in your laptop you might have 3600 to try to save battery power ok or things like the readwrite rate of NAND flash might matter or the signaling rate of a network link ok so these these things can impact basically what you're going to get and it may not just be the bus so whatever is the bottleneck in the path is the thing that slows everything down okay so let's talk a little bit about storage devices because that'll be our first kind of canonical device that we examine a little more detail there's at least two types of devices that you use probably every day so one is magnetic disk the magnetic disk is is very reliable storage it very rarely becomes corrupted very large capacity at low cost buying four terabyte drives these these days is almost a no-brainer you know 16 terabytes whatever not a big deal it's block-level random access except for a shingle magnetic recording which we'll talk about a little bit later so basically you can go and pretty much get any block from anywhere on the disk it's just very slow because you've got to seek and rotate to get to it so the performance for random access is very slow much better performance for sequential access so these properties are going to greatly impact the way that file systems are designed to operate on disks and we'll talk about how file systems have grown over the years to adapt to these constraints the fact that you basically want to do sequential access pretty much all the time flash memory which has become much more common these days is again very reliable it wasn't originally it's much more reliable now it's got a capacity that's not as cheap as magnetic disks although that keeps changing getting better block level random access just like with disk really good performance for reads worse for writes so writes actually take time to cause the change in magnetic levels and and power okay you have to erase large blocks you can only write a block once and then you have to erase it first before you can write it again so that actually causes some issues with file systems and then with the other thing with flash memory is it wears out so if you write a given block on flash memory too much it actually gets to where it doesn't store data anymore and then your devices did so or at least that block is dead so that's a downside of flash memory the upside is much faster in general than disk the random access is great and you know and overall it's pretty low-power solution so I don't know how many of you have ever opened up a disk drive or looked inside but it's pretty fascinating technology it's a series of platters and the data is stored in concentric tracts and the the question here that we have by the way is if a block dies as a storage device know this and avoid storing stuff of that block in the future so the answer is that the device actually can tell that the block is is starting to fail and in fact it there's explicit something called we're leveling that is tries to spread the the writes all over the blocks to try to make them fail less frequently but yes there are actual codes on the disk to help on the blocks and the flash to notice that things are failing so with a with a hard disk drive that's it's spinning storage these heads are extremely sophisticated so the the tip of the head actually requires the same kind of patterning that they do on chips themselves and is very sophisticated and for a long time only IBM was capable of making these things it's interesting when you start looking at the original IBM personal computer 80 in 1986 that a 30 megabyte hard disk was 500 bucks ok 30 megabyte ok it had a 30 to 40 millisecond seek time and and it could get about a megabyte per second off of the spindle things are a lot faster now I'll show you some up-to-date timings in a second but quite a bit faster I also wanted to show you these cool little devices which I actually even had some these were they fit into cameras that took this size of flash memory but they little spinning disk drives inside there so there was a single platter there was a two heads on either side that spun on either side of the disk and for the time these were actually bigger than what you could get in flash so this is an actual form gigabyte device inside what looks like a flash chip believe it or not pretty cool the other thing I did want to mention is these heads have have a read/write head on both sides of the platter excuse me so we read both sides of the disk and there's and there's a head on either side of the platter for each platter okay and by the way these micro drives were made by both IBM and Hitachi at the time they were pretty amazing but they were only lasted for a short period of time because flash densities caught up with them very quickly and they became impractical and not cost-effective anymore so what about discs okay so here's a here's a little bit of a another version of the disc here to look at so if we have a platter okay that's this there's a surface as there's two surfaces for every platter there's a series of platters there's two heads for each side and then there's this arm which as a unit basically scans in to a certain spot on the platters and so you have a series of platters and a series of heads and they're all tied together okay and so when you move from the outside into a particular track it's only possible to move all the heads at once okay now the unit of transfer here is a sector there it is that's the smallest unit that can come off a disk and as I mentioned I think in one of the Piazza posts or whatever the sector size is is kind of irrelevant to today's operating systems because it's relatively small it's like 512 bytes and you never want to transfer only 512 bytes and so typically these sectors are put too into a block and that's where a 4k block comes from okay if you take a ring that's a track and you look at all the tracks in US that are in together above each other that's called a cylinder so it's kind of like if you took a tin can and you went straight through all of the platters you would find all of the tracks in a cylinder and so they had positions on a cylinder and then a particular arm is a particular head is activated to read or write at that point so dis tracks are very narrow okay microns wide or less so this is you know the wavelength of light in this case is 0.5 micrometer is in typical human eye size and so we're basically very small relative to the size of of the heads there's a question here does the arm zigzag back and forth to read a single sector no so what happens that's a very good question because this is a bad figure I'm realizing what happens is this whole system is spinning and so all that has to happen is the arm has to go to a particular cylinder and then all of the data goes over the arm goes over all the data as the platters are spinning so these platters spin and as a result the whole track traces out and the head basically gets to read or write potentially the whole track or if it's pretty interested in a particular sector what will happen is it the arm goes in and then you have to wait until until the track or the sector goes under the head and then you can read and write it okay so there's guard regions on either either side to help in the if you have single tracks to help sort of avoid corrupted data during writes although what's kind of interesting is it's a little different these days I'll talk about jingled magnetic recording in a moment but so the track length varies across the disk and so if you think about this a little bit if the arm is out at the outside of the disk there's a lot more disk surface that goes by then on the inside that's just you know that's just basically the circumference that's that's acting here and and so what does that mean well that means essentially that the the sectors are all the same density because you want to store bits at a given density on the disk and so that means that as the arms are on the when the arms are on the outside the sectors are going by a lot faster than when they're on the inside and so the data rate actually varies on the outside versus the inside to make sure that the data is stored at a constant areal density okay and what's other the other thing that's kind of interesting here is that these disks regular disks are getting so big that the time to read a whole disk is becoming too long to even back up and so oftentimes these days companies like Google will have part of the disk is actually used for active data and the rest is used for our chiral storage that's almost never touched ok and so they actually kind of split the disk into two pieces those that are archival and those that are active and that's just because the active data if it needs to be backed up somewhere you just you can't read that active data off fast enough to kind of make sure it's safe all right now the other thing I wanted to mention is four really really high density what they do today is they do what's called shingled magnetic recording and these are disks that are entirely for sequential reads these do really poorly it at random reads and I'll show you why if you look the track the head that's writing actually writes a wider swath than the track itself and so what happens here is you write one track and then the next time around in the next track you're actually overriding the previous track somewhat this is shingled so this looks like shingles on a roof of a house and so you might ask yourself how the heck do you ever read the data afterwards and the answer is really good digital signal processing okay and so the earliest first versions of for instance Seagate eight terabytes and Itachi 10 terabytes actually used jingled magnetic recording to get the high density the density improvements have increased enough these days that you can actually get a terabyte drives that don't do shingling but you could imagine that if you have shingling you actually have to be very careful of how you use it so this is more for either archival storage where you're not writing it very often or for something like your tebow where you're writing a big device or it's gonna be you're writing a large video and so you essentially just go around many times for the video and so you you don't tend to write this randomly and you can basically write over big chunks of of tracks at a time for a particular video so now we can briefly do a performance model here so let's let's sketch this out a little bit so the heads are tied together and there's a there's a head on the top and the bottom and tracks are a ring on a particular surface sectors are the minimum thing that you can read and write and then this cylinder is sort of all of the tracks on both sides of the platter that are on top of each other okay so so basically when we want to read data it's is there always one head for platter good question pretty much yes the other question which nobody has asked yet but I'll ask for you anyway is why are these heads all tied together it seems like you'd want to independently move them so that you could read and write off of different heads or different surfaces at the same time can anybody think of why that might be so we have it seems very hard space consuming yes physical limitations yes you guys are kind of going to the right answer there more moving parts as error-prone yep so this actually so all of what you say here is correct but that isn't the reason they don't do this they could the answer is that actually these are commodity items so disk drives are so commodity and the heads are such complicated expensive positioning motors and so on that it would just be too expensive and it wouldn't be worthwhile and nobody would buy them and so when would you read multiple heads at once well you've got multiple processes running and they want to read different things you could imagine wanting to read different parts of the disc at the same time they they just don't allow that because it's just too expensive so it's really a cost reason and that you know all of the things that people said here where they're talking about physical limitations and more moving parts and so on those all relate to them bigger cost now our three-stage process here though is seek time which is the time it takes to move the head into the right cylinder rotational latency which is then the time it takes for the the sector you want to rotate under the head and then transfer time which is the time once you've gotten the sector in the right place to read all the data okay and the seek time of modern discs is like four to eight milliseconds it's pretty common somewhere in there that range maybe three in some cases the rotational time one rotation is typically 8 to 16 milliseconds all right and that's because we've got about 3,600 to 7200 revolutions per minute that's pretty common type of device you might have in your laptops all right and once you've got sort of these pieces then you could say the following that the latency of the disk is what you see in this diagram here so the request comes in from the operating system or excuse me from the user and it's queued so the might be a software cue in the operating system or a cue and the device driver itself which is part of you know the lower part of the operating system that queuing time is an interesting aspect we'll have to talk about in a moment and then there's the time to get into the controller okay and then there's the time to the disk so these two components the cue and the controller are totally independent of the actual device itself and so once you get into the device then it's seek time plus rotational time plus transfer time all right and by the way if you think of this probabilistically this rotational latency what's the average time it takes for us to get a to the to the sector we want once we've rotated once we've gotten to the right track and anybody guess so the question is repeating the question this seek time we need to know the average time to get to the cylinder okay you could imagine coming up with some average rotational latency has something to do with how long once we get to the cylinder how long does it take to get to the sector we're interested in and then transfer time is once we've gotten to the sector how long does the transfer take and that transfer time has something to do with where we are on the disk because it's how fast is the media flying by the head so that transfer time we could figure out but what about this rotational latency can it can anybody guess how we might compute rotational latency so we know the speed of rotation yep yeah so it was very good time for this to go halfway around that's right so whatever the time per revolution is we would take half of that and that's what we'd plug in for rotational latency how does the head know it's hit the right sector well you guys are asking some good questions so the answer is this sector has a header and some data and that header is a self-synchronizing thing so that when the head comes by it reads the the address of the sector and the header and then it knows that the data that's coming by or that it's about to write is gonna go in that sector so so that's part of the the process of formatting the disk alright and then the other question was how does the device driver or does scheduling really matter and the answer is yes so scheduling of what blocks we read and when really matters and that's going to impact in a great fashion how fast the disk drive works and so if you look at the complexity of file systems which we'll get to next time a lot of that complexity has to do with this weird device here that we have to make sure that we almost never move the head in and out because that's really expensive and we'd prefer to basically but put the head there and and spin around and grab a whole file because that's the fastest thing to do all right now here's some typical numbers so 14 terabyte Seagate drive that's pretty common thing now easy to get if you're if you're a cloud service provider typically eight platters and three and a half inch form factor okay and greater than a terabit per square inch on the platters so that's just crazy and they also suck out the air in there and replace it with helium to make it less resistance to spinning okay so they're trying to reduce the reduce some of the energy lost to the actual spinning of these devices okay the average seek time is in the 4 to 6 millisecond range depending on where you are and where you're going it could be 25 to 33 percent of this number so this is where scheduling of the device driver really matters or the file system is rather than four to six milliseconds you'd like to get a much lower amount and that's basically the file system we schedule things so we don't move very much all right average rotational latency I told you this 3,600 to 7200 rpm gives us somewhere between 16 to 8 milliseconds and however server discs get up to 15,000 rpm so those are very fast the average latency for these regular discs that we talked about is half of that 16 to 8 milliseconds and so it's 8 to 4 milliseconds and then transfer time 250 megabytes per second common right compare that in your mind to 1 megabyte per second in the original IBM disk quite a bit faster okay now let's see so I'm going to give us about let's give us a brief break so people can run off for a second and I'll be back momentarily and we'll continue all right so we're gonna talk about SSDs yes in just a moment that was the question in the break and so let's finish up with disks here so I wanted to give you an example here we're going to ignore the queuing and controller time for the moment and average seek time of 5 milliseconds 7200 rpm so how do we deal with that so there is a question by the way on the channel saying that I love to hear about SSD file systems yes we'll see if we can talk a little bit about that to next time but so if we have a 7200 rpm discs then the time for rotation is and this is where units mattered so hopefully you remember from your high school chemistry the importance of units and so for instance at 60,000 milliseconds per minute divided by 7,200 revolutions per minute gives me about eight milliseconds so you could do that computation okay in this 60,000 you can figure out where that came from right that's 60 seconds per minute etc and thousand milliseconds per second the transfer rate of say 50 megabytes per second and a block size of 4 kilobytes so notice that what I'm doing in my example here of this example is I'm actually putting a bunch of sectors which might be 512 bytes together into 4 kilobyte chunk which I'm going to assume is along the same track and together and so we can just basically once we've positioned ourselves and we've rotated the right spot then we can just read okay all of them at full speed and so how do we transfer 4 kilobytes well that's four four oh nine six bytes okay and remember that for data this is really a kitty byte right so it's 4096 bytes divided by 50 times 10 to the 6 bytes per second so this is a bandwidth so that's not in Middies and then we basically compute that out and we get about 0.08 2 milliseconds to get to get our block okay and the question here is to seek time and rotation time overlap the answer is no now can you figure out why we can't overlap seek and rotation given what I said earlier it's a great question anybody you can have an answer for that one nope it's not about dollars yeah great so we need to find the header that's right so you only after you seek then you as the disk is spinning you're sort of looking at each header on the track to decide when you're at the right spot so you can't actually start looking for the sector until you've moved in all right and so now we read the block from a random place on the disk we have the seek of five milliseconds a rotational delay is four milliseconds why four because that's half of the eight that we computed transfer 0.08 two milliseconds and so that basically gives us 9.0 a two milliseconds so you could say it's approximately nine milliseconds to fetch and put data of 409 six bytes divided by the time and so what we're really getting there in this particular situation is 451 kilobytes per second so notice what I've computed here assuming that we're randomly writing a 4k block anywhere on the disk the best we could do is 451 kilobytes per second even though the transfer rate off of the head is high right so look at the difference 451 kilobytes per second versus 50 megabytes per second so this is showing us how bad it is to keep ran reading a random block off the disk okay so on the other hand if we don't have to seek and we're going to just read from a random place in the same cylinder then the only thing we have to worry about in that case is waiting for the block to show up by rotating and so the rotational delay is four milliseconds transfer time is pointed Oh a two milliseconds which gives us four 0.082 milliseconds to do that read which has gotten if we keep doing that over and over again that's 1.0 three megabytes okay so the answer there was a question of why four milliseconds instead of eight and again this eight is the time for a complete rotation but we're going to assume that probabilistically on average when we pop into a track we only have to wait half a rotation to get get what we're going on so this is a an on average it's four milliseconds okay and so if we want to read the next block on the same track then we don't have to do any Sikh or rotational transfer time or rotational time excuse me and it's just the transfer time and that's our fifty megabytes per second so you can see this progression here for fifty one one kilobytes one megabyte 50 megabytes and so there's a significant advantage to locality all right so the key to using the disk effectively is to minimize seek and rotation delays and so when we get into file systems for disks we're gonna have to talk about that okay any questions before we go on okie dokie so there's a lot of intelligence in the controller so sectors have all sorts of sophisticated error correcting codes and so the disk basically is able to correct all sorts of errors automatically there's a wider field than the track so when you're writing you're sort of messing up bits on either side and so there's a complex DSP just an error correction to fix that sector sparing basically automatically the controller figures out bad sectors and will do replacements and so when the operating systems asking for particular sectors these days they're typically asked for in a virtual sense and the controller might actually be replacing the sector you thought you were getting with a different one because of errors there's also remapping a whole range of sectors to preserve sequential behavior so these are other things that controllers might do tracks queuing so sector numbers offset from one track to a next to help with moving and and getting high-speed even when you're sort of rotating your way in and moving seeking so there's a lot of interesting intelligence that's been built up over the years and so it's not the case that you typically say well I know exactly what track I need to go to and what's you know a sector and so on and you're gonna optimize exactly for that because in many cases the controller has a different you of how sectors are numbered it is interesting to see that disk prices have basically been falling kind of with a Moore's law growth rate although they've they've sort of fallen off a little bit over the last couple of years but they're still pretty pretty dense part of the problem is it used to be that there is a big issue of people worrying about the bytes getting so small on the disk the bit storage on the disk getting so small that mere heat would cause the data to go away but there's been a variety of new ways of sort of doing vertical storage of vertical magnetic domains inside to really take care of that and so some of what has tailed off these days has really been that you know industry can make huge disks that people can't necessarily even use because they're so big so I just wanted to give you a couple of examples of things here so the Seagate exos X 14 I mentioned earlier has 14 terabyte hard disk eight platters 16 heads in this little tiny device it's got helium-filled to reduce friction it's got a four mil at four point one six millisecond seek time one of the trends that's happened over the last five years or so is that 512-byte sectors which were pretty much the norm for decades have now started to get bigger because the disks are so big and so actually on some of these newer drives you can't even write a 512 byte sector anymore it's just a 4k sector and as I mentioned earlier nobody was using the 512 byte granularity anyway they have typical high speed like six gigabit per second or 12 gigabit per second that's SAS 2 or 3 interfaces and the price might be about six hundred and fifteen dollars which if you look that was about 0.05 dollars per gigabyte as opposed to the old IBM PC which was basically $17,000 per gigabyte so you can see there's a difference there obviously discs are a lot cheaper so let's talk about solid state disks solid state disks are basically made out of special flash memory and then oftentimes put into a form factor that you can plug in to the same interfaces as a regular disk and so back in 1995 they started replacing disks with these battery backup DRAM cells but then around the late 2000s NAND flash became very dense and so then they started using flash and the good thing about flashes there's no moving parts it eliminates the seek and rotational delay low power downside is limited right cycles all right now rapid advances in capacity in fact I have a really fun flash drive I'll show you in a moment but um so the basic architecture is inside the flash drive is a bunch of these NAND flash devices and a memory controller that basically is busy figuring out how to do what's called wear leveling so that we only we don't overwrite individual cells too often because if we do we wear them out and so the flash controller is taking the file systems request for data and putting a virtual layer on it and actually writing to its own notion there's a translation table inside the SSD to decide sort of which cells get used and that's all done in a way that's transparent pretty much to the computer you plug it into okay and typically it'll read a 4k by page in about 25 microseconds so that's pretty fast ok so there's no seek you know rotational latency transfer time to transfer a 4k page might be about 10 microseconds ok and notice also that these 4 K pages are something that you might find a little surprising so we're still reading and writing in 4 kilobytes size chunks even though you could say well these are just you know single bits at a time but that interface of a page at a time is used because the devices themselves are organized in a page at a time okay the latency here for this device is going to be queuing time plus controller time just like with the disk plus transfer time which is different and random access is going to be fine with these right it's not a big deal to read one part as supposed to another because there's really no Secor rotational latency so there's no advantage to locality other than within a block so the highest bandwidth on this device is sequential or random okay now so the question here is if flash memory controllers also have to worry about balancing rights are they notably slower than disk controllers No the the thing that's really limiting all of these is the speed of reading and writing the the individual bits themselves and so this this controller that's doing this is not the it's not the limiting piece in most cases now what it does do is occasionally it will be transferring data around to rebalance things and those circumstances you can actually run into a situation where you want to read or write and you're being held up because the controller itself is reading or writing so that is one possible cause for things to be a little slower than you might expect but it's still a heck of a lot faster than disks so writing data is pretty complicated okay because you can only write empty pages in a block and so yes here we have a fork a block we could only write for K at a time but then we have to erase a whole chunk of blocks at a time so we have to erase say 256 K bytes at a time and once we have erased a bunch of chunks that are co-located in the physical device then we can start writing them and so part of the process that we have to do in order to solid-state drive is that we have some number of groups of erased pages that are ready to go and so the controller is busy making sure that it has a free list of chunks of erased pages and when those chunks run out it's got to erase some blocks to get more chunks and so part of the interface between the file system and the SSD has to be to tell the SSD controller which blocks are no longer in use so it can put them on the free list gather them up and put together groups of empty 256k blocks for instance and then go through an erase process before you can reuse those blocks or something else I'll say a little bit about how the SSD works in a second in just two slides to hold that off for a sec so erasing the whole chunk of a block here is a 1.5 milliseconds writing is faster controller has to maintain a pool of empty blocks and writes are about 10x the time for reads erasure is about 10x the time for writes and so erasing things is definitely a slow process here but it's still a lot faster than DRAM the I guess I don't have a figure for the for what's going on here but basically the question is how the SSDs physically work and the answer is that there's a set of parallel so they're exactly like transistors of NAND flash with the exception that there's a to basically plates that are separate from each other with a with an insulator between them and the write process traps electrons basically on one of them and then you can notice that they're trapped there and erasing basically raises a raises a voltage high enough to drive those electrons off and so really the reason the high the write rate is so high is your basically shoving electrons across an insulator to get them to be trapped and they're thereby sort of indicated 1 or a 0 I guess I don't actually have a picture of so basically you're charging and discharging capacitors not quite the it looks like a capacitor except that you're basically shoving things across a capacitor plate and so you're basically have to get it high enough to drive the electrons across something that's not actually a conductor so it's a little different than a capacitor I wanted to show you here's a typical SSD drive that you can go and buy from Amazon without too much trouble it's say 15 terabytes so it might be a six thousand dollar drive about 0.41 dollars per gigabyte so that's not too bad this is the one I wanted to show you this one you still can't quite buy but this is the Nimbus it's a hundred terabytes it's got 12 dual 12 gigabyte interfaces can write very rapidly and it says you it'll give you as a guarantee unlimited writes for five years can anybody figure out why a company could offer unlimited writes for five years even though flash wears out as you write it yeah so both there's two comments here that are essentially correct yes they know they have there we're leveling working well and if you think about it there are so many blocks in here that they could write continuously for five years and not touch every block in here and so the company here can say for a fact that doesn't matter how hard you write our wear leveling is just going to basically keep redirecting you in a way that it won't wear out and I wanted to point out that by the way I tried last year when I was teaching this class to find out what the price of this thing was going to be and there was speculation about 50k for this guy but who knows it's still not available the question is what's the difference between a NASA seta SSD and a PCI SSD so the difference is the SATA SSD here looks just like a disk drive and runs the disk drive in earth interface whereas the PCI SSD is a slightly different interface looks more like memory okay let's see SSD prices keep dropping and so things are looking like at some point SSDs might cross over but hard disks in terms of price per byte but hard disks still somehow keep going forward and you know this 'hunter terabyte device is pretty cool and it's a very small form factor but it's still really really expensive all right I wanted to show you an amusing calculation so the question here is Kindles which are believe it or not specialized reading devices I don't know if you guys have even seen those anymore but I happen to love them because you can sit in the Sun and read you might ask the question is an empty Kindle heavier than or is a full Kindle heavier than an empty one and so here's the experiment you get a Kindle from Amazon that no books on it and then you you add all the books in to fill it up and the question is is it heavier when you're done and the answer is actually yes okay now what was funny was somebody from the New York Times forwarded this question to me once upon a time and for one of their little science columns and the answer is yes but it's you got to be very careful about what you're mean when you're saying this so flash works by trapping electrons and so when you trap electrons you're basically raising the energy of the transistor by trapping electrons in it because that's a higher energy state and so the array state has lower energy than the written state and assuming that at the time you could get your Kindle with a 4 gigabytes of flash in it half of all the bits in a full King Kindle are the high energy state this is just saying that when you have books it's a random set of ones and zeros you do a little bit of a calculation you use e equals mc-squared for the energy to mass conversion and what you come up with is that a full Kindle is about 1 Atta gram heavier than an empty one and now what's an anagram that's ten to the minus eighteenth grams the most sensitive scale you can find out there is ten to the minus ninth grams so you can decide whether this is heavier or not there's also there's also a lot of caveats to this like for instance if the thing is warmer that will add more weight than then this 10 to the minus 18 grams or if you wear the battery down the energy lost in the battery will make it much lighter than what you gained and weight so the only way you can really even do this experiment is you take the Kindle you fill it with books then you cool it back down and recharge it so it looks exactly the same as it did when you started the process and then you do that measurement which you can't actually do because it's too late but anyway so this is an amusing calculation you can actually look this up for an October 24 2011 what's amusing about and I I did confirm this with a number of my colleagues and so on what's amusing about this situation is right after we published this amusing little calculation suddenly everybody in the world was talking about how heavy the Internet was and so somebody came up with a calculation that the internet was the weight of a strawberry and he had a whole video that he posted and that was rather amusing but you guys should check that out some of that may still still be up and by the way this calculation is only doable because we're extraordinarily careful about what we say and we set up a careful experiment none of the things that claim that the Internet is the weight of a strawberry make any sense so the summary for SSDs here the pros vs. hard drives it's very low latency high throughput no seeker rotational delay no moving parts very lightweight low power silent shock and sensitive you can essentially read at memory speeds you could imagine that the file system that you set up for an SSD might be completely different than the file system you set up for this we'll talk a little bit about that some of the cons are the storage is relatively small inexpensive relative to disks but it's catching up I don't remember the last time I bought a laptop with an actual spinning storage in it I always buy SSDs now because they're big enough for what I need for a laptop and they're much more reliable and lower power and so you know spinning storage is still used a lot in still used a lot in clouds places you know cloud computing areas and so on but you know I think for a lot of the portable devices definitely SSDs are it certainly things are no longer small okay so that's important one of the cons is this an asymmetric block right performance so rights are more expensive than reads and by the way you have to have a bunch of spare ones that you've erased so that's a little different there's a limited drive lifetime because if you write too much on a given cell you wear it out so average failure rates about six years life inspected so you might be 9 to 11 years but all of this stuff keeps changing okay I did want to point out by the way that there's a lot of really cool alternatives so the thing about flash its flash is basically non-volatile so when you turn the power off you don't lose any data just like with a disk drive but it's kind of more like memory okay however what's very interesting is that you can do better and there are a lot of interesting ones one of the ones I think it's pretty cool is there's a company called Nanterre oh and they have nanotube memory so what you see here is a carbon nanotube looks actually like this where there's carbon atoms at all of the different spaces or spots here dots and they can basically set up a situation with a crosshatch three-dimensional set of nanotube cells where there's a difference between one state where electricity can go through fairly quickly in another state where it's broken up a bit so that's the difference between a 1 and a 0 and you can rewrite it over and over again there's no wear out and it's potentially as fast as DRAM and potentially denser so they are were actually talking about maybe getting more data than DRAM could per physical chip so this this is kind of exciting and exciting enough that if this were ever to take off there's a couple of other technologies if any of them were ever to take off we might not have DRAM anymore and just have non-volatile Ram and so pretty much none of your data would ever go away even in memory and that's going to change the way people build operating systems and the way that they build you know storage systems if all of your memory everywhere is always non-volatile ok so next time so there was a question on here let's see so there's a you know a question on how does what are a lot of different technologies there are other ones by the way where they actually have magnetic domains I don't know if you remember at the beginning of the term I kind of talked about core memory which were these little lifesaver like things that would have a charge or have a a magnetic field or not and that would give you a 1 or 0 so there's a version of that that they shrunk down to to the size of chips that's being looked at there's the nanotube memory there's a there's a phase change memory where you there's two phases of the material in here a crystal and one and a melted one and that's another way to get ones and zeros so I think there's a lot of exciting things on the way alright so next time we're going to talk about performance a little bit more but you know one of the things we're going to have to worry about here is in this user thread versus Q versus controlled versus IO path what are the most important things and as we get through that we're gonna have to confront this curve which is the queuing curve we're going to figure out a little bit of where that comes from and talk about how to confront that okay so it's not just the device itself or the controller but also the queue itself is going to cause an important part of our response time and we're going to want to make sure that we're not at the point where the curve really starts going up rapidly but rather more in this linear region down there okay so in conclusion we've been talking a lot about disks queuing time puts controller plus C plus rotational post transfer time the rotational latency being a half of a rotation transfer time based on how fast things are rotating and what's the bit density of the disk we talked about how devices have a very complex interaction and performance characteristics where the response time is queuing plus overhead plus transfer time for instance for hard disks queuing post controller post seek post rotation plus transfer we talked about SSDs instead where you don't have the seek and rotation but you have to worry about erasure and wear out file systems are going to be designed to optimize this and so next time we're going to talk more about that queueing component and then we're gonna dive into some file systems and we're going to talk about how our file system is designed to basically deal with these response time characteristics and fundamentally what's the difference about an SSD file system will come up so that'll be something that comes up and Burson high utilization are also going to introduce queuing delays that we're gonna have to confront you know one of the things I didn't talk about was midterm two we're gonna give you more information about that on the Piazza so watch for that I hope you guys have a great have a great couple of days and we'll see you in Thursday's lecture all right talk to you later you 