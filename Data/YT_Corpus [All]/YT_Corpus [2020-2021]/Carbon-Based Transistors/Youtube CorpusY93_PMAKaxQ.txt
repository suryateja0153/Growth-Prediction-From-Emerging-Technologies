 [Music] [Music] okay welcome to the climate impact of machine learning session i'm nicola fousey i'm a researcher at microsoft research new england in cambridge massachusetts um and the reason i think many of us are here is that we are kind of seeing that the cost of training machine learning models is increasing increasingly becoming a problem and you may have seen this article in the economies that discusses this issue and you may have seen in that article and this picture that is making the rounds is based on data from open ai but economies did a pretty good job at kind of disentangling the various tasks that people have been trying to tackle over the years and the energy consumption so you see time on the x-axis computing power used on the y-axis and you see that roughly ai can be split in two years from this point of view the first era goes up to 2012-ish and it's where the power consumption doubles every two years kind of following moore's law and then in the modern era you see a doubling that has increased in speed then it increases it doubles over 3.4 months and you can see that these holds across different tasks vision models computer vision models speech language games with reinforcement learning agents and it's becoming a serious problem because as you keep going in time you can see that the power consumption is becoming untenable and this is part of the reason why personally i'm here my research interest is in automated machine learning basic optimization and automated model design and this is what the typical table i see in in our papers it's a typical table i put in my own papers and i protected the innocent by blurring most of the of the table out but you see that you have a certain search cost basically the idea with automall is that you replace human time with computation but the question is what's a multiplier on computation and you basically see that there are approaches that take 3 000 gpu days to search over models but on the other hand you see that the test accuracy on a benchmark test this is imagenet is kind of all in the same ballpark and over time these are roughly chronological all the order you see that over time things become cheaper and cheaper but still i would argue that even a couple of gpu days is a significant amount of time so when i started seeing these tables in papers in my own domain i started getting worried because automl is the thing that you do on top of already expensive models so as automl becomes more expensive and as models get more expensive things become really out of control a little bit so that's kind of by means of introduction but um i'm really grateful to all the three speakers uh there are incredible researchers in this area and i'm really really grateful that they they agreed to present um one the first the first speaker is emma strobel from carnegie mellon university uh the second speaker is vnc from mit and finally we have diana marklesk from ut austin and they're going to each present um i have a talk and then at the end we're going to have a panel uh joined across all of us and finally i want to uh acknowledge uh my co-organizers who have done most of the work for these in practice um particularly phil rosenfield was a senior research program manager at msr new england in cambridge as well and sharon gillette who is the technical advisor and senior principal research program manager in our lab as well and with that let's hear the speakers hi i'm emma strubel um and i'm doing uh thanks for having me um i'm doing a one-year post-doc at fair before starting at cmu as an assistant professor in the fall and my goal this year at fair has been to figure out how to actually be able to train and use a pre-trained language models like bert once i'm at cmu in the fall and of course have far fewer compute resources so today i'm going to talk a bit about why that's important to me even as an academic survey some recent approaches for making these models more efficient and then if we have some time at the end i'll identify some promising future directions um so as you've probably already noticed recent advances in techniques and improvements in hardware for training deep neural networks have enabled really impressive accuracy improvements across many fundamental tasks in machine learning so in nlp we'd be naming these models after muppets and more recently other cartoon characters with the largest and most computationally hungry models tending to obtain the highest scores so here i've plotted on our log scale the number of gpu hours required to train four recent nlp approaches this transformer which is a machine translation model elmo and burt which are pre-trained language models that are used for a ton of different downstream nlp tasks and the evolved transformer which is neural architecture search applied to the transformer and so as you can see from this plot the number of gpu hours required to train these models has increased seemingly super exponentially over the past about three years and since i originally wrote the paper where we quantified some of the energy requirements of these models this trend has continued so google's mina chatbot which was announced in january was trained using neural architecture search on 2048 tpu cores for an entire month um and of course gpt 3 was announced just about a month ago now and while it's hard to place it exactly on this chart um it's estimated that it required about 12 million us dollars worth of cloud compute resources to train a single model and as a point of comparison training the evolved transformer required only about 150 only about 150 000 in cloud resources so the point i want to make here is that the energy consumption of these models is growing in step with their computational requirements so here i've listed the carbon footprint in pounds of a number of common or what used to be common benchmarks such as air travel for one passenger between new york city and san francisco and the carbon emissions for the average american for one year so in a paper um that i presented at acl last year we quantified the energy required to train and develop deep neural network models for nlp and we converted that to estimated cost in terms of cloud compute credits and carbon footprint and so we found for example that the gpu time required for me to tune and develop a model that i presented at the nlp conference a couple of years ago like effectively doubled my carbon emissions that i would have otherwise produced that year and trading a machine translation model using neural architecture search if done using gpus could produce more than two times as much carbon as a car during its lifetime including fuel and as i pointed out in an earlier slide we also estimated that this model would cost about 150 000 um in tpus to train which highlights another important side effect of the enormous computational requirements of these models which is that they're becoming increasingly limited to a really small set of researchers and practitioners who actually have access to these enormous resources that are required to develop and deploy these models um so most of the carbon footprints that we computed for that paper and then i'm citing this plot are for training you know really large models um like large future language models like burt and you may be thinking well you know let's leave the training of those models to big companies and whoever else maybe really cares about supervised english leaderboard chasing um but i actually think it'd be hugely beneficial for researchers and practitioners outside of a few privileged institutions to be able to work with these models so um so for me for example i'm mainly interested in solving problems where solutions can be found in text like many of you maybe are so for example here i'm showing a caricature of a project i worked on with materials scientists at mit where our goal was to automatically extract inorganic materials recipes from the text of journal articles um so of course the reality of getting this recipe out requires inducing some kind of structured annotations over the unstructured text so in our case this is some form of syntax and semantics which should also be thought of as entries in a knowledge base so this is an example of a problem that i think is interesting and some of the best approaches for obtaining the structured information that we want from the raw text require good distribute distributed representations of words and large language model pre-training just does a really great job of building these representations of words in context and in a way where you can get pretty substantial improvements not only in supervised learning but also in the unsupervised setting or in out of domain performance where the data that you test on comes from a different distribution in the data that you trained on um all of this is sort of as long as you have a large corpus of unlabeled text to the demand you care about which is often the case um so my point here is that even though i'm not necessarily trying to be google or microsoft on a question answering leaderboard um as someone who's interested in making nlp tools that are useful for extracting structured information from unstructured text across a wide variety of domains like such as the collaborators that i mentioned in materials science it would still be really useful for me and my collaborators to be able to train and use the representations that come out of these big pre-changed language models right so i started by pointing out that although these models can provide high quality representations of words in context which could be useful to researchers such as myself that comes at a huge computational cost which makes it inaccessible to most researchers and practitioners so now i want to talk about some techniques for scaling down these models ideally with minimal impact on the quality of the representations that they produce but first i'm going to briefly remind you how they work to make sure that we're all on the same page before describing how they might be made more efficient so essentially all mass language models are based on the transformer architecture which consists of j stacked general network layers with the output of each layer provided as input to the next so this allows the model to build up increasingly complex representations of the text with each additional layer and as a quick reminder this is what the internal mechanism actually looks like inside of each of those layers so this is a figure from the original paper from 2017. um so this um it consists of multi-head self-attention um which has run time that's quadratic in the length of the input followed by a large feed forward projection which is linear in the length of the input and then after the final layer there's a soft max layer which predicts into the entire vocabulary of tokens okay so this neural network architecture is the basis for a lot of students that are nlp right now including birth style mass language models and um and while there's been other architectures that have come out like gbt and xlnet um and they do differ slightly the overall structure is similar enough that i think many of the same ideas apply all right so the way that mass language modeling works is that rather than providing all the tokens to the model each time a sentence is observed during training some of its tokens are masked and the objective that the model is trained to perform is to predict what the mass tokens were so in practice it's been found that it's best to mask about 15 of the tokens so in this fragment that would be one token um we don't actually train on sentences we train on long long sequences of sentences of about 128 to 512 tokens in length and uh tokens are actually you know not tokens or words or they're not words but they're sub-word units um by parent-coded sub-word units so typically limited to a vocabulary about thirty thousand word pieces um and this alternative trading objective compared to traditional language modeling where each token is printed incrementally in like reading order um from the past context this allows for much faster training since predicting the mass words can be parallelized uh on gpus and tpus and this is really at least in part what's facilitated training these huge models that are enormous quantities of data which has led to you know way better performance than older lsdm based language models okay so how can we improve the efficiency of training these models so roy schwartz and jesse dodge and others ai2 characterize this in a simple equation that i really like in their green eye paper so they define the cost of obtaining a result as of as proportional to the cost of processing a single example with the given model multiplied by the size of the data set the number of examples you're training on multiplied by the number of hyper parameter configurations required to fit a good model so i'm going to start by describing some recent techniques for reducing the computation required to process a single example which is by far the most popular approach to trying to improve efficiency in these models all right so the premise behind a lot of these techniques is that these large models are actually really over parameterized but trading a smaller model from scratch just doesn't perform as well as training a larger model first and then making it smaller post shock um so we think this is an optimization issue um and there's an interesting recent line of work based on the lottery ticket hypothesis which basically suggests that um large models work better because their random parameter initializations are more likely to contain lucky values that converge to good performance but um so one popular approach to making models smaller is known as knowledge distillation so the idea here is to take a large model that works well and distill it into a smaller model by training the smaller model to imitate the larger model's predictions and intermediate activations um so this picture here is from this paper by senator um and it's showing two of these transformer self-potential layers and the dotted lines between sort of like the two layers are indicating the places where the larger model is trained to imitate the smaller model um and this works reasonably well um there still aren't any approaches that i know of for distilling bur that work as well as the original model and of course this also requires that you have a pre-trained model that works well for your task sitting around already and you just want to make it smaller oops um so another popular approach is taking a pre-trained model and removing or pruning unnecessary model parameters um so there are a lot of different examples of this um so for example you can prune different parts of the model or you can propose different metrics for what makes a parameter prunable um so a common example is just to prove the smallest weights and um so this figure is from a recent paper by paul michelle at cmu on pruning self-attention heads from transformers and the system is basically showing the number of heads that can be pruned for the model while maintaining a glib in um you know accuracy on machine translation or language modeling or fine tuning on mla and you can see that not only are many of the heads pruneable but in some cases pruning heads actually improves accuracy presumably by sort of acting as a regularizer um so another approach that i really like that's similar to pruning is quantization so now instead of removing parameters entire parameters the goal is to decrease their precision um or the number of different values that they can take on so typically models are trained using 16 or 32-bit floating point numbers but this can actually be reduced to as low as two to four bits per parameter um and so this can vastly reduce memory requirements um and depending on the underlying hardware can be extremely fast um but it does require the underlying hardware you know actually has the specialized instructions to support math and these on these lower precision values um but as the training models with fewer parameters training models with less less expressive parameters is a lot more challenging um than taking a pre-trained model and quantizing it post-hoc so again you generally need to start with a big model and then make it smaller um okay so the last category of approaches i'm going to refer to as bespoke architectures so these are model specific modifications usually to reduce like the quadratic all pairs self-attention um that is the computation bottleneck and the transformer so just as a brief aside i'm going back to a little illustration of the transformer so in my personal experience profiling an equivalent to a bert language model about half of um inference time was spent in the um in the multi-odd self-attention about a quarter was spent in the feed forward network and about a quarter was spent in the softbacks um so the bulk of the computation is happening in the self attention um so most approach is focused on reducing computation there it also makes intuitive sense that we should be able to do this because every token really should not have to attend to every other token especially since the model is like iteratively aggregating information from the context into each token's representation at each layer so one of these example architectures is using known as the reformers using hashing to cluster token representations and then it only computes self-attention between tokens that are in the same cluster so these are tokens that already have like the closest representations they're going to have the highest dot products another set of approaches is factorized attention or sparse attention where that model attends to a subset of its neighbors according to a fixed width pattern like every toke every other token or every three tokens sentence and another example is uh training the model to attend to fewer tokens by learning for a given token um to attend to a smaller or larger window of context so like certain tokens may need to look at more tokens versus less tokens um so one of the nice things about these this set of approaches is that unlike the other methods they can be used to train the model more quickly from scratch but of course there's always a trade-off between efficiency and accuracy and one thing that remains largely untested in many of these approaches is how well they generalize to new data or how efficiently they can be fine-tuned but that's the topic for a different doc so that was a sampling of the many approaches that have recently emerged for improving per example efficiency and i'm not really going to go into detail about approaches for improving the number of hyper-parameter configurations except to say that um model sensitivity to hyper parameters is woefully under-reported in papers and i think we need to do a lot better to clearly characterize you know when models are more less sensitive to hyper parameters and are therefore going to require you know more computation to do fine tuning to actually deploy on new data um but what i've been interested in looking at recently is reducing the amount of data that these models are trained on because i think there's actually a lot of potential leeway there um potentially some interesting things to learn as well from better understanding of which examples are more informative versus less informative for training these models um so reducing the size of the train data is starkly in contrast with the recent trend um so in this tweet colin raffle who's a language model researcher worked on t5 says hot take the most surprising thing about bert isn't how well it worked when it's proposed but how much better it would have worked if they had just pre-changed for longer and a more diverse data set um so indeed since bert was first released big companies have claimed to scale up training in terms of both model size and the amount of training data which these two things go hand in hand so microsoft isn't on this slide but um they're also uh participating in this with their turning lg model um but what i want to show here is that to me at least it seems like naively adding more data is not an efficient way to improve performance um so we see similar trends in roberta and excellent which are two post language models from facebook and google respectively where um about 10 times more training data the same model can obtain about a point or two absolute in terms of f1 or accuracy um you know across various common language modeling benchmarks which is equivalent to about a 10 or 20 error reduction so um but my intuition is that we really don't need all 125 126 gigabytes of trade data or 30 billion tokens to learn the same models um so as an alternative to making models smaller i think would be really beneficial if we could figure out how to better select an informative subset of examples from these enormous corpora for training the model um so what's been done towards this end of improving sales efficiency um so the closest thing i know of is the electro model from kevin clark and collaborators at stanford and google so as an alternative to the mass language modeling objective that i described before electro makes training more sample efficient by setting up training such that the model gets a signal from every token and every example rather than only 15 of the tokens in each example and they do this by dividing the model into a generator and a discriminator so the generator is basically just a small mass language model and its predictions are provided as input to the discriminator whose objective is to do a binary classified classification on each token identifying whether each token was from the original sentence or generated by the generator so it performs best when the two models are trained together at the same time which i think is really interesting uh because it suggests because the generator is essentially inducing a natural curriculum um for the discriminator to learn from um since presumably earlier during training the generator's predictions are worse than later during training um and this is an interesting idea should we be doing curriculum learning or active learning for language model pre-training um so i've been taking a look at this and so far i haven't gotten anything super straightforward to work and and one key question that arises um is what makes it easy versus a difficult example for a large language model i think we don't know the answer to that um so to get a sense for this um i applied a simple study initially proposed in computer vision to better understand which examples are more or less difficult for mass language models specifically apply this to the roberta language model um so the analysis is pretty simple so during training you just maintain counts of how often the model changes its prediction for each token of the corpus the intuition being that tokens with a high forgetting count are the most difficult because the model kind of like isn't doing a good job of learning you know what its prediction should be for those tokens um so this is an example of what those counts look like on like an actual sentence of colored the tokens here according to forgetability with red and orange being the most forgettable or difficult and blue being the least forgettable and easiest so you can see here that the easiest words are mainly determiners and the verb to be and the hardest words are pretty clearly these content where it's like modifiers and nouns like the word metaphors are different um if we zoom out and look at corpus level statistics for what are the least and most forgettable tokens this trend continues um so the most forgettable or hardest words um once you filter for words that occurred more than 100 times in the training data and this is wikipedia training data um these are really specific verbs and nouns and the least forgettable or easiest are punctuation determiners um and of course super deterministic sub-word pieces like rick and like only happens after her in english um and so this makes a lot of intuitive sense so the most forgettable words are exactly the words you would expect to have sort of like the highest entropy distribution in the soft max for a model trained to do language modeling um because who's to say whether the next word is shimmer or sparkle um and which one is correct given the context um yeah so if we plot this token mean forgetting count averaged over the corpus versus token frequency as a 2d histogram um we also see that it's not necessarily the infrequent tokens that are forgettable but the medium frequency tokens like all these medium frequency content words um so i guess my takeaway here is that determining example difficulty in a way that is really going to impact training is a bit harder than looking at sort of like really maybe the most obvious metrics like token or diagram frequency or even output distribution entropy um because just focusing on the helplessly ambiguous examples you know isn't going to accelerate training either so we need to figure out some middle ground way of finding examples that'll help the model learn to disability predictions that are actually disambiguatable i think we don't know yet what those examples exactly look like all right so now i've summarized a bunch of work that's out there and giving you a little taste of the stuff i've been thinking about lately um so now i'm about done um and i think i'm actually out of time um so i'm not gonna summarize a few future directions um but i will leave you with this slide um to look at and maybe discuss in questions um yeah and thank you so much for listening um yeah i'm excited to take your questions live all right so it's great to be here today i'm excited to talk to you about the role of efficient computing and reducing the carbon ambitions of machine learning computing and discuss some of the challenges and opportunities that we have in this space um so it should be no surprise that there's been a growing demand for computing both in the data centers as well as in our devices and a lot of the factors that drive this computing comes from the demand for machine learning and the also the amount of compute required from machine learning it has been growing exponentially over the past few years so this is something that we really need to address um in particular because you know the cost of computing connects to carbon emissions so let's see how this can connection resides so we start with what do we actually want to compute this requires knowledge about you know what are the compute demands what is the task that we're computing for what are the requirements for that um typically in order to achieve that task we might have some form of computing algorithm that we need to run these days typically a dnn or deep neural network model um then we you know that algorithm becomes a workload on some form of computing hardware itself that does the computation and turns out the results and together we can say that this is how we actually compute so the computing algorithm plus the computing hardware now to drive the computing hardware we have some form of compute energy and where does this come from well often is dictated by two aspects the energy source which basically converts um you know energy and also generates some carbon emissions and then when it generates the energy we need to deliver this energy to the computing system itself or make it into compute energy so there's some energy delivery there and there's also some factors called like for instance like cooling since you know the computing hardware generates heat we need to cool in order for it to operate now these two factors of energy source energy delivery and cooling really depends on where we're gonna actually compute so we'll cover these three topics in today's talks we'll first focus on how to compute then we'll discuss where to compute and what we're actually computing and look at various opportunities and challenges in these spaces so first start let's start with how to compute in particular uh the computing hardware itself so one of the big challenges in designing you know efficient computing hardware is that transistors are not becoming more efficient so over the past few decades we had moore's law which was giving us faster and smaller transistors and we had dinard scaling giving us more efficient transistors but we can see that this trend has really leveled off over the past decade and so we really need a new approach to you know make more efficient hardware and one of the approaches is to design specialized or specialized or domain specific hardware rather than using general purpose cpus in order to achieve significant improvements in both speed and energy efficiency so if we're going to design specialized hardware we need to first ask the question well where is the energy going in the hardware itself and so as it turns out energy is dominated by data movement and what i mean by data movement is that you know basically how do you deliver the inputs to your computing engine let's say you need to do a multiply how do you deliver the cost of delivering the inputs to the multiply itself and also the cost of you know writing out the results of that multiplication now if we compare the cost of um doing these arithmetic operations like multiplying ads to memory reads or data movements we can see that there's a drastic difference in terms of energy consumption so for example reading from a very small on-chip memory like an 8 kilobyte memory of sram on the actual system reading 32 bits of that it's already going to cost more energy than a 32-bit floating point multiplication and this energy is going to increase even more if we look at the last line when we read the data from off chip memory so the further the data needs to come from for example from dram the more energy cost is going to be and i should highlight that the scale here is in the exponential scale in summary memory access is going to be orders of magnitude um higher energy than the you know cost of computing these arithmetic operations and that should be what we really want to focus on if we want to build energy efficient hardware now if we start thinking about how we can reduce the amount of memory access that we have or the you know data movement cost one thing we can do is exploit data reuse in particular there's a lot of data reuse opportunities in deep neural nets and this basically means if i you know access a piece of data from memory can i reuse it multiple times for different operations and there's a lot of opportunities for this in dnn so for example if we think of a convolutional neural nets we have what we call convolutional reuse where the activations and weights the same ones are going to be used just in different combinations for different multiply and accumulate operations we also have feature map reuse where we apply multiple filters to the same input feature map and so as a result any activation in the feature map is going to be reused multiple times across the different filters or you can think about it as generating across the different output channels um and then finally you also if you're going to process more than one image if your batch size is greater than one we're going to have filter reuse because each weight in a given filter is going to be reused multiple times across the different feature maps so we want to be able to exploit this reuse we also want to have the right hardware that allows us to exploit this reuse and this reuse can be exploited with hardware that contains low-cost memories and by low-cost memory what i mean is very small memories under one kilobyte and the benefit of these small memories is that it's very cheap it's located very close to the compute and so accessing these memories is going to be very low energy so just to give you an idea let's say a multiply and accumulate in an alu cos 1x reading from a very small memory which is under one kilobyte is also going to be the same amount of energy as compared to if you read it from a larger memory like a global buffer um it's which is about from 100 to 500k but it's going to be 6x the energy and of course going to dram is going to be way worse and so in general the farther and the larger the memories the more energy and power it's going to consume so the key idea is ideally i want to reuse and read my data from these very low cost memory so i read it once from this very expensive memory and then reuse it multiple times in this very small memory the challenge of course that these small memories are small in capacity and we have these very large neural net with millions or billions of parameters and activations and so what we need to do is we need to chop up this very large null net into smaller chunks and then we need to figure out you know what is the order of chunks that we should process in so we can maximize the data reuse and these chunks have to of course fit in this low cost memory and so the order of you know processing these different chunks is referred to as the data flow so how the data flows through the system and how can we exploit the most reuse i mean there's been a lot of research in this space one approach is a data flow that we explored and the iris architecture and this data flow really tries to balance the amount of reuse across the various different data types weights activations and partial sums and by having inefficient data flow we can exploit enough reuse to reduce the amount of memory access from this global buffer by 100x and reduce the amount of off-chip memory access which is the most expensive by over a thousand x and so actually we realized this iris architecture in an actual chip that we fabricated and when we measured the power consumption of this chip it's or the energy consumption of this chip it's an order of magnitude more energy efficient than a mobile cpu really again by focusing on minimizing data movement through energy efficient florida flows by that exploit reuse now this is one approach for doing this if you look at more emerging architectures in terms of how to reduce or make computing hardware more efficient one area to explore is in memory computing and the idea here is if we want to reduce data movement why don't we just move the compute into the memory itself as opposed to you know sprinkling some memory beside the compute and the idea here is can we you know perform computations with the memory storage elements themselves in order to do this we actually have to turn to analog computing which basically means we need to represent the activation weights and the partial sums using analog voltages currents and resistance so for example one way to do this is that your activations are represented by voltages your weights are represented by the conductance of a resistor which is the storage element itself using ohm's law when you multiply the two you get a certain amount of current that comes up and that's already going to be the product and then finally using kirchhoff's law you're going to accumulate the current across these different resistors and then that's going to get do your accumulation itself so as a result you get a free basically multiply and accumulate by you know computing in the analog domain now there's some challenges here as well of course there's going to be increased sensitivities to the non-idealities of these devices of the resistors and so on um and then also you since you're doing the compute in the analog domain there's already overhead to compute to basically convert from digital domain to analog debate and back um there's a lot of exciting research into this space in particular you know there's a lot of work on coming up with new memory devices that are more efficient for this computing it might be um suffer from less non-idealities another way to do efficient computing is actually using light rather than electricity so the cost of moving a photon can actually be independent of distance so as we mentioned before often you know in terms of uh for electrons if you move it further you're going to consume more power but here it's going to be independent of distance and also the multiplications can be performed passively in light as well so that also makes it more efficient you can think of you know these are two examples of some uh processors that are designed with light and kind of some of the key ideas here are things like well you can have a beam slitter that kind of you know dictates how much light goes in one direction versus the other and that's the splitter itself how much light you divide into these two directions depends on the weight of the neural net and then the light the intensity of the light coming in is the activation um so that's you know some of the exciting work that's going on in computing hardware and making it more efficient we can also make the algorithms themselves more efficient in particular there's been a huge amount of work um in the design of efficient dnn algorithms um we can roughly categorize this work into three various areas one is what we call network pruning which is basically setting some of the weights of the neural network to zero and we know that anything times zero is going to be zero so we can save energy there we can also come up with more efficient network architecture so we can have these large networks or filters that we decompose into smaller filters for example like depth wise or pointwise layers and then there's also been a lot of work looking at reduced precision can we reduce the number of bits of the multiply and accumulate or the max and the weights um in general a lot of this work particularly the network pruning and the efficient network architectures focus on reducing number of max and weights but the real question if we care from the system's point of view is does this translate to energy savings and reduce latency and in particular for the stock does this translate into energy savings well unfortunately you know a deeper dive into this show that actually number of max and weights are not good proxies for latency and energy for example uh if you took it look at this plot released by google they show that the number of operations does not approximately latency well so for example for a given number of macs you can have a 2x swing in terms of latency and for a given latency you can have a 3x swing in terms of max similarly just by looking at the number of weights for example having a small network alone is not going to be a good metric for energy because actually you need to account for all data types so for you know saying that energy is determined by data movement it's not just the weights that are moving through the hardwood but also the input feature maps and the output feature maps and so on and we can see here from example looking at google net the weights only account for 22 of the energy so really if we want a bit design energy efficient algorithms we really need to focus on you know the direct hardware metrics like energy efficiency and latency as opposed to number of weights and max a number of weights and max alone so one example of doing this is incorporating energy into the network pruning algorithm so in the past what folks looked at was they looked at the magnitude of the weights and then removed the weights that were small and so for the same accuracy this can give you a 2x reduction in energy consumption which is great however we know that the magnitude of the weight really is that doesn't have very much to do with the energy consumed by the way and actually what you'd like to do is remove the weights uh that consume the most energy um and so one way of doing this is that we could sort the layers of the neural network in terms of highest energy layers to lowest and then start pruning the highest energy layer first and the idea here is that hopefully you'll be removing the highest consuming energy consuming weights first um because we know as we remove more weights and the accuracy is also going to drop so you're trying to get a better trade-off between accuracy and energy and by doing this by factoring in energy into the prunings of energy aware pruning we can now reduce the energy consumption by 3.7 x which is about 1.7 x uh less energy than using magnitude based printing and this is for the within the same uh accuracy so the key takeaway message is that it's really important to directly target energy in the design of efficient nuts if that is the ultimate goal which it is here similarly when you start thinking about not just the weights but thinking about network architecture design or using uh neural architecture search to automate the design of the network architecture it's important to also there take in you know measurements like latency and energy to drive that optimization itself so one example this is this netadapt work that we did in collaboration with google's mobile vision team and really the goal here is that we want to automatically adapt a dnn model for a given mobile platform to reach a certain latency or energy uh target and we use empirical measurements from that given platform of latent latency and energy to drive and make decisions about the network architecture itself in this case the number of channels for example for every given layer um and by driving the design with um empirical measurements again rather than maximum weights but really looking at measurements we can get actually a 1.7 x speed up for a mobile net with similar accuracy and another really important key thing i want to highlight here is the fact that this particular approach uses very few hyper parameters which makes the tuning effort method less and that actually reduces the energy consumption or for the actual training itself so not only do we want to reduce the energy consumption of the resulting model but through the training process it's also important to reduce the energy consumption as well and the few fewer hyper parameters you have there um then the less tuning that you have to do um finally you know we talked about in-memory computing it's also important to think about how you might want to design dnns for this new emerging type of processor i mean it actually could be quite different from how we typically design dnns from our existing digital processors so for example if we take a look at the accuracy of dnns on existing digital processors they might be different from uh how they might perform if you run them on these in-memory computing processors because you need you have the factor of non-idealities and in memory computing so it's important for the network to be robust of these non-idealities so for instance shown here um if we look at how these networks might rank with zero noise um the top ranking accuracy or the top accuracy network might actually not be the top one when we start injecting noise it actually falls quite quickly and so basically the same networks that perform well on digital processors might not perform well on in-memory computing so that's one thing to think in so again we need to rethink the design another thing is again there's this trend of reducing the number of weights in dnn and this might be less desirable for in-memory computing one because the main focus of memory computing is reducing the weights of the data movement um or the disorder the data movement of the weights and so as a result if you you know reducing number of weights is not going to help if we've already reduced the its data movement it might be more beneficial reduce the data movement of the activations another thing is you know as we mentioned you need to convert from analog to digital and there's some overhead for that so it might be useful typically in these architectures to have in these in-memory computing processors to have a very large array but then if you have very few weights you might have very poor utilization of that array um so while for in-memory processing or memory computing it might be preferable to have shallower and larger filters this is quite different from the current trend of deeper and smaller networks for digital processors so really again when we're designing these efficient dnn algorithms we need to think of the computing fabric below as well to get better results um and there's a lot of research in this space so actually we recently um published a book on this topic really kind of cross cuts from computing hardware to computing algorithms and how can we do this efficiently for deep neural networks so i invite you to visit that if you're interested in this topic all right so moving from how to compute to where we actually compute this also has a lot of implications in terms of um you know the energy consumption itself and the carbon emissions and how it relates to the compute energy that's being consumed by how we compute um so where we can view one thing is the energy source itself so depending on where you're doing the computation um the electricity grid in that region might have different levels of carbon intensity which is basically the ratio of the carbon emissions or the weight of carbonations over the amount of energy that's actually being produced in kilowatt hours that we're showing right here so we should factor that in also if you're using renewable energy that percentage of renewable energy varies with the time of day so obviously for example during the daytime you're gonna have more solar energy available than in the evenings and so that's something to consider these uh very um also energy delivery so once you've you know acquired this energy how do you deliver it actually to the computing hardware and community itself you need to factor things like power conversion from going from let's say ac to dc the loss in the you know the wiring that you're sending it changing in voltage and also cooling costs as we mentioned computing hardware generates a lot of heat so you also have to spend energy to remove the here cool it down um a prime example of this is in the data centers itself um so shown here is a breakdown of the energy consumed in a given data center um typically they measure this efficiency of confusing converting from energy to compute energy as the power usage effectiveness or pue it's the ratio of energy over compute energy and typically this is in the range between two to one point one where one is optimal so for example if you have a ratio of two that means only half of the energy that you are you know generating actually gets used by the computing hardware um and as it turns out this is an exciting opportunity where machine learning can actually help improve this efficiency um by providing better cooling okay so we go from how to compute where to compute and now we should look at what to compute itself and this is really trying to factor you know where the commute demands come from they can come from number of the requests that we get the amount of data that we need to process and the required quality of results which we typically think of as accuracy um in order to reduce the number of requests you know one thing that we can do is make hyper parameter tuning easier we already hinted towards this if we can for example reduce the number of hyper parameters to tune we can reduce the amount it takes to do training for example to find the right hyper parameter reproducibility is also very critical uh for reducing unnecessary uh requests due to replication difficulties so if you can make your results easy to replicate that's going to be fewer requests that need to be issued in this therefore less carbon being generated this is of course also good for advancing research and machine learning and there's a lot of ongoing efforts both in the machine learning community as well as the systems community uh we want to reduce the amount of data that's required because we know that data movement is expensive so thinking about ways that we can exploit data reuse and thinking about new algorithms that are more data efficient and that could maybe incorporate prior knowledge into these dnn models this is already kind of an area of research in machine learning in general but if there's an additional benefit now that it also has energy benefits if you need to use less data um and then finally we want to evaluate carbon like the trade-offs between carbon admissions and quality of results right so what is the cost benefit of these situations for example is it really worth a while to get that point two percent let's say increase in accuracy for image net for a huge amount of increase in carbon emissions for much longer trading time or much less efficient models these things should actually be taken into account and in order to really assess this we also need a deeper consideration of you know what is a meaningful quality of result for a given task so for example again look using image as a um example is achieving point two percent better accuracy on your imagenet dataset actually going to allow you to achieve a good quality of result in your final goal so understanding the trade-off there is really important um excitingly there's also a lot of things that we can also do today that kind of you know best practices can help us improve our efficiency um so we have a lot of you know hardware and out there that can be already very energy efficient can we make those energy efficient settings or configurations the default or make them very easy to access this requires you know software and framework support for things like reducing precision um and specialized hardware that's easily accessible this exists today but it could be made easier to really reduce the buried entry so that this could be the default for many practitioners and researchers to use um can we enable us can we measure and report the energy consumption and carbon emissions um so that people know what their carbon footprint is this requires software hardware support to measure the energy consumption and kind of system level support to report the carbon intensity of whatever given energy source that's being used um there's already research that's emerging in this space in particular people looking at frameworks for standardized reporting so invite you to take a look at this archive paper they also have some nice commentary on how to reduce carbon emissions or think of carbon emissions in this space um with this information then it's important to try and run your experiments when feasible and locations at times where there's low carbon intensity and again ensure reproducibility for the obvious reasons and what's really exciting about all these things that much of this we can do today or in the very near future is just a matter of putting in the engineering effort to enable this um so what are the key takeaways because we covered a lot so the first is in general the very high level we want to jointly consider energy efficiency and accuracy and machine learning research we want to consider the trade-offs between accuracy and energy for a given application and maybe try and develop data efficient machine learning techniques that also benefit energy efficiency when we're designing machine learning algorithms for energy efficiency we should actually make sure that we directly target energy consumption as opposed to just looking at number of max or weights um and also look at you know designing specialized machine learning hardware that really reduces data movement um in our you know day-to-day activities we should incorporate energy efficiency considerations in our best practices so we should lower the barrier of using existing energy efficient computing options and enable you know reporting measuring of energy consumptions and carbon emissions we should try and compute um in the locations with the lowest carbon intensity and highest power efficiency and again reduce unnecessary computing by ensuring reproducibility so all of these slides are available on our website we've done a lot of work in this kind of efficient computing space and so i invite you to visit some of the other resources that we have on our website including videos and again if you really want to do a deep dive on efficient computing primarily for deep neural nets i invite you to visit this book thank you very much thank you for attending my talk um my name is dianna marcolescu i'm a professor of electrical and computer engineering at the university of texas at austin i'm also on the junk faculty at carnegie mellon university i will tell you a little bit about uh what climate climate meets machine learning um and what is the connection between hardware and machine learning and how we can do their code design this is joint work with my collaborators and students at carnegie mellon so what is the connection between machine learning and climate well the connection comes through hardware machine learning applications have pushed hardware to its limits we see them everywhere from automotive to smart home mobile wearables vr ar and drone applications uh but to push these machine learning applications to the edge um we really need to consider hardware constraints they are the key limiting factor for deep learning to move to mobile platforms to keep this in mind uh consider the object detection that drains the smartphone battery in less than an hour think about how energy consuming this is and um how we have to work much better uh to make this more energy efficient other constraints have to do with pushing inference to the cloud and that means increasing the edge to cloud communication which affects user experience as well as on device inference response time that means that some of these constraints may not be maintained so when we try to achieve this machine learning model design under hardware constraints we discover that it is actually really hard in general finding the deep learning model with optimal learning performance is hard but when you put everything together the uh deep learning model hyper parameters number of layers units the learning rate this is a huge search space and we're trying to find the machine learning model that satisfies a given performance or classification error when you add to this huge search space the design space of the hardware it becomes even more complicated so to give you an example assume we consider the energy cost or energy consumption for these machine learning models as it turns out you have up to 40x difference for these models depending on the platform and the type of model that we consider uh for the same type of classification error so the challenge for us is to identify which of these models at the knee right there that have minimum error but also have minimum energy are the best for our application to do so we will need to follow through 12 000 models 800 gpus or almost 62 gpu years and that's the equivalent of 68 tons of carbon dioxide which is more than half of a u.s car lifetime and 34 times more than an east to west coast flight now we can't really optimize a model that we cannot measure so that makes the point for developing joint deep learning hardware models typically a deep learning practitioner will need to identify what are the design space components for that machine learning model but a hardware designer we need to look at the hardware constraints together they will need to achieve deep learning objectives like accuracy but also hardware constraints like energy latency or perhaps a battery lifetime or memory so we hope that our framework will be able to accomplish these challenges and the first thing we want to achieve is the ability to measure energy latency and power consumption and our framework adopt neural power achieves this with a 90 accuracy for hardware metrics power energy and latency for deep learning applications running on hardware platforms how do we achieve this um typically this is this starts with a machine learners that need to identify a convolutional deep neural network architecture um but then we want to characterize it for a given target hardware software platform so we develop layer level models and network level models in the neural power framework these determine a very brief detailed power runtime energy cost with breakdowns that tell you which of these layers and which of these networks are costly in terms of latency for inference or power and energy this provides guidance to the machine learning developers and also to the hardware developers such that they achieve a best compromise for both performance and hardware so what are the components of neural power if we think in terms of network level models the energy is basically the product between the latency and average power which in turn is summed up across all layers in this case pn tn are the different layers run time or latency is end to end computed for each layer and add it across throughout the network and power is the average power for the entire network or the entire machine learning model when it's run on the platform each of these are gathered through profiling and then models are built as described here using i'm not going to go into a lot of details but they are typically polynomial terms polynomial models with additional terms that accomplish or link us to specifics of the computation like for example number of multiply accumulated operations the feature space for these polynomial models uh consists of the kernel size stride size padding size so all the mod all the features that the feature space that are important in convolutional neural networks similarly for the power model we have another polynomial term a bunch of additional terms so with this we accomplish the model selection and we fit these two models for a variety of synthetically generated neural networks and how good is this by comparing to an existing framework paleo which was published right before our work from acml 17. we accomplished a much better one mean square percentage error as well as mean square error compared to the original work that they show for a variety of convolutional fully connected and pulling layers and as you can see the estimation error drops from almost 80 to around 11 which is impressive we also perform the same thing for power consumption and compared to the actual uh model size the model size as well as the uh original type of convolutional fully connected network we're able to achieve less than ten percent accuracy 90 accuracy so when we put everything together we can see the network level results breakdown and as you can see um for the fully connected um layer 6 which is right third from the bottom for runtime for bgg16 you can see that we're actually able to identify that as the most um time-consuming layer as opposed to prior work which was unable to identify that furthermore we're able to preserve the ordering in terms of the latency span per layer compared to the original actual platform platform numbers for power for the same network vg16 we achieve a very good agreement with the profiling results and therefore neural power is able to estimate both latency and power appropriately within 90 accuracy when you put everything together runtime is estimated for a variety of networks from vg16 all the way to lxnet and others with accuracy that is very close to the actual runtime and actual power and if you can compare this with energy you can see that also energy which is the product of the latency and average power these are almost uh very close um with great agreement between the actual and the estimated numbers now how can this be used when you look at other ways to compare uh different kinds of networks not just in terms of accuracy but also in terms of their energy efficiency you can look at a combined energy precision ratio metric that tries to minimize both the error as well as the energy per inference and depending on the factor alpha that we consider here we can give more weight to the error classification error meaning we care more about the machine learning performance or we can give more weight uh to the energy so the hardware metric so as you can see in this analysis in this table uh depending on the alpha values depending on whether we care more about energy or whether we care more about accuracy different networks um are considered as winners most of the time is alexnet but when we care more about accuracy um pgg16 is the winner uh this kind of methodology is general it can be applied to any type of platform in addition to the one we presented here and the code is available you can use it and apply it to other platforms as well a second component that is essential in identifying or reducing the cost for finding these neural networks on a particular platform has to do with um searching for the right configuration as i mentioned this is a very costly process um typically neural architecture search can bring about an order of magnitude improvement in energy or latency if we take into consideration the hardware metrics but it it's very expensive as i mentioned before uh it can take quite a bit of time so our work um in single path uh neural architecture search which is hardware aware um allows for that to be done at three orders of money to faster the basic idea is the following typically rml is used to identify convolutional networks that provide the highest image classification accuracy for example under given latency constraints for example so that they can run properly on a smartphone typically they start with a search space um and then they use some sort of optimization formulation to perform the neural architecture search um but to be able to do so in a hardware aware fashion you need a latency model like i presented previously the issue though is that finding the proper network configuration can take more than 100 of gpu hours and as i mentioned before even gpu years so the question is can we reduce the cost for identifying these neural architecture configurations that satisfy these hardware constraints and the answer is yes we started from um the basic multipath differentiable neural architecture search approach that starts um with multiple different types of paths that can be considered and selected uh during the architecture search process a supernet uh considers all this candidate as a separate path layer and therefore the nas problem is viewed as a path level selection problem so in other words we need to identify uh which of these paths is selected only one can be selected for a given architecture um this is this is an expensive combinatorial large design process and it's typically very costly um our work takes that um in a different direction by combining um in this super net in a single super kernel in other words instead of looking at all the possible parameters weights that are part of these all these multiple paths we train a single the largest kernel in the super kernel and um the problem of the kernel level selection is an efficient one because the number of parameters per layer is um the one for the largest candidate operate only so that makes it for a more efficient architecture search we do so by using this differentiable encoding done in a novel fashion this approach allows us to achieve state-of-the-art accuracy for imagenet um with more than three orders of money to reduce search cost so how do we make these architectural decisions differentiable we start by having the selection variable um looking at whether you choose just the inner kernel or also the other kernel as well but this selection variable obviously is not differentiable so we do it when we make it differentiable by using a group lasso approach that allows you to perform trainable kernel threshold using a trainable kernel threshold variable during the search learning process and you can do this not just uh for uh the kernel selection but also for expansion ratio which is considered in this case expansion ratio three six or just a drop out of the layer again with group lasso and trainable channel threshold variables and when you put everything together you get uh the search space that is differentiably searched um and you can basically use any um off-the-shelf um optimizers like super stochastic radiant design for example um by doing so we're actually able to put in this objective not just the cross entropy loss that quantifies accuracy but also this mobile runtime loss term that quantifies the hardware a specific metric in our case latency for this mobile smartphone platform in our case this platform is a pixel one phone and we have all these latency per layer models which could be developed uh or detailed as i described before um and using this uh type of approach uh provides again using an architect architectural choices as a selection process where the latency as well uh provides good agreement between uh the predicted and actual measured runtime on the platform we perform the uh neural architecture search process and compare with existing work uh on the left you have mobile mat v2v3 these are manual designs that do not involve architecture search on the right um you have all the standalone nas approaches as well as the multi-path nas approaches that we based on work or work on and then single path mass achieves uh state of the art accuracy with the same 80 millisecond runtime that we have uh on pixel one just like all the other approaches the difference though is that it achieves this configuration in much faster time more than three orders of magnitude um faster and at the same time if you put it in terms of carbon footprint at a fraction of carbon dioxide in terms of overall impact now if you put it together there are other things that one could do in addition to searching for the architecture search you could cut down the model size by using quantization and we also have work here um flight on end as well as pruning in more detail typically you would start from a full precision and then you could go all the way to a binarized neural network in between you might have different quantized lighter versions of the neural network that allow you to perform all these multiple accumulate functions with a smaller footprint memory footprint as well as using just logic functions as opposed to full precision approaches you could also improve on their accuracy like in our work we presented last year by allowing flexibility in the number of bits um for representation as well as making binarizional networks even better by using regularized loss uh to improve accuracy and finally you could also add pruning as an additional approach to make these even better in terms of low power and efficiency so when you put everything together you can expect in terms of training uh three orders of magnitude in efficiency as well as up to twitters of magnitude in inference when you deploy these models overall when you implement this methodology you can perform optimal design for a hardware constraint deep learning application running on edge which can enable you to cut down on the energy cost during both training and inference with this i would like to thank you for listening to my talk and i would like to thank my collaborators at carnegie mellon google microsoft as well as my students if you're interested you can find our code and more information on these slides thank you all right well thank you everybody for for attending and thanks to the three speakers for three incredible talks we're very very lucky uh that uh to have such such an incredible lineup and i really really enjoyed all the presentations it's uh it's rare to see this kind of work presented one after the other in rapid succession and it was it was really really interesting um so please the attendees can write questions in the chat we have a lot of eyes on the chat so every time you ask a question it will pop up we will see it and we'll get the speakers um to answer but to just get things started um i have uh i have a few questions i have a lot of questions i think this is really exciting work and it's really timely work um but one thing that came up in the chat and i've seen one of these ask about it um was about the importance of if you could focus on one thing right now to try and make uh energy consumption better and and the two things the two aspects you could focus on are training models and tuning them versus uh after you train them and tune them the deployment and the energy consumption at deployment time which one do you think it has the biggest impact in terms of energy consumption so the the premise that i have is that um which which shows up when in this accounting for tuning as part of the total energy consumption is that the more you query a model you use it you run forward and backward passes the more energy you require and at some point when you deploy it you have a lot of users a growing number of users using a growing number of models that are growing complexity and so do you think these two lines of cost will intercept each other and one will overtake the other do you think one is clearly ahead of the other um yeah so maybe i will i will ask everybody um but i will i think i will start with emma because you i think in in in in your talk you mentioned uh you talk a lot about training and so i was curious to either what you think sure yeah um yes i've just been focusing on training because i think there's a little bit there's been less work there um and i think people like sort of actually discount the amount of um energy that is used for training actually because we think like in an ideal world you know you train the model once and then do inference a bajillion times and it's wonderful but like in reality in production um you know you're retraining these models on a very regular basis on more data every time like you know as as you get more data um but yeah i mean there's been a ton of work on making inference more efficient um and i would love to do a follow-up work to you know our earlier work on like um uh the carbon footprint of training these large models unfortunately i haven't quite figured out a way of um quantifying that for inference you know it'd be great to figure out like for five minutes of browsing your facebook feed or something like what is the carbon footprint of that facebook would never let me like actually release that but like maybe it's in some form um i think that'd be really interesting but i think it's hard to quantify i mean my intuition sorry i'll stop talking really really quickly no no i i have a follow-up yeah so you were saying um yeah i mean i'd still say my intuition is that like if you could compute like the amount of energy being spent on inference versus training um probably inference is going to be like way way more um yeah but your comment about the facebook feed reminded me that there was a page i don't know if it's still live on the carbon footprint of a google query uh it used to be used to be live at some point i don't know if there's if it's still even updated or uh but that's not gonna burn in there they like stopped reporting it yeah that's that's interesting because if you think about it when you go on a website or or whatever you know web service in the background that queries so many models if you start thinking about like when you write an email and it does character by character prediction of the next character or the next word or the current word that's a lot of queries yeah i think for making policy decisions and stuff it's actually going to be really important for us to know like what these numbers are like as the world moves towards like ai being integrated more and more everywhere you know if everyone starts using like siri or alexa or whatever to like turn their lights on and off like what is the possible savings of being able to automatically turn the lights on and off versus like the cost of like having to do this entire like nlp pipeline every time you want to like like a switch essentially yeah no i think it's important but sounds great uh it's going going clockwise in my own view of teams which may or may not be the one that you have i'm curious to see what diana thinks as well sure so um i mean we actually had a short chat before we even started live so um i actually think um there's quite a bit more understanding about what happens uh with the cost of inference and surveying as opposed to a cost of training and i think if we push models to the edge truly to the edge so um right now queries that alexa queries or even smartphone queries actually have to go to the cloud anyway so if you're going to push them to the edge devices then you need a way to characterize them in terms of power on that device not in the cloud um especially because other costs may be hidden or hard to characterize the cost of communication and actually there are additional issues related to that privacy i'm not going to go into details on that but um but uh i actually think um once you have an idea of your platform it's actually pretty like in your power we were able to characterize relatively easily the power energy latency of these constructs that we see in neural networks so once you develop them once for a given platform you can use them in composable fashion um the question becomes more complicated if you also wanted to co-design so if you want to build an accelerator then you don't know what the hardware is so in that case you need a good starting point and then you need to iterate between both the model the machine learning model as well as the hardware together but i actually think there is a lot of expertise in power modeling energy modeling um latency that can be translated to carbon footprint right now we i mean we used really high level um back of the envelope calculations to find the carbon footprint but there's no reason why we can't be more precise um and i think that's very important yeah thank you um thank you so i have uh from the chat since since i have you on the line essentially we are all on the line but you were talking um there is one question for you specifically uh and the question was has neural power been extended to include extremely sour extrinsic sources of latency such as network interconnect uh between multi-processors spread out in space right also any instrumentation regarding total memory footprint right so right now neural power is targeted for a given platform so you have the methodology you uh train the power energy and latency model for that platform um so you do model selection for a given platform everything um like the sources of latency network interconnect memory footprint are embedded in the models that we build if you want to be hardware uh aware um such that you can actually do this uh hardware design at the same time then um the model cannot be just neural network specific it has to be hardware specific so uh you how you're gonna have to do model uh selection and model training for power energy latency that includes uh the configurations for the hardware including uh network and memory uh footprint um so in in some sense it is included but it's um it's not you know visible it's in it's hidden uh you can expose it such that you can use it as a knob that's always possible the the challenge as i said in my comment is that the model becomes more complex so you need a way to separate the two so it is possible because it has been done in the past especially in embedded system design there's the concept of hardware software code design and people have done this successfully so i think it can be done in this case too thank you um so now moving on to vvn with the with the same question with the first question that i was asking which was the uh the attention paid to training versus inference do you think you know inference will overtake training is already ahead of training in terms of total impact um so i think i guess i'll answer the question in a different way so i think certainly for each of these things we know that inference consumes less energy than training but then there's more inference requests i would say similar to what deanna said that there's a lot has been a lot of work in making inference more efficient so if you're talking about which one has the most excess compute which has more possibility of improving i would actually say the training side has a lot more possibility of improving inference i say is that there's been a lot of work that might be more challenging on that extent um on the inference side though you do have and this actually came with the discussion from emma on the chat before what i was thinking about for inference is you can actually tailor to make it much more application specific and this kind of also leads to what diana was saying about like specialized hardware so you have this often a lot of inefficiency and including the data movement comes from flexibility of the design and if you can tailor it to a much more specific demand or specific application then you can also shave off a lot of that inefficiency that's both in the algorithm side and in the hardware side so we did a study of this in this cast paper i think it's 2017 we're just comparing like basically handcrafted approaches but more like hardwired weights versus general weights that can be reprogrammed um and they're still like orders of magnitude difference in terms of efficiency another good example of this is if you compare um running dnns on images compared to video compression so prior to all of this i was working on video compression and the energy that we spend on video compression which we're all it's very ubiquitous is again orders of magnitude less energy per pixel than dnns and one aspect of that is because we very much specialize video compression at standardized so you could just imagine like the weights are fixed basically and if you can fix them for this application the efficiency is much better so i think there's from that point of view there's a lot of opportunities on the inference side of things as well but just in terms of just inefficiencies in terms of even like like i tried to really emphasize on my talk and i think i'm also mentioning like reproducibility just like people doing the same thing over and over and over again there's a lot of waste just there happening that's really unnecessary that's that's that's great that's interesting thank you which which kind of leads me to the second question to a second question that i have meanwhile i'm trying to interleave questions that uh actually we have a question uh for the panel um what would our ai world look like if when transformers go from terra parameters to beta parameters and if there isn't enough computing or energy available to train them to reach further advancements this is a open mic so whoever wants to take this one i don't think i'll go ahead and i'll initially say like i think this is like a fun direction you know for a couple companies to do you know in the next couple of years to see how far we can push these models by just making them bigger but i think like they're not going to be useful unless you're going to make them smaller and i think at all these companies also there's a bunch of like engineers like frantically and researchers but mostly engineers like francis they're trying to cram these models into production with like quantization and distillation um and like maybe varsity sometimes so i mean i think they're just not going to be in production unless we can actually make them a lot smaller um think also people are typically not running on the hardware that has enabled this like confusing gpus actually still cpus at most companies um and like no researchers are really interested i mean feedback that i've gotten is like it's not interesting from a research perspective to do stuff on cpus because everything is just going to be um gpus or like specialized um you know co-designed hardware in the future i don't know oh and and i ran there yeah no we love renting panels that's what makes panels interesting uh do you know if you have anything to add to this so as you can see so i sort of cut out a little bit for you but like am i your comment was that researchers don't want to use specialized hardware for their research is that oh no i'm actually just like cp so like i think currently a lot of companies are still using cpus or production to run their workloads um uh yeah so i think people need to make these types of platforms efficient platforms more accessible i think that that's really one of the key things right i think there's a lot of research but like actually making it useful and having i think the software stack people are working on that but to make it much easier to use and i mean like it's kind of it is kind of ridiculous that this stuff exists but someone just needs to close the gap um to make it usable and also educate people on how to use it also i mean i've heard this anecdotally that people you know in the past didn't want to use 16-bit floating point because you're going to lose a couple of like point whatever percent loss and accuracy but you know the efficiency is much higher right so also as a community i think it's important to properly evaluate the trade-offs of um doing compute more efficiently versus the incremental gains and accuracy or whatever quality yeah before we move on i wanted to see if diana had anything to add yeah so i mean um the question was whether we're gonna have enough computing energy or dollars i don't think that i mean we are here because um gpus uh have democratized in you know like vivian said have democratized access to the kind of applications and advancements that we see the reason we now talk about deep learning is because of the computing because of the advances in the hardware so um i mean machine learning neural networks have been around for more than 40 years um and now everyone is doing research on them because they do have access and they think they understand i don't think we do understand what happens in all geometrics or all kinds of machine learning models um but we do have access to to these resources so far and i actually don't think um i think democratizing access is something that we need i find more and more um it it's more difficult especially for academics unless they have access or a partnership with industry to actually move forward in advancing these things i mean emma's models they take a lot of resources right my my students whenever whatever research they do uh pretty much they have to have internships for at least a summer every year if not more than a summer to get data and to get meaningful um results so i think what it needed is a meaningful partnership between academia and industry to actually make sure that you know we have energy or dollars to continue advancement because we have uncovered the more computing we have available the more we can understand about machine learning in general but um machine learning is now here computing is here at least in access so we need to bring it here to advance to the next level so it has to be a partnership um so i it's a question of uh how we can do it not a question of this yeah i i agree and i've i've seen this play out in my own research with collaborators who are in universities and um and used to be at microsoft and they uh and at some point we had to continue projects and computational resources became a problem because we have them on this side uh they have grants and everything right you know but but it's it's the the scale is different so i completely care but i thought i saw a proposal recently about like like a national supercomputer for ai workload specifically i don't remember it's one of those things that floated in my twitter feed i didn't really go very deep in it but um but yeah it's definitely a big issue i think and it's an issue as we have interns who are now about to finish internships right and access to completely becoming a problem um but something that came up in this discussion and and it's super timely and top of mind for a bunch of people because it's in europe's reviewing deadline tomorrow uh so plea if you haven't submitted your reviews and you're watching please submit the reviews um but now the new thing in the eurips format was that you have a statement of impact and i look at a lot of paper in the near architecture search and basic optimization and that kind of style community and pretty much i would say 20 statement of impacts were on the reduction of energy consumption and some of them i think were genuine and some of them were just because i reduced this thing by an hour so this used to take 24 hours now it takes 23 hours so statement of impact i reduce the the carbon footprint of this this model um so my question is i will get to my question my question is there seems to be a part of the community of we know here well represented by you which truly seems to care about energy efficiency energy consumption of models and and you know sustainability of progress in ai there is seems to be another part of the community that doesn't seem to care at all like they look for those you know 0.2 percent not a typo 0.2 percent improvements at the expense of i don't know as a proxy for energy 200 million more parameters and then there seems to be a middle ground in the community which seems to be caring a lot about well let's make this faster and by making it faster i can say make the point that it's it's also more efficient um so my bigger question is how do we actually get everybody all these ends of the spectrum together uh such that energy consumption of a model becomes like a primary metric in in a table in an europe's paper uh because right now accuracy is involved and then you see number of multiply ads you know or multiply accumulates and then you see uh maybe number of parameters but you know uh is it a matter of getting uh more accurate measurements more easily accessible measurements uh what and whoever wants to take it if i if i can take that um i think this happened in other fields right i mean if you think about power consumption it has to with electrons and holes and things moving and the fact that vivian and i come from different fields actually i'm i'm not a machine learning expert uh she's a circuit designer at least she started as a circuit designer i studied as a computer hardware cad many years ago but this happened in computer architecture 20 years ago so people didn't care about anything but performance so they threw more resources until they realized what i'm getting is marginal performance when i say performance i mean not machine learning performance but you know um instructions per cycle or you have what you have so um so around 2005 um and i think we've seen one of those charts there everything started to flatten because um you know if you don't address power things start to combine you from behind so that's where we moved to multi-core and that created the gpu revolution and this is why we're here now so i think something similar happens here what i think needs to happen is a few things first of all there needs to be an acknowledgement from the community that this is important so uh we can come up with models we can come up with accurate ways to characterize and come up with efficient ways to identify the best way to design the hardware needed for machine learning but if there's not an acknowledgement that these are important and like you said deserve a column that people care about people are still going to hone in uh oh i went from 75.62 to 75.82 that's still going to be important of course once there is that acknowledgement we need it then we need accurate models we need a way to connect the physics to the machine learning model including the hardware in between and once you have the models then you can say okay how do i do a better job of building my machine learning models and the hardware or together either separately or together to deliver on the promise so i know it was a long quite an answer but i think it needs buy-in from the community in general and you know the answer to the broader impact of whatever work is submitted to eurips should be quantified i mean it's okay now because maybe it's not we don't have the tools to quantify them properly but once this tool exists um they should be usable by everyone uh you know you run your model through this if it's understood that it's acceptable and then report those numbers and then your your statement is going to impact energy by this much will actually be backed up by numbers um so i think that's how we get there i don't know how long it's gonna take but without that we're not gonna bridge the gap right between what hardware is able to do now and what machine learning won thank you yeah and i completely agree with that i'm curious to see if emma or vivian want to chime in yeah so kind of building on that i think another way to build consensus is to identify common issues that we could address that would benefit both improvement in accuracy and advancement of the machine learning field and improve uh these carbon emissions which would also be again i had to harp on this would be reproducibility um and understanding the deep learning models i think everyone should be interested in reproducibility and understanding how the deep learning models work if you understand a bit better then you won't be doing like random hyper parameter tuning to get higher accuracy you would actually also understand if this point two percent accuracy is actually important i think maybe as a machine learning field one should ask should one care about point two percent i mean we're picking on that number but like you know what i mean like that that does that merit a publication if it doesn't explain how that was achieved and if it was not achieved through some reproducible means right so i think uh maybe a deeper inspection in terms of where the improvements are coming for and that should be valued more in the field as opposed to purely what is state-of-the-art right in some sense um so i think that's beneficial for both from a compute site and a machine learning field advancement side and i think that's something that i would think people should be able to agree on now i think one challenge of that is that to do more um to do more like robustness studies you do need a lot more compute so it is like there is exactly if you want to when i focus this test yeah so when you have these models that are super you know expensive to trade once doing it multiple times is gonna be um so but i mean even when people are doing hyper perimeter tuning and they could just release their intermediate results and what did they tune together right like i mean just being more informative of what what you did with the um with the attitude or the goal of advancing the field as opposed to getting a nervous publication i think that maybe i should put it that way people should be thinking about contributing uh from a scientific and understanding point of view as opposed to being able to claim they have the highest accuracy or they have this many publications and i think that is a joint goal that would at least cut some of the issues and i also agree with what diana said that which is then people should also ideally report you know the the latency on what platform and maybe from that you can extrapolate um you know how much carbon emissions you have and i think people have started to work on towards more standardized tools to do that i think that would be great um it could also be informed by uh cloud providers also providing some feedback in terms of where like i mean they will know what the carbon intensity is the person who runs it doesn't uh people who build the framework should kind of also have some of those aspects easy to um be accessible for the researcher um so i think there's you know thing making things more accessible and then working on joint problems i think that are important to to both improving the field and computing is a good idea to do i agree i think yeah go ahead yeah so i was just gonna say i completely agree with all the points that have been made um just to go a little bit further on one of the things that um vivian brought up um i think like this is also sort of a symptom of a problem that we have in the field so it's like why is the metric like you know how can i just get a nurbs paper um and it's like you know this pressure that you know students and junior faculty feel to just like publish publish publish like as quickly as possible you know you're you know coming into grad school you're expected to now have like multiple papers and talk to your venues and i think that encourages this like really short-term thinking like low-hanging fruit type of research that's just like okay well if i get the number higher like the people get accepted which also is a symptom sort of like lazy reviewing um and which is also simply the fact that you know there's so many papers it's hard to review like when you get 11 papers to review for a conference um yeah you know you can't blame people for kind of uh taking the easy way out and reviewing being like okay this is an easy paper they made the metric higher great i understand i'll accept it um so it's kind of like really a big big problem um i think in the community is like how can we change how can we better prioritize like doing laboratories more visionary research um and how we better like evaluate people on that which is harder obviously than like counting beans counting number of papers and things like summer citations and things like this yeah i i think that's that's a great point i i wonder though if one could do like an interventional experiment in our brain and remove new rips and icml and uai and a stats from existence they don't exist anymore archive is the only venue would that solve the problem of you know accuracy column chasing or table chasing and uh and would people be more honest about the uh impact of their experiments and the the actual improvement over whatever the baselines i mean i don't think just getting rid of the conversations wouldn't necessarily help it's still i mean i don't know if it's like maybe the number higher right it's like very satisfying and straightforward like in a you know research is really hard and but like if you have this like really clear metric you know it's a lot easier yeah i was not i was not arguing to remove eurips i was arguing that how much of this is the the reviewing process which and and the the conference format and the fact that we have i don't know six deadlines per year that we kind of have to hit uh versus okay let's remove deadlines let's go to a kind of a journal style publishing model where the timing doesn't matter really and you can publish when you're ready and nobody really reviews it other than the community you know by citing it uh would that solve the problem and and to me it's not clear because people would still chase uh the same kind of metrics that's right so i mean just a short comment i think you would need to convince um i guess university administrators that um metrics like conference publications no no no no no no but i'm not arguing for it yeah it was just a like a thought experiment i think i think conference should say anyway there's some like um activities going on on the side for a lot of these conferences but reproducibility so as i understand um uh there's a reproducibility challenge that happens for nurips and iclear and so people kind of when they go through that if i guess if they are their paper somehow gets a check then people know that these things are reproducible i know on the acm side from the systems point of view there's been um these artifact evaluation committees at conferences so for machine learning like ml system which is a conference for machine learning and systems we actually have an artifacts of some community where like people after paper gets accepted there's a whole committee that will verify the code and see whether or not they can reproduce it and then the paper actually gets the badge and i think that that actually this could be a motivation because there's so many papers in machine learning that it would be great if you only looked at the papers that have these badges because you know that if you pick it up it actually works right and that would actually be an incentive to like elevate it above the rest of the stuff so maybe that's a way to further push that kind of effort and that's again good for the field and good for computing more types of these recognizing papers that have been reproduced before um yeah i actually agree i mean my students have been involved in some of these efforts in the systems area and it opened their eyes um into the importance of reproducibility so when they cannot reproduce results from someone else um there's a lot of back and forth with the original authors and until they can reproduce they will not stop right so uh sometimes it doesn't happen but it's it's really important to be able to achieve that oh one one follow-up quick things i think i think academia can provide the manpower for that but it would be gr person power for that but i think the important thing would be for companies to provide the compute flower for that because i think the compute effort to do reproducibility is very high and it would be great if corporations kind of stepped in and helped support that effort just in general i completely agree um sorry i said hey microsoft i know exactly well other companies may be walking this thing is public eventually so right it's a good thing to be associated with your company of course um no i completely agree um but one comment that as a follow-up is you remember at some point in europe said this kind of paper correct paper format checker you would upload your paper you would get kind of like a pass or a fill without that you couldn't really upload the submission i don't think they did i don't remember it in the last couple of years the server was going i mean it was you can imagine as europe scaled it became unfeasible um but i wonder if there is an equivalent service um that could you submit a pythorg model and gives you back the energy consumption at least of the model um based on just the format the shapes that's so you can do you cannot really pass data uh and maybe that could be the stamp a stamp um i mean we did something similar like that preliminary in our group where we like at least from a more custom hardware based on the shape of each layer of the neural net we could try and estimate what is the potential data reuse and based on the sparsity the amount of computer different levels of memory hierarchy and that's what we actually fed into the um energy aware pruning stuff that we did right now there's a much bigger effort i think diana's team has also did a lot of great in like the uh kind of power estimation based on the incoming layers i think if they're again if there were standardized ways of doing this actually you know who would be great at doing this companies who build gpus and those uh hardwares that they also released a model like that that would be um again making their platform much more attractive to users so there's an incentive to do that maybe so yeah i think that's a great point any last comments before we close all right well thank you again it has been an honor to us still um thanks so much for the attendees um to the attendees um there is going to be closing remarks starting in a second uh so please stay around for those and uh thank you to the speakers again this is incredible thank you so much for having me yeah this is great well hello again i'm sandy blythe and i wanted to take a moment to thank you for joining us we're grateful to all of you who've been able to come together virtually to participate and engage in discussing the current and future perspectives on the frontiers in machine learning i know the msr team got a lot of value from the rich content and the interactions and i hope all of you got a lot out of it as well now i'd like to invite vani mandeva who is our general chair for the event to provide some of her perspectives thank you sandy and thank you all of you for tuning in to the concluding session of this event the frontiers in machine learning event was built on the tradition of the faculty summit event that microsoft research hosts in redmond and has been doing so for the past two decades we had to cancel the event this year due to the covet 19 lockdown and it took the talent and dedication of many many people to pivot the event to this virtual format specifically chris bishop our executive sponsor and the scientific program committee our logistics lead sarah smith and our speaker manager jen vancic did a heroic effort in keeping everything on track craig tushar our producer along with henry honig graham reeve and many many others on jeremy crawford's amazing media team in microsoft research there is a silver lining in all of this we were able to register almost 2000 attendees for this event from over 300 organizations in 35 countries we were also able to include a significant number of graduate students at the event this year this would not have been possible if we wouldn't have transformed this event into a virtual format all the talks that were presented throughout this event are available for on-demand viewing on the microsoft research website a big thank you to all the microsoft research session leads and all the faculty speakers who worked with us patiently and gradually as we transformed this event to what you experienced over the past few days so this concludes our frontiers and machine learning event and while we regret that we weren't able to meet you all in person this year we're also thankful for the opportunity that the broad machine learning research community has provided us in coming together for the event microsoft research is committed to continuing to foster research and academic collaboration and during my opening i mentioned that there were some ways that we support the academic research i would note that just this last week we opened the nominations for the microsoft research at a lovelace fellowship as well as our north american phd fellowship nominations by the department chairs office are due for each of those by august 14th there's always a lot going on at msr however so please do stay in touch with us through all of the ways that you see here on the slide and also please let us hear your feedback via a survey thank you all and be well 