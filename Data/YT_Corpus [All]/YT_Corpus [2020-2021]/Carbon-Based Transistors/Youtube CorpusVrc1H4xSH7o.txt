 VIVIENNE SZE: All right. So it's great to be here today. I'm excited to talk to you about the role of efficient computing and reducing the carbon emissions of machine learning computing and discuss some of the challenges and opportunities that we have in this space. So it should be no surprise that there's been a growing demand for computing both in the data centers as well as in our devices, and a lot of the factors that drive this computing comes from the demand for machine learning and also the amount of compute required from machine learning. So it's been growing exponentially over the past few years. So this is something that we really need to address. In particular, because the cost of computing connects to carbon emissions. So let's see how this connection resides. So we start with what do we actually want to compute. This requires knowledge about what are the compute demands? What is the task that we're computing for? What are the requirements for that? Typically, in order to achieve that task, we might have some form of computing algorithm that we need to run. These days typically a DNN or deep neural network model. Then that algorithm becomes a workload on some form of computing hardware itself. It does the computation and turns out the results. And together we can say that this is how we actually compute, so the computing algorithm plus the computing hardware. Now, to drive the computing hardware, we have some form of compute energy, and where does this come from? Well, often it is dictated by two aspects, the energy source, which basically converts energy and also generates some carbon emissions, and then when it generates the energy we need to deliver this energy to the computing system itself or make it into compute energy, so there's some energy delivery there. And there's also some factors, for instance, like cooling since the computing hardware generates heat, we need to cool it in order for it to operate. Now, these two factors of energy source energy delivery and cooling, really depend on where we're going to actually compute. So we'll cover these three topics in today's talk. We'll first focus on how to compute, then we'll discuss where to compute, and what we're actually computing, and look at various opportunities and challenges in these spaces. First, start let's start with how to compute, in particular, the computing hardware itself. So one of the big challenges in designing efficient computing hardware is that transistors are not becoming more efficient. So over the past few decades we had Moore's law, which was giving us faster and smaller transistors and we had Dennard scaling giving us more efficient transistors. But we can see that this trend has really leveled off over the past decade. And so we really need a new approach to make more efficient hardware, and one of the approaches is to design specialized or specialized or domain-specific hardware rather than using general-purpose CPUs, in order to achieve significant improvements in both speed and energy efficiency. So if we're going to design specialized hardware we need to first ask the question, well, where is the energy going in the hardware itself? And so as it turns out energy is dominated by data movement, and what I mean by data movement is basically how do you deliver the inputs to your computing engine. Let's say you need to do a multiply. What is the cost of delivering the inputs to the multiply itself and also the cost of writing out the results of that multiplication. Now, if we compare the cost of doing these arithmetic operations like multiply and adds to memory reads or data movements we can see that there's a drastic difference in terms of energy consumption. So, for example, reading from a very small on-chip memory, like an eight kilobyte memory of SRAM on the actual system, reading 32 bits of that is already going to cost more energy than a 32-bit floating-point multiplication. And this energy is going to increase even more if we look at the last line when we read the data from off chip memory so the further the data needs to come from for example from DRAM the more energy costly it's going to be and i should highlight that the scale here is in the exponential scale. In summary, memory access is going to be orders of magnitude higher energy than the cost of computing these arithmetic operations and that should be what we really want to focus on if we want to build energy efficient hardware. Now, if we start thinking about how we can reduce the amount of memory access that we have or the data movement cost, one thing we can do is exploit data reuse. In particular, there's a lot of data reuse opportunities in deep neural nets and this basically means if I access a piece of data from memory can I reuse it multiple times for different operations. And there's a lot of opportunities for this in DNNs. So, for example, if we think about convolutional neural nets, we have what we call convolutional reuse, where the activations and weights the same ones are going to be used just in different combinations for different multiply and accumulate operations. We also have feature map reuse where we apply multiple filters to the same input feature map and so as a result any activation in the feature map is going to be reused multiple times across the different filters or you can think about it as generating across the different output channels. And then finally you also if you're going to process more than one image so if your batch size is greater than one we're going to have filter reuse because each weight in a given filter is going to be reused multiple times across the different feature maps. So we want to be able to exploit this reuse we also want to have the right hardware that allows us to exploit this reuse. And this reuse can be exploited with hardware that contains low-cost memories. And by low-cost memories what I mean is very small memories under one kilobyte. And the benefit of these small memories is that it's very cheap, it's located very close to the compute, and so accessing these memories is going to be very low energy. So just to give you an idea let's say a multiply and accumulate in an ALU cost 1x. Reading from a very small memory, which is under one kilobyte, is also going to be the same amount of energy. As compared to if you read it from a larger memory like a global buffer, which is about from 100 to 500 kbytes, but it's going to be 6x the energy, and of course going to DRAM is going to be way worse. And so, in general, the farther and the larger the memories, the more energy and power it's going to consume. So the key idea is ideally I want to reuse and read my data from these very low-cost memory. So I read it once from this very expensive memory and then reuse it multiple times in this very small memory. The challenge, of course, of these small memories are small in capacity and we have these very large neural net with millions or billions of parameters and activations. And so what we need to do is we need to chop up this very large neural net into smaller chunks and then we need to figure out what is the order of chunks that we should process in so we can maximize the data reuse and these chunks have to, of course, fit in this low cost memory. And so the order of processing these different chunks is referred to as the data flow so how the data flows through the system and how can we exploit the most reuse. And there's been a lot of research in this space. One approach is a data flow that we explored and the Eyeriss architecture and this data flow really tries to balance the amount of reuse across the various different data types weights activations and partial sums and by having an efficient data flow, we can exploit enough reuse to reduce the amount of memory access from this global buffer by 100x. And reduce the amount of off-chip memory access which is the most expensive by over a 1000x. And so actually we realized this Eyeriss architecture in an actual chip that we fabricated and when we measured the power consumption of this chip, it's or the energy consumption of this chip, it's an order of magnitude more energy efficient than a mobile CPU really again by focusing on minimizing data movement through energy-efficient data flows that exploit reuse. Now, this is one approach for doing this. If you look at more emerging architectures in terms of how to reduce or make computing hardware more efficient, one area to explore is in-memory computing and the idea here is if we want to reduce data movement why don't we just move the compute into the memory itself as opposed to sprinkling some memory beside the compute and the idea here is can we perform computations with the memory storage elements themselves. In order to do this, we actually have to turn to analog computing, which basically means we need to represent the activation weights and the partial sums using analog voltages currents and resistance so for example one way to do this is that your activations are represented by voltages your weights are represented by the conductance of a resistor, which is the storage element itself using Ohm's law when you multiply the two you get a certain amount of current that comes up and that's already going to be the product and then finally using kirchhoff's law you're going to accumulate the current across these different resistors and then that's going to get do your accumulation itself. So, as a result, you get free basically multiply and accumulate by computing in the analog domain. Now there's some challenges here as well of course there's going to be increased sensitivities to the non-idealities of these devices of the resistors and so on. And then also you since you're doing the compute in the analog domain there's going to be overhead to compute to basically convert from digital domain to analog debate and back there's a lot of exciting research into the space, in particular, there's a lot of work on coming up with new memory devices that are more efficient for this computing it might suffer from less non-idealities. Another way to do efficient computing is actually using light rather than electricity. So the cost of moving a photon can actually be independent of distance. So as we mentioned before often in terms of electrons if you move it further you're going to consume more power but here it's going to be independent of distance. And also the multiplications can be performed passively in light as well so that also makes it more efficient you can think of these are two examples of some processors that are designed with light and kind of some of the key ideas here are things like well you can have a beam splitter that kind of dictates how much light goes in one direction versus the other and that's the splitter itself how much light you divide into these two direction depends on the weight of the neural net and then the light the intensity of the light coming in is the activation. So that's some of the exciting work that's going on in computing hardware and making it more efficient. We can also make the algorithms themselves more efficient in particular there's been a huge amount of work in the design of efficient DNN algorithms. We can roughly categorize this work into three various areas one is what we call network pruning, which is basically setting some of the weights of the neural network to zero and we know that anything times zero is going to be zero so we can save energy there. We can also come up with more efficient network architecture so we can have these large networks or filters that we decompose into smaller filters for example like depth wise or pointwise layers and then there's also been a lot of work looking at reduced precision can we reduce the number of bits of the multiply and accumulate or the max and the weights in general a lot of this work particularly the network pruning and the efficient network architectures focus on reducing the number of max and weights but the real question if we care about from the system's point of view is does this translate to energy savings and reduce latency and in particular for this talk does this translate into energy savings. Well, unfortunately, a deeper dive into this show that actually number of max and weights are not good proxies for latency and energy. For example, if you took a look at this plot released by Google they show that the number of operations does not approximately latency well so for example for a given number of macs you can have a 2x swing in terms of latency and for a given latency you can have a 3x swing in terms of max similarly just by looking at the number of weights for example having a small network alone is not going to be a good metric for energy because actually you need to account for all data types so for saying that energy is determined by data movement it's not just the weights that are moving through the hardware but also the input feature maps and the output feature maps and so on and we can see here from example looking at GoogLeNet the weights only account for 22% of the energy. So really if we want to design energy efficient algorithms, we really need to focus on the direct hardware metrics like energy efficiency and latency as opposed to number of weights and max a number of weights and max alone so one example of doing this is incorporating energy into the network pruning algorithm so in the past what folks looked at was they looked at the magnitude of the weights and then removed the weights that were small and so for the same accuracy this can give you a 2x reduction in energy consumption which is great however we know that the magnitude of the weight really is that doesn't have very much to do with the energy consumed by the weight and actually what you'd like to do is remove the weights that consume the most energy and so one way of doing this is that we could sort the layers of the neural network in terms of highest energy layers to lowest and then start pruning the highest energy layer first and the idea here is that hopefully you'll be removing the highest consuming energy consuming weights first because we know as we remove more weights and the accuracy is also going to drop so you're trying to get a better trade-off between accuracy and energy and by doing this by factoring in energy into the prunings of energy aware pruning we can now reduce the energy consumption by 3.7 x which is about 1.7 x less energy than using magnitude-based printing and this is for the within the same accuracy so the key take away message is that it's really important to directly target energy in the design of efficient nets if that is the ultimate goal which it is here similarly when you start thinking about not just the weights but thinking about network architecture design or using neural architecture search to automate the design of the network architecture it's important to also there take in measurements like latency and energy to drive that optimization itself so one example this is this NetAdapt work that we did in collaboration with Google's mobile vision team and really the goal here is that we want to automatically adapt a DNN model for a given mobile platform to reach a certain latency or energy target and we use empirical measurements from that given platform of latent latency and energy to drive and make decisions about the network architecture itself in this case the number of channels for example for every given layer and by driving the design with empirical measurements again rather than max and weights but really looking at measurements we can get actually a 1.7 x speed up for a mobile net with similar accuracy and another really important key thing i want to highlight here is the fact that this particular approach uses very few hyperparameters which makes the tuning effort less and less and that actually reduces the energy consumption or for the actual training itself so not only do we want to reduce the energy consumption of the resulting model but through the training process it's also important to reduce the energy consumption as well and the few fewer hyperparameters you have there then the less tuning that you have to do finally we talked about in-memory computing it's also important to think about how you might want to design DNNs for this new emerging type of processor I mean it actually could be quite different from how we typically design DNNs from our existing digital processors so for example if we take a look at the accuracy of DNNs on existing digital processors they might be different from how they might perform if you run them on these in-memory computing processors because you need you have the factor of non-idealities and in memory computing so it's important for the network to be robust to these non-idealities so for instance shown here if we look at how these networks might rank with zero noise um the top ranking accuracy or the top accuracy network might actually not be the top one when we start injecting noise it actually falls quite quickly and so basically the same networks that perform well on digital processors might not perform well on in-memory computing so that's one thing to think in so again we need to rethink the design another thing is again there's this trend of reducing the number of weights in DNN and this might be less desirable for in-memory computing one because the main focus of in-memory computing is reducing the weights of the data movement or the data movement of the weights and so as a result if reducing number of weights is not going to help if we've already reduced the its data movement it might be more beneficial reduce the data movement of the activations another thing is as we mentioned you need to convert from analog to digital and there's some overhead for that so it might be useful typically in these architectures to have in these in-memory computing processors to have a very large array but then if you have very few weights you might have very poor utilization of that array so while for in-memory processing or memory computing it might be preferable to have shallower and larger filters this is quite different from the current trend of deeper and smaller networks for digital processors so really again when we're designing these efficient DNN algorithms we need to think of the computing fabric below as well to get better results and there's a lot of research in this space so actually we recently published a book on this topic really kind of cross cuts from computing hardware to computing algorithms and how can we do this efficiently for deep neural networks so i invite you to visit that if you're interested in this topic. Alright, so moving from how to compute to where we actually compute this also has a lot of implications in terms of the energy consumption itself and the carbon emissions and how it relates to the compute energy that's being consumed by how we compute so where we can view one thing is the energy source itself so depending on where you're doing the computation the electricity grid in that region might have different levels of carbon intensity which is basically the ratio of the carbon emissions or the weight of carbon emissions over the amount of energy that's actually being produced in kilowatt hours that we're showing right here so we should factor that in also if you're using renewable energy that percentage of renewable energy varies with the time of day so obviously for example during the daytime you're going to have more so solar energy available than in the evenings and so that's something to consider these vary. Also, energy delivery so once you've acquired this energy how do you deliver it actually to the computing hardware and community itself you need to factor things like power conversion from going from let's say AC to DC the loss in the wiring that you're sending it changing in voltage and also cooling costs as we mentioned computing hardware generates a lot of heat so you also to spend energy to remove the heat or cool it down. A prime example of this is in the data centers itself um so shown here is a breakdown of the energy consumed in a given data center typically they measure this efficiency of confusion converting from energy to compute energy as the power usage effectiveness or pue it's the ratio of energy over compute energy and typically this is in the range between two to one point one where one is optimal so for example if you have a ratio of two that means only half of the energy that you are generating actually gets used by the computing hardware and, as it turns out, this is an exciting opportunity where machine learning can actually help improve this efficiency by providing better cooling okay so we go from how to compute where to compute and now we should look at what to compute itself and this is really trying to factor where the commute demands come from they can come from number of the request that we get the amount of data that we need to process and the required quality of results which we typically think of as in order to reduce the number of requests one thing that we can do is make hyper parameter tuning easier we already hinted towards this if we can for example reduce the number of hyper parameters to tune we can reduce the amount it takes to do training for example to find the right hyper parameter reproducibility is also very critical for reducing unnecessary requests due to replication difficulty so if you can make your results easy to replicate that's going to be fewer requests that need to be issued in this therefore less carbon being generated this is of course also good for advancing research and machine learning and there's a lot of ongoing efforts both in the machine learning community as well as the systems community we want to reduce the amount of data that's required because we know that data movement is expensive so thinking about ways that we can exploit data reuse and thinking about new algorithms that are more data efficient and that could maybe incorporate prior knowledge into these DNN models this is already kind of an area of research in machine learning in general but if there's an additional benefit now that it also has energy benefits if you need to use less data and then finally run it evaluate carbon like the trade-offs between carbon emissions and quality of result right so what is the cost benefit of these situations for example is it really worth a while to get that point two percent let's say increase in accuracy for imagenet for a huge amount of increase in carbon emissions for much longer trading time or much less efficient models these things should actually be taken into account and in order to really assess this we also need a deeper consideration of what is a meaningful quality of result for a given task so for example again look using image as a example is achieving 0.2% better accuracy on your ImageNet data is that actually going to allow you to achieve a good quality of result in your final goal so understanding the trade-off there is really important excitingly there's also a lot of things that we can also do today that kind of best practices can help us improve our efficiency so we have a lot of hardware and out there that can be already very energy efficient can we make those energy efficient settings or configurations the default or make them very easy to access this requires software and framework support for things like reducing precision and specialized hardware that's easily accessible this exists today but it could be made easier to really reduce the buried entry so that this could be the default for many practitioners and researchers to use can we enable us can we measure and report the energy consumption and carbon emissions so that people know what their carbon footprint is this requires software and hardware support to measure the energy consumption and kind of system level support to report the carbon intensity of whatever given energy source that's being used there's already research that's emerging in this space in particular people looking at frameworks for standardized reporting so i invite you to take a look at this archive paper they also have some nice commentary on how to reduce carbon emissions or think of carbon emissions in this space with this information then it's important to try and run your experiments when feasible and locations at times where there's low carbon intensity and again ensure reproducibility for the obvious reasons and what's really exciting about all these things that much of this we can do today or in the very near future it's just a matter of putting in the engineering effort to enable this so what are the key takeaways because we covered a lot so the first is in general the very high level we want to jointly consider energy efficiency and accuracy and machine learning research we want to consider the trade-offs between accuracy and energy for a given application and maybe try and develop data efficient machine learning techniques that also benefit energy efficiency when we're designing machine learning algorithms for energy efficiency we should actually make sure that we directly target energy consumption as opposed to just looking at number of max or weights and also look at know designing specialized machine learning hardware that really reduces data movement in our day-to-day activities we should incorporate energy efficiency considerations in our best practices so we should lower the barrier of using existing energy efficient computing options and enable reporting measuring of energy consumptions and carbon emissions we should try and compute in the locations of the lowest carbon intensity and highest power efficiency and again reduce unnecessary computing by ensuring reproducibility so all of these slides are available on our website we've done a lot of work in this kind of efficient computing space and so i invite you to visit some of the other resources that we have on our website including videos and again if you really want to do a deep dive on efficient computing primarily for deep neural nets i invite you to visit this book thank you very much  