 - We have a wonderful session today with Megan Price here who is the Executive Director of the Human Rights Data Analysis Group, or HRDAG, and thank you Megan for taking the time to come. Before we get started though I wanna just give you a quick snapshot because we're right at the end of, or close to the end of our quarter and there's just a few sessions left so I wanna make sure everyone is aware of what's in store for the next few weeks. So first of all this Wednesday we'll have a Jupyter Notebooks session at one o'clock. And if you come at 1:00 or even a few minutes later that's fine. That's great, you don't have to know Python or R to participate, but it's a way to explore the topics that we're talking about in more detail. And this time we have two Jupyter Notebooks from Google. These are fairness tools and that they developed actually for their own products and one looks at measurements of fairness like false positives, false negatives and that sort of thing. And another is more of a what if tool that's looking about if you add more data, what would be the impact on your fairness overall? So anyway please join us, that's this Wednesday and then next Monday we won't have a class here so just please mark no class next Monday and that's because we'll have the Women in Data Science conference running all day. In fact Megan was one of our best speakers a few years ago and that, if any of you are interested can be live streamed and we also have around 200 events around the world happening in the next few weeks around this conference, there's one session during the day, so it's I think 9:00 to 5:00. There's one session in the morning on ethics that you all might be particularly interested in. It's a panel led by one of our earlier speakers in this series. Professor Margot Gerritsen who's here, yay. So definitely tune in to that or the entire day. So then instead of next Monday we'll have a special session on Thursday next week, and that will be held in a different place. It will be held in CEMEX which is in the complete opposite end of campus in the business school and hopefully you can all make it there on time because it's a live podcast. We're doing this in conjunction with the school of engineering and it's a Future of Everything podcast, and this will be featuring Provost Emeritus, John Etchemendy and Professor Russ Altman in a conversation about the future of AI. I think it's gonna be fabulous. I definitely welcome you all to attend. But please try to get there especially next week on time 'cause they have to I think even close the doors just a few minutes after starting so they have silence for this live taping. - [Man] Is that at 4:30? - It starts at 4:30, same time. - [Woman] So, about five minutes after that. - Probably 4:30, 5:00 when we'll actually close the doors. And then finally we're going to end on March 9th with a look at the multi-disciplined approach to AI featuring Timnit Gebru who's at Google doing some really interesting work in fairness and accountability and bias. And then also Professor Omer Reingold who also looks at fairness from a different perspective, algorithmic fairness and also it's going to talk about privacy, in particular differential privacy. So I think that will be a great way to end and we of course welcome all your thoughts. If we do this again, what other topics you'd like to hear next time. So okay, so back to today we have Megan Price here and just real quickly I'd like to say this human rights topic is one I'm particularly interested in. Megan has a great way of describing both the challenges with AI being used and potential issues with human rights, but on the other hand how AI, how data science and machine learning can also be used as new ways to approach and help human rights advocates to really boost their work and gain visibility for their issues that they're facing around the world. So we're delighted to have her. Her background just to let you know is statistics, master's and PhD from Case Western Reserve University and then she went on to get a PhD in bioinformatics from Case Western, and I believe went to HRDAG soon after that. So she's been there quite a while. So anyway thank you again for coming and she'll stick around for a few minutes afterwords if you have any questions. All right. (applauding) I'm gonna hand this over to you. - Thank you. Great, well thank you so much. I'm just gonna keep myself honest here so I leave us time for conversation and questions. So thank you, thank you so much for that introduction and thank you for inviting me to be a part of this conversation. I have been watching the earlier talks in this seminar series and I've been really enjoying the discussion that's coming out of those. And I also feel like it's made my life and my job for this talk really easy 'cause I kind of slot right into a lot of the issues that have been raised in some of the other talks and a lot of the concerns that folks have talked about. So just to set the stage, what I plan on doing is very much what was mentioned in the introduction. I'm gonna talk just a little bit about my organization and who we are, what we do, how we got into this particular area of work. I sort of thought for this audience that it might be useful to pause for just a minute and talk about what we mean by human rights, what they are and what this work has to do with them. And then I'll give a couple of case studies from our work specifically and then I'll talk just a little bit about some other examples. Really excitedly I'll get to draw from some of the Google AI Impact Challenge grantees who I know you heard on a panel earlier so it's nice to be able to circle back to some of those points. And then I'm gonna close with this question of well how could this all go horribly awry and hopefully I'll be able to close on a positive note, but essentially what my team and I end up worrying about a lot is how can we devise a rubric to identify these case studies where these tools have the possibility to do good versus these situations where they might do more harm? And so that's kind of how we think about that rubric is by asking ourselves well if things went wrong, what would those things be and who would they impact, so that's what I'll get to at the very end hopefully. So my organization is the Human Rights Data Analysis Group or HRDAG. We are a nonprofit organization based in San Francisco and we are a team of data scientists. We have training like me in statistics, computer science, political science now that data science is a degree that folks can get, we have folks with that degree. And essentially we think of ourselves as the behind the scenes technical experts working with partner organizations to strengthen human rights advocacy efforts. And so we've been doing this work for almost 30 years. We've had the good fortune to work with a wide variety of partners. Large international organizations you may have heard of like Human Rights Watch and Amnesty International, small grassroots NGOs in various countries that we've worked with and everyone in between. But that's really our approach to this work is we bring the technical expertise and our partners bring the field and contextual expertise. And we bring those together to hopefully answer a substantive question or questions of interest and strengthen some sort of advocacy effort. And so when that advocacy effort is about human rights, what is it that I'm talking about? There are a lot of international treaties, international law that codify human rights, but it all starts with the Universal Declaration of Human Rights and for those who are not familiar, this is one of my favorite photos ever. This is Eleanor Roosevelt holding a giant copy of the UDHR. Our server in our office is named Eleanor, we have a copy of the UDHR framed by the door so we can reference it as often as we need to. But essentially this is the starting point. The UDHR contains 30 articles, it outlines all of the rights that any individual by merely being a human person is entitled to regardless of what country they live in, what jurisdiction they may be under. And my organization in particular tends to focus on right to life, liberty, the right to not be tortured, the right to not be arbitrarily detained. But there are a wide variety of rights that are covered in this document and in the various ensuing international documents, right to privacy, right to equal representation under the law. And so as we talk about these human rights applications, they actually cover some of the examples that have already been brought forward and some of these other seminars talking about AI and government and some of these other social good examples that you've already talked about. Now for us and for our team, where the data science work comes to play in these advocacy efforts is in really taking what we think of as a moral obligation to do the absolute best work that is technically possible. And that's what drives all of the projects that we take on because we believe that we have that obligation to pay due respect to the victims and the witnesses who are sharing their story with us. And so for us that's the starting point for why would we use this method instead of that one? Why do we need to debug this code in this particular way? Why do we need to use principle data processing? All of the other technical rabbit holes that we ultimately end up going down are driven by this sense of moral obligation. And I should say that this is a direct quote from my co-founder and colleague Dr. Patrick Ball who really sets the tone on our team for the level of work that we do. And I also just wanna say that this is another one of the touchstones on our team. This is a political cartoon that I have framed on my desk from 2007 and we talk a lot and think a lot about truth on our team. And one aspect of that is what I think of as one part of what's being depicted here which is the progressive discover of truth, which I think is part of what science does. We are constantly questioning our methods, updating our methods, trying to do a better job of progressively uncovering the truth. We also think and worry a lot about just what is the truth? We think the truth and facts still matter very much but we are used to working in spaces where people want to deny that truth and those facts. This is a common tool of perpetrators of violence and violators of human rights. And so that's another place that we derive a lot of our responsibility to doing the technical work right. We think that the role data science has to play in a lot of these efforts is as a footnote, as a technical appendix. So it doesn't necessarily need to be front and center, but it has to be right. In human rights work, we talk a lot about speaking truth to power. And if that's what you're doing, then you have to know that your truth can stand up to what we think of as adversarial environments. Now sometimes those are political environments. Sometimes those are courtrooms. Not all of our projects result in expert witness testimony. But that is sometimes the most direct and clear outlet when we know that our analysis has to be sufficiently rigorous to stand up to the kind of adversarial environment that you find in a courtroom. And so just to run through a couple of quick examples, this is again Patrick Ball, my colleague and co-founder testifying at the International Criminal Tribunal for the former Yugoslavia against Slobodan Milosevic. And what he was presenting in that case was a comparison of the pattern of refugee flow across the border into Albania and known deaths to other information that was agreed upon about the flow of military units and bombs during the NATO bombing campaign in the '90s and trying to make an argument about what might have been the cause of that refugee flow. He testified again, twice actually, in the case against general Efrain Rios Montt. He was the defacto President of Guatemala in the early '80s. He was charged with acts of genocide. And what Patrick presented in court was a comparison of the relative risk of death for members of the Mayan population as compared to non-Mayans in the same geographic location and time period. And in essence we were able to show that the incredible dramatic difference in relative risk of death was consistent with violence that was ethnically targeted, or violence that was consistent with acts of genocide. In this particular case, Rios Montt was found guilty once in 2013 for 10 days, the verdict was overturned on a legal technicality. The trial advanced again and Patrick testified again in 2015 and then Rios Montt died before there was a verdict in that last case. And then lastly Patrick testified against Hissene Habre who was the President of Chad and in this case, Patrick is presenting the crude mortality rate in secret prisons that were maintained by President Habre. In this case he was found guilty and he was sentenced to 40 years in prison which he is currently serving. So those examples are not necessarily AI-related examples but they are the stakes. And so that's what I want all of us to have in mind as we think about these other areas where we might employ algorithms of some sort. So I'm gonna be a little mushy about machine learning versus artificial intelligence. Much of what we do at HRDAG I would consider machine learning. Much of what I'm gonna talk about today I would say is more accurately described as machine learning. And one of those first examples is identifying individual victims who are reported across multiple sources or lists. And I'll elaborate on that in just a minute. I'm also gonna talk about identifying geographic areas that are likely to contain previously undiscovered graves and evaluating the performance of models that are currently in use in the US criminal justice system. And I know you guys have talked about that one a little bit with at least the opening panel that really set the tone for this seminar series in a way that I thought was really really great. So the first example that I wanna talk about, at HRDAG, one of the things that we worry about the most is missing and biased data. And I'm very gratified that that's a thing that gets talked about a lot now when we talk about algorithms and when we talk about artificial intelligence, and it feels very familiar to us. In the cases that we work in we hardly ever have representative data, complete data. It's all what people were able to document during one of the worst periods of their lives. It's still very valuable, but our job as statisticians or as data scientists is to analyze that data appropriately given that it's incomplete and not representative. And so one of the ways that we do that on our team is through a method that when it was developed in ecology it was called capture recapture. We prefer to call it multiple systems estimation or MSE, which I realize is an overloaded acronym in statistics. Can I just out of curiosity, who is familiar with capture recapture or MSE? Awesome, I get to start. Okay so the intuition behind this method, it was developed in ecology to estimate the size of animal populations. And so if you think about animal populations, you're not gonna count every single one, you're not gonna do a census in most cases. And so a simple example is if you wanted to know how many fish are in a lake, so it's a nice closed population. You're gonna pick a time period when you don't think there are gonna be births or deaths and so you go out into the lake maybe in the morning and you cast a net and you tag every fish in your net, and then you go out again and then you throw them all back into the lake. And then you go back in the afternoon and you cast your net again and you count the number of fish in your net the second time who have the tags from the morning. And to show sort of the extremes, if every fish in your net has the morning tags, you probably think that's how many fish are in the lake. If none of them have the tags from the morning, then there's probably a lot of fish in the lake and then there's somewhere in between which is probably what will actually happen. There's some pretty simple algebra that you can do to actually estimate the number of fish the size of the population. And so that's the simple case where you just have two lists or samples or in the case of animal populations, trappings or taggings. In our applications, that's how we think of lists of victims of human rights violations, specifically in most of our cases victims who have been killed or who have been missing for such a long period of time that we assume they will never be found. And so before we can do that bit of population estimation which is what we're really after, we have to know well how many of these different lists did an individual appear on? If this description of this person on List A, the same person who is described over here on List B. And so that's record linkage or entity resolution, it comes up in a lot of different settings. And it's a pretty standard machine learning problem. It's the kind of problem we've been tackling for about 15 years. And so in the grand scheme of things I don't really think of that as one of the sexier, trendier things that we do and yet it's probably the machine learning-iest thing that we have been doing for the last decade or so. And so one way to think about that problem, this is public data from Syria so I'm not violating anyone's confidentiality here. And this is an example of one way that you could think about this problem. And if I have any Arabic speakers in the audience, you'll notice that it looks a little funny 'cause when you mingle English and Arabic, one of them ends up backwards, in this case the Arabic is backwards. But essentially this what our data starts out as. We have the names of victims and we have the date and location where they were killed. And we need to decide for each pair of records, does that describe the same person? Yes or no? And so that's one way to sort of tackle this, is just yes no, yes no for each pair of records. Another way that you could think about the records is as clusters. Among all of these records, how many unique individuals are described? And in this particular case, this is a dendrogram output from one of the packages in Python and you could just decide well what threshold am I gonna set for my clustering algorithm and then that's how these records are going to get grouped. But that's the thing we're trying to tackle, that's the substantive question. We think of this problem or we organize this problem on our team using this relatively complicated flow diagram that I am not gonna cover in any detail right now. But in essence, this is a highly supervised record linkage problem. And so all of the ovals running down the middle are the algorithmic piece. They are the scripted piece that we're trying to automate. And then all of the ovals that run along the outside are the human review piece, are all of the inputs that a human is labeling data in one way or another. And so that's one of our biggest and most common machine learning challenges. I'm happy to dig into it in much more detail during the Q&A if you would like. We also have a section on our website called Tech Corner and we have a post there that is exactly as described, a geeky deep-dive, more than you ever wanted to know about how we de-duplicate human rights violations records using record linkage methods. So now I'm gonna go ahead and move onto another example which some of you may have heard me talk about before and this is work we've done with partners Data Civica and Universidad Iberoamericana in Mexico City, and this is work predicting the geographic location of previously undiscovered graves. And so this is one of those projects that came to us really as a feasibility study. Our partner said we have this list of municipios, think of them as counties or states in Mexico where we know that what are called fosas clandestinas, clandestine graves, undiscovered or hidden graves, essentially they are anywhere that a dead body is found that's not a cemetery. And our partner said well we have these locations where we know those have been found and then we have these locations where we're pretty certain they have not been found. And that's the hardest part of this project because the absence of evidence is not evidence of absence. But in this particular case, that's one of the things we rely on our partners for is this contextual knowledge that in this time and this time period, probably these graves will not be found. So we have this list of labeled locations, yes and no. And then we have this list of unlabeled locations. Can we predict them? What would that look like, what would that tell us? And so the answer was yes, it turns out that a pretty straightforward random forest model will predict with extremely high accuracy the, based on our test, our held out test data, where it's most likely to find these undiscovered graves. Now what was most interesting to us in this particular use case is that when we showed for example this table of high probability locations to our partners, they said well of course, anybody knows that, that's where the most violence is occurring. Everybody's in agreement. So it's not so much that our models were telling us something we didn't know. It's that they were providing our partners with an interesting new tool. Because in Mexico, it's very dangerous for families of the disappeared to petition their local governments to conduct these investigations and to go look for these potential hidden graves. But if what they have is the output of a predictive model, if these crazy scientists in California said this is where you should be conducting your investigations, it gave them a level of distance that made it a little bit safer. Not safe but a little bit safer for them to do that petitioning. And so that was really interesting because usually in most of our cases the distance that science can provide us is a bug, not a feature. But in this particular case our partner said no actually this is a really useful tool for our advocates, for our partners. And so that's ongoing work, we are updating these models, we're collecting new data. The next step is actually going to be to incorporate more covariates into these models so that ideally we can say things like not only this geographic region but with these attributes perhaps near a body of water or perhaps in this kind of climate. So we're collecting, or for this distance from an urban center, those kinds of covariates. So that's the next step with this work. And then the next project that I wanna talk about you may also have heard me speak about before and this is an analysis that my colleagues Kristian Lum and William Isaac conducted of so-called predictive policing. And this was really what started a lot of my organization's work now in the United States. Which is that they looked at PredPol in particular published some of their work in a peer review journal. They still didn't publish their specific algorithm, they considered that proprietary. But they published enough information that we could figure out what they were probably doing. And Kristian and William used that to compare well what if we implemented this so-called Predictive Policing algorithm in Oakland looking at what's on the left-hand side here is drug crime known to police. And what's on the right-hand side is self-reported drug use based on public health surveys, yeah. So drug crime known to police is in some very specific areas of Oakland and drug use is pretty much everywhere. And so if you use this kind of incomplete biased data that's the crimes the police know about that are included in arrest data to build a predictive policing model, that model tells you to keep going to those neighborhoods. It doesn't necessarily tell you anything about the actual use of drugs which may or may not be the question of interest. And so Kristian and William wrote this up in Significance magazine and then they followed up with a blog on our website with a deeper dive into some questions that were sort of coming up as a result, so you can find more details about all of this work online. But in essence what I like about this example is like I said, on the one hand it's very gratifying that there's much more conversation now about biased data and the impact it's gonna have on all of these AI models. On the other hand this is a conversation that was happening in the criminology literature in the late 1800s. And so I don't know how to help us keep the lessons that we've learned so that we don't have to keep relearning them over and over again at the expense of vulnerable members of our community. But this is, I mean this quote just floored me that any use of actual crime and I enjoy the scare quotes, to draw comparisons or patterns over time should be held grossly invalid and yet that's exactly what vendors are selling to police forces and to communities across the United States right now. Now that was the work that started some of our investigation into these cases in the United States. Most recently, Kristian and San Francisco's newest District Attorney Chesa Boudin and I published an analysis of a risk assessment model. So there's been a little bit of movement now from these predictive algorithms in police forces to predictive algorithms for so-called risk assessment. So being used to make recommendations about bail setting, to be used to make recommendations about pretrial supervision. And I really wanna emphasize the word pretrial there. 'Cause in the United States you are innocent, you are presumed innocent until proven guilty. And yet there are many many many people being held pretrial. And so one of the things that's happening now is that these predictive models are being used to make recommendations about the level of supervision that an individual should have pretrial. And Kristian, Chesa and I looked at this very specific aspect of risk assessment tool that's in active use here in San, I guess, I'm not in San Francisco at the moment but in San Francisco and potentially being considered in wider use in the State of California which is that one of the covariates in the model is the initial booking charge which is recorded by the arresting police officer. And there is a well-documented incidence of over-booking, of inflated booking charges that then get reduced once a lawyer gets involved and a little bit of investigation is conducted and much lower charges are then brought to trial. And so what we did was we just said what would this model recommend from the booking charge versus the conviction charge? Now that's an imperfect comparison, there's a lot of discussion in the paper, we can get into it during the Q&A but essentially what we found is that just comparing those two, the charge that they're initially booked with versus the charge that they're ultimately convicted of increased the recommended supervision rate of 27% of individuals who the tool was used on. And so this is work that Kristian presented at the Fairness, Accountability, and Transparency conference in Spain earlier this month. It's being renamed, it's got a different acronym now but still fairness, accountability and transparency are the main points of interest. And so that's sort of the direction that I think some of this is moving in is looking at some of these predictive models that are being used in the US criminal justice system. And so thinking about those last two examples, I just wanna take a step back and think about well why is it that these models will go wrong? Because they're models, they are approximations of the real world. They will make mistakes, otherwise they're over fit and what good are they? And so really thinking about what's the population that the model is being applied to? What's the population that the model was trained on? And what are the potential systematic differences, what are the potential missing data and biases? And in our experience and I think in a lot of the real world applications that you're talking about in this seminar there is a very strong correlation between the reason why you can't observe something and the thing you're trying to predict with your model. And when that relationship exists, you have problems. And it's not totally clear how to prove the absence of that relationship, how to convince yourself that you in fact do have training data that is complete or that is unbiased. And in particular in the case of predictive policing and risk assessment, one of the things that we have been thinking about a lot on our team is well what would it mean to have unbiased arrest data? What would it mean to have unbiased crime data? Well we'd have to know about every crime that's committed. Oh, that sounds like more surveillance. That might not be better. And so these are really challenging problems out in the world. How do we use our technical tools to make the way these are implemented better? Because as some of your other speakers have said, they're out in the wild, the cat's out of the bag. So I think that is the task that's in front of us at this point. And so one of the rubrics that we have been using a lot on our team is what's the cost when these models go wrong? What's the cost of a false positive? What's the cost of a false negative? And who bears that cost? And so thinking specifically about predicting graves in Mexico versus predicting crime, when we get it wrong in Mexico we're inefficiently using our resources. But we would argue in the case of Mexico that the use of this tool to predict places to conduct investigations is an improvement over the status quo. And we would argue that in the case of predictive policing it is not. And specifically because the cost that is born when the predictive policing model makes a mistake is by community members. And it's the marginalized and the vulnerable members of our community who are being over-policed. And then there are also going to be members of the community who are under-policed, there are going to be crimes that are missed because of the misallocation of policing resources. Our argument is that this is not better than the status quo. But I think these are the discussions that we can have. In each of these use cases we may not agree about whether or not it's an improvement over the status quo, but we need to be explicit about these costs and about who's bearing them and we need to think about how can we structure this in a way that the cost is born by the vendor, that the cost is born by the person making the model, not the population the model is being applied to. And so with that rubric in mind I just wanna talk a little bit about a couple of examples that are not HRDAG work and this one I imagine many of you have seen before, this is a very well known article from ProPublica talking about the COMPAS point algorithm and the disproportionate impacts that it had. And I have started sort of collecting these in a folder called Adventures in Predictive Modeling because they happen a lot. And I'm hoping that in a couple of weeks when you hear from Timnit, you might get some of these examples as well and I'm pretty jealous about that one, I'm hoping to watch that one on the live stream. But other things that may or may not have crossed your path. This is an article from The Guardian about using algorithms to predict child abuse. And this is one of those cases that I think is really tricky because we could all agree that an early warning system for child abuse is a good idea. We want to prevent as much child abuse as early as possible. But what happens when that model makes a mistake? What is the cost, their real cost to having Child Protective Services show up on someone's doorstep? There are real costs to having a child taken out of their home. And what's the data that that model is gonna get trained on? And what are gonna be the systematic biases in that training data? So who are the populations who are most likely to have those mistakes put on them? And then I wanna close with a positive example going back to the Google AI Impact Challenge. I know you guys heard from The Trevor Project a similar one that is also one of the award winners is Crisis Text Line. And this is really one of my favorite examples I have to say I've been talking about it since before the Google AI Impact Challenge. I wanna claim a little bit of credit. I think this is a wonderful example of the uses of AI. I think for me at least it seems pretty clear that this is an improvement over the status quo. I think this is a huge win. And so when I was at, I had the good fortune of being invited to the summit a couple of weeks ago. And I was so gratified to collect more positive examples, more use cases of organizations that I think whether they're implementing this rubric explicitly or not are landing on use cases where the improvement over the status quo seems like a really big win. So I do wanna be clear that sometimes when my organization has a bit of a reputation of being naysayers and saying you really shouldn't do that or your really can't do that. And I think that there are a lot of opportunities here in the human rights space for these kinds of models to be a big help. But I think that they're not magic, they're not like anything else, they're just like anything else. They're another tool in our toolkit and it needs to be subjected to the same kind of evaluation and the same kind of questions about positive and negative outcomes. And I will, I'll leave it there and I'm eager to do Q&A. (applauding) - [Man] Mentioned someone who is present to give out how (mumbles) and I just wanted to know what your thoughts were on sort of the risk to be run. For example if the cartel would (mumbles) about how we look for (mumbles) bodies and they started to use that as a way to hide bodies in places that (mumbles) aren't looking. - Right, right, yeah. So the question was if I can paraphrase it this way the question was about sort of enabling people to game the system by making algorithms more transparent. And I think that this is probably ground that's gonna get covered a little more maybe next week in the topic of fairness and accountability and transparency. But I think that that is a reasonable concern. And I think much like any other risk evaluation, you need to kind of know who your adversary is. I may be unreasonably optimistic or naive about our use case in Mexico. I don't think that that's something the cartels have the capacity or interest in doing. But I hear you. I hear you that there are a lot of other examples where folks are keeping algorithms private for reasons other than profit. But I guess where my snark sort of kicks in is when it's proprietary for profit reasons. And I think that as researchers who wanna do good work we can sort of agree that that's not a sufficient reason. That was kind of a wimpy answer. I don't have a really good answer to a legit risk case where putting the algorithm out would do more harm. But it's a good question to ask. Yeah. - [Man] So I'm curious about with the Crisis Text Hotline example, that the percentage out here seems pretty good. Although I guess you'd have to compare it to human expert's total accuracy in terms of where they disagree. But I think the larger question there is whether enabling that process to be automated, especially with the defunding of those kinds of services is going to lead to an inevitable sort of enthusiasm for putting suicide, suicidal folks into an automated system where they basically, they're not actually talking to a person. They're basically speaking, going to some kind of Blade Runner test where it's just testing their voice for high suicidal tendencies. Is that actually the, sort of the most empathetic solution that they go? - Yeah, so the question was if there's a risk with examples like Crisis Text Line of individuals not getting to a human and instead only being treated by an AI. And so I should, well I should clarify a couple of points. One is that the Crisis Text Line is not work that I've done personally so I shouldn't and cannot speak about it with any authority. But also that for sure everyone speaks to a human. What the AI is using is essentially to triage the queue and so to reorder the queue and the rate at which someone speaks to a human. My understanding is that they do kind of constant evaluation to find out if that reordering of the queue is better than what a human would do but we have folks here from Google who maybe could speak to that with more authority, I don't know. No. So, but that's the best I can offer. But I mean I would say that yes, that would be a bad negative outcome if folks talked to an AI instead of a human, that would not be better. Anybody else? Yeah. - [Woman] Yeah, could you just talk a little bit about the model of your organization, sort of as data science and AI (mumbles) and how we're solving and tackling some big issues. You guys are a sort of organization that does, I mean I guess nonprofit work but you're doing it for these bigger organizations that don't have the capacity and data scientists of their own. Do you think that that's model going forward that you replicated? Do you, what do you think the future sort of of that relationship is and should be? - I love that question. The question was essentially what is our business model and what should it be? How are these organizations gonna, what's the best way for these organizations to become sustainable in terms of being able to do this analytical work? And like so many nonprofits, we would love to live in a world where we were not necessary. That would be a successful outcome for us. I don't see that world coming. One thing that we are seeing that is gratifying is we're seeing some of the larger international organizations, so Human Rights Watch, Amnesty International building up a lot of their own internal data science expertise and that we think is a huge win. We would love to see a bunch more organizations like us. I think folks could certainly argue and I don't really know, I don't have a firm position on whether it's a more sustainable model for each NGO to develop their own in-house expertise or whether it's more sustainable to have a series of essentially consulting firms. One of the things that I do worry about a lot right now, and I keep saying we're at this transition point and I have to remind myself that transitions often take decades. But because data science is so exciting and useful right now, I think there's also a lot of snake oil being pedaled, especially to organizations that don't have in-house expertise, not just in the human rights sphere but in a variety of social good spheres. And so that's one of the other legs of our mission and one of the other things that I would love creating an army of folks to help work on which is helping organizations who lack their own capacity to evaluate whether or not what's being offered to them is actually useful. Because I think that, I think that sometimes what's offered in this space absolutely does more harm than good. Way in the back. - [Man] So, looking at the Universal Declaration, are there any points on there that you guys aren't serving that you would like to serve? - That's a great question. Are there any points in the UDHR that we're not serving that we would like to? Yes, for sure. And it's very funny when I always say, I don't really describe our work this way publicly and yet I'm about to in a very public way. So there really isn't a way to say this without sounding crass, and so I'll just kind of apologize ahead of time for how crass it sounds. But one of the reasons why we study death is because it is very measurable. It happens once, it has very clear bounds. It lends itself to the kinds of analytical work that we do. There are a lot of human rights violations that we think, that we wish could benefit from the same level of analytical study, but it's not nearly as clear how one would quantify them in the same way. So I was just talking to a colleague this morning about human trafficking. And so what is a unit of trafficking? A person could be trafficked many times, they could be trafficked for a various period of time. The same goes for a variety of other human rights violations. What is a unit of torture? And those are, those are things that I don't want it to be my job to define. And I don't think they are served by the sort of academic discussions about how to define them for the sake of counting. Those are violations that I think we can advocate for using other tools. And so I think one of the things that we collectively should do is to identify the situations where the particular tools we have in our toolbox have a value add. So that was a really long answer, the short answer could've been yes lots. (laughing) Yeah. - [Man] What are some ways the vendors of these pre-sentencing tools could bear the costs of some of their errors? - I love that question. So it was how can we push the cost back onto vendors in the particular case of pre-sentencing? I don't have a good answer for that. I can imagine scenarios that politically would never occur. We could do things like in the public health world there are lots of ways to estimate sort of the dollar value of someone's day and we could find vendors that dollar amount for how much time someone was not out making their living. Is any jurisdiction gonna agree to that? Probably not. In a broader way, we could try to hold more of these vendors accountable such that when, not if, when their models underperform, the jurisdiction that has bought into their product is empowered to seek a competitor. I think capitalism is here to stay and so that market forces are probably the best tool that we have, but I also don't, I don't really know how to enact those either because it would require educating every jurisdiction and providing them with evaluation tools. Now that's coming, New York City, Philadelphia, a handful of other cities have these risk assessment councils, committees who were doing just that. And so I think there are sort of mechanisms to provide feedback loops. How to work in the accountability piece of those feedback loops is trickier. Yeah. - [Man] Yeah, before you mentioned that you see your team as the technical experts and you work with a lot of NGOs to kind of assess to gain field entities right? I was wondering how you see that collaboration working and how to best under limited time and under resource experience to collaborate when you don't have expertise in that specific domain? - Yeah, so the question was sort of how does our collaboration with our partners work I think, is that? - [Man] What effective care (mumbles)? - Yeah. The effective of, there are no shortcuts. I would say that this is one of the reasons why most of our projects are deep engagements over a long period of time. The best way that we have found so far to really have that iterative conversation with a partner to figure out what analysis is going to advance their advocacy is to go meet with them in person and spend time with them. We are getting a little bit better mostly I would say in contexts where we have worked before. So where we already have kind of established relationships to build on. We're getting a little bit better at carving out more kind of quick engagement analyses and studies. And I would say actually one model that I would say I'd like to see us do again was a partnership with the investigative journalists at Columbia University looking at deaths in the drug war in the Philippines. And that was a very sort of quick from start to finish project. But yeah, I think that in general to do this kind of work and to really understand the needs of your partner just takes time. Yeah. - [Woman] Some thoughts on if people are excited to get involved. If the students wanna learn more about HRDAG or just in general about how they can help with this. What do you recommend? - Yeah, thank you. So closing thoughts and how folks can get involved. Well we're super easy to find. And we do take interns and fellows and researchers, visiting researchers and those kinds of things. All of that being said with a caveat that we're a little bit on hold for summer 2020 because we don't quite, we have a few balls in the air right now, ask me again in a couple of weeks. But in general we offer those kinds of engagements. We love nothing more than for somebody to read our really geeky technical posts and then write to us and say I had this question about this thing. We love that, please do that. And I would say, well come to WIDS next week because I'll have a handout with a list of things. But although there aren't a lot of other organizations doing exactly what HRDAG does, there is a lot of excitement and a lot of growing opportunity in this space. And so I would say that there are a lot more data for good categories of things out in the world. And depending on what your particular area of good that you're interested in, I can suggest blogs or Listservs or folks to follow or organizations to read about and things like that. So the good news is there will be more soon. (applauding) 