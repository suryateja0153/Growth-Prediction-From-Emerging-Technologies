 all right everybody I will don't hear me oh good awesome thank you so much so yes I am an architect of ethical AI at Salesforce and what we really wanted this section to be about as a focus on practices and how do we actually put our principles and our values into practice in the day to day work that we're doing so KPMG at the beginning of the year stated that there are five critical hires that companies will need to be successful in 2019 and I was very happy to hear that AI Ethicist is one of those top five and it's not surprising that that would be the case as we have all seen in the media headlines over and over again AI can end up causing harm more often than not this is unintentional this is due to ignorance not malice and so having an AI ethicists to help guide the practices guide the work that we are doing really is going to be critical for the success of any company that's going to be creating or implementing AI and so talk of AI and ethics really is on the rise this was an analysis done by CB insights and you can see in 2017 this sharp spike in the number of mentions more and more people are becoming aware that this is an issue that we need to figure out how to address now there are dozens of what I call ethical tools that are out there ethics and design is not new it may be new for many companies but it but the the concept itself is not new as as identified by all of these different tools in fact dot everyone has a document 37 page document of all of the different tools that are out there I highly recommend checking that out and we've seen this explosion of AI ethics conferences and events some of them are AI conferences with an entire track divided devoted to ethics and responsible use and some of them are conferences dedicated just to this and we're gonna be seeing more coming so this is something people are trying to figure out in real time so why do companies need a responsible AI practice I would hope that this is something I wouldn't have to explain but I think just from the sheer number of instances that we see of AI doing harm we know this is something that we do have to convince some companies of it's important internally because it can guide internal research and design and development decisions and really help focus priorities to get companies and individuals within those companies asking not can we do this but should we do this externally it's important because more in more companies if you're in b2b or customers if you are b2c do care about ethics they may not use the words ethics they may not think of it in terms of ethics but they want to know that the products that they're buying that they are designed responsibly and so you need to be able to communicate increasingly how you are doing that and of course if we think about nothing else it helps you avoid legal and PR risk competitively ethics is going to become a competitive differentiator it may not be today but it will become a differentiator and finally I would hope we all want to be the change that we want to see in the world there have been a ton of ethical principles and guidelines published but they all come down to about the same five core themes AI should be responsible that means it needs to safeguard human right and protect the data that it's entrusted with it needs to be accountable so it needs to seek feedback from all of those stakeholders that it impacts it needs to be auditable it must also be transparent so explaining and communicating through the user experience why the AI is doing what it's doing it should also be empowering it shouldn't be focused sheerly on optimizing the human out of the loop it should be empowering and making society better for all and then finally it really does need to be inclusive it needs to respect the societal values of everyone and not just the creators who are designing and building those AI systems so how do we put these principles into practice how do we measure what success looks like how do we know if we are getting better today than yesterday these are the really big questions that many of us as practitioners are trying to think through you can have great intentions but unless you know how to measure if you are getting better or not it's a very hard thing to do so a colleague of mine Daniel Kass who is a director of AI FX at workday she and I created a survey and we sent it out to our networks and the PAI the partnership on AI their membership list and then also did just snowball recruiting asking people to share it out in their networks with the people that also work on ethics in tech we got a hundred and eight respondents from people in ethical roles there are many different names right now for this but they are practitioners consultants and researchers and it covered US and Europe this was something that we wanted to get a much more diverse response set but for right now that was what we were able to access and then we followed up in the end of February with a workshop in San Francisco and we got 23 of the participants who filled out the survey to also come on-site so that we could dig deep into understanding how are people working what's worked well for them what are the biggest challenges that they're facing and why so I thought I would share with you today what's working and what the biggest challenges are that we all need to think about how to resolve and then hopefully if you're in a similar role you can take advantage of some of these insights so the first key tip that respond or the workshop participant shared was having access to experts that includes things like office hours or review boards so having a group of individuals so that anybody within the company who has questions they can come and get feedback one of the tips that they found is to have an intake form just a really simple form that asks people to fill out what's your question what is the what's the information that you need to know and that's for a couple of reasons it forces people to do some upfront work and be able to articulate what is the challenge what is what are the things that they're working on maybe ask them to include a link to the product development guidelines or some some other document of the products that they are working on but then also it helps you prepare in advance and see are there other people that you might want to bring into that conversation so everybody comes into that conversation fully prepared the next is to leverage existing processes I think for a lot of us when we're brand new in a role it's very tempting to create net new processes when that's really not necessary at the beginning I mentioned that there's 37 pages worth of existing ethical tools that you can take advantage of but also look at your internal processes so for example if you are on the agile development methodology at your company you should have a definition of ready and definition of done you can add ethical questions into those two steps of your agile methodology and it doesn't significantly change how people are already working but just adds some extra questions for them to think about if you have any machine learning or other AI classes for employees at your company you should also add in ethical education into that training start developing an ethical mindset among individuals as they are working on AI because they may not have had these classes or these topics in their training in school they may never have been in a company that has had them ask these types of questions product reviews are another great place if your company really does value ethics or the responsible creation of technology there should be a discussion of that in product reviews among executives so again take what already exists leverage those and don't create any net new processes unless you really need it and you have the buy-in for it third recommendation is to create digestible role-specific content just as there are a ton of ethical tools out there there's quite a lot of Education available like from the markoulis Center for example about ethics and tech or ethics in AI these kinds of generic case studies can be very interesting but for an individual who is focused on what their key deliverables are they want to know how do I apply this specifically to the work that I'm doing today tell me exactly what I need to know so that I can work differently today so it should be specific to the role whether it's engineering design marketing legal what are the things that those roles really should take into consideration checklists are super easy I always say ethics is a mindset not a checklist but checklists are really helpful as people are working on problems you can also create one-page documents of guidelines or principles to keep in mind but you should also be documenting the decisions and deliberations that are being made this helps people learn from other difficult challenges or conversations that are happening throughout the organization it also increases transparency so that people understand why did we decide to do a and not B and it can help with consistency so that one business unit isn't deciding to do a and another business unit is deciding to do B and there's no good explanation for why so documentation for others to learn from is really important final recommendation is to create empowerment and agency at every level in every function you must have a tops down in a Bottoms Up approach to be successful one of the workshop attendees no board no berto Andre said that ethics lives in the small design decisions that are made each day you can't possibly be in every meeting every business unit where important decisions are being made so you need everybody throughout the organization to be able to make those decisions in your absence so you need the sponsors those are the leaders from above that are lifting you up they are bringing you into the conversations that you might not normally have access to they're also providing access to resources that you're going to need to be successful maybe it starts off with a sponsor for one business unit not the entire company but you build wins one success at a time your supporters are the other people throughout the organization these are the people that are doing the work each day they have the religion they are there channeling you in each of those meetings and some of your biggest successes and impact in the company may very well be the work that they are doing that was inspired by you all right so the open questions here are the things that are the big challenges that we are still wrestling with the first is how do we measure impact so I alluded to this in the beginning how do you measure harm avoided because you worked responsibly to begin with how do you know if you have actually had impact many companies have okay ours objectives and key results at Salesforce we have v2 mom vision values metrics methods and an objective or obstacles these are things where you actually have to measure what have you done so trying to figure out how do we measure are we getting better today than we were yesterday is something we're all trying to figure out and of course the lengthy timelines to implementation from process to outcome can also be a challenge so you have to take a long view next how do we scale this practice across an organization there are so many different business units and roles that are that are available in a company and so trying to figure out how you embed these practice practices throughout and have the resources available is something that we are grappling with third how do we create a shared taxonomy these words ethical responsible biased fair they mean different things to different people especially if you're say an engineer versus someone in legal where these words have a very specific connotation in legal in legal circles so if you have different definitions it's going to make it difficult to have a shared conversation and come up with an agreement finally how do we operationalize ethics how do we create those tools and guidelines that work for each of those business units how do we know that they're going to be used consistently across an organization and how do we change incentive structures right now many companies especially engineering our tech companies are rewarding individuals based on launch revenue user engagement clicks those types of things don't necessarily support an ethical framework so how do we change up the incentive structure we're we're not saying profits don't matter but we are saying that working responsibly does so I just want to wrap it up this is a phrase that I say quite a lot some people attribute this aphorism to John F Kennedy but this is actually a Chinese saying that's existed for centuries a rising tide lifts all boats although working ethically can be a competitive advantage it's important that we all work together that we share our insights of what's working how do we all come together to define what this responsible AI practice is because we all get better as a society because high tide really does rise all boats all right so we're now going to move into a panel I am very happy to say that I have three fantastic ethical practitioners here with me today so I'm going to introduce them to you first up we have the very freezing Rachel Caldecott she is the CEO of dot everyone which if you are not familiar with it it is a fantastic independent think-tank that Champions responsible technology her focus is on putting ethics into practice in business in government and in civil society she spent the last 20 years turning emerging technology into products and services and has helped many organizations adapt to the digital world Rachel has created and delivered large-scale content driven services for Microsoft BBC and channel 4 was a pioneer in the UK digital arts sector and has come Belted for many ftse companies across a range of sectors including finance energy and health care and she sits on a number of ethical and advisory boards for corporate and Geritol projects next we have looped church he is an affiliated lecturer at the computer laboratory in Cambridge and the director of innovation and learning at Africa's voices foundation which is super cool highly recommend you check out their organization he leads technology research and media teams Luke's research seeks to improve the experience that people have with socio technical systems including programming languages AI systems building buildings public policy and humanitarian interventions and recently he founded lark systems to explore ways of integrating different forms of intelligence and last but absolutely not least is Julie Dawson who leads the ethics framework development for the yo T digital identity platform digital identity enables people to prove who they are or how old they are digital identity can be a game-changer for rebuilding trust it empowers consumers but it can also be misused for surveillance or to exclude people yo D is a founding B Corp B corpse our for-profit businesses committed to growing profits and purpose in parallel so I'd like to begin by asking each of you to take a few minutes to tell us about how you are weaving ethics throughout your organization I can see my brother maybe everyone goodnight I didn't know the chair and work is really about how to turn a lot of this talk about ethics into easy everyday things and we started this journey about three years ago with a hunch that there could be ways of making technology differently that would lead to better outcomes and I spent lots of time over the years working in product teams and I was very aware that they tend to be quite inward looking and quite insular and so the challenge that we had is is how to only get up and we have just done a survey which I'll be talking about later this afternoon but the two really interesting things that came out of this we spoke to just over a thousand people working in the industry in the UK and the headlines that are relevant here are there only 2% of people think that um codes how and there are loads of codes there are like hundreds and hundreds and hundreds and then actually the things that people were really after were practical and company visions and ways of working the thing that really came out is that particularly in May I wear lots of the people we found that one in six that the people has left in the job because of the negative ethical in fact of that work which is huge are people right and so actually those people are really after practical things and we have spent a long time trying to think of that how to do that and I will just tell you really quickly I think a little bit more about the journey that that has entailed we started off thinking about a trust mark and trying to think is it possible that every purchaser or user of a piece of software has mate maybe the the trademark and I can tell you the answer is no it took us a long time to work this out and it's partly because I think if you compare us a software of any time to and bananas right the difference is the software is living and it changes and it changes every time a person interacts right and it lives in the world where it's kind of a banana is a banana is a banana right so having thought about that we then started to think about how can we actually make it easy for people who have jobs who have lots of the things to think about to just add a things in to have them working and we have a site here that's got a number of tools I think that the three important things that we've unsettled on are supporting people who aren't engineers but are people who who are around engineering and so the first thing is how can Fords come up with a metrics that are actually helpful well how can a method be at the heart delivery for everyone we have spoken to loads of teams who have said we're being asked to be ethical and being lost deliver against metrics that totally undermine that and so start there the next other thing is we've been looking at how to upgrade the agile process and thinking about what are the really easy rhythms and routines to fit into that we're really aware we don't want to be asking people to do different things will be learning extra skills and then importantly this isn't a thing that belongs to but an ethicist by out of the team it belongs to the whole team and lastly we are working on kind of changing how we think about user design and thinking about understanding the context that we're operating in in creating the software so we've just started we launched that the first resource about two months ago challenge now is getting it out there and we have a stand in the expo if anybody is interested in finding out more thank you hi everyone so I'm going to talk about about Africa's voices foundation so what it is that we do is pretty much written on the slide right so we listen intelligently to achieve impact and we listen intelligently to citizens on their own terms that's a nice kind of platitude sounding thing let's try and talk through a bit about what that actually means in practice so an example might be that at the moment for example we're very interested in different group relations going on with in Somali contexts at the moment so we'd run a radio show where we say well we have a kind of the media part so those those two characters they're falling in love they're wondering about whether they should get married and one of them is from a minority group with an small ear and the other is from the majority and so people would which would encourage people to text in their opinions and if you do this there's a couple of reasons for following their strategy first of all we're not imposing a frame on anybody so we're not acting as in you can have the conversation but you can only have the conversation in terms that we set which is what you get if you provide someone with a fixed choice survey the second is the what we're creating is a space where people actually really want to talk so in this context of this media show people really want to talk about the issues and so we get a lot of these text messages so there they're not easy to interpret they're not the kinds of things that you could count with the standard process because there are actually real voices people talking in their own terms in colloquial context in their own local languages about really complicated very sensitive topics and so the question is if you want to curate a conversation like this you're going to have tens to hundreds of thousands of these messages that you have to be able to turn around in a short enough time to really be able to maintain the conversation and so we turn to technology to help us do that but this is now a difficult problem right so how is it in that context how is it that we can think and maintain this principle of not imposing a frame on the conversation that is going on and we do that in a number of ways so one of which is that we do not seek to automate the analysis and interpretation of what is going on we could just build systems that just count it what people were saying that isn't actually providing any sense of understanding so instead what we say is that actually for really complicated subjects like this the goal is not to have a computer sensing it the goal is to have a person a researcher able to understand it and we use AI techniques to make it so that that researcher can read tens to hundreds of thousands of messages but the understanding lies in the researcher not in the computer the second thing that we do in practice is that we maintain provenance of all of our information so for any given outcome that we might say off here are the broad sets of beliefs that we think this community has we can trace from that outcome back to something a voice a citizen said to us and the third thing is that this is now a really fascinating problem right of the how is it that people think about these really sensitive really challenging topics it's something that's actually really genuinely interesting to understand so we designed the systems so that the user interfaces of all of them promotes a kind of a sense of curiosity in the researchers working within the system they don't promote the answer that the computer is right they try and encourage doubts in the end the statistical analysis or try and encourage other questions that might not have been asked so that's kind of designed to augment rather than alternate the sense of provenance tracing and this sense of curiosity enablement is I think the kind of the three broad practical design approaches that we've been taking throughout all of the systems excellent thank you very much and Julia you want to grab that thank you so I'm Julie Dawson I'm from yotie which is a digital identity platform a lot of people who I might mention that to friends I haven't seen for a while sort of look at me curiously digital identity um and really it's all about how do you in all sorts of different parts of your life prove who you are it could be for instance arriving as a speaker how is it they know that you're the right speaker and you're not just you know walking in and being an impostor it could be when you're on a dating site how does somebody know you're actually off the gender and age that you say you are or traveling to an airport buying a bottle of wine in a supermarket lots of different contexts we need to prove eight your identity and the way this has happened to date happens to be normally you're trotting around with your passport or driving license if you have one so just taking the UK we have about a third of all under-18s that don't have a photo ID document it might be because they're not traveling they're not old enough yet to have a car their parents don't take them on holiday and even for over eight teens in the UK we have 24% without a photo ID and if you look at that on a global level the World Bank estimates between 1.5 and 2 billion people in the world just don't have a route identity document so with two different areas within the ot1 is looking at overall how people that do have documents can prove who they are and we have a three to five minutes set up and then in a couple of hours any organization can do checks and find out that you are the person you say you are but then we started to look at this principle of how do we make this available to all proving who you are or how old you are and we set up some principles right at the beginning including making a digital identity approach available to everyone and putting the consumer at the heart of it and we started to look at what could you do that wouldn't require someone to set up an app and that wouldn't require them to have a document and we have a very rich data set through people that are onboarding where we're prevent fraud that we've started to learn what does a 20 year old face look like what does a 30 year old face look like and we've built with a new network and approach that can actually estimate age the way it does it is just looks at you and the image is instantly deleted so what we did earlier this year was to actually refine that more and we thought we've got to do this in a really open and transparent way we're a B Corp switch it's meant that over the last few years we have gone down a track of always inviting scrutiny we have an external Council of Guardians and actually Gavin Starks is beating on a another stage today one of our Guardians we have who look specifically at the open data open source community Renata Avila is a Latin American human rights lawyer and we have consumer rights activists Doxil they meet quarterly all the minutes are published openly and they're we're inviting their scrutiny they're holding us to account you can ask them any question they can ask us any question and it's all documented so we tried to bring the same approach in scrutiny through to this AI approach and brought together after having looked at these multiple frameworks a group of practitioners to really thrash through the unintended consequences and then publish a white paper and the things that were really important to people was well how good is this for men and specifically for women how good is this across people with different skin tones and bringing that diversity to light so we've actually published very openly a white paper I think yeah so that is the white paper and that is specifically the section that looks at the mean average error rates across the genders male and female looks at the different age bands and we update that every sort of two months approximately can I go backwards all the way yes so that's the the age scan white paper we've also signed up to the biometrics Institute's principles around safe a on ethics and similarly another paper that's come out of a safe face pledge that's come out of MIT looking specifically at the inclusiveness and an on surveillance which is pretty much an ethos that we we have running throughout the yotie system with the main identity app we chop up all the attributes give someone the private key and only they can choose who they share their details with and with the age estimation the user is just looking into the camera area which could be for instance at a supermarket self checkout it could also be with a live streaming platform were working with just for instance one live streaming platform has done over 22 million age estimates they have a chat room for 13 to 7 year-olds and alone for over eight teens and as part of their ongoing safety measures they wanted to know that people were in the right self-assertive area and have found that significant tens of thousands were not in the right self asserted area so I think there's a relatively benign use cases in that there are other options at the supermarket self checkout it's it's something which is quite a moment of friction for both the members of staff really hard to train people to look at documents from around the world and when you are in the supermarket you're normally wanting to get out if you're anything like me so that's I think just a really quick review of the different types of approaches that we've done which is bringing together experts and looking very specifically to be open about how good and how bad the actual algorithms are and looking at how that can help through our spotlight and give a really sort of concrete example of something in everyday life that can make life safer and simpler and that Brill's on these principles of transparency all right so I wanted to jump into a few questions so obviously the problems that you are each tackling is massive and so how do you scale sure so I think it's interesting to think about what it is that we mean by the word scaling as it's typically being used in the tech industry and it's it's essentially a socio-economic position of a system that scales is one where the profit aggregates to the tech company and the work aggregates to the distributed population and that means that as the tech company increases the the number of people working through the system they increase profit and they don't necessarily have to pay the working cost and there's often been an argument made that that's kind of the way it has to be in order to build bring the benefits of technology to large audiences so I think ABF's approach has an interesting counterpoint to that of saying actually we'll do the interpretation so will you say whatever you want to us and we all pay the cost of trying to understand and work out what that means and in theory that doesn't scale right in practice what we've found is that by building some careful workflow design by building some some user interfaces to support people and then by applying AI to augment the capabilities of the researchers we can deal in an entirely cost-effective manner with hundreds of thousands of data points so I think in some sense scaling happens if you try it try taking it seriously try paying the cost yourself and see what happens building on that I'd say that part of it will also be the sectors where this can be actually used we've all got really used to over the last two years tapping with Apple pay for instance nobody would have thought five years ago that that type of an approach would have become very prevalent and we're on the cusp of a couple of different major changes in just the area we're working on with age estimation and that this year with the digital economy act in the UK between 10 and 20 UK million UK adults will need to age verify to access adult content which has been designed to prevent young children stumbling across content they're not age-appropriate enough to handle and then if you look next year in the UK the work of Baroness Kidron and I writes five rights looking at the age appropriate design code companies are going to be challenged to look at all they actually dealing with young people online in the right way what is the duty of care that is implicit and how in terms of customer service terms the whole user experience how you deal with people differently so we're starting to see that bit from the worlds of online dating gaming gambling classifieds social media all of these different sectors as well as retail online and offline are looking at how these approaches could be used and once the technology has got an ethical review hopefully it's something that that invites very strong scrutiny the unintended consequences because it could potentially have used in those other areas and that's what we're keen to do at the moment because really we're now engaged in the job of going and advocating talking to businesses getting them involved with the work that we're doing which is a challenge because with tiny there are 14 of us and now very good time to say that we are hiring if anyone would like to come and help us read this work you say I think to us the issue is how can you make it easy to do and easy to take on without being all about kind of huge ethical problems all right so what is the biggest challenge that you are facing in your current role and how are you trying to tackle it um for us one of the very next tricky issues will be looking at age estimation for younger demographics so at the moment we've only dealt with people aged 13 and above because that was where our app has been designed from however we're getting lots of interest from organizations that are saying well for example I'm a gambling company and we're seeing people as young as five and six are getting addicted to loot boxes or skins gambling the likes of large dating sites have had people under 10 going on adult dating sites there's now increasing demand to start to look at a younger data sets and so one of the things we're doing is convening a group of organizations who are very you know strongly steeped around the child protection area and getting their input a little bit like an ethical committee review you might have at a university before kicking off a project because we would like to say well we invited reviewing scrutiny for the over thirteen approach what should be the scrutiny that should be brought by civil society and other organizations if you're going to take that type of approach wider one that's great yes I think the biggest problem the I face with with kind of challenges in AI affects on a regular basis is that I think the rhetoric of AI has become largely detached from reality so the claims as to what AI can and can't do from a technical background I can evaluate a lot of those claims and a lot of them simply just don't appear to be real but that means that it's a very difficult space because I'm often working primarily with people who aren't from a technology background who look at the proposals that we give them as this is the kind of complex analysis we're going to do this is the Acts of interpretation that we really want people to support and like but there's this magic pixie dust over here that will do it all a new one they only want a thousand dollars a year why do you want so much why do you need so much resource to actually solve these problems and of course but kind of ok if you're in spaces were dealing with the 80% cases were the Kate where the pixie dust does work it's fine if you're actually in a context which is which has an ethical commitment to reach everybody then the pixie dust actually harms people and real people and so there's a I think the biggest challenge and kind of going to what we were talking about earlier of a way in which organisations can articulate what works and what doesn't work and can be public we can have a kind of moments of public confession all about the bits of AI that don't do what they say on the 10 would be really useful and I I actually have two things the first is it's it's really easy to get caught up in the story of the day and say you know that there are lots of dilemmas that are suddenly becoming very newsworthy and I suppose our challenge is to continue to hold a space in which people are able to think about how the technology works in the widest sense without getting distracted by particular issues or kind of going crazy over the chain say when actually they're probably doing a much more straightforward thing and the and the other are saying we work quite closely with governments looking at better regulatory and environment and I think that is the biggest challenge and bridging I suppose the lack of technical understanding in in government with a desire for really quick outcomes and immediate change in the things that could be systems that continue for years and years and years and so I think probably Oh overall it's like how do we refrain this as a five year ten year 20th problem to Kathy's point understanding the impact of technology is here to stay it isn't a fad and how to continue to push that on now I feel obligated to acknowledge we are not a racially diverse panel we have some gender representation different locations different backgrounds but the issue of diversity in AI is a huge challenge that we are trying to grapple with in many ways how how are you addressing issues of diversity within your organization's um we found specifically both in our organization and in this piece of work that that is is quite a crucial area we do have a very international team and the the gender bias is warm that we work very hard to keep trying to address we're also trying to do more in terms of the LGBTQ diversity neurodiversity a range of different areas in this work with age estimation and one of the things we found was that it wasn't as accurate as we wanted and specifically for the darkest skin tone and ladies in the 42 sort of 50 age group we were being overly flattering so it was near a 50 we were thinking she was more like the greatest sin but we still didn't think was fair enough so we did specific work with some NGOs in Kenya to address that and is something we will keep looking at on an ongoing basis we're also doing some work through a charity called Sparkle which is a transgender gender charity based up in Manchester because we want to see that there is want to look at the unintended consequences the gender diversity angle and there's also other areas you might not have thought but and beards can make a fraction of an instance impact as well on the age estimation so we've done some specific what looking at what is the impact of beards how material is that and it's not hugely material is the is the conclusion but again it was something that we will be publishing and adding in to give this richness of view it's fascinating to be representative and continues to be a challenge partly because we're not a big team and so we're constantly both working with extra external organizations to improve our understanding and actually the other thing that we're doing well as having completely change their approach to hiring in the last yeah we do lots of design research and ethnography in order that all of our work is really based in kind of realized real opinions it isn't just kind of things that occurred to us and I think that actually a lot of the ethical issues we're facing as a receptor would be improved enormous ly by being more representative and one of the issues probably in a weird way the hardest one is the moment a person is working in software engineering they they have almost been moved into a privileged life and and that probably ongoing the hardest thing is its kind of looking out of that bubble and continuing to work with real people in ways that are including and not letting them yeah so really resonate with that I mean so in a slightly unusual position here compared to previous work in the tech industry and the 60% of our employees or women but 61% of them are from or East African and origin so that in some ways that sounds great but it actually it's nowhere near enough right of what that means is that we have some Democrats and some demographic representation within the organization it's really helpful but it doesn't matter if if that is if the challenges and problems that we're addressing are still set by white man it doesn't matter that the the demographics might be more representative so the real price here isn't to just align those identities though that's clearly very important it's also to start looking at how is it that we can build systems and organizations that work to the problems where the problems are set the goals of the organization are set by the communities that were working with them and I think thinking about ways of doing that as an interesting kind of route forwards for for how is it that we can how is it that we can build genuine diversity into the way that technical platforms get used to enable people to solve problems that they care about one of the ways that we've been thinking about doing that is kind of inverting the traditional logic of you decide what problem you want to go and solve you then try and attract funding and people to go and solve that problem and say let's let's assemble a community a group of people a group of friends that we want to work with that align to some kind of distribution and then let's decide as a community what problems were going to go out and solve the child that's an interesting kind of proposal the challenges tend to be that it doesn't align very well with the way that funding works for example and so looking at how is it that we can provide alternative models that support integrating that from the beginning seems like an interesting challenge going forwards just one other thing that I forgot to mention I think is perhaps helpful and it's slightly different than the view that you'd had earlier Kathy with regards keeping within the sort of functional lines and not creating something Kirt around ethics one things we found that was almost a botton up request was to upskill more people across all the different teams in the business so we've actually created an ethics internal sort of team or working group that works alongside the product teams and all the other different functions but there's a mixture of junior to senior again we've tried to get a really diverse mix so some people from our 24/7 security operations team from the customer service and support team that again as a 24/7 group through R&D through design so it brings a very different set of voices on your sort of cultural demographic backgrounds experience ages and that's something that we found to be it's always like a really good sounding board for people that are trying to make a decision within maybe one functional team or in the R&D group and it's a group of extra people they can just go to and Muse over something with or bring it more formally as a considered topic so so what is one piece of advice that you give to anyone implementing a responsible AI practice in in their organizations do it I mean I think I think the thing that we have seen over the last years is it's really easy to come up with like oh maybe next year maybe in the next and that's like cool maybe after this thing has happened I'm gonna say begin in a small way and and and kind of learn and change I would agree with that one I think we're quite a small organization that probably from when we were about fifty people kicked off down this route before really there were many scorecards or approaches or principles there now there are more and more of them almost that it can be quite confusing so you do have to start somewhere small but I think having something that goes across your business maybe like an initial set of principles that you can get top management agreement to and have thing all the way from induction all the way through people understand is the approach that your business wants at the top level it began it then becomes much easier and actually just blogging about decisions that you've made or running a workshop and getting people in and then blogging about those decisions forces you to be more open about the approach you're going through we talked about some of our ethics sessions in the family meetings as well as making all those minutes available to people so that within the company more people are understanding the decisions that are happening yeah that documentation and transparency is so so important I think it's something that many organizations or many individuals really dislike they they dread that extra type of work but it's so critical for success in these cases so I think I would go back to the the kind of beginning of the talk we're saying you know there's in some sense and don't don't turn AI into something exotic those those are the design challenges that we face as to how do we do how do we deploy AI with an organization are not actually radically different from design challenges as to how do we deploy other forms of technology within an organization in an appropriate way and it's kind of there are there are ways of thinking about that that have stood the test of time one of them for I've been thinking a lot about is is essentially trying to split the problem space into two so design for the case where AI doesn't work and once you've got that designed you can start then being your in a safer space for in some sense for deploying AI across an organization and just as we wouldn't say if we designed a piece of user interface a piece of design and it didn't work we wouldn't say oh well that was the only option we had we'd go replace it right so if you're AI components that you're proposing to deploy don't support designing doubt don't support human curiosity don't support what it is that your organization is trying to do replace them importance of iteration feedback iteration listening to your users getting that feedback we actually have a couple more minutes would anybody would any of you like to leave us with any final words I think one of the programs that we went on was you're responsible tech trust marking and I think one of the frameworks that you recommended that we're going to look at is this framework that you describe to me earlier yes scanning so we've developed an agile event that helps the teams to understand and record the intended and unintended consequences of the technology they're creating and I think most importantly it leads to you creating a logo of those and understanding that the things that you're able to act on now the things it's worth monitoring and the things that are out of your control but you're able to influence and we've worked with dozens of teams over the last year to make understand how how to make this work we released it I think in May so it's really early days and and tomorrow we're running a drug shop here if anybody is interested in finding up and going to that so I'd like to use that for our next workshop and we'll let you know how it goes you you 