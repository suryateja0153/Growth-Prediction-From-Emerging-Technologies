 so it's no secret that AI holds enormous promise for government and our next speakers are going to tell you what it takes to build an ethical platform for the responsible delivery of a government AI project it's kind of a tall order first up is Cosmina Dorobantu on to she's a colleague of mine at the alan turing institute she's the deputy director for the public policy program and she has had a very big morning welcome hi so you know it's a it's a hot topic the minister just announced that cog eggs actually 45 minutes ago the government technology innovation strategy and the government published for the first time an AI guide for the public sector and were very thrilled and delighted at the cheering to contribute the cup to the government guidance on ethics and safety and David is gonna talk a little bit more about that but what I want to introduce you today is you know why is this topic important in a public sector context and why do we why should we give thought to it so you know you're on the Alan Turing research stage the Turing is the National Institute for data science and artificial intelligence we partner with 13 universities across the United Kingdom which sort of gives us unequal access to expertise in in data science and AI now the Turing is organized into 10 research programs and the program that we're sitting in is the Public Policy Research Program so we set this program up about a year ago in May 2018 and the idea was to try to help our C makers to think how data science and artificial intelligence can help them create better public policy and better public services now the way we we went about this when we set up the program is we had hundreds of meetings with policy makers to understand where they should focus their attention and where we should focus our attention so we asked them what questions keep them up at night and why are they unable to answer them and what data can they access and what expertise do they have internally what is the society impact of the projects they're considering what are the ethical considerations and finally is the Turing able to help in any way on the basis of this hundreds of meetings that we've had we decided that there are four areas of opportunity for us covering the technical as well as the ethical aspect of data science for Policy two of them are on the more technical side so using data science and AI to improve public policy as well as the provision of public public services and two of them are more on the ethics side so build Attica foundations for the use of those technologies and contribute to policy that governs these these technologies I'll take it through each one of them just to give you an idea for what they mean so when we talk about using data science and artificial intelligence to inform policy maker making these technologies have an amazing capability to inform the way we design policy and the way we think about policy so I'll give you an example of one of those technologies which is 18 computing an agent computing allows us to model complex and interdependent systems what I find I'm an economist by training and by trade but what I find particularly exciting about those contexts is that they managed to replicate dynamics that we see in the real world that we haven't been able to get to before we're used to thinking in sort of describing the the world in generalities and in aggregate sand in averages and nowadays we can actually describe it in individuals so the easiest way for me to in to explain agent computing to you is you know suppose we want to model the dynamics and behavior of a colony of ants the way you do it in agent computing is start from an individual ant put it into your model give it some rules of behavior that you've noticing in the real world give it some constraint space within which this ant can move and then the system should generate the dynamics of a colony of ants on its own and then you can actually run experiments on that virtual copy of behavior of a colony of ants so the reason why this is exciting in a policy context is that it completely changes the way we would model things so in the past the way with the traffic modeling is to say the density of cars in the road is some function of the the population the local population the number of roads and the economy well what we can do nowadays is to model each vehicle each Road and get the emerging traffic jams and that can lead to much better policy with military operations the old way of modeling casualties you know in in a war is to sort of look at the rules of engagement and the number of the number of army personnel that each opposing party would have well nowadays models their models that have each soldier in and each a weapon and each bullet and you can sort of understand the dynamics of that battle much better what's really exciting to me as an economist is that the way we model financial markets and macroeconomic models in the past is by looking at a representative agent model we aggregate data now this is really startling central banks nowadays still use these models of one representative agent in an age in which we can actually use microdata on consumers and firms to get the dynamics of the economy now I want to tell you just very quickly about a project by rob Baxter who is in the united states and he started out while trying to model trying to model the US private sector so what he did is he put the data 420 million workers into his model and he managed to replicate the characteristics of the US private sector in a way in which we as economists have not managed to throughout our profession and the history of our profession so what he did is he managed to get you know the emergence of six million firms with employees in his model he managed to get the 3,000 million job changes each month in the US which we haven't been able to to get into any model thus far he managed to get his model to to lead to the emergence of a hundred thousand startups each month he got the 20 the 20 largest firms in his model boy half of the workers and impress most impressively of all he managed to get the sort of Walmart of the world the one firm in the US private sector that hires a million employees now this is really exciting for us because what we were basically doing by putting all this data and all those models together is to create digital digital replicas of our economies and then we can test our policies on this digital replicas without actually hurting any any of any of the people in in an economy okay this the second area of opportunity that I mentioned is improving the provision of public services so you know this is my crude introduction to machine learning in the past if we wanted to write a computer program that recognizing the picture that recognizes the picture of a dog well we basically do is is you know take a picture of a dog and say okay you know give give the command and we'd specify the rules that makes a dog a dog so we would tell the computer if you see an image that basically you know has two years and two eyes and you know whatever fluffy nose or something then you should know that this is a dog so we'd basically be telling the computer what makes a dog a dog we don't do that anymore the only thing that we do is we feed images of dogs to the computer and we simply give it labels and we tell the computer this is a dog examined this this image and learn that this is a dog so then what we can do with this you know if our algorithm an algorithm is working correctly the outcome that we'd be hoping is that we'd be feeling this image to the computer and asking it what is this and then the the computer would be able to tell us well this this is a dog so what makes machine learning different from what we've had in the past is that we no longer have to write the rules of what what made in this example what makes a dog a dog the computer learns that on its own but how does it earn it well you need to write a good algorithm for it you need a large quantity of data so if you're only feeling it 100 of dogs is probably not gonna not gonna be able to learn much in it the quality of data to be good so if you're feeding it pictures of cats labeled as dogs it's not going to learn what what makes a dog a dog and you need variation in the data so you can't just feed it pictures of Labradors you know you need to feed it a variety of dogs otherwise it's never gonna be able to tell that the dog on the on the on your left I guess would be a dog so why is machine learning exciting in a public services context well the reason it is exciting you know we can we can learn the private companies to learn why it is exciting so far the companies like Amazon are using machine learning to anticipate needs so the way the process worked on Amazon in the past many many years ago yes they're all be sitting at my computer and wanting to order dog food now the way I would do that is that I would I would order order dog food on Amazon the order would go to a warehouse the warehouse would put it in a delivery truck and then the order would get delivered to me well what what Amazon has done is to build a predictive model that can basically anticipate when I'm gonna be able to order dog food so the way it works nowadays is that you know this is a process that used to take days by the way the way it works nowadays is that Amazon's predictive algorithm knows what I'm gonna need dog food so instead of waiting for me to order it it orders it on my behalf directly from the warehouse the word helps put it on a truck and then by the time I get to my computer to actually order the dog good it's ready to be delivered to me and that's now a process that takes only hours so this is an example in which you're using machine learning to basically predict and anticipate people's needs neck Netflix is another example in which they're using machine learning to personalize their services so on Netflix everything is personalized from the movie recommendations that you get to the previews that you see and and to the images that you see on your on your home page now think about what this means for the for the public sector you know usually governments don't realize that they're sitting on just as much data as as a private company as these huge providers when you think about the government there's a ton of administrative data from its interactions with citizens like us you know so the DVLA for example knows when I lose my driving license knows what car I own it has all this it knows what I pay my road taxes on time it has all of this information on me and the government has all of this information on me that it can use to actually personalize the services that that that that it offers okay so now that we know that those technologies are really exciting for us as policymakers we must also sort of give some thought about the ethics the ethical foundations of those technologies matter and to do that I want to take us to three examples that bring to light why we should be thinking about AI ethics in the context of adapting those those technologies for the public sector so the first example that I want to give you is about child welfare now in the UK because of many years of austerity a local authorities are strapped for cash so they cannot hire as many social workers as they have been able to hire in the past so some local authorities are turning to machine learning to try to identify children who are at risk now they're there they're there various ethical issues that come up with this particular use of machine learning one of them is who has the right to this information so if you're calculating the score if you're predicting you know whether your child is at least who has the right to see the score is it the parent is it the child is it just the social workers involved what should we do with the probability of 95 percent if an algorithm says that a child is 95 percent has a 95 percent chance of being a twist what do you do with that child should you take it away from the environment that it's in it's a very big decision that affects the life of the child and equally what should we do with the probability of 60% so we just ignore it and say and say it doesn't matter another question is can we explain the outcomes so you know if if a child is deemed to be at risk and that child is taken away from their parents based on this decision for example RR is anyone gonna be able to explain to those parents where this decision came from and how this poor was was reached are the outcome biased what we know about the systems is that they learn from the data that we feed them so if if if if social care workers in the past for example made biased decisions about about children those biases are going to feed into to the current systems and finally and I think this is a right question to ask should we even be writing those algorithms in the first place a second example that I want to that I want to talk to you about is policing and we're seeing the machine learning being used in in in in policing now I will give you this example from the London Metropolitan Police who use facial recognition technology at the Notting Hill Carnival in 2017 well we now know that the system performed very poorly well what does that mean there were the Notting Hill Carnival has over a million visitors and that stats for the technology it's for the Mets technology are as follows the technology scan the faces of you know supposedly a million people and it identified 35 total matches against wanted database that they held out of these 35 total matches 30 of them were clear erroneous matches that were ruled ruled out by human reviewers five of them five of the people who were not thought to be erroneous matches were stopped and I did in the streets out of those people only one of them was an accurate match to a wanted list so he got taken to a police station but the problem is that upon further four the question England in research they realized that the list is out of date so you know this person had already been through the criminal justice system and should have no longer been on this wanted database so really the trial had a zero success rate this raises important questions and the Information Commissioner's Office launched an investigation into police forces use of face of facial recognition technologies so what are some of the questions well there was a lack of transparency about the use of the system so a lot of the carnival goers didn't know that their faces were being scanned and matched against the wanted database there questions about how the data was collected and stored so you know are the images of typical horse scan was it deleted right afterwards and who was on the wanted list that the police was was checking this data against there are questions about the right to be forgotten so the one person that was taken to the police station supposedly shouldn't should have never been taken back to the police station because he had already served his sentence there are questions about vulnerable people and it doesn't apply so much to the Notting Hill Carnival example but in a different use of this technology the Met has looked scan people in crowd the faces of people in crowds against the mental health database watch this database and you might wonder about sort of the implications of that there are questions about differential accuracy and one thing that we know about facial recognition algorithms is that they do they perform particularly poorly for black and minority faces and that's because there are mostly trained on faces of white people so they don't do as well for for minorities the other thing that we know is that police forces have a history of over policing ethnic minorities so you basically have an have an algorithm that does poorly at identifying I think minority the faces of ethnic might or minorities that's been tasked mostly would identify in the faces of ethnic minorities and and that's a problem and the final question was not the final question the final question I'm gonna be talking about its proportionality you might want to ask if you know the zero success rate it's worth scanning the faces of a million people in in the crowd okay the final example that I want to talk to you about is about sentences and this comes from from our our friends across the pond so this is from the US where compass' predictive system was used to assist judges in their conviction decisions so what compass did is to determine whether a person was at high risk and provide that information to the judges now the way that the score works is that you know the algorithm returns a continuous score from 0 from 1 to 10 but the judge doesn't see the continuous score so they're not provided with the actual numbers they're provided with three bins so if the score is between 1 & 1 & 4 the judge gets to know that the offender is at low risk of reoffending the score is between 5 & 7 the offender is a medium risk and if the score is between 8 and 10 the rear fender is deemed to be a high risk of reoffending now what is the problem with those bins that the judges see remember they don't see the actual number well the problem is that what we're seeing and what we're finding in our research is the judges award different sentences according to those pins so they would all that's being equal that would award the higher sentence to someone would say medium risk compared to someone with with Lois but consider those two people right on the boundary is there any difference between those two people is there you know in in statistical terms we would say is there any statistically significant difference between them namely you know they might actually win we might not actually be able to tell them apart but the judge will award a higher sentence to the person with a median score which put in countless if I request to try to understand you know where where the confidence intervals are for those predictions and we're told that the algorithm is proprietary so as far as we know we might very well be the absolutely no statistical significance difference between those two people yet one of them is gonna end up in jail for longer than the other okay for the final area of opportunity for us you know I'm not gonna say too much about this contributing to policy that governs the use of data science in AI but I want to draw your attention to one thing to some extent you know some some of the platforms that we have police themselves so Amazon recently for example scrapped a secret secret it wasn't that secret an AI recruitment tool that showed bias against women so that's a that's an example of self policing but we don't we cannot expect all companies to continue to do that we do need more regulation for it there are some studies showing that mortgage algorithm for example perpetuate racial bias in lending but before we all sort of despair and we say okay you know those technologies are gonna be biased and it's not gonna make for a very fair world one of the sort of last ideas I want to leave you with is that you know sometimes yes those technologies are biased but so are our decisions in the past so you know the reason the Amazon algorithm is biased is because in the past computer programmers were they were hired they were mostly men in the and it's the same with mortgage algorithms but what's exciting about about those technologies if you want to look at it more optimistically is that they lay those those biases bare so we actually can see how bias they are well as those decisions in the past were were hidden and we weren't able to scrutinize them so that's all for me today I have two takeaways for for for my talk and the first one is that AI and data science hold holds tremendous promise for government and it does have an enormous potential to improve the way we design public policy and to improve the way to improve the provision of public services but that that's only the case and we're only gonna be able to read those if we get the fundamentals right and with that I'm gonna I'm gonna hand you over to my colleague David Leslie who is the ethics fellow at the alan turing institute and as i mentioned earlier in my talk the government has produced report on understanding AI Essex and state before the public sector David was the author of that report and we're very proud and delighted one that the UK government actually took a lead on this and it is the most comprehensive guidance on the ethics and ethics of safety using AI technologies in the public sector and in the world so let daily take it from here okay first things first can everybody hear me good it's good okay great well as cause Mina suggested we've been busy at work with this guide that we've written on ethics and safety it's called understanding ethics and safety we've written written it for the office of for AI and in any case what that means is I my my remarks today will be a somewhat off-the-cuff so if you'll bear with me in terms of that I wanted to to start by just asking asking for feedback from you so when when you look at the the cog X insignia here what do you see just shout it out what would what do you see from that image you brain anything else what is it you see circuits right you see circuits you see connections right so the the start of our research is really to sort of ask the question what is this kind of connection between thinking of a kind of a human dimension of technology and thinking of the technical or scientific dimension of technology when you when you look at the sort of insignia you see it kind of running together of the human and the technical and and and in a sense the question of connection right are we thinking of connection visa vie the circuits of a deep neural net or are we thinking of connection in terms of how we connect to each other these are fundamental questions when you think about these sorts of technologies so let's just let's just start with a little story that I want to that I want to tell you which will lead us to a little bit of a sense check about this this material let's go back to a 1936 a young 23 year old mathematician from might avail named Alan Turing sat down with pencil and paper in a small room in King's College Cambridge using just the image of a linear tape divided into evenly evenly into squares a list of symbols and a few basic rules he drew a sketch to show the step-by-step process of how a human being can carry out any calculation from the simplest operation of arithmetic to the most complex nonlinear differential equation turnings remarkable invention now known as I'm sure most of you know simply as the touring machine solve the perplexing and age-old mathematical question of what an effective calculation is the question of how how one defines an algorithm not only tutoring show what it means to compute a number by showing how humans do it right how humans carry out a simple practice of computing he created in the process the idea behind the modern general-purpose computer Touring's astonish the astonishingly humble innovation as some would say assured in the digital age now let's think of the of the Turing sense check this is just a quote from the man if you can if you can read it the Turing sense check we need to see the construction of algorithmic models in the way touring himself modestly and soberly did as an eminently embodied and human activity an activity guided by human purposes and values an activity for which each of us who is involved in the development design and implementation of AI systems is morally and socially responsible touring also actually had a concept of of mathematics itself as what he would call a kind of ongoing practice of intellectual search a kind of intergenerational project of a human community in search of making the world a better place and and it's this kind of starting point that we need to think of when we think of the direction of our technologies I would I would call this a touring sense check we need to think of these technologies as guided by human purposes and values sometimes we think of the technologies as as guiding us the technologies become the tail that wags the dog we need to think the opposite direction I would say now the second sense check a picture there of Marvin Minsky the famous theorist of AI so Minsky famously defined AI in the following way he wrote artificial intelligence is the science of making computers do things that require intelligence when done by humans now the sense check when humans do things that require intelligence we hold them responsible for the accuracy the reliability and soundness of their judgments moreover we demand of them that their actions and decisions be supported by good reasons and we hold them accountable for the fairness for the equity and for the reasonableness of how they treat others what creates the need for the burdens of fairness interpretability explanation justification in the design and use of artificial intelligence systems is that their emergence and expanding power to do things that require intelligence has heralded a shift in a wide array of cognitive functions to algorithmic processes that themselves can neither be held directly responsible nor immediately accountable for the consequences of their behavior to state the critical obvious an inert as inert and program based machinery AI systems are not morally accountable agents this has created an ethical breach in the sphere of the Applied Sciences of AI that the growing numbers of frameworks in AI ethics are currently trying to fill targeted principles such as fairness accountability sustainability and transparency are meant to fill the gaps between the new smart agency of machines and their fundamental lack of moral responsibility so let's get a little more concrete when we think about what we need to drive human values and responsibilities in the production of technology we can come up say with four things first of all we need to ensure that our ethical that our AI projects are ethically permissible by considering the impacts they on the well-being of affected stakeholders and communities second we have to ensure that our AI projects are fair and non-discriminatory by accounting for their potential to have discriminatory effects on individuals and social groups by mitigating biases that may influence their models outputs and by being aware of the issues surrounding fairness that come into play at every phase of the design and implementation lifecycle thirdly we have to ensure that our AI projects are worthy of public trust by guaranteeing to the extent possible the safety the accuracy the reliability the security and the robustness of the products of those systems finally we have to ensure that these AI systems and the projects that build them are justifiable by by prioritizing the transparency of the processes by which the models are designed and implemented but also the transparency and interpretability of the actual decisions and behaviors if we could call them at of the models themselves so this is where we come to what our sort of concrete recommendations are for building AI systems in the government and we can think of this as an ethical platform for the responsible delivery of an AI project and we have three levels that we that we we start we start from the first we call some values these are values that support underwrite and motivate a responsible innovation ecosystem we've named these respect connect care and protect I'll go through them in a little while the second level we call the fast-track principles namely fairness accountability sustainability and transparency and if the some values are motivating factors in the innovation ecosystem for the production of AI the fast practice the fast-track principles are meant to facilitate the action of building the AI right so the values are guiding the general perspective on the objective of the technology and the fast-track principles are actionable they're operationalize about the context of the design and implementation of the systems and and finally the the third level of third tier we call a process based governance framework and basically what a what the what the PBG framework does is it sets in in in in concrete terms the the protocols necessary to have transparency and accountability in the productions of the systems right to to to make the the practices traceable to make the humans who do who engage in the in the design of those systems to make them answerable and accountable so in in in guiding stakeholders on how to actually use these systems what-what this kind of platform does is it gives them an occasion to do three things reflect act and justify to reflect using the some values to ask and answer questions about ethical purposes and objectives of a project to assess and reassess the impacts of the project on individuals and communities act to act using the fast-track principles to ensure that every step of the project aims to produce ethical fair and safe AI innovation and to design and implement always responsibly and finally justify justify using the process based governance framework set up governance product processes that ensure end-to-end transparency and accountability so we can quickly just go through the the steps without going too far into them the some values are distilled we could say from two sets of principles there's the the bioethics princess of bioethics and the principles of human rights these are the two main threads that have fed into the production of most of the extant a ifx frameworks there's about three dozen of them and so over the process of the research we in the system we both sort of did an extensive review of all of the the sources of in in AI literature but we also spoke to our stakeholders we spoke to the our stakeholders in in the government organizations about how the rubber hits the road with these values will the values in a sense have good uptake in the processes of the sort of public sector production of the technology so I'll go through these quickly respect respect the dignity of individual persons this idea is ensuring the ability to make free and informed decisions about their lives to safeguard their autonomy their our power to express themselves and their right to be heard to secure their capacities to make well-considered and independent contributions to the life of the community connect connect with each other sincerely openly and inclusively safeguard the integrity of interpersonal dialogue and meaningful human connection and social cohesion prioritize diversity participation and inclusion at all points of the design and development and implementation lifecycle encourage all voices to be heard and all opinions to be weighed seriously and sincerely throughout the production and use lifecycle use the advancement of the proliferation of these AI technologies to strengthen the developmentally essential relationship between interacting human beings care care for the well-being of each and all design and deploy AI systems to foster and to cultivate the welfare of all stakeholders whose interest is interests are affected by their use do no harm with these technologies and minimize the risks of their misuse or abuse protect protect the priorities of social values of justice and of the public interest treat all individuals equally and protect social equity use digital technologies as an essential support for the protection of fair and equal treatment prioritize social welfare the public interests in the consideration of the social and ethical impacts of innovation in determining the legitimacy and the desirability of AI technologies these are these are orienting values that should factor into when we're thinking about the objectives and the purposes of our AI systems right how are we determining our target variables how are we how we defining what what's what's legitimate in terms of thinking of a measurable proxy for our target variables we need to be thinking about these values as they are orienting the the potential results of our technologies okay now the fast track principles when we think about the fast track principles we need to think about two things think process and think action if we think process we start to see what what are what are seemingly abstract principles fairness accountability transparency sustainability we start to see these dimensions as necessary and integral parts of an operating in a responsible AI environment well I'm just gonna check the time I might I could go on for like five hours about this stuff okay okay so so thinking process in terms of fairness so think about the way in which fairness might factor into the design and use of an AI system and and if you think about this to start with don't think about fairness as a this kind of highly contestable topic how do we define fairness there's a hundred definitions of fairness I would say start with a basic minimal threshold we call this the principle of discriminatory non harm principle director thank you the principle directs us to do no harm to others through the biased or discriminatory outcomes that may result from the practices of our innovation so in other words think bias mitigation and think think non-discrimination and when we think about our systems right that's the starting point and now if we think about that in terms of process we start to think about I would say a few dimensions of fairness the first would be what we would call data fairness data fairness is basically the idea that when we when we use data to build a system right data doesn't just show up as given in Latin the word data means literally that's who means the given but humans don't have a neutral relationship to data we decide where we collect it how much of it we collect how we curate it when we put it into our systems how we annotate it how we label it how we how we pre process it in order to be put through and generate insights and so we need to start by thinking about the representativeness the relevance the accuracy and the generalizability of our datasets that's basic data fairness basic data equity the second bit of fairness when we start to think about process would be called design fairness and and this basically involves thinking about our formulation of the problem of building an algorithm in terms of non bias and and non-discrimination so when we set our target variables and we define the measurable proxies for those target variables we need to think about the stakeholders that are affected by our choices would they accept our definition how are they going to be affected by our definition we also need to think about how we how we select features to be included in the feature space right because the selections that we make in the design process are going to largely determine the the results that come at the end of that process so in in a sense if you're including discriminatory features or features that will act as redundant proxies for discriminatory characteristics your system is going to be discriminatory fairness has to be incorporated into every moment of how the human hand designs the system and you know this is the touring insight right these are practices of the production of technology that we have to remember the humans involved in the third bit of fairness is outcome fairness our models in their in their effects have outcomes that have potentially discriminatory outcomes we need to think ahead of time how we set up our algorithms to produce outcomes and in fact there there's a wide variety now of increasingly effective computational technologies to incorporate formal structures of fairness into the architectures themselves fairness in terms of the error rates fairness in terms of the individual impacts before you heard Sandra and and Brent talking about counterfactual fairness which is also has to do with outcome fairness so we have increasingly we have access to structures to deal with the the potential inequitable impacts of the algorithms finally and and I will probably wrap up here because I'm running out of time let's think of implementation fairness this is this is one dimension of fairness in the process of building the technology that isn't often thought about so just to recap we have the data fairness design fairness outcome fairness now I think about implementation fairness right and algorithm produces a result and the result doesn't have an independent standing as an inference right a result is a kind of a statistical production a product it's a number right it's the human interpretive capacity that transforms the results into an outcome right and that moment of implementation of use is crucial when you think about fairness because in some sense there are options for the implementers of an algorithmic system to just rely on the result of the system say a high risk score without reflectively engaging in an interpretive process to think about well are am i considering holistically or within the full landscape of context how this result of a high risk score is going to affect the individual life in the social context of the stakeholder implementation bias is come when we over rely on statistical results when we over comply with statistical results on the other side they come when we just sort of distrust the algorithmic productions when we don't weigh in a reasonable way the production of a reason based conclusion because we think that we in our know-how in our practical engagement of the world have a better grasp of of the empirics of the world we need to think about how to responsibly and fairly utilize the statistical output in in the way we decide on the supposed decisions that are being made by these systems I'll stop I mean there's more slides but I'll stop there thank you so much David oh thank you so much David thank you cause Minah they're going to be taking questions now so there's a microphone that's roving but perhaps given a sound if anybody would like to come up we can maybe do it this way and if you don't want to step up the three steps here I can relay the question to the audience for you hey thanks for the talk really interesting I wanted to ask it seems that transparency is the one suggestion that comes most often when we talk about applying ethics to AI for example it's also a double-edged sword in the sense that it can have negative effects on or the contrary effect of of safeguarding individuals and I was wondering if you could talk about issues in promoting transparency and at the same time making sure that we're not just open sourcing tools that could turn out pretty bad in the wrong hands yeah so it's a great question I think that transparency is a kind of hot you know word that it triggers a lot of anxiety in certain circles for instance you know when we start thinking about the algorithmic black box the proprietary black box in in the in the private sector that transparency seems to be a threat to intellectual property right or it's a threat to people say gaming the system when they discover too much about a model right I think these are are interesting and and and and valid issues but they don't really have I would say bearing on the real normative issue of transparency the real the real moral issue of transparency there are two arguably two kind of levels of transparency you've got what we can call process transparency which we could think of process transparency as basically the way in which when an innovation process is taking place that there's a traceable a series of behaviors that that can then be audited and and held to account for the justifiability of the results right that's process transparency and I think that this transparency can be the need for process transparency can be answered by having good protocols in place having a stakeholder impact assessment having a data set fact sheet so that you can trace the data from start to finish having you know various other ways in which there are self assessments about non-discrimination and statements about fairness policy that are made explicit in public these are dimensions of process transparency where a given stakeholder a company or an organization is is saying this is my innovation process and and you can look inside of it we are acting justly we are acting fairly we are acting responsibly that's process transparency now we also have outcome transparency which is where this whole kind of field of explainable AI comes in which is how do you really understand and interpret in a in a humanely comprehensible way the results of a complex algorithmic process right so we understand say the results of a linear regression or a simple decision tree but can we really understand this kind of complex output of a tree ensemble which is a very complicated series of decision trees that are cobbled together or can we understand a neural net now this this is a this is where it gets tricky with with transparency because in a sense figuring out the the ways to translate the Macan the technical input processing to a humanely understandable output is putting a lot of pressure on people and it's a it's the it's the very beginnings of this field but we need to push for pure and simple explanations of outcome transparency so that's how it ends there's two parts of it I want to add one more thing to that that we also need to be much more transparent about the limitations of those that knowledge and he goes back to sort of the compact scores that I did I discussed we don't know how precise those estimates are and that's not proprietary technology we should absolutely know that and we should make sure that the people making decisions on the basis of the predictions from those technologies understand their limitations and understand where they might or might not do very well hyah so already answered my question in that but I thought if you had any extra thoughts to add how would you propose that you know like large organizations like Amazon who developed this type of my proprietary software all the time how should they show that they're kind of adhering or prove that they are adhering to the guidelines for example that you publish well luckily for for us the guidelines that we published are just for the public sector so it refers to the government's use of those technologies which you know there's much greater over oversight there but there is an entire debate as to you know how should you regulate and who should have visibility into you know large platforms use of those technologies I I don't really have a good answer to that we've you know we we mostly give thought to the government use rather than the public sectors use of the technologies but of course that's that's just as important I would say okay I would say it's important at this at this point that we stress the need for audit ability with regard to large companies I think that there's been a tendency in the private sector to be to hide behind the black box in terms of the ways in which we think about the possibility for insights into these kind of opaque processes I don't think it needs to be this way right there's when we catch when when society and policy catches up to the technology which it has to it simply has to when it does we will figure out that auditability having a third-party auditor right to go in and to look into the processes and the end the models train models is a necessary component of securing the the public interest and and I think as I say I think we're just at the beginning points of that conversation but it's where the conversation needs to go because if you think about due process for the infor the individual citizen due process will not exist unless algorithmic decision making is is is made accessible to the everyday person and so we need audibility regimes because we not thanks for your presentation you mentioned the idea of personalization of services government services in the same way that Netflix and Amazon personalized services I've got two questions about that firstly can you give an example of what that might look like and secondly one of the criticisms of personalization is the echo chamber that you continue to perpetuate you know that the narrow narrowing of views and experiences of people and I'm interested to sort of hear your reflections on how that may play out in a government services context as well I think we're just in the beginning stages to see where where this could lead I mean very sort of like basic and tangible level you can see personalized medicine for example take off and you can imagine those technologies being able to help a personalized treatment plan for someone so I think that's within reach and that's something that that people are already working on and you can already see it around around that a very sort of you know another level you can eat you know you can imagine for example you know the way we provide hope to unemployed people finally finding a job I mean it's quite clear that we don't need we don't all need the same unemployment support from the government we don't need the same formula for it so you know the idea is can you use those technologies to really help someone get a job as quickly as possible and and you can also imagine even sort of like other projects further down the line of you know can you can you imagine a sort of personalized education plan that's designed to the specific needs of a child and and learning styles styles of a child so I mean there's a continuum of projects they can imagine and it's a very sort of in you know it's an emergent field and we started to to give some thought about that I'm not quite sure what to say in response to your to your second question David doesn't really know what to say sorry I don't know about easier um so my question I think is particularly for David you talk about what this session is a call quickly about a young government and you may and so forget your name but you you make a point about about the use of AI by governments now often that that conversation seems to be framed in terms of governments have a large amount of data and we ought to there we ought to we ought to make use of it we ought to it ought to be analyzed probably and do you think David that your framework misses the point slightly in that it suggests that we should think first about what we want to collect and then that we sorry that we should think before we start doing any work with machine learning about what data we want to collect you not think in practice people would simply use the day all the data they have and trust an algorithm to decide what isn't is not useful predictably furthermore how would you you talk about whether data collected ultimately it's like choosing which data to use for a machine learning model based on whether they will liter fair outcomes and things do you think I mean do you think there is any foreseeable situation in which an a machine learning algorithm could fairly predict there's a protected characteristics of characteristic you know to predict something yeah it's that necessarily unfair and how do we deal with it if our machine learning models tell us that protected characteristics are useful at predicting things yeah those are those are great questions I would say on the on the first on the first question or the the first part of the question that we are moving into an age now where data collection or the availability of data it is in a sense being produced by the kind of cyber physical world that we live in the data in a sense data is increasingly available as as such because we just live in a world in which there's a lot more sites of measurement there are a lot more actuators and sensors out there and and so the data will be there now the question is whether or not our values our objectives and purposes we'll be able to kind of steer the way that we handle the data right and so I don't think it's an either/or in terms of you first collect the data and have it data-driven as such or you really decide you know how you're gonna handle a specific context of building a model right so in other words when we think about responsible research and innovation in data science we are thinking about the ways in which the the scientists or or the innovator is going to be able to craft a domain-specific engagement of data to have a good analysis that produces an evidence an evidence base or support for an evidence based judgement right and so I think responsible data science is and isn't one answer to your question which is to say data isn't the tail that wags the dog we are engaged in processes of innovation where we're treating data responsibly and and and so I think that that's a mention is is is important to recognize in terms of processing protective characteristics it's important to recognize that fairness aware processing will oftentimes require us to understand the demographics the protective characteristics that are at play in a situation and so the the the need here is not necessarily to be afraid of the protected characteristics but but it's to start from a a perspective that prioritizes equity and fairness and then uses what means are available even privacy-preserving means I mean at the touring we've got a lot of innovation going on to figure out how we can protect individual data subjects and decision subjects in terms of the protected characteristics that define them but also take into account those those protected characteristics to produce fair systems right so it's important to realize that fairness fair fairness awareness does in sometimes involve being aware of the discriminatory features and we need to sort of proceed with that reflexivity with that knowledge that that needs to be incorporated into the practice cos Meena any last words maybe one thing that I would like to add and this is about the data data held by government and this is certainly the case in this country that the data itself is quite inaccessible so one of the things that governments really need to pay attention to and give thought to is how can they though the data architectures that they need in order to be able to actually use those technologies and that's a question that we haven't discussed today but and I'll leave it right there and thank you very much for for joining us thank you you 