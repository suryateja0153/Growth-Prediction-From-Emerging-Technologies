 You're not going to want to miss this episode of the AI Show where Mehrnoosh talks all about model interpretability. It's pretty cool. [MUSIC]  Hello, and welcome to this episode of the AI Show. I have a special guest with me, Mehrnoosh, how you my friend?  Great. How are you?  Good. What do you do here at Microsoft?  I'm a Technical Program Manager in Azure AI working on machine learning interpretability and fairness.  So you talk about fairness. Maybe you can explain this, because there's a lot of controversy with AI around. Because nobody quite knows what it's doing. Can you explain what you mean by fairness in AI?  So in general, Seth AI is being used in a lot of different scenarios. Some of these scenarios are a really high-stakes scenarios that come as an effect of AI being everywhere. We have really high-stakes scenarios such as [inaudible] and prediction where AI decides whether someone should be released on bail and what is that individuals chance of recommitting a crime. We have scenarios into health care that AI is basically deciding or predicting someone's chance of getting a particular type of disease, all the way to the finance world where AI decides if someone's portfolio carries a high risk for a loan application.  Right.  When we're tackling this very high-stakes scenarios, we would like to make sure that our decisions or AI decision is being fair across the board, and really is reliable, safe, private, and inclusive.  It's not just that we wanted to be fair, but there might be regulatory issues regarding like well, you can't discriminate when you learn and if your AI is being a little discriminatory that could be a problem as well.  Absolutely. Yes.  So what do we do at Microsoft to help explain how we do this stuff?  So that's a very good question because this is a global challenge that is happening right now. In fact, in a very recent study from Capgemini Research Institute, it was highlighted that nine out of 10 executives in the world of AI is struggling to get AI right. They have reported that they have seen some form of challenge in the world of AI in the past two, three years. Basically what is happening is, we as Microsoft have taken a step back and we have come up with six different principles that ensures responsible development and deployment of Artificial Intelligence System, and they are, fairness, inclusiveness, reliability and safety, privacy and security underpinned by two foundational principles, transparency and accountability.  This is a pretty good framework because I'm more of a data science guy and the one that's interesting to me is that I understand all of the other ones, but it feels like the foundation of this is, what is the model actually doing? So what do you mean by transparency?  So that's the focus of this AI Show, and we focus on interpretability that one not a principle, in its own is closely relevant to transparency. What it says is, let's understand how the model has come up with its predictions. Basically, you want to understand at the global level, how the model is making prediction, but also at the local level, how the model's prediction is actually being made for a particular individuals. Say, "Why Nina has high risk of colon cancer".  I see. So it's not just the overall, what's the model doing.  No.  When you predict, why did it do what it did?  Exactly, and it's important at two different stages. It's important at training time for model evaluators and model designers. Because they need tools to sell their models to non-technical stakeholders to build trust. But also they need this in order to debug their model and understand how their model has really come up with the predictions, and improve the model. It is also important at inferencing time. For the same reason, when you deploy the model, you want to make sure how the model is really behaving in the world out and how its decision is really treating people.  That's interesting because obviously these things when I went to school were a lot easier because they're all linear models, right?  Yeah.  You knew that this feature map to this weight.  Yes.  If it's a bigger weight, then you knew that's what's affecting it, but what Neural Network Models it feels it's a little bit harder to do that.  Exactly. That's why when we talk about interpretability, we usually mean two different classes of interpretability solutions. The first class is the Interpretable Models or Intrinsically Interpretable Models, like Linear Regression that you mentioned, Decision Trees. Those are the models that are just interpretable in nature.  Right.  You can train them. You can understand how the features play a role. The challenge with them is, they're usually not generalizable to all scenarios. They have their shortcomings because of the assumptions that they make. Now, we do have this second class of interpretability solutions that we call Blackbox Explainers.  I see.  The way that we explain them is basically they can explain any model, no matter what your model needs is underneath. It can't be a Neural Network, it can be an SVM. It can't be a Random Forest. It can be either Traditional Classifiers or State of the Art Neural Network Classifiers or Regressors. Basically, the way that they do that job is, they use bunch of approximations based on which method you're using to come up with how this model is making good prediction. It's super useful because you have a lot more flexibility. You can use these very strong classifiers or regressors at the training time, and then you can use these Explainers to explain how that model has made its predictions.  So I'm having a hard time visualizing this. Because to me when I look at let's just say you're using a Logistic Regression. The weights are very clear. You can map it and you know, and then maybe you can even give it something without even knowing, you know exactly what is going to predict because of the weight. When it comes to like let's just say you're using a Convolutional Neural Network to predict what's in an image. How does the Blackbox actually work? Or you just giving it data and then figuring out what it's doing? Maybe.  There are multiple different techniques either from Microsoft Research or state of the art tech third-party libraries that are out there. Each one is using a particular technique. For example, we do have shaft that is based on the idea of game theory, and tries to do a bunch of mathematical calculations to understand how different features as well as players are playing to come up with that prediction which is end of that game. We do have mimic explainers or global circuits where we bring some interpretable models to mimic that black-box model. We do have lime that also has the idea of circuit models but at the local phase. So each one is using a particular technique and at Microsoft, we want to bring them all under the same roof, provide them to you within one API and one set of data structures.  So that's pretty exciting. Let me get back to the one before. Basically, now I think I'm understanding, when it's a black box model, you're saying, you have some mathematical ways of knowing how to vary the input, to vary the output in the way that you want and that will tell you the features in that. So that's amazing. Then the second thing that was super interesting is you said, "we have an API to help with that," I'd love to see some of that.  Great, sure, let's go through a demo. So the way that it works is we do have an interpretability SDK which is under Azure Machine Learning SDK.  So this an SDK and AML.  Yes.  Okay, cool.  It's an STK and AML which is great news because it can be used in conjunction with Azure Machine Learning services. You can even deploy these explainers and use them at referencing time to explain why the model is making its predictions on the fly and you can as well use it completely locally, we have bunch of explainers that we're integrating on for tabular data, we have sharp global surrogate, different global surrogate models, feature permutation which is based on what you said, PaterB features and see how the accuracy of the model changes, obviously the bigger dot change. The more important feature, we do have support for a line which is also based on idea of global surrogate but it's at the local phase. The demo that I'm going to do today is completely locally without contacting any Azure services and a way that it works is basically you go through your training phase the way you used to go. So first you just import a tabular explainer or any other explainer that we have, as I said we have multiple options, then you get your Dataset. Now, this Dataset is about IBM employee attrition which is a binary classification that decides whether someone would leave the company or would stay with the company, you get the data, you clean it up, maybe get rid of some of the columns, you try it and you have split it into tests and train, the way that you used to do it.  Standard stuff.  Completely standard stuff. You do the transformations on your numeric and categorical features and then your pipeline for training becomes those transformations and then your classifier. We support any classifier from the world or regressor from the world of traditional Machine Learning, all the way to a more advanced algorithms or deep neural NET frameworks like PyTorch, Keras with TensorFlow, back-end and also TensorFlow models.  As far as the explainer is concerned is just another function is testing.  Exactly.  Cool.  So you fit your model, so far nothing about interpretability, that's where we come into this scenario. You can use the import that you did on top, pass them model to it, pass the background data or de-initialization set, and these are some extra features that you can add, add the feature names, add the name of the classes rather than [inaudible] Class 01 and if you pass the transformations, SDK is able to reverse the transformation and provides to you the explanation in terms of raw features rather than engineered features which is exciting. Otherwise, it defeats the purpose of really understanding what's happening. This little call creates an explainer object where that you can use to call explain global and explain locals. So when you explain global it means that globally what are the important factors for that Machine Learning model and you can print them, look at them. For example, it says over time number of companies person previously have worked are the top two important features or you can see a class, different classes. Here we have class staying and leaving you can see what are the top key important features across these two class. You can also pass a particular data point to the explained local function. For example, you want to explain me or someone else who works at that company and a range of data points and you can similarly see what are the top key important factors for that particular person.  This is really interesting because there may be a scenario where let's just say your model is not very kind to certain class of people which would make it effectively racist, which is not good.  Yeah.  You might have a scenario where the model passes on the global but then as it gets examples that it hasn't seen perhaps at inference time you're starting to recognize.  Absolutely.  There's a huge data drift that just happened from that training to our inference and now we need to fix that.  Absolutely. I want you to note that we also obviously have the model accuracy metrics and they're super useful to understand how the model is making it's prediction but the challenge is, the famous example that pops into my head is, a neural network that wanted to differentiate between the images of wolves versus Huskies. The way that it used to do it was basically looking at the background. If it was snow it was saying wolf, if it was grass it was saying Husky. Now, obviously we do have a problematic pattern that AI has learned and it can be generalized to other models but the challenge was the accuracy was really good because the test set also happened to have wolves with a background of snow and Huskies with the background of grass. So we could completely lose this very problematic challenge or pattern when we were looking at the accuracy but transparency can come to help you to understand, is the model really making predictions based on the factors that makes sense or it's just some random patterns that it has learnt.  That's really cool.  So we have added a visualization dashboard that will allow you to understand this explanation's and also in general your model better. Right off the bat, when you run this visualization, you see four different tabs that are supposed to help you understand some global insights about your model. The first tab is data exploration that will help you to pick any feature on the x-axis, any feature on the y-axis. For example, in this case we have the predictions on the color where the blue points have been predicted to stay, the orange points have been predicted to leave the company. Now, the next tab is where the explanations comes into scenario. We have globally importance which shows what are the top key important features. So basically, the information you saw from the function calls now visualize and you can play with how many of them you would like to see, this says over time, a number of companies and job satisfaction are the top three factors that contribute to someone with leave or stay with the company.  Let me run this value because, if I'm seeing like in this bar chart right here, I'm seeing one bar that's super big and all the ones that are super tiny, you might be leaking the answer in one of your features basically and that would be one way of looking at these things.  Exactly.  Will that be a good way of thinking about it?  Yeah. So basically first of all, you can check it with some non-technical stakeholders and see whether this prior and makes sense. You can also see, for example, are these patterns fair or not. So for example if in some scenarios of loan application, if gender or ethnicity is one of your top key important features or some factors that are heavily correlated with gender and ethnicity, then you know that you have an issue, then you need to go back to some fairness frameworks and try to mitigate that. But the challenge with this chart is this is unsigned. So you can see in what direction for example over time helping. So that's why we have added summary importance which basically has signed local feature importance values across all data points to show the distribution of impact that each feature has on prediction value. As you can see, we have the same setup of features that a high values of each feature is shown with reds, the low values are shown in blue. So for example in this case, you can see that the high values of overtime is contributing negatively toward the prediction of stink which is intuitive, like if the person is really working over time then they probably are going to be predicted to leave the company or high values of job satisfaction is contributed positively to the prediction of staying. So if you're happy with your job, the model says that okay, most probably you're going to stay with the company based on the training data it has received.  So let me see if I understand this chart. So basically red means high values.  Yes.  Negative means impacting negatively for the Class and then blue means low values.  Yes.  Okay. So if I'm reading this right, that means at high values of the overtime impact negatively on people staying.  Exactly.  Which makes sense.  Yes. We also have this explanation exploration, let's ignore the color for a second, demonstrates how a feature is responsible for making a change in model prediction values. So let's just focus on our first observation, ignore the color, we would like to see how the effect of number of companies worked on the probability of staying with the company changes with different values of number of companies worked. So on the y-axis, we have the feature values for a number of companies worked and the x-axis, we have the importance of the fact or a number of companies worked under class staying. As you can see, as people worked on more companies, so we're working towards the higher points of this number of companies work feature, the feature values becomes negative toward the class staying. So that's sort of reinforcing the insight that we saw in summary importance of how these different values of this feature is contributing to the prediction of staying versus leaving. Now, let's bring the color into the scenario. The color is job satisfaction, red values are high job satisfaction and the blue values are low job satisfaction. So as you can see, let's focus on the last row which are people who have switched the most number of companies in this dataset nine. As you can see, the red points for them, the feature importance value is closer to the magnitude zero and for blue or purple, the feature importance magnitude is further from zero. So now what this is saying? It says that number of companies has less impact on the probability of staying for people with higher job satisfaction and more impact and probability of staying for people with lower or just mediocre job satisfaction. So you can see how these two features interact in a way.  This is really cool because, is this visualization available for all the models?  Yes.  This is pretty cool. Because one of the problems that I have, and I hate this, I hate this. If you do this FYI. I remember going to a pretty smart place and they were showing me this neural network and they're like, "We don't even know what he's doing on the inside", and to me, that's super frustrating. Like it's almost like you had something like make a function out of random whole cloth, and you're okay with what it's doing, that's a problem.  I totally hear you, and the great thing about this is basically you can pass any as I said model trained with PyTorch Tensor flying Keras with TensorFlow back in as long as your model has that.predict_proba functions implemented that is conforming to psychic convention or you can pass another function that the output is conforming to psychic convention of predict or predict_proba, we can support that, and we can explain that for you.  So pretty much anything?  Yes.  If it's completely random, you just make another function that takes your anything, it makes it look like regular going to work.  Exactly. The cool part about this is now I just gave you some visuals about basically overall how the model is predicting. You can click on any particular data point and that populates another window for local explanations. So basically, I clicked on this person, this person was on orange points. So yes the prediction was leaving, here you can see a confirmation of that, that person has been predicted to leave the company with probability 81 percent. Now, you can see what are the top k important factors that contributes to that particular prediction. You can play with them. For example, for this person the fact that he or she was working overtime, the fact that he or she has changed seven different companies in the past or belongs to the sales department are the top three important factors that contribute to the prediction of leaving with 81 percent. Another tab that we have, you can see that individual's data points here. She has for example the travel situation of travels rarely. She's 26 years old, she belongs to department sales, and you can run different what-if analysis scenarios here. For example, she travels rarely, what if she travels frequently? Well, the prediction is going to be leaving again, but now with 91 percent. So you can play bunch of these games with this perturbation exploration scenario, and that's a very, very good proxy into the world of fairness as well, because for example say this is a loan application scenario, you can literally change the gender of the person from female or male to other or different values of gender and see how the prediction would change. Technically, in an ideal world, it shouldn't change because there are other factors that are really important for maybe someone's financial status and things like that.  Now, I'm curious.  Am not relevance to gender.  What if I want to change it, the gender?  Ninety-three percent.  So insane.  Yeah.  It's about the same.  About the same. But in some very sensitive scenarios, even that is not tolerated.  Right.  Eventually, we have this ice plot, let me put it on a continuous factor. This ice plot is basically individual conditional expectation plot that is talking about, let's keep all of his or her features the same. Just change one feature from a minimum value to a maximum value, and see how the prediction would change. For instance, in this scenario, I change just his or her number of companies that the person has previously worked at from a minimum value zero to nine, which is apparently the maximum value in this dataset, and as you can see, the value or the prediction probability of leaving is increasing as we move from a minimum value and number of companies to a maximum, and obviously this is symmetrical to a decrease in chance of staying because this is a binary classification, but if it's for example multi-class classification, you can see some beautiful patterns of how changing one feature from the minimum to maximum value would change the prediction.  Is he not only like for example we change male to female that was obviously insightful, but what if you have multiple like a linear change, you're able to see that various lots of features, which is really cool.  Exactly. Correct.  This is pretty amazing. Where can people go to find out more about this?  Well, we do have a documentation in Azure Machine Learning. Simply if you search for Azure Machine Learning interpretability, ours is the very first one that pops up. It walks you through basically bunch of visualizations. Everything that I explained is written there. You can learn about interpretability at scoring time and inferencing time as well as training time, and we do have a link to the GitHub of AML that basically walks you through a bunch of different examples either in conjunction with Azure services or completely locally on your computer.  So again, go to Azure Machine Learning interpretability, you've got to find that. Maybe we'll even make a link and we'll put it below so you can go there. This has been super insightful. Thanks so much.  Thank you for having me.  Thanks so much for watching. We've learning all about model interpretability. [inaudible] thanks so much and thanks so much for watching, and we'll see you next time. Take care.  Thanks. [MUSIC] 