 You're not going to want to miss this episode of The AI Show, where we're going to learn all about building more inclusive machine-learning models using Fairlearn, make sure you tune in. [MUSIC].  Welcome to this episode of The AI Show, Build edition. We're going to learn about building more inclusive machine-learning with Fairlearn. I've got a special guest with me, Mehrnoosh, how you doing my friend?  Great hope you are staying safe, I'm doing great.  So tell us who you are and what you do?  My name is Mehrnoosh Sameki, and I'm a Senior Product Manager at Azure AI, driving diprotic efforts behind some of our responsible AI offerings like InterpretML and Fairlearn, which is the subject of today's AI Show.  Fantastic. So when we talk about fairness and AI, what does that mean?  Actually, let me show that with my slides. So Seth, we are obviously at an age where many processes and applications have become or are becoming automated by ML systems. Increasingly, ML is being used in a lot of decision and processes that are critical for individuals, businesses, and society. Now, it is a misconception that because AI is built upon rigorous mathematical and statistical paradigm, it is neutral and in that sense, fair. However, there are many ways that AI systems can behave unfairly, and I'll give you two examples for that. For example, AI can give rise to harm of quality of service, which is whether a system works as well for one person as it does for another. You can see an example of it on the screen, which is basically a voice recognition system that might fail to work as well for one sex compared to another sex. Another example that I would like to highlight is harm of allocation, which basically is talking about AI system extending or withholding information, opportunities, or resources to specific groups of people. Again, another example you can see on the screen is screening loan or a job application, and that AI system might be much better at picking good candidates among white men than other categories or other demographics, I should say.  This is interesting that we have both types, we have quality of service and allocation, which is really interesting.  Yeah, it is worth mentioning that there are so many other types of harms that AI can give rise to, but these are the most common ones and that's why I chose to focus on them. These two are the ones that are machine learning fairness assessment and mitigation toolkit is also covering.  So what actually causes this unfairness in these AI systems, what is it that causes it to behave that way?  This is another very awesome question. Seth, at the very end of the day, AI is the product of human processes and decisions used to create it, the data used to train it, the environment used to test it. So the AI systems can exhibit different and sometimes very negative behaviors as a result of this process. One big source that can introduce unfairness in machine learning lifecycle is training data. So when you think about it, it often comes from the society and real-world, and thus it might reflect on the societies, unfairness's and discrimination towards minorities. So if we ignore that and do not compensate for that, a tool is perpetuating historical unfairness's that are happening in the society.  I see. So as I'm looking at these things, it seems interesting that, number 1, let me go back to our phase because I want to talk about this, it seems interesting that for some reason people say it's AI, so it's intelligent, but you're saying that it's only as intelligent as the data that you give it?  Yeah, and it's not only data, it's sometimes the collection process can introduce some unfairness, sometimes the model itself can introduce some unfairness. Sometimes all of these might work fine, but the deployment and the testing might introduce a very, very obvious unfairness. So it is super important to bring a very diverse group of people into the room when you are designing a machine learning model and AI system end-to-end, all the way from ideation and collection of data, all the way to testing and deployment and monitoring in order to make sure that there's nothing that you're overlooking there.  So clearly, this is a problem. How does Fairlearn help?  Great question. So we have Fairlearn which is an AI toolkit that empowers developers of artificial intelligence systems to assess their systems fairness and mitigate any observe fairness issues. Now, Seth, I would like to first call it out that fairness is a socio-technical challenge. So many aspects of fairness might not be even capturable by technology or by quantitative ways that we can measure them. This is a problem that is being studied by philosophers, psychologists. It's not that we suddenly come out of technology world and say, "Oh, we're going to solve it." There's no solution to this problem. However, there are ways that we can quantify some aspects of fairness with computer science and machine learning and try to mitigate that. Now, we you think about what Fairlearn covers, there are really two components to Fairlearn. The very first one is an assessment dashboard with both high level and detailed views for assessing which groups are negatively impacted. The second is a set of algorithms, some of them are fresh from Microsoft Research, New York City, and also proven third party libraries that can allow you to mitigate that observed fairness issue that you basically just analyzed in the assessment phase. These strategies are based on variety of support at fairness definitions such as demographic parody or equalized odds for classification task, and also bounded group lasts for regression task. Together they can be very easily incorporated into existing Machine learning pipelines to really allow data scientists to navigate the trade-offs between model fairness and performance and to select a mitigation strategy that really best fit their needs.  So as I'm looking at this, because you and I, we've done some stuff on the looking at where there might have been, for example, a dashboard stuff I've seen before with you. But the thing that maybe I haven't seen is this mitigation. Can you tell me a little bit about the mitigation phase?  Absolutely, first of all, Seth, the dashboard is also new. I remember that we recorded another AI Show on our machine learning interpretability toolkit. That has a separate dashboard that allows you to understand your model, what are the top key factors that went into our model. This dashboard is purely for the assessment and for understanding the fairness issues that can be introduced inside your models. So I would love to showcase that in your dashboard to you as well. But going back to your question, there are two categories of mitigation algorithms really that we support, within Fairlearn. One of them is the one that will be used during training time. It's very simple, you can use algorithmic techniques to convert a standard machine learning algorithm into one that optimizes performance under fairness constraints. I will showcase that to you very briefly and I we'll talk a little bit more about that and how it works. But the second category is the one that you can use during the post-processing. So you already have an existing model and you have assessed that and you understood that there are some unfairness happening inside it. What you can do is you can just pass the predictions of that model to this post-processing module and it finds the output transformations that optimizes the model performance under fairness constraints.  So just to see if I'm understanding right. Because this dashboard you're saying is completely different than the one we saw before.  Yes.  Then the other thing is that we're looking at the mitigation strategies. This is actually running during training, is that right?  That's what I was telling you about that there are two types. One is it runs during the training. We call it the reduction approach. What it does, Seth, it takes us standard ML estimator. So for example, you have the model, it can be a LightGBM model and it takes it as a black box and generates a set of retrained models using a sequence of reweighted training datasets. So think of it as it wraps your machine learning algorithm and each time it reweights and possibly relabels the data and call this black-box model and generates a brand new model. Calculates its performance, calculates its fairness insights, and goes through this reweighing and relabeling of training data again and call this basically black-box model with that reweighted and relabeled input points. So that's one that we call the reduction approach, but the other one is absolutely doesn't care about your model lifecycle. Once the model produces the prediction, it works as a layer on top of your classifier and derives a transformation of the classifiers prediction to enforce the specified fairness constraint that you have selected.  I see. So it's basically you could do either while you're training or you could add like a layer over the top to enforce of fairness policies, for example?  Exactly, and the reason why we did that because there are so many different personas out there that they come to us and they're like, "Oh, I do not want you to touch my model training phase. I just bring my model to you and I want you to do some post-processing and transformations in order to generate some predictions that are more aligned with the fairness constraint that I have defined." Then we have other genre that they feel absolutely fine basically the Fairlearn accessing their model and going through this in-processing in a way and a pre-processing too, because it also applies weights and labels to the input features. So depending on your flexibility of where you want to use Fairlearn, you have options.  All right. Well, can we see a demo?  Absolutely, let me switch to my demo. Great. So as I said, this toolkit is absolutely open source so you can insulate with pip install Fairlearn. Right now I'm showcasing it on a Jupyter Notebook that I have put on hosted notebook VM in Azure Machine Learning. Seth, I'm super excited to announce that the Fairlearn is also being integrated within Azure Machine Learning shortly after the built. But today I'm going to teach that to the audience.  Fantastic.  First, it's very simple. You go through your normal machine learning training phase. In this particular case, I'm getting the census dataset which consists of almost 32,000 individuals, and the purpose of this model is to predict whether the annual income is above or below $50,000 per year. However, just for the sake of this demonstration, I would use that prediction signal as a loan approval versus a rejection decision. So imagine that this is a loan approval versus rejection scenario, and we would like to know whether it has treated different groups similarly. Now, one thing to mention is the focus of Fairlearn is also on group fairness, so it goes really well when you want to investigate the disparity and fairness insights across different demographics like different age groups, different genders, different races, and things like that. In this particular case, I get the data, I drop sex and race from it because I do not want to use them throughout my training, but I obviously need that for assessments because I want to make sure that my model has not used this protected attributes to mix its predictions. I would apply some feature transformations, then I would train my model on top of it. What's happening here is, once I train my model, which can be any model from traditional machine learning models like logistic regression, random forest, SVM, all the way to deep neural networks like Plato, Kairos, and TensorFlow. Then, I will pass that model predictions to my Fairlearn dashboard. So now you see the dashboard on the unmitigated model, around the original model, the way that I want to call it. As you can see, Seth, there are two configuration steps that you need to first go through. The very first one is you need to pick your sensitive feature, and the second one is you need to pick your performance metric, how you want to really measure the performance of your model. I would start with this, here I have passed two protected attributes to this dashboard. Let's say I move forward with sex, which is female and male in this particular dataset, and now I have my accuracy metric or performance metric that I can choose. Let's say I move forward with accuracy rate, which is the fraction of data points that are classified correctly. After I go through this configuration, I will land on a results page that will basically showcase the assessments of my model with respect to these two particular configurations that I've done.  As it's loading, there're a couple of questions, because I think it's important to note that when you were showing us the actual configuration of your training, you took out gender and ethnicity. But there might be other combinations of variables that would still generate a model that will be unfair to those attributes, even though you might have pulled them out.  Absolutely. So the challenge here Seth is, there are so many factors that can act as a proxy into a protected attribute. So if you think about one famous example that made it to the media a lot, which was recidivism prediction, it was understood that the model was basically assigning a higher rate of prediction of recommitting the crime to the black population compared to the white population. The interesting part was they even hadn't used race in their modeling phase. So what was used instead was neighborhood, which probably could leak some information about the race inside the model.  All right. Well let's go back to the dashboard.  When you land on this results page, you can see bunch of things based on the things that you had configured in the last two steps. First, you can see the overall accuracy rate of your model. In this case, the model is 83 percent accurate. However, you can also see the accuracy of your model across these two different sexes. For instance, the male group, for them, model was 79 percent accurate, and for the female group, the model was 92.4 percent accurate. So as you can see, there is a disparity in accuracy rate of 92 minus 79, which is 12.9 percent disparity in accuracy we're observing here. The other insight that you can see is how the error has been characterized across these two different sex groups. For the male group, there is 14 percent under prediction, which is the prediction was zero, but the truth was actually the person needed to get the loan. So in this case it's false negative, and that is higher than 5.6 percent, which is the under prediction for the female group. However, on the opposite side, the over prediction, which is false positive, where the prediction was granting the loan, however the granters were suggesting that the person is at a higher risk of not returning the loan. For those particular over prediction, you can see that males got more over prediction compared to females. So in a way, it characterizes where the errors had happened, and you can get a sense of how your model is treating different groups. On the other side, you can see disparity in selection rate. So when I talk about disparity in selection rates, Seth, I'm really talking about how the favorable outcome has been distributed. So when you look at the overall model, 17.9 percent of people have got disapproval on their loan application. But when you break it down across the two gender groups, or sex, in this case, male and female, you can see that the selection rate for the sex male is 22 percent, meaning 22 percent of my male sex have got the approval on their loan applications, and it is 7.55 percent approval rate on the female. So there's a disparity of 22 minus 7 percent, which is 15.3 percent disparity in selection rate. So at this point, if you're a bank and obviously you have the most context around what you can tolerate and what you cannot tolerate. This absolutely I cannot tolerate, this 15.3 is way above the threshold that I had set for my model treating different sexes. So you move forward to bring these mitigation algorithms from Fairlearn into your scenario to basically mitigate the fairness issue that you just observed. What I'm loading here, Seth, is one of those reduction methods that I explained to you, which as you can see, has got this learner or machine learning model that I just trained on top. Also, I have passed this constraint, which in this case is demographic parity, which talks about loan approval decision should be independent of protected attributes. So really, if you think about it, it's trying to address the situation that loan approval should be independent of sexes. So now it gave me a new algorithm wrapper, and I can fit that on my training data. After I do that, it basically does reweighting and relabeling of my input data and creates bunch of different training models. At the very end of the day, I can choose to get rid of non-dominated models where there is not enough difference in accuracy or performance metrics or fairness metrics, and I can just keep the ones that are dominant.  So when you're looking at this grid search, this is the part of mitigation where you're actually augmenting the model on top of it, or you're actually training a new model with the new mitigation strategy?  This is a very great point. It's basically retraining the model. You keep the learners. So as you can see, I still pass the logistic regression to the grid search. So I kept the ML estimator that I wanted to use on top which was the logistic regression. In this case, I'm just putting a wrapper on it on and I'm passing this fairness constraint. It takes the input data to this model or to this learner and it adds different weights and possibly sometimes relabels that as well very intelligently in a way that it reduces the fairness of that model with respect to the fairness constraint that I have just passed.  Got it.  So in a way it's really re-learning, but it keeps that ML estimator in its heart.  Awesome. Well, let's keep going.  Awesome. So as I mentioned, basically, I got rid of the non-dominated models. Now I'm loading this dashboard again. So that's familiar to you. That's the exact same dashboard. I pick the same figure configurations, sex and accuracy. Now, instead of landing on the results page, I'm landing on this model comparison chart. Let's see what it is. So when you think about it, our original model, if you remember, was almost 80 something percent accurate. Let me double-check again just to make sure. So my original model was 83 percent accurate and it had the disparity of 15 percent. So it was somewhere here. So my original model was here. But now what I can see, these are all these different mitigated models that the grid search has generated. What you can say as a stakeholder, as a data scientist on this project is, "Okay, I am fine losing the accuracy by almost three percent because this model has almost 81 percent accuracy. However, I'm saving a lot on disparity by almost six or seven percent or even more. Or even further, I'm okay losing the accuracy by maybe five, six percent, however, I'm really saving a lot on this disparity in selection rates." So let's see if this is a model that I would like to move forward with. Now, I clicked on this mitigated model. I can see that sure, I have lost a little bit of accuracy, 83 compared to 77, which is almost six percent. But when I look at this disparity in selection rate set, you can observe that from 15 percent it was dropped to less than one percent. So me as a stakeholder I might say that, "Okay, this is something that I can tolerate losing the accuracy a little bit, however, what I really care about is treating these similarly situated people the same, and not decide based on gender or sex in this case. So basically I would say that this is the model that I would like to move forward. If I do want it, then I go back to the model comparison then I keep clicking on the other options that grid search has provided to me. Eventually, I will pick a model that best fits my needs and then I'll move forward with deployment.  Well, this is pretty awesome, primarily, because it looks like it's generating a lot of models. As a stakeholder, you are just basically given fairness facts that you can use to do trade-offs with accuracy and fairness. Is that accurate?  Absolutely. The cool part about this set is, if you don't do it via Fairlearn mitigation algorithm, basically you have to go through multiple rounds to train each of these and you don't have a good direction of, okay, how should I move forward to reduce on fairness that I had observed in my assessment stage. But this particular technique, what it does, it smartly wants to minimize the classification error but subject to this fairness constraint that I have passed to my grid search. So in a way, it takes it away from you to go through multiples in terms of coming up with different options and investigating and just provides all of these for you on one chart on the model comparison and you happily and freely can choose which one is more appropriate for your use case.  The reason why I like this is because it's not making any judgments about anything, it's just giving the information and then you can make the judgment as an organization around what's important in your organization and what's fair for you and whatever output you are doing.  Absolutely, Seth. That's what we wanted to shoot for because, obviously, fairness is a very context-driven concept. So we cannot really provide a magical tool that addresses everything automatically. We want the stakeholder to have the full control on this tool kit choosing what basically fits his or her needs and not make any judgment or any decision the way that you described it. Now, another thing that, Seth, I wanted to call out is, I'm very excited to showcase that to you that, basically, Fairlearn has been integrated with Azure Machine Learning as well. This disintegration will be released shortly after build. So basically, the exact same flow that you just saw me walk you through, you can either go to models if you have any registered models within Azure Machine Learning run history. You can go on the Models tab, click on that model, and click on the "Fairness" tab and look at the dashboard, or in this case, I just went through an experiment, I click on this experiment and what I can do is, I can go through the exact same flow, see my multi-model comparison chart and land on any of the models that I want. In this case, I did it for race. I can see, for instance, what are the disparities there? What are the accuracies there? Then you can go back and forth between your Notebook and Azure Machine Learning Studio to keep improving your model with respect to fairness insights and then land on something that you feel comfortable moving forward and deploy.  Well, I'm a huge fan of this. I've always been a proponent of understanding models a little bit better by making them more fair. I love that this doesn't take a philosophical approach, instead it takes a scientific approach to understanding what models are doing and then mitigating any disparities that might exist.  Absolutely. We also observe that a lot throughout our conversations with our awesome clients and customers, they really care about debugging, verifying, basically understanding whether their model are aligned with compliance requirements and purposes. So the combination of Fairlearn and interpret ML, which is another open source tool kit that we have and has been also integrated with Azure ML, can provide a lot of understanding, a lot of debugging capabilities, and just allow you to build trust with these black-box models. Otherwise, it is very difficult to understand whether it is a good model or not, whether I should move forward with it or not. So hopefully these are there for you to get a full trust, hopefully, and understand your worlds better.  Well, fantastic. Thank you so much for spending some time with us. Thank you so much for watching. You've been learning all about how to build more inclusive Machine Learning with Fairlearn. Hopefully, you learned a lot. Make sure you use it today if you can. We'll see you next time. Take care. [MUSIC] 