 buddy before we get started I have a question for you so a little bit of participation on your behalf how many of you have already been thinking about responsible AI or fear a I just show hands awesome I'm so happy to hear that I I was not even this was not on my radar twelve months ago and a very smart and good well good friend and smart colleague Lena Bengali suggested that we look a little more closely into AI standards and tools and part of the impetus for that was a White House request to the National Institute for Standards and tools for them to engage with the bigger community and consider what tools and standards they should develop for artificial intelligence she suggested this and my first thought was oh my goodness it's gonna be this is just big and hairy and a lightning rod and I don't want to do this but but her instincts are always so good about what's important they she got the team to really do a little bit of research and consider what can we do as good corporate and global citizens to try to help the community guide AI a little bit better and so in doing this research I came across a lot of information sorted through a lot of information and actually some old stories that came to light I so there's a thread of some thinking I think that goes back quite some time so many years ago this picture is actually from a 1960 so this is the US nuclear command center and they as you would imagine had and still have very advanced well engineering early detection of threat so they have a lot of systems out there in the world you try to detect if there's a possible issue and interestingly October 5th 1960 they believed that we were under a massive Soviet nuclear attack that something was coming and what's interesting is at this point in 1960 is it with the technology they believed that we had about 30 minutes until something could make its way from the early detection to the US and with a little bit of analysis that the president would have about 10 minutes to make a decision and you can imagine with any kind of a system that was this serious and this well engineered they also were looking at well what's our accuracy so they wanted to know what the accuracy what's the predictions and interesting on this one is their prediction their system said we're certain with 99.9 percent assurity that there's an attack coming we went into DEFCON 1 they started doing all sorts of analysis looking at the systems and there was one gentleman who did not look at the system itself but said what's going on right now in world politics where are things at and he happened to notice that Khrushchev was in the US at this point he was part of the UN delegation and so this the likelihood of attack just by human reasoning was next to nothing so they were able to walk back from DEFCON 1 and prevent something catastrophic from happening based on what their systems that they had worked so hard on and we're actually really well engineered had indicated what was going on now any history buffs out there that want to take a guess is what early warning system was actually detecting I suppose in Greenland is where the system was doing the detection oh it's absolutely yes then moon was rising over Norway and in Greenland it looked like a massive attack so yeah and thank goodness somebody had the human oversight and the thought to say hey wait a second let's look at what else was going on and this is an example of trying to marry technology and human oversight and why that's so important but it also the reason why this story came to mind as I was doing research on artificial intelligence and being more responsible in my mind I was leaking there were some linking to some things that I was seeing out there and so there's this belief in the magical technology that's going to solve everything I think Lance was basically gave us a couple examples on that whole silver bullet thinking and the trough of disillusionment the hangover afterwards but we're seeing some of the same stuff so it's very human for us to want to see this new technology and say this is going to solve all my problems and a little bit of a few of the myths that I'm seeing that I think are actually impacting the way we implement AI today is you know new is better if it's shiny I'm really excited I want to do news articles because it's different I want to fund that I want the classes are gonna be in this new technology and I may be overlooking things that currently exist or could be developed that that may even be better the other thing that I find really interesting is there's this idea that there's an unavoidable trade-off between accuracy and fill-in-the-blank now usually that is accuracy and interpretability or accuracy and privacy but that's actually been shown that that is not unavoidable might be a little harder it might not be in every case but it's not unavoidable if you carefully select and craft your models that you can have both accuracy and either interpret II or privacy the other myth that's interesting is all he needs more data and I don't know how many times I've heard from data scientists if I just had more data I could be accurate and things would be perfect but if you think about things like bias and data if I'm just adding more biased data that I'm not really fixing anything I still have a problem so it's not just about volume it's also about quality of data as well and the fourth myth that I see impacting us now is the idea that we can take something that was developed for let's say a movie recommendation very important but if I'm recommending movies to somebody I add I have a five percent error rate that okay I can maybe I can live with that and maybe because I have cost sensitive issues that's okay but if I'm to predict patient care or patient outcomes 5% may not be okay and I also want to know why a decision was made I really don't care why a decision was made on a movie or maybe I care less I'm so these kind of I would almost say magical thinking about AI is causing us to see a lot of uncomfortable results one example that I think we're probably most of us are probably familiar with is this idea of biased AI the first example here is about bias and data itself so I don't know how easy it is to read but the blue is male red is female in technical roles at some high tech companies have got Apple Facebook Google Microsoft and you can see excuse to to male right now well Amazon had a recruiting tool that they shut down after they realized it was bias against women they hadn't intended for that they actually would like to see more balanced but they were basing their AI recruiting based on what their current employees looked like now they weren't looking at names so they didn't explicitly know a gender but it turns out when you look at LinkedIn profiles and resumes of men and women men and women have a tendency to choose different sports we have a tendency to choose different social activities or clubs and also interesting is we choose different adjectives to describe our accomplishments so the AI was picking up on these differences and trying to find recruits that matched what they saw was successful internally and so bias and data unintentionally can leak into our results on the other example here this is some photos from this gender shade project is on facial recognition and I would say recognition in general is kind of a tricky area but there's been a lot of calls for regulation on the use of facial recognition after this project and some other stories have come out where they looked at how good is face recognition depending on your gender and the shape your skin color shade and what they found out is if you're a white male 1% error they could see you recognize you file you know not bad it's actually pretty good if you were a darker-skinned female on the other side your error rates were in 35 percent area as low as that now if it's just a game that you're playing okay maybe not such a big deal but consider the use of this and it is used in security facial recognition is used in policing in certain areas and so the potential impact is significant and that's why even Amazon is asking I think this articles from July is asking the u.s. to consider the US federal system to consider regulating facial recognition software as well because there's bias and data potentially or bias and algorithms in the the second case so beyond bias there's also another area that I hadn't thought a lot about but is kind of concerning is this unknowable AI so a lot of us have heard about the idea of black box it's very hard to explain what happens but if it's accurate who cares but there's risk assessment software called compass it's used multiple cities in the US and Canada that does risk assessments and in this particular example Glenn Rodriguez who's in the picture here was sentence after participating in a robbery when he was 16 he spent 10 years being a model inmate he did a lot of volunteer programs he went through and tried to mentor other people on how to do better and when it came time for parole or and to go before the hearing everybody including himself and his lawyers and even the parole board was in favor initially of granting him parole but they denied it and they said it was because the risk score that the compass office gave the compass software gave him was a high risk score they were surprised but they weren't willing to go against the software and you can imagine his you know your of course he was disappointed in the earlier said okay we'll tell us how this assessment happened now tell me how you came up with this score there's over a hundred different factors that go into how it assess risk and lots of different weights and it was deemed protected because of commercial proprietary nasir that with him so you can imagine if you're denied something insurance credit parole and to not be able not even have the rights to look at how that score came up he eventually after being denied twice on a third subsequent parole he eventually did get parole they found looking into the data and comparing his scores and answers to questions to other people's scores but there was one question in particular that was subjective that would have changed his risk score significantly and they suspect that's what it was but they can't know so when I think about a knowable AI there's two different types or any kind of unknowable software there's the type where it's just really complex and it's hard for us to understand and then there's a type that we're not allowed to look at and both of those can be a problem depending on the situation so the third type of issue with AI that I think we need to think about is inappropriate AI and this is where AI is working exactly like it was supposed to but maybe shouldn't so Lance said this morning just because we can doesn't mean we should that is what this is about so the Chinese have implemented a social credit system that will credit both your financial or your financial credit worthiness is also your social worthiness and it takes into account things you might consider like were you late paying a bill but also things that you wouldn't at least in this country normally include that our behavioral did you jaywalk and they can use facial recognition to see and track a few jaywalk let's hope their facial recognition is good they can do the same if you smoke in a non-smoking area that can affect your social score and that can impact everything from whether you're going to mortgage what your rent cost to whether you can get high-speed Internet so a lot of ramifications to your right rights and they believe by next year there'll be 1.4 billion people the have a score so this for me is an example of Ken but perhaps shouldn't I don't believe we should so these three areas for me really the idea of bias and data and algorithms unknowable AI and maybe inappropriate in AI is really kind of set it in my obviously my bones that we have a responsibility as creators of artificial intelligence and the systems that rely on them to guide how they're implemented and developed in a way that fits with our social values now that's different for every country and different cultures and where it's implemented I think in this country for me I think about things like accountability which requires some sort of transparency I think about fairness which is a lot about appropriateness is this decision appropriate and I think about public trust which has two components is the system operating as intended and is it operating as the users expect it to be so as expected by the end users so I'd like to do a double click into why AI in context together makes sense but first to just clarify when I talk about AI and AI systems I'm talking about the what so the processes that have been developed to have some kind of action similar to a a human does and that is very probabilistic so you think about how you and I make a decision I'm never a hundred percent sure usually I'm hopefully a it's probable this is a good solution that's or this a good decision that's why I make it but it's very probabilistic we never know for sure machine learning is the how of it so we have what and then we have the how so that's really about the algorithms that are trying to iterate to optimize a solution based on a set of training or examples it's been given but I don't have to tell it how to do that now that actually takes quite a bit of data and it's really not surprising because think about how we make decisions and we tens of thousands of decisions every day you do that by looking around your surrounding circumstances you grab information probably you don't even know what you're taking in you mentally make connections and then you try to make the best decision you can with the context and the information you have at the time and then you move on it shouldn't be any different for our ai ai requires that same type of information that context and that connections so it can learn based on the context it can make adjustments especially as circumstances change because they always change don't they and graphs are actually the best fastest most reliable way to add context to whatever you're doing and so it's a natural fit and AI without context is really limited and I I love this example because it always makes me laugh this is four little words we saw her duck you can interpret this many different ways what does that really mean does it mean I ducked hopefully nobody threw anything had me but yet maybe I I duck we saw her duck it also could mean that we saw her pet bird it might mean somebody named we saw her pet bird or maybe even maybe even we went over to dinner and we were gonna have duck and we saw her duck okay that's not so pleasant there's probably some other ones as well but so without this context AI is very limited it's nearly focused on exactly what we've trained it to and only that data you have subpar predictions this morning Lance talked about James Fowler's work that showed that relationships are highly predictive of future behavior if you don't include those you're losing out on what could be very good predictive indicators and you have less transparency which of course if we can't explain how a decision was made then you you can't hold people accountable and you're not going to trust how decisions were made either so it's probably not surprising why searchers are also using graphs to enhance their AI to push beyond the limits this is a chart 20 of starting in 2010 to 2018 looking at how many AI research papers use graphs as part of their research it was under a thousand in 2010 by 2018 were just under 4,000 so a four-fold increase in in eight years and so we're seeing that accelerate significantly as well and so if we think about graphs adding context and context for AI why is it that we naturally and I do this all the time I almost talked about grasses if they were context and and that's because graphs were built to understand relationships that's how graph theory started they were built to understand relationships and they were built with relationships and that's significant because in nature you don't get any unrelated isolated data points that was said earlier today that you know news stories everything starts to look like a network or a graph when you start looking at it and that's because the nature data is very connected and grass helped us add in that fabric of connections in the data we have so we can actually hook it together and it enriches that data to add more value and if we think about the Stars lots of individual pretty points of light very nice to look at but that individual star doesn't mean a whole lot in and of itself I can't do much with the other than admire it but when I start to look at it in context of other stars and where it is in the night sky I now have a constellation I can now navigate or at least get in a general direction this is not quite the Big Dipper but let's pretend and say that generally that's north and that of course was how how we used to navigate quite a long time ago and if we wanted to add more context and more information we then start to have maps how do I get from point A to B how many routes are there to point A to B what's the most efficient manner for me to do that and then if I add in more contacts like traffic how do i reroute during different times a day when the traffic's bad and then if I add even more contacts we can we can see things like what lyft is doing to innovate that you're now able to add more information and layer on more context and add more value I'm always amazed that I get into like I did last night into my lyft ride share at the airport you know and I get to the hotel and it's coordinated between myself and some other writer it's picked us up both up efficiently it's been efficient for the driver for us a pleasant experience and everything happens seamlessly and so the more you can add context the the more you can add value to the data and what you're already doing and thinking about graphs as context neo4j invented the property graph model in 2002 based on a sketch on the back of a napkin I don't know if anybody still has that napkin that maybe that'll be my next my next mission to find that napkin but the I'm always amazed that such a simple elegant dots and lines can it have so much flexibility and allow us to innovate on it we've got nodes with properties and relationships with attributes extremely flexible and has allowed us to scale to millions to be able to process millions of data points and connections per second and analyze billions of nodes and relationships so why would we put the context of grass with responsible AI together how do these things fit there are really four different areas that I believe that are really important and they fall into two different larger buckets and I've got examples on each one of these four areas one is one bucket is around robustness and having solutions that are more predictably accurate and having solutions that are more flexible and the other word area about responsibly I think's important is trustworthiness and how do we enhance and increase the fairness and the explained ability as well with our models so the first example is a rather serious example this these numbers are a few years old the Oh with the opioid crisis it was estimated several years ago that in the insurance fraud area that there was 72 billion dollars a year in opioid insurance fraud more recent estimates not looking at the insurance cost but looking at healthcare legal child care support care that they were at we believe we are at and this is just the u.s. a hundred billion a year in cost so a pretty significant issue and there's they believe that fraud is an area that we can target because we have the data we know where the money flows and there's actually been some really interesting work that's been done to try to do fraud prediction on opioids and looking at that insurance information looking into the relationships between doctors pharmacies and patients and looking at how tight are those communities using graph algorithms to do community detection there's certain I was talking to somebody earlier about fraud and their fraud has shape to it and graphs are really good at understanding the topology of your data and you can use things like relationships and community detection to then extract out different communities and then use your machine learning as they did in this research paper to estimate which structures of communities were more predictive of fraud there's actually some really interesting research going on in this area and using graphs to do graph feature engineering has a tendency to improve your model accuracy you don't have to change what you're doing in the models in fact you shouldn't change your normal machine learning models you should just add these predictive features to it and kind of increase the accuracy you have with the data you already have there's also well one of my colleagues Marc Quinn's Lynne is doing some research is just starting to come out and doing some talks on more recent data and looking at opioid fraud as well so if you're interested in this area come find me later and I'll start to to filter out that in for me as it comes out because there's some interesting work that some of our colleagues are looking at here so beyond accuracy another example is having confidence and things that are fairly serious driverless cars is one of those areas that I sometimes get very nervous about especially when you hear the stories about cars being easily tricked by stickers one of the recent ones was the Tesla car that changed lanes based on stickers on the on the in the lanes makes us very apprehensive but I think it's really not just about cars but it's about anything important that has autonomous decisions or semi autonomous decisions a situational awareness is very important in these kind of circumstances especially if they are implemented in such a wide area at being able to be flexible being able to learn based on context not just on a specific static data point is really important and being able to incorporate adjacent information which is what graphs are all about right it's not about the first hop or the fourth hop maybe it's about the eighth hop out out of curiosity does anybody know what the number one cause for accidents are in regards to semi autonomous or autonomous vehicles like I can't actually know it's being slammed in from behind by a human driver so so it's not that human drivers aren't smart in these cases it's that the semi autonomous cars don't act like we expect them to in particular they have a hard problem with birds on the road so you and I you're on highway flock of birds you just go you know the birds get out of the way hopefully most of them will get out of the way and we may choose to brake for a small animal in the road nobody else's around you know it's just safe go ahead and do it but if you're in traffic and it's raining and maybe it's a little dark who knows and the little bunny rabbit comes out maybe you don't brake maybe it's safer to keep going feel bad for the money rabbit but the autonomous cars don't deal well with birds plastic bags flittering in the road a boot in one case so dealing with the situation more comprehensively being flexible to different situations is going to be essential for us to take AI so that it can be flexible to different circumstances another area is about humans behaving badly humans trying to subvert systems this example is a in from financial services where we had some very smart criminals trying to misrepresent data in one area to fly under radar that was be reporting in another area because the data filtered through and so they were smart enough to know if I if I tweak information over here it'll change results you know on the back end and you know Financial Services that's very important I also think about this in other situations in other scenarios I want to be able to rely on my AI systems and what come comes out of them and I can have the best system in the world but if my data has been manipulated how do I rely on that what if I'm what if I'm managing an energy grid or maybe more relevant for many of us at the moment what if we're talking about voting systems I need to know where the data has been I need to know who's touched it I need to know when it was changed I need to know what the chain of relationships are and how that data may be used somewhere else and that's a classic graph data lineage problem it's not sexy but it is so essential to be able to trust your systems that you can trust the data not just was it good data maybe it's not biased ad maybe it's fantastic data but if somebody tweaked that data I no longer can trust the outcome and graphs are really good allowing us to track the chain of data change and the ripple effects for it so another going back I guess a bit to this idea of amplifying bias this again is I'm going to show you example of skewed based on either practices or bias data we're not really sure this is an example that again is based on the compass software example has Vernon Honda left and Brisa on the right a risk assessment is also used in many cities when you get booked so says before you even are indicted or you don't have criminal charges they assess whether you're going to reoffending had two armed robbery charges and one attempted armed robbery Brisa had she borrowed a bike from her neighbor's yard and she had a couple of misdemeanors Vernon was caught shoplifting Vernon got this low score of three this was three out of ten Boresha got a score of eight on this breach I never offended again Vernon went on to commit grand theft so this compass software and I won't go into more stories publicly I actually did quite a bit of work on it they made the data available but it's consistently been inaccurate and it is black box and that we're not allowed to look at how it's looking at information and it consistently gave african-americans higher risk scores then white Americans and it's still in use today so how how can we approach this well one is of course to make this something that we're all aware of but the other way that you can use graphs in particular because this is a graph talk on how to approach some of this is to just reveal the bias in the first place and part of that is knowing where your data came from who collected it how they collected it when do they collect it there's all these chain of events in just data collecting and how it's used the if we better understand that that actually is a very much a graph problem as well that we can help to reveal bias that may not be apparent in our data right away a graph so graphs can add this context and then as I explained in the opioid example you could also use graphs to add different predictive features that might be based on relationships as opposed to demographics or like where your zip code may happen to be so my last example is not an AI example but I think it's important I think the 737 max issues involving not incorporating or being impact by not incorporating the human element and pilot behavior is a really good note for us to take on why being human centric when we talk about AI because of the reach is so important so a lot of our AI solutions look at very honestly idealize circumstances and maybe it's not idealized maybe it's just you know if you're a developer and you're coding to what you were told to code for and the personas that you were told to code for and you've done that you know how do you know that you're incorporating all the possibilities there and humans tend to act in unusual ways so it's hard to predict and one of the things that graphs can at least help in this regards is grass this is a little white board model kind of hard to see but it's basically circles and lines on a white board that these are the way we think about systems these are the way we think about the world so you can use this as a tool and talk to users incorporate human behavior incorporate your expert advice to say here's what we're doing with our data should we is this the right thing to do and graphs have that ability to just more naturally be inclusive of kind of that human point of view so what's what's next what's coming in graphs and AI in the future well the first one shouldn't really be a surprise to you guys by now we're already seeing human values be implemented and driving change in AI regulation and I think that when this starts to accelerate it's also going to accelerate the adoption of AI itself because AI is not going to be widely adopted until we feel that is trustworthy and it's aligned to our value so it behooves us this if we want to accelerate adoption that we consider the human values in the society in which its implemented and the other thing is that we are already seeing a lot of tractions and graph and data science machine learning and AI and a number of different use cases I've just thrown up a few drug discovery I was talking to somebody about that or her earlier of financial crimes fraud anti money laundering of recommendations especially in retail we see a lot there but things like turn prediction and subscription service as well cyber security predictive maintenance so lots of different use cases but I think what's going to change in the future is that that's just going to be standard if you have an AI system there's going to be some kind of graph component somewhere in fact about a month ago I was at a conference in Anchorage and I had an interesting chat with a gentleman an AI researcher from a large well-known software company and asked him what he saw as the evolution or coevolution of graphs and AI and he said in the very near future any large enterprise AI system will have some graph component in it and part of that is bringing in context and the power to remove some of the limits of AI and in fact google deepmind's as well as some other researchers published this paper late last year looking at what's the next frontier for AI and it was graph made of learning and they were looking for what are the short comes of AI today and what's it gonna take and machine learning today and what's it gonna take for us to overcome them in the future and they felt that graph networks were the bigger we're a bigger idea than any one other machine learning area and the reason was a graphs ability to abstract and generalize on structure so that is one of the magic things of these you know simple dots and lines is you can abstract a lot about the topology and the structure of your data and what's really going on and you can represent relationships and generalize to other areas which is an area that the AI really needs to expand into it's a difficult paper but what the heck does graph Native learning me I won't pretend to understand it completely but when I think about it I think of it as a way to implement machine learning inside of a graph in a graph structure itself and the idea is the users can add in data as a graph as connected data you can do your learning and preserve each transient state and still have the graph preserved and then when you come out with your solution it's still a graph so graph in graph out preserve the transient States now this sounds simple difficult paper sounds simple but what's so amazing about that is that's going to allow experts to be able to track and validate the decision path of some of our AI solutions a big problem with the the black box problems we had today so it's going to allow us to track that a little better and it's also going to allow us to be more accurate with less data and to be able to bubble up some of the more important features so just trying to figure out what's predictive in your data can be a big challenge you spend a lot of time on it and still be wrong we believe that this approach will help bubble up what what those significant features are and what that really means for the rest of us is that we're going to be able to go from these rigid narrowly focused black box systems and go to models that are more flexible that are more accurate and are more transparent so this is years out this is just really exciting research we think it's a couple years out we're watching it very closely I think this is going to change the way a lot of us run AI in the future but really early but very exciting work and if you're interested in the paper come find me and I'll I'll make sure you can you can find it the other thing that I want to bring up is we've been talking a lot about kind of these big ideas very conceptual but for those of you in the audience that code or manage people that code I realize this can seem really daunting but there are some very practical things we can do today to be more responsible about AI and I'll make sure that the the slides are distributed cuz I realize that's a little small on some of these and so there are things we can do today that aren't really that difficult to get started if you are just starting the first thing I would say is please know what's in your data and track your data very simple that's table stakes hopefully you use neo4j to do that but graphs are very good for do that to do that but regardless you need to know what's in your data and you should be tracking it you can do things like D biasing your data there are toolkits out there this is one example the a I ferret is 360 toolkit they can help balance your data and see if there is imbalance in it you can do things like involve experts please involve your experts your data doesn't necessarily speak for what is most predictive or what success looks like so involving those expertise is important and then of course there are resources for developers like the algorithmic justice league where you can get more tools to understand how you can do things like more fair fair algorithms I'll try not to go through all of these in details but some of the other things you can do especially in the graph areas like add those predictive relationships we talked a little bit about graphed feature engineering there's some more information on our the neo4j website on that in particular you can also if you're doing things like behavior analysis you can you can look for counterfactuals as well if you're looking for causality so you can just use neo4j for a search to look for what you might not be predicting but things like model exchanges using interpretable models where you can so if you have a blackbox model that's very accurate and you have an interpretable model that's very accurate please use the interpretive model use the model that you can explain unless you don't have such a model but the other things you can do if you're in the process of implementing or about to implement you can add context as like a knowledge graph so think about a chat bot you can add knowledge graph to a chat bot to make it more intelligence it sends people to the right department in your store you can do things actually another table stakes that you should be doing is some sort of risk assessment if you're not doing a risk assessment today you need to you need to look at how what your model performing poorly or badly is going to do so you could be something as simple as a checklist I've talked to teams that it's just the developers that created some checklists at least that's something and I've also talked to companies that are doing full-on committees to review a model before you have to be in place and this is very domain-specific I wish I could point you to one perfect example I will say if you're out there searching the insurance companies and the insurance industry seems to be a little bit ahead of the other industries in risk assessment maybe because it's the business thing they've been in but doing things to develop the checklist and the full some of the committees as well to look at that final thing is think about explanations if you have high-stakes decisions and insist on something that's explainable so final three thoughts on the future of AI the first one is it's not all about machine learning I do this I know other people do this but context structure reasoning are really important for us to improve AI graphs connected data are just a key element that allows us to do that and that's why we're seeing the research involving graph go up significantly and the other thing is and I love this quote from Vivian being if you're not thinking about the human problem AI is not going to solve it for you we cannot AI our way out of all of our problems and in particular if we're codifying our own human flaws we've got to deal with the human flaws as well at the same time or we're just going to amplify and the danger of that an AI is the power and the reach is so significant I would probably say more significant than has ever been so final quote here I'm gonna tweak just a little bit this is from my so I would say our future is really whatever we make it and what we decide to make it and so why not make a choice to make it as good as possible so if you want to hear more about this topic or graphs in general three-day conference April New York amazing conference good individuals and and really the largest graph conference in in the world also just so you know later on I think during the next break I will be out in the foyer a with some of the graph algorithms books I'm one of the co-authors and we have a few extra copies that I'll be out there and ready to sign so with that thank you everybody for your attention I really appreciate it [Applause] 