 In the next 10 minutes, we'll talk about AI at scale, what it is, and why it matters. I have three announcement for AI developers and a demo of AI selling a long standing dispute from a Star Wars. As you understand more than most, the explosion of Internet data and large amounts of compute from the Cloud have pushed a fast progress of AI. Microsoft has contributed to this progress by advancing the state of the art in areas like speech recognition, computer vision, and natural language understanding. We have made many of these capabilities available to everyone as Azure Cognitive Services. Much of these has built unsupervised learning, where we train AI models from a scratch to perform a task by using large amounts of labeled data. Because of this dependency, learning is constrained by human effort. Over the past year, we have seen the emergence of a new training approach called self-supervised learning, where AI models can learn from large amounts of unlabeled data. For example, models can learn about language by reading large volumes of text and predicting words and sentences as they do so. No labels are needed because the labels are in the data itself, the words and the sentences being predicted. As the model of these billions of times, it gets really, really good at understanding how words relate to each other on different contexts. These results in the model acquiring a very good understanding of the language. The more data that is used for learning, the richer the understanding. Thankfully, in today's world, there is an immense amount of web data that we can crawl and curate to being. The language understanding encoded in the model can then be fine tuned to support a large amount of language tasks such as sentiment analysis, search, question answering, or summarization. All these through a process called transfer learning. This removes the need to train task at specific models from a scratch, and increases the accuracy of tasks while dramatically reducing the need for labeled data. Now, besides data, the size of a model determines his ability to learn and encode the complexity of language. The model size is described by its number of parameters. Roughly the number of connection in this neural network. The more parameters that a model has, the better it can capture the difficult nuances of language. A year ago, the largest AI models in the world had around one billion parameters. Or Turing NLG, Natural Language Generation model, the largest today, has 17 billion. A rough analogies that we have gone from third grade reading a comprehension level to a high school level. Let's look at it in action. Let's start by asking the model a couple of random questions. The model has learned facts, concepts, and grammar from large amounts of data, including all the Wikipedia, tens of millions of web pages, and thousands of books. When did we first land on the moon? Notice that the model is not doing a query against a database or an index, but instead understanding the semantic meaning of the question, relating it to the world of information that he has learned and synthesizing a grammatically correct sentence. What are the Enterprise's main weapons in Star Trek? Those are phasers and photon torpedoes. Nice. Let's now copy and paste a document that the model has never seen before, like this memo from Satya about the Microsoft response to COVID-19. Let's ask the model some questions about it. How many messages does the health care bot respond to? You can find the answer here, one million messages per day, great. What was a first for the University of Bologna? With no semantic understanding of the content, you can find the answer here. They move 90 percent of its courses for 80,000 students online to Teams in three days, which is awesome. How are PowerApps and PowerBI being used? They're being used to manage bed count and inventory critical supplies, and sharing that information with others across the region, amazing. How are scientists using GitHub? To power distributed computing project to assist researchers developing potential therapeutics using volunteer computers, which is fantastic. Before we move on, as I promised in the beginning, let's finally settled the longest standing Star Wars debate, of who shot first in the Mos Eisley Cantina by copying the Wikipedia article about the relevant scene and asking the model, who shot first? There you go, Solo shot first. If you watch the video again, notice that the response is not found explicitly in the article. Once again, the model has understood the content and synthesize the appropriate answer. Now, let's ask the model to do something different, to summarize content. Let's start with something that most are familiar with, the story of Romeo and Juliet, by copying it from Wikipedia and asking the model to generate the summary. As the model use a text, it abstracts the most important ideas from it and a stitches them together into a cohesive summary, tweaking it as it makes progress. See how the completed summary contains the main points. Romeo and Juliet fall in love. Juliet's parents forced her to marry Paris. Romeo poisons himself, and Juliet stabs herself. He discards other points less relevant, like DeWalt Colleen Mercutio. Let's now summarize a complete long article about Microsoft's commitment to be carbon negative by 2030. Once again, as the model makes progress on the article, it abstracts the most important ideas into the summary, reassessing the relevant priority, and tweaking the summary accordingly while maintaining cohesiveness. Given the length of the article and the importance of multiple aspects, this is going to be a much richer summary. We referred to the current trend of increasingly larger AI models and their ability to power a large number of tasks as AI at scale. We believe that is trembled continuing to the hundreds of billions and eventually trillions of parameters. Training models with tens to hundreds of millions of parameters requires big clusters of hundreds of machines with a specialized AI accelerators interconnected by high bandwidth networks inside and across the machines. Azure provide with such clusters, for example, with the latest Andy v2 VM Series with AB 100 GPUs connected by NVLink and InfiniBand. Training models at this scale requires the ability to distribute an optimized training of the model with large amounts of data across all of the AI accelerators in the cluster. Models are so large that their training requires, on one hand, is pleading the neural network layers across multiple machines, and on the other, partitioning the huge training dataset into batches to simultaneously train multiple instances of the model across the cluster while combining the learning. Over the past year, we have developed a state-of-the-art optimization techniques to make these processes significantly more efficient by reducing redundancy in the state stored and replicated across the GPUs. We recently opened sourced  these techniques into our Python library called Deep Speed to enable everyone to train AI models 10 times bigger and five times faster on the same infrastructure. We have continued innovating at a fast rate, and I'm happy to announce that today, we're open sourcing the second version of deep speed, which now enables you to train models up to 20 times bigger and 10 times faster. Besides deep speed, Microsoft Research has created various complementary state of the art trainee optimizations targeting different levels of the serverless stack between a framework like PyTorch or TensorFlow and the hardware. We wanted to make it easy for you to use all these training optimizations together with the framework and the hardware of your choice. I'm happy to announce that today, we're bringing together the optimizations from deep speed, as well as other libraries for Microsoft to the popular ONNX Runtime. This adds support for highly efficient training besides of inferencing to this framework agnostic and hardware agnostic AI runtime. AI at scale and the reuse of laterally AI models is transforming the way that we operate on Microsoft. You can see these already showing up in your products. For example, Bing Fine-tunes are Turing Language Model to support search and question answering over the web, and then Word does this over documents. Outlook uses it to suggest replies to e-mails, and the most relevant documents for a meeting. Dynamics uses it to suggest actions to a seller based on its interactions with a customer. However, language learn from web data is sometimes not enough to support a specific domain needs of our products. For example, the Productivity domain in Office, the Business domain in Dynamics, or the Professional domain in LinkedIn. Every organization uses its own distinct vocabulary that a language model must learn. To achieve this, we're training one personalized language model per organization by augmenting or base during model with the data generated within the organization itself. As with the base model, we use self supervised learning to do this without the need for labeled data. This allows to ultimate the learning process within each organization while ensuring privacy. AI at scale is improving not only your models understanding of language, but also their understanding of relationships between language and other types of data. We're training large self-supervised bottles jointly across texts and images to support tasks combining these, like searching visually for images with a text description in Bing, or automating the capturing of images for accessibility in Office. Similarly, we're training models across text and video, for example, to automate the creation of outlines with the sections identify in a video. Finally, we're experimenting with adding folder data dimensions into our training, like the layout in Word documents and in PowerPoint slides to automatically generate one from the other, or customer activity sequences in Dynamics to identify potential issues and opportunities. We think of our large-scale AI models and the systems that enable their use as a new software platform, one that enables faster innovation and better collaboration. We believe that this platform needs to benefit everyone. So I'm happy to announce that in the coming weeks, we'll open source of Turing Language Models together with recipes to use them in Azure Machine Learning. We're very excited to see what developers build with this. [MUSIC] 