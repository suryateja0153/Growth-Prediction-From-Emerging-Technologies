 earlier you saw a beautiful inspirational video about how machine learning and android were used to create an app to detect crop diseases so for devfest i wanted to get together a few of my friends from google and beyond to show you how you could get started in building something just like that from scratch in a few minutes we'll build an android app and a web app so to get started with android let's see what chat can teach us i want to create an app that's able to recognize information about plants it's going to need camera functionality as well as machine learning inference let's see what that looks like in code the app is written in kotlin and uses camera x to take the pictures and ml kit for on-device machine learning analysis the core is in take photo where we take a picture analyze it and display the results first we call take picture on a camera x image capture object that was created earlier one of the parameters is a callback object which has this on capture success function we get the received image into the format we need for ml kit then we create an image labeler object and process the image when this succeeds we receive a collection of image labels which we turn into text strings and display a toast with the results let's see what the demo looks like so we'll take a picture and it says i see an insect and a plant so that was pretty easy rigging up camera x and ml kit to detect arbitrary objects in the camera view but the results were pretty generic because the dataset didn't have enough information about our domain so let's dig a little deeper okay let's go deeper now we need a model for something very specific detecting diseases in being plants instead of cassava let's explore how to build it on this guide we'll use some of the great tensorflow tooling available let's start with colab you can understand collab as a cloud-hosted development tool we will do all our coding on it and you will not need to install anything on your machine let's start with a new notebook let's just send the title to beans we will need to install some packages that we are going to use later this package are not installed on your machine they are on a cloud machine that was created for your collab nice it finishes out the packages let's download the data and do some visualization to understand how our data is separated perfect we download the data let's take a look on some of the images so we can have a better understanding of what we are doing here here they are these are some of the images that will be used for training our module later now we have the data we need to create a model we are not going to create one from scratch we are going to use a technique called transfer learning data flow hub is a repository for tensorflow models you can find all kinds of models here let's start with this one let's go back to our collab let's define a model handle nice now we have the data and the base model how can we do transfer learning to do that we are going to use one of the tooling that i mentioned before called model maker model maker make your life way easier when you need to do transfer learning let's create the spec for our base module let's create our train variables here using the data set beings that we've just seen and now are going to put everything together with model maker by defining a model with the training data and the spec that we got from tensorflow hub this will take a couple of minutes it finished training and as you can see here our accuracy is at 87 percent of course let's evaluate the model with some data didn't see yet and see how good it is nice 95 the tensorflow lite model gus just created contains all the metadata android studio needs to recognize it and automatically build classes for it to get started you can update your bell.gradle file to include the following tensorflow lite dependencies then you'll want to import your generated tf light file into the ml folder of your project let's check out the details of our imported model from here we can see an example of how to use the model in our app let's move over to the main activity class to take advantage of it inside of our image capture callback here on line number 78 we create an instance of our model next we use it to process the captured image here on line number 84. and finally here on lines 92 through 98 we display the results of consuming the output inside of a toast message let's run our app now instead of telling us it's looking at a leaf or a plant it can actually tell us if it's looking at a bean leaf and give a diagnosis sweet so this concept works but it's very much a raw demo what if we want to make this a more successful app well we'd probably need to add services like authentication so our users can sign in analytics and a b testing so we can find out how our users are really interacting with our app some crash reporting or performance monitoring and an easy way to save our users data to the cloud luckily that's where firebase comes in now the new and improved firebase plugin in android studio makes this simple i'll start by adding some analytics so i can find out exactly how our users are interacting with our app and the plugin does most of the work to get the library integrated into my project now that i've done that well we can get an instance of the library up here and then we can log what kind of results we're getting from ml kit and then once we've done that there's a lot of ways to get at this data it'll start showing up here in the firebase dashboard but i find one really fun way of viewing this data is to use stream view which kind of gives you a real timey sample of what kinds of analytics results we're seeing looks like i've already recorded several of these select content events and i can dig into these event properties and see what kinds of results our users are getting and i could start using that information to maybe refine my ml kit model or a b test different alternatives firebase helps you build better apps and analytics is just the tip of the proverbial iceberg maybe we could let our users upload their own pictures and store them in the cloud using cloud storage for firebase there's so many possibilities this is a sample app but if we were to productize this it's important to keep in mind how our ai design decisions impact our users for instance we need to consider if and or how it makes sense to display confidence intervals to help your users interpret the ml model output or say how you design the onboarding experience sets user expectations for the capabilities and limitations of your ml-based app which is vital to app adoption and engagement for more guidance on ai design decisions check out the people plus ai guidebook at pair.withgoogle.com guidebook this use case focuses on plant diseases but for other use cases where our ml based predictions intersect with people or communities we absolutely need to think about responsible ai themes like privacy and fairness which you can learn more about at tensorflow.org resources slash responsible dash ai and don't forget about the web i built a pwa that can be installed across all your users platforms it combines the web camera with tensorflow.js and by integrating machine learning we can make an amazing experience that runs across all browsers now let's take a look we have our standard project layout with a html file a manifest and a service worker to make it a pwa we have some styles to make it look good and our data folder that contains our tensorflow configuration and trained model that we're going to use in the app now to the heart of the project let's go back to the html file and see what's happening we're also loading the webcam object this is just a class that wraps some boilerplate logic to make it easier to pass camera data from get user media to tensorflow and now let's dive into our app logic in index.js so i'm just going to use chrome and the debugger here and this is johnny so you can kind of see how easy it is to integrate machine learning into your application so let's get started by clicking the classify button and get the machine learning gears into action and immediately we break into the tensorflow tardy function this is just there to help you kind of clean up any of the memory that tensorflow uses whilst it makes a prediction we get our image from the web camera and then we pass our image back into the into the machine learning algorithm to make it a prediction then once we've gotten prediction we access the data and then we can use that data to update the user interface kind of based on any application logic that we want and that's pretty much it great so now you have the platform for building a real app with the tooling from android studio the apis from camerax jetpack ml kit colab tensorflow firebase chrome and google cloud you have a lot of things that just work better together this isn't a finished project by any means just a proof of concept for how a minimum viable product with a roadmap to completion can be put together using google's developer tools and apis you might also want to open source this project too so developers can suggest improvements optimizations or and even additional features by filing an issue or sending a pull request it's a great way to get your hard work in front of even more people we'd love to help you with this and you can learn more about the process at open source dot guide starting a project indeed we've already open sourced the bean disease sample we discussed in this video so you can have a great place to start thanks puja and as you mentioned open sourcing a project is a great way to make it grow and inspire people to adopt and extend it if you want to learn more about what you've seen in this video please visit us at developers.google.com 