 You're not going to want to miss this episode of the AI Show where we delve into some of the ethical concerns with AI, how to think about it well, and what you can do today to make your software ethical. If it uses AI, make sure you tune in. [MUSIC]  Hello and welcome to this special edition of the AI Show. We're going to talk a little bit about ethics. I have some special guests with me. Let's start with you, Josh, tell us who you are and what you do.  Hey, thanks for having me. I'm Josh Lovejoy. I lead design for a team called Ethics and Society at Microsoft. What our team does is a little bit like a design agency meets ethics. We partner with product teams like right in the mix of making actual products, and try to collaboratively answer the question, what does it mean to be responsible?  Awesome. Sarah.  Hey, I'm Sarah Bird, and I am one of those product teams that's trying to figure out how to be responsible. I lead responsible AI for the cognitive services, and we work closely with Ethics and Society to try to ensure responsible development and use of our products.  This is amazing, when it comes to machine learning and AI, they're used interchangeably. When it comes to machine learning specifically, when I think about ethics and machine learning, for some reason and maybe others have said this too, I think it's a lot broader, but I've heard a lot that the ethics of machine learning models lies primarily only in the data, and if your data is unethical or using data wrong. But I've heard ethics centered around data. I feel like that's a little short-sided. Josh, what do you say about that?  Yeah, I think you're right. Sometimes I relate it to a teaching metaphor, machine learning teaching. Saying that you're designing a system and all you have to do is make sure your data's right, is like saying, you're going to design a course to teach people stuff, and all you have to make sure is that you have all of your references in order. There's a lot more like, who are your students, and what are the subjects, and what do they know already, and where are they going to learn? All these different pieces that come to play a pretty significant role. So data is super important, don't get me wrong, but sometimes it misses a little bit of a bigger picture.  How do we frame this? Because I know when it comes to ethics, it's all about literally the questions I ask myself before I do something. I feel like there's got to be large set of questions that we ask ourselves when we build AI or machine learning systems. Josh.  Yeah, there's no shortage of questions. The first one is really just to your point, like when we talk about ethics, the first thing is what are you trying to build and for who? I might flip to some slides that I put together for this talk, would be cool?  Let's do it.  Sometimes we start in this really broad strokes picture about building technology. People will show up with a really high level goal, like speech recognition or computer vision or what have you. Speech recognition, fantastic, overarching, technical capability. Literally trying to turn the sound of someone's voice into typed text so you can do stuff with it. But then we end up with this interesting fiction question. Well, how do we make this work for people and who actually needs to use it? This is where, as you said Seth, there are a lot of questions. But also they're the road-map towards getting a stronger connection between the technology and set of capabilities, and a set of people, it's pretty unclear. What are the things that our team tries to do is take on work, working with folks like Sarah, when actually it's really unclear what the steps towards being responsible or building reliable technology looks like. If people have done it before, then it's a known known. But there are some things that are different about this space fundamentally that require us to really take this much more granular at times view of things. It starts by, I think really just respecting the work that we're used to doing and engineering. We've neglected a little bit with machine learning for some reason. One of them is just respecting and appreciating the types of constraints that we're working with. Physicists have all constraints that go into the work that they do, and electrical engineers have all constraints that go into the work that they do. But sometimes when we're building AI and machine learning, we're just like 200,000 cores that I want to run concurrently to build my nine billion layer model, whatever, let me do that. Then when it comes to actually fit into a production system, we have a lot of weird hacks that we have to employ. Well, the other one is accuracy because accuracy is different fundamentally in machine learning than it is in traditional deterministic logic systems. You can't just say, "Yeah, that's working as I intended it." Because actually the fundamental value of machine learning it's fuzzy, it's fuzzy logic, it's a hunch building, it's not truth building. The addition and I'll throw and read off the top is, well, you have to learn how you measure not only accuracy, but also inaccuracy. Another way of saying that from a human-centered perspective is, you have to understand the things about people that should contribute to a more accurate system. You also have to understand the things about people that shouldn't contribute to inaccuracies in the system. Let me get crazy for a second and then put them in a system screen.  Josh [inaudible]  Yeah, please, Sarah.  It seems like that gap can be huge. When you talk to our science teams and they're building a very general purpose model, and some of the beauty is, it can be used in so many different cases, how does the team even start reasoning about the limitations or the people that might use it if the whole goal of AI is to build this very general purpose technology?  It's a fantastic question. I think I have two answers to help the top. One, the generalizability of machine learning is, I think one of the most overstated things in like the history of technology. It blows my mind when you think about what these systems can do and yet how brittle they end up being because of the nature of the weights and biases that go into multi-dimensional hidden space. There's hidden work and parameters that go into this, and so back to the teaching metaphor, I still go back and I'm like somebody could be an amazing teacher reaching college students and they might suck at reaching preschoolers. The generalizability of machine learning is similar to the generalizability of teaching, which is just the similarities and I think we can get into those. When you want to talk about accuracy and reliability, the specifics matter. I know what your experience has been like that, the types of starting points that teams. Come to you Sarah.  Yeah, I think going back to Seth's point even about the data, there is a finite set of training data that's in this model. It probably works better for those use cases and not the use cases that you have no data representing it. So I think getting out of this notion that is general is a huge starting point, and I think going further and thinking about how to build this technology for people.  Here's a question because I love hearing this discussion between the ethical considerations and the product. One of the questions I have is, I'm having a hard time understanding the question, and the question is I should be asking myself, and I think the answer that I'm arriving at from what you're telling us is that it really depends on what you're building and who you're building it for, just like it does whenever you're building any software. Is that an accurate statement? Then you have a lot of other things in there with people that I'd love to understand. I want to get to the point where if I'm building a machine learning system, I'm asking myself the right questions to maybe even consider, this probably isn't the right thing to build using machine learning.  I think that's spot on. Yeah. The question, you're trying to of build a bridge in a way, between what's at the outermost ring, Seth, that there are a set of capabilities that potentially would feel magical. The ability to actually reason about the visual space, to turn vision into sound, or vision into text, or speech into text, or speech into an understanding about physical space. There's all this incredible stuff that we can do. Then you need to bridge this gap, because you meet people on the other side. Ultimately the value, or the accuracy, or the utility of your technology is measured by whether people like it or not, and whether they find value in it. Starting with some of these first questions, I think there's much more questions baked in, and maybe we can riff through those really quickly. But even this just up front, there's a difference between, for example, when you're building a speech recognition system, to go to that example. What am I trying to accomplish at that point? What are my needs? Do I need to be better understood by a group, or by an individual that I'm trying to relate with? Are we speaking the same language? Am I trying to make a good impression? Are these people I know? Who am I? Do I tend to speak loudly or quietly? Do I perceive myself as having an accent? Not everyone does. This is one of my favorite conversations with my kids, and they're, "Dad, we don't have accents, do we?" and I'm, "No, no. Check your entitlement for a second buddy, you just don't realize it." So these types of questions, and then maybe one that might jump out to your viewers, is this social identities question. Stuff's going to fail, technologies failed for us for forever. We have a long relationship with, "Have you tried turning it on off and on again?" But when something fails for you because of something that you feel is very personal, like maybe your gender identity, or maybe your race or ethnicity, or maybe a practice or a tradition that you have, maybe a way you dress, those are things where sometimes they can start to cross this line, and you're, "Wait, is it me? Do they build for me? Do they not build for me? Does the world not believe that this was meant for me?" Those are hard questions that I, not for a second, I don't believe that's the starting point for any engineer, or data scientist, or research scientist, I don't think anybody sets out to exclude. There's just a lot of stuff that's buried, and maybe we could take a pass through some of these layers and look at some of those issues that maybe end up getting taken for granted too often.  Yeah, I would love to see the layers, because I love the we're building stuff for people, so maybe we should start with them. But like for example, contexts, environments, apps, I'm not understanding what is that mapping look like to people. Maybe if we can go through some of the layers, so that viewers can get a sense for what you're talking about.  Yeah. Absolutely. Maybe as we're going through, Sarah, I think you bring so much direct practical experience working with product teams. Maybe adding on, I think some of that specific what you're seeing in practice, could be really great to hear.  Yeah, I'd be happy to.  The first thing about this system is each successive ring, has a dependency on the prior ring. Asking about the context, we still carry with us those questions. We haven't built our bridge yet to technology, to know whether or not in the speech recognition case, if we can effectively turn spoken words into typed text. But we started to get a bit closer and we build. If we know the people, and we know some of their characteristics, and we know some of their needs, then it gets into this question of jobs to be done, and things like dynamics. So it's a very different scenario if there's three of us jamming on some stuff going back and forth, and maybe it's good if there's cross talk and some overlap. Whereas if somebody is presenting into a more one-to-many, I think you have some pretty different dynamics in terms of expectations for what you should pick up on, and the types of accuracy or measurement. But also the specific types of transcription or whatever, that'll change if you need to understand whether I'm say a doctor, who is trying to prescribe something, and so the vocabulary or the lexicon I'll use is really different than if I'm a teacher, or if I'm in an environment with other people talking about a technical domain. So much jargon and so many things that go into that, that the jobs to be done has to really be brought to bear to know how you would measure accuracy and usefulness.  I see and how do we take this into account with our products, Sarah?  It's very interesting because in essence we are starting a lot more of that technology layer. We have something that powers many different products inside of Microsoft, as well as customer products, and so being that lower outer layer, what we really have to do is work closely with the Microsoft products that are incorporating our models, or with the customers to actually understand some of the answers to these questions, and really understand what are we trying to achieve by deploying this technology.  Let's move to environments and let's see if we can rifle through these, because I want to get a really good picture of what I should be thinking, except when I see things like environments, apps, etc. You're using these words in ways that I had never considered and so this is super interesting.  Cool. Yeah, I think the closer to people we get at, the closer it gets to more of social sciences, or user experiencing type of thinking, and you'll see we get a little bit more towards the engineering stack. I think that transition happens here where we'd all agree at an environmental level, if we're in a noisy room versus a quiet room, those parameters should have a fundamental difference on the quality of speech recognition. They also need to be taken into account. If you're building an app or a model that should work in say, helping me get directions while I'm driving and I can speak to my car, you need to have really different ambient audio detection, than in say a transcription note-to-self application environment. So those environments play a significant role. Jumping on a layer again to that app level, and this is where I think the biggest transitions start to happen, because you get stuff like, "Well, how will people interact with these? Are these swipes? Is it a touch target? Are there are devices that are actually just attached to parts of the room? Is there something that's always listening? Do I have a feedback mechanism that I actually am playing with? Can this thing learn from me? How do I know what it does?" This is the most central layer in the human and computer interaction part of the system where you'll get these big mismatches, and we know what we think of as the conceptual model of a user, and the conceptual model of the engineer who built it. Think about thermostats. For forever, you just dialed this thing further to the right when you wanted it hotter, and further to the left when you wanted to colder, but you had no notion of how quickly it would get hotter or how quickly it would get colder, and people will do these crazy things with dragging it to the slider all the way to the side, because they thought it might speed up the rate that the room would get hotter or colder, which is not true. But again, there's no feedback to the user in that process. We have to think about that stuff when we're thinking about optimizing these models. Because again, if nobody can improve the quality of the speech recognition over time, and they just have to receive the results of it, then how are they going to build confidence? If they can't build confidence, are they actually want to keep using it?  Yeah, and I think that this layer is probably one of our biggest opportunities in practice, where we both can really use it as a way to better contextualize potential errors that the model is making, as well as allow us to learn and understand where the model is failing. So we really lean into that app layer and co-design with the model and the technology. I think there's a lot more we really could do and in some cases it's been a bit neglected.  As a designer, that's definitely been my experience. There's a little like, let's assume that everything in the model is right and then people will just get the results of it. It's like, what happens when it's wrong because that's going to happen. It's guaranteed to happen with machine learning. Moving to the considerations for the actual device that you're going to use, again, brings up more of these questions. Speech recognition is another one just to continue riffing on, the difference between an omnidirectional mic versus a cardioid mic versus a shotgun mic or whatever types of of detection range and proximity that you would need in order to sustain a consistent volume for good transcription recording. The types of processing capabilities that you might have. Do you have on-device models that you're able to run to do things like attenuation of background noise or preprocessing to determine, say, if there's certain weak words or something that you want to bake into it that actually should have downstream effects for which model might get kicked off into. Lots of stuff to ask at this layer against getting closer and closer into things that typically we talk about through the lens of engineering reliability. But they're still critical when you're trying to say, how do I train my model? What is acceptable loss given, all these considerations.  As you go to stack, I'm sorry, one of the orthogonal concerns that I'm not seeing is for example, if I'm thinking about jobs to be done, I might be building an application for adults versus children versus people that are older. I don't see anything about the questions of because this is going to sound wrong, and I don't want it to sound because I want to get the right ethics. Basically once you get to the jobs to be done, are you not starting to segment people into who your target audiences, how do you do that in an ethical way?  That's a really, really, really important question. I'm glad you asked that. I'm hesitant to dig in with all these different ideas in this screen. So audience bear with us. There's a mistake that we're working our way backwards from, and some of it has to do with an over indexing on political correctness. There was this belief for quite some time and I think we probably, all three of us might have grown up in this era where it was easier to just say, I don't see color or I don't see gender or world just the same and everyone should be treated equal. To want to believe in that type of a dynamic is quite alluring. That's a really nice set of attributes and ideals to try to encode, in a generation of kids. Our parents, they did that because they wanted to believe that their efforts and progressive movements and what not have manifested in a healthier, more equitable future. It's not wrong, those aren't bad ideals, but it's not realistic. Human beings do have differences. We do judge each other, we do take into considerations our past experiences and build stereotypes about the future for right or for wrong. But those are attributes that it would be wrong for us to say everyone is exactly the same and we all have the same background. Because that itself actually marginalizes the experiences of people who have been historically forgotten. There's just too many people who have been historically forgotten because it's too uncomfortable to start calling out by name, the fact that we've forgotten them for so long. So to your points as when you start to build these groups, it is not a comfortable activity. I'm sure Sarah can speak to this on the Product Team side. I'm curious the way that you're helping groups start to wake up to that need to actually segment, not for the purposes of judging people, but from making thing more equitable.  Yeah, I think it's as you said, in an uncomfortable exercise and that it's very different than what we're used to doing. A lot of the work is just how do we even start thinking about this? How do we think about the groups of people the technology is designed to work for? How do we add that next group in that next consideration? It's definitely a very different way of thinking than when we're trying to think about the technology agnostic to the users.  One thing just to build on what Sarah's saying. One of the things that we require as it's like for our example for ethics and societies, whenever we partner with the Product Team, we go through this exercise of trying to imagine potential futures, both positive and not so positive futures and asked ourselves, who's not involved? Who is not part of the conversation? Then when we do our research, we intentionally overrepresent those groups, and we've been building out this wacky. It's like of giant spreadsheet of different ways that people identify. Because depending on the jobs to be done, depending on the context, depending on even the devices, how expensive the device might be and who might have access to reliable service, and where you're going to get the data from. All these different layers, they call into question who was involved and how early were they involved? We do have to expand our default settings of being able to categorize people. Because counter-intuitively, we want to develop for a future where nobody should have to change something about themselves to make a technology work for them.  There's this notion that I've been developing in my head because I grew up the same way, everyone's the same. But there's a huge difference between equality and equity, they're different things. One has to do with outcomes and one who has to do with opportunities, and I understand that. For example, when I'm thinking about the technology that I want to build and the jobs to be done, how do I ethically look at both of those orthogonal concerns that I think are with the people and the jobs to be done? How do I do that? Because obviously, for example, if you want to cater to a segment of the population, programmers that are blind, I've met them, they're amazing, but you have to consciously think about that segment of the population when you're building a product. How are you inclusive when you build these things, and at the same time, have the time to build something. Because you can't build something for everybody, there's just not enough time. How do you ethically build for what you can to start? Do you see what I'm getting at?  Absolutely. Absolutely. I think the different ways that you're trying to maneuver through even those words, highlights just how weird this conversation ends up being, so the first thing is to not judge each other for the stuff we don't know. The reality is if you want to paint this full image of all these different rings of considerations, building a product is messy. There's never enough time. You're always bound by certain constraints. The dilemma you should be faced with isn't let's get this perfect so that it's going to work for all people, it's let's go through the activity of describing the things that should contribute to accuracy and shouldn't contribute to inaccuracy, so we at least can say, here are the limitations of the technology we're building. There's no perfect and this is part of the issue going back to AI being a little over-hyped in terms of its generalizability. We might get to a point where metal learning and transfer learning and we'll have like multi-headed adversarial networks that will somehow build in resource management systems and reinforcement learning can somehow take us to a spot where you've got entropy built into systems such that they need resources to keep on going and robots essentially mimic humans. We could get there. Also, we have what we have right now, which are systems that are often quite brittle, and a series of layers and questions that we just haven't built up a good set of musculature as teams for how to talk about. What I'm saying to your question, Seth, and it's a fantastic question isn't actually that we can solve it. It's we have to embrace how imperfect what we're going to make is, and then be specific about who we're going to make sure it does work well for.  I see. Sarah, a quick question. Well, not a quick question because it's going to be a long answer I think. How does the product group engage with these kinds of questions? How do you work together with our ethics division? I'm saying ethics division like it's a baseball team or something.  [inaudible].  What does that look like?  Yeah. Thinking about what Josh was just saying as well, probably the most significant thing that we've been doing is just changing how we think about the products we're building and how we think about the technology. For something that's a more general platform technology like the Cognitive Services, the most freeing thing has been really leaning into, why are we building this tech? What is it for? What do we want to achieve? Then being much more comfortable and open about the limitations of it. I think in fact, that in a lot of cases led to actually just a lot of clarity in the product groups thinking and led to more innovation because they're like, ''Oh, that's what we're doing? If we're doing that, we could add this feature and that feature and that would be really cool and customers would love it.'' A lot of it has just really been focusing on, why does this product exist? Why did we build it? Who's it for? Then after that, there certainly are uncomfortable conversations around, well, who is it for? Who is it not working well but it should be for? With that kind of framing at least we're on a path to starting to say, ''It should work well for this group of people.' We haven't checked that, so we might have some very unexpected errors." Which is, I think basically the norm is what we found is. The errors manifest in very surprising ways. First with that more principled thinking, then we can start going and doing our normal products work, designing tests to line up with that goal, designing release criteria that lines up with the reason the technology exists and who is it for. It's been a great journey together with the ethics team, think more about how we build that thinking around impact and the purpose of a technology into our thinking from the beginning. There's lots of open questions, but it's definitely helped create clarity for us and how we move forward?  I've never had an ethics person on the show. Here's a pet peeve that I've always had with machinery and I want to run it by you-all and tell me if my thinking is right. I personally can't stand when people anthropomorphize machine-learning models because my inner self is telling me that it allows people to disclaim liability or to move the ethics needle over to the machine when it really is a human who controls those things. Am I wrong here? Should I not be upset about that?  It's actually one of the biggest complaints about the term responsible AI because it's not that we want the AI to be responsible, we want to build it responsibly and we want to use it responsively. It's really developing AI responsibly not responsible AI. The term is what it is. [inaudible] real questions.  It's a really good question. We've tried to look at that issue, Seth, from a couple of different perspectives. One, we just have this little axiom on the team. We say the fidelity of the AI should not exceed the fidelity of the capabilities. When you anthropomorphize something, you are for all intents and purposes, representing its capabilities as being human level and they're not. That's one issue that we would call it maybe an ethics issue because you are implicitly over-representing the capabilities of this thing. There's other stuff about like relationship formation, which I think goes to both the points that each of you were making about who is the responsible party? It could be a goal, but is our goal to help people build better relationships with robots? That could be the goal, but let's be explicit about that goal. I'm saying that's got to be a difference between in this ring, so you talk about the data sources in the optimization goals in the architecture when we're at that layer and we're talking about building a synthetic voice, the optimization goals for that synthetic voice, is how well they deceive people into believing that it's a real person. That's the measure. It's a subjective measure where people listen to a voice and they rate it from a 1-5 scale of realisticness. When it hits that top scale and they basically are like, ''Yeah, I'm listening to a person.'' You've won, you've succeeded, you've optimized your model to a point where your loss is sufficient. That's something we have to unpack. If that's your intent, is to deceive people and to potentially over-represent their functionality so that they can build a relationship with a robot, that's different than if it's right or wrong. It's just isn't your intent?  I see. We've had a long discussion and we should definitely do this again because there's a lot more stuff and I want to make sure that we keep this tight. Sarah, could you maybe really quickly go through this anion with maybe a product that we built and how you've considered all these things. I know I'm putting you on the spot.  I'm actually hoping you'll let us come back and do that with some of our upcoming releases that we can't talk about this week.  Cool.  Can I take a rain check on it?  Absolutely. Because for me, this is all super highly conceptual, but I'd love to see and maybe we'll do this down the road where we go through a product and you can see like, oh, I see how this maps here, I see how this maps here, I see how this maps here, and I see how now we're being holistically ethical while at the same time being imperfect, which is what we only can do. We can strive for perfection, we will probably never reach it, but I love to see how this actually maps. To finish up because I want to make sure that people can digest this episode. Josh, what's the key takeaway here from an ethical perspective? Then Sarah, last word, how are we taking this to heart when we build AI products? Josh.  Yeah, thanks. It's a great question. Thanks again for having us. I think the top thing I would hope folks takeaway is to be ethical or the start of a journey of being ethical is not even really about right or wrong or the moral values or what have you. There's lots of time and consideration needs to go into that. It's about saying what you're going to do and then doing what you're going to say. The ability to be intentional, to start with a purpose, and then follow that purpose through and be clear about where there are limitations about how accurately or reliably you are able to deliver on that purpose, that's really the bedrock. I think that's really within the capability of any developer or engineer or scientist that's working in this domain. I would ask you to go seek out your neighborhood UX designer or UX researcher, and engage in a conversation on that subject because we've got a lot of ideas and hopefully we can help you meet people where they're at.  Sarah.  Yeah, I think for us the biggest, most concrete change has just been starting with a purpose. Why are we building this technology? Then everything else in the diagram falls out from there. When you have a clear purpose, it's easier to even ask some of the hard questions and to make some of the more challenging trade-offs. I'm going to come back on the show and talk more about all of the specific technologies that we'll be announcing next week at Ignite and also moving forward in general so we can get really specific about for each technology, what we've done and what does it look like to put this into practice?  Well, this has been amazing. Thank you so much for spending some time with us, and again, dear viewer, thank you so much for watching, we've been learning all about the ethics of AI. I think it boils down to what are we going to do? Are we going to stick to it? It's pretty cool. Thank you for watching and we'll see you next time. Take care. [MUSIC] 