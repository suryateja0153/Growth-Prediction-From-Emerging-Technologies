 In this special build edition of the AI show, join us as we hear from Mehrnoosh Sameki, Senior Product Manager on the Azure Machine Learning responsible AI team. She will talk about our open source machine learning fairness toolkit, and how to test models for fairness with Fairlearn. Make sure you tune in. [MUSIC]  Hi everyone. My name is Mehrnoosh Sameki, I'm a Senior Product Manager at Azure AI, driving the product efforts behind to our responsible AI tools Interpretml and Fairlearn. Today, I'm excited to share the latest developments in our open source machine learning fairness toolkit called Fairlearn, which takes steps toward enabling to build fairer and more inclusive machine learning models. AI has the potential to drive considerable changes in the way we do business. Thus, it will have a broad impact on society as well. This impact raises a host of complex and challenging questions about the future we want to see. As we look into this future, we should ask ourselves, how do we design, build, and use AI systems that create a positive impact on individuals and on society? At Microsoft, as a foundation to guide our thinking, we have defined six responsible principles that AI system should adhere to. The focus of this presentation is on the fairness principles, which aims to tackle the question of how can we ensure that AI system treat everyone fairly? Now, there are many ways that an AI system can behave unfairly. For example, AI can give rise to the harm of quality of service, which is whether a system works as well for one person as it does for another. An example of this particular type of harm is a voice recognition system that might fail to work as well for one sex compared to another sex. Another type of harm that AI can give rise to is the harm of allocation, which is the harm that can occur when AI systems extend or withhold opportunities, resources, or information to a specific groups of people. An example of that type of harm is a model for screening loan or job applications that might be much better at picking good candidates among a particular race compared to other races. So really the purpose of fairness in AI is to avoid negative outcomes of AI systems and machine learning models for different groups of people. Today, I'm excited to announce our open source machine learning fairness toolkit called Fairlearn, which is a toolkit that empowers developers of artificial intelligence systems to assess their systems' fairness and mitigate any observed fairness issues. The focus of Fairlearn is on group fairness, and there are really two components to it. The first component of the Fairlearn is the assessment dashboard with both high-level and detailed views for assessing which groups are negatively impacted. The second is a set of algorithms for mitigating the observed fairness issues. These strategies are based on variety of supported fairness definitions such as demographic parody and equalize odds for classification tasks and bounded group lasts for regression tasks. The supported unfairness mitigation techniques can easily be incorporated into existing machine learning pipelines, and thus together, these two components of assessment and mitigation enable data scientists and business leaders to navigate any trade-offs between fairness and performance, and to select the mitigation strategy that best fits their needs. Fairlearn supports a wide range of models to provide the most flexibility to its user to assess their black box models fairness and mitigate the apps their fairness issues. Today, I'm also excited to announce that Fairlearn is being integrated within Azure Machine Learning to enable an easy access of our Cloud customers to Fairlearn. Now, let's switch gears and take a look at a demo of how to use Fairlearn in action. What I'm going to show you is a sample notebook that I have put under hosted notebook VM in Azure Machine Learning. It is a binary classification on census data. The dataset is a classification problem, given a range of data about 32K individuals, it predicts whether their annual income is above or below $50,000 per year. However, for the purpose of this notebook, we shall treat that as a loan decision problem. We'll pretend that this label indicates whether or not each individual repaid a loan in the past or not, and we will investigate how this model has treated different demographics and different groups of people. After installing the package, what I'm going to do is I'm going to go through my model training the way that I used to do it. I probably will drop my protected attributes, in this case, I chose sex and race. I will do some transformations on top of my features, split the data into tests and train, and in this particular case, I train a logistic regression to basically predict whether someone would be qualified for a loan or not. The very first step with Fairlearn is to run this assessment dashboard to assess the fairness of your model on top of these two features. First, when I load the dashboard, it asked me to do some configuration. I need to pick my sensitive attribute, and also I need to pick the performance metric that I would like to use in order to measure the performance of my model. For the sensitive attributes, I move forward with sex, which has two values, male and female in this dataset. Then I move forward to pick my performance metric, and in this case, I choose accuracy, which is the fraction of data points classified correctly. Once I go through this configuration stage, I now land on the result page to get multiple different insights about how will my model has done the job and how fair my model is? Let's go through this. First, I see the overall accuracy rates of my model being 83 percent. I can see the breakdown of accuracy across the two sexes, 79 percent for males, 92 percent accuracy for the females. So we can see that there is a disparity in accuracy of 92 minus 79, which is 12.9 percent disparity in accuracy rate. Now, I can also see how the model has made mistakes. For instance, I can now compare the false positive rate and false negative rate across two different sexes. For the underprediction, which is the ground true was being one and the predicted being zero, there is more underprediction for the male group, 14 percent versus 5.6 for female group. However, for overprediction, which is the ground true was zero, and the predicted was getting the loan or year 1, you can see that the overprediction is more for males compared to females. Another insight that I can get is by looking at this disparity in predictions or disparity in selection rate. Overall, I can see that 17.9 percent of the overall population have been picked to get the loan. When we compare it across the two sexes, we can see that for males, 22 percent of the males have got approved for the loan, and for females, 7.55 percent have got approval on their loans. So there is a disparity of the difference between these two, which is 15.3 percent disparity in selection rate. I might say that in my particular context, I cannot tolerate this 15.3 percent of disparity in selection rate. So let's move forward and see how we can use Fairlearn's mitigation algorithm GridSearch in order to mitigate that particular unfairness that I just observed. GridSearch is a state of the art mitigation algorithm from Microsoft Research, New York City, which acts as a wrapper on top of any standard machine learning algorithm and finds a classifier that minimizes classification errors subject to a user-defined fairness constrain. What GridSearch does it iteratively calls the black box model and runway and possibly relabeling the training data. So each time it generates a sequence of relabeling and runways and train a predictor for each one. So I am calling GridSearch now, I'm passing my standard black box model that had trained on top. I'm also defining this fairness constraint, which is demographic parity, which really talks about the loan approval decision should be independent of protected attribute. I'm specifying the grid size as well. After that, I would fit that again on top of my train data, and this time, I also pass the sensitive attribute, which I want to use for fairness considerations. After I'm done, I get rid of non-dominated models, and here, I load this dashboard again to navigate the trade-offs between model performance and model fairness, and also compare the mitigated models with the unmitigated one. I go forward with the same configuration of sex and accuracy rates. Now, instead of landing on the results page, I'm landing on this model comparison chart which really allows me to see the accuracy of different models with respect to their fairness, which in this case is disparity in prediction or disparity in selection rate. This is my unmitigated model. If you remember, it had almost an accuracy of 84 percent and the disparity in selection rate of almost 15 percent. I can verify that here, the disparity in selection rate was 15 and the accuracy was 82 percent. Now, what I can do with this comparison chart is I can look at different models, which are the models that might mitigation algorithm has generated, and depending on my context, I can choose which model is more appropriate for my use case and which model I really want to move forward and deploy. For instance, this is my original model, and I might say that, I am okay losing the accuracy by almost four percent, however, saving a lot on disparity in selection rates. So I move forward to see whether this model looks good to me. The overall accuracy is 82 percent, so I haven't lost much. However, the disparity in selection rate was dropped from 15 percent to 6.55 percent. If I like it, I move forward to deploy it. If not, I can go back to the multi-model view and for instance, click on some other options for me and see how much sacrifice I can make on the accuracy of my model in order to get better disparity in selection rate. We put that completely in your hands to decide based on your context and based on the new case, as which of these models would be more appropriate for you to move forward with. As mentioned earlier, Fairlearn is also getting integrated into Azure Machine Learning, which provides you with the same dashboard experience that I just walked you through in the Jupyter Notebook, but inside Azure Machine Learning Studio. This way you can register your models, and by clicking on each of your registered model, you can go to the Fairness tab and see the fairness inside of your model, or you can log multiple different models under an experiment and compare the fairness insights of those models inside the Studio. For instance, I just go through the exact same flow of picking this time, race, accuracy rate, and landing on this multi-modal comparison, and by clicking on each of these, I can see the accuracy rate insights and also disparity in predictions. Thank you very much for watching our video. Please check out our open source offerings on fairlearn.org, and stay tuned for our Fairlearn integration with Azure Machine Learning. Please also check out our customer highlight with Ernst & Young. Thank you. 