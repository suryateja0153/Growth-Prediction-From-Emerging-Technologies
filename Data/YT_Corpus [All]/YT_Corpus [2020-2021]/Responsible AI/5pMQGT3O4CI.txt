 I'm an Operations Manager so my role is to ensure that we're making our considerations around ethical AI deliberate actionable and scalable across the whole organization in Google so one of the first things to think about if you're a business leader or a developer is ensuring that people understand what you stand for what does ethics mean to you for us that meant setting values-driven principles as a company these value driven principles for us are known as Rai principles and last year we announced them in June so these are seven guidelines around AI development and deployment which assigned to us how we want to develop AI we want to ensure that we're not creating or reinforcing bias we want to make sure that we're building technology that's accountable to people and we have five others here that you can read it's available on our website but at the same time that we announce these aspirational principles for the company we also identified four areas that we have considered our red lines so these are technologies that we will not pursue these cover things like weapons technology we will not build or deploy weapons we will also not build or deploy technologies that we feel violate international human rights so if you're a business leader a developer we'd also encourage you to understand what are your aspirational goals but at the same time what are your guardrails what point are you not going across it's the most important thing to do is to know what is your definition of ethical AI development after you've set your AI principles the next thing is how do you make them real how do you make sure that you're aligning with those principles so here there are three main things like suggest keeping in mind the first one is you need an accountable and authority body so for Austin Google this means that we have senior executives across the whole company who have the authority to move or decline a launch so they have to wrestle with some of these very complex ethical questions to ensure that we are launching things that we do believe will lead to fair and ethical outcomes so they provide the authority and the accountability to make some really tough decisions secondly you have to make sure that the decision makers have the right information this involves talking to diverse people within the company but also listening to your external users external stakeholders and feeding that into your decision making criteria jameelah we'll talk more about engaging with external communities in a moment and then the third key part of building governance and accountability is having operations who's gonna do the work what are the structures and frameworks that are repeatable that are transparent and that are understood by the people who are making these decisions so for that in google we've established a central team that's not based in our engineering and product teams to ensure that there's a level of objectivity here so the same people who are building the products are not the only people who are looking to make sure that those products are fair and ethical so now you have your principles that you're trying to ensure that people understand what does ethics mean for you we're talking about establishing governance structure to make sure that you're achieving those goals and the next thing to do is to ensure that you're encouraging everyone within your company or the people that you work with and for are aligned on those goals so making sure one that you've set overall goals in alignment with ethical AI so how are you going to achieve ethical development and deployment of technology next you want to make sure that you're training people to think about these issues from the start you don't want to catch some ethical consideration late in the product development lifecycle you want to make sure that you're starting that as early as possible so getting people trained to think about these types of issues then we have rewards you have to make sure if you're holding people accountable to ethical development or deployment you may have to accept that that might slow down some development in order to get the right outcome making sure people feel rewarded for thinking about ethical development and deployment and then finally making sure that you're hiring people and developing people who are helping you achieve those goals next we have you've established your frameworks you've hired the right people you're rewarding them how do you know you're achieving your goals so we think about this as validating and testing so example here is replicating a user's experience who are your users how do you make sure that you're thinking about a representative sample of your users so you think about trying to test different experiences mostly from your core subgroups but you also want to be thinking about who are your marginalized users who might be underrepresented in your workforce and therefore you might have to pay additional attention to to get it right we also think about what are those failure modes and what we mean by that is if people have been negatively affected by a product in the past we want to make sure they won't be negatively affected in the future so how do we learn from that and make sure that we're testing deliberately for that in the future and then the final bit of testing and validation is introducing some of those failures into the product to make sure that your stress testing and again you have some objectivity to stress test a product to make sure it's achieving your fair and ethical goals and then we think about it's not just you you're not alone how do we ensure that we're all sharing information to make us more fair and ethical and to make sure that the products we deliver are fair and ethical so we encourage the sharing of best practices and guidelines we do that ourselves in Google by providing our research and best practices on the Google AI site so these best practices cover everything from ml fairness tools and research that Margaret Mitchell will talk about in a moment but also best practices and guidelines that any developer any business leader could follow themselves so we try and both provide that ourselves as well as encouraging other people to share their research and learnings also so with that as we talk about sharing with external it's also bringing voices in so I'll pass over to Jamila Smith to talk about understanding human hi everyone I'm gonna talk to you a little bit today about understanding conceptualizing and in assessing human consequences and impacts on real people and communities through the use of tools like social and equity impact assessments social and equity impact assessments come primarily from the social science discipline and give us a research-based method to assess these questions in a way that is broad enough to be able to apply across products but also specific enough for us to think about what are tangible product changes and interventions that we can make so I'll start off of one of the questions that we often start when thinking about these questions I always like to say that when we're thinking about ethics when we're thinking about fairness and even thinking about questions of bias these are really social problems and one major entry point into understanding social problems is really thinking about what's the geographic context in which users live and how does that impact their engagement with the product so really asking what experiences do people have that are based solely on where they live and that may differ greatly for other people who live in different neighborhoods that are either more resource more connected to internet all of these different aspects that make regional differences so important secondly we like to ask what happens to people when they're engaging with our products in their families and in their communities we like to think about what our economic changes that may come as a part of engagement with this new technology what are social and cultural changes that really do impact how people view the technology and view their participation in the process and so I'll start a little bit we're talking about our approach the good thing about utilizing kind of existing frameworks of of social and equity impact assessments which come from if you think about when we do new land development projects or even environmental there's already the standard of considering social impacts as a part of that process and so we really do think of employing new technologies in the same way we should be asking similar questions about how communities are impacted what are their perceptions and how are they framing these engagements and so one of the things that we think about our kind of would is a principled approach to asking these questions and the first one really is around engaging in hard questions when we're talking about fairness when we're talking about ethics we're not talking about them separately from issues of racism social class homophobia and all forms of cultural prejudice we're talking about what are the issues as they overlay in those systems and so it really requires us to be ok with those hard questions and engaging with them and realizing that our technologies and our products don't exist separately from that world the next approach is really towards thinking anticipatory I think the different thing about thinking about social inequity impact assessments from other social science research methods is that the relationships between causal impacts and correlations are going to be a little bit different and we really are trying to anticipate harms and consequences and so it requires you to be okay with the fuzzy conversations but also realize that there's enough research there's enough data that gives us an understanding of how history and context impact outcomes and so being anticipate ory in your process is really really an important part of it and lastly in terms of thinking about the principled approach is really centering the voices and experiences of those communities who often bear the burden of the negative impacts and that requires understanding how those communities would even conceptualize these problems I think sometimes we come from a technical standpoint and we think about the communities as separate from the problem but if we're ready to Center those voices and engage it throughout the whole process I think it results in better outcomes so to go a little bit deeper into engaging in the hard questions what we're really trying to do is be able to assess how a product will impact communities particularly communities who have been historically and traditionally marginalized so requires us to really think about history and context how is that shaping this issue and what could we learn from that assessment it also requires an intersectional approach you know if we're thinking about gender equity if we're thinking about racial equity these are not issues that live separately they really do intersect and being okay with understanding that intersectional approach allows for a much fuller assessment and then lastly in thinking about new technologies and thinking about new products how does power influence outcomes and the feasibility of interventions I think that the question of power and social impact go hand in hand and acquires us to be okay with answering answering might not get the best answer but at least asking those hard questions so our anticipate Auri process is part of a full process right so it's not just us thinking about the social and equity impacts but it really is thinking about them within the context of the product so really having domain-specific application of these questions and then having some assessment of the likelihood of the severity of the risk and then lastly thinking about water mitigations meaningful manipulations for whatever impacts that we have developed and so it's a full process it requires work on our team in terms of understanding in the assessment but it also requires partnership with our product teams to really do that domain-specific analysis centering the assessment I talked a little bit about this before but when we're centering the assessment really what we're trying to ask is who's impacted most so if we're thinking about a problem that may have some economic impact it would require us to disaggregate the data based on income to see what communities what populations are most impacted so being okay with thinking about it in very specific population data and understanding what whose at the most another important part is validation and I think Jen mentioned that a lot but really thinking about community-based research engagements whether that's a participatory approach whether that's focus groups but really how do we validate our assessments by engaging communities directly and really centering their framing of the problem as part of our project and then going through iteration and realizing that it's not gonna be perfect the first time that it requires some pool and tugging from both sides to really get the conversation right so what types of social problems are we thinking of we're thinking about income inequality housing and displacement health disparities the digital divide and food access we're thinking about these in all different types of ways but I thought it might be helpful if we thought about a specific example so let's look at the example of one of the types of social problems that we want to understand in relation to our products and users the top of inequity related to food access which this map shows you and it's you know definitely a u.s. context that we're thinking about this question for now but also always thinking about it from a global way but I thought that this map was a good way for us to look at it as you can see the areas that are shaded darker the areas where those users might have a significantly different experience when we're thinking about products that give personalization and recommendations maybe for something like restaurants so we're thinking about questions about how those users are either included or excluded from the product experience and then we're thinking about going even further and thinking about how small businesses and and low resource businesses also impact that type of product so it requires us to you know realize that there's a wealth of data that allows us to even go here as deep as the census tract level and understand that there are certain communities who have a significantly different experience than other communities and so like I said this map is looking at communities at a census tract level where there's no car in no supermarkets store within a mile and if we want it to look even deeper we can overlay this information with income so thinking about food access and income disparity which are also often connected gives us a better understanding of how different groups may engage with a product and so when thinking about a hard social problem like this it really requires us to think what's the logical process for us to get towards a big social problem and have very specific outcomes and effects that are meaningful and are making a change and it requires us to really acknowledge that there's context that overlays all parts of this process from the inputs that we have from the activities that we do which may in my case be very much research-based activities and then thinking about what are meaningful outputs and so to go in a little bit deeper in kind of this logic model way of thinking about it we have a purpose now in thinking about the food access example to reduce negative unintended consequences in areas where access to quality food is an issue we're also very aware of the context so we're thinking about the context of food access but we're also thinking about questions of gentrification we're thinking about displacement we're thinking about community distrust so we realized that this question has many other issues that that inform the context not just access to food but it's part of the process we're identifying resources we're thinking where they're most of multidisciplinary research teams that can help us think through what are our external stakeholders that can help us frame the problem and then what are the cross-functional relationships that we need to build to really be able to solve this kind of problem while acknowledging what our constraints are oftentimes time is a huge constraint and then gaps just in knowledge and comfort and being able to talk about these hard problems some of the activities and inputs that we are thinking about can help us get to some answers are really thinking about case studies thinking about surveys thinking about user research where we're asking users perception about this issue how does engagement based on your geography differ and being able do that analysis and then creating tangible outputs some that are product interventions and really focused on on how we can make changes to the product but also really community based mitigations and thinking about are there ways in which we're engaging with the community the ways in which we're pulling data that we can really use to create a fuller set of solutions and really it's always towards aspiring for positive effects in principle and practice so this is one of those areas where you can feel like you have a very principled approach but it really is about being able to put them into practice and so some of the things that I'll leave you with today in thinking about understanding kind of these human impacts are really being able to apply them in thinking about applying them in specific technical applications building trust through equitable collaboration so really thinking about when you're engaging with external stakeholders how do you make it feel equitable and that we're both sharing knowledge and experiences in ways that are meaningful and then validating the knowledge generation when we're engaging with different communities we really have to be ok that information data and the way that we frame this can come from multiple different sources and it's really important and then really thinking about within your organization within your team what are change agents and would have changed instruments that really make it a meaningful process thank you now Margaret will talk more about the machine learning pipeline thanks to Mila so I'll be talking a bit about fairness and transparency and some frameworks and approaches for developing ethical AI so in a typical machine learning development pipeline the starting point for developers is often the data training data is first collected and annotated from there a model can be trained the model can then be used to output content such as predictions or rankings and then downstream users will see the output and we often see this approached as if it's a relatively clean pipeline that provides objective information that we can act on however from the beginning of this pipeline human bias has already shaped the data that's collected human bias and further shapes what we collect and how we annotate it here are some of the human biases that commonly contribute to problematic biases and data and in the interpretation of model outputs things like reporting bias where we tend to remark on things that are noticeable to us as opposed to things that are typical things like out-group homogeneity bias where we tend to see people outside of our social group as somehow being less nuanced or less complex than people within the group that we work with and things like automation bias where we tend to favor the outputs of systems that are automated over the outputs of humans what humans actually say even when there's contradictory information so rather than this straightforward clean and to end pipeline we have human bias coming in at the start of the cycle and then being propagated throughout the rest of the system and this creates a feedback loop where as users see the output of bias systems and start to click or start to interact with those outputs this then feeds data that is further trained on that's already been biased in this way creating problematic feedback loops where biases can get worse and worse we call this a sort of bias Network effect or bias laundering and a lot of our work seeks to disrupt this cycle so that we can bring the best kind of output possible so some of the questions we consider is who's at the table what are the priorities in what we're working on should we be thinking about different aspects of the problem in different perspectives as we develop how is the data that we're working with collected what kind of things does it represent are they're problematic correlations in the data or are some kinds of subgroups underrepresented in a way that will lead to disproportionate errors downstream what are some foreseeable risks so actually thinking with foresight and not anticipating possible negative consequences of everything that we work on in order to better understand how we should prioritize what constraints and supplement should be in place beyond a basic machine learning system what can we do to ensure that we can account for the kinds of risks that we've anticipated and can foresee and then what can we share with you the public about this process we aim to be transparent as we can about this in order to bring about information about how we're focusing on this and make it clear that this is part of our development life cycle I'm gonna briefly talk about some technical approaches this is in the research world you can look up papers on this if you're interested for more details so there are two sort of ml machine learning techniques that we've found to be relatively useful one is bias mitigation and the other one we've been broadly calling inclusion so bias mitigation focuses on removing a signal for problematic variables so for example say you're working on a system that is supposed to predict whether or not someone should be promoted you want to make sure that that system is not keying on something like gender which we know is correlated with promotion decisions in particular women are less likely to be promoted or are promoted less quickly than men in a lot of places including in in tech we can do this using an adversarial multitask learning framework where while we predict something like getting promoted we also try and predict the subgroups that we'd like to make sure isn't affecting the decision and discourage the model from being able to see that removing the representation by basically reversing the gradient and back propagating when we work on inclusion we're working on adding signal for something trying to make sure that there are subgroups that are accounted for even if they're not well represented in the data and one of the approaches that works really well for this is transfer learning so we might take a pre trained network with some understanding of gender for example or some other standing of skin tone and use that in order to influence the decisions of another network that is able to key on these representations in order to better understand nuances in the world that it's looking at this is a little bit of an example of one of the projects I was working on where we were able to increase how well we could detect whether or not someone was smiling based on working with some consented gender identified individuals and having representations of what these gender presentations look like using that within the model that then predicted whether or not someone was smiling some of the transparency approaches that we've been working on helped to further explain to you and also help keep us accountable for doing good work here so one of them is model cards in model cards we're focusing on reporting what model performance is disaggregating across various subgroups and making it clear that we've taken ethical considerations into account making it clear what the intended applications of the model or the API is and sharing generally different kinds of considerations that developers should keep in keep in mind as they work with the models another one is data cards and this provides information about evaluation data about when we report numbers what is this based on who is represented when we decide a model can be used that it's safer use these kinds of things are useful for learners so people who generally want to better understand how models are working and what are the sort of things that are affecting model performance for third-party users so non ml professionals who just want to have a better understanding of their data sets that they're working with them or what the representation is in different data sets that machine learning models are based on or evaluated on as well as machine learning researchers so people like me and you want to compare model performance they want to understand what needs to be improved what is already doing well and help be able to sort of benchmark and make progress in a way that's sensitive to the nuance differences in different kinds of populations our commitment to you working on fair and ethical artificial intelligence and machine learning is to continue to measure to improve and to share real-world impact related to ethical AI development Thanks [Applause] [Music] 