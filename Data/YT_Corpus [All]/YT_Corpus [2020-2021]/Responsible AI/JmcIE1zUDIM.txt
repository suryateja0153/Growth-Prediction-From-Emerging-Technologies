 [Music] hi everyone and welcome to the session on machine learning reliability and robustness um my name is besmiranoshi and i'm a researcher in microsoft research today in this session we are going to discuss open challenges and ambitions and recent work on how to make machine learning more reliable and today we will have three amazing speakers to help us frame this problem with their work and insights the first one is going to be tom dietrich who is a distinguished professor at oregon state university and he will talk about anomaly detection in machine learning next aja kamar a senior principal researcher in microsoft research is going to talk about uh blind spot detection of ai in the real world and then such as arya who is a professor at johns hopkins university is going to share with us key insights about safely and reliably deploying machine learning in high stakes decision making scenarios such as healthcare but not only so um feel free to send your questions to our speakers as the presentations are running they will try to answer your questions in the chat and maybe also elaborate on them afterwards in the panel q a it's great to have this opportunity to share with you some of my thoughts on the current status of anomaly detection in machine learning and computer vision so anomaly detection is very important for trying to achieve robust artificial intelligence and i wanted to review some of the use cases that arise the first is open category detection so imagine that we're training a self-driving car and we've trained it in europe so it knows about deer and so on but when we take it to australia what does it do the first time it encounters a kangaroo that's an example of what's known as a novel category or an open category learning problem and we can treat it as a kind of anomaly the second is a novel subcategory so we can imagine that we've trained a system that knows how to recognize dogs but it actually only knows how to recognize beagles and golden retrievers what happens the first time it sees a chihuahua it might correctly get classified as a dog but it should be much more cautious when it sees something that's uh very different from what it trained on and another uh sub area of research is known as out of distribution detection and an example of this might be a medical x-ray analysis system that was trained on chest x-rays uh but now at test time someone feeds it a instead of a front view of the chest x-ray feeds its side views or back views and it needs to know detect that that's different and that it should not be trusted in those cases so those are three examples where it's important for robust artificial intelligence in addition there are applications in data cleaning where we have a database of of images for example but some of them uh were contaminated by by uh noise of various kinds or or maybe we're looking at cases such as fraud detection or cyber attack where we also want to use anomaly detection so there are many challenges in anomaly detection first of all there's little or no labeled data and and the anomalies themselves are typically quite rare which is one of the reasons there's little or no label data [Music] an additional issue is that the anomalies may not come from a well-defined probability distribution especially in the adversarial case so so it doesn't make sense to to approach them from a statistical standpoint and the final issue which comes up particularly in image type data is what we call nuisance novelty not all anomalies are really relevant to the task or the use case so if we're looking at internet traffic for example there might be lots of irrelevant novelty in the data that does not send it signal a cyber attack or in the image case there might be changes in the background or the context that are not particularly important so how do we avoid getting lots of false alarms in those cases well in the for for data that comes in well-defined features there are three main technical approaches the first if we let capital d denote our training data the first thing we can do is try to perform density estimation that is we learn a probability density p sub d that describes our our data points and then given a query point xq here we calculate our our estimated p of d and take its negative log which is a measure of surprise from information theory so if something has very low probability then it will have a very large negative log probability so one issue with this approach is that it is vulnerable to nuisance novelty because it tries to model all of the of the aspects of the data and in high dimensions it's really not statistically feasible to do that a second approach is distance based metrics so if we can come up with a meaningful distance metric in our problem then given a query we can ask well how far is it from that query point to the nearest data point that we trained on and this also is vulnerable to irrelevant or nuisance novelty in the in the because that influences the distance metric one approach to trying to deal with the distance metric problem is projection methods so imagine if we let you know capital pi sub k be a random projection usually sparse projection into some lower dimensional space we can apply our anomaly detection methods in those lower dimensional spaces and then aggregate them back to get an overall anomaly score and this can go a long way toward dealing with the relevant features and all these methods can be if we're training on contaminated training data it helps to robustify them using things similar to the huber loss for those of you who are familiar with that so my master's student andrew emmett has just completed a large comparison study where he used 19 existing uc irvine datasets and compared eight of the leading uh sort of feature based anomaly detection algorithms generating thousands of different benchmarks with systematic variation of various factors that we believe are important but i just want to show here this plot which is the mean effect of the what's on the area under the roc curve for making this decision anomalous versus nominal and we see that the k nearest neighbor algorithm which is nothing fancy actually gives the best performance now the k nearest neighbor method it tends to be expensive to implement but the isolation forest i forest here is almost as good and it is much more efficient and uh disappointingly some of the most popular machine learning algorithms like the one class support vector machine was the worst performer in this in this study but i want to highlight that with the exception of one method all the other algorithms employ some form of a distance measure and that's really the heart of the matter in trying to come up with good anomaly detection techniques now the uh the main challenge going forward is in the area of computer vision uh speech recognition other uh large complex signals time series things like this and the challenge with these high much higher dimensional spaces is that there are no easy distance metrics uh images whether they you know if you double the number of pixels in an image you don't really change its essential content so the uh so there's a high degree so so that's one of the issues the distance measured in the pixel space is not uh semantically useful um and uh and it turns out that uh state-of-the-art methods really have troubles with uh with images for example i'll talk about a method later that is trained on these images of faces of celebrities and then when it's asked to make predictions on these uh numbers from street view house numbers it doesn't realize that they are not the faces of celebrities so that that shows just uh shockingly how badly the anomaly detection methods can uh perform in on image data so i want to give a brief tour of five of the leading ideas we have not solved the problem of anomaly detection in computer vision by any means but here are five of the main ideas and then i'll wrap up so the first family of ideas are all based on just training a standard deep learning classifier and so this is the typical organization of such a deep learning classifier starting with the image over here and going through various layers and eventually we end up with predicted probabilities p of y given x so y is the class label and x was the input image and the first thing we can do is is use some measure of indecision so if the classifier doesn't strongly recognize any of its known classes then maybe that's a sign that the input is anomalous um and so we can use one minus the the maximum probability that it predicted or the negative of the uh entropy uh which in the entropy will get large uh we get uh yeah actually it shouldn't be the negative of the entropy but the entropy has the uh as an anomaly signal now on theoretical grounds this shouldn't really work because a classifier that is trained just to predict p of y given x uh ideally would discard any information that was not relevant to making that discriminative decision and but the but we see in practice this is a very surprisingly strong baseline method which shows that our deep learning methods are not throwing away finding only the the critical information the second approach is to focus on the so-called penultimate layer you can see here on the right which is usually a much larger layer just before the classification and we can build probability models in that penultimate layer and then we can ask we can score a new data point in according to that probability model and if it has very low probability we could say it's an anomaly a third approach and one that's the most intriguing i think might really lead us to a solution our perturbation approaches and these really came about from people studying adversarial attacks on systems but but these are not non-adversarial changes but instead of computing an anomaly score for a of xq the query we're going to perturb x cubed by an amount delta q to try to reduce its anomaly score in some sense we try to increase the prob the the score that it is assigned to one of the existing classes and we find that if xq is near an existing class the perturbation can substantially increase the predicted probability of that class which is to say reduce the indecision of the anomaly score whereas for anomaly points these perturbations have much less effect and so this is currently probably the the leading strategy another family of methods are based on what are called autoencoders these are deep neural networks that take an input x map it into some restricted reduced space z the latent space and then map it back to the input space with a function g the so-called generator and then compare x tilde there on the right to the x value that we started with and the difference between those is known as the reconstruction error and also there are groups that try to fit a probability model in this z space and then use a latent probability model while this is very elegant and and works well in small spaces just using principal component analysis as a technique in deep networks it hasn't about produced very good performance another strategy that's somewhat related is so-called normalizing flow models so there are techniques for deep density estimation uh and these are based on learning invertible functions uh f sub phi of x and there are some very general non-linear neural network style functions that can be developed and you can convert a density over this map space f of x back into a density over p of x by using a normalization using the jacobian which is easy for us to compute with our deep neural network techniques and then we can compute an anomaly score again as the surprise and but this is an example of a method that fails miserably on the celebrities so down here we see these are the training data and the on the horizontal axis here is the uh log probability so uh larger values are to the right and uh and so you can see that it's actually assigning higher probabilities to the street view house numbers than it is to the celebrities faces which is totally the wrong thing that the red point should be to the left of the blue not to the right and this was quite a shocking discovery that was uncovered just last year so there are another family of methods that try to learn distance functions so we take two data points x i and xj and map them using this function e the encoder into this latent space and then if x i and xj belong to the same class we try to move their z points close together whereas if xi and xj belong to different classes we try to push their z points apart at least by some minimum amount and then the hope is that we can use a nearest neighbor classifier in that space and we can also use that distance to uh to find anomalies unfortunately we find that anomalies tend to be mapped very close to the normal points that that the network has been trained on so again this this doesn't work as well as we would hope and the last idea and maybe the weirdest is the following we define a set of auxiliary tasks these are typically things like rotating an image by 45 degrees or 90 degrees or 180 degrees and and flipping it and stretching it and things like this and when we we we train a network that given the image to which a transformation has been applied the the network simultaneously tries to predict what class label the image should have and also which transformation was applied to it so for example the digit 3 we might rotate by 45 degree angle we will get that that kind of transformation and we would like the network to then output that the high probability for that particular transformation as well as predicting that it belongs to the class 3. so then given a test query we can apply these different transformations and if the transformation sub network this part here if it is correctly predicting those and we know what the right answer is because we applied the transformations then we're willing to trust that its classifications are also trustworthy but if it can't predict the the transformations then we'll declare it to be an anomaly and the the intuition for what's going on here is that in order to understand uh that this three has been rotated you really have to know what a three looks like in its normal orientation and so it seems to force the classifier to pay attention to what's important in the image and this also helps remove some of the nuisance novelty so in conclusion anomaly detection is very important critical for building robust ai systems and also it can support practical applications such as fraud detection but it's also very difficult fundamentally virtually all the methods rely on some notion of a distance but this is very challenging for images because we need some notion of semantic distance and our deep neural networks uh don't directly give us that uh i guess uh reasons for hope are that research in this area is advancing very rapidly but but with relatively little theoretical understanding so i'm hoping that in the in the near future we'll understand much better why these methods are succeeding or failing and we will really be able to to address this this question of anomaly detection for robust ai thank you very much my name is ajakumar i'm a senior principal researcher at microsoft research ai today i will talk about the challenges that arise from having ai systems functioning in the open world i will also discuss directions towards improving the reliability and robustness of our systems guided by the work we are doing in the company towards developing responsible ai i don't need to tell this audience about the advances in the field of ai we see machines reaching human level parity or even exceeding it for tasks like speech recognition object recognition when these systems are evaluated on static benchmark test sets we are also seeing these systems mastering game games like go and pacman as ai systems are being used in domains that matter for our society like helping with legal decision making medical decision making or having self-driving cars on the roads we acknowledge that the behaviors of these systems in particular how they fail have consequences for our society recognizing the impact of ai systems for our society in 2016 microsoft releases principles for responsible ai these principles talk about having fair systems that are reliable and safe that respect people's privacy while being inclusive accountable and transparent however stating these principles is the easy part ensuring that company practices agree with these principles has been the challenge that i've been working on with many of my colleagues at microsoft research we organize our work in this space around the company wide initiative called ether these are the initiatives we have for developing responsible ai my efforts in this space has mainly focused on reliability and safety understanding the limitations of ai systems and thinking about scalable approaches for addressing them i'll start my talk today by discussing three main insights i gained from being involved in this company-wide initiative then i will briefly present three research projects that are guided by these insights and i hope you will see the connections the first insight i would like to share is that making progress in responsible ai stems from understanding the different limitations ai systems have the way they fail but not only in the lab conditions but also when they are deployed in the open world the way ai systems are evaluated through aggregate performance metrics today on static test sets hide the complexity of the real world in the real world we see that training data and models are almost always incomplete and lead to failures we also need to worry about adversarial attacks and the errors that come from the humans that these systems are interacting with understanding the different ways ai systems fail is important as we think of different ways to make ai systems more reliable and robust the second insight is that there is no silver bullet solution to address the different kind of limitations we see in the ai systems today however we see a variety of emerging techniques that can be used together in coordination towards this goal most work in the ai community focuses on algorithmic advances to improve model reliability and robustness and i'm confident other speakers in this session are going to be sharing work from the space the point i would like to make though is that it is important to acknowledge that ai systems do not exist and function in the world alone they exist in a larger ecosystem in which they are developed deployed and functioning we know that ai systems are developed by people they learn from data provided by people and they also work with people in the real world therefore opportunities to overcome the limitations of ai systems should also take into consideration how to empower the people involved in the development and deployment of ai systems finally the third insight i would like to share is that we need to align our efforts in the responsible ai space with the way these systems are developed in practice to understand the challenges that ml practitioners face in this space last year we did a large-scale survey with more than 500 machine learning engineers at microsoft and the findings were very interesting the survey revealed that the practitioners lack sufficient tools in particular for testing debugging and evaluating ai systems we see that concerns around reliable ai are not only about model development attention should be paid through the life cycle of an ii system from design to development to deployment and continuous monitoring and improvements of these systems so today after sharing these insights i will briefly talk about three research projects that are guided by these insights and observations the first project will be on explorations of ai limitations explorations of how these systems fail and what are the reasons behind it then i will talk about how we are turning our learnings into tools for developers to use for error analysis and finally i will talk about reliable reliability considerations on backward compatibility when models get updated during ai lifecycle let's start with blind spots of ai systems when we look closely into the performance of ai systems we see that in many systems failures are not random but they are in fact concentrated around some input spaces a great demonstration of this comes from the gender shay study performed by joe balomani and timnit gabriel on evaluating the performance of common gender prediction models across the groups of gender and skin color they showed that large performance discrepancies of these models can exist when they are evaluated across these different groups the question is why we are seeing such disproportional error rates why we are seeing so much higher high error rate for women with darker skin color compared to lighter skinned men in our work on blind spots we explore the role of data incompleteness in understanding disproportional error rates in ai systems so let's get to the fundamental assumption of supervised learning the fundamental assumption of supervised learning is that our training data is a complete representation of the real world concept we are trying to learn this assumption breaks down when we have blind spots in data so in this hypothetical example let's say we are developing a classifier of cats and dogs and in the world there are many different kinds of kids and dogs a mission learning classifier needs to learn about let's imagine that the training data lacks sufficient representation of small dogs but it has sufficient representation from small cats what the model ends up learning is associated every small animal with being a cat we call these types of errors unknown unknowns and the spaces with high concentration of unknown unknowns as blind spots of ai systems in ir in our triple ai paper we present a human in the loop technique for searching for unknown unknowns in unlabeled benchmark data the proposed technique involves two steps the first step is called descriptive space partitioning and the second step is a multi-arm-banded based approach for guiding the search in our paper we show that this technique can create an interpretable representation of blind spot spaces and can find unknown unknowns more efficiently than simple heuristics and you can find the details of both experimental results and the techniques in arts reply paper next i want to talk about how we are turning these insights from this triple ai paper into an easy to use tool for ml practitioners discovering the blind spots of ai models is the first step towards improving these models we really need to know where these models are failing to be able to fix them in fact after performance disparities in face recognition models was discovered by researchers the relevant teams at microsoft took the necessary steps to increase their data diversity for this application and the resulting models had close to equal performance across gender and skin color groups on benchmark data sets as our survey of ml practitioners have shown the challenge here is empowering the developers with the right tools so that they can discover these blind spots efficiently as a part of their daily routine of the way they evaluate ai systems in practice the current practice of evaluating ai models focus on aggregate metrics for example aggregate accuracy or aggregate auc which hide the structure behind how failures happen in these models in this example the aggregate evaluation may tell us that the model is 73 percent accurate but we don't really know where these errors are concentrated are they random or are they part of a subspace of the training of test data and most importantly what is the most effective way what is the most effective action to improve model performance to get this accuracy to a higher point these aggregate metrics hide all of this information what we try to do in the pandora project is using interpretable techniques to discover how the model performance varies across the different subspaces of data the goal is developing these techniques into a tool set that help engineers accelerate the development iterations by identifying errors faster more rigorously and systematically the workflow for of our tool set is pretty simple it starts with a benchmark data set if the data already has human interpretable attributes in them that can correlate with the errors that is great otherwise we recommend a future augmentation step to enrich the data set with such with such attributes we run the ml model we want to evaluate on the benchmark data set and collect error labels indicating which instances the ml model is correct and which mod which instances the map model is making mistakes on then we use interpretable machine learning techniques to automatically learn patterns that can best separate the errors from the successes the tool provides views at different levels to help developers explore what the error spaces are and develop hypotheses and then come up with actions to improve these models let me demonstrate how the tool works for error analysis of the gender chase data set where the data set is augmented with high level human interpretable features although detecting gender from faced images might not be a great idea after all we'll look into these models for the sake of understanding the different types of errors that can creep into the model because the training data is not representative enough okay here we see a view from our tool on the left side we see different features that are going to be used for exploring the errors and here you see ground truth features like gender and skin tone and also automatically inferred features about the facial hair hair lengths accessories age and so forth and we generate these predicted features from azure cognitive services on the right hand side we see a decision tree that was trained to predict system failures based on such features basically what the tree is doing is it is trying to separate success cases from failure cases the notes of the each note the color of the each note represents the error for that particular note and we can follow the critical error path by following the darker shades of red to see how the error rate in this model is increasing when we get into different subspaces so the overall error rate of the model on this data set is around five percent but it becomes higher and higher if you are a woman now we have the error rates of 11.48 percent and if you have no makeup on now the error rate becomes 23 and if you have short hair and if you are not smiling in fact error here increases from five percent to 35 percent and it is revealing these hidden conditions of failure that um the low level aggregate analysis would not be able to discover of course not all the important features that relate to errors are not part of this dataset so it is very important to empower the developer to be interactive with the tool to develop hypotheses so for each node of the tree we enable seeing the failure and success instances side by side so that developers can engage in an interactive way with analyzing errors developing hypotheses and thinking of ways for improvement finally on the last project i want to discuss that model performance is is a consideration through the life cycle of an ai system and we need to pay attention to the backward compatibility of ai systems during updates to ensure such reliability reiterating what i said earlier in real-world systems ai models do not function in isolation they are part of a larger ecosystem where they work with other ml components or traditional software components interact with other systems or with people this integrative nature brings up additional considerations for the reliability of ai systems as ai models and other components take dependencies people build expectations and trust on the ai systems as they are working together this issue of how these dependencies trust and expectations are affected the issue is all of these dependencies trust and expectations are affected when a model gets updated ml practitioners update models to improve model performance for example ml practitioners may increase the data set size or add more features or may decide to change the architecture in the hope of achieving higher accuracy from these models however increased aggregate performance does not mean that the new model is backwards compatible with the old model that was functioning in the world already so what i mean by the backwards compatibility is that the updated model can make new types of errors that the old model was not making breaking backward compatibility thus breaking trust and dependencies these sort of compatibility issues introduce risks of reliability why first when we are introducing new previously unseen errors this can break the error handling we have deployed or developed in our systems imagine models later in the pipeline are trained to actually handle the errors of the earlier components when earlier models get updated and now they are making new types of errors the error handling of the later models become invalid and now errors can propagate through the integrative pipeline and result into errors second incompatible updates might create maintenance issues especially when components take dependencies on each other for example imagine the later components were trained to optimize the accuracy knowing about the errors of the earlier models now the earlier models get updated we need to update the whole pipeline which may become infeasible and finally when we are building putting models into the pipeline without backwards compatibility this may actually require doing all the testing reliability tests all over again which increases the workload we have for testing backward compatibility issues also arise in human ai collaboration as well when ai systems work with people users develop models of trust issues with backward compatibility can break user trust as an example this particular article discusses how users of tesla cars develop mental models about when to trust these self-driving car functionalities and when model updates happen in a backward compatib in a non-backward compatible way this can break user trust and create usability issues the main message from this discussion is that improving accuracy is not the only metric one should care about when we are updating our ai models in systems here we propose looking at compatibility scores as well in order to understand whether the new version of the model has introduced new errors this definition is saying that the updated model is fully compatible with the older model if all the instances that the old model was getting right the new model is getting right as well however one can ask isn't backward compatibility something that comes naturally in machine learning especially if the update is simple like just adding more training data into um into our systems in fact we see that compatibility is not built in here we experimented with three different data sets in the domain of high stake decision making respectively recitivism created risk more credit risk mortality at hospitals and so forth the update here is as simple as just increasing the size of the training set and we see that model performance improves with an update as expected that is what we expect but there are even cases when compatibility can be as low as 40 percent what 40 compatibility means is that the model is not making a mistake on 60 percent of the instances it was probably it was before getting right creating a big issue on expectations and trust when these systems are updated in the real world now that we have quantified compatibility for model updates we can incorporate that metric into our model trading as a regularization factor to penalize incompatible updates and actually get more compatible updates as a result of model training in this new formulated ros loss lambda controls a trade-off between model compatibility and final model accuracy by varying the lambda value from low to high we can create different classifiers that vary in the performance versus compatibility spectrum here the y-axis denotes the performance of the updated model and the x-axis denotes the compatibility score and the dashed line represents the performance of the original classifier when we set lambda to a very low value now we are not putting a lot of emphasis on compatibility so we are getting a high performance model that has a low compatibility score if we set lambda to a high value then we get to the point on the bottom right corner where it has a high compatibility almost a perfect compatibility but we are not really getting the accuracy gains we would expect to get from a model update but when we use intermediate values for lambda we get these curves for different regularization losses we have tried in this project by increasing the weight of the lambda we can increase the compatibility and see the effect we have on the final accuracy of the classifier this indicates that there exists a trade-off between compatibility and model performance of the update but also we can see a flat region especially if you look at the blue curve where we can gain a lot of compatibility benefits without sacrificing any accuracy in the updated model so we are actually getting compatibility back for free when we introduced this regularization factor into our updates this gives developers a tool they can use to minimize the number of new errors while maintaining performance to conclude we have to acknowledge our responsibility in developing ai systems since the way these systems function in the world have real consequences for our society understanding the limitations of existing techniques processes is an important step in finding ways towards more reliable robust and safe ai systems while it's important to invest in algorithmic advances in this space we need to consider the larger ecosystem that are involved in training developing and maintaining these systems and also invest in tools design practices design guidelines frameworks for empowering both ai developers and users of ai systems thank you for listening and i'm looking forward to the panel discussion hello everyone i am very stoked to be here and uh i'm going to talk today about implementing uh safe and reliable ml and especially new developments in the air in this research area in the last few years as machine learning is making its way into more and more fields where uh non-um especially fields like healthcare and education with a massive consequences uh into uh you know life decisions very crucial life decisions are being made using uh software it's very important for us to make sure that the software implementing works correctly works the way we expect it to behave it's uh not biased in any way um and so this notion of uh you know thinking about how we get there is is crucial and so as examples um here there i'm showing you a study that was done out of mount sinai where they showed extra x-ray images of a chess scan where you're using the x-ray image to automatically identify patients who have pneumonia or don't have pneumonia and turns out you could actually develop a really nice algorithm that could identify these uh patients with the ammonia diagnosis very accurately but the challenge is uh you know the way they developed it it was very easy to obtain an algorithm that was very sensitive to a specific hospital so it worked really well in one and two but didn't work so well in a third hospital and then you looked into why turns out it's because it was learning strange things about the scanner that was used being within a particular hospital that then generalized as you move from one hospital to another another example is this is um predictive policing in l.a where the police department there was using machine learning algorithms to identify what are neighborhoods where they could monitor more um intensively and if they could predict which neighborhoods are likely to have crime on a particular day they could start you know preparing the workforce to go there but the challenge was that as they did this they were introducing feedback loops in their data where the more they monitor the more uh you know what more of the workforce they sent to specific neighborhoods more the more data they got the more the ml algorithm learned that those are the areas with more crime and so there was this unfortunate reinforcing loop third very simple example like a very common example in machine learning where you know uh on the left and right these are two images of a panda which to most humans look exactly the same but oddly enough you know a little bit of noise which is what you see the image in the center if you add that little bit of noise to the panda image you produce image to be um you know as the output which again like i said humans can't tell the difference but for machines they would behave very differently so the image on the left it thinks um you know this image is definitely a panda and on the right it thinks this image is a given very confidently which is very odd and uh would be very uh confusing to a human expert user who would be using say machine learning to start making decisions so i gave you like a collection of decisions like here's a fourth last example uh i mean there are many more here's an example where uh you know this is in a hospital setting we're using data to be able to identify and predict adverse events like sepsis but turns out over time as practice patterns for doctors how they prescribe certain labs and procedures change these models model behaviors don't generalize or models can be um can model performance can deteriorate and perform poorly if for example they're very used to relying on a particular laptop as being then over time practice patterns change and those lab tests are not there um and so how do you think about um you know building machine learning uh systems or that are actually robust to these kinds of changes in the very worst case we want to know when these kinds of issues are happening so that if we if it does occur uh you know it isn't sort of leading to incorrect uh judgments or incorrect uh uh predictions or incorrect uh decision uh support um causing you know obviously in healthcare this could be some this could do something as bad as actually like unfortunately harming patients so this introduces the first most important concept which i think in machine learning a lot of excitement has been around model development model training you know you take a data set and you develop and deploy a model against that data set but what we've missed which is very common in other areas of engineering like civil engineering nuclear engineering these engineering fields where safety uh being safety um you know these are disciplines where safety is crucial and you're designing systems in these disciplines there's a whole understanding of how do you design for reliability and so the equivalent concept here is let's think about what is ml reliability so in the ml sense it's you know how do we build ml components uh such that they're be first we understand how should they behave secondly then they're behaving the way we expect it to be here and we understand when is it likely to misbehave when is it robust but is it not robust when is it behaving in biased ways and because machine learning software or ai software depends a lot on like data points that are inputs that we have no control over the unpredictable or stochastic nature of those inputs make this notion of studying ml reliability quite hard different from sort of like deterministic software uv you can exactly dictate how the software should perform in particular settings and emerald's reliability right now is getting confused by this notion of like uh you know the kinds of bias that emerge out of like poor system design so like fairness and ethics so we still have to think hard about when we're going in and designing a system thinking about is the system that we're designing fair and ethical you know we need to understand uh to be able to answer whether um is fair and ethical like what does it mean to be fair you know how do we like are there any harmful unintended consequences from us designing in a particular way that makes it unfair unfair and reliability goes one step beyond that which is once we've designed a system that we think like we want the system to behave in a particular way we need to be able to guarantee that it behaves that way in the uh you know when it's being applied in an everyday setting and this is like other system requirements like privacy security and other system failures we are used to thinking about so in thinking through reliability what i uh you know and again borrowing from other fields of reliability engineering and other aspects of engineering there are three key areas first is failure prevention so how do you um ensure like build your systems in order to be more failure-proofed which means you're learning from data and you're designing systems to avoid you know your front identifying high risk failure points and you're designing them to be uh robust to those failure points the second aspect of this is test time monitoring which is as you've deployed them in real time you're able to monitor and identify when failures might exist in real time and be able to do correction and then the third is maintenance which is how do you actually maintain these systems over time now these are three areas that we i think actually in the field of machine learning last few years because new whole new threads of research starting to emerge but overall we don't approach machine learning research this way in machine learning deployment and applications this way but um if we and i think in each of these areas there's numerous uh possibilities for actually uh improving developing new algorithms that can allow us to make these systems more robust so let's now imagine we have this framework i want to talk a little bit about one of these areas like failure prevention and in particular what are some ideas uh we're thinking about in failure prevention so can we develop learning methods that proactively learn to avoid failures so one big misunderstanding and this especially to outside the field of machine learning when the practitioner is employing machine learning like for example in healthcare there's this discourse that like ai and ml are inherently biased ai and ml are not inherently biased these are just what algorithms do is they take data as inputs and they produce certain outputs nothing about this particular process for construction is biased it's the way we design it that can yield biased algorithms so for example if people naively take some data don't check for how good the data is don't check for how biased these data are and they don't do any kind of correction then you're going to get into producing algorithms that are biased so you can actually train algorithms uh to anticipate and avoid particular types of failure and um there are several threads of research here so for example there's these types of biases like the panda problem i mentioned is this notion of those are called algorithmic blind spots specific types of noise where the algorithm misbehaves and there are whole learning algorithms or paradigms that allow you to train and develop models that are robust to that kind of failure so adversarial robustness is that what's called similarly shift-stable models that are more immune to failures so another type of failure like the predictive policing example the practice patterns changing in hospitals those are examples of uh data set shift and failures due to data set shifts so there are shifts in the underlying population or feature sets or the practice patterns and as a result when these shifts occur naively trained algorithms are not immune to them they start you know model performance deteriorates but you can train these notions of shift stable models that are more immune to these kinds of failures and i point to a tutorial below that sort of overviews kind of overall these three categories and examples of types of algorithms people have been pursuing in each of these areas i've also listed if you're curious some example papers if you want to learn more about how people are using notions of causality and notions of like techniques algorithmic techniques for being able to prevent failures and learning these kinds of table models finally sometimes algorithms are biased because of poor study design or human error so for instance if we're not careful in designing the right kinds of algorithms the result saying you know uh products are going to be biased for example here's a recent paper that showed that you know optum which is a healthcare company that was using algorithms for the last 10 years they've been using these algorithms to determine in the you know among the sickest patients or like the high risk high cost patients which patients should get extra assistance in order to prevent them from having complications that end up leading them to the hospital and turns out the algorithms they were using the way they developed it was racially biased so it was much more likely to prescribe care to white patients than black patients and part of it had to do with something if they had i realized was easily correctable it's just that at the time of development they didn't realize that the um the objective function they were using for training was emphasizing identifying high cost users instead of the thickest patients another example of human error is you know a very well-sided example where face recognition algorithms that people are developing on data sets that have a lot more white faces than black faces or uh more men than women in this example they showed uh you know lighter males and lighter females had much more coverage than darker males darker females especially darker females and as a result uh the performance of the algorithm was very varied and it uh was very it performed much more poorly in recognizing uh black female faces and so this obviously again at the time if they'd realized it could be corrected for but so some of it is by enriching your data set some of it is by better study design like in the optic example and then a lot of it in more sudden cases like practice pattern changes like not all of it which you can anticipate you can design more clever algorithms that are more stable to those kinds of changes this is a growing field with many new techniques emerging and i think is crucial as you're employing these kinds of methods in your own setting to think about uh the degree to which the software you're deploying is robust to the different kinds of failures that can occur in practice and then finally a third thing and this is especially important for people when they're first moving from research into translation which is um you know developing an algorithm for a new use case where you can take your data you scrub your data you clean your data and you're training a first cut of an algorithm and producing a first what i call the auc you know the um performance characteristic like an auc sensitivity specificity first time and you can do a couple of tweaks and get to a pretty happy place where your overall metric like an auc looks really nice but to get from there to an actual usable system where you have real users like so i'll give you an example in our setting you know we're building and deploying algorithms that are being used by clinicians like doctors and nurses in assisting with decisions around complications that providers often miss doctors and nurses often miss and so what we found is you know you can take a data set of patients and show pretty high quality performance but then when you start deploying it in the practical setting you start realizing that what it takes to get pretty good auc to actually case level accuracy and case level actionability is actually a whole lot harder i would say you know from my standpoint now when i think of going and attacking a new use case area i think of it as maybe if i have all of my algorithmic tools available to me let's say i take one month to develop that initial result that looks pretty good going from there to something that actually is good enough on a daily basis that doctors and nurses would trust it is more like maybe a year and a half worth of effort after that so huge amount of effort in going from and and this is sort of a misconception where if you've never actually translated these algorithms something that's often missed but it's super crucial again when we think of areas where right healthcare education social justice where there's real consequences they're not being right and if your users are expert users who are really busy they're going to you know probably uh not trust it and not adopt it if they don't feel the quality in their daily experience is very good so again in this aspect of uh validation i think there's enormous research opportunity because you know we spent in a field a whole lot of time thinking about new algorithmic training methods we've spent not as much time thinking about how do we you know improve model validation how do we improve monitoring at test time and so uh here there are some um like lots of super interesting new research coming out a couple of papers i note at the bottom where you know uh there are ways in which um you know obviously one way to think about it is as you deploy the system have a team of humans review every so often a case that comes out maybe like uh so that you can identify um you know if they see anything maybe one in five cases are reviewed this is obviously very expensive and very cumbersome but uh critical and so then algorithmically maybe the way to make this whole process smarter is if you could identify cases the way you're more likely to be wrong or the system is more likely to be wrong or system is more likely to produce untrustworthy results now you can go and only review those so that that's sort of this notion of what i call test time auditing or real-time auditing of outputs and individual predictions and a second algorithm can run and monitor to see if it can be trusted or not and you can audit to see if it's correct using a human in the loop um and there's beautiful work going on in the field where people are you know trying to develop ways to make the audit simpler and easier by identifying what would be the reason you know by building interpretability and transparency into the system uh finally i think it's so important we bring discipline into how we validate i think we should really be treating algorithms like prescription drugs where we conduct forward-looking trials we uh one of the challenges here is that algorithms unlike drugs aren't frozen they evolve over time so we really need much more sophisticated frameworks to be able to monitor for uh how good this algorithm is how is it performing over time as the world around it evolves as practice patterns evolve as the data evolves the population underneath that's being monitored evolve and for us to be able to identify in smart ways when the system performance may be deteriorating so that we can actually do something to fix it with that i'll end here are some example papers if you're curious to read some more from our lab and with that thank you okay here we are back again uh thanks everyone for the stay talks um i thought they were very much aligned and uh they were all bringing different perspectives of how we should be seeing reliability and robustness um so why don't we start by warming up with the definition of robustness i know it came um came up um yesterday in the security and machine learning session and there was sort of agreement in that i know that we are having a hard time to define robustness um i don't know what your thoughts are and maybe if you have on top of your mind aspects of robustness but maybe we are not paying enough attention and that we want to bring up so that we will have them going out later um let's start from uh maybe tom i guess i would start by talking about uh the distinction that i think zucchini raised between um uh anticipating problems and and making sure the system is invariant to them versus doing the runtime monitoring or maybe it also came up in his talk um but uh you know the uh the anomaly detection stuff that i was talking about is really um uh for runtime monitoring and so it's it doesn't make the machine learning system any more uh robust than it was it only i mean it tries to close the barn door after the horses have escaped in some sense right uh uh and detect that there's a problem at run time uh so i would define robustness as the former that that we identify some collection of uh of uh threats or challenges or problems and we make our system uh invariant to those so that it won't be it won't be affected by them and i think a sort of deep uh intellectual question is um sometimes it seems that by making the system robust to certain problems we as a side effect gain robustness to other things that we didn't anticipate whereas other times when we make the system robust we actually induce brittleness in some other direction and uh and this is a big mystery right now so there was a talk at icml that showed that uh you know one way to try to make a machine learning system more robust to these adversarial examples like the panda is to uh force the system to be invariant in some sort of a epsilon ball around each data point so if the data point was perturbed then it would still give the same answer but what this tends to do is make the system very confident of its of its classifications in that ball and then as soon as you step outside the ball uh you you it it's still confident but now it's it's no longer robust so you get sort of a bad combination so there was some work um this is matthias hein and his students on trying to get the confidence to drop as you get close to the to the edge of the ball so that it doesn't it's not overconfident outside the ball so this business after continuous testing and monitoring and data was actually talking about this in in her top and even in the it came up in the chat and the questions about how this is an iterative process and maybe there is no silver bullet but we need to think about different tools yeah there's definitely a difference between robustness of machine learning models versus robustness of the system we are developing i think the robustness of emission learning classifier is a very technical definition where you talk about how your training set generalizes to unseen test instances and what that really means however when we actually get into real world deployments then we are not only thinking about one decision boundary and the robustness around that decision boundary we are actually thinking about the whole ai development life cycle and the data and the people that are involved in that process and the many ways that people have to be supporting that process with the outlier detection methods that tom was talking about or the careful thinking that suture was talking about in her talk about like what it really means or it seems to be biased or unrobust in the real world implementation robustness actually is a very human-centered context concept because there are many things that machine learning models can be robust to but the things we really care about are the human expectations what are the expectations of the users that are using these systems and what do they expect as reliable because as humans we get full too if you change all the pixels on an image for example to from like a gibbon to a panda the human will not get that image right as well so there is there is definitely an expectation question in our definition of robustness and what are the cases that we expect our systems to work reliably and what are the kind of shifts that we might be able to tolerate because we know that these are just reasonable areas that all organisms are going to be making and there is uh this other aspect that um suture was mentioning towards the end of the talk about how certain changes in the environment or the way how people are using our systems may introduce these shifts that are not really initiated by the data that we have collected beforehand but from from the interaction itself and it seems like this type of shift is more subtle perhaps it doesn't even happen in one single day it may take some time until we see it um based on your experience would you like when when you deploy this uh resistance in in the real world um what do you think we have in hand now and what we should do next to do this monitoring yeah so i want to start with the first question you asked and kind of building on what uh tommy you just said um i think part of what we lack as if you know when we're thinking about end-to-end systems and deploying n20i systems which is really sort of the goal at the end of the day right we're building algorithms in order to be effective in influencing the real world and succeeding and solving life problems better faster more effectively um i feel like thus far our research prolongs has been all around understanding performance on a data set which is where we've come up with a lot of the terminology and as soon as you move from one research data set to the real world like you just said you have to think about end-to-end systems and i think we lack the framework for even thinking through what are the components here where you know what what are the things that matter what should we be measuring how should we be measuring what uh we look for to test whether something is good or not good and i'm using the word good and not good because good and not good is um sort of a qualitative term you know how to think about i feel like robustness is a very statistical concept uh historically has been narrowly applied in a very particular way and we need both qualitative and quantitative understanding of good in our scenario right and so when we look at other fields like other safety critical fields like nuclear engineering civil engineering where if you screw up and you build things incorrectly it costs lives people have whole disciplines around it so they think about this concept of reliability and what does it mean for a system to be reliable and what are the phases or processes people put in place in order to make things reliable and they talk about sort of these three pieces one is failure prevention the ability to be able to build learning algorithms that prevent failure which speaks to you know our desire to construct these robust learning algorithms second is the ability to monitor over time um and then third is to adapt over time and all of the above are absolutely critical uh and i think right now we sort of still are very much like limited to this one data set and if you look at the field as a whole of like failure prevention you know reliability or as a whole in reliability monitoring as a whole then we start to think beyond just the learning piece and thinking about ongoing monitoring auditing and the kinds of ideas um and for each phase there may be totally different metrics so for example um in an area like when we're trying to build an algorithm to be robust for a particular domain we want to anticipate what are all the failure points and to build the learning algorithm that prevents those failure points but the reality is the world is continuously shifting and if the world is continuously shifting we want to be also be able to build monitoring tools that allow us to identify on the fly when things are going wrong so now to just answer very to your very specific second question i think the tools are very domain dependent there are sort of broad tools like you could put a way by which you measure auc on a continuous basis or predictive performance in a continuous basis you could measure stats about your data on a continuous basis but then there's sort of um domain specific measures which i think are at the end of the day the most important because those are motivated by you know where making getting things wrong is actually very costly and problematic that um resonate i also have to i'm gonna be just for a minute i have to escape but i'll be back i'm i'm here you know i would like to add one thing to what suji said about being you know evaluating ourselves on static data sets because this is what we report in our papers this is what is celebrated in the press when we you know get a benchmarking success however that creates a lot of expectations from the practitioners of ai who are actually looking into the academic world and asking the question of what are the big trends what are the big innovations that are happening right now so that i should get them into my system immediately to get that competitive advantage and they are setting expectations based on numbers that are com you know computed on that static single data sets and where everything looks great where papers do not really discuss what happens when there are distribution shifts or the training data that do not exactly match the real world use case that that practitioner have in mind so us not answering the questions when we are reporting our successes on robustness reliability actually set strong expectations for the practitioners of machine learning and can increase the gap between what academics talk about and what really happens in practice yeah i i just to uh echo some of these points um in terms of what should we be monitoring after deployment i think that's also a very tricky question because we've seen that after we've deployed some systems we the the whole technical community uh there were unexpected uh feedback loops set up for instance in recommender systems which were formulated as making a one-shot recommendation but we're deployed in a loop um and so you could and we it didn't occur to us to measure the dynamics of that loop over time um and i i i i really loved uh has decision tree showing you know where mistakes are being made and that's an example of something we should always be um uh uh monitoring to see if we're expecting to see certain mistakes in certain areas uh and and uh if those are for seeing mistakes in new places our overall error rate might be fine and our uh our area under the roc curve or whatever might be fine but the but the uh distribution of our mistakes might be shifting so these are these are uh tricky things i i um i don't know how to uh hand the ball to suchi but one of the things i think is really beautiful in the work that she mentioned uh on this causal transportability is uh really methodologically asking the the developers and the subject matter experts at the time we're designing the system what do you think are the ways the the world is likely to change and let's try to elicit those early on so maybe you can talk a bit more about that sushi sure tom um yeah thanks for bringing up that work so i think overall thus far in learning algorithms the way we thought of like generally the way machine learning as a field has approached it is we take a data set and we have some aerometric in mind and with that error metric in mind we fit in learning algorithm to optimize it but this is kind of shifting the way we think about learning to say in addition to optimizing for performance on this fixed data set we also want to think about what is the system actually trying to do in which world is it looking to operate and in that world when it's looking to operate can you think about what are some properties or in invariances it should have like it should behave um it should be okay if certain types of you know so i'll give you an example in the context of medicine uh let's say i'm developing a way to identify a disease like sepsis or pneumonia i want my algorithm on the system to be good at diagnosing this even if you know there's some change in the way providers order certain types of labs or order certain types of medications or order certain types of and why is this important because you know people change all the time behaviors change and if some change happens that my algorithm is susceptible to that i wasn't anticipating then suddenly my diagnostic performance deteriorates without me realizing and that could really cost lives so if you know upfront there are all sorts of like changes in behavior changes and ordering patterns changes in subpopulations that we know it should be a robust invariant too you can build that in as uh properties around uh you know in variances you hold to see and develop learning algorithms that are then invariant to it now if you extend this idea all the way to you know to think through what are all the causally you know when you think of like invariance you can start thinking of like what is just causally true like you know um like when you get a bacterial strain that leads to bacterial flow you can't get a bacterial flu without a bacterial strain there are just some truths in the world and if you work through all the troops in the world and enforce those truths in the world what turns out is you just get more robust learning algorithms and learning like learned outputs and so the whole idea behind transportability is this notion of identifying what are truths in the world that you know to be true and you would expect to be true and you would expect to be invariant too those are often causal facts in nature and so if you can enlist with domain experts those causal facts and build algorithms that enforce them you'll as a result end up getting something that is more robust so that's sort of the concept and turns out you can actually prove a lot of things about this you can show bounds in terms of uh you know not they're they're sort of these algorithms are stable they behave well they perform well um in terms of when you shift and change the data set around so there is a question to say about like how do we discover together these truths about the world um because it looks like we have a separation now in the way like where machine learning is created and built and deployed and where who is using it so if you give me a data set on medical data and i am the one who has to discover these truths i will miserably fail at it because i don't know about this stuff how how do we work together with uh professionals and so that we can encode these things back and forth so i think most real world systems when you de like i mean when we developed learning algorithms in the research community initially we used to think of this as look here's my data set it performs really well that's exciting but the game is changing as we're moving from just performing well on a data set you're wanting to build something that is solving an end-to-end task that has complex moving parts then very naturally like the idea agent tom talked about of like monitoring is very important and as you're starting to do ongoing monitoring you naturally discover places where the system misbehaves and as you discover places where the system misbehaves you know you'll end up discovering all sorts of issues so for example in the work we did over the last four years as we transitioned to starting to think about real-world deployments of some of our algorithms we developed for medical diagnostics as more physicians started using them we started discovering what were common failure points those failure points then motivated ways to make the algorithms more robust and then furthermore um i think tom talked about this idea of outlier detection on the fly outlier detection uh two years ago um in 2019 being kim published this paper on this notion of like uh you know identifying whether a prediction can be trusted or not and uh in the presentation i also talk about sort of a piece of work we did similar to that based on identifying like real-time auditing the ability to take you know in real time so it's almost like you have a learning algorithm that is making real-time predictions but then you have a second learning algorithm or second algorithm whose whole job is to monitor the predictions of this algorithm and in monitoring the predictions it's determining whether you can be trusted or not and in determining whether it can be trusted or not in its own right it can leverage all sorts of information including susceptibility to data such shift whether you have enough training points in your data set to even believe you could be right and and so on so forth and those are all things i think we just need richer end-to-end infrastructures i mean it kind of changes the game a little bit in ai research to move closer to what you know systems people do in a way right they have rich robust systems infrastructure to be able to make that happen and now we have algorithms and systems meeting and in fact this is also something that sorry did i interrupt you no no i'm actually curious since tom's sort of seen the field evolve over time i'm actually curious like does this resonate like how do we do research like one thing i've sort of as i've watched this is how do we do successfully research as investigators in this field when it's becoming more and more in resource intensive infrastructure intensive and to do it really well you really kind of need to have it invisibility and in my scenario that's really meant a pretty big and ambitious research group with like 30 or so odd people at one point that was like the collaborators etc managing it and now some of it is part of you know the company bayesian that i manage with like a fair number of people there so um but i think it feels a little daunting as a an individual investigator to try to do all that so i'm curious like you know if you have any ideas to share for young researchers in terms of how should they think about this well i yeah i don't i don't know um uh but certainly one one big trend i'm seeing is uh you know until today we we still mostly test our machine learning systems by collecting test data but uh but i think we're seeing a move toward uh building a more active probing systems that that in some sense are generating more informative tests than we can collect just by collecting data i mean this is very clear in the self-driving car world where you might have to drive billions of hours of of actual real world uh miles on or miles on the road whether um to find those rare cases but maybe uh using computer graphics simulation and and uh and probing up programming kind of agent-based models you can try to create um these corner cases and uncover the weaknesses of the system um and so i uh alan ewell at johns hopkins has also been pushing this idea of uh that it's time for machine learning to move beyond data in some sense uh particularly when we think about how we test our systems and he works in computer vision where you can also imagine using all the tools of computer graphics to to synthesize uh images that will exercise the the our systems in important ways so so that's a big trend i see um but uh but but now i mean it also then again forces you to say well um if i were trying to attack the system in some sense puts you in the adversarial mindset right i think most machine learning people have been in the oh i'm going for 99 correct and now now it's sort of saying how can i break it is much more of a reliability engineer's mindset but now i need maybe another person on my staff to be kind of the bad guy to think about that so yeah i guess i'm agreeing that that it means bigger bigger teams i know if you have started to do some work with the ether committee to to provide guidance to engineers on on this aspect um yeah and i resonate both with tom's comments and such as comments about you know what do we need to do who we need to bring together to be able to move our systems to be reliable and safe i think one thing one point i would like to make is that it is too late to include domain experts once we have these systems out there they should be working with us early on from the design stages because even deciding things on what is the right data to focus on and what is the right objective function and how i think tom made this point in the chat like the costs and what are the costs of you know different kind of mistakes and of course the user interaction is a big part of it too because in many ways we can't tolerate some of the shortcomings of our models if we have the right user interaction um so we cannot really decouple what we are showing to the users with what these systems are doing so that thinking of domain expertise and what we are really trying to do cannot really come at the testing stage and the continuous improvement it needs to come there of course but it needs to come much earlier the second point i would like to make is that this interaction between ai researchers and system researchers i think is going to be more important going forward for example we have started doing some work on adversarial ai trying to look at real systems and trying to understand what kind of mitigations are possible to prevent adversarial attacks the first observation we have is that the assumptions that the machine learning community does around what an adversarial example looks like do naturally translate into the real world in the real world perturbations are very different than the l2 norm around the instance the patches the stickers you know it's just the attack modes are very different in addition to that we cannot really solve these problems by just you know changing our algorithms right now at the moment it's just all of those algorithmic techniques are broken so what we come up with is actually thinking about the system level mitigations not really machine learning mitigations but system level mitigations about how the models and the training datas are stored and encrypted and how can we thinking think about the adding different modules into our machine learning systems in a way that we can do real-time outlier detection and anomaly detection and have anti-spoofing in the loop and have continuous learning for those components so just to reiterate that point that i believe there is going to be a lot more work we need to do to think like system researchers and think about how our system architecture influences the risks we have in the ai systems and just like a shameless plug on this we just released an archive paper on the real world safety risks we see and the these connections between machine learning and systems so you can find that on archive just if you want to read a little bit about our thoughts in the space a lot of these um things that that we mentioned today seem like they are converging towards like either model or system understanding and for a long time this problem of like understanding the model knowing what it's doing which features it's using and how it will behave has been in the interpretability and explanation domain which is also the theme of these two sessions um this morning um but somehow interpretability and robustness are not necessarily um unrelated to each other uh we see a lot of connections um as interpretability being either a prerequisite or having a robust model or the other way around um what do you think we're missing to link this these two aspects or is there any new approach to interpretability that focuses more into the system understanding rather than model understanding well certainly we find when we put an anomaly detector in as a second system that's monitoring the first system um the reality of anomaly detection is you get many false alarms and uh so you need the um so so you it's wonderful if you can get human feedback at that point to say oh that's not real that's not a problem oh this one is a problem um and uh uh so we had a paper um uh uh the with the um by my my colleague alan fern and his student omran siddiqui at kdd maybe two years ago now um on how to harness the feedback during that that anomaly detection process now obviously you now you want to give an explanation to the user here's here's an anomaly and here's why i think it's an anomaly and then then the then the user can uh can say oh yes you're right i mean like you're doing fraud detection for instance uh and you see this and you say oh yes i definitely need to to start an investigation on that case or no no this is just a typical you know thing because of course the human has a lot more contextual knowledge than the computer um there was a nice uh talk at icml last week from uh some folks at google on uh their smart buildings and they have anomaly detectors monitoring all the sensors in the buildings and uh and then uh you're right those alarms come up they give them an explanation and then they decide to create a trouble ticket or not to to go fix that so um yeah so that uh yeah i don't know that just directly relevant to your question but but definitely uh it's not at all useful to say well here's an anomaly um you know it's your problem you have to say why you think it's unusual um yeah and i assume the same thing h.a goes for saying here you made a mistake okay but uh the the classifier made an error but uh but now the person needs to be able to say well okay why did it make you have to diagnose the cause of the error before you can fix it so i i want to add to tom's point though because it's not um i think you can think of it as like develop a time to success so if you know exactly where the error is you're able to identify what the cause of the error is you're able to pinpoint exactly what you need to do to fix it then they're going to be super effective and efficient and kind of tackling the issue versus if you just tell them i think there's a problem they're going to be hunting all over the place but i still think to me it feels like there isn't a white and black answer of like like right now in the interpretability dialogue i feel like a lot of the answers interpretability is the answer to all of man's problems and i don't think it is like sometimes you can build interpretable systems sometimes you can't and sometimes interpretable systems give you insights that are totally wrong because it's interpretable but only in the sense that it seemingly tells you what the answer is but it isn't exactly the answer it's even the wrong root cause so from my standpoint i think we kind of there's like the soft metric we're trying to optimize which is how do we become effective in debugging these systems faster and faster which means any and while also making it safer and safer machines anything that allows us to identify problems is great anything that allows us to identify those problems and then tackle those problems faster is great and the whole thing is a continuum and we will need probably a whole ton of techniques to get through this you know broad array of like very qualitatively broad set of goals so i think yeah i just want to make the point that it really comes to how we define robustness like come to the beginning of the of the discussion right if we define robustness to be a human-centered concept where we want to be able to reliable to user expectations and have developers in the loop to create this continuous feedback loop that our systems can gradually get better we have to build the interface between our systems and users and developers and data providers and i think interpretable techniques are going to be important in building that interface layer however i completely agree with such that just thinking about any interpretability technique as a patch to solve problems we are running to is not going to be getting us there we have to be really guided by what do we aim to gain from interpretability and be really careful in that thinking instead of just saying that through interpretability in there and your problems are going to be user problems because i don't think that's what we we wanna be wanna impose into the community yeah i like to say that the interpretability is just like the stack back trace into debugger it's the first step but you still have to figure out what's what's really going on right and perhaps we have to figure out what is going on with the other components in the system to air provenance going back to the data problem and systems for interviews and and first of all um so i wish we could talk longer and that is this conversation has been uh really great um we will have however to switch to the next session which is happening at 11 we'll have a break of 18 minutes now um the next session is going to be on interpretability and explanation um and rich marijuana is leading the session with a great line of speakers i want to thank suchi tom and ajay for being with us today and for sharing their their thoughts and making us think more deeply about system reliability and robustness together with machine learning thanks thank you thank you for organizing okay you 