 So for our next panel, I teased a little bit before the break. It's going to be about human-centered AI and the role of building trust with your users, understanding your users, and how critical that is to building impactful and responsible products, which we heard a lot about this morning. It's going to be hosted by Di Dang, who is a Design Advocate on Google's People + AI Research team, and I'm especially happy to have Di here because she's actually been an integral part of this program. She's personally mentored many of the teams you’ll see here. And she also traveled with us to London to deliver a workshop for them in one of our boot camps. So welcome Di and welcome panelists. Hello everyone. Welcome. Thank you for being here. And thank you Mollie for that lovely introduction. My name is Di, and I work with the People + AI Research team. Essentially we're focused on advancing the research and design of people-centered AI systems and I'm really excited for all of you to be here today. So as we go deeper into our panel, we've heard from Brigitte, we've heard from Tulsee, and Nancy earlier, how important it is to build user trust, right? and we have also gotten some tips in terms of what is the data collection practices around that. So let's go one step further now and talk about how do you design for user trust by exposing certain design decisions in the UX/UI of your application or your product itself? And so that's what we'll be learning from our panelists who I’ll briefly introduce here. So with us today, we have Azza El Hayek. She is a - I'm so excited about your role because it’s a very rare role or a very rare title that I've come across in my own experience. Azza works as a Human-centered Design Specialist at Skilllab which is an employment services platform and prior to joining Skilllab Azza worked at the UN Organization for Food and Agriculture. Is that right? Yeah. Information management and data analysis. We also have Dhruvin Vora here. He is a Product Manager at Wadhwani AI and Dhruvin’s specs are really interesting as well because by now you’ve bridged working as a  Product Manager for B2C applications at Amazon and Ola. Where you're optimizing for business impact and now you're at Wadhwani where you're optimizing for social impact, measuring social impact specifically in the healthcare and agriculture space. So looking forward to hearing more from you Dhuvin. And last but not least we have with us Peter Gault who is the Founder and Executive Director of Quill.org, which is an EdTech tool that's helped millions of students across the country improve their writing and critical thinking skills. So, welcome panelists. Thank you for being here today. [Applause] I want to echo something that Brigitte said earlier. When it comes to AI for - so AI being used in user facing products is a relatively nascent space of the last five or ten years or so, right? AI for social impact is an even more emergent new space. And so I really appreciate all three of you being here today because I think you have very diverse perspectives to contribute to this conversation as we determine what are best practices for helping our users calibrate trust in our AI solution. We have Azza the Designer, Product Manager, Executive Director, Founder working across EdTech, agriculture, healthcare as well as employment services and creative advancement. So thank you again. Without further ado - oh actually, before I tee up my first question. I'd like to say to all of you, please join the conversation. The first 15ish minutes of this will be a conversation amongst ourselves as a starting point, so we have the shared vocabulary. And in the last 10 minutes, I please encourage you to come up to the mics, which you will find in center stage. We'll also have mic runners coming around so over the next half-hour keep in mind things that you want to know. Insights that you want to hear from our fellow panelist to dig in deeper. So that being said, first things first, we've heard from many of our different speakers today the importance of putting our users first, right? So I'd love to hear from each of you and let's start with you Azza. Who are your users or stakeholders? And what problem are you solving for them? First of all, hi everyone and thank you Di for having us here. Talking about our users. Our users are in general marginalized job seekers such as migrants and refugees. We focus on those who are struggling on finding their position in the labor market because of many challenges they have. So we are the Skilllab, developing an AI based skill assessment to help them or try to help them to overcome some challenges like language, labor market complexity, diversity. We know that a lot of, or many of the refugees just come to new countries. They don't have their certificates or even if they have their certificates they need to be recognized and they need to go through a long process for recognition and they need that as a proof for their past experience or their skills that they gain from this experience. So with Skilllab we are trying to help them prove this experience and make it visible for the employers for them also to know exactly what they need to do in this new labor market. So sounds like Skilllab is an AI powered recommendation engine for not just helping to certify and accredit your job seekers, these recently resettled refugees, and match them to jobs that are a good fit for them. Dhruvin. What about you and Wadwhani? Tell us about your users and what problem you're solving for them? So thank you for introducing me Di. As you introduced me I transitioned from the B2C world to the social sector, being Product Manager at both places, but one of the first differences that I noticed moving from the B2C world to social sector is the term user gets too nuanced and in some ways broader, and now we use the term stakeholder so we don't just optimized for the users but the stakeholders. So at Wadhwani AI, specifically for the agriculture project, we're building a pest management tool which helps early pest infestations detection using AI, and tells the farmers the right recommendation or the right action to take. But here the farmers are not the only users, we actually have users, we have beneficiaries that are the farmer, the users which are the field staff deployed by the government and also the decision makers with other government bodies. While the goal for all of them, the final goal for all of them, is farmer wellbeing, the way they achieve this goal is quite different and while building the product we keep each one of them in mind to make sure we fulfill or we make sure we benefit each one of them. We take into considerations their values and core benefits they're looking for and then the second nuance that comes in is from the AI. Where building a software product is more of a deterministic product where you know that the flow is going to be defined everyone understands it but for a farmer or for a field worker, they don't understand the nuances of AI and the probabilistic nature of it. So building that trust in them and letting them know on the other side that they might fail some of the times is something that we consider a lot. So it sounds like as you've transitioned from the B2C space to social impact you've had to unpack and disambiguate what we mean by users, right? As you're designing your solutions to drive impact not just for, in this case, actually, the users are the extension officers who are actually using the agricultural pest detection app. But they're doing it on behalf of the beneficiaries, your farmers. And at the same time you need to make sure that you're conveying these results, these impacts, to government officials and other stakeholders who care deeply about the impact that Wadwhani is having.  Peter. I'm curious. Do you also find that you need to disambiguate or unpack users from other stakeholder types in your work at Quill? Absolutely. So with Quill our goal is to help the 27 million low income students who struggle with basic writing and this can be really challenging because our users are in schools. They have teachers and administrators who are making decisions on their behalf. What materials they should be using, how they should be learning, and so we serve an audience but we have to go through a number of other stakeholders first. So, there's a number of challenges that come with building AI for students and helping students. But then also collaborating with all these other stakeholders who are also really invested in students education and working together to help these students become strong writers. Knowing how critical it is today for everybody to be able to write well and we're all writing in Google Docs and emails and when we see that 88% of all low-income students struggle with basic writing, it's this massive barrier that everyone needs to be able to overcome and to really excel in and getting all the stakeholders aligned to be successful there is an incredibly difficult challenge. So sounds like across all three different organizations you're having to unpack from the users, the beneficiaries, the funders and other government officials, right? And this leads me to my next question. So when we talk about user trust, it's really important that we understand the UX concept of mental models. It's important that we understand how our users, our stakeholders, assume or understand that your product, your solution, works because their own mental model for how that thing works affects how they feel about your products or solution. It also affects how they interact with it and use it over time. And if you can understand user’s mental models then you can help design the solution in a way that helps them build trust over time. So Azza or Dhruvin, perhaps you'd like to start us off with, tell us a little bit about, what have you learned that was unexpected or surprising about your users or stakeholder’s mental models when it comes to using Skilllab or Wadwhani? As I mentioned before, our users are marginalized job seekers, so they are facing a lot of problems not only being unemployed, but other problems like cultural problems, language problems also, and sometimes psychological problems because if you are talking about migrants and refugees, they left their country because of many reasons, conflict, wars and all of that and they just come to a new place. They need to prove themselves, they need to find their potential, professional potential in the labor market. So to understand them actually really needed a lot of effort, a lot of empathy, a lot of patience. And the best way to do that is to be close to them, to work also with them. So we in the Skilllab actually when we are starting to work with our users, we started just to give them the application to use it and we observe the results and their reaction. Like a user research environment? Yes, like a traditional, let's say, user research environment. But after that when we are between our users and talk to them and know their full story and more information about them, we realize that no, it's more than collecting data and just informed product development and solve their problems in this way. So we, in general in our company, try to go beyond this thought or this way of thinking, and starting to think how to be empathic with our users, how to provoke that adrenaline. Provoke their adrenaline? Oh interesting, OK. Yeah, while they are using our application, because as I just mentioned, this is a group of users they have low resilience of feeling inferior or feeling frustrated because if they found that application is not easy to be used or they don't get the results. So if you want me to give a specific example how all of those things just reflected as a change in our application. So for example we just realized that our users feel so bored, and uninspired if they go through a linear user's journey. Just - OK, signing up, then filling all experience, then go through for a long scale assessment, and then wait for the result. So we realized or we decided to change all of that, to be like a circular user journey. Circular user journey? Yes, so the users they can move freely between data entry and seeing the result, editing the results. And so they know exactly what is the relation between what they are doing here and what is the result of them keep working on this skill assessment. Interesting. So you found through conducting user research on your work at Skilllab that it was really important for your users to see the benefits earlier on, as soon as possible, as a direct result of input that they were providing to Skilllab’s recommendation engine. And then that further incentivizes them to encourage - or not incentivizes but motivates them to encourage using Skilllab and hopefully getting more job recommendations. Yeah, we don't want them to feel like they are patronized, or they are under an investigating process. And it’s an interesting point that you make there as well in terms of conducting user research to understand the various emotional states that your users, your end users, are bringing to this experience. Dhurven or Peter, I’d love to hear from you. Tell us something unexpected that you've learned about your user’s mental models and how that impacted the design of your application. So one of the things that was very interesting for us is, we realized that the users, the beneficiaries here, the farmers, are in a lot of ways solution takers and not the choosers. Solution takers and not - Yeah. In some sense when we did a lot of user research and gave farmers the recommendations. We realized that they take the recommendations on the face value and not question it because they have had a history of various government organizations or NGOs coming to them and helping them out with various thing. So there's a level of trust that is already built. And that is where the challenge with AI comes in because AI again, we know will fail some of the time but these users don't understand that well, and in a lot of ways we feel the onus to make them understand that this is an AI model and it might fail some of the time so you don't need to trust it all the time, but you need to actually question it. This is a very hard thing to do. So we realized that the we need to understand the user’s mental model just not from the product perspective or solution perspective, but go broader and understand their life journey because they've been having this system of people coming and helping them a lot of times, and most of the times those were experts so they had to rely on them where they brought in more knowledge than what they actually had. But with AI they need to be partners in this and AI brings in certain expertise, whereas they also need to augment it with what they see on the ground. And I love that you made that point because it also ties very nicely this UX concept of understanding, the importance of understanding, mental models with explainability and trust, right? So your point acknowledging that machine learning and AI - it's probabilistic and based on statistics. So it will make errors, it will get it wrong. And so you need to help, as you're designing these solutions, you need to help your users calibrate how much trust, you don't want them to blindly trust it and just put aside their own knowledge of what actually feels right, what feels intuitive. But at the same time, you don't want them to not understand whats going on, and they risk abandonment of the solution all together. And that's a big challenge in education right now. There's a whole bunch of EdTech tools that are starting to use AI and there’s a lot of fear about the black box, you know, that these tools are making judgments on what kids should be learning and teachers are starting to have some fear about what the student’s feedback is getting. Students aren't quite sure, is this right? Is this wrong? And so one of the things that we're seeing is how critical transparency is. That when Quill is making a judgment on how you can improve your writing, what feedback to give, we also need to give the students and teachers the logs that show them exactly what did Quill say? What feedback did you get? How can we make what we're saying as transparent as possible? So that, you know, we're not going to get it right 100% of the time, and if we can create that expectation and give that context, that can really build a lot of trust with our users. But in the absence of that, you really risk seeing a lot of folks right now who fear that these tools aren’t necessarily best serving their own interests. And this is something I'd like to post to all of you. Peter you touched on this powerful point of the importance of transparency, the importance of transparency in the UX and UI itself, right? But how do you determine what level of transparency is enough, is relevant or is actionable for your users. Because at the same time you don't want to overwhelm them with transparency and leave them feeling paralyzed or not knowing how to move forward, right? So for us at least one thing that was so challenging is that, with writing - it's subjective. That when we're analyzing writing, there's not one right or wrong answer. We're making judgment calls on this is good writing, this is bad writing. You show the same piece of writing to say Fitzgerald and Hemingway they're going to say way different things about the quality of it. And so for us, we need to be opinionated about, hey, this is what we think good writing looks like, and provide those resources to teachers to explain our pedagogy to explain our theory. Now, we don't have to force every user to read our whole theory of action and to require that to use the product but that needs to be available. We need to, sort of, when we're taking a stand and taking a position, to be very clear about the positions that we're taking. And again, not every user will access those materials. But if you haven't been clear, if you're making subjective judgments and using AI, you really need to at least somewhere be clear about what those judgments are and how you’re making them. Az, I’d like to hear from you, when it comes to the right level of transparency, right? How have you made design decisions in Skilllab to help users calibrate just enough trust in what they're seeing? After, as I just said, understanding our users and the type of users and assessing their digital literacy level. So we found it will be so scary for them if we tell them. OK. This is an AI system and you get all of those recommendations based on AI prediction or something like that. So for us to be transparent with them, the best thing that we can do is to explain for them. What is going on. What is the relation between the data that they fill and the results. And also the data source, from where it was recommended for them. And also we are talking about trust, maybe it's difficult somehow in our system because our recommendation engine or the skill assessment. Users have the control. Control on the results, so they can, if there is a skill they found, like a skill they didn't do, they can say, OK no, I didn't do this skill. So they have the freedom to select recommended skills. So based on the input that they're providing, they’re immediately able to see how it impacts the output, right? Dhruvin, what about in your work at Whadwani? I think Azza touched on an interesting point here around explaining or even conveying that your solution utilizes AI, right? And how that impacts how your users, how your farmers and extension officers, and so on and so forth, trust what they're seeing. Yeah, I think we resonate with them on the same bit beneficiaries in some sense would get over, when, if we explain to them that there's some AI engine running in the background and giving them this recommendation. But when we think of trust, we again break it down into various user types that we think of. So if you think of the government organization or the NGO partner we're working with, they understand AI well, or they are at least have the level or capabilities to understand AI so when building trust with them we talk about different levels where we will conduct various tests to see how good our models are doing on the ground and what sort of implications will an error have on the overall well-being of the farmer. So when we think of trust for the NGO partner, we would run multiple experiments with them to build that trust, versus when we're thinking of a user or a beneficiary, we have some human in the loop to make sure that whatever recommendations we are giving or whatever recommendations we are unsure of are always double checked. And that is where the explainability might take a backseat for some time. I know we have just about 8 minutes left here, we'd love to hear from you all as we’ve had this discussion around how Wadwhani, Skilllab and Quill have made design decisions to help calibrate user trust and understand their users and stakeholders mental models. What's coming up for you all? Yes I see a question from this person over here. If you wouldn't mind there's a mic just here to your right, so that everyone can hear you more clearly. Hi. Thanks for sharing. Dhruvin, my question was forming and Peter I think you touched on it too, around stakeholders and you just touched on them in your last response. But I want to push a little further on that because so much in execution, especially with like AI driven decision making tools is reliant on an intermediary, right? You're kind of going for a lower cost solution, you're working through partners to make sure you can deliver it in the field. Did you face any kind of pushback? Because in some of these cases the decisioning is maybe at odds or, you can say collaborative, but sometimes it may eat into the responsibility or kind of the day-to-day work of some of your partners. So did you receive pushback from your partners in any cases? Or how did you think about designing the tool to make sure that the partners felt included and not threatened, I guess, by the decisioning tool? I think if we talk about today, there's a lot of fear of AI taking our jobs and I think in some ways you're referring to that. And we are aware of that. To make sure that that does not come in our way, what we do is, we take our partners along from the time of problem identification. So as we identified that pest management is a big problem and there are many people trying to solve it, there were partners that were already understanding that that's an area we are working in and that's a problem that also has resonated by a lot of partners. And the way we think about building our AI tools is not to replace any human, so it's not automation in some ways but augmentation. So this would be a tool that would be carried by extension officers. Helping them extend their reach to the farmers, instead of replacing the extension officers. So in some ways the partners are always brought along from the start and we have not faced that challenge. And that was a big challenge for us as well where with Quill it’s based off of more than 50 years of research on what works. But the very first question we always get is does this work and where's the proof? And so with Quill we did a 1,000 student randomized control trial study which found significant results that students using Quill improve their writing skills about 50% faster than students not using Quill. But that study took more than a year, cost more than $100,000. It was a very expensive process to build that trust and to be able to show that these things work and that there are real results. So it can be sometimes that building trust is not something that happens over night. It really is a multi-year process and working with stakeholders to show that these things are working. My name is Ian. I'm in trust and safety at Google, mostly for YouTube. One of the things that we struggle with, with transparency in our algorithms for more content allowed, what content gets moderated or demonetized is, the more transparent you make it the more kind of gaming of the rules that you see. And there's other cases where if you have a decision engine and you’re spelling out kind of, oh 85% of people who have this characteristic also have this one and that's a privacy issue. So I guess how have you designed to, you know, mitigate what exploit pathways there are, if any? So that's an interesting question. Luckily for us, as I told our users, our solution takers and for them to understand the AI - so we are on the other side of the spectrum here where the users are looking at AI to help them rather than like, understanding the algorithm to game them. So if we talk about our journey till now, we have not faced that problem in some sense and for some foreseeable future we won't face that but that's a good point you've raised where there are chances where if the users understand the system enough, they might try to game it. And to add to that, as I told you, for our beneficiaries, in some ways, we have not explained our algorithms a lot because that again overburdens them. So in some sense we are far away from that problem. In the area that we work in. Azza or Peter, I'm curious. In your experiences at Skilllab and Quill, have you run into that issue in terms of being so transparent that your users, your stakeholders can game the system or exploit it? As I mentioned before it's different in our case because the user asks more as they know exactly - or as I talked about, the transparency level, we are explaining to them the information or the relation between the input and the output so as we control this to this level so they know this piece of information. So I don't think that this will make any problem with the user but it will be better for them to utilize the skill assessment if there's a problem, for example, they know exactly why they have this problem. They have a relevant skill just recommended. They understand why those skills are recommended. So I do think that we face any problem in this issue. That is a challenge for us. If you look at Quill’s keywords,  'Quill.org hack' is one of the top keywords after Quill to be searched online. We have 2 million students using Quill now and they know how to use Google to search for and try to find the answers and so we are proactive here. When students are using Quill they're getting feedback and as they're working we'll tell them, hey, here's what the right answer is. Here's how you can learn from this. And in giving them the answer, that is beneficial to them. But it means that Quill can't be a test, that it has to be low stakes, that we're giving you this practice, we're giving you this feedback, but in doing so we try to really de-emphasize the grading aspect and making this a low stakes experience. I think if you have a high-stake experience there is a job on the line. There's money that can make gaming the system a lot more pressurized and so for us we really try to make it low stakes and in doing so, acknowledge that this is a learning tool, and in doing so, can be more forthright with how our algorithms work. But it is really critical because we get a lot of pressure from teachers that are asking for grades, scores, they really want to use this as a testing tool and we have to say no there. And say that that's not how Quill is meant to be used and that that's not the right application of our technology. I think we have time for one last question. But I will of course mention that all of our great panelists will be available throughout the day. So, please feel free to continue pulling them aside and asking any questions that may come up to you. But I think I saw you at the mic first. Hi, I’m Rose Shuman and I'm here today on behalf of data.org. I had a question about the perception of users or beneficiaries on the expert quality of AI and whether anyone's ever experimented with posing the AI as not an expert but maybe an expert in training or something that's learning as well and sort of personifying it that way so that they could have a bit more push and pull with it. I'm just curious if that's ever been tried. Thanks.  We do that a lot with our stakeholders, government partners, when we initially build algorithms, we partner with them to collect the data as well. Where we make them aware that AI is not an expert and it learns as it gathers more data or more context. I have not tried that with users at this point or the beneficiaries, but the partners in some ways understand that the AI is going to learn more as we go and as it gathers more data. So they understand the push and pull mechanism in some ways. Az or Peter, if you'd like to chime in on that question? Yeah, I think it's a challenge. I think it's really hard to explain clearly some of the limits and trying to make it more accessible. Our students, again, can have a lot of - there's a potential for a lot of fear and frustration when they're getting feedback and they might not be getting the right feedback. We try to measure very closely when students are being successful and not successful. And try to really help with that, but there's room for growth for us as well to really try to make this a more personal experience and try to acknowledge the limits of what we can and can't do. Thank you very much. Thank you to all of you for being here and for letting us go a little into your lunch hour, we very appreciate that. And thank you so much for our great panelists. To Azza, Dhurvin and Peter. Thank you. 