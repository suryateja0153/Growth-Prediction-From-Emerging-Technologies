 Today on this special Build edition of the AI Show, we will be diving into responsible ML. On today's show, Mehrnoosh Sameki, Senior Program Manager on the Azure Machine Learning responsible AI team will talk with Samuel Jenkins and Harsha Nori, both data scientists from Cosine Data on the machine learning interpretability toolkit or InterpretML. Make sure you tune in. [MUSIC]  Hi everybody, I'm Harsha Nori, a data scientist at Microsoft who works on responsible AI and applied machine learning. I'm also one of the co-founders of InterpretML, which is a set of open source, tools and packages that makes it easy to add interpretability to your machine learning workflows. I'm joined here today by two of my colleagues, Sam and Mehrs, who will also introduce themselves. Together we'll give you a walk-through of InterpretML and the field of AI explainability.  Hey guys, I'm Sam Jenkins. I'm one of the co-founders of InterpretML, and I work on the machine learning.  My name is Mehrnoosh Sameki. I'm a Senior Product Manager at Azure AI, driving the product efforts behind some of our responsible AI offerings like InterpretML and Fairlearn.  Let's get started. So you've probably seen this slide a few times by now, but for those of you who haven't, these are Microsoft's six responsible AI principles. This talk is really going to focus on the transparency principle. Before diving into the details, we wanted to motivate why you should care about interpretability. On the top of our list is debugging models, which is something every ML practitioner has to do. With interpretability, you can not only look at a model's mispredictions, but you can understand why it made those mistakes and fix the underlying source of the problem. To give you some examples, I've used interpretability to detect signs of overfitting, identify leaky features, and fix bugs in my preprocessing. On top of debugging, interpretability can also help detect bias in your models, inspire trust between models and humans, and satisfy legal requirements. One other case to call out is if your models are ever making high-risk decisions, like in the medical or financial sectors, interpretability can be a really nice sanity check before blindly trusting what a model says. All that being said, these motivators will become way more clear over the course of the talk. Let's talk about InterpretML. So as we've mentioned before, InterpretML is really a collection of tools and packages, and at its heart is Interpret, which houses a lot of the key explainability techniques and defines what the API looks like. Interpret text is an extension of Interpret that focuses on text and language data, and the Azure ML Interpret package is a wrapper that enables you to run all these great capabilities in the Cloud as part of the Azure ML framework. Outside of the Interpret ecosystem, we also have active collaborations with SHAP and LIME, two major packages in the explainability space. Both of the primary authors of these packages, Scott and Marco, are members of Microsoft research and they're both giving talks as part of this session. I'd really encourage you to check those out after this one. But for this talk, we're really going to focus in on the core package, Interpret and walk you through how to use it. Within Interpret, the explainability algorithms are organized into two major sections, glassbox models and blackbox explanations. I'll walk you through what we mean by each of these terms and provide some light code examples along the way. Let's start with the glassbox section. By glassbox models, we mean learning algorithms that are designed to be interpretable, like simple decision trees, rule lists, and linear models. Glassbox models typically provide exact or lossless explainability. That is, you can trace and reason about how any glassbox model makes its decisions. The flagship glassbox model inside InterpretML is a technique called the Explainable Boosting Machine or EBM, which was developed at Microsoft research. To give you a little intuition about how this algorithm works, let's start by looking at a linear model, which is widely considered to be the gold standard of interpretability. In the bottom right is a visualization for how a linear model operates on a single feature. While linear models are highly interpretable, they often fail to match the accuracy of more complicated models. To combat this, statisticians pioneered generalized additive models, which keep the additive structure, and therefore the interpretability of the linear model, that make them more flexible and accurate. At MSR, we've taken this one step further, by further amplifying GAMs with modern machine learning techniques like gradient boosting and bagging. We also automatically include pairwise interaction terms which boosts their accuracy even further. After all these improvements, EBM still preserve the interpretability of a linear model, but often match the accuracy of powerful blackbox models like random forest and boosted trees. As an added bonus, they're also incredibly light and efficient for deployments. For a deep dive into EBMs, please see Richard's talk later in the session. Here's all the code you need to train an EBM. With just two lines you can train the model. If you've ever used scikit-learn, you'll notice that we matched the API exactly. With another two lines, you can understand the model's behavior. In this case we're calling the explain global function, which will give us a high level overall or global understanding of how the model works. The other half of InterpretML are blackbox explainability techniques. Unlike glassbox models, which are trained directly on raw data, these techniques that on top of existing models are pipelines. They generally work by treating the model like a blackbox, and assume that they only have access to the inputs and the outputs of the model. By repeatedly perturbing the input, passing them through the model, and analyzing the changes in the output's, a blackbox explainability technique can glean some understanding of what a model considers important. One huge advantage of these algorithms are that they can work on any model like deep neural nets or even complex pipelines. However, it's really important to note that these techniques only provide approximate explainability. Sometimes blackbox explanations can be incorrect and not faithful to the original model. Popular examples of blackbox explainers are SHAP, LIME, PDP, and SA, and for a deeper dive into blackbox techniques, please stay tuned for Scott and Marco's talks. In this code example, we're using a simple neural net to represent a blackbox and explaining it what the Kernel SHAP explainability technique. In this case, we're using the explain local call because we want explanations on individual data points. At this point, I'm going to hand things over to Sam, who has a Jupyter Notebook demo of the algorithms and visualizations produced by Interpret.  I'm going to walk you through an income classification problem, where we are trying to predict if an individual is earning more than $50,000 per year or not. We'll be using a dataset known as adult income, which is derived from the US Census in around the '90s. It gives us basic features such as age, education, capital gains, etc. Now, when you want to train a glassbox models such as an Explainable Boosting Machine, it conforms to the same circuit API you may already be familiar with, calling fit in the same in terms of you've got default parameters. Now when you go into get explanations, you're going to call the explain underscore methods. In this case, we are going for explained global, and just to reiterate, that's going to give you general model behavior across the population. The first graph we see with this is the summary plot. It states that age is the most critical feature in determining if someone is more than 50k per year or high-income, we'll define in this case, followed by marital status, capital gains, and so forth. Now this should already be very familiar. A lot of machine learning models already provide these variable importance plots. But what about the further question of, well, I want to witness how, if I'm at age 20, how does that differ to if I was say, 60 years of age across the population? Do I get a higher income as they get older or not? We can look deeper into that when we click on the feature plots, the EBMs. Now, how do you read this? Well, the score technically, in this case, is legit since we're dealing with the classification problem. The way to read it is this: The higher you are in the y-axis, the higher the odds that you are a high-income earner. Now, what's noticeable is when you're under 25 years of age, you've got a pretty low chance of earning a decent amount, and that's most likely due to the fact that you might be studying, you may not be a full-time worker by then. That starts to change as you get older and you just pretty much get a monotonic increase in how much you might earn. It's fairly logical, as you get all the chances, you've got more chance of promotions, you've got more experience, and you see an opposing effect once you hit retirement and somewhat chaotic decline as you get older. Now, what I find very interesting about this is when you look at, say, just the variable importance plot, as I showed earlier, you get to know what features are important, but you don't get to know how the segments within that feature relate. For instance, in this case, of age, when you're super young, you're not going to have a high chance of earning much. If you're somewhat middle-aged, you're going to be doing pretty well, and then once you hit your retirement, it starts to go down again. These are very interesting relationships to look into, especially at the model debugging stage, where you are actively looking for problems such as overfitting or possible sampling bias from the dataset itself. So that's what I wanted to talk about for the global behavior of the EBM. Let's have a look at the individual predictions. So in order to look at a single individual and say, "Why did this model make that decision, why did it say that someone is earning high or not." You will call explain local, and it's going to try to explain the first 10 instances of the test set here. Now, I've clicked on "Instance 6", this is somebody the EBM has predicted is earning more than 50k, and surprise he is. Now, what's interesting in this particular plot, bear in mind positive means high chance of earning more, is that capital gains is completely dominating. He's earning $15,000 in capital gains, and this is completely overriding the decision. Like other features, they do matter, but nowhere near as much as this one feature for this man, and so that's how the EBM has predicted him as being high-income. Of course, there is a balancing act like there are things such as, he's a high school graduate, which suggests a low odds of him earning that much, but then there's just so many characteristics which are balancing out in suggesting otherwise. Now, let's go on to the blackbox side of things for Interpret. The [inaudible] a little different to the glassbox, but we'll walk through it. So we're going to train a very stock, simple blackbox pipeline. Principal component, analysis component, random forest, that's it. What we're going to do is we're going to run partial dependence on top of this pipeline to try to tease out how it's making decisions, in this case, the overall behavior. Like how does the model overall see which features are important and not, and which regions. So we can look at specific feature plots in the case of partial dependence and witness with this dark blue line that the relationship is very similar in terms of age to high-income as EBM. When you're young, you don't have that higher chance of earning much, but it starts to increase as you get older and then back down again as you get closer to retirement and onwards. I'd like to state, again, that with blackbox explainers be a little careful, they are approximations, they try to give an educated guess and stating how the model works, but it's not telling you exactly how the model is making its choice. So always keep that in the back of your head when you look at these graphs, specific to blackbox, glassbox is a different story. That's always exact. It is what you see. That sums up how you utilize blackbox and glassbox methods within Interpret package. Enjoy.  Great. So let's see how SHAP enables you to understand your model predictions better. What SHAP calculates is how much each feature has contributed to the prediction of a particular data point compared to the average prediction. For instance, imagine that we want to understand a model that predicts housing prices. We have these particular data points, which is a house that has a park around it, cats are banned, it's in a very safe neighborhood, and wall painting is required. This particular house has been predicted to have the price of $300k, which brings it to be 10k less than the average house price prediction for all the apartments in that dataset. Now, SHAP enables you to see how much each of these features have contributed to make this particular house prediction to be 10k less than the average prediction of the houses. The result can be something like, the fact that wall painting is needed contributed minus $10k, the fact that there is a park around it contributed plus $10k, the fact that there is a secure neighborhood around this house contributed plus 10k, and the fact that cats are banned contributed minus 20k. Now let's switch gears going to an example of using SHAP in action to explain a Blackbox model. Here you can see a sample notebook that I have put under hosted Notebook VM within Azure Machine Learning. What I would like to do is to showcase how SHAP can be used to explain the results and predictions of a binary classification model that has been trained on top of employee attrition dataset. First, I install the package that I need in order to run this interpretability technique, and I go through my model training phase. I get the dataset, as I said, it's an employee attrition dataset, so binary classification of labels is beings yes or no, meaning leaving or staying. I get rid of some of the features, I split the dataset into train and test, and I apply some feature transformation on top of my numeric features and categorical features. Then I train an SVM classifier that is basically predicting whether someone would stay or would leave the company. After I fit the model, that's where interpretability comes into the scenario. Tabular explainer is based on SHAP and basically uses a very light logic to look at the type of your model and accordingly, call one of the appropriate SHAP explainers. In this particular case, Kernel explainer of SHAP has been called. What you need to parse to it is the model and the background data, and optionally, you can parse the feature names, class names or labels, and the feature transformations. Parsing the feature transformations that you have defined on top of your numeric and categorical features will allow this SHAP tabular explainer to reverse the feature engineering pipeline for you and return the feature importances that are in terms of raw features rather than engineered features, obviously, that promotes understandability of your model explanations. Once you create this explainer object, you can call two different functions on top of it, explain global, which overall explains how your model has made these predictions. For instance, here I can see a dictionary of top K important factors for me over time, number of companies worked, environment satisfaction, and I can call explain local and pass a particular data points to it and see for that particular person, what are the features of him or her and what are the feature importances for him or her. Alternatively, you can use our visualization dashboard inside the Jupyter notebook or in a new tab to take a look at the model performance, explore your dataset, and understand your model explanations, either global explanations or the local explanations. So let's go through what this visualization dashboard does for you. Before doing a deep dive on what these four tabs are, let's focus on this header for now. When you load this visualization dashboard, you can see only one cohort, which contains all your dataset. Alternatively, you can add cohorts like what I did for age less than or equal 35 and age greater than 35 in order to compare the model performance, dataset insights, and global and local feature importances, across different sub-cohorts in your data. Now that I have created these two cohorts, I can compare the performance of the model across them, and I can also look at the distribution of my prediction values across all these three cohorts. The very first thing that I'm observing is the accuracy of my model is overall 87 percent for all data, and when I break it down to different age groups, it is 81 percent for the age less than or equal 35 and 94 percent for each group greater than 35. So there is almost at 13 percent accuracy gap between the two age groups. This accuracy gap makes me a bit suspicious that there might be an imbalance in the ground trues labels of age group greater than 35. For instance, we might have had less people in that age group that has the actual ground trues of leaving the company. Let's see whether that hypothesis is true. I come to the Dataset Explorer, I switch to age group less than or equal 35, and now I can see the actual ground trues classes or labels for different sexes existing in the age group less than or equal 35. The orange is the people with ground true staying and the blue is people with ground trues leaving. Now, I switch to age greater than 35, and I can see that this label almost became half in size. So the age group greater than 35 had almost half of the people in age group less than or equal 35, who had the actual ground trues of leaving the company. Another insight I can get is comparing the reality on the ground trues values with the prediction values. For instance, for the age group greater than 35, now I see how many of females and males have got the actual ground trues labels of leaving and staying, and I can switch to predicted classes and see that, for instance, for females of age group greater than 35, there was no prediction of class leaving. Although in the ground trues, it's clearly suggest that there were some people greater than age 35 who left the company. Now, let's discuss the last two tabs which are more about understanding your model explanations. The very first one is aggregate feature importance, which allows you to explore the top K important factors that impacted your overall model prediction. First, I only activate my first cohort, which is all data. I can take a look at what top K important factors are: overtime, environment satisfaction, number of companies worked, job satisfaction, are the top four important factors influencing the predictions of this model. I can then bring particular age groups into the scenario, and change the sorting to see for each age group, what are the top K factors? For instance, for age group less than or equal 35, the top K factors are overtime, environmental satisfaction, number of companies worked, and job satisfaction. For each group greater than 35, the top K important factors are overtime, environment satisfaction, number of companies worked, year since last promotion. It is interesting to see that for age group less than or equal 35, overtime is a lot more important in affecting and influencing the prediction compares to the same factors influence for age group greater than 35. Now, going back to the default, I'm interested in investigating how overtime is impacting my model predictions. So I click on it, and now I can see this dependence plot that suggests that people who worked overtime, for them, the factor is a positive indicator of prediction leaving. Alternatively, clicking on "Job Satisfaction" suggests that, as people had more job satisfaction, for them, the factor of job satisfaction becomes a negative indicators towards leaving, which is positive towards staying. In other words, as people were happier, that happiness made the model to predict they're going to stay with the company. Eventually, I come to the individual feature importance and What-If Analysis tab in order to gain a better understanding of the factors behind the prediction of my individual data points, and run some what-if analysis scenarios by perturbing my data points features, to gain an understanding of my model, and debugging it further. I choose to click on this particular person. This person has been predicted to leave the company with 87 percent. I can see some of this person's features here, the age 29, the person is a male, belongs to education field, Life Sciences, travels frequently, but also what I can see is, what are the top K important factors contributing to this prediction of leaving? For instance, the factor over time, business travel, distance from home, and stock option level are the top four factors of this person who are contributing to this prediction of leaving with 87 percent. In the opposite side, there are some factors of him, job satisfaction, environment satisfaction, number of companies worked, education field, that are impacting this prediction of leaving negatively, but perhaps the other factors could affect it more strongly, so the prediction ended up being leaving. What I'm going to do is, I'm going to perturb some features of this data point. I'm going to change his traveling situation to non-travel, that drops the prediction from leaving with 87 percent to leaving with 75 percent. Moreover, I'm going to change the fact that he is working overtime, and reduce his work assignments. It is interesting to see by these two changes of not having him work overtime and changing it to non-travel from traveling frequently, the class actually change from prediction of leaving to prediction of staying with 65 percent. So I can get a lot of understanding just doing this perturbations by seeing whether my model is making sense, whether my model is making this prediction based on the factors that are intuitive. The other thing that I can do is I can look at individual conditional expectation plot. For instance, for this particular person, I can see that perturbing the age feature of this particular person, given that all other features of him or constant, change the probability of leaving from 90 percent to 64 percent. So I can continue comparing the eye spots and feature importance of multiple people and see how, for instance, this person and this person compare when it comes to the top K important factors that have affected their predictions. Eventually, what I'm excited to announce today is all of the capability is that you just sign our demo, are going to be integrated within Azure Machine Learning Studio under the Explanation tab. So by going to a particular model that you've registered or an experiment that you have run, you can run the same visualization dashboard within Studio, share it with other stakeholders, and have a very easy way of accessing the explanation of the models that you had previously generated. Thank you so much for joining us today. Please feel free to check out our open source offerings under interpret.ml, and its integration with Azure Machine Learning under Azure ML docs. Please also check out our customer highlight with Scandinavian Airline. Thank you so much and have a great day. [MUSIC] 