 good morning so I'll have to say I'm incredibly honored to be able to speak to you today I always view MSR as the place that disrupts and inspires Microsoft and while I'm not a researcher my team works incredibly closely with MSR to funnel as many good ideas as we can into the depths of the product teams and so today I'll walk you through some of the things that we're doing and how we're thinking about approaching responsible innovation around these emerging technologies so first I thought it might be good to just go back in time and turn back the clock a little bit what did the world look like ten years ago do you remember do you remember what it looked like in 2009 well in 2009 the financial crisis was at its tail end right in March of o.9 the Dow Jones Industrial Average had lost over 50% of its value since October of 2007 this was the economic context back then the iPhone was just a year old do you remember that I mean those wild right and every other phone was pretty much outdated by the end of 2009 so these phones just weren't good enough by the end of that year the App Store was five months old remember that slogan there's an app for that it was home to about 25,000 apps the App Store now serves as one of the most important software stores on the planet there are over 20 million registered developers two million apps and in that decade the App Store has generated over a hundred billion dollars in revenue which is astounding the apps were weird right they're kind of goofy developers and users we're enjoying the power of iPhones multi-touch display and motion sensors this was the app that everyone had to download it was the hook to get you into the App Store really sweet Amazon had just announced its Kindle 2 which looks dramatically different now Google had introduced an app that was meant to transform the communication space it was magical and new and it was called Google Wave and we didn't quite understand it at the time but it combined email and messaging and social media and event planning into something that kind of looks like slack now and so maybe Google was way ahead of its time early in January of 2009 Facebook surpassed MySpace at the time Facebook was around 68 million monthly active users in the US and this trend has continued Facebook now has over 2 billion monthly active users just incredible and then there was this video that shocked all of us write it rallied the world around a movement Neda agha-soltan she was 26 years old and she was shot dead it was captured by a cell phone and uploaded to Facebook and YouTube and it quickly went viral on Twitter the media outlets like CNN grabbed it and ran with it and Netta's death set the stage for social media activism and never before had citizens in crisis been able to disseminate their stories so easily to millions of strangers around the world and this really set the stage for the Arab Spring that happened two years later and then Kinect was first announced and this was now under the codename of project natal and it brought a multi-person skeletal tracking face recognition voice recognition to the mainstream making you the controller and this left a lasting impression in the technology world these features soon began to percolate all throughout the tech industry this is kind of sweet minecraft was first publicly available it led to a major cultural transformation it's changed the way that video games are perceived in the classroom teachers began using minecraft to teach various lessons from circuitry to math to history to language and and this has been really interesting for teachers because having a customizable 3d environment gave them the opportunity to teach lessons in a novel way in an interactive way and minecraft still kind of looks like this which is interesting and so I would say that at that time we were really early on we'd probably a little blind to the impact of technology may be a little naive and really optimistic as well and since then some would say that we're living in an age of magic right the AI revolution has occurred we have you know seeing AI these assistive apps that help people with low vision navigate the world we're able to use vision technologies and drones to go and check on power lines we can track the workforce and how they're operating in their workflows in factory settings we can use AI to track what's happening and weather patterns in the environments project Emma in the top right over here is helping individuals with Parkinson's disease and in reducing the tremors and so this era is a really interesting one and it's happened so quickly and if you haven't been watching closely or you've been heads down doing your work you might be feeling like you're looking up and finding a world that is quickly shape and changing its shape around you and in forming a new type of world around you even for artists like myself like this is my artwork I can take original artwork like this and transform it and create novel new artwork using machines and this is an experiment where we took 75,000 images of my art I create video art we sampled it we trained a neural net that would mimic mirror and my style of artwork and I thought how interesting I can collaborate with a machine we can take words like a bird with wings that are blue and belly and generate completely novel synthetic images this bird doesn't exist in real not real life this is a synthetic image and it was generated based on this text and a neural network that was trained on images of birds a bird with wings that are black and a white belly this bird also doesn't exist in real life and a bird with wings that are red and a yellow but yeah and that yellow belly again another synthetic image and this is really quite amazing when you think about the power of using language and converting it into imagery and so we've seen massive shifts along the perception and cognition services the ability to reach human parody in in certain categories and this has really been quite incredible and if you think about it we've spent the last thirty years teaching people how to use computers and we're now teaching computers how to understand the world and and this is one that's an amazing field is transforming and changing lives is transforming how we think about technology and Microsoft has really been leading the industry Microsoft research has been leading the industry in creating some of these amazing breakthroughs as you look around along mixed reality this is an innovation that's transforming how people collaborate and work it's allowing the merging of the physical and digital worlds to overlay virtual objects in physical spaces lets you collaborate over time over spaces and here is an example of some field technicians working with engineers and using the hololens to collaborate with one another there are assistive apps like seeing AI that are built out of the suppose out of my group and this is helping individuals with low vision narrate the world around them here's an example of the currency reading experience in seeing AI and so as we think about technology and how it's transforming the world we're starting to see it transform huge industries from medical around skin cancer diagnosis to a eyes that are helping to detect when a heart attack might occur to collaborating with radiologists in the agriculture space you know creating more efficiencies real-time crop monitoring predictive analytics to Big Pharma design drugs being able to predict how chemicals and compounds might interact and how they work to target different molecules autonomous vehicles the idea of taking raw pixels and translating that to steering commands really kind of profound right and data centers thinking about server workload and optimization how do you reduce the downtime how do you allocate staff and these are just some categories right there's a huge number of industries that are being transformed so this is a massively transformative set of technologies that we're dealing with now very different from 10 years ago and the debate the flip side of this technology is equally transformative we're no longer looking at technology in that blind simplistic utopian view the debate on the dystopian side is equally real there's discussions about addiction maximizers we see individuals who are hooked on to technology we're worried about that we have questions are in automation and how do individuals work in a space where technology is quickly automating jobs how do we think about autonomous weapons we don't even have a definition of autonomous weapons and we think about bias blackbox algorithms deep fakes fairness ethics corporate responsibility the environmental impact human rights privacy this notion of meaningful human control right how do we think about sustainability transparency surveillance and then importantly the unintended consequences and this is just a small list of all of the topics it's obviously a much larger list behind here but these are some of the big topics that are coming up in this category of work and we can't really look away right now right and so we've come to recognize the incredible power of technology even the pervasive of social media we're starting to look at the power of face recognition technologies and in a political climate where both sides can't seem to agree on anything they seem to be able to agree that with face recognition this is a technology that we need to regulate San Francisco going even so far as to ban this technology synthetic media this year was huge this is a huge leap in synthetic media techniques being able to create volumetric versions of people just using images being able to clone voices using small samples of speech and recorded audio these technologies are massively transformative I'm pretty sure Bill Gates I'm going to go out on the limb right now I'm pretty sure he didn't consent to his voice being cloned and these technologies can be used in deeply harmful ways such as revenge porn and so we're seeing a massive shift in the capabilities of tech and also the general societal reaction and perception into how technology fits into our lives okay and principles everyone has them right every corporation every industry group every Association everyone has principles around how we should think about emerging tech and it's important it's important that this conversation is happening this wasn't happening 10 years ago I don't think anyone had principles people weren't talking about this leaders weren't talking about it that's important and and so there's this recognition that across the board we're creating massively complex systems these are impacting cultures they're transforming the way we interact they're transforming the landscapes and their interactions between all of us and that acknowledgement is an important one in the industry and I think our language is poorly suited to how we talk about the complexity of technological interactions these normal cause-and-effect metaphors they don't quite work here and what we're understanding is that we have to look at technology in context because most technology is often dual use and it's difficult to separate the good and the bad we need to understand how it's being used by whom and what context where and what the effects are and so this is I think a really pivotal moment for us in technology you're hearing our leaders talking about this quite a bit Satya has been incredibly bold in shaping how Microsoft approaches artificial intelligence and emerging technologies some of you might recall at Build a couple years ago he referenced 1984 had that up on the big screen and talked about his concerns about dystopia and dystopian futures Masaccio talks a lot about building trust you know building trust in technology is crucial it starts with us taking accountability for the algorithms we make the experiences we create and ensuring that there is more trust in technology each day this was two years ago every time he speaks he talks about ethics and responsible innovation and so this at Microsoft is an important concept and at the corporate level we have something called ether it's our corporate ethics board it's an advisory group I'm a member of it we look at big topics that the company ought to consider and come up with recommendations for the for the company but we've also recognized the need to embed people who think about ethics and responsible innovation deep in the engineering stack and that's where I said I sit in cloud and artificial intelligence and we work alongside with researchers engineers designers developers and we look at how we can augment those folks with responsible thinking with techniques methodologies in order for us to build this from the bottom up and so I think you heard this in the intro but my team is a guide and we lead we help guide the the technical experience innovation within cloud and AI and and we have a mindset that is really around stewardship and a conservers society and the idea that we have real deep concerns for the future knowing that the decisions were making right now have potential disruptive destructive destructive impact in the medium to long term and so this is the mindset that we bring in to this group and what I'd like to do is share with you a little bit more about how we think about responsible innovation deep in the tech stack the idea here is that this is about stewardship and it's about taking care of the future through the decisions we make around research innovation development in the present and there's a number of facets that we think about when we say responsible innovation it means reflexivity inclusivity anticipation responsiveness and intentionality now I'm going to walk through each one of these and share with you how we think about it and kind of break it down a little bit so a big caveat first I think it's important to say this we don't have all the answers we can't stop that from us having starting and and we are going to iterate quickly and so we think about starting somewhere learning as quickly as possible and iterating iterating methodologies iterating on mindset innovating on tools and and we optimize for creating a system where we can learn as quickly as possible in the machine learning space you often hear of developers talking about their inner loop and what they mean by that is the inner loop between developing experimenting testing and creating a really tight group between iterating on their models because the more you can iterate and turn the crank on AI models the better your models are and so we think about this inner loop as the ethics inner loop and how do we create together with engineers designers PMS and how do we create an inner loop that helps us learn as quickly as possible around these methodologies and I think it's also important to recognize that with any new tech there are huge domains of ignorance that come up there are things that we cannot possibly understand or know or anticipate ahead of time all of the new ecosystems and unique circumstances surrounding technology developments and so we know that with every new technology development these domains of ignorance pop up and we need to create this space for us to understand and excavate and learn more and also create mechanisms for listening to signal and and finding you know Canaries in the coal mine so to speak and so I want to kind of caveat all of this with with these and and lastly the fact that we're building on amazing work from other people and other disciplines in other industries I take a lot of inspiration from Ursula Franklin who was a Canadian physicist and her thoughts around technology in the way intersects with society we look at Wendell Berry whose work around sustainable agriculture was really you know he was a thought leader in his time and and we can apply a lot of those techniques and ideas in the technology space and then lastly in the Science and Technology studies there are people and disciplines whose work it is to understand the impact of tech and society as we want to leverage that thinking and bring that to the forefront so on the first category of reflexivity the idea here is really about examining our moral character and holding a mirror up to what we're doing and why we're doing it what are our motivations what are we doing what our commitments what assumptions are we making where are some of our biases where do we think our blind spots are what we're trying to do here is have an honest conversation about what we're doing and why and and understand that we are trying to avoid any sort of moral distancing that might occur or the desire to look away from a certain piece of technology and say you know what I'm uncomfortable with it but I'm just going to put my head over here and pretend it doesn't exist we don't want that to happen and so this is about honesty and this is about openness it's also about acknowledging that there's very different views and value systems around the world we're not a homogeneous world we have diversity we have very valid viewpoints that are not based in Redmond Washington in the United States and we have to acknowledge that we have to bring that thinking into the way we think we spend a lot of time looking at organizational culture because we recognize that organizational culture is all about the values and behaviors that happen and contribute to the environment in which you build software and we want to influence that we want to influence how people interact the way they create technology the way they share information and and ultimately the most some of our most important work is about shaping the mindset of our workforce because it's people that are building technology and so I'll share with you a few things that we're doing here and the first one is an experiment that we're running right now this is an experiment where we're incorporating our principles the things that Sasha is talking about the things that Brad Smith is talking about at the highest levels and we're looking at incorp that into our personal commitments and reward system and so what does it look like to tie principles of fairness into your connect into your rewards what does it look like to train the workforce to understand and internalize and translate that into how they change and do their day-to-day work and so we've been embarking on a number of workshops where we actually work with groups to help them write out their actual accountabilities here's how they translate to accountabilities we've worked with training managers to recognize the work that people are doing and translate that into how they reward individuals and so this this exercise of translating principles into your day to day actions is an important one because sometimes they feel quite abstract and sometimes they're hard to you know it's hard to look at them say well what does this mean for me what does it mean for someone who's a build engineer working on the system to have a system that is fair what does it mean for someone who's working and the design discipline and maybe for those who are closest to the tip of the spear it's actually easier because they're thinking about stakeholders and they're thinking about who might be impacted but as you go down the stack it becomes increasingly difficult to take those abstract ideas and then apply them and so we work on breaking that down and helping people think through those types of things and then look at how we can change the whole system so this is an experiment where running is a few thousand people going through it right now we're learning a lot it's been a tremendous growth and learning experience for all of us and as we start scaling this type of practice out again this is the way we we innovate we do small experiments we iterate really quickly and we learn and then we start scaling it out based on what we learn the second piece is around inclusivity and the idea here is that all the projects that we're working on we want to vet them against competent and diverse individuals we want to vet them against people who bring perspective and who might even object to the work that we're doing because we deeply believe in the value of diversity and inclusion we want to hear from people who don't believe in the work that we're doing because we want to understand what's happening there we want to create the space for introspection we want to create the space for challenging dominant views we want to bring in experts and leverage those capabilities and we want to understand who's impacted and start talking to those impacted groups and and I'll say this you know dominance systems they maintain themselves and without diversity in the room without people challenging that system they don't change they reproduce they maintain themselves and dominant groups are rarely challenged to go and even think about their dominance and so this idea of listening to others of incorporating feedback even when it's uncomfortable that's a key element for how we think about responsible innovation and it's important that we create the space for this in our organizations things like open town halls or we can have honest discussions with leaders and ask pointed questions things like having forums where you can anonymously give feedback and have people listen to it and take action on it are important forums and we've created those types of systems another important one that we've created is really around how we do research this is around inquiry research validation and almost all of the research that my team does is on stakeholders who are typically forgotten and these are individuals who are often excluded from mainstream research they're members of social groups like the LGBTQ community or minority groups women introverts and these are individuals who are at risk of harm in social situations and what we want to do is we want to listen to these groups that are typically excluded children and the elderly are often excluded right and so these types of groups are important because we understand that if we listen to these marginalized groups we're better able to address the needs of the broader range of people and so we intentionally go and recruit individuals to listen to their feedback we want to hear how they think about privacy I guarantee you that women think about privacy different than men do and if you design for women in certain scenarios you actually address the privacy needs for everyone and so we take this mind we start applying it to how we do product development and this sort of intentionality around just research and who cuts gets to come into the room give feedback to you on what you're building and helps to shape it as an important attribute for how you design responsible technology so psychological safety is something you hear everyone talking about and this is I think one of the critical pieces to creating a workforce in an environment where people can speak up and this is the you know the shared belief that the team is safe for interpersonal risk-taking you're able to bring your real authentic self into work without any sort of fear of negative consequences that team members feel accepted they're respected and in creating this type of environment is an ongoing task and it's easy to destroy it and we're very thoughtful about how we how do we create that and an example of what we've done here is and this might seem a little silly but it's actually not it's we've created a card game that we call judgment call and and this card game has been interesting because what it lets you do is it lets you exercise your moral imagination and test your analytical thinking in a really safe and inclusive environment if anyone's interested I've got a few of these and you can come by and pick one up at the end but but this card deck has been unique because it really alters the relationship and the environment for you because when you enter into the context of game play you suddenly open yourself up to have conversations and dialogues that you might not normally have you give yourself the space to open up issues and concerns and talk about it because it's under the guise of gameplay as part of the game right and so by creating these types of tools and mechanisms what you start to do is alter the relationship between individuals and give you the space and the freedom to have the conversation so the game works something like this there are three different types of cards there's a rating card one saying that the product is extremely poor to five fantastic exceptional there's a stakeholder card where you have to enumerate all of the different stakeholders that might be using your product the direct ones the end one's and I'll tell you it's really hard for people to think about stakeholders and the you know vast spectrum of people that might be using it just that exercise alone of enumerated all the different individuals that might interact with your system not just the ones paying for it but the ones who are bystanders different advocacy groups different types of individuals and so they articulate different stakeholders there's a set of cards that represent the ethical principles that Microsoft has publicly talked about now what you start to do is you pair these cards up and you get delta hand that says hey you have a 1-star your stakeholder is an elderly individual and your ethical principle is all about fairness now you write a review from the mindset of that individual talking about the system or product this is a replace you doing the real research behind it to understand how these people actually feel and and how the product works for them but what it does is it puts you in the mindset and so this sort of activity pushes you into that mindset where you're forced to think about well what would I give a one star like what would they say if it was a one star review let's say it was a face recognition system in an airport what would you say you might say well I keep getting pulled out of the lineup it never really works for me I find I feel targeted it feels really uncomfortable and I really don't like the system it seems to take longer to go through this system than into the old one well that's an interesting thing to capture right and by writing down the review it gives people the space to articulate some of the concerns they might have about what they're building because it is hard to bring up negative things everyone is interested in shipping everyone is interested in moving the ball forward stopping and pausing and having a dialogue around what might be problematic and what you're building we want to create the space for that so we put all of our new employees through this boot camp we have them go through a set of exercises we find lots of different forums for us to share this type of activity and this is really one about building that muscle around thinking about stakeholders creating that safe environment to talk about things that might not be ideal and perfect and to be able to internalize what do those ethical principles really mean what does it mean to be fair what does it mean to be transparent what does it mean to be accountable here that sort of thing okay so then the third category is about anticipate and looking around the corner thinking ahead and what are some of the likely outcomes not just what are all the outcomes but what are the likely outcomes what are the likely futures what are some of the unintended consequences and I think it's important to recognize that there's no such thing as just introducing technology everything changes when you introduce something you even the introduction of a dishwasher changes the routines the patterns and the relationships within a family and so we have to think about how we introduce technology in a really thoughtful way intentional way and it's important for us to be able to talk about harm and break down this notion of harm because we often say oh we don't want to create technology that causes harm nobody does alright nobody goes out there and says hey I want to create tech that's going to hurt harm someone but we need to be able to break down what we say when we say harm what does that mean and for us what we did was we created a set of operating foundations and obviously there's a lot more than this but three important ones that we wanted to articulate as a company was first upholding the Universal Declaration of Human Rights second upholding the principles the underpinnings of democracy and third upholding the mechanisms of an informed citizenry and the third one's actually a component of the second one but given some of the technologies that we've seen enter the space we actually wanted to pull this one out and elevate it to a high level and I'll show you in a second what I mean by that so we broke down harm into four major categories risk of injury denial of consequential services and those are things like education access to public services the erosion of democratic and societal structures and that's why I've shown you that third foundational pillar because we are seeing a lot of technologies that are having an impact on societal structures and we wanted to call that category out as a top level because that's something that we're seeing and then the infringement on human rights and we broke this down into categories and it's been broken down into sub further sub categories with questions underneath each one but the idea here is we want to create a framework for how we think about harm because harm is a complicated topic and it is difficult for people to say what do we mean and we wanted to break it down and this is probably going to get tweaked if you come back to me in three months this will look different because we're going to learn and we're gonna find things we've missed we're going to find nuances that we wanted to capture that we haven't captured here but the idea is here like in risk of injury we're thinking about the emotional and psychological distress that technology can cause people think about the effect of deep fakes on people right the physical infrastructure damage that think that technology could affect opportunity loss economic loss the loss of agency manipulation social detriment loss of Liberty loss of privacy loss of dignity and like I said underneath this as an entire set of categories that breaks down what we mean by each of these larger categories but then what we've done is we've taken some of our tech and we've started applying a lens on top of this and saying hey what does this tech actually do or does it think if you were to put your tinfoil hat on and think about the worst things and the worst likely things that could happen in the worst hands and the different ways you could exploit or abuse and manipulate this tech what could happen and here's an example of one project where we said look this has potential for psychological distress or right about that manipulation it can affect how people make money we think this might affect the freedoms that people might have like this is an example rate of how you might now layer on a lens and some level of analytical thinking and by no means is this perfect there is no like rubric in the back that gives you like a score at the end of this this is a judgment call and it all depends on who's in the room and making sure the right people are in the room having the dialogue around this because some of this is a judgment and leveraging research and the latest knowledge in developing this kind of framework there's multiple characteristics right how severe is the impact what's the scale is there disproportionate impact with the likelihood of exposure if you've done any sort of threat modeling this will seem very familiar right ease of updating the system this is an interesting one some systems are much harder to update when you've discovered that they cause some low blow heart so hardware for example if you need to make an update that has a longer life cycle and lead time to making updates we need to be extra careful around hardware and so we start thinking about these various facets and characteristics of systems and then we can start applying this lens and so there's some rigor that we want to apply here and then there's also the understanding around infrastructure because infrastructure emerges all around us and that infrastructure lends to certain types of technologies growing perpetuating and once technology is widely accepted and standardized there's a relationship that changes between people who use the system you suddenly have less and less power as a user once a system has been broadly standardized and widely accepted and that's not the type of relationship that we want to design and so we want to be really intentional to understand here what kind of infrastructure is starting to pop up around some of these technologies because that will give us a little bit of a clue into the direction that some of this technology is heading okay so the fourth one is around responsiveness and the idea here is new knowledge is emerging all the time norms are changing technology is changing people's perspectives and their views on technology are changing and we have to respond to it we can't carve things in stone and say this is how we think about harm that will not change what we need to change as things change right and this changes disproportionately around the world and we have to be aware of the worldwide impact and view and so the landscape is shifting dramatically and we're actively adjusting and responding likewise and so a few years ago we did some research this is now old but I thought I would share this here because we started looking at the socio-political environment and how does that affect the belief systems and how people feel about technology where they have fear where they have security we started looking at the intersection of how people think about the kinds of relationships they want to have with technology the kinds of experience they have had with technology and then we look at context and so how do some of these technologies pervade throughout people's lives were the situations that people find themselves in day to day and we know that norms are built in you know more than that just these three circles but these are some important ones for us to consider and we did some research a few years ago we looked at different countries and different variations on trust because we wanted to understand well what was the difference and how people looked at technology how they thought about technology in the government and in the u.s. we found that a lot of people had a lot of skepticism both around corporations and the government people felt like no one was looking out for them that the government was there too there was a lot of suspicion around the government supporting businesses over citizens interests I bet some of this is still accurate I'd be more so in China and for the people and I'll caveat this you know for the people that chose to speak with us there was higher trust in corporation than government because they tended to work a little bit more in tandem and people were starting to see the benefits of government and corporations working together for the collective citizens Germany is quite different right there's some tougher regulation in the in Europe and so there was more trust in the US in the government's ability to regulate regulate corporations I think this has changed a little bit too since we've done this ethnography but the idea here is that around the world there are different relationships that people have with technology with government with corporations and we need to factor that into how we think about rolling out new technologies because it is not just the US and so lastly I want to talk a little bit about intentionality and how do we design with principles how do we design with intention with appropriate technologies and so this is where Sachi I think in 2016 share these principles around ethical AI development the principle of fairness that all stakeholders should be treated equitably and that we want to prevent undesirable stereotypes and biases this notion of systems being robust and reliable that they perform well in the worst case scenarios across different environmental factors privacy and security is a big underpinning of at Microsoft that all of our data we want to prevent it from misuse and unintended access we think about privacy in a pretty robust way at the company inclusion is really about empowering everyone regardless of their abilities making sure that people have ways to give feedback and transparency is all around these black box algorithms and how do we ensure that whatever these algorithms are doing they they're understood by the stakeholders and people that are interrogating the system and then lastly principles around accountability and ensuring that we as a company as individuals take responsibility for what we build and what the impact is and that's why we're being so thoughtful and intentional especially at the engineering level around how we develop tech and one of our researchers Salima Amir she has done an incredible amount of work where she's gone and scoured 150 plus AI recommendations conducted multiple rounds of iteration and validation and looking at what are some of the guidelines we should have around human AI collaboration and and each of these has examples and proof points that we can point people to and say hey if you want to do X here's some examples here's some good examples of what you can do this is publicly available her research papers out there and and things that we do here are you know how do we be super transparent about technology so we recently published something called a transparency note for face recognition we recognize that there's a lot of complexity and face recognition and we want to make sure that people understand how the technology works what the capabilities are what the limitations are how to achieve the best results as a company as technology companies they rarely open the lid and say here's where it just doesn't work you guys don't do it like this don't use it here what we wanted to do is create a mechanism for us to be able to share what is the most appropriate use of technology and we want you to understand the complexity here I want to understand what can cause false positives false negatives what can you do as a customer to create the most appropriate use and deployment of a piece of technology and so this sits somewhere between marketing material which is often meant to make products look amazing and develop your API is which is really functional it's somewhere in between and it's about capabilities how something works and how to help guide the deployment of this for responsible use and this is a pattern that we're starting to use across other technologies and then there's this concept of being seen full right in technology and in software we're often striving to create a seamless as an experience as possible and there's many benefits to this sort of seamless philosophy it's easier right if it looks beautiful it's frictionless but sometimes we want to insert speed bumps they want to insert friction because we want to encourage conscious decision making and so this is where we put controls in place around privacy identity exchanges of information and we intentionally want to show you where the seams are because we want you to stop and pause as an individual as a customer and make a conscious decision and I think AJ talked about this in the lightning round this morning we've been doing a lot of work to put ethics in the code path and really what we want to do is we want to translate what someone says at a policy level and translate that into lines of code what does it mean and this is an example of an automated tool that we're building that extracts out conditions where errors are happening in a model as part of a developer's workflow and this is a prototype it's something that we're actively developing but it's designed to sit within the developers code flow and what it does is as you look through the chart here it breaks down a models behavior and here's an example if someone's going through it saying okay four females who are not wearing eye makeup who have short hair and are not smiling we have a whole bunch of errors so we not we don't look at just females and there's a big bucket of errors around female let's go recruit more females what we can start doing is breaking down the model performance and saying for this category of people we have problems we have gaps this model is not performing well well let's go understand that let's go look at whether we have enough data whether we need to acquire more how's that model performing this sort of tooling and inquiry is what we mean by explainable AI we want to be able to understand how a model is performing and and so there's a lot of things like this that we're introducing right into the infrastructure and the engine of how we're developing code and it's meant to just it's part of your process it's just how you do work in the new world and so as we examine our ecosystem as we examine the technologies our building and we say okay what are the attributes of good systems what do they look like and here I think this is more an inspiration from Wendell Berry and sustainable agriculture because a lot of the patterns that he was describing years ago are very similar to good patterns in technology good ecosystems function in harmony with the larger patterns right they're pragmatic there's no science fiction and crazy stuff going on it's nothing far-fetched it solves real problems and there are reasonable limits to it it's something more than one problem it's not doing things in piecemeal fashion it's solving complex problems in a holistic way well what do bad solutions look like these seem very similar to the way a disease might behave or an addiction might behave within a body we see addictive behaviors right now we see we're seeing it right now with social media right these types of bad solutions cause new sets of problems that are equally as bad as the one that's trying to solve it might worsen a problem I might feel heavy-handed it fails it's really brittle and so as we examine good and bad systems we want to be able to identify what do they look like what are some of the attributes around them so let me step back a little bit and just talk a little bit again about the big picture because technology need not be used the way we use them today and it's not a question of no technology or the you know putting up with the current ones we should actively be pushing up against technological determinism and we should be asking questions about that looking at the big picture at the same time as we're looking at some of the details and so fundamentally what I think we need to do and what we're doing is we should be approaching this type of thinking as innovation material it's not a tax it's not a compliance it's not an after-the-fact it's not something you tack on and say okay now let's make this responsible that's put some band-aids on it it's starting to think about it right from the beginning the way we think about privacy and security right from the set of development and innovation material it's an opportunity right it's a business opportunity and and so this is a mindset that my team brings into this world like I said we're embedded at the deepest part of the infrastructure we're embedded in the place we look at all of the face recognition text speech people technologies ambient technologies mixed realities all of these types of synthetic environments as well this is where we see some of these technologies start to incubate and we want to do those in the most responsible way possible so I think I have 10 or 15 minutes left and I'll leave it at that and see if anyone has questions thank you okay so we have some micro neurs and I see a number two here yeah hi there so thanks for your talk I think it's it's wonderful to see a company embedding ethics deeply in its process and I'm enthusiastic about your efforts to try and do good I'm more pessimistic about your efforts to not be evil because you're only one company and if you're not evil than some other startup company will be evil instead and we'll get the same technology and I think the only way to prevent those sorts of of outcomes is to legislate and actually have rules that apply to all companies and so I'm curious whether your ethical team at Microsoft is working with your lobbying team to try and influence legislation going forward yeah absolutely and I agree with your sentiment that the thing that applies to everyone our laws well there's and there's two pieces here one is what can we as a company do as a trillion dollar company in the world what can we do to raise the floor and the water level for the industry and and I think we have the benefit of being around the block a few times right we went through our own era of growing up and our own adolescent years and so we are a much more mature company than some of the newer ones that are going through very publicly some learning and so we can set a lot of examples by us talking about this publicly we're setting examples by us talking to the industry by participating in things like the partnership on AI we talk about things and techniques publicly so that we can give people ideas and give them tools so that they may go and implement them as well because a lot of companies do want to do good and and what they're looking for our examples they're not quite sure how to approach it they might be a little new they're not sure how to organizationally situate themselves and so we want to give them examples on the regulation side you've probably seen this Brad Smith has publicly called for regulation around face recognition my group works very closely with our legal team as well with our lobbyists as well and what we and what we think our role is we want to give proof points we want to show people what it means when we say here's a policy well it's kind of abstract sometimes and the best policy is one where you can point to a proof point and say here's how its articulated so we do work closely with them our focus is not around lobbying and regulation but there are parts of the company that are focused on that we'd collaborate with them so an old cliche says talk is cheap action count Apple has done publicly and says what happens on your iPhone stays on your iPhone and this tech the position with respect to privacy well what's my Microsoft position on and we heard yesterday I talked a behavioral data in the wealth of behavioral data the question of course who gets to see this behavioral data so what's Microsoft position on behavioral data and would Microsoft step up to the same place what stays on your laptop or you know vice there's a new device that's interesting so I don't know the talk that you're referring to around behavioral data that happened yesterday but I'll say that both Apple and Microsoft are are some of the conscious leaders in the industry and so when you look at the cases Microsoft has brought to court the where they have challenged the government the way that they choose where data centers exist and the evaluations that are done around countries and human rights I would say that there's a lot of very public discussion that does translate into actions that are meaningful multi-million billion-dollar decisions that are being made Brad has talked about decisions that we've made as a company where we chose not to sells technology to certain regimes or took it where we've chosen not to sell technology to for example a California Police Department that wanted to go and deploy face recognition and you know body cams dash cams and we've said you know what no we don't think that this technology is fit for purpose we don't we have concerns about how you would use this we've said no to countries that have wanted to deploy technology across their capital cities and and we've looked at their human rights records and potential violations that might occur and we said no we don't want to put technology there and so we let's go back to behavioral data yeah I am electing I'm not an expert on the behavioral data side so you know we can talk about that afterwards and I could find you a contact because I think the behavioral data there's lots of places where would appear in office potentially LinkedIn potentially but those are just not categories that I spend a lot of time in but I can find you a contact or someone to talk to around behavioral data hi so at a moment we see sort of globalization and open source two of those things that limit the impact so if you decide face recognition on body cams is not something Microsoft would want to sell but at the same time Microsoft would put out things as open-source or libraries at open source the level of knowledge required to put it into action for somebody else goes down and I think so for me this question I coming from Germany I think regulation as one of those things we have seen and it's really hard if you want to maintain an open market and they see if you want to have your citizen access everything around the world so how do you see service in one way it's very clear we can do it from always sort of a our own perspective our own country's perspective but we have the global view and then sort of probably what we agree globally becomes very very small like the human rights even there we see sort of looking around the world so how do you see the sort of the open sourcing which basically lowers the hurdle for artists to change things as well as the globalization in order to have success with sort of the ethical principles we put out as sort of some like companies some states but if the others don't join in is it going to work yeah I know that's a tough question and and we talked about this quite a bit which is if we choose to do something but others if we choose to not go into a certain business and others do what does that do and this is where again the question about regulation comes in and and at the end of the day there is a there's a distinction here because we're talking about how we govern our behavior we're not talking about us governing the world this is about us governing our behavior as a company where we choose to put our time and energy and investment and so we have to distinguish that because it's easy to say well how does Microsoft go and shape the world this is primarily about how do we shape ourselves first and at the same time can we shape the world in a positive way but but it's not our job to go and police the world and so we have to think about that as first just a way a mindset around this is about us managing ourselves the the open source question is a really interesting one we've had a lot of debate around this discussion I would say and I think you recently saw a dataset that was pulled off and Microsoft pulled it offline we said look we just don't feel good about this data set right now the researcher that was working on this no longer at the company let's pull this one off line because we have concerns about what might be in it and we want to be really thoughtful about it and so we're actively going in and examining some of these and some and that's an example of that one in addition so I think it goes all the way to the tools because tools make it so much easy to do the things and I think you're conscious about sort of that the product to sell but I think would it really sort of make it yeah what is required that we don't sell tools to people yeah and and so the the angle that we are actually coming at it is from a platform so I sit in the platform at Microsoft I don't sit in the here's the actual piece of software or a product that's shipped we sit in the platform we power Cortana we power all of the speech language vision services around the company and then we provide that platform to developers and so the discussion is around what do we provide some of these platforms and and what technologies are actually gated which ones are gated which ones need extra scrutiny and face recognition is one of those technologies and and so we think about this at the platform level as well I think that's the deepest level we need to think about it and this is where it gets super messy because you don't always know how your platform is being used as soon as you open it up and it enters the market you lose all control over it and so the dialogue around which ones are the most are problematic and and can we articulate then identify and then do we have systems in place to manage who uses it and how it's used that's a needle we're trying to thread right now yeah hi I'm Margaret Burnett from Oregon State I really enjoyed your talk and my question is sort of about a trajectory as you see it now on on Microsoft's organizational culture so it seems like this this trend that you're describing now actually started several years ago and so you know over time it it seems like it's grown you're reflexivity and you're thinking about your products in this way and my question is what kinds of changes have you seen that you can actually measure in Microsoft's organizational culture well organizational culture is hard to measure it I mean it's hard to measure but well let me I'll share some anecdotes how about that because well one thing we do is we measure literacy around principles and values just to understand whether people understand what they need and whether they can apply so that we measured through surveys and things like that but but I want to talk a little bit about behaviors and and what we're seeing are people starting to ask a lot more questions and starting to say hey you know what I actually want to change how we do budgeting around data acquisition because what we want to do this year for example is we want to go create a fair benchmark data set and creating a fair benchmark data set costs a lot more money than just creating any old data set where you just go and gather as much data as you can and so we're starting to see changes in behavior that affect the way you budget the way you plan for technologies we have people that come to us and say I'm starting to articulate things that I've never felt comfortable talking about we have people that have come to us and say I need to rethink in my career what I'm doing Mira I need to go quit my job and I say don't quit your job change how you're doing your job and so there are these cultural transformations that are really behavioral that you start to see we as an ethics group I mean nobody wants to ethics group at the beginning right you've got an engineer that you're gonna slow me down and that's the initial reaction we get pulled into a lot of conversations where they say hey we want to think about this right at the beginning or another group will say I heard about what you guys did with that group can you come and help us and so the types of asks we get and the questions and the way we get pulled in we've seen even in the last year in the last two years a huge shift just in the way people approach us because initially there's a fear of oh you guys are the ones waving a stick you're gonna tell me I can't do what I would do and then we hear wait you're actually my partner you made me do something better and faster and I didn't have time for that thank you so much and so it's those types of things that we listen to and that's how we understand whether this is actually having the kind of cultural shift we want this year what we did was at least in this organization we change how we do budgeting we force groups to factor in their ethical needs I guess I don't know if there's a better way of putting this but we forced them to think through all of the things that come with the responsible innovation all the new data acquisition that they have to do any new benchmarks any new tooling that they have to build and they factor that into their budgeting process and I found that really remarkable because I haven't seen that and heard about that elsewhere and so that to me is another signal that things are shifting and changing thank you hi there um there was one up there I think I saw you waving number three oh hi thank you for the talk this is really interesting I particularly like this idea of ethicists embedded and product groups some science and one thing that I'm just wondering about it seems like a lot of Industry anybody can be an ethicist right in the sense that you have to have a computer science degree and go through a loop in our interview and become a software developer but you don't really need any qualifications to be an ethicist and I wonder if Microsoft is thinking about this somewhat differently do you have to have a degree in philosophy or degree in ethics or do you have to have passed some bar before you are leading the ethics decisions of a team how does it work yeah so the group is actually multidisciplinary and so there are only a handful of people that I would say are like the true ethicists on the team the the organization that I run has data scientists creative coders designers PMS and you know one or two people that I would say are more ethics oriented because this isn't really just about ethics this is about how you develop and so we create proof points we create through code and design and show you here's another way to do it and so the ethics you know the the thinking around the on the ethics side and which type of ethical philosophy you align to isn't really the high order bit the word ethics is used often and you would think that there might be it's all Ephesus it's not and so I think the trick here is is to not 1 don't hire people who are just armchair ethicists and who have some level of experience in background and so we make sure that they've got some degree in philosophy and they've worked in the industry for a while but but these types of groups are not effective if they're only talk they have to do and they have to show and so we staffs the group in such a way that data scientists can go in and examine the data in the composition a coder could go in and that example of the error analysis tool that I showed that was created I was coded by someone on our team that did you know the front end of it so we could understand what how it will manifest and that wasn't an emphasis it was someone coding it in a responsible way so I think the shift has to really happen away from ethics as the talking point to responsible innovation which change it's an engineering function as well thank you there's two in the back thank you I'm so I was really interested that I could remember UC Davis I was really interested in your training program and I was wondering whether you had any curricular materials that you could share and more broadly and what advice do you have for developing curriculum materials in undergraduate social engineering programs so no no curriculum that we're ready to share yet but we hope to make things public but I'll say that curriculum needs to be tailored to your cultural environment in the context in which you're developing if it's too abstract it's just theory at that point and theory doesn't really land well with engineers you want to you want to show real practical here's what you should do and here's how you should think about it and so we've tailored our stuff to our environment the dialogue we have is around the technologies that we're building and so it's not too far removed because then you have people saying I just don't get what you're trying to say and so I think it has to be really tailored and so when we share it it'll look tailored to what we're doing in our group I'm happy to talk about just education curriculum afterwards if you want because I think that that's a longer conversation and it depends on what the outcomes are that you're trying to achieve and and things like that so I think that that's a longer deeper conversation I'm happy to chat after this if you want to come up okay any more questions one more up here a lot of people seem to be talking about intentionality these days but there's not that much agreement on definitions and I was wondering what youth how you see intention and what you think it's sort of important disease yeah it's a it's a good question I haven't heard a lot of people talking about intentionality I just hear a lot of people talking about principles right now and but the way that we talk about intentionality is that every decision you're making there's a reason behind it you've thought about it you've been able to think about who's impacted how and what and like have you asked why five times and gotten to the essence of what you're doing where you're you know we're doing some work around just user flows right now around some of our devices and being really thoughtful about how are people going through that flow how are they consenting and giving information and so that instead of like it's so quickly to go and draw Golden Path scenarios and and don't not think of the stress cases the cases where people will trip up the cases where it will not work for everyone and so we're trying to do is bring that stakeholders value sensitive design into the system and start thinking through well how do it how do people use it how does it degrade how does it fall apart and so when I think of intentionality it depends on the stack and the discipline so designers will think about it from it may be a stakeholder standpoint intentionality at the deepest level of code is different it's more about robustness understanding the environmental factors and so I think you have to break it down by discipline and category of work but but it is being able to answer why and why did you make that decision and what's the information you're using the power it can't be your gut and where you have no data you have to make good choices but where you can reasonably go and collect data there is not there's no excuse to not go and do that and so can we rely on research and can we rely on listening to people and understanding it's being used in context and so it's it's being really informed and more data-driven about it when you when you have the ability to do that it looks like there's one more question over here Tara okay thank you so much for another raw discussion and I'm just wondering you mentioned about intention of building this trust and one way to build trust is when you actually have the openness of the flawed has happened and I wonder if Microsoft or you could perhaps facilitate a roundtable with the tech giants in us really to actually have this kind of discussion but really in a roundtable what do I have that been done that could you know it could be rectified that you know you know moving forward together on this ethical framework and I think when you know one thing that you mentioned building on the works of others if if Microsoft could embrace the rest of the tech giants to move together within this I think a responsible framework I'm sure the rest will follow yeah you know that's a that's a really good point and so there's actually a few things that are happening in the space and I'll touch on two one is the partnership on AI has a number of work streams that we're involved in other companies are involved in there's a sort of work streams around synthetic media deep fakes how do we collaborate with newsrooms to help them learn how to identify some of this tech some of these synthetic pieces of media that are coming into the system and so there's a lot of collaboration happening the partnership on AI has been really amazing it took a while for it to get spun up but it's been really amazing in facilitating open conversations there's also a lot of maybe more ad hoc private conversations that happen between the industry that I haven't seen happen before that I'm involved in as well where we do get together and we say because it's working and here's what is in what are you what's working for you guys hey sure some of those techniques over here and so there's a sharing of information that's happening we share our methodologies ideas and I think there has to be a larger investment in this category I mean we are investing huge compared to I to other companies I remember my son the other day he was I came on what do you work on ethics responsible attack and he's like how many people work on that we back of the napkin some rough you know ten-year-old math and he said well how BIG's the industry shouldn't more people work on that and this is my ten-year-old asking this question and I think more people should work on it and so there is there's existing conversations we're trying to facilitate other ones are trying to facilitate ones with industry and in different categories of people that use the technology it's starting sometimes they're not as like vocal about it as maybe they ought to be and and I think your feedback around hey can you guys be more like open about sharing that is a good one okay so I think we're a ten minutes after you guys have lots of demo booths to go take a look at I'm happy to answer questions if you afterwards just informally if you want so thank you very much [Applause] 