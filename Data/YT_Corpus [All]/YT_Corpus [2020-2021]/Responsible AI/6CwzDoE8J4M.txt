 [Music] hi everyone I'm Jackie and I'm the lead program manager on ml fairness here at Google so what is ml fairness as some of you may know googles mission is to organize the world's information and make it universally accessible and useful every one of our users gives us their trust and it's our responsibility to do right by them and as the impact and reach of AI has grown across societies and sectors it's critical to ethically design and deploy these systems in a fair and inclusive way addressing fairness and AI is an active area of research at Google from fostering a diverse and inclusive workforce that embodies critical and diverse knowledge to training models to remove a current problematic biases there is no standard definition of fairness whether decisions are made by humans or by machines far from a solved problem fairness and AI presents both an opportunity and a challenge last summer Google outlined principles to guide the responsible development and use of AI one of them directly speaks to ml fairness and making sure that our technologies don't create or reinforce unfair bias the principles further state that we seek to avoid unjust impacts on people related to sensitive characteristics such as race ethnicity gender nationality income sexual orientation ability and political or religious belief now let's take a look at how unfair bias might be created or reinforced an important step on that path is acknowledging that humans are at the center of technology design in addition to being impacted by it and humans have not always made product design decisions that are aligned with the needs of everyone for example because female body type crash-test dummies weren't required until 2011 female drivers were more likely than male drivers to be severely injured in an accident band-aids have long been manufactured in a single color as pink in this tweet you see the personal experience of an individual using a band date that matches his skin tone for the first time a product that's designed and intended for widespread use shouldn't fail for an individual individual because of something that they can't change about themselves product products and technology should just work for everyone these choices may not have been deliberate but they still reinforce the importance of being thoughtful about technology design and the impact it may have on humans why does Google care about these problems well our users are diverse and it's important that we provide an experience that works equally well across all of our users the good news is that humans you have the power to approach these problems differently and you create technology that is fairer and more inclusive for more people I'll give you a sense of what that means take a look at these images you'll notice where the label wedding was applied to the images on the left and where it wasn't the image on the right the labels in these photos demonstrate how one open-source image classifier trained on the open images data set does not properly recognize wedding traditions from different parts of the world open data sets like open images are a necessary and critical part of developing useful ml models but some open source datasets have been found to be geographically skewed based on how and where they were collected to bring greater geographic diversity to open images last year we enabled the global community of crowdsource app users to photograph the world around them and make their photos available to researchers and developers as a part of the open images extended data set we know that this is just an early step on a long journey and to build inclusive ml products training data must represent global diversity along several dimensions these are complex socio technical challenges and they need to be interrogated from many different angles it's about problem formation and how you think about these systems with human impact in mind let's talk a little bit more about these challenges and where they can manifest an pipeline unfairness can enter the system at any point in the ml pipeline from data collection and handling to model training to end use rarely can you identify a single cause of or a single solution to these problems far more often various causes interact in ML systems to produce problematic outcomes and a range of solutions is needed we try to disentangle these interactions to identify root causes and to find ways forward this approach spans more than just one team or discipline ml fairness is an initiative to help address these challenges and it takes a lot of different individuals with different backgrounds to do this we need to ask ourselves questions like how do people feel about fairness when they're interacting with an ml system how can you make systems more transparent to users and what's the societal impact of an ml system bias problems run deep and they don't always manifest in the same way as a result we've had to learn different techniques of addressing these challenges now we'll walk through some of the lessons that Google has learned and evaluating and proving our products as well as tools and techniques that we're developing in this space here to tell you more about this it's Sophie thanks Jackie hi everyone my name is full-si and I lead product for the ml fairness effort here at Google today I'll talk about three different angles in which we have thought about and acted on fairness concerns in our products and the lessons and we've learned from that we'll also walk through our next steps tools and techniques that we're developing of course we know that the lessons we're gonna talk about today are only some of the many ways of tackling the problem in fact as you heard in the keynote on Tuesday we're continuing to develop new methods such as T calf to understand our models and to improve them and we hope to keep learning with you so with that let's start with data as Jackie mentioned data sets are a key part of the ML development process data trains a model and informs what a model learns from and sees data is also a critical part of evaluating the model the data set suite to evaluate on indicate what we know about how the model performs and when it performs well or doesn't so let's start with an example what you see on the screen here is a screenshot from a game called quick draw that was developed through the googly eye experiments program in this game people drew images of different objects around the world like shoes or trees or cars and we use those images to train an image classification model this model could then play a game with the users where a user would draw an image and the model would guess what that image was of here you see a whole bunch of drawings of shoes and actually we were really excited because what better way to get diverse input from a whole bunch of users than to launch something globally where a whole bunch of users across the world could draw images for what they perceived an object to look like but what we found is this model started to collect data was that most of the images that users drew of shoes looked like that shoe in the top right the blue shoe so over time as the model saw more and more examples it started to learn that a shoe looked a certain way like that top right shoe and wasn't able to recognize the shoe in the bottom right the orange shoe even though we were able to get data from a diverse set of users even that the shoes that the users show choose to chose to draw or the users who actually engage with the product at all we're skewed and led to skewed training data in what we actually received this is a social issue first which is then exacerbated by our technical implementation because when we're making classification decisions that divide up the world into parts even if those parts are what is a shoe and what isn't a shoe we're making fundamental judgment calls about what deserves to be in one part or what deserves to be in the other it's easier to deal with when we're talking about shoes but it's harder to talk about when we're classifying images of people an example of this is the Google Clips camera this camera was designed to recognize memorable moments in real time streaming video the idea is that it automatically captures memorable motion photos of friends of family or even of pets and we designed the Google Eclipse camera to have equitable outcomes for all users it liked all of our camera products should for all families no matter who or where they are it should work for people of all skin tones all age ranges and in all poses and in all lighting conditions as we started to build this system we realized that if we only create a training data that represented certain types of families the model would also only recognize certain types of families so we had to do a lot of work to increase our training data coverage and to make sure that it would recognize everyone we went global to collect these data sets collecting data sets of different types of families in different environment conditions in different lighting conditions and in doing so we were able to make sure that not only could we train a model that had diverse outcomes but that we could also evaluate this constrained on a whole bunch of different variables like lighting or space this is something that we're continuing to do continuing to create automatic fairness tests for our systems that we can see how they change over time and to continue to ensure that they're inclusive of everyone the biggest lesson we've learned in this process is how important it is to build training and evaluation datasets that represent all the nuances of our target population this both means making sure that the data that we collect is diverse and representative but also that the different contexts of the way that the users are providing us this data is taken into account even if you have a diverse set of users that doesn't mean that the images of shoes you get will be diverse and so thinking about those nuances and the trade-offs that might occur when you're collecting your data is super important additionally it's also important to reflect on who that target population might leave out who might not actually have access to this product where are the blind spots and who we're reaching and lastly how will the data that you're collecting grow and change over time as our users use our products they very rarely use them in exactly the way we anticipated them to and so what happens is the way that we collect data or the data that we even need to be collecting changes over time and it's important that our collection methods and our maintenance methods are equally diverse as that initial process but even if you have a perfectly balanced wonderful training data set that doesn't necessarily imply that the output of your model will be perfectly fair also it can be hard to collect completely diverse data sets at the start of a process and you don't always know what it is that you're missing from the beginning where are your blind spots in what you're trying to do because of that it's always important to test to test and measure these issues at scale for individual groups so that we can actually identify where our model may not be performing as well and where we might want to think about more principled improvements the benefit of measurement is also that you can start tracking these changes over time you can understand how the model works similar to the way that you would always want to have metrics for your model as a whole it's important to think about how you slice those metrics and how you can provide yourself a holistic understanding of how this model or system works for everybody what's interesting is that different fairness concerns may require different metrics even within the same product experience a disproportionate performance problem is when for example a model works well for one group but may not work as well for another for example you can have a model that doesn't recognize some subset of users or errors more for that subset of users in contrast a representational harm problem is when a model showcases an offensive stereotype or harmful Association maybe this doesn't necessarily happen at scale but even a single instance can be hurtful and harmful to a set of users and this requires a different way of stress testing the system here's an example where both of those metrics may apply the screen shot you see is from our jigsaw perspective API this API is designed to detect hate and harassment in the context of online conversations the idea is given a particular sentence we can classify whether or not that sentence is perceived likely to be toxic we have this API externally so our users can actually write sentences and give us feedback and what we found was one of our users articulated a particular example that you see here the sentence I am straight is given a score of point zero four and is classified as unlikely to be perceived as toxic where's the sentence I am gay was given a score of 0.8 6 and was classified as likely to be perceived as toxic both of these are innocuous identity statements but one was given a significantly higher score this is something we would never want to see in our products and some in an example we not only wanted to fix this immediate example but we actually wanted to understand and quantify these issues to ensure that we could tackle this appropriately the first thing we looked at was this concept of representational harm understanding these counterfactual differences for a particular sentence we would want this sentence to be classified the same way regardless of the identity referenced in the sentence whether it's I am Muslim I am Jewish or I am Christian you would expect the score perceived by the classifier to be the same being able to provide these scores allowed us to understand how the system performed it allows to identify places where our model might be more likely to be biased and allowed us to go in and actually understand those concerns more deeply but we also wanted to understand overall error rates for particular groups were there particular identities where when referenced in comments we were more likely to have errors versus others this is where the disproportionate performance question comes in we wanted to develop metrics on average for a particular identity term that showcased across a set of comments whether or not we were more likely to miss classify this was in both directions misclassifying something as toxic but also is misclassifying something as not toxic when it truly was a harmful statement the three metrics you see here capture different ways of looking at that problem and the darker the color the darker the purple the more likely we were to have error rates and you can see that in the first version of this model there were huge disparities between different groups so okay we were able to measure the problem but then how do we improve it how do we make sure this doesn't happen a lot of research has been published in the last few years both internally within Google as well as externally that look at how to train and improve our models in a way that still allows them to be stable to be resource efficient and to be accurate so that we can still deploy them in production use cases these approaches balance the simplicity of implementation with the required accuracy and quality that we would want the simplest way to think about this problem would be through the idea of removals or block lists taking steps to ensure that your model can't access information in a way that could lead to skewed outcomes take for example the sentence some people are Indian we may actually want to remove that identity term altogether and replace it with a more generic tag identity if you do this for every single identity term your model wouldn't even have access to identity information it would simply know that this sentence referenced an identity as a result it couldn't make different decisions for different identities or different user groups this is a great way to make sure that your model is agnostic of you know a particular definition of an individual at the same time it can be harmful it actually might be useful in certain cases to know when identity terms are used in a way that is offensive or harmful if a particular term is often used in a negative or derogatory context we would want to know that so we could classify that as toxic sometimes this context is actually really important and it's but it's important that we capture it in a nuanced and contextual way another way to think about it is to go back to that first lesson and look back at the data we can enable our models to sample data from areas in which the model seems to be underperforming we could do this both manually as well as algorithmically on the manual side what you see on the right is a quote collected through Google's project respect effort through project respect we went globally to collect more and more comments of positive representations of identity this comment is from a pride parade where someone from Lithuania talks about their gay friends and how they're brilliant and amazing people positive reflections of identity are great examples for us to train our model and to support the model in developing a context and nuanced understanding of comments especially when the model is usually trained from online comments that may not always have the same flavor we can also enable the model to do this algorithmically through active sampling the model can identify the places where has the least confidence in its decision-making where it might be underperforming and it can actively go out and sample more from the training data set that represents that type of data we can continue to even build more and more examples through synthetic examples similar to what you saw at the beginning we can create these short sentences like I am he is my friends are and these sentences can continue to provide the model understandings of when identity can be used in natural context we can even make changes directly to our models by updating the models loss functions to minimize difference in performance between different groups of individuals adversarial training and min diff lost two of the research methods in this space have actively looked at how to affect your lost function to keep the model stable and to keep it lightweight while still enforcing this kind of a penalty what you saw earlier were the results of the toxicity v1 model and as we made changes especially in terms of creating manual synthetic examples and agog menteng the data performance we were able to see real improvements this is the toxicity v6 model where you can see that the colors get lighter as the performance for individual identity groups gets better we're really excited about the progress that we've made here but we know that there is still a long ways to go the results you see here are on synthetic data short identity statements like I talked about earlier but the story of bias can become much more complex when you're talking about real data comments that are actually used in the wild we're currently working on evaluating our systems on real comments building up these data sets and then trying to enhance our understanding of performance and improvements in that space while we've still seen progress on real comments and improvements from our changes we know that this will actually help more once we start looking at these real data sets and actually there's a cago competition live now if you're interested in checking this out more overall the biggest lesson is test early and test often measuring your systems is critical to actually understanding where the problems exist where our users might be facing risk or where our products aren't working the way that we intend for them to be also bias can affect the user experience and cause issues in many different forms so it's important to delve out methods for measuring the scale of each problem even a particular single product may manifest bias in different ways so we want to actually be sure to measure those metrics also the other thing to note is it's not always quantitative metrics qualitative metrics user research and adversarial testing of really actually stress testing and poking at your product manually can also be really really valuable lastly it is possible to take proactive steps and modeling that are aware of your production constraints these techniques have been invaluable in our own internal use cases and we will continue to publish these methods for you to use as well you can actually go to ml fairness comm to learn more I also want to talk about design and this is our third lesson for today because context is really important the way that our users interact with our results is different and our design decisions around the results have consequences because the experience that a user actually has with a product extends beyond the performance of the model it relates to how users are actually engaging with the results what are they seeing what kind of information are they being given what kind of information do they have that maybe the model may not have let's look at an example here you see an example from the Google Translate product and what you see here is a translation from Turkish to English Turkish is a gender-neutral language which means that in Turkish nouns aren't gendered and he she or it are all referenced through the pronoun oh I actually misspoke I believe not all nouns are gendered but some may be thus while the sentence is in Turkish in this case don't actually specify gender our product translate it translates it to common stereotypes she is a nurse while he is a doctor so why does that happen well Google Translate learns from hundreds of millions have already translated apples from the web and it therefore also learns the historical and social trends that have come with these hundreds of millions of examples the historical trends of how we've thought of occupations in society thus far so it skews masculine for doctor where is it skews feminine for nurse as we started to look into this problem we went back to those first two lessons okay how can we make the training data more diverse how can we make it more representative of the full gender diversity also how could we better train a model how could we improve and measure the space and then make modeling changes both of these questions are important but what we started to realize is how important context was in this situation take for example the sentence Casey is my friend let's say we want to translate it to Spanish in which case friends could be amigo the masculine version or Amiga the feminine version well how do we know if Casey is a male a female or a gender non-binary friend we don't have that context even a perfectly precise model trained on diverse data that represents all kinds of professions would not have that context and so we realized that even if we do make our understandings of terms more neutral and even if we were to build up model precision we would actually want to give this choice to the user who actually understands what they were trying to achieve with the sentence in the translation what we did is choose to provide that to our users in the form of options and selections we translate friend both to amigo and to Amiga so that the user can make a choice that is informed based on the contexts that they have currently the solution is only available for a few languages and it's also only available for single terms like friend but we're actively working on trying to expand it to more languages and also trying to be inclusive of larger sentences and longer context so we can actually tackle the example you saw earlier we're excited about this line of thinking though because it enables us to think about fairness beyond simply the data on the model but actually is a holistic experience that user engages with everyday and trying to make sure that we actually build those communication lines between the product and the end consumer the biggest lesson we learned here is that context is key think about the ways that your user will be interacting with your product and the information that they may have that the model doesn't have or the information that the model might have that the user doesn't have how do you enable the users to communicate effectively with your product but also get back the right transparency from it sometimes this is about providing user options like you saw with Translate sometimes it's also just about providing more context about the models decisions and being a little bit more explainable and interpretable the other piece is important is making sure that you get feedback from diverse users in this case this was users who spoke different languages and who had different definitions of identity but it's also important to make sure as you're trying to get feedback from users now you think about the different ways in which these users provide you feedback not every user is equally likely to be accepting of the same feedback mechanism or equally likely to proactively give you feedback and say a feedback form on your product so it's important to actually make sure that whether that be through user research or through dogfooding or through different feedback mechanisms in your product that you identify different ways to access different communities who might be more or less likely to provide that information lastly identify ways to enable multiple experiences in your product identify the places where there could be more than one correct answer for example and find ways to enable users to have that different experience representing human culture and all of its differences requires more than a theoretical and technical toolkit it requires a much more rich and context-dependent experience and that is really at the end of the day what we want to provide our users we hope that those lessons were helpful they've been lessons that we've been really really grateful to learn and that we started to execute in our own products but what's next we're starting to put these lessons into practice and while we know that product development and ML fairness is a context-dependent experience we do want to start building some of the fundamentals in terms of tools resources and best practices because we know how important it is to at least start with those metrics start with the ability to collect diverse data start with consistent communication one of the first things we're thinking about is transparency frameworks we want to create and leverage frameworks that drive consistent communication both within Google but also with the industry at large about fairness and other risks that might exist with data collection and modeling we also want to build tools and techniques develop and socialize tools that enable evaluating and improving fairness concerns let's talk about transparency first today we're committing to a framework for transparency that ensures that we think about measure and communicate about our models and data in a way that is consistent this is not about achieving perfection in our data on models although of course we hope to get there it's about the context under which something is supposed to be used what are its intended use cases what is it not intended for and how does it perform across various users we released our first data card last October as part of the open images extended data set that you heard Jackie talked about earlier this data card allows us to answer questions like what are the intended use cases of this data set what is the nature of the content what data was excluded if any who collected the data it also allows us to go into some of the fairness considerations who labeled the data and what information did they have how was the data sourced and what is the distribution of it for open images extended for example while you can see that the geographic distribution is extremely diverse 80% of the data comes from India this is an important finding for anyone who wants to use this data set both for training or for testing purposes it might inform how you interpret your results it also might inform whether or not you choose to augment your data set with something else for example this kind of transparency allows for open communication about what the actual use cases of this data set should be and where it may have flaws we want to take this a step further with model cards here you see an example screenshot for the jigsaw perspective toxicity api that we talked about earlier with model cards we want to be able to give you an overview of what the model is about what metrics we use to think about it how it was architected how it was trained how it was tested what we think it should be used for and where we believe that it has limitations we hope that the model card framework will work across models so not just for something like toxicity but also for a face detection model or for any other use case that we can think of in each case the framework should be consistent we can look at metrics we can look at use cases we can look at the training and test data and we can look at the limitations each model card will also have the quantitative metrics that tell you how it performs here for example you can see an example set of metrics sliced by age you can see the performance on all ages on the child age bucket on the adult age bucket and on the senior age bucket so how do you create those metrics how do you compute them well we also want to be able to provide you the tools to do this kind of analysis to be able to create your own model cards and also to be able to improve your models over time the first piece of this set of tools and resources is open datasets the open images extended dataset is one of many datasets that we have and hope to continue to open source in the coming years in this example the open images extended dataset collects data from crowd-sourced users who are taking images of objects in their own regions of the world you can see for example how a hospital or food might look different in different places and how important it is for us to have that data with the live cago competition we also have open sourced a dataset related to the perspective toxicity API I mentioned earlier how important it is for us to look at real comments and real data so here the jigsaw team has open sourced a data set of real comments from around the web each of these comments is annotated with the identity that the comment references as well as whether or not the comment is toxic as well as other factors about the comment as well we hope that data sets like these continue to be able to advance the conversation the evaluation and the improvements of fairness once you have a data set the question becomes how do you take that step further how do you evaluate the model one thing you can do today is deep dive with the what-if tool the what-if tool is available as a tensor board plugin as well as a jupiter notebook you can deep dive into specific examples and see how changing features actually affects your outcome you can understand different fairness definitions and how modifying the threshold of your model might actually change the goals that you're achieving here's a screenshot of the what-if tool what you see here on the right is a whole bunch of data points that are classified by your model data points of a similar color have been given a similar score you can select a particular data point and then with the features on the right you can actually modify the feature value to see how changing the out changing the input would potentially change the output for example if I change it to the age defined in this example does it actually change my classification if it does that might tell me something about how age is influencing my model and where potentially there may be biases or where I need to deep dive a little bit more we also hope to take this a step further with fairness indicators which will be launched later this year fairness indicators will be a tool that is built on top of tensorflow model analysis and as a result can work end to end with the tf-x pipe i'm tf-x stands for tensor flow extended and it's a platform that allows you to train evaluate and serve your models all in one go and so we're hoping to build fairness into this workflow and into these processes but fairness indicators will also work alone it'll work as an independent tool that can be used with any production pipeline we hope that with fairness indicators you'll be able to actually look at data on a large scale and see actually how your model performs you can compute fairness metrics for any individual group and visualize these comparisons to a baseline slice here for example you can see the baseline slice as the overall average metric in blue and then you can actually compare how individual groups are individual slices compared to that baseline for example some may have a higher false negative rate than average while others may have a lower we'll provide feedback about these main metrics that we believe have been useful for various fairness use cases you can then use fairness indicators also to evaluate at multiple thresholds to understand how performance changes and how may be changes to your model could actually lead to different outcomes for different users if you find a slice that doesn't seem to be performing as well as you expect it to you can actually take that slice further by deep-diving immediately with the what-if tool we will also be providing confidence intervals so that you can understand where the differences that you seeing are significant and where we may actually need more data to better understand the problem with fairness indicators we'll also be launching case studies for how we've leveraged these metrics and improvements in the past internally in our own products we hope that this will help provide context about where we found certain metrics useful what kinds of insights they provided us and where we found that certain metrics actually haven't really served the full purpose will also provide benchmark data sets that can be immediately used for vision and text use cases we hope that fairness indicators will simply be a start to being able to ask questions of our models understand fairness concerns and then eventually over time improve them our commitment to you is that we continue to measure improve and share our learnings related to fairness it is important not only that we make our own products work for all users but that we continue to share these best practices and learnings so that we as an industry can continue to develop fairer products products that work equitably for everybody one thing I do want to underscore is that we do know that the in order to create diverse products products that work for diverse users it is also important to have diverse voices in the this not only means making sure that we have diverse voices internally working on our products but also means that we include you as the community as in this process we want your feedback on our products but we also want to learn from you about how you're tackling fairness and inclusion in your own work what lessons you're learning what resources you're finding useful and we want to work with you to continue and build and develop this resource toolkit so that we can continue as an industry to build products that are inclusive for everyone thank you [Music] 