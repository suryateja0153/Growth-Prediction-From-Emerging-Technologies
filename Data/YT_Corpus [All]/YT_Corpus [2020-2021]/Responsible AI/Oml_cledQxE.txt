 first I would like to make a little bragging about the incredible breakthroughs that we are seeing in a high we're seeing like if you look at that timeline we're seeing amazing breakthroughs like really amazing to the point that is not about what technology is capable of doing we're past that point technology AI technology is so powerful that we're beyond of that question of is technology able to do those things we're now in the question of what technology should be doing so not what is possible but actually what is what is supposed to be doing and how is it supposed to empower us and how is it supposed to help us versus the technology for the sake of Technology right and the way that we are thinking about this is we have set up some ethical principles that you probably have read about if you read the future computer these are the six ethical principles that Microsoft is we're living by it it's more than only talking about it it's more living living those principles and this is I'm going to do a brief introduction on these principles but the point here is that what we are trying to share with you today is how we are bringing these principles into reality and as I said this is a lot of things and it's not only about technology it's about processes it's about wanting to do the right things it's about trying it's about learning about what you are doing and what you need from us and we have to do this with with a really open mind and with with a lot of humbleness because there are a lot of questions that we don't know the answers so having said that for us the most important thing is accountability we all are accountable of making responsible technology not only Microsoft the whole technical community the whole business community we are all accountable of this and this is like the foundation of everything if we don't feel ourselves accountable we have an issue this is all our accountability not only Microsoft but ever then the second one is transparency because you cannot trust a technology that you don't really understand how it works so being transparent again it's a really broad topic we're going to talk today about intelligibility but that's just one part of transparency transparency it's about open sourcing things it's about documenting things it's about understanding how a dataset behaves right it's it's a broader topic like that that we would be happy to discuss with you later and then the other four that you see up there fairness reliability and safety privacy security and inclusiveness those are all very important principles and if you haven't read the future computed I'd really recommend that book I mean it's free so it's an easy recommendation you don't have anything to lose by reading it and it's a really good book it is it's eye-opening and I encourage you all to come to the booth and learn more about these principles but having said that today's session is not about telling you about the principles it's about showing you how we are putting some of these principles into action to be concrete it's about intelligibility and privacy in AI and I'm doing this disclaimer because I want to reinforce again that this is an incredibly broad topic incredibly complex that spans a lot of different things from technology to processors to the mindset of people to culture to it's it's huge we could be days talking all over T's so come by our booth to learn more I can I will be here three days and I'm capable of speaking for three days straight trust me on this so come by and I will be happy to discuss all these things with you and now let's go for the specific things one of the big challenges that we have in AI is privacy yes let's face it and Kim is going to be introducing this topic but let's face it when we are using cloud services we are exposing our data it could be location data it can be health data it can be finance data but we are exposing our data and we believe that that's the only way to do it we believe that if we don't expose our data we can't get the services actually it turns out that we can and Kim has been researching this kind of technology for years so I'm not going to make my introduction longer Thank You Kim Thank You Esther all right of course basically everyone loves this is a very technical audience we all understand the benefits we all understand the downsides also and the limitations of AI and one of the key questions that are very relevant today with with this technology becoming more common is who exactly will have access to my data well what do I mean by this all these machine learning techniques technologies are data hungry learning and just using the model it needs raw data and then it can give you that prediction that valuable prediction that you want if want to learn that model you want to chain that model it requires tons and tons of raw data and in this sense data privacy itself is not really built into AI workflows in fact one could even go to the extent of saying that data privacy softn at odds with the goals of learning the whole point of learning is that the model somehow acquires information about training data well again since this is such a technical audience maybe this is very obvious to many of you let me just mention a few special examples that I really like health data privacy is super super critical if I'm hoping to use some kind of health predictive model and I'm gonna send my data somewhere to be predicted on I certainly have a lot of questions about how my data is handled same for financial data what about calendar data some services could like offer to optimize your day your schedule but that's extremely private data location data well sometimes I want people to know where I am sometimes I don't of course there's many many other types of data that that you could consider here and that are used by all types of and all kinds of apps and services so are you okay sharing this data with service providers and in what situations this is not a simple question it depends and what are their options so typically you have two options one of them is you share everything and you get the service or you share nothing and you don't get the service but now you keep your privacy but this is not really a fair fair choice at Microsoft what we've been thinking about is how could we do something better that's why I wanted to introduce the technology called homomorphic encryption our morphic encryption is a special a new kind of encryption technology it's very different from familiar encryption like AES or RSA in fact homomorphic encryption allows you to compute on encrypted data so this for example AES AES allows you to encrypt data and store it forever but as soon as you want to do any operation on it you have to remove that encryption and then you can compute it one more thing encryption allows you to compute on the data without a key so let me show you what I mean with this little schematic here we have a user on the left-hand side and the cloud service on the right-hand side the user has data they choose a key and they encrypt their data and they can store it in the cloud service this is all standard but because we have used homomorphic encryption here we can actually compute on the encrypted data directly if you have very good eyes you can see that the color of the files has slightly changed changed which is an indication of the data being changed somehow so the idea is that on the on the right hand side the cloud performs a computation on the encrypted data but as you can see the result of the computation remains encrypted and only the customer only the client has the key now the encrypted data can be sent back and with the key the encryption can be removed and the result can be revealed to the client excellent well now your next question would be then how do I get this how do I do this and here's how we have created and released the library called Microsoft seal so it's a homomorphic encryption library it's a low-level library developed in c++ 17 it has dotnet standard wrappers and this is this has actually been developed since 2015 but only last year we open sourced it under an MIT license and we're actively developing seal today the source code is available on it on github and you can download it from that URL there's a fun little picture of the core people involved in the project great so if any of you have heard about homomorphic encryption before odds are that you've heard things like it's very complicated very mathematical may be hard to use may be very slow this was probably true when you heard it but with Microsoft seal we're trying to change the situation so here we have a performance overhead graph and actually homomorphic encryption is very new it was only invented in 2009 and it was extremely slow at that time and for the couple first years it was absolutely extremely slow as you can see we're talking about 10 orders of magnitude performance hit which is really really really since then however until today 2019 the performance overhead has come down massively and today we're looking at FPGA GPU acceleration to this technology and we expect further significant improvement in the performance usability these abuses another thing morphic encryption doesn't have to be difficult to use I believe we can make it easier I think I believe we can make it accessible to normal developers the people who are not mathematicians and not necessarily cryptographic experts this is what we're trying to do with Microsoft zeal as one example of of the practicality of the of this technology we wanted to show you some some kind of demonstration and we chose to do a fitness tracker why a fitness tracker it's because a fitness tracker fitness trackers typically have some significant privacy issues with them because what how does the how do this apps work they record your location maybe they record other health information about you they store it in a cloud service and at some later time you can look at summary statistics or some kinds of predictions on your phone but all of this data is actually stored in some cloud service well this may not be what you want that's why we decided that well let's try to implement our own private fitness tracker which encrypts your location data since the encrypted data to a sure store sit there in encrypted form and whenever you ask oh how much did I run on this and this day the computation the summary statistics some kinds of run classifications are done on the encrypted data in the cloud and thus your phone only gets the encrypted result which it can decrypt your phone only ever has the key you can in fact try this yourself you can come visit our booth the responsible AI booth and we're hoping to release the source code very shortly of course I can talk and talk about this but it's another thing to see it in action on the left-hand side there you can see the fitness tracker this is some Android emulator running it and it's simulating a run but as you can see for all practical purposes this is witness tracker it's there's nothing special in it in some sense from the users point of view you don't see that this uses homomorphic encryption under the hood however on the right hand side we see Ash's view of it this is the database and well it's not rendering those characters very well because it's encrypted binary data so there is it's very small but it says location time run route in those columns but it's all encrypted and I sure never sees this data only the phone sees the data so now go download see he'll go try it out it's all public you can even do pull requests and I hope that he visit our booth Thank You Kim so I'm guessing that all of you can understand a little bit when I was explaining to you that for me this is magic how can you compute an encrypted data it's mind-blowing at least for me I mean it's like I love the way that Kim explained it to me before he said it's like encryption it's like a black box you put your data in a box and you send the box to a container right but actually homomorphic encryption is the ability to view the box to touch the box and do things with them and get something different out of that but without ever seeing that's in the box that's really amazing and I'm going to brag a little bit on his behalf cuz you didn't brag too much so I'm going to break a little bit on his behalf and I'm going to tell you that this team the team that Kim line is part of were the first ones that proof that homomorphic encryption was actually something that you could do on deep neural nets that was an amazing breakthrough so this research is really really advanced and I encourage you all to read the papers spirit seal and come to a booth of course and now we're moving to intelligibility listening really important thing in machine learning understanding how a model is behaving is fundamental to trust right and heart is going to tell you a very beautiful story that's actually grounded in truth so it's the story of how the researcher understood this problem and the boat at the next 20 years to solve it so I really hope that you will experience this part of the representation as personal as it is and I'm introducing you harson are a data scientist that was yeah I planned that I I have a request for you guys there are some kind of code snippets and demos so if you are interested in being able to read those please take a minute to try to come up I realize it might be a little hard there's always a tricky balance between showing you guys real code and glassy slides right so yeah if anyone if everyone could like come up a little bit and fill kind of some of the empty seats just if you want to see that the code snippets that'd be awesome and yeah this presentation will be I mean a story that's that one of my my teammates worked on several years ago and then some real code with some real tools that we built to help solve these problems so I'll start with the story I've got some good news and some bad news the bad news is that you've all been diagnosed with pneumonia and about 10% of you are gonna die but the good news is that years ago we trained a deep neural net to predict which of you are likely to die and which of you are likely to live so that we can give you the appropriate treatment if you're at incredibly high risk of death you have to go to a hospital immediately but if you're at low risk the recommended treatment is to go home take some antibiotics drink some warm soup and call us if you're not feeling better in a couple days and that neural net that we trained turns out to be the most accurate model we could train on this data set we worked with a team of doctors to collect this data we tested out a couple configurations and this neural net outperformed everything else but there is more bad news the neural net is a black box we don't really understand it and we don't understand how it's making its decisions so we're afraid to use it on real patients and I'll tell you a little bit about why when we looked into the neural net we discovered something interesting somehow the neural net learned that having asthma was good for you that somehow having asthma another respiratory illness reduced your risk of dying from pneumonia and this didn't sound right to us and it didn't sound right to the doctors we were working with who immediately told us that asthma is terrible for you if you have pneumonia it increases your risk and there's something wrong with the model we have to go fix it but the doctors told us something else they had an explanation for why the neural net might have learned this pattern you see if you're an asthmatic you're gonna notice problems with your breathing almost immediately and you have a doctor you can call when your inhaler might not be working in solving that problem they're gonna recommend you go get checked out and when you get to the hospital and they find out that you have pneumonia and asthma they're gonna give you very high quality care very quickly so in fact asthma is not good for you as most terrible for you but in this model it's acting as a proxy for getting high quality care getting diagnosed quickly and noticing your symptoms early all of which genuinely do reduce your risk it's not a problem with the model the model is just learning statistical relationships in the data but it is a problem with the data set and it took a lot of effort for us to be able to discover this using interpretability techniques so that brings us to this age-old trade-off we we can get great accuracy using modern machine learning techniques like boosted trees and neural nets but it's been hard to get that level of accuracy with intelligibility with the ability to understand how the model is making its decisions and in fact this has kind of been in the minds of every data scientist that there's a class of models that are really high accuracy like boosted trees random forests and neural nets but that aren't very intelligible and on the other end of the spectrum we've got models like logistic regression and decision lists that are highly interpretive all but they just haven't been able to match the accuracy of these more powerful models so in Microsoft research we've been thinking about this for a number of years and today we're releasing an open-source package with a brand new learning method that we're calling the explainable boosting machine the EBM explainable boosting machine combines the power of modern machine learning techniques like gradient boosting and bagging but it constrains them in a way to keep them really interpretable like a simpler model like logistic regression so there's two things we're claiming here we're claiming we've got great performance and that we've got interpretability but we'll talk about the performance one first it's if you're in the back you might not be able to see these numbers but we ran the EBM on a wide variety of data sets across a wide variety of domains other medical problems financial problems even business problems like churn and we find that the EBM is competitive we were up there with XG boost and random forests so-called state-of-the-art models of today but what's more interesting for us is how we get this accuracy while staying completely interpretable and for that I'm gonna switch to a Jupiter notebook to show you some real code and how this model works I think I can figure this a little switching machines all right okay great so all I've got here is a Jupiter notebook it's an interactive Python environment and all I'm doing up here is loading in that same pneumonia medical data set that we talked about earlier and setting up a standard machine learning experiment splitting the data into a training set and a testing set and then now we're getting into kind of the more interesting part with just two lines of code I'm gonna train the explainable boosting machine if you guys have ever used scikit-learn in python it's the exact same API so training a model there and training our model are exactly equivalent but what gets interesting is what happens after we've trained this model with the explainable boosting machine you not only get a powerful predictive model you can see exactly what the model learns feature by feature and really understand how it's making its decisions and how it works so we have an interactive nope Jupiter notebook experience and a zoom out a little bit that shows these kind of interactive plots to help you dive into this model and understand how it's working and it might be a bit hard to see but the first thing we see is kind of this ranking of features what features to this model find important overall we see things like your blood pressure was probably important in your risk for pneumonia your respiration rate and what I found most important was your age so now that we know that since it's the most important feature we can dive into it a little bit and see exactly what this model learned for this feature so this top graph here is what the model learned the x-axis is your age and the y-axis is what the model thinks your risk is and immediately we start to see some interesting things your risk seems to stay low and flat from the ages of about 20 all the way up to 50 or so and this makes sense and it made sense for the doctors we were working with your risk really doesn't change that much if you're a 33 year old or a 25 year olds after about 50 your risk starts to I'm steadily until about 85 and then again at 85 it tapers off and stays flat until about a hundred and then we see something interesting somehow the model thinks that your risk reduces after you're a hundred that somehow a hundred and two-year-old is less is is that lower risk than a ninety five year olds and that doesn't make sense so we asked our doctors and and they had an explanation for us again the doctor said that it's pretty rare for someone who's over a hundred to come in to the hospital and when they do and we find out that the only illness they have is pneumonia a treatable illness there's a there's a kind of human motivation so to push it let's give them the most aggressive treatment we can I mean we're working with a centenarian here let's try to help them set the record so we think that the drop at a hundred is actually a social factor like a hundred doesn't mean anything to our bodies but it does mean something to us as humans it's a nice round number and these types of effects get coated in the historic data that we collected and of course get coded in the models that we see so this is what the EBM learned but it might be interesting to compare this so another interpretable model that you might be more familiar with and we can really look at the differences so we're gonna turn to the gold standard of interpretability logistic regression and we're gonna look at what the differences are between what the EBM learns and what the logistic regression learns so on the right is that same EBM graph just squished a bit and on the left is the logistic regression and almost immediately you can see the differences the logistic regression is a very constrained model it's only allowed to learn a single line for every feature so it thinks that risk is uniformly increasing as your years grow the difference between being 40 and 20 is the exact same as the difference between 80 and 60 and because of that it's not expressive enough to capture the real patterns in the data set I mean when we talk to the doctors they agreed with us your risk should stay flat between 20 and 50 there's really no physiological difference so picking the right model for the job can be can make a very big difference in how accurate your models can get and so the next question is exactly that accuracy I mean it's it's great if we have a highly interpretive Oh model but if it does a terrible job at predicting it's not really good for anyone so to check out the accuracy we're gonna compare the EBM the logistic regression and we decided to throw a black box model in there too we're gonna train a random forest with a thousand trees in it and see what kind of performance we can eke out of that forest in here we've got a dashboard experience it's a little constrained in the notebook so I'm just gonna pop it into a new tab so we can look at it but all of this code is running locally on this device there's no it's all hosted on localhost it's not talking any servers or anything it's just a nice visualization platform and when we look at the model performance these are ROC curves so a standard machine learning metric for for classification problems we see that as you might expect the black box model outperforms the logistic regression but surprisingly in this data set the EBM outperforms the black box that we can throw at it too so we've got a model that's highly interpretive all it's flexible enough to represent the data well and it's getting great performance and we can look into this model overall and see exactly what it's learning and make adjustments when we see things that are problematic but the last question always becomes what if this model makes a prediction about me what if the model tells me that I'm really high-risk what are the factors that mattered in my prediction I don't want to look at what the models learning overall I want to know exactly why it thinks that I'm high or low risk so in this tool we're also including what we call local interpretability where you can see exactly how an individual decision is made and break it down so we're gonna take a look at the EBM and we picked some an interesting person out of this data set to dive deep into what you're seeing here is is a breakdown of the important features in this person's prediction if the bar is orange and to the right it means it's increasing their risk and if it's blue and to the left it's decreasing their risk so this person got a prediction probability of death of about 40 percent which is pretty high but it turns out they actually did pass away as a result of pneumonia so the real prediction you could say it should have been a 1.0 so now the question is why didn't the model predict even higher risk so the factors that are increasing risk seem to make sense for us their blood pressure is abnormal their age is 90 all of these really should be pushing risk up but when we look at the factors that are pulling that risk score back down we see as you might expect asthma this person had asthma and it pulled their risk prediction down because of those act artifacts in the data set that we talked about in fact there's only one feature that's even lower risk than asthma and it might be hard to see here but it's a history of chest pain which is even more surprising to us but the explanation is very similar to the asthma story if you have a history of chest pain and you notice something wrong with your chest I mean you might think you're having a heart attack you're gonna get to the ER immediately you're gonna be able to cut lines and you're gonna get the most high-quality care possible so just like the asthma case and just like the age being over a hundred this history of chest pain is a flaw in the data set that's encoded in the data but with interpretable models we can see those flaws and and do something about them so I'm gonna switch back to the PowerPoint it's gonna do a magic transition one thing I want to really stress is every single data set we've ever looked at has these flaws it's not just high-risk medical situations or loan approvals and banks although we have looked at those but even bog-standard business problems have these data sets we found flaws with the way we new churn modeling we found flaws with how we draw reliability curves every single data set has these artifacts and I guarantee that your data sets have them too but now that we have interpretable modeling that's ok we can find these flaws we can adjust for them and we can build more robust models as a result so we're releasing a tool today called interpret ml it's on github and I'm gonna tell you a little bit about what we're including in there the first component is this idea of glass box models these are models designed for interpretability it includes the explainable boosting machine but also models like linear models decision trees and rule lists so you can compare them and find the ones that work best for your needs on the other hand we know that sometimes you've got a black box model already that you want to try to explain maybe tease some explanations out of it and for that we're including a suite of popular tools to help you open up the black box tools like partial dependence plots sensitivity analysis Liman shop all of these tools are being combined and released in a package called interpret ml we just got on our github today so we're really excited we're hoping you guys go go check it out it comes with this nice unified API for all of these different methods so if you train one of these you can train all of them it comes with those nice interactive visualizations that we talked about earlier and we've got a small but really eager and active development team working on making this thing better and better so please give it a try check out our github and and please give us your feedback we really want to work with you all and see how we can make interpretability available for everyone and with that I'm turning it back to Esther [Applause] you I'm assuming I'm assuming that you probably have a lot of questions so the way to do questions in this session is to come up to the mic which I know can be intimidating but stand up and ask your questions because if you don't I'm going to start asking mine and as I said before I can speak for days so you don't really want don't want me to do that so think about your questions and while you're thinking about them I'm going to introduce you to Scott huger birth Scott is one of the engineering leaders at Microsoft that's looking at how we well he's looking at a lot of things so he's looking at how to scale these kind of technologies across all of engineering practices and not only these so the the way that I told I spoke before about the complexity of all these responsibilities considerations Scott is really thinking hard about this whole lifecycle and how can we improve our engineering processes so any question that you have on the topic Scott will probably be able to answer no pressure Scott and and I will handle my microphone my microphone to him so please come by and ask your questions and gray we have questions so I won't start with mine Ben first thing I'm gonna do is I'm gonna have the other experts come up so we're all ready up here great let's started off I have a question actually Scott I've seen in your title ethical and society I'm curious about your take on we're very close to to getting autonomous self-driving cars there are numerous examples where a car an autonomous car probably determines that it will run into a crush situation and has a choice between killing the driver or killing five other innocent bystanders or so how do you build these types of ethical questions into a great question in something that gets brought up a lot in our field as everyone in this room is probably super aware there's no clear answer and so what we've developed you know I think that our approach is to get at that nuance because it's a very nuanced question right it it ends up not being an if statement you know if baby break if not baby you know do something else it's really about understanding how the overall system works and so you know I won't go into too much detail but how we think about it here is the first thing we do is we say hey we have these ethical principles at Microsoft and how are we gonna bring those into the engineering cycle at all levels because the autonomous car is is one example but there's also the question the Harsha brought up right which is you know should we deploy this model that we don't know how it works you know in a healthcare situation which could impact your healthcare and so the first thing we want to do is at a broad level understand how these things are making decisions and then kind of make the call on where we put them and how we use them and so at Microsoft that really comes down to all of our ethical principles and then it the next layer down is how we bring those into the engineering cycle through best practices the tools that we've talked about here and different approaches does that answer it yeah I mean soup super complex I think the the challenge is is you know we don't I guess maybe to answer more directly what I don't want at least teams at Microsoft to do is get caught up in these theoretical really tough conversations what I want them to do is think really hard about how the software they're building is going to be used and how they mitigate that and tailor that as much as they can using all these different approaches and so it's an autonomous car team but it's a healthcare team or it's just someone in office that's trying to build an intelligent service that they are laser focused on who they're trying to solve and solve it the best way hi I have two specific questions first for I believe your name's Kim on homomorphic so one of the main applications often is multi-tenant data right so you might have multiple customers each who would like to have ownership of their data but you would like to aggregate all the data across your customers and train a model together is that possible with Hama morphic encryption lost time over fake encryption schemes well some more fake encryption schemes are like single key meaning that there is one party that sets the secret key and the public key so these are public key encryption schemes but there is one secret key one party that can decrypt there are some multi key systems to where multiple parties can kind of share the responsibility of decrypting in it is possible to set up collaborative scenarios where you know for example multiple parties use a shared public key to encrypt their data and store it in some kind of cloud service and do it then some kind of computation on this together maybe it's some analytics or something it gives an encrypted result which then can be decrypted using the secret key and well maybe one of those parties host the secret key maybe it's some kind of external party that holds the secret key and can reveal the result so maybe I'll just give an example so you know let's say I'm a company that does machine learning and then I've got two clients client a client B ia has their data they have their own secret key who they send their data encrypted to us they would like to have assurances that their data is not legible to anyone else ever find B same thing they you know want their data be only legible to them is it possible to do operations on their data together so it is not in using most of the homomorphic encryption schemes because in these in this particular scenario that you described you kind of want to data owners to do different secret keys and you can't cooperate like across different types of encryption because who is the who is supposed to be able to decrypt the result right but like I said there is some recent research on multi key encryption homomorphic encryption which seems like it can be possible but so currently seal actually Microsoft a seal does not have such a multi key functionality so perhaps in the future there might be some scheme by which it's I think it's possible yeah I mean I know it's possible and then if you don't mind this well second question really for interpret ml so pretty simple it's MIT license is that correct yeah MIT license all on github all our developments out on the open okay just a quick comment would be it's pretty amazing and I'm pretty sure I'm gonna use it probably within this month that's awesome yeah and I was wondering what are so you mentioned a few models that are under that glass box EEMA what's the roadmap for future models what are you guys thinking in terms of growing this library yeah so we're we're a team out of research and so we want to stay plugged into what's happening in research conferences and and kind of provide reference researchers implementations of new work I know that there is a lot of active work in the future on kind of building more reliable if-then rule systems so some people have empirically shown that you can get decent classification results by learning a set of really human interpretable rules like if these three conditions are true then predict this otherwise predict this and I think that's that's an area of interest for us so we're looking a lot more into trying to build out better rule systems but it's really about where the research goes on on our side we're trying to continue to improve EBM as well of course and make it even more robust awesome if we can I just add to that too just to give it a little bit of context to so you know I think there's no product announcements today make sure I still but but you can imagine Microsoft follow-up this is a significant priority and so you know Harsha talked about tabular data sets you can imagine visual data sets since each data sets are another focus and so I think you will continue to see kind of a constant release of iterations on things we've already released and new types of data sets and models in kind of the interpretability space yeah I I think that's fair it's not exactly like Scott said and and right now interpret ml to be clear focuses a lot on tabular structured data sets so that's that's where we're putting our first effort thank you good evening as this Barney from wants to mink and instead I think this is one of the most lively sessions I've seen since morning and thanks for that thanks for you be a man because I can truly see how easy it's gonna make the job of the data scientist in terms of explaining things we just always been traditionally difficult the question that I have is that Microsoft as an organization beyond enabling technology has a larger role which is the ethics conversation the area even having today are you guys partnering outside of the institutions any with government operations or EU or Gen Z's in terms of putting this in a more formal standard like becbs seek are the financial reporting is there anything that you guys are doing outside of the technology spectrum in terms of regulatory standards we we are I won't try to list them all out but I will tell you both Sachi our CEO and Brad Smith who runs our who's our president and chief legal officer spend a ton of time not only with industry groups like you'll see as part of like partnership for AI and a bunch of these groups but also just general regulation across the world because I think we think it's super important we get this right and so and we also believe that it's can't just be talked like there's lots of folks talking and and about this types of regulation and there's lots of principles and we love our principles but our job like folks in this stage is to make that real you know what does transparency mean for the data scientists building the model and so we have a really tight relationship with those bodies to say we've proven this out at Microsoft in a product now like this should give it more credence to be part of like a regulation or best practice so I think that is top of mind for us and will continue to be better thanks guys appreciate yes thank you hi I'm great session I really appreciated it I was curious though given the framing of the session why you didn't talk about bias time I wish I had a better answer but it really is that I mean we were talking as we were planning this session you're like where do we start we should just have our own conference because there are conferences on this you know yeah I mean in so bias I mean one of our we put bias and our fairness principle we focused every day on reducing things like bias in our data sets and our model decisions so we didn't touch on it here with that said the examples that Harsha gave are you know kind of this profound example of bias and how dangerous bias can be when you understand it we actually don't use the term bias anymore we use fairness because it's kind of funny actually promises systems you're trying to create something actually extremely biased you just want to know where it's biased and the best systems are just really biased in a way that makes sense and so anyway that's a long way of saying we didn't have time to go into it deeper but it's a huge focus area for us let you know that we are hosting a session on facial recognition challenges and a huge challenge of facial recognition is bias and that session will be tomorrow Tuesday at 8:30 p.m. so awesome so go there cuz Jacquelyn Crohn's is driving that session and it's really mind-blowing so definitely go to that session we were talking specifically about facial recognition but it's the approach exists triple to all the things that we are thinking about thank you thank you I will repeat what was said before that this session was really great and technical question about this especially the variables importance how it works compared for example to them boosted trees like meaning to decrease of Guinea index sure hear me right yeah yeah it's it's a good question I think the way I like to think about it is in importance measures based on like information gain or Gini criteria or any of these things are effectively heuristics right like you're effectively trying to rank things in a list and so you're looking at things like how much information do I gain by splitting on this criteria but there's there's an excellent paper by a student at University of Washington Scott Lundberg who talks about how lots of tree based interpretation methods are inconsistent like a heuristics can fail in multiple ways and so I think a lot of those met like I would really refer you to Scott's paper on how attribution can be kind of inconsistent how he creates these amazing experiments where he grows to almost identical trees and forces one to have a more important feature than the other and shows how all those attribution metrics can fail with interpreter with the explainable boosting machine our our kind of end of like the zoom into each feature is exactly what the model learned right so it is not a heuristic it is not a guess about how the model is performing like a partial dependence plot might be it is exactly what it learned because we constrained it in a way that forces it to represent the data that way so I guess the difference is kind of exact interpretability versus approximate interpretability right and I think with EBM you get exact with other attribution methods you're getting approximate risk for the patient actually was very high [Music] library has played something like in the in irrigation setting is called this prediction intervals or versus confidence interval intervals which are calculated for the whole population sorry could you repeat the last part I mean is there something calculated for for it like some values like prediction intervals in right now not exactly I think that's still a kind of an open area of research about computer when we speak about confidence interval when we interpret the expected value of the predictive variable right versus not also these yeah so there's there's an idea of kind of error bars on each individual feature that we learn but kind of how to aggregate those and and display it in the right user-friendly way for like each local prediction is still something that we're like thinking about but it's an excellent question it's it's something that's definitely been on our minds thank you Matt yeah very thank you for the question Microsoft has Auto ml package that Microsoft has the auto ml package where it will check which model will fit the best its EB I'm going to be part of the list of models as it goes through we don't have like product announcements yet we're just a small team of research the thing the you know the the answer or the public answer I'll give you is nothing to announce today but again you know this is an area of focus for the company you will see I think you will you know you can have confidence you'll continue to see a bunch of these types of tools start being released you know whether it's an auto ml or just kind of our overall you know a sure a I story I think you'll stay tuned thanks I just wanted to ask any thoughts on what interpretability might look like for things like image or sound classifiers yeah it's a great question and something that I think is another focus area for us and there are some approaches like lime you know that that Harsha touched on the key is how do we it's the human interpretability is the human understanding which sounds like you're well aware of and so I think the some of the approaches were most interested in are how do we help you understand classes you know of errors and then maybe drill all the way down in a way that allows the humans maybe understand it so for example if you're building a face trickling facial recognition system you know how do we help you understand that may be thicker rim glass or a thin rim glass is actually your problem with your accuracy and so I think you know in the same thing with speech you know what is it is a certain type of accent or a certain type of speed all those types of things I think building tools that spit out more than just this is the XY coordinate of the pixel that failed and your neural net I'm and actually saying no this is the area of the picture or this is the section of the speech I think is a an area that we're focused on the EBM you can see exactly where indicators you change those because they correlate with the data and presumably with what is gonna happen in the future if you're trying to model you know an outbreak of pneumonia or anything like that would you want to change that it's it's a great question right like I I think it really depends on your setting so in this setting this model was gonna be used to score future people coming in and deciding whether or not they get the right treatment and so in that case we do want to change it we don't want to penalize someone fresh coming in just because they're an asthmatic like the historic data has the effect of their treatment already kind of built into the label but we don't want to deny asthmatics in mass right but you could make the argument that maybe if you're an insurance company trying to decide these things that you should leave it in there because that's kind of an artifact of the real human process right so I think it's really situation dependent I think what we really like is that these models are editable and you can visually see what edits you're making and exactly how it'll affect the model so the power to edit these things after they've been trained without having to go add a bunch of synthetic data and try to tune your models with a bunch of hyper parameters like giving you the power to do that is kind of the first step right and then whether or not you do do it is highly situation dependent um I'll touch on one more example one thing that we sometimes have done in the past is train a model that includes features like race or gender right and because the data has bias in it the model will learn that bias too but you can see exactly what bias have learned and then zero the difference is out before you deploy it right so sometimes it's like I think the worst thing you can do is kind of hide the features from the model like drop gender or drop race and let that effect be hidden all the other features in your data set through correlation the best thing you can do sometimes is let these models learn the bias right and with editable models you can see where that biases and zero it out so that's another case of where we like to try to edit these things before we deploy them I guess it kind of is the same question again but in that case you're removing race or something like that out of there I know in a lot of medical cases like spontaneous pneumothorax it pretty much exclusively affects really tall white males so if you did edit that bias out of there you're all of a sudden you're looking is it anyone on the basketball team because it's not gonna happen it's almost 0% outside of the really tall white male sure for sure and in that case maybe you don't want to do it right like yeah I think you're exactly right like there are situations where you absolutely don't want to and there's situations where you absolutely should and it's kind of your job as a scientist to sit there and parse through these things right like so it sounds like you've got a really good hand on it just to add that like a global level I think they're everything's gray with these probabilistic systems and they're so snare EO specific and so what you're seeing you know kind of us do is say the first step is just understand what's happening and then the person building system can make the call and you know hopefully one day we'll move beyond that where you know we'll be able to answer these questions I'll you know ahead of time we're in some sort of automated way but I think step one is just how do we even understand how these you know these neural nets and all these things are making decisions which is a tough one to figure out right now yeah it's it's a good question we didn't touch on it for the sake of time but one thing the EPM does is detect what pairwise interactions are significant in the data set and learns those as features as well so you're right that interactions make a big difference as that and in certain data sets so we as part of the tool do learn those interactions and do visualize the in just a slightly different way than we do those one feature at a time and we find that in some datasets that makes all the difference right so spot-on question I wanted to wait a tasteful interval so but I wanted to kind of follow up with I think someone else said about sort of probabilistic you know in terms of interpret e it would be nice to get some sort of I think he said variants on various factors and that sort of thing which directly ties in with some of the later questions I think - and so what's the road map for that specifically I think for well for me at least the application that I'm much interested in and you know gets into responsible AI is I think a lot of times the responsible thing to say is I don't know and so having you know really good metrics around that like how certain are you of your result would be very very beneficial especially for a business-to-business product makes perfect sense no again no product announcements just know we're very committed and that's what I can say but I'll tell you you know the way we look at maybe this is the way to frame this space is how we talk to product teams is really just not in terms of responsible AI we talk to them in terms of quality right you know we you'll you'll hear if you talk to a prod team and says oh I talked to Scott like oh he gave me the speech but this B that really is we have decades of research in processes and tools around deterministic systems right QA processes we are at the start of that for probabilistic and and so all of these types of approach is that you know we're inventing our new and so you know I think your question is is great in it so I think the way to say this is in 30 years I think you'll say oh Microsoft has this incredible suite of QA processes and best practices and tools just like they do for deterministic systems at the same time we're also trying to deliver this messages if you don't know what it's doing if you don't know your what your neural network is making a decision and it could be punished me in an asthmatic or whatever then probably shouldn't be deployed and so that's kind of the how we look at the overall problem and specific so actually let's just make it can i is there an opportunity later in this conference to discuss this and other more technical problems yeah for sure where Sam's back there he's been working with me on this project we're both going to be at the booth the responsibility booth for the rest of the day today and all day tomorrow too so yeah I think that's probably the right venue for us to go in a bit more detail a little bit too if you want to grab us and I'm the timekeeper so thank you all for all the great QA thank you very much it's been great and we hope to see you at the booth and if you as a summary of everything that we discussed here all the links to everything that was sure today for you to recap and anything else we'll be at the booth so again thank you very much you've been great I hope you like the session thank you have a great field [Applause] 