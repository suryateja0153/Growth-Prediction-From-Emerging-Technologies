 Welcome, and thank you-all, for joining today. I'm so excited to be here with everyone virtually this year for Build. I'm a maker and an engineer at heart. So this event has always been one of my favorite opportunities to connect with the folks out there doing the real innovation on top of the platforms and tools that we provide. That innovation is something developers have been driving forward for decades since the dawn of the computing revolution. At Microsoft, our job has always been to empower you with the best possible tools for you to do your job, creating the world's digital future. In a present defined by uncertainty, faced with daunting challenges and unprecedented need, what you-all do is more important now than ever. Today, I'm here to talk to you about what's coming next, the technology trends that will shape how software and development look in the future. I'm lucky to have lived through several areas of the ongoing revolution and computing. I think you could argue that revolution is a bigger force shaping our world now than it ever has been. Each era, the computing revolution has been defined by boundaries and constraints. We overcome the constraints and push past the boundaries, and we then forget the boundaries existed as we celebrate breakthroughs on the path to ubiquity. The great author and futurist, Arthur C. Clarke profiles of the future contains a quote that I think embodies the spirit of exploration and discovery perfectly. The only way of finding the limits of the possible, is by going beyond them into the impossible. There are a number of big picture developments that are going to change how we all approach technology in the future. The end of Moore's Law, the explosion of data, the evolution of the edge among them. We've been able to take for granted that there's a PC on every desk and in every home for so long that we forget how impossible a goal that must have seemed in the late '70s and early '80s. Bill Gates wrote his first commercial program on an Intel 8008 microprocessor. The most powerful GPU use for training AI models right now is billions of times more powerful than this device. If you can mind, every 8008 ever made, that would still be several orders of magnitude less computing power than you carry around in your pocket in 2020. That wasn't all. The scarcity of compute at the beginning of the PC era was just one of many obstacles that had to be overcome before we could have personal computing so ubiquitous that we all have the luxury of taking it for granted. Here's the interesting thing, these impossible sounding constraints that Bill Gates and a bunch of other personal computing pioneers confronted in the early days of that era, seem more like challenges and opportunities to them than obstacles that should deter them. None of those pioneers could've accurately predicted what personal computing would become, but they had a pretty good idea of the trends that would enable what they were attempting, as well as the technical challenges that they had to overcome. They invested into those trends, and focused their ingenuity and creativity on overcoming those challenges. Such a day, I want to focus specifically on the trends and constraints that are influencing one of the most significant developments in the history of computing. The explosion of large-scale machine learning models, and rapid advancements in AI. Microsoft has been able to do some amazing things to drive this trend forward. But none of the major shifts in their computing revolution has happened just because of the efforts of one company. PC has became a phenomenon, not because of the PC itself, but because of the breadth of things people did with them. PCs are a platform for others to build on top of. The internet changed the world, not because of TCPIP and HTTP, but because it was a platform for individuals and entrepreneurs in big businesses to create and innovate. Even what you do every day on your mobile device isn't about the technology and the device, but more about the breadth of applications on those devices and the people, content and services those applications connect you to. At every one of these inflection points, that leaps forward we made where because of the contributions of developers. Any platform is only as good as the developers who use it. So for AI to achieve its full potential, it must be a platform, and this platform will need to be powered and delivered at truly unprecedented scale and democratize so everyone can innovate and build on top of it. That's where all of you come in. The future of AI will be in the hands of the developers who can harness its powers, and you won't need to be a data scientist or machine learning specialists to do so. For any developer, the wildest dreams of what you can achieve and create with technology will become accessible as we build AI at scale. The reason that I wake up every morning excited about my job is not because we get to work on these things, but because we get to work on them in service of others. That's why I feel so fortunate to be speaking today with all of you, the explorers who are going to help us expand the boundaries of what is possible. You going to see some pretty amazing things in the next 45 minutes. Giant supercomputers, new resources for training massive AI models, AI assisted coding, and perhaps most importantly, a pizza delivering robot dog. So make sure to stick around to catch it all. But first, we'll step back for just a moment and start today off with what I know has been top of mind for everybody. The global crisis of COVID-19. The last few months have shown us why we need the kind of boundaryless thinking that Clark spoke about to apply against a massive, intractable challenge that's impacted every single one of us. Developers all around the world have risen to the challenge presented by this pandemic to create and deploy new software and tools with unprecedented speed. First up today is Peter Lee, who has been leading some extraordinary efforts within Microsoft to channel the creativity and passion of our own employees to help.  Thanks, Kevin, and hi, everyone. As all of you know, this pandemic has had terrible consequences all around the world. From the loss of life to socialize relation to an economic decline that rivals a great depression, we've all been confronted with profound challenges. But you know, this crisis has also brought out the best in humanity. People and organizations around the world have been working together, often virtually, to build the tools, the technologies, and the solutions that people need. People like health care workers on the front lines, our researchers and scientists racing to find the new drugs and vaccines that we all need. We, here at Microsoft have been especially inspired when we've had a chance to work together and code together with these types of people in organizations. As one example, I'd like to share a snippet of a teams conversation I recently had with Rod Hochman, who is a CEO of the Provenance Health System on their deployment of the Microsoft Health Bot. Then of course, after that first patient, you started to see a huge influx of inquiries from concerned citizens, people who were seeking advice and help if they were feeling symptoms. Can you say a little bit about how the COVID-19 self-assessment bot called grace.  Sure. So immediately when you have a pandemic epidemic, the next thing is going to be is how do you triage people so that they don't go to the wrong place. Immediately what we worry about with someone with COVID-19, is that we're going to quickly overwhelm all of our emergency rooms. We'd infect everyone there and we'd be out of business. So the name of the game is get people assessed way before they come to the emergency room. Well, how did you do that? How do you make people, what kind of tools do they have, so they can figure out, am I sick enough, should I go? Or if I do, how do I get in touch with someone? So quickly, with grace and creating a bot which could sort out for an individual. If I had x, y, and z, then what do I do? Then once they did that, where do you direct them? Well, do you go to the emergency room? Do you go to express care? Do you call your doctor or do you get a telehealth visit? So what quickly we were able to do with a bot, was get people information and then importantly be able then subsequently to direct them to places other than the emergency room or an ambulance to go, and that probably is one of the things that saved our hospitals, and we were able to triage folks without getting overwhelmed.  It was just so great working with your tech teams collaboratively on that bot, and in fact, you then connected us to the CDC to really help them establish the same kinds of capabilities. It was just a fantastic collaboration.  Because I think we had, well, over 150,000 people that use the bot 170,000 other time users, and the exchange of information was 3.2 million messages. That was just in the space from about early March up until a little bit before now, and that led to about 13,000 televisits a day. So we're completely bypassing having to bring people into primary care offices, but getting them the right care at the right place, and then you'll look at the ability for a tool like this to be spread to everywhere so that we can be triaging patients whether you are in New York, whether you're in somewhere else and be able to do it consistently.  It's amazing what even small application of AI can do to help a health system respond to a crisis. In fact, the COVID-19 health bot has now been deployed in over 1,500 health care sites in 23 countries around the world, and it's helped over 32 million people self-assess their own symptoms. But it doesn't stop there. We're also donating massive amounts of GPU computing power to organizations like ImmunityBio, folding at home, and great research groups around the world. They all really amount to working together. No one organization, no one person is going to be able to beat this thing. It's working together, developing new technologies together, that's going to be necessary to overcome this crisis. So for all of you, we look forward to serving you and to working together with you. Please stay safe and healthy, everyone. Thank you for all that you're doing.  Thank you, Peter. As a company and as an industry, we're on a journey to solve the worlds most intractable problems through the massive power and creative potential of AI. One of the biggest developments in the field over the past couple of years has been the training of very large deep neural networks to discover generalized language representations that can be re-used in many different applications. These new models trained on very large volumes of unlabeled data using new techniques known as self-supervised learning. Models train using these new techniques have achieved levels of performance and natural language processing that have appended the field and are now being applied to other domains. Even though the models have already become very big, their performance continues to improve as they get bigger. Given that these models are very large and do most or all of their training on unlabeled data, one of the primary things limiting their performance is having an abundance of compute power to throw at the training to ask. The implications of this and what it means for AI are massive. To tell you more about this trend, let me introduce you to my technical advisor, Luis Vargas, who's been leading and aligning our cross-company initiative to bring together these large-scale models and new computing architectures that together can power AI at unprecedented scale.  Thanks, Kevin. My name Luis Vargas and I'm working Kevin's team helping drive our AI strategy. In the next 10 minutes we'll talk about AI at scale. What it is, and why it matters. I have three announcements for AI developers and a demo of AI settling a longstanding dispute from the Star Wars. As you understand modern mass, the explosion of Internet data and large amounts to compute from the Cloud have pushed a fast progress of AI. Microsoft has contributed to this progress by advancing the state of the art in areas like speech recognition, computer vision, and natural language understanding. We have made many of these capabilities available to everyone as Azure Cognitive Services. Much of this has built on supervised learning, where we train AI model from the scratch to perform a task by using large amounts of labeled data. Because of this dependency, learning is constrained by human effort. Over the past year, we have seen the emergence of a new training approach called self-supervised learning, where AI models can learn from large amounts of unlabeled data. For example, models can learn about language by reading large volumes of text and predicting words and sentences as they do so. Non-labeled are needed because the labels are in the data itself, the words and the sentences being predicted. As the model does these billions of times, it gets really, really good at understanding how words relate to each other on different contexts. This results in the model acquiring a very good understanding of the language. The more data that is used for learning, the richer the understanding. Thankfully, in today's world, there is an immense amount of web data that we can grow and queue it through bank. The language understanding encoded in the model can then be fine-tuned to support a large amount of language tasks such as sentiment analysis, search, question answering, or summarization, all this through a process called transfer learning. This removes the need to train task-specific models from scratch and increases the accuracy of tasks while dramatically reducing the need for labeled data. Now, besides data, the size of a model determines its ability to learn and encode the complexity of language. The model size is described by its number of parameters, roughly the nominal connection in this neural network. The more parameters that our model has, the better it can capture the difficult nuances of language. A year ago, the largest AI models in the world had around one billion parameters. Or during NLG, Natural Language Integration model, the largest today has 17 billion. A rough analogy is that we have gone from third grade reading and comprehension level to a high school level. Let's look at it in action. Let's start by asking the model a couple of random questions. The model has learned facts, concepts, and grammar from large amounts of data including all the Wikipedia, tens of millions of web pages, and dozens of books. When do we first land on the moon? Notice that the model is not doing a query against the database or an index, but instead understanding the semantic meaning of the question, relating it to the world of information that he has learned and synthetizing a grammatically correct sentence. What are the Enterprises main weapons in Star Trek? Those are phasors and photon torpedoes. Nice. Let's now copy and paste a document that the model has never seen before, like this memo from Satya about the Microsoft response to COVID-19. Let's ask the model some questions about it. How many messages does the healthcare bot respond to? You can find the answer here, one million messages per day. Great. What was a first for the University to Bologna? With a semantic understanding of the content, you can find the answer here. They move 90 percent of it's courses for 80,000 students online to Teams in three days, which is awesome. How are PowerApps and PowerBI being used? They're being used to manage bed count and inventory of critical supplies, and sharing that information with others across the region. Amazing. How are scientists using GitHub? To power a distributed computing project to assist researchers developing potential therapeutics using volunteer computers, which is fantastic. Before we move on, as I promised in the beginning, let's finally settle the longest standing is Star Wars debate of who shot first in the Mos Eisley Cantina. By copying the Wikipedia article about the relevant scene and ask the model who shot first? [MUSIC]  There you go. Solo shot first. If you watch the video again, notice that the response is not found explicitly in the article. Once again, the model has understood the content and synthesized the appropriate answer. Now, let's ask the model to do something different. To summarize content. Let's start with something that most are familiar with, the story of Romeo and Juliet, by copying it from Wikipedia and asking the model to generate a summary. As the model reads the text, it abstracts the most important ideas from it and it stitches them together into a cohesive summary, tweaking it as it makes progress. See how the completed summary contains the main points. Romeo and Juliet fall in love. Juliet's parents force her to marry Paris. Romeo poisons himself, and Juliet stabs herself. Can it discuss other points less relevant, like Tybalt calling Mercutio? Let's now summarize a complete long article about Microsoft's commitment to be carbon negative by 2030. [MUSIC].  Once again, as the model makes progress on the article, it attracts the most important ideas into the summary, reassessing the relevant priority, and tweaking the summary accordingly while maintaining cohesiveness. Given the length of the article and the importance of multiple aspects, this is going to be a much richer summary. We'll refer to the current trend of increasingly larger AI models and their ability to power a large number of tasks as AI at scale. We believe that history will continue into the 100s of billions and eventually trillions of parameters. Training models with 10s to 100s of billions of parameters requires big clusters of hundreds of machines with specialized AI oscillators interconnected by high bandwidth networks inside and across the machines. Azure provided with such clusters. For example, with the latest NDv2 VM Series with a V100 GPUs connected by NVLINK and InfiniBand, training models at this scale requires the ability to distribute and optimize the training of the model with large amounts of data across all of the AI oscillators in the cluster. A monitors are so large that our training requires, on one hand is pleading the neural network layers across multiple machines. On the other, partitioning the huge training dataset into batches to simultaneously train multiple instances of the model across the cluster while combining the learning. Over the past year, we have developed a state-of-the-art optimization techniques to make these processes significantly more efficient by reducing redundancy and the state is stored and replicated across the GPUs. We recently open [inaudible] techniques into a PyTorch library called Deep Speed to enable everyone to train AI models 10 times bigger and five times faster on the same infrastructure. We have continually innovating at a fast rate. I'm happy to announce that today we're open sourcing the second version of Deep Speed, which now enables you to train models up to 20 times bigger and 10 times faster. Beside Deep Speed, Microsoft Research has created various complimentary state-of-the-art training optimizations targeting different levels of the Cerberus stack between a framework like PyTorch or TensorFlow, and the hardware. We wanted to make it easy for you to use all this training optimizations together with the framework and the hardware of your choice. I'm happy to announce that today we're bringing together the optimizations from Deep Speed as well as other libraries from Microsoft to the popular ONNX Runtime. This add support for highly efficient training besides of inferencing to this framework-agnostic and hardware-agnostic AIR runtime. AI and scale and the reuse of luxury and models is transforming the way that we operate a Microsoft. You can see these already showing up in your products. For example, Bing [inaudible] to a language model to support search and question answering over the web. Then Word does this over documents. Outlook uses it to suggest replies to e-mails and the most relevant documents for a meeting. Dynamics uses it to suggest actions to a seller based on its interactions with the customer. However, language learn from web data is sometimes not enough to support a specific domain needs of our products. For example, the productivity domain in Office, the business domain in Dynamics, or the professional domain Linkedin. Every organization uses its own distinct vocabulary that a language model must learn. To achieve this, we're training one personalized language model per organization by augmenting or [inaudible] model with the data generated within the organization itself. As with the Bayes model, we use cell supervised learning to do this without the need for labeled data. This allows to automate the learning process with initial organization while ensuring privacy. [inaudible] scale is improving not only your models understanding of language, but also their understanding of relationships between language and other types of data. For training large self-supervised bottles jointly across texts and images to support tasks combining these, like searching visually for images with a test description in being automated captioning of images for accessibility in Office. Similarly, we're training models across text and video. For example, to automate the creation of outlines with the sections identify in a video. Finally, we're experimenting with adding 400 data dimensions into our training, like the layout in Word documents and in PowerPoint slides to automatically generate one from the other. Customer activity sequences in Dynamics to identify potential issues and opportunities. We think of our large-scale AI models and the systems that enable the user as a new server platform. One that enables faster innovation and better collaboration. We believed that this platform needs to benefit everyone. I'm happy to announce that in the coming weeks, we'll open source [inaudible] language models together with recipes to use them in Azure Machine Learning. We're very excited to see what developers build with this. Back to you, Kevin.  Thank you, Luis. The growth and development of these massive ML models is going to represent such a huge step forward in how we approach and use technology. As we just discussed, a primary constraint to growing these models further is the availability of affordable compute. With Moore's Law slowing, we have to find other mechanisms to efficiently scale compute to meet the needs of modern AI. We're looking at the complete system for machine learning and AI from datacenter power and cooling to networks, to computing architectures, all the way up to operating systems, programming languages, and frameworks. As a computer scientist and a Systems Researcher, this is honestly the most exciting period that I've experienced in the evolution of computing infrastructure. All of this unprecedented innovation is converging in the service of building amazing new AI supercomputing technology.  We've been building increasingly powerful systems for AI for a while. Late last year, we completed work on our first AI supercomputer and handed it over to scientists and engineers to start using in their work. The site top500.org keeps a list of the world's most powerful supercomputers. I'm excited to announce today that are Cloud-hosted supercomputer would make that list. By our measures, it's one of the top five largest supercomputers in the world. It uses 285,000 CPU cores and 10,000 GPUs connected within extremely fast network. This machine is purpose-built for the kinds of massive distributed models that Luis talked about earlier that gives this supercomputer all the benefits of a dedicated appliance paired with the benefits of a robust, modern Cloud infrastructure. One of the most exciting things about this machine is that it's Cloud-hosted, is Azure. Because of the power of the Cloud, we were able to develop and deploy this in just six months. Because it's Azure, it benefits from our carbon-neutral commitments as accompany. Let me step back for a second because I'm sure there's at least a few of you out there right now who are saying, "Well, it's pretty cool, but I'm not exactly in the market for one of the world's largest supercomputers just this minute". The work that we've done to get here has a clear benefit to any Azure customer and advances in large-scale clusters, industry leading network design and a software stack to control it all are applicable to the Azure product road map. Good analogy here might be the way automotive technology has pioneered in high-end racing before making its way into the cars that you and I drive every day, hybrid power trains, all wheel drive, ABS, and more. Except in this situation, developers get the benefit of the output as well through the large ML models hosted on this super computing infrastructure. This new kind of computing power is going to try the amazing benefits for the developer community. Empowering a previously unbelievable AI software platform that will accelerate your projects large and small. Just like the ubiquity of sensors and smart phones, multi-touch location, high-quality cameras, accelerometers enabled an entirely new set of experiences. The output of this work is going to give developers a platform to build new products and services. Having access see ubiquitous high-quality language translation, understanding, summarization, and reasoning is going to help supercharge developers. It will increase not only your own productivity, but also the capabilities of the products and services you develop. [MUSIC].  The machine we're talking about today was built for our friends and partners at OpenAI who are going to use it to power their industry leading AI research. A few days ago, I had the pleasure of talking about how these amazing advances in AI supercomputing will translate to real-world impact for developers and organizations with a very special guest. [MUSIC]  So my guest today is Sam Altman. Sam is one of the most successful entrepreneur and investors in the industry and we've had the good fortune of working with Sam through OpenAI where he's CEO. So I'm super happy to have you with us here today, Sam.  Thanks for having me.  So we're talking today about AI supercomputers and these gigantic models that we've been working on, and in a very real sense you-all are at the very frontier, you are doing the most ambitious work in AI today. So I just wanted to get your perspective about what it is that you think we're going to be able to do with these really large models and in particular, how they're going to affect developers in how they do their work and what they're able to build?  It's a great question and it's exciting for us because we view ourselves as an AI development and deployment company. We actually want to build these large-scale systems and see how far we can push it. As we do more advanced research and scale it up into bigger systems, we begin to make this whole new wave of tools and systems that can do things that were in the realm of science fiction only a few years ago. People have been thinking for a long time about computers that can understand the world and do something like thinking, but now that we have those systems beginning to come to fruition, I think what we're going to see from developers, the new products and services that can be imagined and created, are going to be incredible. I think it's a fundamental new piece of computing infrastructure.  Yeah and that's one of the things that you and I have chatted about a lot. So we have finally gotten to the point with these very big models where transfer learning, the ability to train a model for once and then use it in a bunch of different applications or scenarios, that's finally working with these big models. So it's really exciting that you can start thinking about the models themselves as a platform. So I'd love for you to chat a little bit about the incredible breadth of things that you-all have have been doing. It's not just natural language, right?  No, we're interested in trying to understand all of the data in the world, so language, images, audio, and more. But really it's all data. You get input about the world and you try to understand the fundamental patterns and concepts of what's going on there and then use that to build systems that can do useful things for people. The fact that the same technology in the same systems, as you mentioned, can solve this very broad array of problems and understand different things in different ways, that's really exciting to us. That's been the promise of these more generalized systems that can do a broad variety of tasks for a long time. As we work with the supercomputer to scale up these models, we keep finding new tasks that the models are capable of.  Yeah. So I know you've got a demo to show us today, so I'm going to be tremendously exciting. I think this might be the first time that I'm seeing this. So this is an application of what these really big models can do that might be super interesting for developers.  Yeah. So we've talked about how these massive language models trained on supercomputers can do all these different kinds of natural language tasks, but the models actually go much further. When we fine-tune them on specific data, we find that they can do things that we didn't expect. One thing we were curious about was what we could do with code generation, could we help developers write code? So we used the Microsoft supercomputer, our generative text models, and we fine-tuned it on thousands of open source GitHub repositories, and we'll roll the video and show you what happened.  I'm going to show what an OpenAI language model can do when applied to code generation. The model I'm using today was trained on code from thousands of open source GitHub repositories using the same unsupervised techniques as our GBT models. Let's try it out. I'll start with pretty basic Python task, I want to write a function to check whether or not a string is a palindrome. I'll start by writing the function signature and a quick comment string. Now I'll ask the model to generate the code that it thinks should come next. Great, the model got it right. But that was a pretty simple example, if I search for is_palindrome on Stack Overflow, I'm sure I'd find something similar. So let's step it up a bit. This time I'll write a function that definitely wasn't in the training set. Let's return list indices for elements that are palindromes and at least seven characters. Let's see what code the model generates. Great, that looks right and it even used the is_palindrome function from above. This time we can also rest assured that it didn't just copy some snippet from the training set, the model came up with a unique solution to a unique requirement. Let's step it up again. This time I'll write an Order class composed of items. I pasted in some code defining the Item class and the properties of my Order class. Now let's write some methods for the Order class. Let's start by writing a method to compute the total price. But let's add a twist, let's apply a discount to items that are palindromes. Let's see what the model suggests. It helpfully writes a comment string for us. But that's not quite right. I only wanted the discount applied to palindromes not to the whole order. Let's edit the comment to clarify what I wanted. Compute the total price and return it, apply discount to items whose names are palindromes. Let's try again. So far so good, great, that's much better. Actually I wanted the default discount to be 20 percent off, not 80 percent off, but that's an easy enough fix. There we go. Let's round it out by printing a receipt for the order. Print total price and the price of each item. Here we go. Perfect. That's just what we wanted. He even used the compute_total_price method, that the model and I wrote together. Beyond helping implement functions, we've also seen the model excel at other developing tasks, like writing unit tests. All in all, this model can generate useful in context to where code suggestions that would make any developer more productive. To link that on these models will allow you to spend less time on repetitive, time-consuming coding tasks, and focus more on the creative aspects of writing software.  It's truly incredible what these models are capable of doing. It's clear that they're going to assist us in being more productive across all of our work, from reasoning over documents to writing e-mails, to building software. Thanks so much for your time, Sam, it's been a real pleasure.  Thank you, Kevin.  We're actually already using AI powered tooling at Microsoft today. Visual Studio IntelliCodes, AI assistance saves developers time by learning from the input of the broader coding community. Our research teams are working to take these huge models and distill them down to a size where they're practically usable in Visual Studio. You can check out the IntelliCode talk later this afternoon to learn more. So hopefully you can imagine the possibilities once we have this massive power supporting AI platforms for you to build and innovate on. Another super important place where AI will be used as a platform in the future, will be at the Intelligent Edge, building a bridge between the physical and digital worlds. Let's join Lila Tretikov to share some of the ways this has been developing in the recent past, and what it will look like in the future.  Thanks, Kevin. It is impressive to see our extra large AI supercomputers, and what they enable our developers to do. But we have something equally cool to show you in the next nine minutes. This time, our AI will be extra small and extra distributed. It will run across numerous smart interconnected devices, enabling you to program them as one intelligence system. To enable this, you will see our upcoming programming and deployment models that run on network by 5G APIs from AT&T supported by ONNX based AI models that fit seamlessly operate between the Edge and the Cloud. Finally, say hello to my friend Spot here, A Boston Dynamics Robot. Spot will literally run AI on the Edge. Thank you, Spot. Good boy. These past few months have rapidly accelerated how important using AI on the Intelligent Edge is across a host of scenarios, frictionless access, hence recomputing and personal protective equipment became required nearly overnight. At Cardinal Teen Hospital in Taiwan, workers help protect patients and staff from the spread of COVID-19 by deploying Computer Vision at hospital access points. In France, doctors are using HoloLens to collaborate and share information with their patients. In education, for the first time ever, instead of working together on campus, all first-year medical students from Case Western Reserve University School of Medicine practice anatomy at home. These examples build on intelligence systems work we have previously done with Unilever, Shell, Mark, and many others. We are eye witnessing another revolution. All things around us are becoming smart things. You will program and manage them as one AI powered Cloud to Edge Intelligence System. With such a system, we can re-imagine, and reprogram our physical world to get manage our work, automates redundant tasks, see, hear, and collaborate across thousands of miles, as well as keep us safer when we can't be there in person. We want to show you how you will be able to build such systems yourselves. Let's take a look at a prototype application showing some of our forward-looking AI on the Edge features. This app uses an infrared camera and the AI model trained to detect license plates to recap with curbside pickup for my favorite hypothetical take-out pizza restaurant. You see the application detecting a license plate of a person picking up there to go order. Know that if we loose network connectivity, the license plate detection fails. We can't have that. We need zero business downtime. So we're going to fix this by allowing the application to run on the Edge when disconnected, and on the Cloud when the network becomes available. This is our architecture. Our camera is already connecting to an Edge device. We tried now uses an ONNX model in the Cloud. We're going to change this Edge Microservice to also leverage a local ONNX model when disconnected. We could also configure this system to run exclusively on the Edge. Never sending to the Cloud at all for privacy, or cost reasons. We're using the Dapper Distributed Application Runtime, which offers a simple microservices-based programming model. This is our solution in Visual Studio Code. This wrapper of a 5G network API from AT&T allows us to query 5G network conditions. Our Cloud will leverage the 5G API to allow us to switch between calling Cloud and Edge based on connectivity. We'll create a route to handle the network disconnecting. We write the logic to use the local API model. For deployment, our data microservice is already configured with Azure Arc to run local and on the Cloud. Azure Arc uses these deployment specs to automatically deploy to our Edge Kubernetes cluster when we check in our changes. Now that we've made our changes, and Azure Arc has deployed them, lets retest. As you can see, the application works with zero downtime, with on the slide decrease in confidence levels when running just on the Edge. As the network is restored, the AI adjusts back to Cloud-based mode. This isn't all though. We've detected our customer coming to the restaurant, now we need to deliver their food. To deliver the takeout, we'd like to show how we're integrating our work with autonomous robotics, in this case, for the Boston Dynamics Team, and Spot, who you've met early. Gina is going to help us show this work.  Thanks, Laila. I'm going to show how our intelligent Edge solution can leverage AI to have a Boston Dynamics Spot robot look around, use our Edge AI to find the customer's car, and carry their takeout to them. We've mounted an edge device on this Spot robot to allow us to control the robot autonomously and deploy our AI model using ONNX runtime. Here I'm showing the code where we use the ONNX AI model on the edge device that will help us recognize the license plate using images captured from the robot's cameras. We will use this code to leverage the robot's camera perspective and control the robot's position to navigate to the car with our license plate. Lastly, we've built this using custom commands leveraging speech and language understanding in Azure Cognitive Services to activate Spot to deliver the food. Putting everything together, we'll ask Spot to deliver the order.  Hey Spot. Deliver order to Laila.  Note that in the future, Spot could potentially pick up and load the order into the car. Laila will tell us more about the complexity in training this kind of AI. Good job Spot. Over to you Laila.  Yum, Pizza. Thank you, Gina. We just used Visual Studio Code, the AT&T 5G network APIs, ONNX for AI on Amazon Cloud, Azure Arc, Dapper and OEM, and the Boston Dynamics robot to build an app that intelligently crosses the Cloud and Edge. As Gina mentioned, to deliver the groceries would require precise control. We need a high degree of confidence for navigating paths, traversing stairs, and controlling speed on approach. To build such a system, you need reinforcement learning paired with expert guided teaching, in the same way you would learn how to mass transport. This training requires hundreds of thousands, if not millions, of repeated attempts to balance controls and forces that act upon our system. I would like to announce public preview of Project Bonsai, the machine teaching component of our autonomous systems platform, which enables training such models. Bonsai is not for robotics only though, our customers have used it to optimize and bring autonomy to a wide range of scenarios from complex process controls in the chemical industry, to machine tuning, calibration, and motion control in manufacturing, and to even make apparel goods and food processing. The next wave of computing will enable us to manage the world hands-free, remote, even in the most dangerous and difficult conditions. Smart things will be our remote hands and eyes, and we are building intelligence systems to empower you to program them to solve your most critical needs. These critical systems must protect our security and privacy. At Microsoft, we believe it is our responsibility and partnership with developers like yourselves to innovate transparently, respectfully, and responsibly. We are committed to enabling developers around the globe to have a lasting impact on society through our ethical AI principles and AI for good initiatives. With Microsoft providing AI platforms, safeguards, and guidelines, we really look forward to seeing the intelligence systems you build for your customers. Thank you.  Thank you, Laila. I really hope that what you've seen today excites and inspires you. As I've said before, that's really why we're here, to provide great tools and platforms for the people who are tirelessly contributing so much passion, imagination, and ingenuity at the front lines of the technology revolution every day. We saw such amazing examples of how that's taken a place across so many different facets of the digital world and the physical world and how the two can be connected to help us tackle some of our most daunting global challenges. With the explosion of machine learning models and AI platforms driven by super-powered infrastructure, how anybody will soon be able to harness computing power that was unthinkable even a few years ago to create and innovate at a scale that we've never seen. I really encourage all of you to put your acute thought and attention towards the transient computing that we discussed today. I truly believe that what happens in the coming years when we see the maturity of things like AI at scale, unsupervised machine learning, and reinforcement learning, will be a fundamental alteration to the fabric of what technology means to us and what it can achieve. It's going to impact every aspect of your work as developers sooner rather than later. Everything that we talked about here is real technology that we have today. This is stuff that we've been thinking deeply about for a long time. We have the resources in the charter to do so. But in just a few years, every developer at every level will have to think the same way about AI, about Edge, about how to apply machine learning. In some ways you could even think about the intersection we're seeing of these developments between AI and the sciences as the new calculus. In the 18th century, the introduction of calculus gave us the field of physics and helped us to better model and understand the world around us. Increasingly, neural networks are playing that role today in the sciences. This is particularly taking place in biology. Think of some of the efforts we saw earlier to use ML to rapidly understand and cope with COVID-19. That won't be the last existential crisis we face as a species. These new tools are going to help us bear the weight of many elements of an uncertain future with more solid footing. There are a couple of things that need to happen for any of this to have a net positive impact on our society. First, all of it will have to be available to everyone. We must democratize the access to these tools so anyone can use them. Much of the honors to do that is on us, the platform builders, but all of you share in this responsibility as well. Whether you're building a mobile app, creating new tools for precision agriculture, personalized medicine, or commerce, the benefits that flow from the power of scaled AI will need to reach as many people as possible. The barriers to entry must be lower. Second, we must take care to develop this new platform thoughtfully, accountably, and ethically. Microsoft is already committed to a set of responsible AI principles that are core to all of our AI development efforts. But all of us in the technology ecosystem need to be vigilant that the AI products and applications we create are inclusive, equitable, and human-centered. My ask of you today is not just to help us build a future on top of these new AI platforms, it's to do your part to help everybody on the planet share in the empowerment that can come from reaching a new frontier in our shared technology journey. That endpoint is our mission at Microsoft, but we can't achieve it without all of you. Thank you-all so much for tuning in. Be safe, be well, and please never stop building and dreaming beyond the boundaries of what's possible. 