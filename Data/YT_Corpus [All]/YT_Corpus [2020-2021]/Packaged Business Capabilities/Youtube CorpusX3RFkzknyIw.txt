 hello everybody and thank you for joining us today for the virtual Vertica BBC 2020 today's breakout session is entitled machine learning with Vertica data preparation and model management my name is sue LeClair director of marketing at Vertica and I'll be your host for this webinar joining me is will cost Dillon part of the Vertica product management team at protocol before we get begin I want to encourage you to submit questions or comments during the virtual session you don't have to wait just type your question or comment in the question box below the slides and click Submit there will be a Q&A session at the end of the presentation we'll answer as many questions as we're able to during that time any questions that we don't address we'll do our best to answer offline alternately you can visit vertical forums to post your questions there after the session our engineering team is planning to join the forum to keep the conversation going also a reminder that you can maximize your screen by clicking the double arrow button in the lower right corner of the slides and yes this virtual session is being recorded and will be available to view on demand later this week we'll send you a notification as soon as it's ready so let's get started what caused over to you thank you sue hi everyone my name is Vikas Dillon and I'm a product manager here at vertical so today we are going to go through data preparation and Model Management in Vertica and the session would essentially be starting with some introduction and going through some of the machine learning considerations and you're doing machine learning at scale after that we have two major sections here the first one is on data preparation so we'd go through what data preparation is what are the vertical functions for data exploration and data preparation and then share an example with you similarly in the second part of this talk will go through import and export molds I'm using PMML and how that works with Vertica and we examples for that as well so yeah let's dive right in so where to camp essentially is an open architecture with a rich ecosystem so you have a lot of options for data transformation and ingesting data from different tools and then you also have options for connecting to ODBC JDBC and some other connectors to BI and visualization tools there's a lot of them deck where T connects to and in the middle camps it's Vertica which you can have on external tables or you know you can have in place analytics on our con cloud or on-premise oh that choice is yours but essentially what it does is it offers you a lot of options for performing your data analytics on scale and within that it data analytics machine learning is also a cool component and then you know you have a lot of options and functions for that now machine learning in Vertica is actually built onto all of the architecture that the distributed analytic database offers so it offers a lot of those capabilities and builds on talks and so you eliminate the overhead of data transfer when you're working with vertical machine learning you keep your data secure storing and commencing tomorrows that's really easy and much more efficient you can serve a lot of concurrent users to come all at the same time and then it's really scalable and avoids a maintenance cost of a separate system so essentially a lot of benefits here but one important thing to mention here is that all of the algorithms that you see whether they are analytic functions advanced analytics functions or machine learning functions they are distributed not just across the cluster on different nodes so each node has mr. gets a distributed work node on each node - there might be multiple sites and multiple processes that are running with each of these functions so highly distributed solution and sort of one of its kind in this space so when we talk about Vertica machine learning it card size D covers whole machine learning process and we see it as something started starting with data ingestion and doing data analysis and understanding going through the steps of data preparation modeling evaluation and finally deployment as well so when you're using with Vertica you're using vertical for machine learning it takes care of all these steps and you can do all of that inside of the like a database but when we look at you know the three main pillars that vertical machine learning games to build on the first one is to have Vertica as a platform for high-performance machine learning we have a lot of function for data exploration and preparation we'll go through them some of them here we have distributed in database algorithms for more training and prediction we have scalable functions for model evaluation and finally we have distributed scoring functions as well but you know doing all of the stuff in the database that's that's a really good thing but we don't want to isolate it in this space we understand that a lot of our customers our users they like to work with other tools and work with Vertica as well so you know they might use where to go for data prep but another tool for more training or use you know where to go for more training and take those molds out to other tools and do prediction there so integration is really important part of our overall offering so it's a pretty flexible system we have been offering UD axes in four languages a lot of development there over the past few years but the new capability of importing PML models for in database coding and exporting vertical native mods for excel scoring is you know something that we have recently added and another talk would actually go through the tensor point aggressions are really exciting an important milestone that we have where you can bring tensor flow models into Vertica for injectable scoring for this talk will focus on data exploration in preparation importing MML and exporting similar models and finally since Vertica is not just a queue engine but also a datastore so we have a lot of really good capability for more storage and management as well so yeah let's dive into the first part on machine learning at scale so when we say machine learning at scale we are actually having a few really important considerations and they have their own implication the first one is that we want to have speed but also wanted to come at a reasonable cost so it's really important for us to pick the right scaling architecture secondly it's not easy to move big data around it might be easy to do that on a smaller data set on an excel sheet or something like but once you're talking about big data and data analytics at really big scale it's really not easy to move that data around from one tool to another so what you'd want to do is bring models to the data instead of having to move this data to the tools and the third thing here is that some subsampling it can actually compromise your accuracy and a lot of tools that that are out there they actually force you to take smaller samples of your data because they can only handle so much data but that can impact your accuracy and the need here is that you should be able to work with all of your data we will just go through each of these really quickly so the first aspect here scalability now if you want to scale your architecture you have two main main options the first is vertical scaling where let's say you have a machine a server essentially and you can keep on adding resources like RAM and CPU and keep on your own increasing the performance as well as the capacity of that system but if there's a limit to what you can do here and the limit you can hit that in terms of cost as well as in terms of technology beyond a certain point you would not be able to scale more so the right solution to follow here is actually horizontal scaling in which you can keep on adding more instances to have more computing power and more capacity so essentially what you get with this sort of an architecture is a supercomputer which stitches together several nodes and then the workload is distributed on each of those nodes for massively parallel processing and really fast speeds as well the second aspect of you know having big data and it the difficulty around moving it around is actually you know can be clarified with this example so what usually happens is that you and this is a simplified version you have a lot of applications and tools from which you might be collecting the data and this data then goes into an analytics database that database then in turn might be connected to some vr-2 dashboards and application and some ad hoc queries being done on the database now when you want to do machine learning in this architecture what usually happens is that you have your machine learning tools and the data that is coming into the analytics database is actually being exported out to the machine learning tools your train your models there and afterwards when you have new incoming data that data again goes out to the machine learning towards prediction but those results that you get from those tools usually end up back in the distributed database because you want to put it on dashboard or you want to power up some applications of that so there's essentially a lot of data overhead that's involved here there are columns with that including data governance data movement and other complications that you need to resolve here one of the possible solutions to overcome that difficulty is that you have machine learning as far as a distributed analytical database as well so you get the benefits of you know having it applied on all of the data that's inside the database and not having to care about all the data movement here but you know if there are some use cases we'll still make sense to at least train the models outside that's where you can do your data preparation inside the database and then take the data out there prepare data build your model and then bring the model back to the analytics database in this case we will talk about Vertica so the model would be archived hosted by Vertica and then you can you know keep on applying predictions on the new data that's in coming into the database so the third consideration here for machine learning on scale is sampling versus full data set as I mentioned a lot of tools they cannot handle big data and you are forced to sub sample but but what happens here as you can see in the figure on the leftmost figure a is that if you have a single data point essentially any model can explain that but if you have no more data points as in Figure B there would be a smaller number of Mars that could be able to explain that and in Figure C you know even more data points less number of model to explain but lesser also means here that these modes would probably be more accurate and the objective for building machine learning modes is mostly to have prediction capability and generalization capability essentially on unseen data so if you build a model that's accurate on one data point it would not have a very good generalization capability the conventional wisdom with machine learning is that the more data points that you have and for learning the better and more accurate models that you'll get out of your your machine learning models so you need to pick a tool can handle all of your data and does not force you to subsample that and doing that even a simpler model might be much better than a more complex bottle here so yeah let's go to a data exploration and data preparation part where it has a really powerful tool and it offers a lot of capabilities and in this space and as I mentioned which support the whole process you can define the problem and you can gather data in constructed inside Vertica and then consider it a prep our training modeling deployment and managing the model but this is a really critical step in the overall machine learning process by some estimates we it takes between 60 to 80 percent of the overall effort of a machine learning process so a lot of functions here you can use part of worker to data exploration you to plication after detection balancing organization and essentially a lot more you can actually go to a vertical documentation and find them there within vertical AC divide them into 2 parts within data prep one his exploration functions the second-richest transformation functions now within exploration you have rich set of functions that you can use in DB and then if you want to build your own you can use the utexas to do them similarly for transformation there's a lot of functions around time so you expect in matching our fact detection that you can use to transform that data and it's just a snapshot of some of those functions that are available in vertical right now and again the good thing about these functions is not just their presence in the database the good thing is actually their ability to scale on really really large data sets and be able to compute those doors for you on that data set in an acceptable amount of time which makes your machine learning processes really practical so let's go to an example and see it how we can use some of these functions as I mentioned there's a whole lot of them and we'd not be able to go through all of them but just for our understanding we can we can go through some of them and see how they work so we have here a sample data set of network flows we it's a simulated attack for from some source nodes and then there are some some victim nodes on which these attacks are happening so yeah let's just look at the data here real quick we load the data we'll browse the data compute some statistics around it ask some questions make plots and then clean the data the objective is not to make a prediction per se which is what we mostly do in machine learning algorithms but to just go through the data prep process and see how easy it is to do that with Vertica and what kind of options might be there to help you through that process so the first step is loading the data since in this case we know the structure of the data so we create a table and create different column names and data types but let's say you have a data set for which you do not already know the structure there's a really cool feature in vertical conflicts tables and you can use that to initially import the data into the database and then go through all of the variables and then assign them variable types and you know if you can also use that if your data is dynamic and it is changing if you both the data first and then create these definitions so once we have done that we load that data into the database it sits for one week of data out of the whole data set right now but once you've done that we would like to look at the flows just just a look at the data you know how it looks and once we do select star from flows and you know just have a limit here we see that there's already some data duplication and by duplication I mean rows which have the exact same data for each of the columns so as part of the cleaning process the first thing we want to do is probably to remove that duplication so we create a table with distinct flows and you can see here we have about a million flows here which are unique so moving on the next step we want to do here is we want to you know we this is essentially time series data and they seven days in a week so we want to look at the trends of this data so the network traffic that's there you can call it flows so you know based on hours of the day how does the traffic move and how does it differ from one day to another so it's part of an exploration process it might be a lot of further exploration that you want to do but we can start with this one and see how it goes now you can see in the graph here that we have seven days of data and the weekend traffic which is in pink and purple here seems a little different from the rest of the day is pretty close to each other but yeah definitely something we can look into and see if there's some real difference and if there's something we want to explore further here but the thing is that this is just data for one week as I mentioned but if we load data for 70 days you'd have a longer graph probably but a lot of lines and would not be able to really make sense out of that data it would be really crowded plot for that winter so you have to come up with a better way of being able to explode that and we will come back to that in a little bit so what are some other things that we can do we can get some statistics we can take one sample flow and look at some of the values here and we see that the forward column here in the US column here they have zero values and when we explore further we see that there is a lot of a lot of values here or records here for which these columns are essentially zero so probably not really helpful for our use case then we can look at the flow ends so few and is the end time when the last packet in a flow versus end and you can do a select min flow and max flow and to see you know the data when it started and when it ended and you can see it's about one week of data from the first till 8 now we also want to look at the data whether it's balanced or not because balance data is really important for a lot of classification use cases that we want to try this with this and you can see that you know source address destination address source port and destination port and you see it's highly in balance data in source versus destination address space so probably something that we need to do a really powerful vertical balancing functions that you can use with min understanding over something or hybrid sampling here and that can become be really useful here another thing we can look at is you know a summary statistics of these columns so of the unique flows table that we created we just use the sunrise num call function in vertical and it gives us a lot of pretty cool countenanced and F and percentile information on that now if we look at the do raishin which is the last record here we can think that the mean is around 4.6 seconds but when we look at the percentile information we see that the median is around 0.2 7 so there's a lot of short flows clothes that have duration less than point two seven seconds yes there would be more and they would probably be bring them into the four point six value but then the number of short flows is probably pretty high we can ask some other questions from the data about the features we can look at the protocols here and look at the count so we see that most of the traffic that we have is for TCP and UDP which is sort of expected for a data set like this and then we want to look at you know what are the most popular network services here so again simply QE here select destination port count and at the information here we get is the destination port and count for each so we can see that most of the traffic here is web traffic HTTP and HTTPS followed by domain name resolution so let's explore some more we can look at the label distributions we see that the labels that are given with that because this is essentially data for which we already know whether something was an anomaly or not record was not normally or not in VN train our algorithm based on it so we see that there's this background label a lot of Records there and then anomaly spam seems to be really high they are an omni UDP scans and SSH screens as well so another question we can ask is like among the SMTP flows how labels are distributed and we can say that anomaly spam is highest and then comes a background span so can we say out of this that SMTP flows there there spams and maybe we can build a ball that actually answers that question for us that that can be one machine anymore that you can build it up with this day effect again we can also verify the destination Porter flows backward labeled as spam so you you can expect 425 for SMTP service here and we can see that SMTP with destination 425 you have a lot of counts here but then there are some other destination ports for which the count is really low and essentially when we are doing in analysis at this scale these data points might not really be needed so as part of the data prep size data meaning we might want to get rid of these records here so now what we can do is I'm going back to the graph that I showed here we can try and plot the daily trends by aggregating them again we take the unique flows and convert it into into a flow count into a manageable number that we can then feed into bonus algorithms now a PCA principal component analysis it's a really powerful algorithm in Vertica and what it essentially does is a lot of times when you have a high number of columns which might be highly correlated with each other you can feed them into the BC algorithm and it would get for you a list of principal components which would be linearly independent from each other now each of these components would explain a certain extent of the variance of the overall data set that you have so you can see here is component one explains about seventy three point nine percent of the variance and company two explains about sixteen percent of the way so if you combine that those two components alone would catch up for around ninety percent of the variance now you can use PC for a lot of different purposes but in this specific example we want to see if we combine all the data points that we have together and we do that by day of the week what sort of information can we get out of it is there any sort of insight that that this provides because once you have two data points it's really easy to plot them so we just applied the PCA we first feign it and then we apply it on our data effect and this is the graph we get as a result now you can see a component one is on the excesses excess air component two on the y-axis and each of these points represents a day of the week now with just two points it's easy to plot that and compare this to the graph that we saw earlier which had a lot of lines and the more weeks that we added more days we added and the more lines would have versus this graph in which you can clearly tell that five days traffic starting from Monday till Friday that's closely clustered together so probably pretty similar to each other and then Saturday traffic is pretty much apart from all of these days and it's also farther away from Sunday so these two days of traffic is different from other days of the traffic and we can always dive deeper into this and look at you know what exactly is happening here and see how this traffic is actually different but we just you know a few functions and some pretty simple cycle queries we were already able to get a pretty good insight from the data set that we had now let's move on to our next part of the say this talk on importing and exporting PMML models to and from Vertica so current common practice is like when you're putting a machine learning models into production you'd have a dev or a test environment and in that you might be using a lot of different tools scikit-learn and SPARC are and once you want to deploy these models into production you'd put them into containers and there would be a pool of containers in the production environment which would be talking to your database it could be analytical database and all of the new data that incoming would be coming into the database itself so as I mentioned in one of the slides earlier there is a lot of data transfer that's happening between that cooler containers hosting your machine learning a crane models versus versus the database which from which should be getting data for scoring and then sending the scores back to the database so why would you really need to transfer your models the thing is that no single machine learning platform provides everything there might be some really cool algorithms that Python provides but then spark might have its own benefits in terms of some additional algorithms or some other stuff that you're looking at and that's the reason why a lot of these tools could be used in the same company at the same time and then there might be some functional considerations as well you might want to isolate your data between data science team and your production environment and you might want to score your pre-trained moles on some edge nodes where you cannot host probably a big solution so there is a whole lot of you cases where model movement or more transparent to another make sense now one of the common methods for clustering models from one tool to another is the PMML standard it's an XML based model exchange format sort of a standard way to define statistical and data mining modes and helps you share mores between the different applications at RF PMML compliant really popular tool and that's the tool of choice that we have for moving moles to and from Vertica now with this model management of this model movement capability there is a lot of model management capabilities that Vertica offers so malls are essentially first-class citizens of urtica what that means is that each model is associated with the DB schema so the user that initially creates some more that's the owner of it but he can cast for the ownership to other users he can work with the ownership rights in any way that you would work with any other relation in a database would be so the same commands that you use for granting access to a moral changing its owner changing its name or dropping it and you can use similar commands for more this one there are a lot of functions for exploring the contents of mods and that really helps in putting these mods into production the metadata of these molds is also available for model management and governess and finally the import/export part enables you to apply all of these operations to the more that you have imported or you might want to export while they are in the database and I think it would be nice to actually go through an example to showcase some of these capabilities and our model management including the PMML model export and import so the workflow for export food here would be that with train some data will train a logistic regression model and we'd save it as an Indy be vertical model then we'll explore the summary and attributes of the world look at what's inside the model what's the training parameters are some coefficients and stuff and then we can export the model as PMML and an external tool can import that model from PMML and similarly we'll go through an example for export this will have an external kml model trained out Africa will import a female model and it could come there on essentially wicked cricket as an indie BPM ml model will explore the summary and attributes of the model in much the same way as an indie be model will apply the model for indie be scoring and get the prediction results and finally we bring some test data we'll use that on test data for which the scoring needs to be done so first we want to create a connection with the database in this case we are using a python jupiter notebook we have the vertical python connector here that you can use really powerful connector allows you to do a lot of cool stuff with a database using the jupiter front-end but essentially you can use any other sequel front-end to or for that matter any other python ide which lets you connect with the database so exporting model first will clean a large take regression model here select rajasic regression will give it a model name the input relation which might be a table temp table overview the response column and the predictor columns so we get a logistic regression model as a result now we look at the models table and see that the model has been created this is a table in Vertica that contains a list of all the models that are there in the database so we can see here that my model that we just created it's created with vertical models as a category model type of logistic regression and we have some other metadata around this one lesson so now we can look at some of the summary statistics of the model we can look at the details so it gives us the predictor coefficient standard errors evaluate T value we can look at the regularization parameters we don't use any so it would be a value of fun but if you had used step it would show it up here the call string and also additional information regarding iteration count rejected to account and accepted to account now we can also look at the list of attributes of the model so select get model attributes using parameters small name is my model so for this particular model that we just created it would give us the name of all the attributes that are there similarly you can look at the coefficients of the model in a columnar format so using parameter name my mode and in this case we add the attribute name is equal details because we want all the details for that particular model and we get the predictor name coefficient standard error Z value and P values here so now what we can do is we can export this model so we use the select export models and we give it a path to where we want the model to be exported to we give it the name of the models that need to be exported that because essentially you might have a lot of models that you have created and you give it the category here which in our examples is PMML and you get a status message here that export models has been successful so now move on to a left over to the importing worlds example in much the same way that we created a model in Vertica and exported it out you might want to create a model outside of política in another tool and then bring that to Vertica for scoring because vertical contains all of the hot data and it might make sense to host that model in Vertica because scoring happens a lot more frequently than clean model training so in this particular case we select import models and we are importing a logistic regression model that we that was created in spark the category here is again PMML so we get the status message that import was successful now let's look at the attributes look at the models table and see that the model is really present there now previously when we ran this QT because we had only my model there so that was the only entry you saw but now once this model is in portrait you can see that as line item number two here sparta logistic regression it's a public schema the category here however is different because it's not an individual model rather an important model so if you get PMML here and then for the metadata regarding the model as well now let's do some of the same operations that we did with the individuals we can look at that summary of the imported kml model so you can see the function name data feeds predictors and some additional information here moving on let's look at the attributes of the PMML model of legged model attribute essentially the same theory that we applied earlier we just the difference here is only the model name so you get the attribute names attribute feelings and number of rows we can also look at the coefficients of the PMML model name exponent and coefficient here so yeah pretty much similar to what you can do with in in DB model you can also perform all the operations on an important model and one additional thing we want to do here is to use this important model for our prediction so in this case we do a select predict PMML and give it some values using parameters model name and logistic regression and match by position it's a really cool feature this is true in this case set to true now so if you have mold being imported from another platform in which let's say you have 50 columns now the name of the columns in that environment in which your training the model might be slightly different than the names of the column that you have setup for vertical but as long as the order is the same vertical can actually match those columns by position and you don't need to have the exact same names for those columns so in this case we have set that to true and we see that predict ml gives a status of 1 now using the important model that in this case we had certain values that we had given it but you can also use it on on a table as well so in this that case you also get the prediction here and you can look at the config metrics to see how well you did now you know just a fragment sort of wrapping this up it's really important to know an important distinction between you know using your moles in any tool any single node solution tool that you might already be using like Python or R versus Vertica what happens is let's say you build a model in Python it must might be a single or solution now after building that model let's say you want to do prediction on really large amounts of data and you don't want to go through that overhead of you know keeping to move that data out of the database to do prediction every time you want to do it so what you can do is you can import that model into Vertica but what Vertica does differently then Python is that the PMML model would actually be distributed across each node in the cluster so it would be applying on you know the data segments in each of those nodes and there might be different threads running for that prediction so the speed that you get here for model prediction would be much much faster similarly once you build a model for machine learning in Vertica the objective mostly is that you want to use up all of your data and build a ball that's sacred and is not just using a sample of the data but using you know all the data that's available to it essentially so you can build that model the model building process would again go through the same same technique it could actually be distributed across all the nodes in a cluster and it would be using up all the threads and processes available to it within those nodes so really fast model training but let's say you wanted to deploy it on an edge node and you know maybe do prediction closer to where the data was being generated so you can export that model in a PMML format and always deploy it on the edge node so it's really helpful for a lot of use cases so and just summarizing the takeaways from our discussion today so where to get a really powerful tool for machine learning for data preparation model training prediction and deployment you might want to use vertical for all of these steps or some of these steps either way vertical supports you know both approaches in the upcoming releases we are planning to have the model import and export capability through PMML models initially were supporting k-means cleaner and logistic regression but we keep on adding more algorithms and the plan is to actually move to support think custom models if we want to do that with the upcoming release tensorflow integration is also say which you can use but with PMML this is the starting point for us and we'd keep on improving that vertical models can be imported can be exported in PMML format for scoring on other platforms and similarly mods that you have built in other tools can be imported for NDB machine learning enters in DB scoring with Vertica there are a lot of critical model management tools that are provided in vertical and there are a lot a lot of them on the roadmap as well which would keep on developing many ml functions and algorithms they're already part of the NDB library and we keep on adding to that as well so thank you so much for joining the discussion today and if you have any questions we'd love to take them now back to you soon 