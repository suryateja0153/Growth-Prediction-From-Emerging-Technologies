 hi everyone and welcome to this data plus ai summit europe a talk on notebook-based ai pipelines with elira and kubeflow i'm nick pantry i'm a principal engineer at ibm you can find me at mlmek on twitter github linkedin i work within a team at ibm called the center for open source data and ai technologies or code where i focus on machine learning i have a long background in the apache spot project where i'm a commence and pmc member and i'm author of machine learning with spark i've talked at various conferences meetups and more recently many online events around the world on topics related to machine learning and data science before we begin a little bit about code or the center for open source data and ai technologies we're a team of over 30 open source developers within ibm and we aim to effectively improve the enterprise ai lifecycle out in open source we contribute to an advocate for any projects that are part of ibm's ai offerings and are foundational to those offerings so this includes apache sparklers as a core component of course as well as the python data science stack open source projects for open data and model uh sharing and exchanges deep learning frameworks as well as ai fx and model survey as well as orchestration on kubeflow so today we're going to talk a little bit about that python data science stack in particular jupiter and a project that we developed within our team called elira so today we'll start with an overview about the machine learning workflow then talk about jupiter and jupiter notebooks and the lyra we'll give a live demo of some of the lyra's capabilities and then wrap up with the conclusion so to begin with the machine learning workflow typically starts with data we take that data and we we analyze it we explore it typically in order to use raw data machine learning we have to do some kind of preprocessing we can't just throw that data straight into a machine learning model it doesn't arrive in a neatly packaged format that is numerical vectors we need to extract features do pre-processing and then you know get that data into a format that it can be used we then go through model training selection once you have a trend model we then deploy it and that's typically where a lot of the discussions sort of ends but but actually you know even once a model is deployed we need to still do quite a lot of work to uh to maintain that model and monitor the predictions that it makes and then a model that's out there running in the wild will also be impacting its own data effectively its own training data and new data will be arriving so we kind of complete this this workflow and becomes something of a loop now this workflow spans teams when you talk about data side that's typically the domain of your data engineers who are responsible for your data storage providing schemas and managed managing access data governance and so on this loop or workflow in the middle is the provenance of your data scientists and researchers so they're typically pulling data into their process doing that the analytics pre-processing and feature selection and extraction model training and then the final team is machine learning engineers and production engineers who are responsible for the production infrastructure we're actually deploying models at scale there's a lot of potential conflicts that comes between these teams and also a wide variety of tools that are being used so there's a there's a wide variety of both standard and non-standard data formats uh for analysis and data visualization uh as well as the machine learning and data science toolkits and frameworks you know each team will have typically multiple frameworks and toolkits being used and every data scientist researcher has their own favorite and typically all of those need to be supported and when it comes to deployments you have a again a variety of formats and mechanisms ways of deploying things and even the languages and infrastructures are used for large-scale production deployment it's typically quite different from what you have in your data science and research workflows so there's a lot of potential conflict here and all these teams need to work across silos and across frameworks and any their production machine learning system to begin to support all of them now a core part of the data science and research workflow is iteration and experimentation so this happens at each phase in this in this process so when data scientists are analyzing data a data process doesn't just happen once you don't just go through this this uh this workflow of loading um data cleansing exploration interpretation and analysis and then you're done now typically what happens is you start with a problem that that may be either well-defined hopefully if you're lucky or fairly ill-defined and that problem is a set of questions that the business needs to answer or a particular problem or use case that one that needs to be solved with data so typically the data scientists will load the data it almost never comes in a nice clean format and there's very many different data sources so a lot of data cleansing has to happen and then you start asking questions of the data so doing analytics exploration computing statistics aggregations and creating data visualizations dashboards reports that are then interpreted to give an outcome but typically either one of one of few things typically happens sometimes one of them typically most of them either the answers that come back themselves create more questions or potential issues are found in the data itself or in some part of the process and then you have this this effectively iterative process of refining um this workflow so going back to the beginning uh maybe maybe fixing some of the data maybe bringing in more different data sources i'm adjusting the way that it's clean adjusting the analytics and stuff itself so there's a lot of iterative workflow that happens here this is similar in the machine learning space where we follow this process of taking the raw data extracting features pre-processing those features into a format so that's that's usable for machine learning in training and that's typically a model selection process um you don't just train one model typically you're training many different types of models many different pipelines to try and find the one that fits the best and then that that's the evaluation process where you're actually evaluating the model on the set of test data based on some metric and typically this is also an initiative and process that's heavy on experimentation uh data scientists need to try things out test out different combinations of parameters different preprocessing steps feature extractors different models different combinations of all of these things so this is a also a loop where the process in the workflow is constantly refined so experimentation and iteration are critical and that's why notebooks and in particular jupiter notebooks have become the de facto standard for this type of workflow for constant rich interactive work now notebooks bring a wealth of power to the data scientist and to machine learning research into this workflow but there are some issues with with notebooks and what you typically see happen in this sort of work workflow and process is that uh the notebook where you know the the initial exploration is happening grows and grows and grows and it becomes a monolithic structure that does everything in one place so it makes it very difficult and increasingly tough to actually extract out pieces of code function functions um you know create like create little mini libraries and modularize the code so it becomes a bit of a behemoth and that makes it a lot more difficult to productionize so rather than have nicely modular pieces of code or functionality that are connected you have this one big notebook which which can't just be chucked over the wall to production and it's also very difficult to scale up and deploy notebooks in a scalable manner so to address this and some of these issues elira was created by our by our team elia is a set of ai-centric extensions to jupiter lab the jupiter lab is an open source notebook environment that is highly extensible and a liar is a set of these extensions it's actually named for illara which is a one of the moons of jupiter and you can see the lyra here is orbiting the jupiter ecosystem so we'll go through a couple of the core components and features of lyra before we actually walk through it in a demo so one of the most important pieces here is a visual pipeline editor so we can see here this is a way to to visually build ai and data science pipelines consisting of both notebooks and python scripts so this is a bag corrected a cyclical graph um it can have multiple input and output nodes effectively and multiple branching structures uh this shows a typical process of loading data performing some some data processing and cleansing and then splitting into multiple downstream tasks some of which may be analytics and data science related some of which may be machine learning related so the power of the lyra is that this one pipeline specification can be run both locally and remotely allowing you to test things out locally really easily and quickly but also when the time comes to scale up by running in this case on and at the moment on supported platforms such as kubeflow pipelines so there may be more coming down the line and planned the kubeflow pipelines is the main promotes workloads this means that once the iterative and experimentation phase is completed locally the code and set of notebooks can be modularized into a set of notebook modules which are the nodes in this graph as well as potentially python scripts and these can all be packaged together as a batch job that runs in tube flow so each node is executed in its own isolated container environment and has availability and has access to the the full cluster resources so related to this is the ability to execute single notebooks as batch jobs and effectively this is a single node pipeline and this is also possible for python scripts so python scripts are also first-class citizens they can be edited within a lyra in the editor and executed against either local cloud-based resources there's a few other features such as the automatic generation of a table of contents from the markdown within in notebooks based on the the heading structure which allows you to navigate easily between the different headings within your notebook it's a code snippet module or plugin that allows you to create reasonable code snippets across various languages and insert them into your notebooks and finally type the integration with git for tracking project changes this allows you to to investigate your diffs and compare changes as well as to easily import your remote projects from kit into your lyra workspace so this means that you can you can easily share code amongst projects and teammates so elias is an open source project then there's a few ways to to get started you can either go to binder and start it with no installation from your web browser you can use the pre-built docker containers or you can install the lyra on your local machine the other links here to check it out okay so we've gone through some of some of the highlights and the features but um i think what speaks uh the loudest is to do a live demo of some of the functionality all right so here we see um elira running in jupiter lab and you can see the jupiter lab launcher has added these components for elira we can open a python file and our pipeline editor on the left here we see our file browser now if we have a look at the pipeline editor this is what it looks like so you can see we've got a pipeline here that involves loading data from two data sources processing it merging it together and then doing some downstream analytics tasks in this particular case we're going to be using two data sets which are actually hosted on the data asset exchange which is one of the other projects that is coming from code and the data asset exchange with dax is a place where you can find free and open source data sets and we're going to be using two of them namely the airline flight delay data and the weather data set for the jfk airport and our aim here is going to be to both analyze flight delays and the potential causes as well as to trying to build a model to see whether we can predict uh whether a flight could be delayed the pipeline it's as we as you see can can handle both python script files as well as notebooks and we have the ability to drag these these components around and to create comments for each node we can then connect nodes to each other and as you can see here we can have one to one relationships as well as many to one and one too many relationships so we can create any kind of dag structure that we wish now each of these nodes has a set of properties and we can see here that the for example the runtime image is able to be specified where you can use pandas but you can specify a few pre-built ones uh they're covering the main frameworks as well as creating your own key here is that this particular script uses an input environment variable where we're specifying the location how to download the data and it creates some output files now we then move on to our data processing where again we're going to be specifying a runtime image and some output files that are going to be coming out of this of this node now the output files are not strict actually strictly required in local mode but these this output file location is used to make the the output of each node available to downstream tasks when you're running on kubeflow so after we do some pre-processing and kinetic heating we're then going to merge these two together these two data sets together and we're then going to use that merge data set which combines flight delay data as well as weather data to do some analytics and some predictions now in order to execute we can just execute this workflow and select whether we want to run it on kubeflow or locally so for now we're just going to execute something locally and as you can see here this is our lab console on the command line we can see that it's busy busy executing so it's going to be downloading its data and executing each of these these notebooks so we can see that in local mode effectively the data is actually going to be saved to our local file system you can see that from our loading data phase these files have appeared here now this notebook is busy executing and in the meantime what we can actually do is we can kick off a run that's going to be running on queue flow and you can see here that uh eliara takes care of packaging all the nodes and putting putting all the dependencies together and shipping them off to our kubeflow cluster and submitting that so this is just a local key flow customer but you can see that that's that is busy executing at the moment and if we want to we can go and have a look at how it's doing see the logs now we can see that our notebooks have finished executing so when we're running in local mode those notebooks are actually updated in place so if we go ahead and open our flight data notebook we can see that these cells have actually been have been filled in here so this is a typical data processing workflow where we start by reading in the raw data and this contains a set of records related to flight delays for airports across the us this is a small sample of the data we're going to clean up that data to uh to be within a certain date range and to only be related to flights word originating from jfk airport we allocate some some airline names to ids to make things more human readable and then we do a bit of column renaming so there's not actually a lot of pre-processing that happens in this particular data set because it's already pretty clean but you can see here that we've got things like the flight date the airline the origin destination um the departure time distance and of course the parts are delayed whether the flight is actually delayed or not that's what we're going to be using later to analyze and predict similarly we can open up our weather dataset processing notebook and see that we should be effectively stepping through very similar steps here reading the raw data cleaning it up there's a little bit more cleaning that has to happen here and in particular we can see that this data set has a categorical feature which is a set of delimited strings that represents the weather type that is present at that particular weather reading as well as wind speed visibility precipitation and so on so there's a bit of cleaning up here and and categorical feature extraction that has to happen uh and you can see that actually our pipeline is actually finished running in full so if we have a look here we can see that each of the notebooks is executed and we'll be able to work through all of them locally we also see that the outputs of each node have actually been written into our data subdirectory okay and then we go through to merging the data which we simply read in each of the data sets and we combine this flight delay data with the weather record for the previous the hour previous to that flight and what you want to do here is try and use some of that weather information to see if we can help predict airline delays so this is illustrating a very typical piece of the workflow which is joining two disparate data sets together once you've done that most often we're going to be doing things like trying to trade models and analyze data so this notebook works through analyzing the causes of flight delays and trying to see whether flight delays are linked to or related to in some way for variables such as the day of the week the departure time of the flight for example the airline and handle the destination the distance of the flight as well as weather so for example here we can see this is the proportion of flights delayed when there's drizzle and mergers are present versus when there's snow or thunderstorms present and this this helps because it gives us an idea of whether these factors are play a role in flight five delays and the final step in our pipeline is to try and build a predictive model and this is using the the same inputs where again we're reading the data we're doing our typical data science steps such as creating training and test data splits encoding categorical variables numerical variables combining them together and then perform performing a model selection process we're using cross validation across a different a few different models with a selected metric to decide which model you're going to be using and all of this can be accompanied by by charts and reports that illustrate cross-validation and evaluation performance and then at the end we you know we take the best model and we're going to put it on our full training set and create a classification report on our test set things like rsc curves precision and recall codes looking at confusion matrices as well as future importances for our two-based models so all the typical components of your data science workflow now we should see that this particular pipeline is obviously still running it may take a little while um but previously here we we have a pipeline that is executed um and we can see that it follows exactly the exact same dag that we have here and we can go through any of these nodes and again take a look at the logs now what happens is each of these uploads its results to cloud object storage or effectively some s3 compatible storage here for example we see that each notebook as well as logs is shown so for example we can look at our predict flight notebook and we can actually look at the html version of that which will give us the results similar to what we saw in our local run we can go down here and get exactly the same output that we had okay so um that that shows a typical uh pipeline and what we haven't shown here is obviously model uh deployments and some some more advanced model tuning but because we're running on kubeflow we can take advantage of some of the components that are part of a keyframe for our pipelines so for example um we can link into model treating with katib or we can deploy it to kubeflow survey once we've trained on okay so that concludes the demo so i hope that that demo and the presentation have really illustrated some of the power and functionality of lyra elia is still a young project very active developing rapidly and we really welcome community involvement so please go and give it a try you can you can run it locally you can run it by docker you can run it on binder you can check out uh that link ibm dot business what's fashion my way demo uh the notebooks and and pipeline definitions for the demo that we've seen today for flight delays are available on github as well as a set of notebooks running on elira that illustrate a pipeline for analyzing covert 19 data that we have developed we encourage you to join the community and get involved come and help us be part of part of this journey so thank you very much please also encourage you to check out coda.org find us on twitter github developer.ibm.com check out the data asset exchange that i mentioned for other interesting data sets and finally i know that feedback is super important so please leave feedback by the online mechanisms for the data plus ai summit europe on the session thank you very much you 