 welcome to the spark in AI summit 2020 so my name is Daniel Coelho and I'm with the Microsoft sequel server big data clusters team I'm a Senior Program Manager focused on driving the experience and feature set for the data engineer and the data scientist personas on this platform a bit of my history prior to joining Microsoft engineering side of the company a couple years ago I've worked many years in consulting services delivering data back then then big data and machine learning in twin scenarios across many verticals today we're going to talk about production izing ml with a bunch of spark ml flow and onyx from ground to cloud using sequel server on the sequel server big data clusters more specifically here's our agenda at first I'll be introducing sequel server BDC where I will quickly go through the platform first understand the capabilities of Seco BTC 2019 then let's make it more concrete going through a complete end-to-end demo it is important to phrase that this is just one of the many possibilities that the platform offers and we will talk about a lot of other options finally we will get to the key takeaways and reference the right resources to move on please remember to rate and review the sessions this is very important for us to keep improving the conference every year let's talk about psycho server big data clusters VDC as we call it as a hybrid transactional processing platform h tab what it is and what it could do for you and your organization the key capabilities of a modern-day the platform it needs to provide a center around two main themes in my opinion I would phrase it as a flexibility and scalability be flexible and scalable to deal with the plenitude of data sources available today the expected integrations from transactional data to data warehouses data lakes where governance standards are you measuring the I know more powerful if used properly because it can be augmented with AI FML a key point here is to endure to the possibility that moving data around can become impossible and virtualizing data access is a or capability and he is address by BBC the other side where flexibility and scalability applies is being able to keep up with technology enhancements using the right computer optimization spark or others to come data virtualization as we mentioned scale innovations like kubernetes and others that was the landscape now let me introduce you to a sequel server BC platform and how it helps you solve those challenges and deliver solutions you need EDC allows you to deploy scalable clusters of sequel server spark in HDFS running on kubernetes that's a long statement so let's lay out the components on the diagram we start with a kubernetes cluster we support vanilla kubernetes in chaos OpenShift and others can be deployed on premises on private clouds our most common scenarios and also on a hybrid and public clouds if you have Cabrini's weird game here on the diagram we have this tough computer icon representing the client applications or group of users we start with the controller the controller who hosts the core logic for the play and managing big data clusters it takes care of all the interactions with kubernetes eco server instances and other components like HDFS and spark the master instance is a sequel server or Linux instance responsible to manage connectivity is chaotic where E's metadata user databases and machine learning services here's where you connect just like you would any other sequel server instance you would use the standard tools of the trade such as set as SMS as a data studio or abs as we will call it P SQL visual studio power bi and etc before we move on a key capability of B DC is it's a native integration with Active Directory for authentication and authorization as you would expect from a sequel server platform this is a key feature for enterprises also introduced into compute pools and the data pools all the components we introduced on the diagram that have those surrounding boxes are multiple pods within the class sir representing the scalability of the components a compute tool is a group of stateless sicko instances be able to provide the compute resources for distributed km queries and scale out a data pool consists of one or more sicko server data po instances it provides these abilities are the sequel server storage for the cluster now we get to the storage below the storage pool consists of the storage nodes of sequel server on Linux SPARC initiative s all the storage nodes in a cqb the big data cluster are members of an HDFS cluster we will spend our time here during the demo finally there is the application pool at Paul enables deployment of applications on BBC by providing easy interfaces to create manage and run applications on top of kubernetes and it close to the data as you can see there's a lot of surface the key takeaway here is that all the components are running side by side to neighbor you to read write and process big data from transact sequel or SPARC allow you to easily combine and analyze your high-value relational data with high volume Big Data it is flexible to multiple scenarios as you pick and choose the right components that you need to build a solution that's why we position this platform as a hybrid transaction of analytical platform it was a lot to unpack and I do suggest you come back to this live on the recording now let's make it concrete by going through a complete demo so our scenario is a simple machine learning development workflow on a real estate agency the IT department column provides a market value of applications of its agents so today there is an application that relies on sequel server for both the transactional and analytics for example power bi there are also opportunities here to use BDC and the data pool specifically to get more performance on some big data tables for the power bi dashboards let's say a data scientist were responsible to develop a better model for market valuation finally that model will be deployed within the DC to augment the current app with AI let's get started with the demo we are here using Azure data studio as our interface to work with the co big data clusters it is connected to my PDC cluster that runs on a plain vanilla kubernetes server you can use this management dashboards to check status of other services running on the cluster create and open new notebooks and perform both management tasks and development and development tasks as expected you can hear in the left navigate you know database object server objects and the HDFS objects and this one we're gonna use to load data into the storage poll one of the greatest features of ATS is its jupiter notebook interface it serves the DBA data engineer and the data scientist and it does this by supporting multiple care notes as you can see here for this notebook in particular we are using the PI spark are no but you have a Seco kernel and all the other kernels at your disposal throughout this demo I'm gonna play different roles and I will start as the data scientist just hired to solve this house market value prediction solution did he be a DM created a database for me to use right here and the data engineer told me that I can became my analysis just using the CSV file that he provided me I've previously loaded this file into HDFS in a sandbox space right here so now that everything is in place let's move on the scenario objective is to create a solution to have our real estate agents - price house efficiently I'm going to hide this connections have here I'm gonna focus on the notebook we're gonna use the Boston housing data set but our focus is not much on the model itself and the use case itself is more understanding the workflow of building an end-to-end solution using big data clusters so this dataset gives us the possibility to tackle this following usage patterns a quick ingestion as I shown loading data into the HDFS you know it's a small fire in this case but it can stay big data easily and that's where the power of spark relies we're going to demonstrate some of the visualization capabilities we're gonna demonstrate of course spark for data engineering and data science tasks and we're gonna use kubernetes ml flow and onyx n sequel server to general you know patterns that could be generalized to a greater number of use cases so you can get this sample notebook and apply of course with different data and different models there's a lot of other use cases let's get a spark session for us to work with the first step we're gonna focus is on ingesting data we uploaded the data set there as you can hear the spark session is loaded and now we is like business as usual first part we're going to define the variable here just to hold the location you can assume there's a HDFS prefix here and then we're gonna read the data frame with spark and this is business as usual first part and then you do the initial data preps prints cameras and you move on and on and on but for now let's focus on some of the DWIs Asian considerations the engineers just might use different strategies to grant you access to that data he gave me a CSV but in a corporate strategy yet of course the flexibility of carrying a CSV is very important for analysis but in a corporate strategy probably he might have travel different roles to grant access to the data for he could have just virtualized a table for you on Zico server this table could be a simple table under seal the sequel server master instance it could be a data pool table designed for big data like shard of data sets could be a totally virtualized external table modular referencing external databases such as Oracle MongoDB Teradata generic ODBC data sources or it could be even so a storage pool virtualize a table actual tables pointing on top of files on HDFS as a data sense using spark for us it doesn't matter we should use Parker methods for reading those objects but in this use case we're gonna just demonstrate how a data engineer would virtualize housing this CSV data judge just uploaded into the sequel server database we're going to use this apache spark connector for sequel server manager sequel which is an optimized connector that communicates perfectly in a distributed fashion with the sequel server instance we're gonna copy data to both sequel server tables one master on the master instance and one in the data pool let's take a look good look at them using a sequel notebook which is an awesome feature for Azure data studio and then we're really back for a twist partying frame just to demonstrate the capability before we start modeling now let's change hats to the data engineer so the data engineer is gonna read that data set using the same pattern the native scientists used to read them and he's gonna have a spark session off his own and then he's gonna use the Apache spark connector for sequel server natural sequel to create those tables within the sequel server instance this is the connector type there is a special class that it knows how to connect in the best way possible to the sequel server an engine it supports as your Active Directory Integration so all the benefits of having an ad enable connection apply then we can use standard spark patterns to write the data out or across so we're gonna get the same how's DF here I'm gonna write using that connector to a table name house now here we are writing is basically the same pair writing pattern but we are writing into a data pool table which is a sharded version designed towards like more big data centric use cases so once we are done we're going to use a different notebook here just to validate this is a sequel notebook so let's focus on the connection here so let's take a look here we are connected to the Microsoft sequel server big data clusters and let's take a look if the table has loaded as you can see the notebook already set tells us that the data is there this is the sequel server master instance table and over here you can see the data pull based table we're using a config function which is a very useful function for big database tables now let's close this notebook here's because if we're not data engineers anymore and we're gonna go back to the data scientist notebook exchanging hats again now just for the sake of validation we're not gonna actually use this data frame but just for example sake let's read the data from the data pool table into this specific data frame here's the notebook running and we get the same 506 lines now let's focus on the modeling side of things we're gonna use ply SPARC standard SPARC ml and Emma flow to actually go with the training here's the standard data analysis like taking a look at you know the correlation between the variables we're not going to focus too much on that I just wanted to point that if you print you know where are we you can see that we are within actually a directory under one of the containers on the kubernetes cluster we're not working on the local machine the notebook is attached to the cluster from this point on is basically using ml flow we have a tracking server running to track all the experimentation that was done um I'm not gonna rerun all the cells but basically you can point to a corporate tracking server corporate established ml flow solution to you know to focus all your experiments images and models and pipelines this was mostly done as an example but we use M of how to track although our model experimentation here so the ml flow library was loaded within the cluster successfully we created an experiment name called Rio state market fell create an experiment ID and then we moved on this is a simple vectorization split the dataset and then we use the standard ml flow run to go and a lot of the parameters set some tags and basically build the models using spark ml so we have the first model which is a linear regression model the train R squared around 74 percent of the variance explained you know this is a very bad ramasees a very small data set but it's doing its job you know with a bigger data set a a bit more modeling we would get better models so we created the first model a little worse on the test data set it's expected you know we got a baseline here take a look at took a look at the residuals this is the other signs as as usual then we train a couple different models we train the GBT regressor and then we train the decision tree you know one of the one cell that I'd like to point out is you know you can get exactly the environment that's been generated as you move on the dependency no Python the dependency on PI spark in an ml flow the decision tree regressor gives us feature importance which is always good and here's a is a print screen of the ml flow tracking server with all the experimentation done for these models and then from this we can decide which model is best for us before we move on and operational and validates the model now do we have some models built let's get those models and make it them useful for our application developers and when you talk about training model modeling with spark you have this non exhaustive list of options to personalize that model and this is one of the common patterns that we have so the first one is to convert the model spark ml model into an M leap bundle which is a portable package format and we're going to personalize within sickled server using the java language extension so the scoring would happen within the C code server instance this is very good for low latency or scenarios where you have data dependencies that rely within the Seco server database we could also have used ml for spark and sparser standard spark serialization to create an application but that application would be a spark context dependence so we would need PI spark and a spark back hand to run there are very good scenarios where you want test so mainly applications that maybe rely on spark streaming or rely on actual big data for batch influencing our use case is more simple and we just want to kind of convert that data and so on another very common package for packaged formats for models called onyx and we're gonna generate a binary onyx file out of our spark ml model from that point on our model is not gonna be dependent on spark anymore it's a portable format and then we're gonna use the kubernetes app deploy for be from BDC to serve the model as a REST API so our example assumes no data dependency it's just a standard model we're going to retrain it eventually as new data comes in and it kind of requires rest api compatibility so we focus on onyx and app deploy so into spinal cells we're gonna use ml flow onyx and onyx choose to convert this this perk a male model into the portable formats design does the job we're gonna save the model into ammo flow just for tracking and then we're gonna use the F write function to serialize the model to disk when we save the model to disk it creates a spec specification for deployment that we're gonna leverage further to app deploy it creates a specification I created a scoring function it's a simple function to load the model and score it and of course the requirements for the Python virtual elf with all the libraries it relies on you can see that the PI spark dependency is gone I created an onyx loco interesting test just to test that scoring function basically the same code loaded onyx runtime load numpy load the model here generate some data as the inputs and then we do the session run which is the inferencing session and we get a scoring result for the model now that we have a portable model working validated let's get to app deploy I previously created the deployment profile based on that specification that I showed before using AZ data which is the primary tool to manage an interface with the big data clusters it created the application and the application was waiting for creation let's validate if the application is now ready it is so now you can use the command line to test the applications and the inputs and getting the prediction success the results doesn't make sense but that's not the point and finally you can describe the applications so you can give this swagger API description to your developers so he can move on and integrate the application with final application so let's take a look at this swagger description let's take a look at it at the swagger editor it's going to convert automatically from JSON to llamo and here it is the endpoint M if you predict Q you ready to be integrated into the application with that we reached the end of the demo let's go back to the presentation to take the learnings and summarize what have we learned hopefully a lot sequel sorry big data closer is a modern comprehensive and flexible platform to provide the answer and analytics it leverages what you know in need so sequel server T sequel scale compute storage Active Directory Integration governance cycle PI's features most organizations name on-premises private hybrid cloud deployments you can differentiate with big data and machine learning use Apache spark HD effects the de-facto differentiators for big data engineering and data science at scale you can either attach remotely or move data into it is a very flexible approach to every scenario we can use part or machine learning services aspirant for platform for a Iook augmentation and you can operationalize on modern standards kubernetes and DevOps at its core and you can use that now flow and Apple appointment possibilities so there are a lot of resources out there here are the main resources so the first thing is that developer edition history and easy to deploy on a chaos on a local community cluster there's the getting started guide our community site on teams that you can join to ask direct questions to the product group documentation around the spark connector and the hundreds of samples and ready to deploy scenarios that you can adapt to your organization thank you very much for attending this session please remember to provide feedback and I hope this content was very useful to you and gives you additional insights and tools to build awesome machine learning and data solutions thank you very much feel free to ask questions on the chat window now you 