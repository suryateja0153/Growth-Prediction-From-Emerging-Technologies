 hey everybody my name is stuart reischling from google cloud i'm a product manager working on traffic director with me is kelsey hightower he's going to be doing a demo later today we're here to talk about building an enterprise grade service mesh with traffic director so i'm going to go through a little bit of pretext and prelude for why a product like traffic director even exists and why you should consider it then we're going to talk about what traffic director is we're going to go into an awesome demo that's going to give you a good sense of you know how do you interact with this product how does it actually work i'll talk through some of the problems that customers are solving with traffic director and then we'll go into some of the announcements that we're going to make today so what is traffic director to understand traffic director i think it it helps to kind of think through a very simple model so i talk to customers all the time who are dealing with problems like a retailer who wants to try and make their deployment production ready and so as a retailer you might be doing very you know retail things you have a shopping cart service you have a payment service the shopping cart service uh helps your customers add items to their carts and then the payment service allows your customers to pay for the stuff that they want to buy really straightforward stuff right the types of things that you might want to solve as a retailer let's look at what that looks like once you start trying to make it production ready and so here i have some code it's not great it's the type of code i might write it's a checkout function a charge customer function and then a function that calls on both of those you know very straightforward the types of things that you might want to do as a retailer and you'll notice here that there's this line of code that is the payment endpoint this is the end point that the shopping cart uses when it wants to call the payment service and it's pretty straightforward you know customers aren't really doing this stuff anymore but kind of bear with me it's an ip address that the shopping cart's going to call on you'll notice that this simple model you've got some business logic you've got some non-business logic which is that shopping cart the payments endpoint pretty straightforward and some more business logic you know not too bad you're just calling this thing from your shopping cart and now you get to a production you know scenario where you need to make this thing something that works for the holiday season and during the holiday season you might have a lot of customers and you can't really afford to have your payments back end be overloaded and so what do you do you need more capacity well one way you get more capacity is you create another payments backend and your shopping cart now needs to be able to call both of those back in so if one of them is overloaded well that's okay you've got another one you know you can't afford to have your payments failing because that's you know money left on the table and so you have two instances of your payments back in your shopping cart needs to kind of figure out how to send traffic between them we'll go back to our code and here is the exact same snippet i showed before i hit some of the checkout stuff because you're going to see this starts to become pretty clunky pretty fast so rather than calling the one payments endpoint we need to be able to call two payments endpoints in fact we need to be able to load balance between them and so one way that you might do this is you might write a load balanced endpoints function and so rather than calling just that one endpoint 1005 you now have this 1006 as well and you randomly choose between one of them this is you know pretty straightforward not too complicated way of implementing round robin load balancing occasionally you'll send traffic to one endpoint on other occasions you'll send traffic to the other one and so this gives you a pretty straightforward way to just load balance between both those payments back-ends but one of the things that you'll notice here is that whereas before i only have that one end point now i have two end points i've also had to think about how i choose between those endpoints how i load balance between those endpoints and you know take a second to pause and put yourself in the shoes of a retailer what does this stuff have to do with selling goods and services and products to your customers well not very much but you're now responsible also for figuring this code out and owning and maintaining this code pretty straightforward we just figured out how we could add another payments backend and load balance between them but you'll also notice that if you look at your shopping cart code you're now spending more of your time on this non-business logic and what i mean by non-business logic here is basically code that doesn't have very much to do with you delivering your business at all it has to do with things like resiliency and up time and dealing with you know big problems that you might face that are more infrastructure level than actually delivering goods to your end customer so okay you just got a little bit more capacity for your holiday season well now we have a new requirement and that's you know we can't just randomly choose an endpoint we want to make sure that the shopping cart only calls on payments backends that are healthy and so rather than just randomly choosing one which is what we did here is we are going to actually test those endpoints make sure that they actually are healthy before we send traffic to them and so rather than get load balanced endpoint we update that function to be get healthy load balanced endpoint and you'll notice there's even a function here is healthy which i don't know how to write this basically what this would do is figure out it's got that ip address 10.0.0.5 is it actually healthy it would go and check you know get healthy endpoints with look at for each of the end points whether they are healthy or not and suddenly that chunk of code that you're owning goes grows more and more towards infrastructure level problems networking problems and that's where you're dedicating more and more of your time and so again like we saw before your code that is non-business logic continues to grow and again i need to ask you what does that have to do with selling goods and services to your customers well not very much and the examples i just showed you are pretty straightforward you know ones that a lot of customers might face well i talk to customers who ask for all kinds of things that i would put in the non-business logic category my customers have requirements like i have payments back-ends or other backends in multiple regions and clusters i need to be able to gracefully handle timeouts you know i might be handling private information it needs to be encrypted tracing and logging code needs to be in there too dynamically scaling up different payments back-ends i might even have development teams that want to operate in multiple different languages and so all of these things i would posit are you know non-business logic and your engineering efforts go to more and more handling infrastructure concerns which takes away focus and resources from delivering your payments from delivering your shopping cart from delivering your actual business and so the good news is you know the example i just showed you is a little bit contrived we've already found ways to solve this customers have for a long time been doing things like running a load balancer in between the shopping cart and the payment service and so here that load balancer is basically a proxy the shopping cart sends traffic to it the load balancer figures out where that traffic should go all of your non-business logic lives in that load balancer but even this pattern has its flaws which is you know that load balancer sits in between but you might have more and more client instances sending traffic through it and that starts to become a bottleneck you now have to figure out you know how do i scale this thing up how do i make sure it doesn't go down because it's a single point of failure and you even see other patterns emerge that i would also characterize as smells you know you might have for example a support service which supports important don't get me wrong but at the end of the day your customers are paying for goods and services you know they're not paying for support and so that's maybe a little bit less critical but i see plenty of customers who are sending traffic from both shopping cart critical services and support front ends less critical services through the same proxy and again you've got those issues around scaling making sure it doesn't go down but then also you have this configuration that now lives in your proxy belonging to multiple different types of services you need to make sure that if you make a change to the support front end proxy code it doesn't cause an issue for your other services and so now you're dealing with isolation problems and again all of these things go back to the idea that you aren't really delivering value to your customers when you're working through these types of problems over the past couple of years we've seen a pattern that helps us to solve this and the idea here is you have a dedicated client side load balancer often that is a sidecar proxy and so that might run right next to your shopping cart on the left hand side you see an individual instance of that shopping cart it has its own sidecar proxy that only has logic for the shopping cart routing for load balancing for the shopping cart and you know if you want to scale up and you have another shopping cart instance well that has its own sidecar proxy and so both of those can independently load balance between your payments back ends if one shopping cart instance or one shopping cart instances sidecar proxy misbehaves the other one is unaffected and so this gives you a really nice simple way to have client-side load balancing you get rid of those concerns around bottlenecks of things that sit in the middle you've got configuration that's highly tailored just to that shopping cart it's not shared across multiple services that's what the sidecar proxy model that we've seen in the last couple of years can do for you and so far so good you know you basically solved a lot of the problems that i described earlier but now you're getting yourself into a new game and that is that you need to figure out how you're going to configure that sidecar proxy so one way you might do that and i just made this up this is a proxy.com file imagine you know a json file or a yaml file it contains you know the configuration for your sidecar it has the endpoints for the payments backends it has maybe how you're going to do health checks for those you know it's got the information that the proxy needs in order to support that shopping cart and so okay fine writing a proxy.com for one sidecar proxy no big deal right and so what happens when you have a new payments backend well you're configuring your proxies again and so a way to do that is you write yourself another proxy.conf and i think you can already see where i'm going with this but you've now effectively started to take on this additional responsibility which is how do i write distribute store proxy configuration and in this case you know this is still pretty trivial when i talk to customers who are doing you know really enterprise enterprise-grade stuff um they have deployments that look far more complicated they have deployments that look more like this you know multiple regions multiple services some even hundreds or thousands of services each running their own sidecar proxies or middle proxies you know with things like encryption turned on or different load balancing policies and so now you've got yourself into the game of writing and configuring proxy.conf files and if you talk about expanding the number of services expanding the number of regions this all just becomes way more complicated you see customers doing things like checking these in have validation in place for proxy.com files figuring out how to make sure that you know there's no issues in those proxy.com files and i'll go back to the thing at the very beginning which is if you are a retailer what does that have to do with selling goods and services to your customers and again you know people have solved this there's a solution for it it's called a control plane and so the control plane is a service that is basically responsible for writing distributing making sure proxy.com gets to the right sidecar proxies it might have an api it might have you know different checks to make sure the sidecar proxies are helpful and you've probably seen things like this for example istio is a great example istio has a component called pilot and all that really is is a control plane that runs in your cluster you have some apis to tell it hey this is what the world should look like these are the policies that i want to enforce and then pilot will take that and go and distribute it to all of your sidecar proxies and so you know i talk about pilot as an example of a control plane another example of a control plane is traffic director traffic director is a control plane as a service it's a gcp managed service i'll talk a little bit more about what traffic direct what makes traffic director special but at its core it's a control plan you don't have to install it update it any of that stuff it runs within gcp it knows about the world because you tell it what the world should look like and it also collects signals really intelligently from your infrastructure and it generates that configuration for your proxies and so what is traffic director well there's four things that i want you to keep in mind traffic director is a universal managed control plane and so what do i mean when i mean universal i mean one part of that is that it's global by default you know it looks at your whole network as a global thing and so that includes not just containers but also virtual machines you know plenty of customers aren't ready to go you know 100 into kubernetes yet and that's totally okay whether you're on vms on containers traffic director really doesn't make a distinction about which services it's supporting if you know it's got for example an envoy proxy on the virtual machine or on the container that connects to traffic director using a set of apis that known as the xds apis traffic director will send it configuration back the other thing that's really cool about traffic director is it's not just gcp we're starting to now have and you'll see more about this later you know customers have deployments in gcp and non-gcp environments and other clouds and so for us being able to support those other environments is really key you'll see in a second about what that means practically in terms of what we're delivering but just think of traffic director as global as something bigger than just gcp as something that's bigger than just vms so if i can reinforce that further it's the fact that traffic director is more than just a service mesh you know service mesh is a type of deployment that traffic director can support it works really well with service mesh but we're also starting to see customers working with more than just these sidecar proxies deployed in service mesh we're starting to see customers who are using application libraries without proxies and we'll talk about what that means in a second as well and also we're seeing customers who are just deploying you know an envoy proxy on a virtual machine that acts as a load balancer you know similar to that model i showed earlier the one before the sidecar based model where you don't necessarily have to be ready to add sidecar proxies everywhere to get value from traffic director our goal is to support a lot more than just the traditional model where you have sidecar proxies next to your application workloads we want to follow you where your deployment is and what your topology looks like is something that we want to support the third thing to keep in mind is that's programmable so we've got an api that used to configure traffic director we've got a rich set of policies so traffic management policies layer 7 based policies we've got a lot of other interesting policies coming up as well but this gives you a centralized way to manage all of your configuration your policies in a single place you don't need to worry about distributing policies about sort of hand crafting them it's a programmable interface that you can use to configure your application network and then finally and this is what really distinguishes it from a lot of other solutions is it's a control plane as a service what i mean here is that it's a managed service managed by gcp it's got an sla our sres are kind of making sure that it doesn't go down and if there is an issue we're taking care of that for you it gets rid of a lot of the things that i would call you know not really related to your business around how do i install a control plane how do i keep it up how do i make sure that you know the state of my control plane is replicated across multiple regions these are all really fundamentally difficult problems that you if you're a retailer if you're a bank if you're a pharmaceutical company that's not really your business and you're not really deriving much value from taking on that responsibility and so with that i'm going to pass it on to kelsey who's going to make it make it real he's going to show a demo kelsey off to you all right thank you stewart so we're going to jump right into it as we just talked about traffic director is kind of center for all of our service configuration and the heart of our service mesh in gcp it's often paired with the thing called envoy so sidecar some people look at it as just a proxy the nice thing about it is envoy supports the standard xds protocol for its configuration that means i can point in that traffic director to grab all this configuration a lot of you will be familiar with the similar process that you see in something like istio but what happens if the overhead of having an additional sidecar like envoy becomes too high maybe for a high performance scenario or what about the management overhead of having to manage another binary and its configuration and the interesting thing that we're going to talk about today is proxy list service mesh if you think about it there's lots of high performance frameworks out there grpc is one of my favorites inside of grpc we now have native xds integration so what does that mean well it means that we can actually get our configuration from traffic director just like envoy and once we have our configuration we can actually have back-ends and we can do some of the traffic management features that you normally find in a broader service mesh or a proxy like envoy now to set this up we need some services now i can run my services on vms but kubernetes is one of my favorite platforms so what we're looking at here is just a basic kubernetes cluster called next if i were to drill into this kubernetes cluster i have a deployment of my calculator app and i'll show you how that calculator apps works in the moment just know i have a set of containers deployed to kubernetes now i have three of these and they're set up with the auto scaler to go up to up to 10 if i need it now typically when you have multiple instances of something you'll typically have a low balancer so let's click into that so here's my service configuration you'll notice this is just a standard kubernetes service object there is no load balancer at all all i have is a collection of pods and their ip addresses so we're going to need something else to handle the load balancing aspects for us now the nice thing about this is the proxy list grpc integration we can actually do this on the client side by getting the configuration from traffic director so before we do that we need to make sure we understand how the calculator app actually works so the thing you have to keep in mind is we're not going to change the way we write code here for example i can write a standard grpc application and test it on my laptop so let's do that now i'm just going to cd into this calculator directory this is where the source code lives and here's the server instance i'll just go ahead and build that really quickly so we can actually take a detailed look at how it actually works now the server calculator app all it does is expose an endpoint where i can call an add method and give it an array of numbers and have it give us back a result once that's done compiling we'll run it right here from my laptop so here we go we'll run that here so you can see that it's starting listening on port 5051 commonly the default grpc port and i also have my health checks on port 8080 so that's running locally here the next thing we'll do is we'll take a look at the client now the client side code is pretty straightforward lcd into that directory and then what we'll do here is i'm just going to compile the client and this is going to be a very standard kind of request and response grpc application so as we're building our client once it's set up we'll take a look at its command line flags to see what options we have all right so now that the client is built let's take a look at those flags and we'll see that we can give it a calculator flag and we can tell it where the calculator service is running so by default it's going to look on service host on port 551 and then we can give it a list of things to calculate pretty straightforward so let's just run that now so we'll say client and then we'll give it a couple of numbers we'll say 10 20 and you don't have to be great at math to get the answer for this and we can see that the result is 30 right pretty straightforward now what if we want to send our application the server somewhere else well remember i packaged up that server component into this calculator app that's running inside of gke or kubernetes and you'll see here that we have multiple instances running ideally you want an ha service and then we also have it configured to listen onto this port 551 now what if i wanted to actually do this from a virtual machine and no longer my laptop now we have to deal with the complexities of service discovery and making sure that we can load bounce across all those back-ends so the first step we got to do before we can do anything else we need to configure traffic director so i'm going to pop over to traffic director now what you'll see here is that i have one service healthy but if i scroll down here you'll actually see something interesting you'll see that i have one network endpoint group when i created my service inside of kubernetes i told the gke cluster to actually manage a network endpoint group that has all those pods behind it so if we have three or ten all of them will be grouped behind this network endpoint group you also see this green check box here what this is saying is that traffic director is doing its own set of health checks to make sure that all of those pods are actually healthy before we send the configuration with those backends in it to any client-side grpc application looking to leverage these the last thing we need here is also how do we communicate what service do we want and this is where some of the routing rules come into play now if you have any experience with google cloud load balancer products this is going to look very familiar to you first we're going to start with the grpc protocol we're going to associate a set of services that we created early on the services tab and if you click on calculator here what we'll see is that we give it a couple of name matches so here calculator plus that port combination will route us to this grp service called calculator right so now all the configuration is set on the server side now the next thing we have to do on the client side is we need to tell grpc how to do this now you're probably thinking do i have to rewrite my entire application to make this work well the good news is with one import statement we can make this work automatically i'm just going to pop over to github really quickly so we can actually see what the code looks like so here we're going to click on our main code i'm just going to zoom in a little bit here so we can see what's going on this is the only import that we have to do and if you look at this it's not going to we're not going to actually use xds anywhere we're just going to load the client-side load balancing and service discovery components that make it compatible with traffic director so this is an xds compliant server side client-side load balancing so with that in place we've got one more step that we need we actually have to tell that bit of code where traffic director is so here we have a service uri so this is going to be our xds control plane in this case traffic director we're going to use our service account credentials so we can authenticate to it and when we register with traffic director we have to pass a little bit information about ourselves mainly our project number and what network we care about now one thing to keep in mind here is my kubernetes cluster is running in a network in my vpc called google kubernetes engine and that's super important and we have to let traffic director know that's the network that we would like to find services in and the last thing is traffic director is really intelligent about routing our traffic to let's say the nearest zone so if i'm in us1a and i have multiple kubernetes clusters with the same application deployed it will be nice to have traffic director user locality information to give me an optimized set of back-ends maybe for things like latency or for zone is down redirected me to another zone that's available all right so now that we have all our configuration let's copy this client binary to a vm so we're going to do here is i'm just going to copy the pre-built binary to one of our vms that i have in my infrastructure so we'll just take a look at that calculator binary make sure that we have it for the client you see it here and what i'm going to do really quickly is i'm just going to scp this binary to a little vm i call proxy list grpc so let's just run that command really quickly so we're going to do here is just going to copy this binary that i just ran on my laptop unmodified code to that virtual machine and now what we're going to do is ssh to that virtual machine and once we're on that virtual machine we'll have the ability to run the same code again but this time we're going to try to hit the containers running in kubernetes so we need a couple of things first we need to make sure that our environment is set up correctly we have to tell xds where our bootstrap file is so you'll see this environment variable being set to my local file system where the xds bootstrap configuration file is so we'll just source that to make sure that our environment is ready and this time we're going to do something slightly different with our client connection so we're not dealing with localhost anymore we're going to do something slightly different with our calculator flag we're going to give it the xds schema and what this will do is it will tell the xds library and all our client side load balancing to look up the service inside of traffic director that's going to be just enough so it bypasses the normal lookup service in my case dns by default let's give it a couple of numbers we have 1 and 56 we'll run it now and we see that we get the results of 57 just to make sure that this is a live demo let's just pick some random numbers here one plus 999 it's 1 000. everything is working great i know you might be thinking like this is a virtual machine how hard would it be to install envoy in a virtual machine and just use envoy to keep everything standard and you'll make a good point it doesn't necessarily address the need for high performance if you want to avoid the proxy but you're right on a virtual machine we do have the ability to just run envoy as a sidecar right there on the virtual machine and bypass doing any of this so this is another thing that i wanted to try now this might get me in trouble with the pm team because this is not something that we advertise is working but what's a live demo without a little bit of creativity one environment that i like a lot and really makes sense for a proxy list integration with traffic director is cloud run if you're not familiar with cloud run it's our serverless offering where i can run my containers so earlier we just showed running on a laptop running on a virtual machine but think about cloud run i don't have the option of running any additional sidecars i can only provide my application and that's it so in this particular environment is pretty constrained in terms of what i can do so the nice thing about this is i have the ability now to do the exact same thing that i was doing on that virtual machine on a serverless offering like cloud run you'll even notice here that i'm passing in the command line flag that matches the exact same one that i was running earlier now we did something a little bit different here i took all of that client code that you saw earlier and i moved it into an http handler so if this is all working i should be able to call curl hit the cloud run url and then the client code will basically use this is the same address that we were using before call out to traffic director get all of the back ends that it needs to route traffic to now there's one more gotcha here cloud run runs in a slightly different network than my vpc so i had to set up one more thing before i could run my application i had to set up a vpc connector now in the serverless world we can set up these connectors to be very specific to a specific network in this case the google kubernetes network that i have set up earlier is where my kubernetes cluster is running and all those containers and it also matches the configuration that traffic director has all i'm doing here is setting up this serverless connector called calculator so when i deploy my cloud run application i can also give it that vpc connector name to ensure that when this container starts up it'll also be able to have a lag into that vpc and the last thing we can also replicate the fact that i have this xds bootstrap config inside of this environment variable letting me know where to resolve just like in the vm and we'll take one more peek at something else just to make sure we're all clear on how this actually works if i go over here to the calculator i want to show you the docker file that i used to create that container i want to show you that we don't actually have to change our code too much there's a lot of noise here so i'm just going to break it down i'm building the calculator app just like i built earlier on my laptop the big difference that i'm doing here is i'm taking the binary from this step i'm also copying in this xds bootstrap config into the container image now the nice thing about that is i have all the things i need necessary to call traffic director and i run one deployment script and if everything is working i should be able to call that particular endpoint so i have my curl command this is the url from cloud run and what we'll do is we'll pass in a few integers here to see if we can get this calculation you can see it coming up pretty fast those are pretty big numbers and some of you may be struggling with math so let's make it a slightly easier for the viewers so we'll give you one three and five and you see that our answer is nine and with that i would like to end this presentation as you can see traffic director is super flexible in terms of implementing a standard xds protocol combine that with proxy list grpc integration we can now take a standard xds client run it on my laptop a virtual machine or even a serverless environment like cloudrun and we don't have to manage any other binaries to get what feels like a native service mesh integration and with that back to you stuart all right nice thank you very much kelsey so you know so far we've gone through uh context on why something like traffic director exists what it is kelsey brought it to life by showing you you know how someone might interact with it what are some of the really cool things you can do with it another thing i wanted to get into is just what our customers what i see customers actually using this product for because it's good to understand it it's good to see you know some hands-on but what are real problems that customers are solving when they use traffic director and i'm going to interweave in this some of the new announcements that we have as well and so let's go back to the case of a retailer and i know i keep harping on these large global retailers but i think they're a great example because in retail you really can't afford any down time and so something like black friday coming up where every single second is money lost if you're down is exactly where a product like traffic director shines and so what you see in front of you right now is an example of the type of deployment that you might see with traffic director so we've got traffic director running separate from your region separate from your deployment it's a managed service that you don't have to manage yourself and here we've got an example deployment we're in multiple regions you're in u.s central one you're in asia southeast one you've got replicas of the same services so you might have a retail front end hosted in kubernetes engine in both us central one and in asia southeast one you have that because you know some of your customers maybe are in the us other customers are in asia southeast one so you want to make sure that when they send you a request it goes to the nearest services so you reduce your latency you reduce your network travel costs you also want to make sure that you've got replicated services in case one goes down and so what i'm going to show you is how traffic director makes it super easy to just automatically fail over so you've got your retail front end your shopping cart your payments each of these are hosted in multiple regions so that you can meet those goals around latency around cost around high availability and so customers are using traffic director in combination with other gcp products as well what you see here on the left hand side is the global load balancer so this might be you know a global https load balancer and what that does is it gives you an ip address that your clients on the public internet can send traffic to and so that might look something like this your client sends traffic to your global load balancer and let's say your client happens to be in u.s central one while the global load balancer is smart enough to send that traffic to u.s central one that minimizes latency that minimizes cost and so now your retail front end you know which is what they're interacting with they add something to their shopping cart and your retail front end needs to be able to reach the shopping cart that shopping cart might be on compute engine and it might just happen that for some reason in u.s central one well your shopping cart is down maybe you deployed a new version of your service that didn't work maybe you um you know you've got some planned down time and so what would happen here is rather than having your retail front end send traffic to the shopping cart and hit errors for that end user that traffic just goes to your shopping cart in asia southeast one it just fails over seamlessly to another region and this is actually pretty straightforward to set up a traffic directory you just add back ends in multiple regions as part of your service and traffic director will make sure that your retail front end knows how to reach the ones that are healthy and so the shopping cart can go to your payment service all's good your customer gets served despite the fact that your shopping cart in u.s central one happened to be down that automatic failover for highly availability super critical in the type of business where you have to make sure that every uh request is handled in a high quality way and gives a good end user experience because if that doesn't happen that's money left on the table for you as a business another use case i see i've got a customer who's one of the world's leading logistics providers is that they have to make sure that when they deploy a new version of the service they don't have any downtime and so just for a little bit of context here this customer has a prediction service running and what that service does is it tries to figure out when a package is going to be delivered well they do that by training a machine learning model and deploying that model as part of a workload that's running on kubernetes engine they need to retrain that model every once in a while and redeploy that and so what that means is when they do that deployment they need to make sure that that model actually works and that the deployment happens successfully anyone who's done deployments knows that a deployment is inherently risky you might try to make sure that you have a development environment a staging environment a production environment sort of qualify that new version as it goes through but no two environments are identical and so even if you do everything in staging and validate that it works in staging there's still some risk that once you get to production it's going to go down and so this is a great example of how we see customers using traffic director in this example the front end is sending traffic to the prediction service there's a prediction v1 which is the old version prediction v2 which is the new version of the model the the customer has set a traffic split policy which causes the front end to send a percentage of traffic to the old one a percentage of traffic to the new one and as you validate that the model is good you can continue to increase that percentage of traffic and so suddenly your deployments are a lot safer because you only subjected a small percentage of your traffic to the new version you've checked that it worked if it didn't actually work well you can flip that back to 100 to the old version this gives you a lot of safety around your deployments it also unlocks all kinds of really interesting devops patterns like blue green deployments canary deployments you know these are the types of things that organizations are doing these days and you can use traffic director to deliver those types of use cases another really interesting one is this idea of network edge services for gcp and also this is new for non-gcp environments and i'll talk a little bit more about what i mean by network edge services in both gcp and non-gcp environments so i've got plenty of customers you know large banks for example who have services that run in gcp and they also have services that run in their own on-prem data center and that might be you know a temporary state as they're migrating everything to the cloud but it also might be a permanent state there are all kinds of really valid and good reasons why you might want to have some workloads in the cloud and some workloads in on-prem and this is great right makes a lot of sense one of the things that customers come to gcp for is you know our global load balancer and this is similar this is the same as what i mentioned earlier and this global load balancer is the thing that delivers to you this group of services that i call network edge services and so what do i mean by that i mean that when a client sends traffic to the global load balancer there's all kinds of different policies and services that you can apply there an example is cloud armor you want to make sure that you have ddos protection in place so that your clients on the public internet aren't overloading your back ends and so this checks to make sure that you know the traffic that's coming in is safe you might have cloud cdn for content distribution so this helps you deliver results more quickly to your end users it also saves you a bunch of cost by caching different static assets that are delivered by the load balancer the load balancer also has all kinds of really interesting features built into it things like a global anycast vip so that you get a single ip address that you can use around the world and your clients can just address that ip address it'll go to the closest instance and that's how you minimize that latency minimize that cost and i mentioned these things because you know customers look at this and they go great i can use all of this with my cloud service i can make sure that if i have a cloud service it's protected i'm caching things the right way but you know i as a you know bank for example don't really have that same global footprint that gcp has i don't have points of presence all across the world where i can deliver a global anycast vip where i can do you know different security mechanisms that are you know technically or for cost reasons infeasible for me to deliver myself well wouldn't it be great if you could actually have that for your on-premises services as well and so one of the things that we see as very critical for traffic director is the ability to support more than just gcp um and so today what i'm showing you is the ability to actually use that those edge services with your on-prem services and so what does that look like well that looks like a you know pool of proxies deployed here on a compute engine managed instance group that managed instance group is just part of our infrastructure it scales up and down dynamically each of those is running a proxy and it acts as a middle proxy load balancer it connects to traffic director to get configuration an example of the type of configuration would be hey there are these on-premises endpoints each of them belonging to the on-prem service that you see on the right-hand side traffic director knows about that because you've configured it that way traffic director lets the proxy know about it and so now the next time a client sends traffic to the global load balancer that traffic can go to the metal proxy that proxies and forwards the traffic onto your on-prem service and so this is a really powerful use case where you can get things like cloud armor cloud cdn global anycast you know these network edge services for both gcp services and services running in non-gcp environments we also see you know this this is a a key investment for us which is this infrastructure is really interesting but you know how do you go from zero to deployed well we want to make that as easy as possible and so what you see here is the current way of deploying something that you would call a service mesh you have some compute engine services these could be kubernetes as well you've got maybe a front end and a back end each of them has a sidecar proxy and setting this up um you know it's not that hard but it does require some thought you know where do i get this proxy what's the version that i want to use how do i make sure the version is secure and i keep it up to date um you know how do i you have to do things like configure ip tables um and if you're like me you know before i started even looking at traffic directory i had never even messed with ip tables and i talked to customers who were like this thing is really scary because if you misconfigure it well you can suddenly drop a whole bunch of traffic and you don't want to have to do that and so a lot of customers find that deploying sidecar proxies not the easiest thing in the world so we want to make that a lot easier for you and so another thing that i'm showing here today is a new way of deploying uh your sidecar proxies and you know a really easy way to think about it is it's basically a virtual machine template it's a virtual machine template that you control with some flags with some parameters you know dash dash service proxy enabled and what that will do is it will automatically onto your virtual machines which could be running like a client application or a server application too onto those virtual machines the template will install envoy a recent version that we've qualified it will connect it to traffic director it will do things like set up ip tables for you so you don't have to deal with you know things that might be very foreign to you which again have very little to do with actually delivering your services and it gives you a really easy way to do updates um you know it's as easy as do is issuing a couple of commands to initiate a rolling update you don't need to think about picking an envoy compiling that envoy the envoy proxy compiling it deploying it setting things up that's all handled for you um and so another thing uh that we're announcing um and kelsey showed this before is this idea of using an application library instead of an envoy proxy we've got you know traffic director support for proxis grpc services and what do i mean by that um what i mean by that is if you look at the most recent versions of grpc they support this protocol called xds xds is a group of apis that are used by traffic director the control plane and things like the envoy proxy or like grpc to exchange information about the world about the network about your topology about what the endpoints are that you want to reach and so much the same way as when you have envoy deployed when you're using a recent version of grpc you can provide it some configuration when you start it up you use a different xds resolver it connects to traffic director traffic director tells that proxy what are the different endpoints associated with the services they're trying to reach because you'll you'll notice very quickly if you've used grpc before it's a super powerful application framework but it doesn't come with everything that you need to do a distributed service um you see a lot of customers who are doing things like adding an envoy proxy so your application goes to that onboard proxy which then sends the traffic and that's totally fine it's super feature rich it works really well for a lot of customers but if you're finding that you know adding an envoy proxy is something that you know maybe you can't do for reasons like you know you can't really add a sidecar you can't mess with ip tables those types of things well now you have another way where if you're using grpc you can very easily get the benefits of service mesh simply by updating to a new version changing your x your resolver to the xds resolver and connecting it to traffic director and this gives you already straight out of the box service discovery you know what are the end points that i'm trying to reach it gives you load balancing and we're working on all kinds of other interesting things that are super relevant here things like more traffic management and so stay tuned for that you're going to see that the goal is that we bring all of the capabilities that are relevant to grpc that you can get in something like envoy to grpc as well so example benefits here are much easier way to adopt service mesh also performance we found that in some cases this can be more performant than if you have to go through an envoy proxy first and obviously this is going to depend on your use case what you're doing exactly but also a very interesting thing to explore especially if you're already thinking about grpc for performance benefits this can get you even further um and so with that i want to wrap up and say thank you very much for listening today if you've got questions there's a dory where you can ask those questions we'll be looking at the questions trying to get them answered for you if you're new to traffic director check out the docs there's a setup guide that walks you through end to end how this stuff works and also you know some of the things that i showed you today may not be released yet at the time where this comes out so if you have questions you know drop them in the dory um or i created this form that you can use go to the form check it out you can request access and we'll try to get back to you as soon as possible so with that i want to say you know thank you very much for having me big shout out to the traffic director team who keeps pushing this product forward thanks kelsey for helping us with the demo today and that's it for me you 