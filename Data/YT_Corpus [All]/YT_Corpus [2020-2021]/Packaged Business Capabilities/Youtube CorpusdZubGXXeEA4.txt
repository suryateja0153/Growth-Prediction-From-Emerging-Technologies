 is richard i'm in software engineer at any skill and this is travis who's also going to be talking um in the second half of this talk he's a software engineer at uber we'll be talking about rey and growing uh ecosystem of machine learning libraries so for a quick overview of today's talk i know a couple of folks tuning in today who haven't heard about ray so we'll begin with a quick whirlwind tour we'll then be overviewing some of the new exciting integrations around rey's broader machine learning ecosystem and finally travis will be diving into the integration of ray into uber's open source machine learning ecosystem so you'll be seeing the forces joined between horvald luguig ray tune and beyond uh desk on ray so i want to start with a quick overview of rey for those who are not familiar rey is a project with a mission to simplify distributed computing so it's a project that we've been working on for the last four years and it's associated with three different labs at uc berkeley these labs have had a long tradition with open source and projects of that have come out of this lab include spark cafe uh apache mesos and now rey so um rake itself consists of two primary components the first part is a ray core which is a simple framework for distributed computing we'll be talking about the primitives that raycor exposes in a later section of this talk the second primary component is the ray of ecosystem so there's uh many libraries that are packaged with ray for domain specific utilities including uh hyperparameter tuning reinforced learning model serving so i just want to talk really quickly where does ray stand in relationship to the rest of the compute and python ecosystem uh so ray is very similar to desk or celery if that's uh if you're familiar with those sort of systems it's a framework for distributed computing for python uh with respect to the rest of the compute ecosystem really runs on any cloud so it works with aws gcp azure or even kubernetes and it's also very broadly compatible with other libraries um in the python ecosystem so this includes numpy pandas tensorflow paper spacing etc so now that we have some context about rey you might be wondering well why does ray exist in the first place so in order to really understand ray we have to first take a look at the history of distributed computing now distributed computing is not new almost every decade a new category of applications when distributed in the 1980s we saw the emergence of high performance computing or hpc for short led to powerful simulators which allowed engineers and researchers to do weather forecasting molecular modeling and much more in the 1990s we saw the rise of the internet and the web this led to building highly distributed systems to support popular websites with millions of users next in the 2000s we saw the emergence of big data so we had many of these internet companies accumulating large vast troves of user data to improve the user experience while also improving their own business strategies and finally as we enter the 2010s we're seeing the emergence of deep learning so companies are starting to leverage this data collected to build powerful feedback systems and models uh enabling new applications and these models are getting larger by the day and often tied to distributed training capabilities so as you can see over the years more and more workloads became distributed and what's interesting is that actually once they became distributed they remained distributed so now after after four decades we have these four pre-existing workloads hpc big data microservices and and deep learning and what we're seeing is that a common trend is that these workloads are starting to become tightly integrated with each other and this is this growth and this concentricity is largely due to the growth of machine learning and ai workloads so for one um many of these artificial intelligence workloads are already overlapping with high performance computing distributed training frameworks like corvad leverage mpi underneath the hood a system developed by the hpc community in the last 30 years we're also seeing micro services uh which par power many of today's web services to um to start to integrate with machine learning components this allows applications to to better uh to be tailored towards specific users ultimately improving the utility of these different micro services finally deep learning applications uh ai applications and big data are strongly overlapping because many of these ai models and applications today require huge amounts of data to train and become a performant so creating these apps distributed ai applications is actually quite hard despite the convergence of these four different workloads the systems used to uh support these different workloads are as diverse and as distinct as ever there's really no single framework built to support these end-to-end distributed applications so ultimately the promise of the ray project is to allow developers to build end-to-end distributed applications that are both general and easy to use so now that we've motivated ray let's quickly talk about the core primitives that of rate which are promised to simplify distributed computing the ray api mirrors the way that programming is traditionally done with function and classes in array functions are converted into tasks while classes are converted into actors i'll elaborate more about this in later slides but first let's first talk about functions to tasks so here we have a couple simple python functions the primitive that ray exposes here is the ray.remote decorator which takes in a arbitrary python function and designates it as a remote function you can then invoke these remote functions with functioning.remote which immediately returns a future this feature is a object reference that that represents the result of represents the result in the background ray will schedule the function notification on the cluster or on your machine on a separate thread and execute it now this way many different functions can run at the same time and you can also construct different these computation graphs using this api now if you really want to actually get the results of this computation you can call ray.get which blocks until all the tasks have finished executing and will retrieve the results this api though is simple already enables the straightforward parallelization of many python applications and you can read more about this under grade documentation so the next thing is is reactors so these are essentially remote classes again here's a simple python class with a method of incrementing a counter and you can add a remote decorator to convert it into an actor then if you instantiate this class with counter dot remote or class name.remote and a worker process that contains this class is created somewhere on the cluster each method uh that creates tasks that are scheduled on that worker process in this example we have three or two different tasks inc remote and that's called twice and they're executed sequentially on that actor they all share the same state as the same actor obviously which is the semantics of a class and you can also easily access a gpus with with the ray api by simply specifying some resource requests this allows users to easily parallelize their ai applications so so given that what we see here this is a really powerful api and now we're seeing a large growth of different libraries using the ray core framework to to distribute and parallelize their workloads so one area that we've seen enormous growth is is with our the libraries that we've developed on top of ray so um so there's you know there's basically two categories of ecosystem libraries um so the first category is libraries that we developed as part of ray these native libraries and the second half is the third party libraries that integrate with raid the two of the first libraries that we began building as part of ray who are our live for reinforcement learning and tuned for high prem research today these are some of the most popular libraries for reinforcing learning and hyper gram research in the world today more recently we've been began working on libraries for model serving deploying models in production and distributed training now of course the most exciting part of this growth is this third party libraries that are built on top of ray so spacing and hugging face which are hacking phase transformers which are two of the most popular libraries for nlp integrate with raid to train uh on multiple gpus and tune deploy models you can also scale horivod pi torch and x-ray boost on your ray cluster and use them with ray tune and race serve so travelers will talk a little bit more about how uber's open source ecosystem is leveraging this in the second half of this talk we also see auto automl frameworks such as ludwig from uber uh explore integrations with ray and travels will also expl uh we'll explain about um the roadmap for this integration in in in a couple of minutes in addition to these training libraries we're seeing major cloud machine learning platforms such as sagemaker and azure uh ml integrate with ray tune and rlip for it for providing training and reinforcing learning services and this just goes on so we see you know um hyperopt and noptuna some of the most popular hyperparameter tuning libraries integrate with ray tune for a high parameter search das which is a popular distributive system in python with a great data frame library uh can now be run on top of ray we have fluids and biases and cell did which integrate with ray tune and also leverage ray for massively parallel model explainability and some these are just some just a small section of the many libraries are integrating with ray today so what we're actually seeing is that ray is becoming the go-to framework for not only scaling a simple python code but also to scaling python libraries the benefit here is not that not just that you can use one of these libraries but rather you can use them all together and the skill entire application chain with all these different libraries on a single cluster so if you're a library developer and you're interested in in scaling your library with ray do reach out to us on the race stack or on the right github we'd love to help out so that's it about the ray ecosystem for where it is today and travis will now talk about some new exciting developments for ray with uber's open source machine learning stack thanks richard yeah so as richard said i'm travis i'm a software engineer at uber i am the maintainer for the horovod project that does uh distributed deep learning and i also am a co-maintainer for the lulu project that does automl and today i'm going to tell you a little bit about how we see ray fitting into both the uh internal work that we're trying to do at uber in terms of providing ml infrastructure to our internal data scientists as well as how we're leveraging ray and the open source products that we use within uber and also contribute to externally in order to provide a more seamless experience for our open source community as well so let's start by talking a little bit about horabad so horavad is a project that was originally developed at uber it's a framework to do large-scale distributed training of deep neural networks uh its goal is ultimately to make distributed training fast and easy for any framework and any platform so we originally open sourced horva back in 2018 or at least it became a member of the linux foundation in 2018 and since then one of our primary goals with horovod has been to figure out ways that we can make it more accessible to people who maybe don't have the kinds of sophisticated high performance computing clusters that research labs or big companies do and that's one of the reasons that we were really interested in exploring ray because it enables this kind of capability to scale up your compute resources very naturally without having to get involved with a lot of uh you know data infrastructure kind of expertise so horvat on ray was a project that started actually just a couple months ago i think and richard actually was able to implement it and i believe about 400 lines of code so it just kind of speaks to the the very seamless kind of apis or very like reusable apis that ray provides to kind of make such an integration possible and as i'll explain in some of the further slides you know being able to run horvat on top of ray in isolation is cool unto itself because a lot of people struggle sometimes with launching a horrified cluster being able to train your job you know in the cloud something like that but what really makes this particularly powerful is how it interacts with other pro other stages within the machine learning workflow process and to kind of talk a little bit more about that i want to mention as well and kind of deep dive as a case study the ludwig project so ludwig is another open source project that came out of uber ai and its goal is to make deep learning uh kind of an abstraction on top of just um the problem space of i have some inputs i have some outputs i want to train a model so it's effectively an automl toolkit for you so given any inputs and outputs ludwig will build the right model for any task and to do this we have a very novel uh architecture that we call encoder combiner decoder where we take your input features whatever they happen to be we encode them into a lower level representation we combine them and then we decode them into the outputs that you're trying to predict so ludwig is a good case study for um lots of in all problems at uber because when lubric started and this is also true up to this point it focused fundamentally on the research problem how do i solve this research problem of being able to predict any attribute i want to predict given any input data i happen to have and typically in these sorts of scenarios the researcher will do what makes what's easiest to do for them in their local machine or local development environment which is usually something like pandas it's a very elegant api for doing data processing and so we read the data in whatever formats it's in csv part et cetera we read a very simple yaml file config that defines the steps that you want to what your features are in your data set how you want to interpret them as text or categorical data etc and then we do the pre-processing on the single worker using pandas and output the result as a set of training data sets as numpy arrays so we have the training set as numpy validation set test set etc in the next stage we use tensorflow to actually do the training on these data sets we output a trained model and then we similarly run evaluation on the held out test set and then finally output the results to some kind of storage layer usually a local file system something like that and so this is at the point where the scalability challenges start to creep in as you say okay well this works very nicely on a single machine it's very easy to iterate on very easy to develop but how am i going to scale this up to process very large data sets how am i going to run this in production in kind of a retraining scenario something like that and this is the kind of situation that we encounter very frequently on the on the machine learning platform at uber is data scientists having this exact scenario wanting to scale up their pandas and and local tensorflow code to very large scale so with ludwig we have a single worker for pre-processing we do support horavod currently for doing training but single working for preprocessing and one of the biggest constraints is that because we're using pandas the whole dataset has to fit in memory so okay we definitely have to address that in terms of uh other challenges hyper parameter optimization is a big one so in our situation we're not just optimizing over learning rate or something like that or even the architecture of the model or even optimizing over the pre-processing steps or what you might call future engineering and trying to find the best total combination that produces the best model so what this means is that if we want to scale this up we also have to think about not only how do we scale the training but how do we scale the pre-processing and how do we scale that together within the hyper parameter optimization so conventionally how you would typically approach this problem and this is how we approach it at uber today is that you would break apart your single worker training script into a series of components that run on specific infrastructure and this infrastructure is typically heterogeneous you use a certain piece of infrastructure for one stage and a different piece of infrastructure for another stitch for example if you're doing a lot of data processing you almost always are going to be using apache spark it's a very popular framework for doing large scale horizontal scaling of your data processing workloads and then once you want to do the training you would want to use something like horovod to distribute the tensorflow process across multiple gpus and sync the parameters and then when you do evaluation because you don't have to sync any state between workers you might want to switch back to something like apache spark again maybe this time using gpus to accelerate the inference so how are you actually going to stitch all these things together well since these are very heavyweight steps very heavyweight piece of infrastructure you would typically use some kind of workflow engine like airflow apache airflow to define each of the steps that you want to perform and make sure that you spin up the right infrastructure you have the rights and sources and sinks for your data and that you transition uh with fault tolerance to the next stage because these operations are so heavy weight you need to be prepared for any mishaps that might occur along the way so after the training step occurs you write your model to some large intermediate storage like hdfs something like that and then because all of this is happening in a fully remote setting you then might need to have a final deployment step that actually takes the model from the distributed file system and puts it into your model store so why is this not ideal um well i think one of the first challenges that we came across when we were looking at doing this for ludwig was that we looked at the pre-processing code that we had written in pandas and we realized that we were going to have to basically do a complete rewrite to make it work with spark transformers and so what this effectively means is that we're having to maintain two distinct code paths one for local or small scale training and one for large scale training when we talk about scalability we really don't want to think of scalability as just tens or 100s of machines to thousands or millions of machines i mean that's certainly great but really the difficulty is in going from one machine to the tens to the hundreds to the thousands of machines and so that's where something like ray can make a really big difference so similarly these heavyweight steps this very heavyweight infrastructure that's requiring airflow um you know that's kind of an inhibitor to getting this laptop to massive scale from making sense because no one's going to be wanting to run these heavyweight steps with airflow on their local machine as part of their normal development process and what about hyperparameter optimization so airflow is really great for defining a static series of steps that you want to perform but hyperparameter optimization is a dynamic process difficult to model with static workflow definitions so you really have to think creatively if you're going to solve this using this traditional ml workflow design and that's why we started looking at the ray ecosystem so one of the great things about ray is that not only does it simplify the infrastructure component for you but also comes with this huge ecosystem or bag of frameworks that you can pick and choose from and find the one that fits your scenario the best and this is a really powerful thing i want to mention because very often because infrastructure is usually such a heavyweight thing you're typically locked in so it's like if your company is invested in spark like you got to use spark for everything right and spark is great but sometimes you might say well maybe for this problem i want to use das so you know with ludwig everything's in pandas it'd be great to use something like das or moden to swap out um the pandas dataframe for a distributed data frame very seamlessly right and that's something that ray makes very simple to to try out similarly das is also appealing it's a pure python data processing engine so it's very low overhead when you're debugging you don't have like java calls in your call stack and it has gpu acceleration as a as a core feature through the rapids api and uh qdf horabad as i mentioned before at uber we heavily rely on horabad to do the heavy lifting and distribute training um it's very fast uh it works with tensorflow pytorch and xnet which is important because in ludwig we want to maintain framework uh agnostic codes so that you know we currently use tensorflow in the future we also want to be able to support pi torch so horabad makes that very simple uh with the elastic api introduced in version 0.20 we also have fault tolerance and auto scaling out of the box which can also be run on ray without any special considerations and it's very flexible too so there's no restrictions that horabad puts on the structure of your training code most other frameworks require that you have to kind of restructure your code to make them work with their method of doing distributed training but with horrified you just add a couple of extra lines in your code you know wrap your optimizer and then you're good to go and finally ray of course it brings everything together into a single info layer and also provides several additional frameworks that we can't find any you know similar alternatives to and that are really great like uh the ray tune framework for hyper parameter optimization uh ray serve for doing uh serving so those kinds of capabilities are not only the just provided by ray for free but they're also the best in class of what they do and being able to leverage that is really powerful as well so what's lubligon ray look like then so the nice thing about this setup is that from the user's perspective from the code that we actually write everything is exactly the same as in the single node scenario for the most part except that we have an additional abstraction layer where we replace the pre-processing side with das which effectively distributes our pandas code and we replace the tensorflow training side with horobot on ray which distributes the training code and then finally we can do evaluations similarly with das running separate copies of the model in each worker to do the large scale evaluation and then finally we don't have to have any separate deployment step because we can return the model the train model directly to the client running on the gray head node and write it directly to our model storage without any additional overhead and similarly for retune we can take our entire processing pipeline and scale it up into multiple parallel trials and because it's all running on a common infrastructure layer we can even run the das distributed pre-processing the horobot distributed training within a single trial using the resources available to us from the cluster and let's talk a little bit about serving so as i mentioned before we can deploy our model to a model store or write it to a file system something like that and then for our open source users this is then very simple for them to just take the um the model that's been written to disk and use the luvig api to launch ray servers on the on top of their array cluster that can serve the model for prediction so what's next so this is uh this is work in progress that you'll see in the ludwig v 0.4 release i'm really excited that this work is progressing so quickly and uh really thankful to any scale folks for all the the work they've contributed to us on the uber side as well going forward there are some other things that are very interesting that ray enables i want to briefly touch on one of them is the fact that when we move from pre-processing to training there's this additional barrier where we have to materialize the data set to disk so that we can hand it off to the training side but people often ask is it possible that we can eliminate this somehow and with ray we actually have for the first time a very simple way that we can so you know theoretically because these ray workers um you know for the pre-processing with task or modem for example and the horrible training are all running on the same infrastructure we can theoretically do things like co-locate them together we can have the uh hor bod workers directly access the memory shared memory of the of the desk or modem workers or the partitions that they're operating on and be able to do the training that way and so this is something that we're very interested in exploring and again it's all possible thanks to the ray uh the way that ray is ultimately designed so that's it for me please check us out on github if you're interested in contributing or trying out horovod or ludwig uh the links are there the urls are hopefully very simple horabah.ai and lukeweek.ai and with that all hand it off to richard to close us out thank you so that's it for this presentation on rey and it's uh growing ecosystem um so this project wouldn't be where it is today without extremely passionate community and that's something that we're really grateful for um if you're looking to try out rey or horvath or ludwig uh we encourage you to get involved and reach out to us either on slack uh go go ahead and try downloading rey or horrorbot and lubric and just let us know if you have any thoughts or questions thanks again for coming to this talk 