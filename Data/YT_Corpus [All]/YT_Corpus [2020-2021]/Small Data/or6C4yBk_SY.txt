 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu so are we ready to go any questions on 1806 5 so it will be as I said before a mixture of linear algebra and math questions along with online using the material ok so I'm in this first week or two reviewing the highlights of linear algebra and I've reached this point to remember well so we I just said two words about multiplying matrices by using column times row as a as a way to do it and now I want to illustrate that by the five key factorizations of matrices okay so what are they and do you recognize them everybody uses those letters in fact those some of those letters like lu or QR would be the most used matlab commands in linear algebra so a April Lu I'll maybe say something I'll develop today but it's about elimination solving linear systems so that's always the start of a linear algebra course but I it will go fast here I just want to show you a different way to get to L times u lower triangular times upper triangular probably you've seen that those triangular matrices so do you know what QR is what's QR about these squares is the big application the factorization so what kind of a matrix gets that letter Q orthogonal the columns are orthogonal often orthonormal so orthogonal means they're perpendicular to each other and or so normal means they're unit vectors so that that is so so Q often represents a matrix with orthonormal columns so so you we could say gram-schmidt if you want to remember a couple of old-timers who whose algorithm produces Q and R how about this one this is really a central one in math pure math applied math everywhere applications so S stands for symmetric so this is a special factorization for symmetric matrices and you can see that it's symmetric this lambda is the diagonal eigenvalue matrix always lambda for 4i ghen values q is like that q different q of course that q you can find sort of just straight forward from gram-schmidt this q involved has the eigenvectors so you don't find eigenvectors without some extra work okay so that's eigenvectors yeah so that would be worth expanding so here are the eigenvectors of of the matrix sn2 them normalized here are the eigenvalues lambda 1 to lambda n and here are the eigenvectors now transposed okay so remind me of the great fact about two facts I guess one fact about the eigenvalues and one fact about the eigenvectors this is like an important fact in a statement in linear algebra what is what do we know about the eigenvectors oh well i guess i'm giving it away the eigenvectors are orthogonal that's very important make some matrices well they're beautiful matrices they're the kings of linear algebra Q's are the queens in my opinion orthogonal matrices are the queens of symmetric matrices are the king so so these are orthonormal eigenvectors and the key point an important point that's implicit here is there are n of them there's a complete set the matrix can be diagonalized and those well what's special about the eigenvalues other matrices could be Q lambda Q transpose but symmetric matrices are something additional about lambda so they're all real so eigen values are real and eigen vectors are orthonormal can be chosen orthonormal can be chosen I just have to say okay good good good oh now maybe I'll use that as an example of matrix multiplication so let me just do that here simple matrix multiplications but it makes the point so Q lambda Q transpose okay well what was my point about matrix multiplication let me be it really involved two matrices here I unfortunately have three so I'm gonna have to squeeze lambda in with one of the cubes too to see it nicely as two matrices so they just do that yeah there now I've made two matrices that was easy okay now what's the rule and in the first notes this was a and this was B and when you multiply two matrices the rule is this is columns of Q lambda times rows of Q transpose I'm multiplying columns by rows and so it's a column vector times a row vector and that gives us a matrix so each and it's a special matrix what so this is a column this is a row and when I multiply n by 1 times 1 by n I get an N by n matrix and it's pretty special and what is the special fact about I'm sort of recalling from last time what's special about a column times a row it's rank is special it's rank is 1 it all its column space well the only column around is this one so all columns are multiples of this guy all rows are multiples of this guy as we could see from an example so they just do an example 1 2 times 3 4 to take a random example so that would give us 3 4 6 8 I'm sure enough the columns are multiples of 1 2 the rows are multiples of 3 4 and the rank is 1 okay so those are the building blocks now I want to build something so here we go so this is a sum of rank some of Rank 1 sum of column times row so I take column 1 times Row 1 that's my first thing in the sum so column 1 of that so you see I had to sneak the lambda end to have just just two factors so what's column 1 of Q lambda oh that's a good question so column 1 of Q is Q 1 the first eigenvector but now I I'm multiplying by this diagonal matrix do you see in your mind what's the first column of Q lambda just think about that a second so here's we can steal any little corner for for for a matrix ok so here's Q 1 and the rest of the columns and then here's lambda 1 and that's Q lambda so I'm putting those together and I'm asking what's the first column of the answer and you see how that works it's Q 1 lambda 1 exactly that lambda 1 will multiply Q 1 these other lambdas multiplied later columns so the first column is lambda 1 times Q 1 right so that's it's the first guy lambda 1 Q 1 and then the first row of this will be Q 1 transpose that's the first guy in our sum of N Things and let me put the next one in the last 1 lambda 2 Q 2 Q 2 transpose and lambda n qn QN transpose that's really a nice way to write to break up the product Q lambda Q transpose this is called the spectral theorem so that's the symmetric that's s that's s there is that so that's the we've broken up s into rank one pieces that's that's like a constant theme and these write one pieces are quite special because they're symmetric Q 1 Q 1 transpose will be symmetric and oh so can we so so let's just I follow the rule for multiplying matrices but maybe I could just check that it's the right thing that it came out right so what do I mean by checking I guess I'll just check about s times Q 1 so look at if this thing times the first eigenvector and what do I get ok so you'll like this so there's I've split up s into a sum of Rank 1 pieces and that splitting is you see it all over it's really showing you what the pieces of the symmetric matrix are and now I'm just gonna check that that's a correct correct formula for s so I'll multiply it by Q 1 and I'm hoping to get the right thing and what do I actually get if I multiply this whole business times Q 1 I get lambda 1 Q 1 Q 1 transpose that's the first guy times by Q 1 plus right I'm multiplying this by Q 1 and this first term gave me that and what does the next term give me put put me out of my misery here it's uh I'm looking for this thing to simplify like mad okay so what's the second term when I'm supply this guy by q1 what do I get zero that's right that's what we want and when I multiply the last guy by q1 I get 0 because the Q's are orthogonal so this is all I get and then so I don't need this plus anymore that's it and then what can I do to improve that or is that a little somewhat repetitive formula for the answer what do I want to do finally I want to remember that the Q's are normalized their unit vectors so what does that tell me here to one principal q1 transpose times Q 1 this is just what it sits so that's what normalized means that the length squared is the length the length of the vector squared and it's 1 so I can cancel that term and I'm getting the right answer that's all I that's all this was about I was just checking and wanted to see how it would fall out and it falls right out that this formula is the correct matrix s because it's got the right eigenvectors qs and it's got the right eigenvalues lambda so it's got to be the right matrix yes is that ok that's a like a first example to see how this splitting into rank ones gives you back what you expect easily enough it gives you the information you expect okay so that's the symmetric eigen value picture for symmetric matrices and we'll see it again it's it's well all five of these are big are important I don't know if you know this but it's going to be a foundational factorization for this course and for all of data science do you know it's named so what does it mean first of all just just a comment on this and then we'll save it for a couple of weeks so this U is actually an orthogonal matrix and so is V so it has it has two orthogonal matrices so that's why people called them U and V rather than q1 and q2 which was like too much just get subscripts so orthogonal times diagonal times orthogonal let me say orthogonal diagonal or sonogram and 1806 would as of now reach this topic because it's jumped up in importance and it's called singular value decomposition well those are long words or everybody calls it the SVD the singular value decomposition the point is it's it's the it works for every matrix rectangular matrices there's no issue of does it have enough eigenvectors or not that's an issue here well it's an issue here not every matrix has got enough eigenvectors to make that work every matrix that one works because instead of one set of eigenvectors it's got two matrices two different sets of singular vectors oh we'll we'll see that that's that's important okay so that's a that's really a quick overview of fundamental factorizations and I'd like to say just another word about elimination a equal Lu and then we'll leave it leave it alone so elimination yeah do you remember that that first the beginning of linear algebra when you're solving ax equal B you do these row operations can I just what I want to say is all those row operations that you that you do are perfectly expressed by L times U and so that that's a key point in 1806 but I have a different way to look at it so that's what I wanted to show you I have a I want to show you a sum of rank one's a row times column it fits in today so so I just like to see why does a matrix invertible this is a square matrix now and invertible and it factors if all goes well with elimination and the pivots are nonzero it factors into lower triangular times upper triangular so that's a key step that that is that MATLAB would do with lvo event would produce those two factors now I want to do them in a column times row way which I just realized late was there was a neat way to do it so can I take a matrix and do elimination how big a matrix shall I take two by two you say you three by three somebody not convinced totally by two by two let me do it two by two and then if you want if you really want a three by three yellow I'll do it okay here's two by two two four three seven how's that okay so what is yeah so let's remember what elimination does it subtracts a multiple of that row from that one to get two to three and the multiple is two so it knocks out the four two threes or six so it leaves a 1 and we're oh yeah thanks for allowing me to do two by two I've already done it now I've reached you so here's a and here's you the upper triangular guy with the pivots on the diagonal good and then the question is express that step in matrix language and the right answer is L times u so the right answer is that this that this a is so I'll just erase that letter U is L times u so what is L L is the lower triangular guy and it has there the number that you used here and what was that I subtracted two of that row from this so I wanted to there so that would be I would call that a multiplier I multiplied Row one by two subtracted to get that zero and for a two by two example I was finished okay now I want to see this so there's L times U what happened I would like to see how L times u comes out of this row a column times row so let me start let me think again what so the really the point of elimination what was why did we do this in the first place because here we had two coupled equations they were a couple together we couldn't solve them instantly that step of elimination juice me two down here in this corner one equation that I've eliminated the first unknown X from the second equation so the second equation is zero X plus 1 y equal right hand side and I solve it immediately okay so how did I get to that one by one problem with these guys removed well yeah just right can I write here the my parallel way to think of it in it two by two is pretty small I admit okay so I start with two three four seven I want to split it in two I want to get the first row and column in one piece something goes there and the other piece is something there okay that's what elimination has done it's taken the original matrix its splitted these are both ranked one so let's just first of all you could tell me what goes in that blank space in the first rank one matrix so what can I say this in words the first stage of elimination pulls off from a so a is some big matrix it pulls off from a it takes account of the first column and row so it it it writes a as here we go as a first column say column 1 Row 1 plus plus the easy part the easy part will be a matrix with all zeros there Oh zeroes there and here I have a 2 can I call it a - this is my way now to think about what elimination is really doing it's starting with an N by n matrix it's pulling off a rank one matrix which has gets that column and that row correct and it gets whatever it has in here and then the rest of what's in there is a - you see that we've done that here the first step got the first row and column correct and if it's rank one what number goes there six six and then this is the rest this is what we have one size smaller to work on and it looks like it was seven six has been used so it's a one that's really I want to think of this rank one matrix as the first column of L times the first row of U and then this guy is the second column of L times the second row of U I'm okay I haven't presented this proof in the class before and for two-by-two it's looking like overkill to me I mean like why you don't have to do all that deep thinking to get to get pieces but my idea is that it gives the breakdown and this of course is by our column times row rule that's Lu so we're we're starting with a and we're breaking it up into Lu where Lu the first piece of Lu is the first column times row and then the next pieces are the rest of the matrix and though was get broken down the next stage of elimination would break if I had a 3x3 this stage peeled off the first column in row then the next stage would peel off the second the new second column in row and the third stage would have the third column in rows just last pivot does this make any sense to you you could email me and say it's not that great but I think it's to see that the final result of elimination is L times U is it's there's a little magic in in seeing what you're doing and I think this is a way to see what you're doing that you're you're peeling off a first part to leave a second part like that then the second part you would peel off the second column times the second row may be divided by the pivot to make it correct and that would that would put something in the rest of the box and then a three would be the rest of that box okay I'm stopping here I'm glad you let me do 2x2 since I see that 3x3 would it would have ruined the day yeah okay a question or let me pause for a minute so I've talked about these factorizations this one we won't see again this one we will see big-time and this one we will and this one we will yeah yeah 2 3 & 5 are the ones that we're really gonna see a lot of questions or thoughts or okay I guess I want to tell you now to complete today's moving forward in this subject the fundamental theorem of linear algebra the fundamental theorem of linear algebra okay ready for that or you may have seen it already because it's like the highlight of of this subject of the basic ideas in this subject right and then maybe I can after I'd tell you that theorem people around the world send me homework problems to do now you would think any sensible professor would never do those problems like he would say it's your problem but I get carried away and I solve them sometimes so one came from India last week and it involved the fundamental theorem of linear algebra whoever teaching it there really was on on the ball and well I'll tell you that problem after after the fundamental theorem okay fundamental theorem it's about four subspaces so I invented the name four fundamental subspaces so can i list the four subspaces fundamental basis well we know one of them already the column space so for a matrix we're given a matrix a that's M by end of rank R that's our that's our normal starting point so what are those four subspaces and how are they related and what's their dimension and what what those a key fact okay we already know the column space column space of the matrix and actually we already know the row space of a matrix and we have the notation for that column space of a transpose and what is the dimension so that was the key point in the first lecture anybody who missed the first lecture should go back to the notes of one point one for the for the thinking that goes into the dimension equals what which of those three numbers do I want to is the dimension of the column space are or and what can I say about our right away compared to M&M yeah less or equal or could I couldn't have more independent columns than I have columns so I've got n columns so our our of them are independent so our is somewhere less or equal hopefully equal to n what about the dimension of the row space how many independent rows as the matrix got our thank you that's that's the great fact with a new proof less time in section 1.1 that those have the same dimension same dimension which is you know it's you think okay you look at a simple example it's true but you know if you if you're given a matrix that's fifty by hundred really the fact that those hundred columns have the same number of independent ones as those 50 rows that's that's like great okay now the other spaces are the null space of the matrix in of a and just to make everything naturally symmetric a null space of a transpose those are the last two those are the four fundamental subspaces which you've seen and they're even on the cover of the linear algebra textbook okay so what's the null space the set of solutions to ax equals zero right no space is all solutions to ax equals e so the null space has vector these vectors in the XS the null space isn't taken from the matrix the row of the row space in the column space those those numbers are sitting in the matrix the null space and the null space of a transpose are solutions to the word null is reflecting the fact that that's a zero and that's what makes it a space now can you just let me just ask you to think again what's implied when I say when I use the word space a space of vectors I can add yeah so so you I can do the most important operations of linear algebra in that space I can add two vectors here let me just add them so here I'll have a vector X and let me say another another one a vector Y then I do addition I follow the rules I see that this can be written as ax plus y is zero plus zero so what have I learned I've learned that if X is in the null space and Y is in the null space then X plus y so the null space is as you said closed meaning I don't go outside it if X is in it and Y is in it then the sum is in it and similarly from ax equals zero I get to a times C x equals zero it's just multiplied by C by a number C so those two facts that that means I can do linear algebra I can multiply by numbers and I can add in other words I can take linear combinations that's what you do with vectors and the point is if i do it if i take combinations of two null space guys I'm still in the null space okay so that's the point of the null space and well now so now part of the fundamental theorem is to figure out how many independent vectors are in the null space how many solutions independent solutions does that system of equations have so that'll be the dimension and I have to ask you what it is let me draw a picture while you're thinking about those spaces it's fantastic to have these beautiful clean boards okay so here's my picture of the row space rows that's the column space of a transpose and here's my picture of the null space of a and that's the solutions to ax equals zero and why have I put these two together and these two together so the other pair will be the column space C of a and null space of a transpose so there are the four spaces where their relationship is the fundamental theorem of linear algebra so first of all what sorry so I have an M by n matrix so that tells me that my rows a typical row has n components right I look at an M by n matrix let's do it two by three matrix so if I look at the row space this is M and this is n so so I see three the rows have length three and of course they multiply the X's which also has length 3 X 1 X 2 X 3 that's why these are together because they're both in n-dimensional space then why are these together because the columns are in two dimensional space for this example and null space of a transpose would be just two components like y 1 y 2 to give zeros so do you see that this is are these guys are in RN so that's the first like get things straight 2 spaces in RN 2 spaces in RM now then what am I going to ask about these spaces I guess I already started asking and didn't wait for an answer their dimension so this has dimension R and what's the dimension how many this is really such a key fact if I have M equations ax equals zero and if R of those equations are independent how many solutions so the dimension of the space is going to tell me how many solutions to X into two M equations but really only are genuine independent equations in in this system ax equals zero how many so can I ask the question again and I want the answer in terms of M and n are so I have I really have our equations if I look at a x equals 0 it looks like M separate equations but but M minus R of those are just copies or combinations of others so there are independent equations so what how many of I got and that's what I'm gonna write in here yeah so yeah so X has X has n components and there are real you know active equations that they have to satisfy and that leaves n minus R that's the key point that's the key point that there are n components of X and unknowns in unknowns and there are are constraints independent constraints so the those n you get if I want to satisfy those constraints that knocks out dimension R and leaves and minus R so that's the dimension here and the beauty of the account is that those two numbers add up to n everybody's accounted for every vector has a piece in the row space and a piece in the null space and that's there was two pieces give you back the vector do you see that that's just nice that the numbers come out right and of course they come out right here too you could say just transpose a matrix and write it right the same thing again what's the dimension of the column space equals the column space of the matrix has dimension R right and the row this this guy is left out of some linear algebra books as if it like it doesn't belong but and they clear that you know without it everything is of only 3/4 ders done we have to have this guy and it's dimension is Emma - yeah it's yeah that count is just for a transpose what this count was for a yeah so we've got those dimensions are n minus R R and M - yeah you you know you'll have known this but we need to see it once again in 2018 before we start using it now is that the fundamental theorem is that all all - it is no there's another piece to the fundamental theorem which is sort of you could say the the geometry here I have a subspace here I have a subspace of this big n-dimensional space so I visualize those subspaces as some kind of a plane an R dimensional plane and an N minus R dimensional plane and I want to see how are those two planes connected how are those two claims connected and let me get a piece of the blank piece of the board to remember the final step right so we've got dimensions R and n - R and then over here R and M minus R okay this is for the rows this is for the null space so this has the rows in it this has the null the solutions to ax equals zero what is the beautiful geometry that how do you visualize those two spaces how do you visualize let me take an example but a B 1 2 4 2 4 8 sorry about that you see it's a hooked up example so this is 2 by 3 so there n is 3 what's in the null space of this matrix can I can you see a vector that solves a x equals 0 and in fact how many will there be what's yeah what's our for this matrix just tell me all the good stuff for that example M is 2 and is 3 and R is 1 1 everybody sees one for the rank the rows are dependent it's there's only one independent row the columns are dependent there's only like every column is a multiple of one - it's a rank one matrix ok what about it so it's row space has dimension 1 and it's null space has dimension 2 2 so cuz n minus R will be 2 so I'm looking for a couple of vectors that both give 0 I believe they're I think I've only got one independent row there so I should be able to find two different vectors that are that solve ax equals 0 so what's a solution to ax equals 0 0 91 0 minus 2 1 yeah that works and what's an independent solution for 0 I don't throw me off for 0 and - one yeah that looks good that looks good and then the claim is that every solution would be a combination of those two and that this is how many there are and now it's the geometry I'm completing so we have like two minutes left in this lecture you just have to tell me how what's the relation between the these guys in the row space and that guy in the null space what's the relation between the rows of a and the solutions to ax equals zero between if you see it if you saw that vector in that vector well a times X is zero now so what does that tell us well what do we see for the for the relation between 1 2 4 and 0 - 2 1 they are orthogonal terrific yes orthogonal I test by the dot product 0 - 4 4 add to 0 yes so so the and that's a completely general fact when I look at ax equals 0 it's telling me that X is orthogonal to the rows did you see that you just put it in again here if I look at ax a has a bunch of rows X has one column and I get zeros that's the point of the null space and that equation is just say that Row 1 is orthogonal because that's the dot product of Row 1 with X so here's Row 1 Row 2 over 3 and Row 4 with X the rows with X and I get zeros so the point is then these two spaces are at 90 degree angles that's a really a neat picture of the four subspaces and these two are for the same reason at 90 degree angles off in M dimensional space so this is this is the fundamental theorem of linear algebra to see that the dimensions come out right and the geometry comes out right yeah and then now next time following the notes and I have a few more copies of the of the hand that one hand out will we'll move on quickly next week to eigenvalues and positive definite matrices good this is really linear algebra moving on 