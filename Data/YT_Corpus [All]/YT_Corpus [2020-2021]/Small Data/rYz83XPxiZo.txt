 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu so this is a big day mathematically speaking because we come to this key idea which is a little bit like eigenvalues well a lot like eigenvalues but different because the matrix a now is more usually rectangular so for a rectangular matrix the whole idea of eigenvalues is shot because if I multiply a times a vector X in n dimensions out will come something in M dimensions and it's not going to equal lambda X so ax equal lambda X is not even possible if a is rectangular and even if a is square what are the problems just thinking for a minute about eigenvalues the the case I wrote up here is the great case where I have a symmetric matrix and then it's got a full set of eigenvalues and eigenvectors and they're orthogonal all good but for a general square matrix either the eigenvectors are complex eigenvalues are complex or the eigen vectors are not orthogonal so so we we can't stay with eigenvalues forever that's what I'm saying and this is the right thing to do a is know so what are these pieces so these are the left and these are the right singular vectors so this is the the new word is singular and in between go the not the eigenvalues but singular values so that so we've gotten the whole point now you got pick up on this there are two sets of singular vectors not one for eigenvectors we just had one set the queues now we have for rectangular matrix we've got one set of left eigenvectors in in m dimensions and we've got another set of right eigenvectors in n dimensions and numbers in between are not icon values but singular values so these guys are let me write what that looks like this is again a diagonal matrix say my two two sigma or let's say so it's again a diagonal matrix in the middle but the numbers on the diagonal are all positive or 0 and they're called singular values so it's just a different world okay so really the first step i have to do the math step is to show that any matrix has can be factored into u times sigma times V transpose so that's that that's the parallel to the spectral theorem that any symmetric matrix could be factored that way so you're good for that part we'll have to we just have to do it to see what are you and Sigma and V what are those vectors and those singular values okay let's go so the key is that a transpose a is a great matrix so that's the key that's the key to the math is a transpose a so one of the properties of a transpose a this is a is rectangular again so maybe m-by-n a transpose is so this was em by n this was n by M so we get a result that's n by n and what else can you tell me about a transpose a that's a big deal and it's square and well yeah you can tell me more now because we talked about something a topic that's there's a little more than symmetric last time it the matrix a transpose a will be positive definite its eigenvalues are greater or equal zero and that will mean that we can take their square roots and that's what we will do so a transpose a will have a factorization it's symmetric it'll have a like a Q lambda Q transpose but I'm gonna call it V lambda no yeah a lambda I'll still call it lambda V transpose so these V's what do we know about the eigenvectors so these fees or eigenvectors of this guy square symmetric positive-definite matrix so we're in good shape and what do we know about the eigenvalues of of a transpose a they are all positive so the eigenvalues are well or equal to 0 and these guys are orthogonal and these guys are greater equal there and okay so that's good that's one of our that's a will depend a lot on that but also you got to recognize that a a transpose is a different different guy a a transpose so so what what's the shape of a a transpose how big is that now I've got where do I have M by n times n by M so this will be what size M by M different shape but with the same eigenvalues the same eigenvalue so it's going to have some other eigenvectors you of course I'm going to call them you because I gonna go in over there they'll be the same well they're saying yeah I mean no I shouldn't I have to say when I say the same I can't quite literally mean the very same because this has got n eigenvalues and this has got em eigenvalues but the the the missing guys the ones that are in one of them not on the other depending on the sizes are zeros so really the heart of the thing that nonzero eigenvalues are the same okay now well actually I've pretty much revealed what the SVD is going to use it's going to use the use from here and the V's from here but that's that's the story you've got to see that story okay so fresh start on the singular value decomposition what are we looking for well as a factorization so we're looking for we want a we want vectors V so that when I multiply by V so if it was an eigen vector it would be a V equal lambda V but now for a it's rectangular it's not you hasn't got Argan factors so AV is Sigma the new I singular value times u that's the first guy and the second guy and the are scag I'll stop at are the rank yeah is that what I bought hey let me just see AV is Sigma you yeah that's good okay so this is what takes the place of ax equal lambda X a times one set of singular vectors gives me a number times the other set of singular vectors and why did I stop at are the rank because after that the Sigma's are 0 so after that I could have some more guys but they'll be in the null space 0 on down to of VN so these are the important ones so that's what I'm looking for can I sell let me say it now in words I'm looking for a bunch of orthogonal vectors V so that when I multiply them by a I get a bunch of orthogonal vectors u that is not so clearly possible but it it it is possible it does happen I'm looking for one set of orthogonal vectors V in the input space you could say so that the a V's in the output space are also orthogonal in our picture of you know our picture of the fundamental the the big picture of linear algebra we have these in this space and then stuff in the null space and we have views over here in the column space and some stuff in the null space over there and the idea is that I have orthogonal V's here and when I multiply by a so multiplied by a then I get orthogonal use over here orthogonal to orthogonal that's what that's what makes the V's and they use special right okay that's that's the property and then when we write down well let me write down what that would mean what so I've just drawn a picture to go with this those equations that picture just goes with these equations and let me just write down what it means it means in matrix so I've written it oh yeah I've written it here in vectors one at a time but of course you know I'm gonna put those vectors into the columns of a matrix so a times v1 up to let's say V R will equal oh yeah it equals Sigma times use so this is what I'm after is u1 up to you are multiplied by Sigma one belong to Sigma ha what I'm doing now is just to say I'm converting these individual singular vectors each V going into au to putting them all together into a matrix and of course what I've written here is a V equals u Sigma a V equals u Sigma that's what that amounts to well then I'm gonna put a V transpose on this side and I'm gonna get to a equals u Sigma V transpose multiplying both sides there by V transpose u I'm kind of writing the same thing in different forms matrix form vector at a time form and now we have to find them now now I've used up boards saying what we're after but now we got to get there so what are the V's and what are the use well the cool idea is to think of a transpose a so you're with me what we're for and now think about a transpose a so I if this is what I'm hoping for what will a transpose a turn out to be so big moment that's going to reveal what the v's are so if I form a transpose a so a transpose so I've got a transpose this guy so a transpose is V Sigma transpose u transpose right and then comes a which is this u Sigma V transpose so why did I do that why is it that a transpose a is the cool thing to look at to make the problem simpler well what what make what becomes simpler in that line just written u transpose u is the identity because I'm looking for orthogonal in fact orthonormal use so that's the identity so this is V Sigma transpose Sigma V transpose put processes around that because that's a diagonal matrix okay what does that tell me what does that tell all of us a transpose a has this form now we've seen that form before we know that this is a symmetric matrix symmetric and even positive definite so what are the V's the V's are the eigen vectors of a transpose a here where this is the Q lambda Q transpose for that symmetric matrix so we know the v's are the eigenvectors the the eigen vectors of a transpose a and I guess we're also going to get there the singular value so the Sigma transpose Sigma which will be the Sigma Squared's are the eigenvalues of a transpose a good sort of by looking for the correct thing you Sigma V transpose and then just using the u transpose u evil identity we cut it back to something we we perfectly recognize a transpose a has that form so now we know what the V's are and if I do it the other way which what's the other way instead of a transpose a the other way is and if I write all that down the a is the u Sigma V transpose and the a transpose is the V Sigma transpose u transpose and again this stuff goes away and leaves me with you Sigma Sigma Sigma transpose u transpose so I know what to use are - they're the eigenvectors of a a transfer isn't that a beautiful symmetry you just a transpose a and a a transpose they're two different guys now so they have each has its own eigenvectors and we use both it's just right and i just have to take the final step and and we've established the SVD so the final step is to remember what I'm going for here a times a V is supposed to be Sigma times a you see what I'm what I have to deal with now is I haven't quite finished it's it's just perfect as far as it goes but it hasn't gone to the end yet because we could have double eigenvalues and triple eigenvalues and all those horrible possibilities and if i have triple eigenvalues or double eigenvalues then what's the deal with eigenvectors if i have double eigenvalue suppose a matrix has a type say a symmetric matrix has a double eigenvalue let me just take an example so symmetric matrix like say 1 1 5 make it why not what's the deal with eigenvectors for that matrix 1 1 5 so 5 has got an eigenvector you can see what it is 0 0 1 what about the eigenvectors that go with lambda equal 1 for that matrix what's up what I what would be eigenvectors for lambda equal 1 unfortunately there's a whole plane of them any vector of the full of the form XY 0 any any any vector in the XY plane would produce XY 0 travel plane of eigenvectors and i've got a pick two that are orthogonal which i can do and then they have to be in the in the SVD that was two orthogonal guys have to go to two orthogonal guys in other words is a little bit of like detail here little getting into this exactly what is well actually let me let me tell you the steps ok so I use this to conclude that that the v's the singular vector should be ionized I concluded those guys from this step now I'm not gonna use this step so much of course it's in the back of my mind but I'm not using it I I'm gonna get the use from here so you one is a V 1 over Sigma 1 the you are is a V R over Sigma R do you see what I'm doing here I'm picking in a possible plane of things the one I want they use I want so I've chosen the V's I've chosen the Sigma's they were they're fixed for a transpose a the eigenvectors RV's the things this eigen values are Sigma Squared's okay and now then this is the U I want are you with me so I had to I had to give I want to get these use correct and if I have a whole plane of possibilities I got to pick the right one okay and now finally I have to show that it's the right one so what is left to show I would show that these use are eigenvectors of a a transpose and i should show that they're orthogonal right that's the key right i i would like to show that these are orthogonal then that's what goes in this picture the v's I've got orthogonal guys because I they're they're the eigenvectors of a symmetric matrix pick a more thorough but now I'm multiplying by a so I'm getting the U which is AV over Sigma for the basis vectors and I have to show they're orthogonal so this is like the final moment does everything come together right if I picked the V's as the eigen vectors of a transpose and then I take these for the use are they orthogonal so I would like to think that we can check that fact and it that it will come out okay could you just help me through this one I'll never ask for anything again just just just get the SVD for okay so I would like to show that a that you one that's so let me put up what I'm doing I'm trying to show that u 1 transpose u 2 is zero there are soluble okay so u 1 is a V 1 over Sigma 1 that's transpose that's u 1 and u 2 is a V 2 over Sigma 2 and I want to get 0 the whole conversation has ended right is ending right here why is that change 0 the V's are orthogonal we know the visa orthogonal there are saga normal eigen vectors of a transpose a to be repeat that the V's are orthogonal eigen vectors of a transpose a which I know they're there there we can find them then I chose the use to be this and I want to get the answer 0 are you ready to do it we want to we want to compute that and get 0 so what do I get let's we just have to do it so I can see that the denominator is that so is it V 1 transpose a transpose right times AV 2 and I'm hoping to get zero do I get zero here you hope so yeah v1 is orthogonal v2 but I've got a transpose a stick stuck in the middle there so what do I what happens here how do I look at that v2 is an eigenvector of a transpose a terrific so so this is V 1 transpose and this is the matrix times V 2 so that's Sigma 2 transpose V 2 isn't it it's the eigenvector with eigenvalue Sigma 2 squared times V 2 yep divided by Sigma 1 Sigma 2 so the A's are out of there now and do I so I've just cuts these numbers Sigma 2 squared so that would be Sigma 2 over Sigma 1 is I've accounted for these numbers here times V 1 transpose V 2 and now what's up there orthonormal we got it that's 0 that is 0 there yeah so so not only are the V's orthogonal to each other but because they're RI ghen vectors of a transpose a when I do this I discover that the a V's are orthogonal to each other over in the in the column space so orthogonal V's in the row space orthogonal AV s over in the column space that was discovered late much long after eigenvectors and it's a interesting history and it just comes out right and then it was discovered but not much used for Oh a hundred years probably and then people saw that it was exactly the right thing and data matrices became important which are large rectangular matrices and we have not oh I better say a word just a word here about actually computing the v's and the Sigma's and they use so how would you actually find them you what I most want to say is you would not go this a transpose a root why why is it like it is that a big mistake if you have a matrix a say five thousand by ten thousand why is it a mistake to actually use a transpose a in the computation we used it big heavily in the proof and we could find another proof that wouldn't use it so much but why would I not do why would I not multiply these two together it's very big very expensive it adds in a whole lot of roundoff you have a matrix that's now it's vulnerability to roundoff errors is squared that's called its condition number gets squared and you just don't go there so the actual computational methods are quite different and we'll talk about those but the the a transpose a because it's symmetric positive-definite made the proof so nice that's that's what you've seen the nicest proof I'd say of the and now I should think about the geometry so so what does a equal a well you sigmat maybe I take another board but it's it will fill it but it's like when you sing my V transpose I so it's got three factors there and I would like then each factor is kind of a special matrix U and V are orthogonal matrix so I think of those as rotations Sigma is a diagonal matrix I think of it as stretching so now I'm just going to draw the picture so here's unit vectors and the first thing so this if I'm multiplied by X this is the first thing that happens so that rotates here's here's X s then V transpose X is that that's still a circle length didn't change for those when I multiply by an orthogonal matrix but the vectors turned it's a rotation could be a reflection but let's keep it as a rotation now what does Sigma do so I have this unit circle these I'm in 2d so I'm drawing a picture of the vectors these are the unit vectors in 2d X Y they got turned by the orthogonal matrix what does Sigma do to that picture it stretches because Sigma multiplies by Sigma 1 in the first component Sigma 2 in the second so it stretches these guys and say let's suppose this is number one and this is number two this is number one and this is number two so Sigma one our convention is Sigma one we always take Sigma 1 greater equals Sigma 2 greater equal whatever greater equal Sigma rank and that's and that they're all positive and the rest are zero so Sigma one will be bigger than Sigma two so I'm expecting this ellipse it'll change circle goes to an ellipse when you stretch the I didn't get it quite perfect but not bad so this was this would be Sigma 2 V 2 V Sigma 1 V 1 and this would be Sigma 2 V 2 and we now have an ellipse so we started with X's in a circle we rotated we stretched and now the final step is take these guys and multiply them by you so this was the Sigma V transpose X and now I'm ready for the U part which comes last because it's at the left and what happens from what's the picture now what does you do to the ellipse it rotates it it's another orthogonal matrix it rotates it somewhere maybe there and now we see the use u2 and u1 do ah maybe maybe they're there maybe I let me let me think about that basically that's not that's right so so this SVD is telling us something quite remarkable that every linear transformation every matrix multiplication factors into a rotation times a stretch times a different rotation but possibly different actually when would the u be the same as the V here's a good question when is U the same as V when are the two singular vectors just the same because a would have to be square and we want this to be the same as Q lambda Q transpose if they're the same so the use would be the same as the V's when the matrix is symmetric and actually we need it to be positive definite why is that because our convention is these guys are greater equal zero these guys if it's going to be the same then so for a positive definite symmetric matrix the the S the S that we started with is the same as the you as the a on the next line yeah the Q is the U the Q transpose as the V transpose the lambda is the Sigma so those are the good matrices and they're the ones that you can't improve basically they're so good you can you can't make a positive definite symmetric matrix better than it is well maybe diagonalize it or something but okay now I think of like one question here that it helps to helps me anyway to keep this figure straight how I want to count the parameters in this factorization so I'm two by two I'm two by two so a has four numbers ABCD then I guess I feel that four numbers should appear on the right-hand side somehow the U and the Sigma and the V transpose should use up a total of four numbers so we have a beautiful town ting match between the left side that's got four numbers a b c d and the right side that's got number is buried in there somewhere so can we dig them out how many numbers in Sigma that's pretty clear to Sigma 1 and Sigma 2 the two eigenvalues how many numbers in this rotation so I if I had a different color chalk I would put two for the number of things I counted four by Sigma how many parameters does a two by two rotation require 1 and what's a what's a good word for that one or that one parameter it's like I have R cos theta sine theta minus sine theta cos theta there's a number fada it's it's the angle that rotates so that's one guy to tell the rotation angle two guys to tell the stretchings and one more to tell the rotation from you adding up to four so so those account those match up with the four numbers ABCD that we start with of course it's a complicated relation between those four numbers and and rotations and stretches but but it's four equals four anyway and I guess if you did three by threes oh three by threes what would happened then so let me take three do you want to care for three by threes just it's sort of satisfying to get four equal four but now what do we get three by three we got how many numbers here nine hello so where are those nine numbers well how many here that's usually the easy three so what's your guests for the how many and a rotation in a 3d rotation you take a sphere and you rotate it how many how many numbers to tell you what's what to tell you what you did three we hope three yeah it's gonna be three three and three for the for the for the three-dimensional world that we live in so people who do rotations for a living understand that a rotation in 3d but how do you see this I mean it's three words and we've got it right okay yeah roll pitch and yaw yeah I guess about pilot hopefully knows about those three yeah yeah yeah which is roll when these are like forward and back oh yo you're in a you stay in a plane and you okay beautiful right right and that leads us to our four four dimensions what's your guess on 4d well let's we could do the count again if it was four by four we would have sixteen numbers there and in the middle we always have a easy time with that that would be four so we got 12 left to share out so six somehow six six angles in four dimensions well we leave it there yeah yeah yeah okay okay so there's the SVD but without an example examples you know I would have to compute a transpose a and find it so the text will do that does it for a particular matrix Oh yeah the text does it for a for a matrix three four zero five that came out pretty well uh a few facts we could learn though so the if I multiply all the eigenvalues together then I for a matrix a what do I get I get the determinant what if I multiply the singular values together well again I get the determinant you can see it right away from the big formula take determinant take determinants well assuming the matrix a is square so it's got a determinant then I take determinants the determinant of this product I can take the separate determinants that has determinant equal to one an orthogonal matrix the the the determinant is 1 and similarly here so so the product of the Sigma's is also the determinant yeah yeah so the product of the Sigma's is also the determinate the product of the Sigma's here will be 15 but you'll find that Sigma 1 is smaller than lambda 1 so here are the eigen values lambda 1 lesser equal lambda 2 say but but the singular values are outside them yeah but they still multiply Sigma 1 thing times Sigma 2 will still be 15 and that's the same as lambda 1 times lambda 2 yep but overall computing the examples of the of the SVD take more time because well yeah you just compute a transpose a and you've got the V's and you're on your way and you have to take the square roots of the argument so that's the SVD as a piece of pure math but of course what we'll do next time starting right away is use SVD and let me tell you even today the most yeah yeah most important pieces of the SVD so what do I mean by pieces of the SVD I've got one more blackboard still the right on so here we go so let me write out a is you the use times the Sigma's Sigma's 1 to R times the B's V transpose V 1 transpose down to V R transpose so those are across yeah actually what I've written here so there you could say there's a large big economies there's a there's a smaller size SVD that has the real stuff that really counts and then there's a larger SVD that has a whole lot of zeroes so so this would be the smaller one M by R or this would be R by R and these would all be positive and this would be R by n so that's the that's only using the are non zeros all these guys are greater than zero then the other one we could fill out to get square orthogonal matrix the Sigma's and square V's V 1 transpose to be and transpose so what are the shapes now this shape is M by M it's a proper orthogonal matrix this one also n by n so this guy has to be this is the Sigma now so it has to be what sighs M by n that's the remaining space so it starts with the Sigma's and then it's all zeros accounting for null space stuff yeah so you should really see that these two are possible that that all these zeros when you multiply out just give nothing so that really the only thing that non zero is in these bits but there's a complete one so what are these extra use they're in the null space of a transpose a transpose or a transpose a yeah so two sizes the large size and the small size but they're then the things that count are all in there okay so I was going to do one more thing and I'm let me see what what it was there so this is section 1.8 of the notes and you'll see examples there and you'll see a second approach to the finding leaves and B's and Sigma's I can tell you what that is but maybe with just two just to do something nice at the end let me tell you about another factorization of a that was famous that's famous in engineering and it's famous in in geometry so this is any a as a u Sigma V transpose we've got that now the the the other one that I'm thinking of I'll tell you its name it's called the pole decomposition of a matrix and all I want you to see is that it's it's virtually here so a polar means what's polar and for complex number what's the polar form of a complex number a real guy so the real guy R will translate into a symmetric guy and the e to the I theta will translate into so what does he say what kind of a matrix reminds you of e to the I theta or sorry size one so orthogonal so that's a very using very kind of nice every matrix factors into symmetric matrix times an orthogonal matrix and I of course describe these as the most important classes of matrices and here we're saying every matrix is a s times aq and I'm also saying that I can get that quickly out of out of the SVD so I'm just want to do it so I want to find an S and find a Q out of this so to get an S so let me just start it you Sigma but now I'm looking for an S so what shall I take put in now I put a better put in if I've got you Sigma something and I want it to be symmetric I shouldn't put in you transpose will do it but then if I put in u transpose I've got to put in you so now I've got you Sigma u transpose U is the identity now I've got to get V transpose and have I got what the polar decomposition is asking for in this line so yeah I think what do I got here where's the where's the S and this so you see I took the SVD and I just put the identity in there just shifted things a little and now where's the s that I can read off for a straight that's an S that's a symmetric matrix and where's the Q well I guess we can see where the Q has to be it's here yeah yeah so just by sticking u transpose U and looking and putting the parentheses right what I recover that decomposition of a matrix which in in mechanical engineering languages language tells me that any strain can be you know which is like stretching of a of elastic thing has a has a symmetric kind of a stretch and internal twist yeah so that's good well this was a lot of three six nine boards filled with matrices well it is 1806 five so maybe that's all right but the idea is to use them on a matrix of data and I'll just tell you the key fact the key fact if I have a big matrix of data Hey and if I want to pull out of that matrix the important part so that's what data science has to be doing out of a big matrix some of those some part of it is noise some part of it is signal I'm looking for the most important part of the signal here so I'm looking for the most important part of the matrix in a way the biggest numbers but but that and of course I'm look at individual numbers so what's the biggest part of the matrix what's the what are the principal components now we're really getting in if this is a if this you know it could be data and we want to do statistics so we want to see what's what has high variance what has low variance we'll we'll do these connections with statistics but what's the important part of the matrix well that means look at the you Sigma V transpose here yeah let me look at it so what's the one most important part of that of that matrix one the right one it's a rank one P so when I say a part I'm of course it's going to be a matrix part but so the simple matrix building block is like a rank one matrix something something transpose and what should I pull out of that as being the most important rank one matrix that's in that product so I'll erase it one point eight while you think what do I do to pick out the big deal the the thing that the data is telling me first well these are north of normal I can no one is bigger than another one these are orthonormal no one is bigger than another one but here I look here which is the most important number Sigma 1 Sigma 1 so the part I pick out is this biggest number times its Rho times its column so it's u 1 Sigma 1 V 1 transpose is the the top principal part of the matrix a it's the leading part of the matrix a it's the rank one the biggest rank one part of the matrix is there so computing those three guys is the first step to understanding the data yeah so that's what's coming next is and I guess tomorrow since they moved Monday my teach declared Tuesday to be Monday there they didn't change Wednesday so I'll see you tomorrow for the the principal components 