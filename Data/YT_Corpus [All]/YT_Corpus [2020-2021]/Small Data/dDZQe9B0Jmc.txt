 hi everyone and welcome to the MyDataModels webinar my name is Paul Baqué I'm a digital sales manager for my MyDataModels here is my email address where you can contact me after the webinar if you have any further questions if you wish to have like a personalized demonstration if you want to exchange regarding your needs for small data and today's subject topic of the webinar will be how to extract value from small data with TADA so first of all what is MyDataModel so we like to define ourselves as the only small data automated predictive modeling software publisher for domain experts that's a bit of a long definition that I'm going to explain further during the during the webinar first through a presentation of my data models then through demonstration of Terra which is our product and then through a small session of Q & A I think you have access to the to a Q&A window through the zoom interface that allows you to to ask questions and we will answer them in by the end of the webinar so my data models we our company software publisher with publish data that is a software for domain experts to build predictive models around small data the first question that we have to ask is what is small data for us for a small data is data that connects people with timely meaningful insights and what is important to see is that data today is at the heart of the talk a lot of people said it's the oil of the 21st century that is vital to all to everybody and to all of the organizations but most of the time when you talk about data when you were talking about data you you people are actually talking about big data meaning that the big data it's this huge data legs data warehouse is with a lot of information a lot of volume as we are talking about small data - so - as data small data is data that is organized and packaged to be accessible understandable and actionable for everyday tasks and most of the time it's numerical data that comes from machines that will come from measurements made by professionals and that will be in form of numbers or in form of values such as good bad true/false so late week etc and the most important part for us is that small data are collected locally by a single individual or a group of professionals within their departments within the departments of an organization so it means that small data is used by professional by domain experts and that it's data that they use on a daily basis basically the most common example of small data is examples that you all know very well because you use it every day it is a famous Excel spreadsheet so know that we've defined a bit very small data we can we can see the different aspects of small data within an organization can be within a company can be within a public organization can be within all types of organization so on one hand you've got the the Big Data projects that are usually very time consuming very big as the name says and there are a very few within a company utilizer from one to five in the same time but in the meantime you have a lot of small data projects that are at the level of the department's it can be for example for marketing department it can be the analysis of the data of email campaign for a sales department it can be a CRM extract that will help salespeople determine where they should focus for R&D it can be the results from the latest experiment that give them numbers and data regarding whatever experiments are conducting or for production departments it can be the extracts and the data coming from sensors from a production line for example so we see that within this organization within the same organization we can have a lot of different small data project and all of them so they have data they have a lot of those data but they have something that is missing and that is the capacity to build predictive model and to use machine learning to on their small data and that's where we are we intervene with our products that is called so data octet I don't know what is the exact English accent because as you can hear I'm French but tada has been designed for domain experts the idea of theta is really to have an easy-to-use tools for domain experts who have no knowledge at all in code or in machine learning so that they can build easily some predictive models using their own small data being let's say sales CRM extracts being campaign email campaign results being whatever they want to predict we want to help them build their predictive models out of their small data and and we want to help them doing it easily and quickly and how do we do it is that is that we have an approach that is a bit different from the traditional machine learning and artificial intelligence approach on traditional machine learning artificial intelligence approach first you have the data that is as it's big usually it's terabytes even petabytes of data so that's why you need to have a first phase that is and phase of exploratory data analysis where a data scientist will use data visualize visualization tools such as tableau for instance well they will go through the data examine the variables and features examine the missing the tags I mind values that are not current and do something about it and it can be secreted creating some new features rating getting rid of some data it's already a very long process is that is doing the data preparation time once I've done this they go to the model building where they will try different algorithms on the data set to see which one fits the best then it will go to the phase of model selection according to different criterias and finally the deployment and all of these takes at least six months because the volume of data is big you need to have a lot of computing power besides so that you can run your models and you can compute all of your models and you need to have different departments involved it can be top management it can be data science teams it can be business teams can be domain experts so you need to have a lot of people involved in this project a Terra project on the contrary is quite simple because first of all and that's a very important part is that we are we go from small data it can be an Excel spreadsheet so it can be 100 200 300 500 rose and a number of columns and we skip all of this data PDA data preparation model building and model selection steps ought to be fair we don't skip them but we do it do them much quicker for the exploratory data analysis phase right since we are on short on small volumes of data we don't need to explore all of the data we is much quicker for all of the data preparation with the algorithm we use and really use one algorithm I will come back to it a bit later but this algorithm doesn't need to to have clean data it can work on between a dirty data on data that has not been prepared that has not been claimed so it works well with this data for the model building part as I mentioned we only have one model so it means that you don't have to select between different types of model to do it and the process is very quick as you will see during the demonstration and so far the model selection in the end you only have one model and so it goes quickly and then to do it's easy to deploy it too and so it's one click data in model out model no coding or machinery skills are required meaning that you as the domain expert can use data and it has a high return on investment as you don't need to use a lot of human resources within the company of the organization but you also don't need to use a lot of cloud or data center resources to render models and to to build the models and to run them so you save a lot of money as of today we have good number of companies that are working with among them are Telus and Gmail - which are not partisan same group that Ellis is sorry Telus is a French company that works in the defense and space area we also work a lot with research institutes in France in some that is medical research public medical research Inara is the working aloha with IM air a is the agronomic research institute the public agronomic research institute in here is the computing science public institute in france and see an RS is the French public Research Institute so we are well-positioned with all of them and they use that on weekly basis and they are very happy with it we also work with Sanofi which is a French farmer company and we also work with with institutions such as hospitals such as the one in Nice and with private research and development Institute Institute sorry such as team only family chiz a quite famous in the South of France for insert sham development for industry main so that's it for the presentation of my data models I'm going to move to the demonstration so as I mentioned earlier and I can see that some people arrived at I'm not there at the beginning if you have any questions you can you can ask them through the small Q&A tool that is provided with zoom and we will have a short Q&A session by the end of the webinar okay so just quick drink of water I'm sorry if I sound a bit a bit nosey but time I have a cold and it might have a small influence on my voice but okay so let's move to the the webinar now but not to the webinar to demonstration so we are here on the main on the home page of tada on our website as you can see if you wish to create an account no problem you get you can create free accounts on our website if you haven't done it yet but as you can see we will have a short service interruption for maintenance on Monday from 11 a.m. CET to 1 p.m. CET more or less so just be be careful about that and so to get back to the demo so we are on the main on page of tada I will go on full screen and you see that you have the possibility to create a new project here and also to access to video tutorials where you can few videos how to create a new project on blah blah then you have the use cases that are available on the website and also on the app and but I will go back to them just after and we have the Help Center with different topics and also FAQ so if you need help build your predictive models do when you are trying tada you can use thank you in the Help Center and you can also contact me or also support the address is given in the in the app so going back to the use cases part so I click on View all and a new page opens and we see that it's the same page that we have on the website I don't know if you've been there or not but yep so it's basically the same with different use cases of the use of data and for example if we took if we take machine learning for IOT applications we see that we have use case description that is once again available on the website but once you are logged into the app you have a few more stuff first of all you've got on the right the data set that has been used for model creation and the data set that is used for prediction and you also have case studies that is an operator's manual for you to reduce to reduce the use case with this data by yourself so it's a operators manual - so I'm going to download the to you two data sets and I'm going to I'm going to use them for further demonstration to create a new project so create a new project I click on here I so I'm on the project creation page a project is the color is linked to a data set and that I said as of no are can only be uploaded in CSV so eyebrows I'm on the good page IOT for model creation and I can name my project so project name I'm fine with IOT for model creation but if I want wanted to do something else I can get I want but just so you know and also something very important is that on the right you have a small help that is available on every page that helps you guides you into doing to understanding what is going on on the main on the app what you can do on the page you also have the possibility to see some videos to access to the to the Help Center and you also can access directly some best practices so it can be helpful if you want to move to go further with data so once we've uploaded the CSV we'll click on next and we see that we have a program with the filled separators that are in the CSV they are semicolons and by default tada cease consider that it should be commands so you can change that by clicking semicolon and you see that the data is well alignment now and we can check out the data set quickly so we see that we have the different variables I tend ecology around 1 to 2 23 and to expand you quickly what this data set is about so the purpose of this data set is to predict cardiac arrhythmia it has been the data is coming from a maker of connected tissues which have sensors there are different sensors on the on the t-shirt and they are this sensors they are sending information regarding different type of alarms such as such as variable the heart rate the heart rate the pressure the temperatures the tension stuff like that and the the purpose is to build a machine learning model at atom model that will according to the data that is - the data coming from the sensors that is able to say there's a risk of predict if there will be I'm a rhythmic cardiac arrhythmia or not using the data that's produced by the sensors and once the the model is validated and cost not as good the idea is to put the model directly in the sensors so that the detection is made close to the data and you can detect it right away with no identity so we're going to create the project and once we've done so we can review the data set a bit and we can see that we will see that we have two type of variables the first one are binary ones so false true 0 1 2 binary 2 options and the second option is that we have some numerical variables such as alarm 18 where you have different numerical values that go for that go from 0 to 4000 but here we see that most of them go from 0 to 1500 and a few of them are what we call outliers so they are way above way different from the vast majority of them and as I mentioned earlier in traditional machine learning approaches you would need to go back to the data set to get rid of these outliers sorry I'm going to still not so in the case of theta you don't have to get rid of the outliers so but when we're back on the first page we can go back to we can go back to the main page I'm going to move the camera up here now and so we have the graphs in the data we can not create a goal and the goal will be the variable that we are trying to predict thanks to Ted up so and of course the viable is part of the data set and you can select it directly from the list so in that case it will be the viable pathology so as we said before it's binary variable so it's true or false so we see that by default once you've selected the viable theta we'll choose what type of prediction will be done so we see that it's binary prediction I'm going to stop it stop here quickly to explain the differences between these three types of predictions a binary prediction is when you have to click between two values A or B so that's the case here you then you have the regression when you have to predict a numerical value so a lot of different values and you want to predict one and then you have the multi-class when you have the where you have a choice between a number of values that is superior to 2 so a b c d and g so as i mentioned when you choose a goal by default it's that selects the type of of prediction it will do for instance if we go to two alarm 18 that is a numerical value we see that it became a regression 1 but it's not a regression that we want to predict it's a binary one it's it's a pathology so we go back on pathology we see that the name of the goal is the name of the variable and since we are in binary prediction we have the possibility to say to tada what we want eat what we want data to consider as negative and what we wanted to consider as positive here we see that data consider that false is the positive and true is negative we're going to swap them because true is positive it means that the wearer of the t-shirt has a risk of cardiac arrhythmia so it means that it's it's positive in medical way so we want it to be positive and so one point once we've done that we validate then once we've validated we can create a viable set so the variable set is the set of variables that will be used to to build the model so the set of variables that tada will analyze to consider which ones are the most important to predict the goal that we've set by default tada selects all of them except the goal because the goal can't be part of the viable and you can't you can't select it but you also have the possibility to and to antique some of them if you wish to wish to we'll stay like this but when you are a bit advanced you can also see and unselect which some of them is she wants the other them not very interesting so we validate and now we are going to generate the predictive model that is what what is the main purpose of tah-dah so just click on generate model first you can give a name to the model or you can stay with model one that is by default name and then you have to set the analysis parameters I'm going to stop and explain a bit this one because they are quite important as you can see there are two of them the first one is iterations and iterations is the number of times that tada will analyze the data the different variables that are to be considered so by default the value is 100 when you are bit advanced too you can you can change that value you can reduce it you can increase it and of course the higher it is the longer it takes time for tada to generate a model and the second one is the population and this one is very important not as much as the value but as what it means behind remember earlier I said I would stop and explain a bit about your algorithm and this time is now because basically we use what is called a genetic algorithm this algorithm is based on the Darwin theory of evolution of survival of the fittest and what happens during model generation is that they are a division of 3 parts of the division of the data set into 3 parts and there are three phases of the generation during the first phase we use a first part of the data set to generate 500 models if we put 500 600 put 500 etc so these 500 models are generated during the first phase and then during the second phase and second subset of data these 500 models will be put into competition to feed one another so that in the end of the validation phase you have only one model left and then you have the test phase where the only remaining model will be tested on the remaining subset of data and we will and it will this test will help us determine the quality of the model if it's good or not and if we can use it or not so and then again the higher the population is the longer the generation takes so that's why I'm going to reduce a bit the values just to so that it can go go a bit quicker so I'll divide everything by files so that we have 20 iteration and a population of 100 sorry for go and once we've done that we click on generate and we see that it's very quick processing 95% and it's done we have our model we see that it's a binary classification and we can click on visualize model so that we can access to the model review and the different metrics that will help us determine the quality of the of the model so you have the metrics you are the actual versus predicted and you are the confusion matrix that in binary classification is quite important because it helps you visualize the prediction and it's used after that to build your matrix so that's why I'm starting with the coefficient matrix to move to the matrix afterwards so the confusion matrix shows you in a small chart Oh what was the results of the test phase so we see that in total there were 72 values that were tested during the during the test phase and that we have the ten that Tedder predicted that ten ten ten outcomes would be positive when they were actually positive so that is we have ten true positives and we see that it predicted ten outcomes as positive while people were actually not suffering for cardiac arrhythmia so we have tell actual negatives we have ten sorry we have ten false positives ten troopas it is ten false positives for a total of twenty predicted positives on the other hand we have two people two people that were predictive negative predicted negative meaning that today predicted that they would not suffer from cardiac arrhythmia while they actually suffered from it and fifty people who predicted as not suffering from cardiac arrhythmia and we did not actually suffer from cardiac arrhythmia so a total of 52 negative prediction to these two ones are called false negatives and this fifty ones are called true negatives and in our case and especially as we are into health data to prop the the main goal the main objective of the model would have would be to have zero false negatives because it's it's very bad if you don't predict something that is about to happen in that case because it has an important impact on somebody's else so of course you want to avoid fake you want to avoid non made predictions you you can have fake predictions like fake positives but you can't tolerate to have fake native to other false false negatives so once with these that we've seen the matrix the coefficient matrix sorry we can move to the matrix and so we see that we have the three phases the training the validation and the test but for evaluation assistant purposes were only gonna check out the test ones so the first one that we want to look at is the accuracy that is the sum of the true positives and true negatives on divided by the total of of data that had been that had been reviewed during the test phase so and it's this value is between zero and one and of course the closer to one it is better it is and here we see that we have a 83.3 person that is good but not great and something that is very important is that accuracy is a good way to start assessing the quality of the model but you don't you must not stop at accuracy because it can be deceitful for instance if we have the data set that is very unbalanced with 95% of the values that are positive and 5% of the values that are negative and we see that we have an accuracy of 95% it can mean that the model is very good at detecting positive ones but very bad at detecting negative ones and we can it's not good because you want to detect both of them with the greatest accuracy possible that's why it's important to check out the confusion matrix and the other values and the other metrics the second one that you want to check out is the MCC that is analyzing the correlation between variables and go and it's it's a metric that is whose numbers are between minus 1 and 1 so once then again the closer to one it is the better it is and if we see that we are at 0.55 that is quite good because we are in the upper quarter but it's not that great but it's normal because we didn't take a lot of time and a lot of precision to to build a model we want to be quick so that's normal that were results that are not that great then you have the AUC that is an indication of the quality that is like the most advanced indicator of the quality of the model and it's it indicates the quality between and the values are between 0.5 and 1 and this row dot 5 is considered as total randomness it's a heads or tails it's thought else I think there was no best example than this and one is the perfection of the model and here you see that it's a bit above 80% a bit above 0.82 so once again it's good it's not that great you also have other metrics that you can check out the recall the specificity need the precision and the f1 score I'm not gonna detail them now but you have all of the Precision's and the indication that are in the in the read more best practices section so if you want to check them out feel free then we have the actual versus predicted matrix that is a visual way to visualize your results well that is quite intuitive well you can see the true positives to make true negatives and and so on and finally you have this a formula encode window where you can see first the variables that are being used in the model so here you see that only three out of 25 I think one used to build the model and then you've got the formula the mathematical formula that we provide because what's important to consider is that the formula we provide our mathematical formulas and that you can reuse easily into an Excel spreadsheet if you want to validate the results that that I've found for instance and since we're in a binary binary prediction we actually have two outputs one or two two formulas one per output and the highest value of each output will indicators what will be the prediction so output one or two and the difference between output 1 and output 2 will be the one that will will give us a number that is called the confidence score that is important to validate the quality of prediction into binary in binary prediction and I will show you that just in a few minutes and finally if you are a pro or premium user so basically using a paying version of data you can access the mod code of the model that is generated automatically and then you can this code can be implemented within your apps or within your sensor for instance in the case of that of that use case so of course here it's not available because we are on the free version so once know that we have our model is good we are happy with it but a model is useless if we if you don't do predictions so if you want to do predictions Intelli it's quite easy you click on predict once you've done the model you browse so that you can open your prediction file that must be on a similar template than your model creation file it's necessary for Tara to to recognize it you can give it another name which I'm not happy by default name given then you click on next here again we have the problem of the separator so we go back to click on the semicolon and then we click on predict it's being computed and we arrive on the main page when we see that the prediction is is done so we can just download it and we are going to open it as a CSV spreadsheet and so we see that we have the file the variables that were present in the original data set but we also have two new columns the first one being the prediction and the second one being the confidence scores that I mentioned earlier so you can check out the prediction and you can see in detail where it went wrong for which values and what what confidence does that I had into his predictions so that's it I don't know if there are any questions from from you to check out the Q&A we may not know questions so I you shy or maybe I was very clear that's also a possibility so all you know thank you very much for attending I'll send you this in a few days most probably on Monday and feel free to reach if you have any questions and if you wish to to know more regarding data if you want to have a talk regarding your needs for small data prediction and I'll be very happy to help thank you very much have a nice day and goodbye 