 good evening everyone my name is Rob rish I'm delighted to welcome you here to Stanford University for an evening of conversation with Yuval Harare Fei Fei Lee and Nick Thompson I'm a professor of political science here and the faculty director of the Stanford center for ethics and society which is a co-sponsor of tonight's event along with the Stanford Institute for human centered artificial intelligence and the Stanford Humanities Center our topic tonight is a big one we're going to be thinking together about the promises and perils of artificial intelligence the technology quickly reshaping our economic social and political worlds for better or for worse the questions raised by the emergence of AI are by now familiar at least to many people here in Silicon Valley but I think it's fair to say that their importance is only growing what will the future of work look like when millions of jobs can be automated are we doomed or perhaps blessed to live in a world where algorithms make decisions instead of humans and these are smaller questions in the big scheme of things what might you ask you're the large ones well here are three what will become of the human species if machine intelligence approaches or exceeds that of an ordinary human being as a technology that current rely currently relies on massive centralized pools of data does AI favor authoritarian centralized governments over more decentralized democratic governance and are we at the start now of an AI arms race and what will happen if powerful systems of AI especially when deployed for purposes like facial recognition are in the hands of authoritarian rulers these challenges only scratch the surface when it comes to fully wrestling with the implications of AI and as the technology continues to improve and its use cases continue to multiply I want to mention the format of the evening event first given the vast areas of expertise that you've all in faith they have when you ask questions via slide Oh those questions should pertain or be limited to the topics under discussion tonight so this web interface that we're using slide o allows people to upvote and downvote questions so you can see them now if you have an internet communication device if you don't have one you can take one of these postcards which hopefully you got outside and on the back you can fill in a question you might have about the evening event and collected at the end and the Stanford Humanities Center will try to foster some type of conversation on the basis of those questions a couple housekeeping things if you didn't purchase one already you vols books are available for sale outside in the lobby after the event a reminder to please turn your cell phone ringer off and we will have 90 minutes for our moderated conversation here and we'll end sharp after 90 minutes now I'm going to leave the stage in just a minute and allow a really amazing undergraduate student here at Stanford to introduce our guests her name is Anna Sophia lesap let me just tell you a bit about her she's a junior here at Stanford majoring in economics with a minor in computer science and outside the classroom anna sophia is a journalist whose work has been featured in The Globe and Mail al-jazeera the Mercury News The Seattle Times and this campuses paper of record the Stanford daily she's currently the executive editor of the daily and her daily magazine article from earlier in the year called cs+ ethics examined the history of computer science and ethics education at Stanford and it won the student prize for best journalism of 2018 she continues to publish probing examinations of the ethical challenges faced by technologists here and elsewhere so ladies and gentlemen I invite you to remember this name for you'll be reading about her or reading her articles or likely both please welcome Stanford junior anna sophia lesson thank you very much for the introduction Rob well it's my great honor now to introduce our three guests tonight you've all know a Harare Faye Faye Lee and Nicholas Thompson professor you've all know a Harare is a historian futurist philosopher and professor at Hebrew University the world also knows him for authoring some of the most ambitious and influential books of our decade professor her Ari's internationally best-selling books which have sold millions of copies worldwide have covered a dizzying array of subject matter from narrative izing the entire history of the human race and sapiens to predicting the future awaiting humanity and even coining a new faith called Dadaism in houma dais professor Harare has become a beloved figure in Silicon Valley whose readings are assigned in Stanford's classrooms and whose name is whispered through the hallways of the comparative literature and computer science departments alike his most recent book is 21 lessons for the 21st century which focuses on the technological social political and ecological challenges of the present moment in this work Harare cautions that as technological breakthroughs continue to accelerate we will have less and less time to reflect upon the meaning and consequences of the changes they bring this urgency is what charges professor Fei Fei leaves work every day in her role as the co-director of Stanford's human-centered AI Institute this institute is one of the first to insist that AI is not merely the domain of technologists but a fundamental fundamentally interdisciplinary and ultimately human issue her fascination with the fundamental questions of human intelligence is what piqued her interest in neuroscience as she eventually became one of the world's greatest experts in the fields of computer vision machine learning and cognitive and computational neuroscience she's published over a hundred scientific articles in leading journals and has had research supported by the National Science Foundation Microsoft and the sloan foundation from 2013 to 2018 professor Fei Fei Li served as the director of Stanford's AI lab and between January 2017 and September 2018 professor Fei Fei Lee served as vice president at Google and chief scientist of AI and machine learning at Google cloud Nicholas Thompson is the editor-in-chief of Wired magazine a position he's held since January 2017 under mr. Thompson's leadership the topic of artificial intelligence has come to hold a special place at the magazine not only his wired assigned more feature stories on AI than on any other subject but it is the only specific topic with a full-time reporter assigned to it it's no wonder then that professors Harare and Lee are no strangers to its pages mr. Thompson has led discussions with the world's leaders in technology a knight and AI including Mark Zuckerberg on Facebook and privacy French President Emmanuel macron on Francis AI strategy and Ray Kurzweil on the ethics and limits of AI mr. Thompson is a Stanford University graduate who earned his BA double majoring in earth systems and political science and impressively even completed a third degree in economics of course I would be remiss if I did not mention that mr. Thompson cut his journalistic teeth in the opinions section of the Stanford daily so Nik that makes both of us like all our guests today I'm at once fascinated and worried by the challenges that artificial intelligence poses for our society one of my goals at Stanford has been to write about and document the challenge of educating a generation of students whose lives and workplaces will eventually be transformed by AI most recently I published an article called complacent Valley with the Stanford daily in it I critiqued our propensity to become overly comfortable with the technological and financial achievements that Silicon Valley has already reached lest we become complacent and lose our ambition and momentum to tackle the greater challenges the world has in store answering the fundamental questions of what we should spend our time on how we should live our lives has become much more difficult particularly on the doorstep of the AI revolution I believe that the kind of crisis of agency that author JD Vance wrote of and hillbilly elegy for example is not confined to Appalachia or the de-industrialized Midwest but is emerging even at elite institutions like Stanford so conversations like hours this evening hosting speakers that aim to recenter the individual at the heart of AI will show us how to take responsibility in a moment when most decisions can seemingly be made for us by algorithms there are no narratives to guide us through a future with a I know ancient myths or stories that we may rely on to tell us what to do at a time when Humanity is facing its greatest challenge yet somehow we cannot be more at a loss for ideas or direction it's this momentous crossroads in human history that pulls me towards journalism and writing in the future and it's why I'm so eager to hear our three guests discuss exactly such a future tonight so please give me a very please get join me in giving them a very warm welcome this evening [Applause] Wow thank you so much annasophia thank you rob thank you Stanford for inviting us all here I'm having a flashback to the last time I was on a stage at Stanford which is playing guitar at the coho and I didn't have either you ball or face a with me so they're about six people in the audience one of whom had our headphones on but I did meet my wife all right so a reminder housekeeping questions are going to come in and slide oh you can put them in you can vote up questions we've already got several thousand so please vote up the ones you really like if someone can program an AI that can get a really devastating question in and stump you all I will get you a free subscription to Wired I want this conversation to kind of have three parts first lay out where we are then talk about some of the choices we have to make now and last talk about some advice for all of the wonderful people in the halls so those are the three general areas of feeding questions as we go we may have a specific period for questions at the end but let's get cracking you've all yeah so the last time we talked you said many many brilliant things but one that stuck out it was a line where you said we are not just in a technological crisis we are in a philosophical crisis so explain what you meant explain how it ties to AI and let's get going with a note of existential angst yes I think what's happening now is that the philosophical framework of the modern world that has been established in the 17th and 18th century around ideas like human agency and individual free will are being challenged like never before not by philosophical ideas but by practical technologies and we see more and more questions which used to be you know the bread and butter of the philosophy department being moved to the engineering department and that's scary partly because unlike philosophers who are extremely patient people they can discuss something for thousands of years without reaching any agreement and there fine with that the engineers won't wait and even if the engineers are willing to wait the investors behind the engineers won't wait so it means that we don't have a lot of time and in order to encapsulate what the crisis is I know that you know engineer is especially in a place like Silicon Valley they like equations so maybe I can try and formulate an equation to explain what's happening and the equation is B times C times D equals aa which means biological knowledge multiplied by computing power multiplied by data equals the ability to hack humans and the AI revolutional crisis is not just AI it's also bio biology it's biotech when we haven't seen anything yet because the link is not complete there is a lot of hype now around AI in computers but just that bit is just half the story the other half is the abilities the biological knowledge coming from brain science and in biology and once you link that to AI what you get is the ability to hack you mañana GLE explain what it means the ability to hack humans to create an algorithm that understands me better than I understand myself and can therefore manipulate me enhance me or replace me and this is something that our philosophical baggage and all our belief in you know human agency and free will and the customer is always right and the voter knows best this just falls apart once you have this kind of ability once you have this kind of ability and it's used to manipulate or replace you not it's used to enhance you also an it's used to enhance you the question is who decides what is a good enhancement and what is a bad enhancement so our immediately our immediate fallback position is to fall back on the traditional humanist ideas that the customer is always right the customers will choose the enhancement or the voter is always right the voters will vote there will be a political decision about the enhancement or if it feels good do it we'll just follow our our heart we'll just listen to ourselves none of these works when there is a technology to hack human on a large scale you can't trust your feelings or the voters or the customers on that the easiest people to manipulate of the people who believe in free will because they think they cannot be manipulated so how do you how do you decide what to enhance if and this is a very deep ethical and philosophical question again if philosophers have been debating for thousands of years what is good what are the good qualities we need to enhance so if you can't trust the customer if you can't trust the voter if you can't trust your feelings who do you trust what what do you go by all right Fei Fei you have a PhD you have a CAS degree your professor at Stanford there's eight times B times C equal h is it is you Vols it's you've all staring the right way to look at where we're headed wow what a beginning thank you you're welcome well one of the things I've been reading your vows book for the past couple of years and talking to you and I'm very envious of philosophers now because they can propose questions and crisis but they don't have to answer them now as an engineer and scientist I feel like we have to now solve the crisis so um honestly I think I'm very thankful I mean personally I've been reading your book for two years and I'm very thankful that you've owl among other people but have opened up this really important question for us and it's also quite a when you said the AI crisis and I was sitting there thinking this is a field I loved and felt passionate about and researched for 20 years and that was just a scientific curiosity of a young scientist entering PhD AI how did what happened that 20 years later it has become a crisis and it actually speaks of the evolution of AI that that got me where I am today and got my colleagues at Stanford where we are today with the human Center a AI is that this is a transformative technology it's a nascent technology it's still a budding science compared to physics chemistry biology but with the power of data computing and the kind of diverse impact AI is making it is like you said is touching human lives and business in broad and deep ways and responding to that kind of questions in crisis that that's facing humanity I think one of the the proposed solution or if the solution at least a try that staffer is making an effort about is can we reframe the education the research and the dialogue of AI and technology in general in a human centered way were not necessarily a fine solution today but can we involve the humanists the philosophers the historians the political scientists the economists the ethicist the legal scholars the neuroscientists the psychologists and many more other disciplines into the study and development of AI in the next chapter in the next phase the smartest people in the world glued to their chairs and I've got slide Oh for 72 minutes so let's let's give it a shot he said we have thousands of years but let me let me go a little bit further in new balls so there are lots of oh you lost opening statement there are a lot of crises about AI that people talk about they talk about AI becoming conscious and what will that mean they talk about job displacement they talk about bias ease when you've always very clearly laid out what he thinks is the most important one which is the combination of biology plus computing plus data leading to hacking he's laid out a very specific concern is that specific concern what people who were thinking about AI should be focused on so absolutely so alien technology humanity has created starting for fire is a double-edged soul so it can bring improvements to life to work to society but it can bring the perils and AI has the perils you know I wake up every day worried about the diversity inclusion issues AI we worry about fairness or the lack of fairness privacy the the the labor market so absolutely we need to we need to be concerned and because of that we need to expand the study the research and the the development of policies and the dialogue of AI beyond just that that the codes and the products into these human realms into these societal issues so I absolutely agree with you on that that this is the moment to open the dialogue to open the research in those issues even though I would just say that again part of my fear is that the dialogue I don't fear AI experts talking with philosophers I'm fine with that historians good literary critics wonderful I fear at the moment you start talking with biologists that's that's my biggest fear when you and the biologist will say we actually had a common language and we can do things together and that's when the the really scary things I think can you elaborate on the what is scary you that we talk to by Allah that's the moment when you can really hack human beings not by collecting data about our search words or our purchasing habits or where do we go about town but you can actually start peering inside and collect data directly from our hearts and from our brains ok can I be specific first of all the birth of AI is a AI scientist talking to biologists specifically neuro scientists right the birth of AI is very much inspired by what the brain does fast-forward to 60 years later today's AI is making great improvement there's a lot of data from our physiology and pathology being collected and using machine learning to help us but I feel like you're talking about something else that's part of it I mean if there wasn't a great promise in the technology that would also be no danger because nobody would go along that path I mean obviously there are enormous lis beneficial things that AI can do for us especially when it is linked with ours biology we are about to get the best health care in the world in history and the cheapest and available for billions of people of the other smartphones which today they have almost nothing and this is why it is almost impossible to resist the temptation and with all the issue of privacy if you have a big battle between privacy and health health is likely to win hands down so I fully agree with that and you know my job as a historian as a philosopher as a social critic is to point out the dangers in that because especially in Silicon Valley people are very much familiar with the advantages but they don't like to think so much about the dangers and the big danger is what happens when you can hack the brain and that can serve not just your healthcare provider that can help that can serve so many things from a crazy dictator to let's focus on that what it means to hack the brain like what right now in some ways my brain is hacked right there's an allure of this device it wants me to check it constantly like my brain has been a little bit hacked yours hasn't because you meditate two hours a day but mine has and probably most of these people have but what exactly is the future brain hacking going to be that it isn't today much more of the same but on a much larger scale I mean the point when for example more and more of your personal decisions in lives are being outsourced to an algorithm that is just so much better than you so you know you have we have two distinct dystopias that kind of mesh together you we have the dystopia of surveillance capitalism in which there is no like Big Brother dictator but more and more of your decisions are being made by an algorithm and it's not just decisions about what to eat or what to shop but decisions like where to work and where to study and whom to date and whom to marry and whom to vote for it's the same logic and I would be curious to hear if you think that there is anything in humans which is by definition unhackable that we can't reach a point when the algorithm can make that decision better than me so that's one line of dystopia which is a bit more familiar in in this part of the world and then you have the full-fledged dystopia of a totalitarian regime based on a total surveillance system something like the totalitarian regimes that we have seen in the twentieth century but augmented with biometric sensors and the ability to basically track each and every individual 24 hours a day and you know which in the days of Stalin or Hitler was absolutely impossible because it didn't have the technology but maybe might be possible in 20 years 30 years so it can choose which dystopia to discuss but they are very close the liberal democracy dystopia faith paid you one answer you've all specific question which is is there something in dystopia a liberal democracy dystopia is there something endemic to humans that cannot be hacked so will you ask me that question just two minutes ago the first word that came to my mind is love is love hackable asked in a defense ating is not the entirety of love I hope the question is which kind of love are you referring to if you are referring to this you know I don't know Greek philosophical love or the loving kindness of Buddhism that's one question which I think it's much more complicated if you are referring to the biological mammalian courtship rituals and then I think yes I mean why not why is it different from anything else that is happening in the body but humans are humans because we're there's some part of us that are beyond the mammalian courtship right so is that perhaps that's the question I mean you know in in most science fiction books and movies they give your answer when the extra-terrestrial evil robots are about to conquer planet Earth and nothing can resist them a resistance is futile at the very last moment it's just us waiting because the robots understand love last moment is one here I quite do that saves us why we do this no no I'm it was a joke but but um okay so so the two dystopian I do not have answers to the two dystopian but I want to keep saying is this is precisely why this is the moment that we need to seek for solutions this is precisely why this is the moment that we believe the new chapter of AI needs to be written by cross-pollinating efforts from historical you manage a humanist social scientists to business leaders to civil society to government's to come at the same table to have that boaty lateral and cooperative conversation I think you really bring out the urgency and the importance and the scale of this potential crisis but I think in the face of that we need to act yeah and I agree that we need cooperation that we need much closer cooperation between engineers and philosophers or engineers and historians and also from a philosophical perspective I think there is something wonderful about engineers philosophically thank you that they you really cut the I mean philosophers can talk and talk you know in cloudy in flowery metaphors and then the engineers can really focus the question like I just had a discussion the other day with an engineer from Google about this and that it said okay I know how to maximize people's time on on the website if somebody comes to me and tells me look your job is to maximize time on this application I know how to do it because I know how to measure it but if somebody comes along and tells me well you need to maximize human flourishing or you need to maximize universal love I I don't know what it means so the that's what the engineers go back to the philosophers and I think what you actually mean which you know a lot of philosophical theories collapse around that because they can't really explain what and we need this kind of collaboration inque's oh but then you've always faced a is face a right if we can't explain and we can't code love can artificial intelligence ever recreated or is it something intrinsic to humans that the machines will never emulate I don't think that machine's will feel love but you don't necessarily need to feel it in order to be able to hack it to monitor it to predict it to manipulate it please don't like to play candy crush but you think they can still this device in some future where it's infinitely more powerful minutes right now could make me fall in love with somebody in the audience mm-hmm that that goes to the question of consciousness and in mind I don't think that we have the understanding of what consciousness is to answer the question whether a non-organic consciousness is possible or is not possible I think we just don't know but again the the bar for hacking humans is much lower the machines don't need to have consciousness of their own in order to predict our choices and manipulate our choices they just need to if you accept that something like love is in the end a biological process in the body if you think that AI can provide us with wonderful health care by being able to monitor and predict something like the flu or something like cancer what's the essential difference between flu and love in the sense of be is this biological and this is something else which is so separated from the biological reality of the body then that even if you have a machine is capable of monitoring predicting flu it still lacks something essential in order to do this with love so I want to make two comments and this is where my engineering you know personality is speaking we were making two very important assumptions in this in this part of the conversation one is that AI is so omnipotent that it's achieved to a state that it's beyond predict predicting anything physical it's got into the consciousness level getting to the the the even the ultimate the love level of capability and I do want to make sure that we recognize that we're very very very far from that this technology is still very nascent part of the concern I have about today's AI is that super hyping of its capability so I'm not saying that that's not a valid question but I think that part of this conversation is beautiful that assumption that this technology has become that powerful and there's I don't even know how many decades we are from that second related assumption I feel we are our conversation is being based on is that we're talking about the world or state of the world that owning that powerful AI exists or that small group of people who have produced the powerful AI and is intended to hack human are existing but in fact our human society is so complex there's so meaning of us right I mean humanity in its history have faced so many technology if we left it in the hands of a bad player alone without any regulation multinational collaboration rules laws moral codes that technology could have maybe not hack human but destroy human or hurt human in massive ways it has happened but by enlarge our society in a you know historical view is moving to a more civilized and we controlled state so I think it's important to look at that greater society in and bringing other players and and and people into this dialogue so we don't talk like there is only this omnipotent AI you know deciding it's gonna hack everything to the end and and that brings your brings to your topic that in addition of hacking human at that level that you're talking about there are some very immediate concerns already diversity privacy labor legal changes you know international geopolitics and I think it's it's critical to tackle those Lao that's uh I love talking to AI researchers because five years ago all the AI researchers like it's much more powerful than you think and now they're like it's not as powerful as you think alright so let me ask but it's because five years ago you have no idea what yeah I'm not saying now you're extrapolating too much I didn't say was wrong I said was the thing let's I want to go into what you just said but before you do that I want to take one question here from from the audience because once we move into the second section won't be able answer it so the question is for you you've all how do we this is from Mara and Lucini how can we avoid the formation of AI power digital dictatorships so how do we avoid dystopian number two let's answer that and then let's go face a into what we can do right now not what we can do in the future mmm-hmm um the key issue is how to regulate the ownership of data because we won't stop research in biology and we won't stop research in computer science and AI so from this the three components of biological knowledge computing power and data I think data is the easiest and it's also very difficult but still the easiest kind of to regulate or to protect puts place some protections there and there are efforts now being made and they are not just political efforts but you know also philosophical life efforts to really conceptualize what does it mean to own data or to regulate the ownership of data because we have a fairly good understanding what it means to own land we had thousands of you of experience with that we have a very poor understanding of what it what it actually means to own data and how to regulate it but this is the very important front that we need to to focus on in order to prevent the worst dystopian outcomes and I agree that AI is not nearly as powerful as some people imagined but this is why I think we need to place the bow low for to reach a critical threshold we don't need the AI to know us perfectly which will never happen we just need the AI to know us better than we know ourselves which is not so difficult because most people don't know themselves very well and often make huge mistakes in in critical decisions so whether it's finance or career or all of life to have this shift in authority from humans to algorithm they can still be terrible but as long as they are a bit less terrible than us the authority will shift to them you've in your book you tell a very illuminating story about your own self and your own coming to terms with who you are and how you could be manipulated we you will you tell that story here about coming to terms with your sexuality and the story you told about coca-cola and your book because I think that will make it clear what you mean here very well yes so I I said I only realized that I was gay when I was 21 and I look back at the time when I was I don't know 15 17 any new church should have been so obvious oh and it's not like a stranger like I I'm with myself 24 hours a day and I just don't notice any of like the screaming signs that saying there you were gay and I don't know how but the fact is I missed it now I come in AI even a very stupid AI today will not miss it so imagine this is not let you know like a science fiction scenario of a century from now this can happen today that you can write all kinds of algorithms that you know they are not perfect but they are still better say than the average teenager and what does it mean to live in a world in which you learn about something so important about yourself from an algorithm what does it mean what what happens if the algorithm doesn't share the information with you but it shares the information with advertisers always government's so if you want to and I think we should go down from the cloudy heights of you know the extreme scenarios to the practicalities of day-to-day life this is a good example because this is already happening yeah all right well let's take the elevator down to more conceptual level of this particular shopping mall that we're shopping in today and Chaffey let's talk about what we can do today as we think about the risks of AI the benefits of AI and tell us you know sort of your punch list of what you think the most important things we should be thinking about with AI are oh boy there are so many things we could do and and I cannot agree more with you Val that this is such an important topic again I'm gonna try to speak about all the efforts that's been made at Stover because I think this is a good representation of what we believe there are so many efforts we can do so in human centered AI in which this is the the overall theme we believe that the next chapter of AI should be is human centered we believe in three major principles one principle is to invest in the next generation of AI technology that is more that reflects more of the kind of human intelligence we would like I was just thinking about your comment about a eyes dependence on data and how that the policy and governance of data should emerge in order to regulate and and govern the the AI impact while technology is we should be developing technology that can explain AI in in technical field we call it explainable AI or AI interpretability studies we should be are focusing on technology that have the more nuanced understanding of human intelligence we should be investing in in the in the development of of less data dependent AI technology that were taking two considerations of intuition knowledge creativity in other forms of human intelligence so that kind of human intelligence inspired AI is one of our principles the second principle is to again welcome in the kind of multidisciplinary study of AI cross-pollinating with economics with ethics with law with philosophy with with history cognitive science and so on because there is so much more we need to understand in terms of a is social human anthropological ethical impact and we cannot do we cannot possibly do this long as technologists we some of us shouldn't even be doing this it's the sss philosophers should participate and work with us on these issues so that's the second principle and the third principle and within this we work with policymakers we convene the kind of dialogues of multilateral stakeholders then the third last but not the least I think Nick you said that at the very beginning of this conversation that we need to promote that the human enhancing and collaborative and augmentative aspect of this technology you have a point even there it can become manipulative but we need to start with that that sense of alertness understanding but still promote that kind of benevolent applications in and design of this technology at least these are the three principles the staffers human-centered AI Institute is based on and I just feel very proud within a short few months of the birth of this Institute there are more than 200 faculty involved on this campus in this kind of research dialog you know study education and that number is still growing Wow let's of those three principles let's some start digging into them so let's go to number one explain ability because this is a really interesting debate and artificial intelligence so there are some practitioners who say you should have algorithms that can explain what they did and the choices they made it sounds eminently sensible how do you do that I like I make all kinds of decisions that I can't entirely explain like why did I hire this person not that person I can tell a story about why I did it but I don't know for sure like we don't know ourselves well enough to always be able to truthfully and fully explain what we did how can we expect a computer using AI to do that and if we demand that here in the West then there are other parts of the world that don't demand that who may be able to move faster so why don't we start why don't I ask you the first part of that question you've all the second part of that question so the first part is can we actually get explain ability if it's super hard even within ourselves well it's pretty hard for me to multiply two digits but you know computers can do that yeah so the fact that something is hard for humans doesn't mean we shouldn't try to get the machines to do especially you know after all these algorithms are based on very simple mathematical logic granted we're dealing with neural networks these days of millions of nodes and billions of connections so so explain ability is actually tough it's an ongoing research but I think I think this is such a fertile ground and it's so critical when it comes to healthcare decisions financial decisions legal decisions there's there's so many scenarios where this technology can be potentially positively useful but with that kind of explanation I'm pretty confident with a lot of smart minds out there this is a cracker bowl thing and I'm professors on it right not all of them doing AI homes on top of that I think you have a point that if we have technology that can explain the decision making process of algorithms it makes it harder for it to manipulate and cheat right it it's a technical solution not the entirety of the solution that will contribute to the clarification of of what this technology is doing but because the presumably the AI makes decision in a radically different way than humans then even if the AI explains its logic the fear is it will make absolutely no sense to most humans most humans when they are asked to explain a decision they tell a story in a narrative form which may or may not reflect what is actually happening within within them in many cases it doesn't reflect it's just a made-up Russian rationalization and not the real thing now an AI could be much better than a human in telling me like I applied for a bank to the bank for a loan and the bank says no and I ask why not and the bank says okay we'll ask our AI and the AI gives this extremely long statistical analysis based not on one or two salient feature of of my life but on 2517 different data points which it took into account and gave different weights and why did you give this this way and why did you give oh there is another book about that and most of the data points our complete would seem to a human completely irrelevant you applied for a loan on Monday and not on Wednesday and the AI discovered that for whatever reason it's after the weekend whatever people who apply for loans on a Monday are 0.075 percent less likely to repay the loan so it goes into into the equation and I get this book of the real explanation finally I get a real explanation it's not like sitting with a human banker that just bullshit's me and so what are you rooting for are you saying is good in this case in many cases yes I mean I think in many cases I mean it's it's it's it's two sides of the coin I think that in many ways the AI in this scenario will be an improvement over the human banker because for example you can really know what the decision is based on presumably but it's based on something that I as a human being just cannot grasp I just don't I know how to deal with simple narrative stories I didn't give you a loan because you're gay that's not good or because you didn't repay any of your previous loans okay I can understand that but I don't my mind doesn't know what to do with the real explanation that the AI will give which is just this crazy statistical thing which okay so there are two layers to your comment one is how do you trust and be able to comprehend a eyes explanation second is actually AI be used to make humans more trust able or be more trust more than a human's the fur on the first point I agree with you if AI gives you two thousand dimensions of potential features with probability it's now human understandable but the entire history of science in human civilization is to be able to communicate the result of science in better and better ways right like I just had my annual physical and a whole bunch of numbers came to my cell phone and well first of all my doctors can the expert can help me to explain these numbers now even Wikipedia can help me to explain some of these numbers but but there the technological improvements of explaining these will improve we it's it's our failure as AI technologists if we just throw two hundred or two thousand dimensions of probability numbers but this is I mean this is the explanation and I think that the point you raise is very important but III see differently I think science is getting worse and worse in explaining its theories and findings to general public which is the reason for things like doubting climate change and so forth and it's it's not really even the fault of the scientists because the science is just getting more and more complicated and reality is extremely complicated and the human mind wasn't adapted to understanding the dynamics of climate change or the real reasons for refusing to give somebody alone but that's the point when you have and let's put a sign to the whole question of manipulation and how can I trust let's assume the AI is benign and let's assume it makes that there are no hidden biases everything is okay but still I can't understand that's variations of Pai people like Nick the storyteller says to explain what I'm saying you're right it's very complex but there are people whose my job to computer like next week but I'm happy to have your confidence with me yeah but that's the job of the society collectively to explain the complex science I'm not saying we're doing a great job at all but I'm saying there is hope if we try but my fear is that we just really can't do it because the the human mind is not built for dealing with these kinds of explanations and technologies and it's true for I mean it's true for the individual customer who goes to the bank and and the Rif and the bank refused to give to give them a loan and it can ever even be on the level I mean how many people today on earth understand the financial system how many presidents and prime ministers understand the financial system in this country at zero what does it mean to live in a society where the people who are supposed to be running the business then agree it's not the fault of a particular politician it's just the financial system has become so complicated and I don't think that economies are trying on purpose to hide something for general public it's just extremely complicated you had the some of the wisest people in the world going to the finance industry and creating these enormously complex models and tools which objectively you just can't explain it to most people unless first of all they they study economics and mathematics for ten years or whatever so I think this is a real crisis well and and and this is again this is part of the philosophical quark crisis we started with and the the the the undermining of human agency is that part of what's happening that we have these extremely intelligent tools that are able to make perhaps better decisions about our health care about our financial system but we can't understand what they are doing and why they are doing it and this undermines our autonomy and our authority and we don't know as a society how to deal with that well ideally a face Institute will help that but before we leave this topic I want to move to a very closely related question attending is one of the most interesting which is the question of bias in algorithms which is something you've spoken eloquently about and let's state with the financial system so you can imagine a loan used by a bank to determine whether somebody should get a loan and you can imagine training it on historical data and historical data is racist and we don't want that so let's figure out how to make sure the data isn't racist and that it gives loans to people regardless of race we probably all every this room agrees that that is a good outcome but let's say that analyzing the historical data suggests that women are more likely to repay their loans than men do we strip that out or do we allow that to stay in if you allow it to stay in you get a slightly more efficient financial system if you strip it out you have a little more quality before between men and women how do you make decisions about what biases you want to strip and which ones are okay to keep yeah let's accept question Nick I mean I'm not gonna have the answers personally but I think you touched on the really important question it's first of all a machine learning system bias is a real thing you know like you said it starts with data it probably starts with the very moment we're collecting data and the type of data were collecting all the way through the whole pipeline and then all the way to the application but biases come in very complex ways at Stanford we have machine learning scientists studying the technical solutions of bias like you know D biasing data and and normalizing certain decision-making but we also have humanists debating about what is biased what is fairness when is bi is good when is bi is bad so I think you just opened up a perfect topic for research and and a debate and conversation in this in this topic and I also want to point out that you've already used a very closely related example machine learning algorithm has a potential to actually expose bias right it you know like one of my favorite study was a paper a couple of years ago analyzing Hollywood movies and using machine learning face recognition algorithm which is a very controversial technology these days to recognize Hollywood systematically gives more screen time to male actors their female actors that's no human being can sit there and count all the frames of faces and gender bias and this is a perfect example of you machine learning to expose path so so in general there's a rich set of issues we should study and again bring the humanists bring the ethicists bring the legal scholars bring the gender study experts agree though standing up for humans I knew Hollywood was sexist even before that paper but yes agreed you are a smart human you've all on that question of the loans do you strip out the racist data you strip out the gender data what biases you get rid of what biases do you not I don't think the reason one size fits all I mean it's a question we need and we need this day-to-day collaboration between engineers and ethicists and psychologists and political scientists but not biologists right but not by our and increasing and increasingly also also biologists and you know and it goes back to the question what should we do so we should teach ethics to coders as part of their curriculum that the people today in the world that most need a background in ethics is the people in the computer science departments so it should be an integral part of of the curriculum and it's also in the big corporations which are designing these tools there should be embedded within the teams people with background in things like ethics like politics that they always think in terms of what biases might we inadvertently be building into our system what could be the cultural or political implications of what we're building it shouldn't be a kind of afterthought that you create this needs technical gadget it goes into the world something bad happens and then you starting in oh we didn't see this one coming what do we do now from the very beginning it should be clear that this is part of the process I do want to give a shout out to rob Reese who just in reduce this whole event he and my colleagues Mehran sahami and a few other Stanford professors have opened this course called ethics computation and sorry Rob I'm abusing the title of your course but this is exactly the kind of classes it's I think this quarter the offering has more than 300 students signed up to that fantastic the course I wish the course had existed when I was a student here let me ask an excellent question from the audience it ties into this is from Yu Jin Lee how do you reconcile the inherent trade-offs between explain ability and efficacy and accuracy of algorithms great question this question seems to be assuming if you can explain it you're less good or less accurate well you can imagine that if you require explain ability you lose some level of efficiency you're adding a little bit of complexity to the algorithm so okay first of all I don't necessarily believe in that there's no mathematical logic to this assumption second let's assume there is a possibility that an explainable algorithm suffers efficiency I think this is a societal decision we have to make you know when we put the seatbelt in our car driving that's a little bit of an efficiency loss because I have to do that seatbelt movement instead of just hopping and drive but as a society we decided we we can afford that loss of efficiency because we care more about human safety so I think AI is the same kind of technology as we make these kind of decisions going forward in our solutions in our products we have to balance human wellbeing and societal well-being with efficiency so let me you've all let me ask you the global consequences of this is something that a number of people have asked about in different ways and we've touched on but we haven't hit head-on there are two countries imaginative country a and you have country B country a says all of you AI engineers you have to make it explainable you have to take Efex classes you have to really think about the consequences what you're doing you gotta have dinner with biologists you have to think about love and you have to like read you know John Locke that's group a Group B country says just go build some stuff right these two countries at some point are gonna come in conflict and I'm gonna guess that country B's technology might be ahead of country A's is that a concern yeah that's always the concern with arms races which become a race to the bottom in the name of efficiency and domination and we are in input I mean what is extremely problematic of dangerous about the situation now is with AI is that more and more countries are waking up to the realization that this could be the technology of domination in the 21st century so you're not talking about just any economic competition between the different textile industries or even between different oil industries like one country decides to we don't care about environment at all we'll just go full for gas ahead and the other countries is much more environmentally aware the situation with AI is is potentially much worse because it could be really the technology of domination in the 21st century and those left behind could be dominated exploited conquered by those who forge ahead so nobody wants to stay behind and I think the only way to prevent this kind of catastrophic arms race to the bottom is greater global cooperation around AI now this sounds utopian because we are now moving in exactly the opposite direction of more and more rivalry and competition but this is part of AI think of our job like with the nuclear arms race to make people in different countries realize that this is an arms race that whoever wins humanity loses and it's the same with AI if AI becomes an arms race then this is extremely bad news for all the humans and you know and it's easy for say people in the u.s. to say we are the good guys in this race you should be cheering for us but this is becoming more and more difficult in a situation when the the motor of the day is America first when how can we trust the USA to be the leader in AI technology if ultimately it will serve only American interests in American economic and political domination so it's it's really I think it's most people when they think arms race in AI they they think USA versus China but there are almost 200 other countries in the world and most of them are far far behind and when they look at what is happening they are increasingly terrified and for a very good reason the historical example you've made is a little unsettling if I heard your answer correctly it's that we need global cooperation and if we don't we're gonna lead to an arms race in the actual nuclear arms race we tried for global cooperation from I don't know roughly 1945 in 1950 and then we gave up and then we said we're going full-throttle the United States and then why did the Cold War end the way it did who knows but one argument would be that the United States you know build up and it's relentless build up of nuclear weapons helped to keep the peace until the Soviet Union collapsed so if that is the parallel then what might happen here is we'll try for global cooperation in 2019 2020 2021 and then we'll be off in an arms race a is that likely and B if it is would you say well then the US it needs to really move full throttle and AI because if we better for the liberal democracies to have artificial intelligence than totalitarian states well I'm afraid it is very likely that cooperation will break down and we will find ourselves in an extreme version of an of an arms race and in a way it's it's it's worse than the nuclear arms race because with nukes at least until today countries develop them but never use them AI will be used all the time it's not something you have on the shelf for some doomsday war it will be used all the time to create potentially total surveillance regimes in extreme totalitarian systems in one way or the other and so from this perspective I think the danger is is far greater you could say that the nuclear arms race actually saved democracy in the free market and you know rock and roll and Woodstock and then the hippies they all owe a huge debt to nuclear weapons because if nuclear weapons weren't invented you needed the would have been a conventional arms race and conventional military buildup between the Soviet bloc and the American bloc and that would have meant total mobilization of society if the Soviets are having total mobilization the only way the Americans can compete is to do the same now what actually happened was that you had an extreme totalitarian mobilized society in the communist bloc but thanks to nuclear weapons you didn't have to do it in the United States or in western Germany or in France because will you relied on nukes you don't need millions of conscripts in the army and with AI it going to be just the opposite that the technology will Inc will not only be developed it will be used all the time and that's a very scary scenario so wait can I just add one thing i I don't know history like you do but I I you said AI is different from nuclear technology I do want to point out it is very different because at the same time as you're talking about these more scarier situation this technology has a wide international scientific collaboration basis that is being used to make transportation better is to improve healthcare to improve education and so it's a very interesting new time that we haven't seen before because while we have this kind of competition we also have massive international scientific community collaboration on these benevolent users and democratization of this technology I just think it's important to see the both side of there also as I said also enormous benefits to this technology and in a in a global collaborative way in especially between this among the global aspect is is more complicated because the question is what happens if there is a huge gap in abilities between some countries and most of the world would we have a rerun of the 19th century Industrial Revolution when the few industrial powers conquer and dominate and exploit the entire world both economically and politically what was to prevent that from repeating so even in terms of you know without this scary war scenario we might still find ourselves with a global exploitation regime in which the benefits most of the benefits go to a small number of countries at the expense of everybody else have you heard of archive.org archival ok so students in the audience might laugh at this but we are in a very different scientific research climate is that the kind of globalization of technology and technique happens in a way that the 19th century even 20th century never saw before any paper that is a basic science research paper in AI today that is or technical technique that is produced let's say this week at Stanford it's easily get globally distributed through this thing called archive or github repository the information is out there yes globalization of this scientific technology travels in a very different way from the 19th and 20th century I mean I don't doubt there are you know confined development of this technology maybe by regimes but we do have to recognize that this global the differences is pretty sharp now and we might need to take that into consideration that the scenario you're describing is harder I'm not say impossible but harder to happen I've just said it's not just the scientific papers yes the scientific papers out there but if I live in Yemen or in Nicaragua or in the Indonesia or in Gaza yes I can connect to the internet and download the paper what will I do is that I don't have the data I don't have the infrastructure I mean you look at where the big corporations are coming from that hold all the data of the world they are basically coming from just two places I mean even Europe is is not really in the competition there is no European Google or European Amazon or European Baidu of European Tencent and if you look beyond Europe you think about Central America you think about most of Africa the Middle East much of Southeast Asia it's yes the the the basic scientific knowledge is is out there but this is just one of the components that go to creating something that can compete with Amazon or always Tencent always the abilities of governments like the US government or like the Chinese government so I agree that the dissemination of information and basic scientific knowledge were a completely different place than in the 19th century and they ask you about that cause it's something three or four people have asked any questions which is it seems like there could be a centralizing force of artificial intelligence that it will make whoever has the data and the best compute more powerful and that it could then accentuate income inequality both within countries and within the world right you can imagine the countries you've just mentioned the United States China Europe lagging behind Canada somewhere behind way ahead of to America it could accentuate global income inequality a do you think that's likely and be how much does it worry you we have about four people who've asked a variation on that but as I said it's very very likely it's already happening and it's extremely dangerous because the economic and political consequences could be catastrophic we are talking about the potential collapse of entire economies and countries countries that depend say on cheap manual labor and they just don't have the educational capital to compete in a world of AI so what are these countries going to do I mean if say you shift back most production from say Honduras or Bangladesh to the USA into Germany because the human salaries of no longer part of the equation and it's it's cheaper to produce the shirt in California than in Honduras so what will the people there do and you can say ok but there will be many more jobs for software engineers but we are not teaching the kids in Honduras to be software engineers so maybe a few of them could somehow immigrate to the US but most of them want and what will they do and we at present we don't have the economic answers and the political answers to these questions that's fair enough I think your vault definitely has laid out some of the critical pitfalls and enough this and and that's why we need more people to be studying and thinking about this one of the things we over and over noticed even in this process of building the community of human centered AI and also talking to people both internally and externally is that there are opportunities for business around the world and governments around the world to to I think think about their data and AI strategy there are still many opportunities for for you know outside of the big players in in terms of companies and countries to really come to the realization it's an important moment for their country for their region for their business to transform into this digital age and and I think I think when you talk about these potential dangers and lack of data in in parts of the world that hasn't really caught up with this digital transformation the moment is now and we we hope to you know raise that kind of awareness and they encourage that kind of transformation yeah yeah I think it's it's very urgent I mean what we are seeing at the moment is on the one hand what you could call some kind of data colonization at the same model that we saw in the 19th century that you have the imperial hub where they have the advanced technology they grow the cotton in India or Egypt they send the raw materials to Britain they produce the shirts the high-tech industry of the 19th century in Manchester and they send the shirts back to sell them in in India and and out-compete the local producers and we in a way might beginning to see the same thing now with the data economy that they harvest the data in places also like Brazil and Indonesia but they don't process the data there the data from Brazil Brazil and Indonesia goes to California or goes to Eastern China being processed there later produced the wonderful new gadgets and technologies and sell them Bank has finished products to the to the to the provinces to the to the colonies now it's not a one-to-one it's not the same there are our differences but I think we need to keep this analogy in mind and another thing that maybe we need to keep in mind in this respect I think is the the re-emergence of stone walls that I'm kind of you know am originally I was a my speciality was medieval military history this is how I began my academic career with the Crusades and castles and knights and so and so forth and now I'm doing all these cyborgs and a I stuff but suddenly there is something that I know from back then the walls are coming back in and I try to kind of what's happening here I mean we have virtual realities we have 3G AI and and suddenly the hottest political issue is building a stone wall like the most low-tech thing you can imagine and what is the significance of a stone wall in a world of interconnectivity and and all that and it really frightens me that there is something very sinister there the combination of data is flowing around everywhere so easily but more and more countries and also my home country of Israel it's the same thing you have the you know the startup nation and then the wall and what does it mean this combination safer you want to answer that look at the next you know what let's go to the next question which is tied to that and the next question is you have the people there at Stanford who will help be building these companies who will either be furthering the process of data colonization or reversing it or who will be building you know the efforts to create a virtual wall a world based on artificial intelligence or being created or funded at least by a Stanford Graduate so you have all these students here in the room what do you want them to how do you want them to be thinking about artificial intelligence and what do you want them to learn let's let's spend the last 10 minutes of this conversation talking about what everybody here should be doing so if you're a computer science or engineering student take Rob's class if you're humanists take my class and all of you read your books are his books on your syllabus no my I teach hard core deep learning it's his book doesn't have equations I don't know if it was C plus D eh but but seriously you know what I meant to say is that Stanford students you have a great opportunity that this is a we have a proud history of bringing this technology to life Stanford was at the forefront of the birth of AI in fact are very professor john mccarthy coined the term artificial intelligence and came to Stanford in 1963 and started this nation's one of the two oldest AI labs in in this country and since then Stanford AI research has been at the forefront of every wave of AI changes and at this 20:19 were also at the forefront of starting the human centered AI revolution or the new writing of the new AI chapter and we did all this for the past 60 years for you guys for for the people who come through the door and who will graduate and become practitioners leaders and and and and part of the civil society and and that's really what the bottom line is about human centered AI needs to be written by the next generation of technologists who have taken classes like Rob's class to think about the ethical implications the human well being and it's also gonna be written by those potential future policymakers who came out of Stanford humanity studies and-and-and Business School who are versed in in the details of the technology who understand the implications of this technology and who has the capability to communicate with the technologists that is no matter how we agree and disagree that's the bottom line is that we need this kind of multilingual leaders and and and and thinkers and practitioners and that is what Stanford's human Center Institute is about evolve how do you wanna answer that question all on the individual level I think it's it's important for every individual whether in Stanford whether an engineer or not to get to know yourself better because you are now in a competition I mean it you know it's the old the old advice in the book in philosophies is know yourself we've heard it from Socrates from Confucius from Buddha get to know yourself but there is a difference which is that now you have competition in the day of Socrates or Buddha if you didn't make the effort so okay so you missed on enlightenment but still the king wasn't competing with you they didn't have the technology now you have competition you're competing against these giant corporations and governments if they get to know you better than you know yourself the game is over so you need to buy yourself some time and the first way to buy yourself some time is to get to know yourself better and then they have more ground to cover for engineers and students I would say I'll focus on engineers maybe the two things that I would like to see coming out from the laboratories and and the engineering departments is first tools that inherently work better in a decentralized system then in a centralized system I don't know how to do it but if you ever I hope that he saw that engineers can can work with I heard that blockchain is like the big promise in that area I don't know but whatever it is part of when you go when you start designing a tool part of the specification of what this tool should be like I would say this tool should work better in a decentralized system than in a centralized system that's the best defense of of democracy the second thing that I would like to see coming out I don't want to cut your film you get the second thing how do you make a tool work better in a democracy I'm not an engineer no okay but it can powder me I'm all right well they go to part two take that someone in this room figure that out because it's very important I can think about it and then I can give you a historical examples of tools that work better in in this way or or in that way but I don't know how to translate it into present-day part two because I got a few more questions astron it from okay so the other thing that I would like to see coming is an AI sidekick that serves me and not some corporation or government so to take all I mean we can't stop the progress of this kind of technology but I would like to see it serving me so yes it can hack me but it hacks me in order to protect me like my computer has an anti-virus but by brain hasn't it has a biological antivirus against the flu or whatever but not against hackers and fraud and so forth so one project work on is to create an AI sidekick which I paid for maybe a lot of money and it belongs to me and it follows me and it monitors me and what I do and my interactions but everything it learns it learns in order to protect me from manipulation by other AIS by other outside influencers so this is something that I think with with the present day technology I would like to see more effort in in the direction not to get into technical terms I think you I think you would feel comforted to know that the budding efforts in this kind of research is happening you know trustworthy AI explainable AI and security you know motivated or where AI so so I'm not saying we have the solution but a lot of technologists around the world are thinking along that line and and and trying to make that happen it's not that I want an AI that belongs to Google or to the government that I can trust I want an AI that I'm I'm its master it's is serving me it's powerful it's more powerful than my AI because otherwise my AI could manipulate your AI if it will have the inherent advantage of of knowing me very well so it might not be able to hack you but because it follows me around and it has access to everything I do and so forth it gives it an edge in the specific realm of just me so this is a kind of counterbalance to the danger that the people that would have a lot of challenges in their society who is accountable for our youth accountable for your actions or your sidekick this is the question more and more difficult question that we will have to deal with the sidekick defense all right Feifei let's go through a couple questions quickly we often talk of this is from Regan Pollock we often talk about top-down AI from the big companies how should we design personally I to help accelerate our lives careers the way I interpret that question is so much of AI is being done at the big companies if you want to have AI at a small company or personally can you do that so well first of all one solution is what you've also said what probably those things we built by Facebook so first of all I it's true there's a lot of investment and efforts putting and resource putting big companies in AI research and development but it's not that all the AI is happening there I want to say that I could be academia continue to play a huge role in ai's research and development this especially in the in a long term exploration of AI and and what is academia academia is a worldwide network of individual students and professors thinking very independently and creatively about different ideas so from that point of view it's a very grassroots kind of effort in AI research that that continues to happen and and small businesses and and an independent research institutes also have a role to play right there are a lot of publicly available datasets there we it's a global community that is very open about sharing and disseminating knowledge and technology so yes please by all means we want global participation in this all right here's my favorite question this is from anonymous unfortunately if I am in eighth grade do I still need to study [Laughter] your as a mom I will tell you yes go back to your homework alright Fei Fei what do you want your boss next book to be about I need to think about that all right well why won't you think about that you've all what area of machine learning you want they fear to pursue next the sidekick but yeah I mean just what I said that an AI can we create a kind of AI which can serve individual people and not some kind of big network I mean is that even possible or is there something about the nature of AI which inevitably will always lead back to some kind of network defect and winner-takes-all and so forth now we're gonna wrap it with this next book is gonna be a science fiction book between you and your sidekick all right one last question for you've all cuz we've got to the top border questions or this without the belief in free will what gets you up in the morning without belief in free will I don't think that falou that the question of I mean is very interesting over recent well it has been central in in Western civilization because of some kind of basically theological mistake made thousands of years ago but it's it's a relay to misunderstanding of the human condition the real question is how do you liberate yourself from suffering and one of the most important steps in that direction is to get to know yourself better and for that you need to just push aside this whole but I mean for me the the biggest problem with the belief in free will is that it makes people in curious about themselves and about what is really happening inside themselves because they basically say I know everything I don't I know why I make decisions this is my free will and they identify with whatever thought or emotion pops up in their mind because I this is my free will and this makes them very curious about what is really happening inside and what is also the deep sources of the misery and and and in their lives and so this is what makes me wake up in the morning to try and understand myself better to try and understand the human condition better and free will is it's just irrelevant for that and if we lose it your sidekick can get you up in the morning Feifei 75 minutes ago you said we were gonna reach any conclusions do you think we got somewhere well we open the dialogue between the humanist and the technologists and I want to see more of that great all right thank you so much thank you favor a Thank You Vonderhaar is wonderful to be here thank you to the audience [Applause] 