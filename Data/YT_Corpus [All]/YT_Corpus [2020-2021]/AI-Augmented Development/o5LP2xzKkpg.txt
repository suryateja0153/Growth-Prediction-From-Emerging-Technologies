 thank you all for coming thank Google for having me today we're gonna talk about convolutional neural networks with Swift and a little bit of Python very broadly we're gonna explore the problem of image recognition purpose my presentation is to take you from will say zero or about as basic as you can get in this field all the way up to the current state of the art so towards that end we're gonna do a quick review of neural networks how they work we'll look at a one-dimensional version of the in this problem which is a well understood problem and computer vision from there we'll introduce convolutions and then we'll tackle illness together using 2d approach from there we'll look at how we can introduce color and start to stack or convolutions in order to tackle larger problems and then from there we'll look how we can take the same basic approach and even more layers to build up to vgg which is our first state of the art approach from 2014 or so from then we can modify the vgg basic network to produce residual networks which are very powerful modern approaching this field and then at the end we'll look at efficient that which is very recent paper in this field and do a quick demo of running that on the edge GPU device so here we go very broadly these are like the four big categories as a computer vision I think you should be aware of I'll convert them into the international standard of cat and dog units so we have image recognition or is this a cat or dog picture object detection or where is the katniss picture image segmentation or which pixels are cat pixels and then finally instance segmentation how many sets of cat and dog pixels do we have today we're just gonna be focused on the upper left quadrant here so just cats and dogs neural networks know this field is well machine learning has historically been focused on sort of reducing problems down to the simplest dimension we'll say trying to figure out if there's just one variable that changes things and so neural networks are kind of like an outgrowth for computer science we'll say they were kind of a curiosity for the longest time the basic trick that a neural network does is they can learn how to separate high dimensional data so images we might think of them as being simple but the machine they're actually kind of complicated you have a red channel a green Channel blue channel a - width component and then you're trying to map it to some category at the end so if you could actually just imagine for each input picture you mapped it to a specific category that's literally what a neural network learns in order to do this we often end up doing a lot of math where we do a applied to B be applied to C C apply to D and so on and so forth in order to do this then we use back propagation and the chain rule from calculus everybody hates to talk about the chain rule and so somebody say hey why don't we have a computer keep track all this stuff so back out of differentiation is not really a new subject in this field it's actually from the 1970s or so what is new is compiling Auto differentiation with the compiler in order to model these neural networks at the language level so Swift isn't really magically special in and of itself this slide up here this upper right thing is sort of a slide I stole from Chris latter's keynote presentation at LLVM Congress earlier but it's basically demonstrating how all these worlds are sort of moving together Swizz real secret power is that it was the first language of LLVM which is a modern compiled that it's used almost everywhere nowadays so basically all these worlds are sort of coming together or you can write your high-level neural network code in your particular programming language it'll be converted to a intermediate language and then finally LLVM will spit out for whatever device you actually need to run it on so right now people are coding stuff for CPUs and GPUs and TP use but in a new area that's coming out is like running stuff on devices so say on your mobile phone or even as like an edge TP devices we'll see later and so the whole theory of this project then is by getting everything to sort of follow path you'll be able to target all these different runtimes so the same cloud code that you're writing can run up in the cloud or on the device in your hands the second level of this then and this is kind of the really new area is this whole ml ir so rather than having each of these languages implement their own abstract syntax for doing this neural network stuff they're trying to sort of model it at a cleaner level so all these languages will generate ml I our code and then from there we can go LLVM to your device so we here at the bottom we have some sort of the different forms of basic neural networks over here we have the perceptron so if you can imagine an imaginary line dividing all your cats and dog pictures that's literally what a perceptron is and this is from 1958 this is not as new as you may think the basic problem is like what I said that you can't actually reduce the data down to one dimension that easily so basically they have these sort of hidden layer approaches where you run thing through a set of neurons and and that's how you get to your actual result so pay attention to our deep fee for a neural network and then we can add some convolutions on top because that's what we're gonna do for our next two steps so the immanent data set is a well understood data set and computer vision it's a collection of hand-drawn digits they're all black and white so these values are from 0 to 255 there are 28 pixels by 28 pixels wide so there's 8 right here is just literally what one of the digits in the data set would look like we're not even gonna treat this as actual image data what we're gonna do is unroll it so we're literally going to take the top row and then pull off each row at a time until we have a really long vector so the second picture right here is sort of demonstrating just a four by four unrolling loop of maybe say like an imaginary one but we can imagine this same concept across the 28 by 20 pixels to produce an input vector that's 784 pixels long so next we're just going to take our input vector of 784 pixels and we're gonna run this through two fully connected layers of 512 neurons and we're going to map it to an put layer obtained categories the number zero through nine so I originally set out to write of this demo but this gentleman named Quan is out in Mountain View he's a GE out there he wrote this code so I simply took his code and modified it slightly in order to produce these results so this is what our very simple neural networks gonna look like it's nothing more than our input layer 784 to 512 512 to 512 again and in 512 210 out at the end the reason we're using these Swift native data types because that means that actually if now we can just define our differentiation function in this simple line right here and the compiler will take care of all the magic of actually making that happen so let's see what this would look like here's all the code his actual code he got all the way down to about 40 lines which is quite elegant but all I did was modify this bit so now we'll run his basic in this demo across the in this data set I'm running this on one of my well my computer back in Missouri but SSH in here so this simple neural network is able to get about 94% accuracy on the in this data set we're kind of cheating because we're using large fully connected layers but bear with me and I hope this approach will make sense convolutions i would love to throw one slide up here and explain to you all convolutions in one slide but i don't think that's possible but i think this slide right here which I stole from an Nvidia deck like a year ago is about the best way I can try to tackle the subject what we have on our back is sort of our input image and then what we're going to export is sort of a blurred version of our input image so we have this sort of three by three convolutional Karluk kernel in the middle and all it is is the number one so what that means is that for each input set of three pixels our output is simply just going to be the sum of these pixels together so I don't know if you can see the numbers very well but it's literally two plus one plus two plus one plus one to get seven out we then take this whole little window move it over one set of pixels and repeat the process again keep on going until we reach the end of the row and then we repeat moving everything down one row so this process of going over the image is called striding and this is a very important concept for you to understand the other concept you need to understand is max pooling so all we're going to do is take this group of 16 pixels and convert it to a set of four and we're literally just for each colored region kind of find the largest pixel and make that be our output so if we take these two concepts together and revisit them this problem we can actually significally improve our quality just by changing how we're modeling our data so we're going to take our same 784 but we'll treat is an actual image so it'll be 28 by 28 pixels now we'll run us through two layers the 3x3 convolution maxvill operation and it will keep our same densely connected layers and output of ten categories oops so here's what the actual swift code for this looks like I've literally taken an example from before and we've added a stack of convolutions on top then we take our input run it through our convolutional layer and then send it to our same output densely connected layers as before this will run this goes a little bit slow I didn't quite install everything in the optimized manner but eventually we'll run we'll get up to about 97% accuracy on the in this dataset so by simply changing how we mater how we've modeled the data using convolutions we'd be able to cut our air in half on this toy problem we'll say where do we go from here let's take on a slightly larger more complicated problem this is a data set called C far it's a collection of color pictures so we have pictures of cats dogs animals as well as like human vehicles so like cars and trucks we have ten categories and now we're going to be working with color data so we have a RGB component but our same basic approach that we use for before well we can scale it up to tackle this problem so and we'll simply take our input data 32 by 32 by 3 channels will run through two sets of convolutions Oh max pull two more sets of convolutions a max pull our same to densely connected layers and then we'll have ten categories for our outputs so here's what this model looks like and we've done nothing more really than add another stack of convolutions if you look at the very first line 3 by 3 by 3 by 32 that's where we introduced color and didn't really actually make our net worth that much more complicated so for this one for this one I took the CFR demo from the Swift tensorflow slashed Swift models repository and just replaced put my model in there over that and then we ran it so that will look like this I'll let it run it'll take a run eventually we'll end up with a network around somewhere around 70% accuracy which isn't gonna allow you to write a paper anytime soon but it does like technically work this approach so you might look at this thing and say well heck let's just keep on doing this approach let's stack up more and more convolutions I think if you could jump in a time machine and go back in time five years you could then be the world's foremost expert in computer vision so this is the vgg network from 2014 or so and it's nothing more complicated than the things I've shown you so far we're dealing with the image net data set so we have a slightly larger input of 224 by 224 pixels and but we take our input two layers of three by three convolutions the max pool two more layers of three by three convolutions we're looking at the VG g19 so we have three layers or sorry four layers of three by three convolutions max pool four layers of three by three convolutions max pool four layers of three by three convolutions max pool I'm using a slightly larger dense layer we are using 512 for the two demos before this one is simply 4k 4096 and then imagenet has a thousand categories so we have a thousand output nodes at the end and then so yeah so we take this and we'll say let's apply like one more sort of mental leap on top rather than think of this as being two two four four four let's think to this is one set of two layers one set of two layers two sets of two layers two sets of two layers and two sets of two layers if you can do that step then we can jump over here to vgg which is their first or second ResNet which is our first solid modern approach in this field the basic so on the left side here we have the same vgg network that we were looking at before so 2 2 4 4 4 in the middle we have the background of what's called resna 34 but it's conceptually no more complicated than anything we've looked at thus far we have three sets of these two three by three players four sets of these two three by three layers six sets these two three by three layers three sets these two three by three layers and then we have our output layer the magic of residual networks is this sort of dotted line that's being drawn down over here on the side basically the problem of the vgg approach is that these convolutional approaches are not very resistant to noise so it's actually about as big of a network as you can make the problem is we'll say if each layer only introduces like 0.1 percent noise by the time he goes through 19 layers that's going to significantly affect your results so ResNet basically introduces this concept of skip connections and basically then neural networks are kind of extremely lazy so if they can find an answer then basically they'll shortcut everything else so the power of these residual networks then is that basically you can stack layers and layers of convolutions until you find something that's sort of over fits your problem and then you can sort of dial it back to produce like a simplified in theory best case answer so that then is ResNet 34 we need to do one more trick we need to go away from our 3x3 convolutions so we're gonna go from the if we look here in this other quadrant what we're gonna do is replace our three by two three by three layers with a 1 by 1 3 by 3 1 by 1 style approach so 3 + 4 6 + 3 is 16 times 2 is 32 + ahead and now put layer so that's ResNet 34 the same 16 times 3 plus head and output is 48 + 2 so this is ResNet 50 so let's do a quick demo of training resident 50 on the imagenet data set using a cloud GPU we'll need to do first is create a cloud TPU so that's simply running this command I did this ten minutes ago so we won't have to watch it get started so here we have a cloud TP you're running up in the cloud so we'll start this whole process it'll spit out a whole bunch of line noise we'll say a lot of warnings about tensorflow too but if we wait about ten to twelve hours this will output a ResNet fifty trained on the imagenet data set so where do we go from here don't let the 2015 up there for you this resin at 50 is more or less probably your best first bet for most computer vision problems still today many people have come up with different networks some of them are you know technically better or technically produce slightly better results but more often than not you should come back to this basic model for your first approach let's look at these bottleneck blocks a little bit more basically I would argue that this 1-3-1 approach is not as powerful as the 3x3 approach we've looked at so for the reason this bottleneck layer has better results is hidden in this 256 that's shown on the last layer this 131 the last layer is technically four times as large as the other stuff so basically I would argue that this bottleneck layer is not as powerful as the 3x3 approach however it's cheaper so because it's cheaper we can run more of it and because we can run more of it that's ultimately why this approach is produces better result so in order to replace res Nets we need something that's not necessarily better we need something that's actually cheaper or to use a slightly different word we'll say more efficient so this is a paper that came out in May of this year and it's a culmination of several years of research by the Google team effectively people have tried to build larger networks with people who tried to build deeper networks and people have tried to the larger networks in terms of the size of the inputs but nobody's really found like the perfect combination so this paper what they did is they took the in NASA approach from NASA net from last year they added in some different ball layer types from other cutting edge networks and basically effectively they left the computer and let it search across all this parameter space in order to find the most optimal set of networks they've done similar things this before in the past notably with NASA and then the ameba net papers from last year but what's interesting to me about this paper is that they're applying sort of human intuition and logic on top so they've literally come up with a formula whereby if you come up with one network they can basically multiply the parameters in your network in order to produce larger versions of it so this is really cool I think there's a lot of times the reinforcement learning stuff you kind of end up with networks that only computers understand whereas this is like humans adding another layer of intuition on top so sort of working together we'll sit which brings us to efficient net - HTTP you we can think of our search space as being like say accuracy or quality of our models but we can also model our search space differently so we can say what does our latency you know how long does this network take to run how large is our network how many different operations are we using how many individual parameters and so they've have these edge dpu devices which Google has been shipping out there like 75 bucks or so you can buy and then they gave this efficient net thing this edge TPU Hardware type and said produce the best type of network for this particular device so what happened was is that we have sort of a one-by-one convolution combined for 3x3 convolution and what the network found is that by combining these two together into a larger 3x3 convolution you could actually produce better results in faster amount of time so what we have up here then is our resident at 50 model and as you can see sort of up here we have what we would call the holy grail of image recognition search we have a network that's smaller faster and more accurate which is all you can really ask for so what we're gonna do now is demo running efficient net - edge TPU - the S variant the one does arrows pointing to on a natural HTP device so the first trick we're gonna do is we're gonna use a TPU three instead of a TPU two so that's in this command here the second trick we need is that this is all a little bit bleeding edge so we have to use a nightly build a tensor flow so we tell the computer to do that right here next we have a bunch of parameters and whatnot but basically very similar to our resident command so here's my edge our side here's my cloud TPU phiiiy running and we'll just literally copy paste the command in here we'll give it a few seconds to get going okay so now we're training edge efficient net - edge TPU - s in the cloud on a TPU v3 this will take about 30 hours to run but at the end we'll have produced the checkpoint next we'll just literally copy this checkpoint from our remote server down to my local machine we'll skip that there the edge TPU device uses innate math whereas the cloud is using floating-point so we need to read convert quantize our models so convert from floating-point into n to eight so for this we'll use another script that the edge TPU people have provided the only fun part of getting this working is that this relies on the tensorflow xla ops which are not installed by default and the tensor flow builds so you have to compile it from source this takes about a minute or so to run so this takes about a minute or so to run and we'll have a quantize checkpoint of our efficient that - edge CP edge TPU bill then we just need to simply run the device using actual local okay then we just need to run our run our model locally using an actual edge TPU device I got a picture of a panda off Wikipedia we're using that for input and as you can see it thinks we have a panda with we'll say 60% proper probability but it might also be a frocks with 11% or 12% probability or stuff so fairly broadly our goal was to explore the concept of convolutional neural network to perform image recognition towards that end we built a one-dimensional neural network we added convolutions and then we approach the m-mister column again using a 2d approach from there we looked at how we could stack blocks up and we're tackle larger and more complicated problems in this field then we looked at how we can introduce residual layers and then finally begin to actually modify our different block types in order to produce the state of our art approach in this field I've talked a lot about images up here we'll say but maybe the more interesting applications of CNN's are in completely different fields so we can add another layer on top of our 2d CNN in order to get a 3d CNN we can use this to start to tackle depth data so like lidar stuff like that people have taken language models and converted them into the CNN style approaches so cue a net was an interesting paper from last year where they did that planet detection they can take like a 1d scene and approach and then do some other tricks on top in order to begin detect exoplanets so this Astra net was a really interesting paper in this field they also fold paper came out earlier this year they use a combination a 1d 2d and 3d neural networks together in order to significantly advance the state of the art and protein modeling and then finally the ever-popular alphago and alpha zero engines originally I tried to put up a little bit of each of these papers up here but this slide got a little bit busy but I reduced it back down to this one picture so what you're looking at is the inner layer of the alpha 0 engine which is composed of 40 of these residual blocks that you're looking at right here what I thought was interesting is that this alpha 0 block is composed literally of a residual layer the same approach we looked at before with two pairs of 3 by 3 combinations so the same approaches that we've used to do our image recognition can and a completely different domain plus a whole bunch of reinforcement learning be used to solve the game of go so that's all I got and thanks for coming [Applause] 