 [Music] [Music] [Music] my dance [Music] [Music] without your life in misery welcome welcome to our sixth couch lessons I'm very happy to see so many faces from all over the world my name is Jeanette and I work for the Goethe Institute in Munich and maybe not everyone is familiar with the Goethe Institute so I just want to say two sentences about it we are the worldwide cultural worldwide active culture Institute of the Federal Republic of Germany we promote the study of the German language abroad and we encourage international cultural exchange we decided to address the subject AI because we believed that the developments in this field have a huge impact on our society ai will contribute to a new revolution in human history and it is already an important part of our everyday life artificial intelligence powers Google search engine enables Facebook to target advertising and allows Alexa and Siri to do their jobs ai is composing music and it and painting pictures ai is also behind self-driving cars predictive policing and autonomous weapons that can kill without human intervention these and other AI applications raise complex ethical issues and a lot of questions how intelligent can machines become and can they make fair decisions are we frightened by the automation of society through algorithms and AI will initially human skills such as the creation of art be computerized or will AI make the world a better place by helping us solve big problems such as climate change pandemics and inequalities with our project generation a is algorithm we want to sensitize young adults for risks for the challenges but also for the opportunities presented by the developments in the field of AI we want to raise awareness of young people because we think that they will set the course for future a generation a and their life with algorithms our goal is to initiate a discussion outside the technologies every community and to provide a platform for dialogue between different disciplines and backgrounds for example with philosophers or if artists and their approaches to AI our couch lessons are a perfect example for this every week always on Wednesday we discuss with AI experts from all over the world about different aspects of this technology we ask them what opportunities AI offers and where critical reflection is necessary as AI shapes our society for better or worse it should be on us all to decide what direction we will take the couch lessons are an invitation to find meaning behind the technical advancements in this field and inspire a new ways of thinking we hope you're ready for a new episode we hope you have made yourself comfortable maybe on a couch maybe with a cold drink in your hand and we hope that you enjoy the music that you have heard at the beginning of the couch lessons it was the song blue jeans and bloody tears the first Eurovision song created by an artificial intelligence therefore a deeper learning algorithm had been fed with hundreds of Eurovision lyrics and melodies so can a I be creative this was the topic of our last couch lesson and this week we want to speak about AI and ethics but before we start with the inputs from our experts we would like to ask you to participate in a poll that we have prepared and you can give as many answers as you want and as long as we wait for the answers I want to introduce some of the guidelines of the couch lessons briefly we always start with an input of our experts each about 15 minutes long and after the inputs we open the discussion you can always contribute your questions or force into our jet and I will read the chat and pick out some of your questions and fourths after the inputs I will ask some people to speak out but if I don't ask you please don't unmute your microphone I also want you to know that the entire event is recorded but we just recorded persons that are speaking you can always turn off your video although we would recommend to switch it on so that we can see all the participants from all over the world so that's from my side let's have a look on the results of our poll it's still going on yeah most people think that I can help us to make better decisions fair decisions but only with the right training that data and with the support of the humans so with this result I hand over to Martine my co-host and the moderator of the session thank you for listening hello everybody my name is Martin phunkfist and during the day time I work as a curator and context developer but tonight I'm the moderator and I will guide you through the coming 45 minutes and with us today we have two prominent speakers Sarah Spiekermann who is an author and head of the Institute for information systems and society at Vienna University of Economics and business and wealth of Karla hostert was a project lead for the ethics of algorithms project at battles Mohnish Tifton I'm in Malmo Sweden our two speakers are based in Vienna and Berlin Janette's that you just heard she's in Munich and if you feel like it please try out the chat function and type where you're dialing in from it's always a beautiful image to see that we're actually gathered here today from all around the world so please go ahead and do that also Bogota and Mexico and Ukraine Jerusalem and you know all the place that you want places that you want to travel right and so far in the couch lesson series we've covered five quite diverse fields in which a is utilized some of which Janet just mentioned and some of the aspects of AI development that keeps coming up in our lessons are those of ethics bias and privacy and they're somehow related and we'll cover them all in our Wednesday sessions before summer vacation is officially on so there's four more and after this and two of them will be bias and privacy but today we start this trilogy with with AI and ethics and as always when two huge quite diverse and a little fuzzy around the edges fields of research and thought meet for an hour long discussion we need to limit the scope of it a bit and the goal today is to raise awareness and provide a bit of framework for how to think about ethical perspectives of development of the development of and deployment of AI systems and I think it's interesting because in the AI stay in the couch lesson session last week on creativity there was a very good discussion in the very end of the fact that it's the intention of an art Peet's that gives it a soul and that is this consciousness this this intention that machines currently lack to move us through artistic practices and for me the unveiling of intention is a good starting point for all for understanding all innovation not the least technological innovation and investigating why a product exists in the first place gives us a good clue to what the innovator really want to achieve with it and it's also right there in the beginning of the product development journey that we want ethics to play a part as a guiding principle but does that really happen and how could it work in practice and and how can we as consumers gain tools to see through the promises of improved lives and China gadgets to understand what the technology is that we use actually see here collect and redistribute this is the type of questions I want you to keep in mind tonight and now it's it's time for our first speaker her name is Suresh Spiekermann and she's the head of the Institute for information systems in society at Vienna University of Economics and business she is also the author of the book ethical IT innovation as she's the co-chair of triple i triple e s-- first-ever standard on ethics in IT design an in addition to this she has published almost hundred scientific articles and social and ethical implications of computer systems so I can think of a better person to introduce us to this topic so please beam your energy to tu seras speak amend the screen and microphone is yours yes hello my name is Sara Spiekermann and thank you very much for the invitation today I'm going to be sharing my screen here and and and starts out on the framework of today's talk now when you use the word AI what do you actually mean there is a big difference whether you talk about the ethics of a machine learning algorithms and certain data flowing in and something coming out or whether you whether you embrace that a lot of people these days use the term AI when they really mean more complex big day it had written information systems was perhaps some machine learning components inside of them and I would like to embrace this letter definition of AI for today and say let's think about very complex information systems and when you hear the word ethics very often people immediately think about morality yeah what is what is right what is wrong what is good what is evil and this focus on morality is not the focus that I'm choosing I deeply believe that ethics is not only about right and wrong it's about it's about what is worse being in the world the true the beautiful something that is worth living for as so ethics is less about morals and more trying to find a good way for our future so against this background let me let me walk you through through a little journey here many people when they and I think about AI so I think the robot Sofia is very well known and people believe that the future of AI looks like this and there are other also say yeah but also autonomous cars have a lot of deep learning components are a form of AI or when the future is that we will be walking around with software agents like Alexa or a series speaking to us constantly well why we why we live we are in the future all of these are visions here is another robot or also when we are going to delve into into virtual worlds where we will be meeting perhaps artificial personalities that are nothing more than an eye I'm sorry to interrupt I think your your screen froze a bit when you when you made it fullscreen maybe so it's still on the first slide let me I will then in this case not go to the presentation mode I will end the presentation and I will just put a big screen on so that you can better see I was here with the Sophia you see her yes we know it seems to work yeah what you and I I saw you the autonomous car and I showed you the zero Alex and so on and the students who are delving into virtual worlds and all of those technologies are presented to us many people would use the term AI and then the question is all of this is new and is it then automatically good yeah is this these innovations desirable just because there are they are new very often today we equate newness and innovativeness as values that are desirable in themselves and we consider that we say let's do this but why but why why is that the case do we not have to challenge whether what is new is it really good and when it comes to all of those sophisticated and huge information system I would rather say and they have a lot of good ideas in them and they are like a good piece as a drink of wine but in fact that can also be a point well it's just enough where it's not good necessarily to have more of it yeah and this is why one of the core hypotheses in my work on ethics for AI or ethics for complex information systems is that we do see value creation with some degree of digitization but the question is whether there is a point of inflection we're more digitization and more AI if you want to is not what we really should be having and wanting why is that because right now every day we can observe that information systems that we are using in our daily lives create problems for people and for society at large for instance when our mobile phones very sophisticated machines you know are configured such that they make us addictive or that we get health issues using them all the time or that we have privacy issues because they are observing and with the way we sit the way we sleep the way we talk the way how emotions are going so they're highly privacy intrusive and good Institute we have a session on that they are manipulating votes or they and some AI systems used in the court now maybe undermining our dignity when they are biased or simply the data that is used for them is simply wrong so what is happening is that we are seeing value problems arising and when I talk about digital ethics I am always trying to understand the value balance how much value is created in the positive sense and how much is destroyed in some negative values this is how I look at X and this is also how I look at Essex for AI specifically now the question is seen these young daily challenges around technology is there a way to in fact build technology in such a way that it will benefit us and foster our values but at the same time not undermining our values like those that I just presented as examples and when you talk with experts these days on how they view this crossroads then almost everyone as of the majority of the institutions and thinkers on this matter would be saying oh what we need is some red lines what we need is we need to focus on certain values that absolutely need to be in place in order to have an ethical technology so you see her on the slide and screenshot from the from a male from an Arctic from a permanent Journal coordinates on machine intelligence and they went out and realized that there were over 80 institution that says something like value lists published in the past five to two to eight years where institutions like the European Commission or British sanitization Board or also companies came up with lists of what they think artificial intelligence should have as values and for example if you I gag when you what they what does it what this drawn an article did is to say so what are these values that everyone is talking about this lists what should be on the list and they saw so that they're not all the lists are identical but they share some commonality for instance I try to dig out what are the commonalities here you see that in theory these institutions were publishing lists would all agree that transparency is very important or that AI needs to be fair and has no bias or that it needs to be secure yeah but data needs to be encrypted or also privacy as a very important value is often outlined and this is this is all good and true I I want to give you an example here of a speech assessment like Google assistant or Amazon echo that outlines how privacy is indeed an issue so um when you when you talk to the machine that tries to understand your voice you believe it's just trying to to understand what you're saying but look at the following US patent that is currently owned by Google I believe and was formerly owned or even fired by Amazon and what we the animation does not work here but what you wonder what all these words here are saying is what does the agent actually record about the conversation it records whether you are stressed what your voice is like whether you have a sore throat whether you are coughing whether you are emotionally happy bored sleep is sad or depressed what your purchases do is what your browser history is but how you can be targeted behaviorally where you're currently cache location is and where you as where you live what you did in the past also whether you have an accent whether you are male or female how many people are in the household what age you are etc etc so in fact a voice assistant like Google assistant or Alexa would be taking in so much more data and then you would expect and now what is this patent about this patent is about calculating the ideal moment for example when you are let's say your coughing in the mall in the morning and that's actually an ideal moment to sell you some sound some sweets for your or some some medication for your sour throat so what this patent is about is about optioning of exactly that moment two pharmaceutical companies so that then the Alexa or with which assistant could tell tell you Oh Sarah and you have a you have a sour throat don't don't you want to buy on such-and-such product so these perfect moments to sell people a product are being auctioned off and then you could ask the question is this Ithaca and you'd probably feel that to a certain extent you are your privacy is being breached but there is more to this example because um let's say let's imagine you are it's not about your ass ours Road but the agent knows you are depressed you're emotionally low is it fear then to offer such a person an anti-depressive how far can you go in in in offering people something to buy so the thing is that I'm the icing the example shows that for certain extent lists like those that the nature magazine brought together have a function they are talking about relevant values values that we must and should respect in the design of machines however I would like to make clear in today's talk that from my perspective such lists are no more than hygiene factors of what the producers and service providers of those machines must deliver hygiene factors the absolute minimum that such privacy breaches should not happen and that it is also young so and why do I say that they're just hygiene factors because Essex is about much more than just prohibiting harm Essex as I try to say at the beginning of the talk is about being good and the people I portray here on the slide if you if you are thinking yourself if you're brainstorming about people who you believe our ethical perhaps you would have thought about the people here on slide and what is it that makes these people role models for Essex this tells us a lot about what ethics really is it is as a first matter it's about being a certain kind of person trying to be a virtuous person as Aristotle would have said and Aristotle in his nakooma Essex describes that a good person would be he describes many virtues like courage and Rosati and being sincere and honest so it's about being a certain kind of person and when you when you look at this slide with these three people you might think oh but it's completely outdated but no if you if you think about modern serious like Netflix series orders other theories that Game of Thrones I could put other actors here in that are now a fantasy like Jon Snow in Game of Thrones for instance who are incorporating virtues and to go a little bit more into detail also of our of known ethical theories what else is that con what contributes to good behavior we know for example the duty ethical perspective to say other certain values of virtues that are so important to me I think they should be universal values personal Maxim's and a Kantian sense or treating human beings as ends in themselves and not just abusing them for instance for profit perhaps you could interpret the Alexa example also where well you could argue that here you are abusing the user and to treat him as a consumer must bias and finally I'm being aware of potential harms and benefits about you weigh the pros and cons so any technical work or organization in Deva that aims to works towards the ethical we need to keep these granda aspects of ethicality in mind this is what I I am proposing I think when we build technology we have to go beyond prescribed lists that tell us tick the box for privacy tick the box for transparency and instead ask how can technology people such that it respects these general guidelines of ethics now we'll give you some more examples of one more example even as an example but what I want to say is that I have been butchering been culture for four years of the first global standard for built by a trochee the largest engineering Association on earth where we really try to get this kind of broader and more realistic thinking about ethics into the corporate process into the process where as an innovation team we are building technology and our goal here is to make us people more virtuous of course people perhaps not be the Pope and we will be not my batteries but but we we want to be technology in such a way that it allows us to be better and to to foster our skills and to foster our values and to promote ACCA's yeah in this broader ethical sense and what the three questions we are asking for that and understand that is that a lot of things have to be well organized in the project but when we look at a technology like Alex or who will speechless we would be asking three questions what are all thinkable positive and negative consequences you can envision from the system's use for direct and indirect stakeholders what are the negative implications of the system for the character or the personality of direct and indirect stakeholders for instance if technologies make us addictive what kind of personality are we when we are passive passive addicts yeah and finally which ones of the identified values and virtual effects of a technology would you consider as so important that you would want want their protection to be recognized as a universal law so these three questions our Institute will astanga to reflect on technology in a broader sense and what a critical philosopher among you might recognize is that these utilitarianism virtue as a duty ethics are from the Western philosophical Commons so that what we also do and that's very important to say but also think about your background your tech your your origins so um when you are coming from India or you're a Buddhist or you can foot in or you're a Christian you have your own spiritual and religious traditions of what is good and bad and what role models aren't we invite innovation teams to think about what technology does to those values as well so out of your own cultural frame of thinking and tradition what values are impacted by the system and I leave you now with one with one little case where you see how important this kind of thinking is because when you go back to the speech assistance yeah I give you a story here when you ask an American Amazon ecosystem you say oh I am so sad and I don't feel well then the American system would answer you or if I had arms I would love to hug you if a user says the same thing in Russia to the speech assistant from Yandex from the company Yandex and and would say oh I don't feel well and I'm so sad today then these actually answers who told you that life was a piece of cake you see that there is a huge difference in the way that the AI is answering the value of consolation that is very culture specific the way you raise your children with speech assistant and the way you want the culture the digital inbred digital working culture to be very much dependent as a lot depends on how you configure the agent for not a piece of cake or for wanting to be hard that's the true question these are the SATs this is an example of a truly ethical question and it shows you that we have to think about much more than privacy and transparency and we have to think about the reality with these devices for instance how you live consolation how you want in the friendship how you can support mental house how these devices will trigger joy in life that is culturally very different to what extend it will foster discipline to what extent it must recognize the locality and the local community and the local heterogeneity thank you very much thank you very much Sarah it was a really good primer to ethics and ethics in AI in particular and we will invite you back at the end for the Q&A so everybody please keep asking questions in the chat and we'll make sure to incorporate them in the discussion after the talks and now we're gonna go into our second speaker a name is Carla who state and she leads the battlements tifton's ethics of algorithms project and that is a product that deals with the social consequences of algorithmic decision-making disappeared and she will present some of the work but I also strongly recommend to check the product out in its entirety I'll post a link in this chat in a minute but for now please welcome Carla the screen and microphone is yours sure thanks for having me and hi everybody it's so amazing to see that people tuned in from so many different spaces around the world I think this is I've given a lot of presentation on this topic but never one that this international so that's just great let me share my screen let's see yes can you see it now yes we do good let me switch to presentation mode and see if that works this time you see it when I switch the slide perfect so maybe I just say one or two words about the bat is much different because some of you might not be familiar with the organization the Battlement shifting is a german foundation it's actually europe's largest operational foundation and were active in a large variety of fields from education to healthcare labor markets and my project the ethics of our business project is doesn't have a sectoral focus but looks at how algorithmic systems are used in areas of life that have profound impact on questions of social justice and participation so the kind of application cases we look at might not be things that you interact with on a daily basis or that you might be aware of such as search engines or the mentioned Alexa system but the impact they have on your lives equally big if not bigger so I'm talking about the use of algorithmic decision-making for diagnosis of diseases or for development of treatment plans for deciding where students go to university for predictive policing so decisions made on where police officers Patrol or in credit scoring processes so algorithmic systems deciding who gets a credit and who doesn't and we do this work we look at these cases through three different roles in my project one being as an agenda setter so in the discourse on the issue which is oftentimes very focused on economic questions and on technical questions we try to raise awareness on the societal issues and we also serve as our second role as a convener so we create spaces where people from different disciplines and sectors can come together and discuss these societal questions you because a lot of the things that sarah has just brought up it showed her presentation showed very well that this is not just the topic that can be addressed from a technical perspective solely or from a philosophical perspective solely but that we rather need an interaction between these different viewpoints and lastly we also work as solution developers and pilot errs which is actually what I want to focus on today so there's a lot of ideas out there already of how we can actually put the ethics into the code solutions we can use to make sure that the technology serves society and increases social justice and I want to present some of these ideas today but before I do that I want to start with three things we need to understand about algorithmic decision making because if we do not understand these things then there's no way we can develop effective solution and I think the first one actually became quite clear in Sarah's presentation which is that algorithmic decision making systems and neither purse a good or purse a bad but they're not neutral either and I actually believe that I will make decision making can help us make more efficient more consistent and thereby also fairer decisions because I'm actually political scientist and I focused on gender inequality for a long time so I looked at how human decision-making leads to discrimination in a conscious or subconscious way and I think that's something we should not forget that human decision-making is not flawless either and I do believe that with the help of algorithmic decision-making and I saw in the survey that you all actually agree we can make better decisions even fairer decisions but we also are all familiar with these cases of facial recognition systems that work better for men than they do for women or better on white skinned people than they do in people with darker skins you might have also heard of the example of Amazon that came into the news I think it was beginning of 2019 where they started using hiring algorithm so they used an AI system to scan applications and what happened was that this system which was trained on data from the past past employees of Amazon started sorting out applications by women solely because the current work first Amazon just like most tech companies was mostly male the second thing we need to know is that algorithmic decision-making systems are social technological systems that means the impact that they have does not just depend on the model the algorithmic model itself or on the data but as Sara said also or particularly on the underlying goals what is the system actually used for but also under social embedding if there's a human at the end the so called human in the loop are they informed on how the system works do they know what the goal is of the system all these kind of questions will have an impact on yeah whether we the system actually helps us to create a more fair society or not and the last one is that I was making systems and not what we need to fear but instead it's the people hiding their responsibility behind the technology and this is something we see quite a lot when we look at cases where AI created more harm than good and it can be for two reasons it can either be because people connected to the thesis number one do think that technology is neutral and therefore overly trusted they don't question the decision or it can be because they consciously try to hide their responsibilities behind the system and this is one of many reasons why I am actually not talking about artificial intelligence what why I prefer the term algorithmic decision-making systems because the narrative around this intelligent being is actually what creates the impression that AI systems are something autonomous and makes us forget that there's always people behind it building the technology deciding over they use and in addition to that there are some cases where the technology used is actually super simple rule so-called rule based systems but the impact on people's lives is equally big so it's not the complexity of the technology but rather where the technologies use that we should focus on and as complex as the development process is and as complex as the causes of problems can be that's why we cannot have a silver bullet there cannot be one AI law that will solve it all and that will help us to create a better world but it's rather I always call it a puzzle of many many different solutions that need to work together and I want to present four of these or like it's--for solution fields I would call them with a lot of action needed within these four fields and the first one is call for a broad societal debate and as for now what we do in our project we we also ask people how much they actually know about our rhythmic decision making how they feel about the topic in different application cases and what we've seen in our representative surveys both in Germany but also in Europe is that there's actually very little knowledge on the topic and that people are currently not aware that this is not a topic of science fiction or of the future but this is happening right not right now that there's algorithmic decision-making already having profound impact on people's lives and as Sarah said there's values that are behind the technology ethical questions there behind the technology and at the end it's the question of what kind of society we want to live in that needs to be debated on a broad societal level because if there's one thing I honestly fear it's the developers making these ethical decisions which is why I'm later going to talk about a tool that actually makes sure they don't the second one is the topic of oversight and accountability I don't think that we do need new fundamental rights for the digital digital age but I do think that we need to check whether current legislation and current protection mechanisms are still suitable in times of automation and we need to look at for example civil society watchdog organizations and oversight bodies within fields like education and medicine and we need to see if they need competence building if they need financial means in order to do with the increased efficiency for example of automation of decision making processes and also whether they actually have access to do their job to these systems and this falls under the topic of transparency and there's often times when you say this in around with people who are coming more from the private sector side there's always this outcry that we cannot open up our code this is like a business secret or people will start manipulating the system and there's different answers to this one being that transparency does not an oversight it does not always mean you need to open the whole code of course if that is possible that's a great thing as in the case of the corona app that was just developed and published in Germany they actually decided to make the whole source code public on github and of course this can help to yeah find mistakes it can help to build trust and it's a great example that I hope will set a standard but there's also other methods that we know from different sectors of how you can achieve oversight without these risks for example by establishing in-camera procedures where certain oversight entities get it have a look into the code and it's not actually released to the wider public the same thing goes actually for the people who are affected by either system if I'm part of a decision-making procedure let's say I'm hiring and somebody sends me the code in order to create transparency this won't really help me because I probably won't understand it it won't really inform me on how the decision was made so what is key here is that we also understand that transparency is not the same as intended intelligibility and that we also need to make we also need to take on a design and a psychology perspective to actually make these very complex decisions understandable for people affect it thirdly we need diversity and ecosystem fostering diversity of systems because if there's just one system in place for example for filtering applications the potentials of harm a much larger in comparison to like a variety of systems and at the same time we need to make sure that the people deciding over the use of the system and building the systems are diverse as well and in the past this is actually not a new thing in the past we've seen many examples of what goes wrong when people involved in technological development do not represent societal diversity for example for years for decades actually women had doubled the chance of dying in a car accident than men for the simple reason that these crash-test dummies that I used to develop the safety systems and cars were developed based on the average body size of men not of women and we see these things repeating although we know that this is a problem that datasets cannot just represent one part of society we see it being represented again and again I already mentioned the example of facial recognition working better on white people than are people with darker skin colors and there's actually a lot to do in this field because if we do look at the workforce of the big tech companies they're mostly white and they're mostly male right now in order to change that it's also not such an easy thing we need to change the image of computer science we need to make it more application focused and of course there is a need for cultural change within tech companies and the last field I think needs less explaining its competence building which basically forms the basis for all the other fields among people who decide over the use of the technique they need to be aware of these three theses that are presented to you but also awareness that is not on a technical level but more on an ethical level among the people building the systems like I said I do not want programmers to be the one making decisions about fairness equality of justice but they need to know that what they're working on does have an impact on people's lives and how they can put the right technical measures into place and how they can work with existing mechanisms to make sure that responsibilities aren't hidden behind the technology how am I was time and that's a good question you have -1 minutes ok then I'll go through the rest really quick this is just some headlines I wanted to share with you because they're just a great example of how that activism actually works they're all from the past two weeks where IBM Amazon Microsoft all announced that they would either drop out of the facial-recognition market completely or at least stop working with law enforcement agencies on the topic of facial recognition and we shouldn't thank these companies in this case we should mostly thank civil society and science activists that have been working on this topic for years if you're interested on the issue of facial recognition and it's really not just about bias to be honest perfectly working facial recognition system can create even more harm you should check out a page called gender shades by joy boonleung wheaty who has really done great work on the topic and last but not least I think we can also move this to the discussion I wanted to present to you one practical solution that we have been working on together with a variety of stakeholders from different disciplines which is the AI ethics label and Sarah Spiekermann has mentioned that there's right now a lot of guidelines out there for the ethical development of AI and ask the Baptist mention if thrown together with the irate slap one of these organizations that put out guidelines the Al Gore's maybe I can post the link in the chat later but after we published these guidelines we were facing the real challenge which is how do we actually put them into practice how do we make sure that they're actually being implemented and so for the past year we've been working on the development of this label which might remind you of the energy efficiency label which we know from washing machines or technological appliances in the kitchen and which can actually be used to of course again it's not a silver bullet but it can be used to create transparency over the values of a system between people developing it so software companies and people purchasing the system so for example if I am a government body and I want to use a system for the issue of procurement or for predictive policing I can use our guidelines to actually put certain ethical requirements into my procurement standards and to then compare different types of systems that are out there and get a feeling for which one serves my purpose the best and the same actually goes for the people developing these systems who can use our method for the operationalization of like very general broad topics such as transparency in privacy to see what actually needs to be done to implement transparency reliability justice privacy and to then make it visible what they have done and to sell their product to yeah customers and ok last thing I'm gonna say is if you take one thing from this talk it should be that we are not helpless that this is not something that's coming over us but that it's actually always humans behind the technology and that we can and we have to shape it thank you thank you very much Carla and thank you to you as well Sara's it's two really really good presentations that at least gave me some new frameworks through which I could think about the opportunities and complexities about ethics and AI and please keep asking questions in this shot I I would like to ask the first question actually and that's both Sarah and Carla and I'm cursed I mean are you optimistic and where do you see this discourse and its attraction are you being heard or yeah what's uh what's the current state of ethics in AI systems you wanna start um I I must say that at at the moment I'm I'm not very optimistic the reason being that there are still as this modern belief about the Machine bringing salvation over us the idea that everything that's new if we can replace simply everything this terminology like digital transformation there is so much money behind it and in so many fantasies that I'm wondering what degree of disillusionment is necessary in order for companies to really seriously embrace more than the hygiene factors when it comes to the embracing of these labels I believe that companies will be fighting heavily to ensure that they are looking good on the labels I could imagine that something like that will still be adopted for sure and III think it's important that at the same time as I try to illustrate in the talk that could still be you know imagine something like passiveness addiction resolution of one of our own identity things like that are not on the label can't be on the label naturally but but these are the grander questions if we as humanity goes through a new second Neolithic Revolution here we are we are facing bigger questions and these are not thought of I think I'm a bit more optimistic I think at least in the of at least looking at Germany maybe or Europe overall because something that I have seen is that there's been more awareness on the importance of the topic in the last couple of years that there's a lot of political processes happening the European Commission has recently published their white book and they opened adapt for consultation there's in Germany a lot of different interdisciplinary inter-sectoral working groups working on the topic and we're already seeing some progress in certain areas I mentioned the the corona app before which is now in Germany they're now using a so called decentralized model which is really the best option when you look at for example data protection issues and they opted for that option because there was such an intense discussion between science and civil society and they had actually they wanted to go for a different one and change their mind which is a great example of again how activism can have an impact if politics are open to listen to them and to start this debate and also to switch once they've made a decision and I would agree with Sarah that there is a huge challenge when it comes to redesigning our society and that sometimes technology can or the way we think about technology can be in the way of that but that's really a broader societal challenge for me and yeah thank you I'm gonna hand over to Janet who will give the microphone to people in the participating audience thank you very much there has been quite a lot of question I've first have to unmute everyone I guess it's already done so we have a question from Anastas about for column maybe you want to speak Anna yes thank you for the talk that was really interesting I wonder what kind of device would have the label G in all three areas so I can imagine the optimistic side but we wish they have but I cannot really imagine a device sort of failing and every six areas do you have anything in mind because if you have no worst case then then the the worst label is just for making me feel better I mean this for now we have not applied this label there's actually only specification of what the different values mean for three out of those how many are there I think six labels so it's it's really hard for me to give you an answer because it depends really on how to operationalize it like I said things like justice and transparency they can mean so many different things and again there's another thing that would that would influence the way I answered this question which is whether you opted for a minimal approach so you say if you rank G in one of the values then it cannot be balanced out through a better ranked value in another area or you can say that you go for multiplying approach so if you're really good in one field and you're quite bad in the other then this can balance each other out and we actually haven't made a decision on this because it's also not up to us but when we presented this label it was really to speed up the debates in a moment where the European Commission started working on the topic because we felt that there were so many passwords out there like transparency and very little understanding for what is actually meant so I'm really sorry that I don't really have a good answer but I hope this clarifies the description of the operational and I can I can post the link in the channel give me one second okay thanks and there have been two questions or two similar questions for Sara about the local culture versus singular systems and maybe Yasha Jane I hope I pronounced it right maybe you can't speak out hello I wanted to ask on the point where you talked about how Russia has a different response to asking the same question do you foresee a future where this is more pronounced where like a neutral cultures would be more given more weight or do you think it will turn into more of a uniform AI for majority of the world thank you so far our technology has a very homogeneous very us centric cultural configuration and as we speak about ethics and as we speak about progress I deeply believe that technology must be more attune to to cultural specificities and I do think that there is no problem to do that because especially AI systems can be tired they are trained on local languages they are trained on local people they can be offered region wise no and I think I think they should I also think that this is not even from a business perspective that makes a lot of sense because people feel more closer to an agent that that that understands their own personal their concerns and when they use words what they understand and they're gonna post the link in the chat sorry which is actually really interesting on this point where you can become active yourself Mozilla has been a project called common voice where you can donate your voice in order to create more cultural diversity on voice recognition and speech systems I'm going to post the link here you can either listen to people speaking and correct it or you can donate your own voice when I I would also also in the p7000 standard what is happening is that um let's say a value like privacy might be important but what people understand by privacy how much they want of it and what are the dimensions of privacy that are important to them is very different from one culture to another also the degree of surveillance that people that societies are willing and wanting to embrace is very very different and I mean first I'm in German so I'm very much opposed against abeyance I don't think it's a good thing to do but there are questions that are more open to surveillance and even if that is a delicate issue I believe that one less thing is for sure every nation state or culture must have their debate with their political system on how much privacy and how much surveillance they're willing to accept I don't see I don't think this is Google that should be and/or Apple that should be pre determining this for us and believing that they know what the right worldwide solution is thank you there has been a question or more comment from annamaria Ballester about a society we are living in maybe you want to speak out hi it was more common question really yeah about what was said about this what I said was very frightening about a device that can see how you feel or that you talk to it and say you know Alexa today I'm feeling really sad and it says you know either it's not a piece of cake or all I want to hug you how far are we thinking about introducing all of those aspects as you were just saying to this technology or how far should we say no this is not something technology should we should think about other social systems that can you know give what people need and in terms of company or is this P is this person alone what can we do to help is this person having mental problems that sort of thing so it was also this this thing you said about either people think technology will solve everything or it's totally bad and you know robots will take over and kill us all so how do we find the balance there because I do think it has good uses like I said for example on also for people who are on the autism spectrum for example they might actually feel better speaking to a machine than to a human because it's you know it makes more sense to them so I don't know if it's a question but a comment I think the question is how far how much of what we're investing in AI should we maybe be investing in other social systems and structures yeah thank you well I I think that there's huge opportunity cost to IT investment I think it's it's outrageous that 1 billion for X and in Austria we now spend 1 billion euros on on 5g networks I think that money would be far better spare spent into into social projects and unfortunately I think that the rhetoric as the the language of the IT industry is is not very humble it's about this will happen this will be like this and because they have so much money and because of the distortion of the financial markets they continue to have access to resources that that that they simply build this technology it's all there right now and perhaps you don't have a speech assistant at home and you perhaps belong to the people so oftentimes don't want to have it at home but we have those sophisticated systems already and I mean if you have a mobile phone a smartphone in your pocket and you did not look at the at the terms and conditions that you signed it maybe right now that that your motion how far how fast you move today how you feel how is your tone of voice what is your vocabulary all of that it is analyzed in real-time data management platforms are sucking on average thirty thousand data points from us from the devices per person to make very complex socio social psychological profiles of us and this is terrible it's and it's happening and this is not science fiction this is happening right now thank you diverse also question from nearly Hernandez maybe you want to say your question yeah sure thank you very much and it was really interesting and well it is related to the question that is because well once I saw this beam I don't know if you have heard about it it's called hair and this is about an S oh that yeah basically it's the romantic for nails disco so I was wondering if mm it will be possible to reach that kind of point because yeah um a guy required to me we humans need physical contact but also I have read that in the development of robots well we humans have I can kind of degree of acceptance if you robot is to like us it makes us out and do you think it will be possible Kalai if you want to go ahead it's fine I don't want to take the voice all the time it says no go ahead I think this is more directed at you the thing is that I I did have an idea of of people of men women myth which is that as we go through the world we our live body is resonating with the people around us was the object around us and what what they found in research is basically that we can only attribute meaning to things we can own here for instance understand what sympathy is and what love is and learn about those values in cooperation with other people as we resonate with each other when it's right resonates with their mother then the child learns what love is all about yeah now what they found is that when when when people interact with machines they cannot they cannot learn about for instance the concept of love or sympathy and they also they cannot even learn the meaning of words when they interact with a non human being now that's research research cognitive psychological research so when we speak to speech assistants for instance a child the child cannot learn from the robot because the robot cannot share into the resonating relationship between the living being and the artificial being so if you ask me how I see this future I believe I'm that there because we already know that humans cannot learn artificial beings and that is a threat that human beings are true as soon as that there is this opportunity cost of time for the spending too much time with the Machine and less time in the real world and that not being in the world makes that human beings might be losing some of the richness in their identity that they currently have if they spend too much time with those devices and I can add something to this which is that expressing a hope which is that if we use the technology right we actually get more time to spend with each other with other human beings because ideally we use automation it doesn't need to be AI it can be a super simple rule-based algorithmic system to take on tasks that are tedious and that have nothing to do with what a lot of humans like to do which is human interaction which is creativity which is the creation of visions and if we yeah use it to take away those tasks for us let's say in the field of medicine doctors that have to look at hundreds of radiology pictures and then have very little time to actually sit down with the patient to discuss different treatment options there's already image recognition systems that I used in radiology that are better and identifying cancer on these scans than the actual doctors so we can use it there and the doctor can actually have more time at the end to interact with the patient okay thank you I think I would like to reply to Carla here and even of course I agree that when we talk about very narrow professional uses of these systems you are absolutely correct but when we talk about systems that are deployed in the real world as long as the business model of an AI service providers is to have our attention and time absorbed yeah this is the current business model the more you stay the more you look or the more money they are making as long as we have those business models for the private AI on deployments we are having that problem that we currently have which is I don't remember any private use of technology that has saved us time so far in contrast we have less time than we ever had Google Maps I wouldn't be here without it I would be lost somewhere but so maybe we just have time for one more question and there have been a lot of comments from prego Brown maybe you want to speak out they go prone not there anymore obviously and there has also a question from Quincy maybe you want to speak ok so just ask your question or tell us your comment on learning from artificial systems I think what is missing is proper yeah let's call it a body falls yeah what is missing is that you have a tone in your voice you can provide mimic as a human being or or touch each other and so on and that's what is harassing is missing to make people be able to learn from artificial systems so I would agree on that for now but I think in future that might change I wouldn't say it in general that use that said for example kids like mansion kind learn from artificial systems that that was my comment about of this statement yeah yeah it's a good so good that you say that because there is this idea that you just need on mental represent a kind of mental robots but and see it's a form of intelligence that children learn when their joint point at something you know that if you ever try to ask to something the cat won't understand it's a specific form of joint attention and joint meaning attribution this is what children learn I think at the end at the age of 80 months or more I don't know but it's a specific point meaning making which which is something that that we don't know how to so its currency is still there so I just want to give you the chance also to ask your question if not I just want to thank Sarah and Karla and also Martine and I want to announce our upcoming lessons so we will have one session about AI and the future of work one about AI and bias and one about a I and health and next time we deal with AI and the future of work with very interesting guests from all over the world and we hope to see you again and spread the word and mention us and nearest social media channels and we would be happy to see you again here so I hope you have a nice evening or day depending where you are and it was great to talk with all of you bye bye you 