 [Music] welcome everybody my name is Sam Silverberg I'm a cloud engineer based in Portland Oregon and I'm lucky enough to have my brother Aaron Silverberg up on stage with me we're gonna be presenting about cloud ml engine for industrial vision inspection but before we get started I just want to talk a little bit about how we got to this point which was my brother and I were discussing all of the advances in object detection and machine learning and Aaron is in in the industrial space and we thought this is a good opportunity to team up and come up with a fund to make it a fun opportunity to work together thanks for the nice intro Sam yeah we don't really get the opportunity to play Legos and GI Joe as much these days so for us to be able to work together on something like this is fun flexible assembly systems is based in San Diego and we specialize in designing and building customer robotic equipment for various manufacturing and industries whether it be an automotive bioscience medical device aerospace defense etc and one thing that all of these industries that we work with strive for is better quality higher throughput better safety standards and the ability to log vision data now I have a short video I want to share with you guys that goes through how machine learning machine vision and automation all fit together welcome to flexible vision give me a sunflower seed all right giving your sunflower seed a Google use flexible vision give me a peanut [Music] all right getting your Canucks in Google vision what can I get for you give me a candy almond are you tired of digging favorite night well for me this was happening all too often and I had to make a drastic change in my life and with a little help from Google now my life runs efficient as the Department of Motor Vehicles but in all seriousness machine learning and machine vision really do have the potential to change the world for the better so what does peanuts and almonds have anything to do with machine learning the answer is machine vision machine vision today is a 10 billion dollar globally global industry and is expected to be 18 billion by 2025 in North America alone the industry has grown 10 percent this past year this market is really broken up into two product categories we have the PC based systems and all-in-one smart cameras these PC based systems have a lot of horsepower capability you have the capability of plugging in multiple cameras being able to run all all the cameras in parallel - and then you also have the flexibility of being able to swap out lenses and lighting to cater it to your vision inspection application and on the other side we have the all-in-one smart cameras they're a really nice tight package that have everything built-in they have the processing the lighting and the lensing all in a nice form factor but you do lose out on the horsepower and you are limited to obviously the one unit for your location now these are real life examples that I have up on screen this the first example is a medical device that we were doing a hundred percent inspection on these products as they were going out the door and it's a silicone injection molded part that was prone to under molding and this under molding would be not enough material was injected in the injection mold process and so we had two cameras were mounted perpendicular to each other taking a picture simultaneously and getting a throughput of one per second on operation the second example is an automotive application this is a electronic device that goes into vehicles and there's process that requires thermal paste being applied to this product before it goes into the automated machinery and so we decided to use an all-in-one smart camera for this application to detect that thermal paste and give the robot the okay to go ahead with the operation another way these cameras are used is for robot guidance the this is an example of a camera mounted on the end of a robot to give it coordinates on where it should go and this is doing server assembly and we were looking at some fiducials on the printed circuit board to decide where those parts could go with great accuracy and other examples might be picking and placing car bumpers from a moving from a rack onto a moving assembly line picking up placing horns onto a sub assembly inspecting a bead of adhesive to verify that it's a solid bead of adhesive and then being able to read barcodes and OCR here I have up on screen two examples of today's programming interfaces and on the Left I have a this is more of a drag-and-drop this is you have your pallet of tools that you would drag and drop onto your screen and create your program in a tree like structure and over on the right hand side is an example of more of a spreadsheet style programming where you can import functions and do more of a programming and a spreadsheet both are based on the idea of having a palette of tool sets being able to pull those tool sets into your program then manipulate them and the thought has always been - in order to add more capability add more tool sets now I'm going to quickly go through kind of today's process on how we program vision systems today and essentially what we're doing is we're looking at a bitmap image of a picture there have been advancements in the industry to triangulate between bitmap locations to give you higher accuracy that this is the product the this is the image that you're looking for but essentially what we're looking at is taking a an image bitmap and comparing it to one that we've trained against first step is to define your camera lighting and lensing and adjusting your aperture for your application then we can go into the programming and programming one of the first steps we typically do is the calibration if it's going to be used in coordinated motion between a camera and a robot you want to be able to take those coordinates and give them real-life coordinates that the robot can interpret then you'll use just like we talked about a minute ago we'll use the drag and drop from our pallet onto screen to create our program for example these pallet tools might include a circle tool if you're trying to define a golf ball moving down an assembly line you might use a circle tool to tell the system hey I'm looking for a round object or a dispensing a dispense bead you might use the line tool for something like that or a histogram tool if you're trying to figure out how bright or dark something is and you'll add these tools and create your program and you might include some pre filters to make those pixels pop out a little bit better and then it's ready to run and we run through the snap and find routine and it's not really until we get to the thousands thousands one that we really feel comfortable with hey this program that we created is going to work for our assembly line and keep in mind that false positives are a part of life in this industry so why is that well the systems today are suspects of lighting conditions we have whether he imagined you're near factory floor you have a skylight and the lighting changes throughout the day well you have to account for that in your software program and your vision program so that as those lighting conditions change that you are still successfully finding the model as you intended camera orientation is important if that camera happened to get bumped that is an environmental issue that you might have to overcome and like I had mentioned earlier that the the the operators because you have these false positives operators on the assembly line tend to open up tolerances that you've built into your software program to alleviate their jobs to get the product out the door but what they're really doing is opening up the opportunity for failed product to be going out the door it's also very difficult to detect any object with variability like you saw earlier with the peanuts the anything organic like fruit is very difficult castings are another difficult object to find because sometimes they come out shiny or dull or have additional flashing on them and logging of data is such a value and it's it's underserved in the market today mainly because it's not accessible it the the application is there but it's up to the engineers that are building their software programs to make that connection to the cloud what efficients vision systems were less difficult to maintain easier to share programs across factory floors camera resolutions weren't as critical lighting and environmental variables were less of an issue systems were less expensive and the same system that you're at home sorting Legos with your children is the same system that you're using at an automotive Factory detect scratches on trim is open source the solution well tensorflow is a great object detection tool OpenCV is good for measurement and calibration node-red is a good logic control and allows you to connect to peripheral devices tez errect OCR is the number one optical recognition software available today and z-bar is a good tool for reading barcodes but not one of these is a silver bullet each one of them comes with its own steep learning curve and integration between them is non-existent we wanted to crack this nut by democratizing the the industrial vision market removing the technical barriers to entry literally harnessing the ml engine and bringing an on-prem when needed simplifying the setup creating an all-in-one easy to use graphical user interface I wanted to make it flexible and modular by providing an open API being able to use any USB camera and keeping it low cost by keeping it open source using low-cost CPUs and an edge devices being able to train in the cloud instead of on-prem integrated cloud and open source is the future to industrial vision we are super excited to be releasing an open-source version of flexible vision which meets all of our democratizing objectives it has endless market potential opens the door to use cases that weren't previously available whether it be to pricing constraints or functionality in the market vision models can now be shared across factory floors globally and version control is built in the system is automation ready and can communicate directly with robots and PLC's on factory floors I have a short five-minute video I'm going to go through the software and then I'm gonna pass it over to Sam and Sam's really going to go into the ins and outs of the software and how we're using Google cloud and Google components to bring it all together so the very first thing that we're gonna do is we're gonna plug in a USB camera and it shows up automatically under connected devices and then you'll see we can see the camera and we're gonna change the resolution we can go up to 4k resolution on this but keep in mind that the lower the resolution the faster the processing and this is an optional step but the camera calibration is required for robotic guidance so we take we can coordinate we can take the pixel location and convert it to millimeters and also take any skew and angle rotation so we could pass that information to the robot so we could pick up correctly and then if we go back to the camera screen we could see that it is calibrated and we could see the conversion from pixels to millimeters and the rotation that we saw earlier all right now the fun stuff we're going to create a project and we're going to call this nut training and the next step is to start taking some images of these nuts so we're going to select the camera and select the project that we want to associate all these images to and we're gonna sprinkle out some peanuts randomly oriented take a snapshot of that and then we'll sprinkle out some almonds and we'll do that one more time with some walnuts and now we're going to move into the image annotation this was we ready we're gonna tag the images and tell the system what each one of these items are so we do have some icons over on the left that tell us that these images have not been tagged yet and we have a whole whole lot of nice tool sets built into this GUI so but the very first thing we'll do is we'll create the tag names and this tag is going to first of all be peanuts and we're gonna do a box detection and just simply drag and drop we're gonna go through and select these peanuts and then we'll do the same thing for almonds and like you saw earlier what we were creating these tags there were a few other checkbox we could have put in there there's object there's object detection which we're doing right now we have OCR and barcode reading all built in we also have some other tool sets like a zoom feature which allows you to if you're trying to do a small components on a PCB that were difficult to to see we have this this lure that you can drag and drop and now we're ready to start the augmentation process this is where we're going to take just these three images and augment them and give them some variables to to create thousands of images from these just three images and click export data here we have a pop-up box which would gives us some variables that we could punch in if we wanted to change how these images were being augmented if we want to be able to rotate them or scale them differently we could put those parameters in here but the default settings seem to work pretty well and you'll notice at the top of the screen we have a indicator that says hey something's going on this preview pre-training is a pretty quick process it only takes 30 seconds or so and you can see that that's been completed and we can go to the image detection page and you could date image data set excuse me and you can see that now we have those thousands of images created then we're going to take these augmented images and export them for model training again we have a small dialog box that allows you to change a few settings but we'll just keep it again as default in this process happens up in the cloud and it can take anywhere between 20 minutes to an hour and a half depending on the model that you're training and will come back and now we have will see that if we go to our excuse me list projects we'll see that the project project is done and we can assign a model to it the reason we did it this way is we wanted to have that version control built in so in a future date as you're refining your models I'm making them better and better that you can go back and update that and now we're ready that's really the last step and now we're ready just to capture and run select the camera select the project that we're interested in running and there you have it it's that easy to create your models this is you'll see in just a minute here we're going to break apart some of these walnuts and the detection is absolutely amazing and with that I'm going to pass it over to Sam so what I'm going to do is I'm going to go through a little some of the concepts that were required to unbe understood in order to make all this happen as well as is the underlying architecture from both an on-prem and from a Google cloud perspective but before I do let's talk a little bit about flexible vision and what flexible assembly is releasing today and it's an open source package that provides end and training all the way through prediction that includes camera calibration it also includes object detection object localization OCR scanning and barcode scanning and it leverages ml engine in the cloud google kubernetes engine to perform that augmentation and training process Google Cloud storage to store the images obviously in persistence and then finally on Prem we're doing the prediction this is what the overall architecture looks like and specifically what you saw in the case of the initial video Aaron showed which is that all the way on the left here so on the left yeah on the left is are the rope is the robot or the IOT devices that you will be manipulating the data from the camera in the middle or the next slide next over you what you have is the workflow engine the workflow engine in the case of what we did is node red so net node red is a nice drag-and-drop tool to do IOT integration but it can be any host of different tools there and then you have your local prediction server in case of flexible vision micro services are heavily leveraged and each one of the different components is actually a docker container in and of itself now prediction locally on Prem is important specifically for industrial applications because you read need very quick sub hundred millisecond cycle times that round-trip time to the cloud may not suffice within those circumstances and finally all the way to the right you have all the components are in Google Cloud you have everything from like I said kubernetes engine it's responsible for training coordination and augmentation Google Cloud Storage that's responsible for all the persistence ml engine that's dead doing the actual training itself and then finally what you saw is Aaron initiated the entire process using Google home initiated through dialog flow and that's also represented here so let's talk about the main underlying concepts that's required as part of this as far as what you've seen today and that is object localization or object detection commonly referred to there are a lot of open-source libraries that allow you to do object detection but the one that was selected for this project was tensor flow object detection API this is an open source API that's provided by Google and is on github and it essentially provides bounding boxes around images as well as what's called instant segmentation what I mean by bounding boxes is what you see in this picture those boxes around the images themselves that identify what those objects are there's a lot like I said there's a lot of object detection models but let's drill in just quickly on a couple examples and how this all works together there are two steps to this process there's actually quite a few but the first big step here is actually identifying candidates for objects that you're going to want to do some detection process on and let's talk about single-shot detection single-shot detection is a very fast method of identifying objects and images by using a small convolutional network to come up with candidates of possible objects that could be in that picture this is really useful for small form-factor or low computational power systems such as cell phones or TP use edge GPUs it's not so good at detecting small objects but is fine at detecting larger objects and pictures the second method is faster are CNN and this works differently than SSD and that it doesn't use a single shot it actually does post-processing of the image to find potential candidates in those pictures and the way it works is finding anchor points in those images and detecting foreground versus background to come up with those possible candidates of objects in those images once those candidates are identified the second step is actually to do the prediction that is what is in those objects themselves and that's the same whether you using SSD or faster are CNN let's talk about two possible methods for doing that there's quite a few but these are two that are probably the most common the first is ResNet rezident is a deep neural network they can go to a hundred and one layers and beyond in terms of its death it's very good at detecting detail it's very good at accuracy of detection and an image the second is inception an inception instead of being deep is a wide neural network there are also deep and wide neural networks enter deception base but essentially you can kind of think of them as rather than going deep going wide regardless of what neural network you decide to attach to the detection process you're going to want to measure to see how accurate these networks are and the primary way of doing this is using what's called mean average precision mean average precision is a combination of the three elements that you see here in the in the slides the first being precision that is are the objects in the picture that are they are are they what they're telling me they are the second is recall because is it seeing everything it's supposed to see in the picture and then finally is intersection of Union intersection of youjin measures how much of the bounding box is around the object is it too much area is a too little area or is it just right all three of those components are combined together to come up with this mean average precision and you'll see this quite often as an example what you see here in terms of measuring accuracy the second key component is once you've identified those objects now you have to put them in real space and how this is done is by translating those pixels whereas the bounding boxes are in two millimeters in addition to that you need to orient the robot in the case of the video that Aaron showed to the camera so that they're oriented in the same location at the same angle and this is all done through a calibration process now the way that flexible vision does it is by using a checkerboard - I do the calibration what I mean by calibration is identifying the angle of the camera to the surface that you're doing the detection on in addition to that you want to be able to align the rotation of whatever is the device that's doing the pick in place with the robot I'm sorry with the camera and that requires a second step in the case of flexible vision that will Lyman is done using a QR code why is this important because like I said object detection is the basis upon which an inspection industrial applications using machine learning will be built once you've identified an object in space you can then do OCR scanning you can then do barcode scanning you can then do pick-and-place and quality assurance on those objects because you've correctly identified them and you have confidence that they are what they say they are so that's why that process is so important so now we're going to go through step by step and a little bit more detail about what's happening under the covers and why all these steps are required in order to do object detection the reason this is important is it doesn't matter if you're using the open source flexible vision or if you guys are building your own tool the steps are essentially going to be the same and the first step in that process is doing the tagging this is the idea of dragging and dropping bounding boxes around the objects that you're most interested in in the pictures that you've collected in the case of flexible vision those pictures are collected from the work cell that you're actually going to do the detection at or you can upload them to Google Cloud storage in either case once you've collected those pictures you're going to want to do this bounding box process and the bounding Braque box process basically does three essential things it identifies the objects for training it also identifies a label that you want associated with those objects and then finally you're gonna select an action that you want would want performed once those objects are identified those actions might be just strictly identifying the object or it might be again doing OCR scanning or barcode scanning one question that comes up quite often is how many objects do I need or how many pictures do I need in order to get a good training and that really depends on the type of objects that you're doing detection on as an example luckily and in the industrial space many objects are well defined in terms of their edge and shape and they don't vary very much in those cases you may only need three to ten images in other situations where you're doing quality assurance and the type of failures may vary quite greatly between different scenarios you may need quite a few different images or samples of those in order to get a good training it may be up to 100 images the second step to this process requires two phases and we called the training process but it's actually two phases the first is doing image augmentation and the second is to actually doing the machine learning training itself image augmentation is not always required but is really nice to have because what it does is it reduces the variability associated with environmental variables like Erin mentioned if you have a skylight and the lighting might change day to day during the day or if the camera angle at the time that you're doing the training is different than the camera angle at the time that you're actually doing the detection and production having done this augmentation is a really nice thing to have what essentially performs is skewing or scaling image contrast or rotation another nice aspect of doing this augmentation process is you might take a sample size of say about a hundred images and it might explode to a thousand images based on each augmentation has a new image in and of itself so you actually end up with a larger sample size and once you have that process complete then you would actually go through the training itself now the training itself is fairly straightforward from the perspective of you've already done all the hard work the trainings done all biml engine in this case now what happens on the back end is that ml engine is spinning up in the case of flexible vision thirty two simultaneous GPU servers to perform a training in about an hour now if you compare that to what if you ran it on your own GPU that might take 16 hours the other benefit of this is that once you do the training if you need to retrain or if you need to retrain it's very quick as well so it's a good benefit to doing it that way now let's talk about the third step the third step in this process is actually to do the production prediction in production in order to do this prediction in production let's talk a little bit about the flow itself and let's talk about what Aaron showed in that first video that is the picking and placing of the nut so he initiated that process using Google Howe Google home then triggered dialogue flow which is in Google Cloud which then triggered a pub/sub event a pub/sub event is a topic that is listened to by external sources that will thaton contain some data that pub/sub event was listened to by node read node read again is an IOT workflow engine it's open source but you can use any IOT workflow engine in this case once it was triggered it then called a REST API that was on a flexible vision server that that said take a snapshot and returned to me the bounding box is associated with image objects that you found in an image with that flexible event vision then triggered a snapshot from the camera returned back the results that were found within tensorflow those results included the bounding boxes and some binary digit associated with the bounding boxes themselves flexible vision then use that information to return back the location in space that is the location milimeters the textual label of associated with those bounding boxes in addition it returned it will return back OCR scan or barcode scan data as well all that the data is returned back to node read in a form of a JSON document and then finally node read then commands the robot to move to a specific spot based on a midpoint of a bounding box and then returned back the night in question so now I'm going to turn it back over to Aaron and he's gonna close things up so we are super excited to be releasing this software and we really can't wait to see what you all use to use this software for it's with flexible vision we've simplified the process we've democratized machine learning and machine vision and again we want to see in education we want to see where we would want to see universities using this technology and taking it out into the workforce we can't wait to see pathologists and radiologists using these this technology to find rare diseases and in the industrial space we want to we want to be able to change and collect the data associated with all of our customers needs and in home automation we we can't wait for you guys to find out when you're low on milk so with that we appreciate everyone's time today and thank you very much I'm going to open it up to questions so yeah we're we're open to questions if anybody has any questions on implementation details or other aspects [Applause] [Music] 