 So today we're going to build our own Real-time AI surveillance camera using a Raspberry Pi and the OpenCV AI Kit so this Raspberry Pi CCTV camera solution will be able to detect a person using a Mobilenet SSD at an astonishing 30 frames a second as well as sound an alarm and light up an led to stop any impending theft the problem that we're trying to solve here is that there are not many real time Raspberry Pi CCTV camera solutions online and if they are real time then the processing is not an embedded solution it's done on a server pc or cloud the other option is to use something like a Neural Compute stick or a Google Chrome which is great but their form factors are slightly bulky especially when we are trying to minimize our security camera footprint so instead we are going to try using the OpenCV AI Kit attached to almost any Raspberry Pi even a Raspberry Pi zero for a small size surveillance camera so let's get right into it so it all starts with our unwelcome guest so this intruder may be human or even an animal if you want to train your own model then stay till the end of this video and i'll show you a custom example for more practical surveillance applications so this intro then gets detected by the OpenCV AI Kit at 30 frames per second the video plus the detections are sent over usb c cable into the Raspberry Pi now the Raspberry Pi has connections to a speaker or a powered siren via the audio jack you can even connect it to your headphones just to test and then you can have an led connected to the gpio ports instead of an led with some additional circuitry you could swap out the leds for a floodlight and lastly we would want to view the output from our camera now we could install motion ios or run a web server to view and record our detections however I opted for the easy road by just using teamviewer remote desktop from my android phone to view the captured video from the Raspberry Pi regarding the requirements you'll need the following a Raspberry Pi of course version 3 ohio will work as well as all the peripherals to run your Raspberry Pi like the cables a monitor keyboard and maybe an enclosure and heatsink the OpenCV AI Kit which comes with a usbc cable any smart device that is capable of running teamviewer I'll use my Samsung Galaxy Note 20 ultra a speaker from which to play the sound with a matching 3.5 millimeter cable an led for flashing the intruder in the eye just kidding don't forget you need jumper wires and a 330 ohm limiting resistor for the LED and finally this one is optional we'll use a 3D printer to print the enclosure for the smart camera the links to all of the components will be in the description down below connecting the Raspberry Pi to LEDs and speakers cool so here is a schematic which I haphazardly drew in fixing so first up we need to hook up our LED anode to the gpio pin 16 and the cathode to the limiting resistor to ground at pin 6. thereafter we can connect either our power speakers or headphones through the audio jackpot on our Pi it is quite simple as you can see from the schematic cool now we need an enclosure for our camera there's two ways to do this design an all-in-one box for both pi and oak device or we can just make a case just for the oak one and run it over USB C cable to a hidden Raspberry Pi i'm gonna opt in for the second option which will allow our camera to be positioned discreetly and also because it's an easy option [oh my god] if you head over to the github link on Luxonos depth AI repo you'll find a variety of cases for both the oak one and od that you can 3d print it's on github.com/luxonus/depthai-hardware and you'll find it under the user-contributed-mechanical-designs.md  now i'm going to go for this one which has a GoPro mount there's also another which has a lens mount which is really cool now the rest is simple just import it into your favorite slicer software i'm going to use kiro set my resolution to 0.2 in full of 80 and put this thing with a broom you can decide if you want it on a raft or abram right copy to your sd card and let's get printing [Music] awesome if you put it together it fits like a charm now we're just going to spray paint it with some inconspicuous black spray paint to give it that stealth look cool so first up head over to the github repo github.com augmented startups open cv ai kit apps there will be a link of where you can download all the necessary files that we'll use in this tutorial series so that's app one to five where you can check out all the links up here for the tutorial playlist so click on this link over here and download all of the files to your raspberry pi and then i also want you to get clone this repo where you'll find the base and final code of the app the base essentially is what we'll use as the template of our code and the final is in case you just want to copy and paste if so you can skip ahead to the integration part of this video once the file has downloaded we can go ahead and extract all the files if you don't already have depth ai installed from the previous videos you can go ahead and install it using python 3 dash m pub install dash r requirements.txt okay cool so once we have our clone repo we can go into there right f1 and copy the base code from there into opencv depth ai go down and paste if it asks you to overwrite just say yes now you can either open up in tony python ide but if you want to be fancy like me we can use pycharm so over here what we have is the base template and most of the code that you'll find here is straight from the luxonis depth ai code we have just modified it a bit so that it will work with our apps you can essentially use this code as a template for your future projects for now we're just going to link up an led and project sound through a speaker when a person is detected so over here we have all our imports don't forget this one which is from play sound import play sound this will allow us to place our audio through our 3.5 millimeter jack to our speaker okay now under global arguments so we're going to set our task to play the sound set this false so right now we don't want it to play any sound as yet and we'll do the same for start led set this to false cool next we have a function for turning on the led so when this function is called what we want is to set our output and open and that button will be band 16 as we mentioned earlier set our gpio open to high and then we want it to stay on for around 14 seconds and then switch off so gpio dot output and set it to gpio dot low cool so that's our first function done and then if we go over here to play sound we're just going to create a simple line of code that will play our audio file so we'll just say play sound audio file simple as that now i want you to scroll down all the way to line two seven two over here and what we have here is a way to access our detections so we're going to get some entries from our oak get camera and we can choose to run either apps 1 to 5. now as mentioned before apps three to five will be only on my opencv ai kit course which will focus solely on the practical applications of how we can use the opencv ai kits to address real world problems so app one is our security camera which will light up our led and project sound to a speaker updo will be our social distance that will send an alert it will essentially play an mpg when people are not maintaining a distance so f3 will be our mass detection and it will allow a person through a gate or not so if they're wearing a mask the gate will open if they are not wearing a mask the gate will close f4 is our push-up detector using pose estimation and then f5 is our employee face logger so if a person will clock in or clock out using facial recognition okay so let's start with app app1 so when detections occur we want to look at the labels of each detected label and if that label string equals person we want to also check if our arguments are to play the sound and then check whether our boolean is true or false so we initially set our task to false and we only have one and we only want to execute what's over here in this code when these arguments are true so if this condition is met what we're gonna do is still keep our task to play the sound as false next we want to set our audio file path so audio file path equals to os dot path dot absolute path and because we're storing it in our immediate directory we're just going to put that dot for slash emergency 003 dot wave you can play any sound file that you want and you can also store it in that same folder next we want to create a trade or trading dot red and this will play our target sound target equals play underscore sound and put in our arguments equals this audio file path put that in brackets comma and then we want to start okay so that part is done now we can move over here to this line of code so over here we just want to check if our device is a raspberry pi and if so we can check our cn model is set to f1 or string label we want to check if that is a person and check the boolean called task start led and cola so if this condition is met we're going to also say task start set our led to false in case it's on by mistake and started drift to turn on our led so trading dot grid target equals turn on target equals turn led putting a comma there and then our arguments equals which is our gpio and the pin and then set that to start okay so head over here to line two one five and this is where we're gonna add in the code where we can specify our gpio outputs we're gonna put in an if statement is underscore r5 so if it's a raspberry pi we want to import our pi dot gpio as gpio we're going to specify open to be pin 16 and set up the gpr parameters that's mode gpio dot board and then repeat that we're going to put that as pin and then we're going to specify the direction of open which is gpio dot out because we are driving an led we can set it as gpio dot out and then we seem to be missing a while loop so but while self dot run thread and that should be it make sure to save your app okay so open up terminal so we can run our app so the command to run our app is python three run underscore app under score you can choose between base or final make sure it's app one final dot bye we're gonna put in dash dd dash cnn app one so we're running f1 and we can choose to run our sound dashbs okay cool let's run it just make sure you set your volume to a bit lower because the sound might get a bit loud go if you move it around person's detected you can see that we are detecting me you can see that we're running at the smooth 27 frames per second real time baby now depending on your input or your configuration you can select over here the audio if you right click it you can select between analog and hdmi so if you're using an hdmi output you can select hdmi or fire your auxiliary cable which is analog so when the person's triggered you can also hear the sound okay so now we have our hardware and software sorted out let's try mounting it somewhere and see if it works [Music] we have power using a power bank but not so much flexibility for an ethernet cable around the house luckily we have wifi on board so we can view our output on an android device via remote desktop i've connected my portable speaker to the raspberry pi to sound an alarm and i can also have a long lead that will notify me when someone is here testing it out we can see that when a person is detected we are successfully able to sound an alarm and light up an led which could later be upgraded to a floodlight now what if you want to train not only for people but for other objects like animals or to taking packages at your doorstep well i had a chat to the ceo of roboflow joseph nelson and he recommended that i check out their blog which walks you step by step through the training and deployment process for the oak devices you can check out the link here at blog theroboflow.com deploy luxonous oak and the link will be down in the description now this will take you through the training process right where you can collect label organize process train deploy and display your model on the oak device so i definitely recommend you check it out but first if you're doing package detection to avoid your packages from getting stolen you'll need a data set which you can find here at public.roboflow.com object detection packages which you can find here at public.roboflow.com so over here you can discover a variety of datasets to choose from and also where you can add in the classes and manage your datasets so to get the particular packages data set head over to public.roboflow.com object detection packages dataset so make sure that you have signed up to an account so that you can fork the data set i'm going to click here for dataset click again and wait for it to complete so on this roboflow web app we can see that we have the amount of images that i mentioned we have the images that we can click on we can also see our train test split so 77 percent of our data set is trained 12 is valid and for this it is 12 we can also pre-process our images so to auto orientate our images so if the image is in the incorrect rotation or orientation this pre-processing step will sort it out we can also resize our images to a certain resolution over here we have augmentation output and we can select how many augmented versions do we want to generate and this will determine the amount of augmented images that we have so we can change this to four you can see that it increases our output size let's set the hours to five and click save now under augmentation options we can add in a variety of transformations so flip transformation either on the horizontal or vertical axis cropping you can see how much we're going to drop into or out of the image we can edit the rotations to allow our model to generalize better as well as brightness and blur transformations you can even add in your own augmentation steps over here you can make a grayscale you can saturate it even more or you can add in a mosaic of images together so i really recommend that you play around to see how this affects your model so once you are done here you can click generate let's create a version name let's call it oak packages v1 click generate and wait for it to finish awesome now you can see that we have around 234 images which have been augmented and we can specify what output we want our dataset to be in now because we work mostly with darknet and yellow v4 we can select darknet yolo or yellowdarknet over here you can specify whether you want to download the zip to your computer or show the downloaded code let's download it to our computer awesome so once downloaded you can see our test strain split and also we can see our data set in yellow darknet format this is really awesome right so the value of roboflow is that it allows a limited amount of data and increases the representation of our data set thus improving the value of this dataset using data augmentation now for those who are interested in object detection object tracking pose estimation buying your own oak kits or accessing the github code all the links will be down below also click this link up here to go to the next video in the series where we will be creating an app that creates an alert when people are not social distancing we'll also create an app 3 with mass detection to open a security boom gate some really cool applications coming up so you won't want to miss this lastly if those tutorials really interest you then i recommend you hit that like share and that bell icon to get notified of the moment when those tutorials get released cool thank you for watching and we'll see you in the next lecture you 