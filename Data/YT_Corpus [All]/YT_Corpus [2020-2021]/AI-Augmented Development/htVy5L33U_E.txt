 I know the title of the session and the title description in the session thing makes no correlation, but I would love to share an awesome story with everyone here. My name is Sweeky, I work for Microsoft, I love Xamarin, and I got recently really into Pokemon Go, and then got really, really into Wizards Unite, the new Harry Potter version of Pokemon Go, so clearly I really like mixed reality. I've been walking around the conference area for all of yesterday and dropping blue boxes, so I've lost all my blue boxes. These aren't just regular blue boxes, these are clearly virtual fake real boxes. But they are all over the camp, all over the conference area. In this session, I'm going to show you how I created the two apps: one that drops these boxes and the second that will help all of you, help me find my boxes. Yes, it's a game because I love games. What are we actually talking about over here? This is the cool world of augmented reality. I'm not talking about VR with a headset on, we're talking about augmented reality, so any device with a camera on it should be able to project models in points or locations where it's supposed to be there. So as I have that cool little Jeffer is showing off over here, the way I like to describe it to everyone who are not familiar with how this works, it's literally just using your camera frames as a canvas and just drawing objects on it. Now, I'm not the greatest graphic designer in the world, therefore we have blue boxes. But if he were much better at 3D modeling, you can put any object that i.e, Harry Potter characters or Pokemon. The way the support works right now, this is tied into each native platform. So iOS has its own version that's called ARKit and Android has its own version, that's ARCore. These are support native binding provided to us from Android and iOS that support the whole concept of, "Hey, I have a device with a camera. It's providing me frames of information of real-world, and now it has enough data points that it can start drawing things on it. So this whole section of the stack is controlled and powered by native components basically. Now let's step into the world of mixed reality. We've had a bunch of amazing talks through the conference about cognitive services, about image services, all provided by Microsoft. In this talk, I'll be talking about a another service that's provided that's called Azure Spatial Anchors. It was demoed at build, it's extremely cool and there's a star next to it because it's still in preview, but it's available to use right now, you get a one-year free subscription, you can just set up a free account. So what's the point of Azure Spatial Anchor? I just told you that each native platform has its own version and they can do it. Where does Azure Spatial Anchors provide value? Well, with Azure Spatial Anchors, you can have all of those precise points of interests, but you can share them across different devices. So you can have a team with a bunch of different, one person could be on a HoloLens, another one could be using a UWP app, another one could be using an Android phone, but then everyone involved can be able to see that rendered 3D model through whichever camera they choose to view it through. That's the beauty of Azure Spatial Anchors. It's been really really fun. They've had support and there's already samples available online right now that support native iOS, native Android. There's a Unity sample and there's one for HoloLens. But we are in the Xamarin Dev Summit here, we're not at WWWC or something, so here's the Xamarin support? I've had a lot of people ask me that, it sounds like, "Let me go find out." Why is there no Xamarin support yet? So good news. There is Xamarin support. They've actually actively been working on it. They just didn't have it ready to release until now. Not a Keynote announcement. Thank you. But I'm super super proud to announce there's this beta bits available. Currently, we're limiting that access only to MVPs. I'll have a link up later on, anyone who's an MVP in the room right now or watching online, hello online friends, please go in and sign up and we can get you beta access to these bits. But don't be sad, I skipped ahead, I was going to have this at the end but I want to just say I'm too excited. We'll have public access for the Xamarin support in early August. So please keep an eye out. Everything that I'm showing today and demoing today will be available to you in early August, and I'm doing nothing fancy, trust me. It's really basic but it's so cool. Back to the power of where Xamarin and Azure Spatial Anchors could live. So the idea would be Xamarin will have drive the native bits, run it on your devices and the backend. The supporting services can be a Cosmos DB, can be Azure App Service. You can also extend this based on what you're trying to apply this to. You can also have an Azure AD-based access only. There is a lot different, depending on what you are trying to achieve, it can support it. So describe a lot of cool features. You have a lot of power here. What can you do with it? Some ideas, I will be honest, the first two points are not me, they're from the Spatial Anchors outside itself, but I found it really interesting. A shared calendar app. If it was me I'd make it a shared DevOps task board but let's do share calendar apps. It's super-cool. I love this concept. The product design in 3D models it's talking about IoT devices or big factories where it's hard for people to go down to the actual equipment to see what's going on, but they figured out rendered models which has IoT devices, giving back information and someone sitting in an office because they're able to look at what's going on, see readings, and charts and figure out a plan of action before actually going to the model, do the actual machine itself. Building mixed reality, and like I said, Mixed Reality IoT solutions, I'm very excited about this space. I don't get to work on it every day, that's what I love doing, talks like this and let me explore. It's awesome the kind of things you can achieve with this, but like I shared before, migrate idea for this is I want to build a virtual treasure hunt man, and that's why I lost all my boxes and all of you are going to help me find my boxes today. This is version 1. It says Alpha of my treasure hunt app. If this works, I promise you I'm going to have an amazing version of it out soon. We'll all play all over the world. I also want to make my own version of Pokemon Go. Did anyone have those on virtual pet things? Do you remember those? Now, imagine that with Azure Spatial Anchors, it'd be so cool. You're going to have your own pet with you-all the time: you can check on it, clean its poo, get your friends to clean its poo, because it's shared I can just give you access to my pet and you take care my bet for me. I think it's great. I love it. It's the best. So I described my game to you, so I'm going to show what the code looks like and I'm going to show, hopefully, my Mirroring apps work. But if you want to play along and you want to help me find my boxes, I'll have the links up again, but that's where you can go to get the Android app or the iOS app and you can join in the fun and explain how it works for you, because like I said, it's Alpha, it's not like the greatest Xamarin app in the world but it works. Let's see how it works. Let me get our PowerPoint. Let's see. So the first thing you want to see is, okay. So what I have opened here is MVPs who already may have access or when you will be given access. This is like the sample basic app that's available. The way it works is, I don't want go into the detail of the code, I come back, yes. Whoa, I had a Donovan Brown moment. This is amazing. Where most of the magic happens is, this is, okay, yes, it doesn't have my packages, oops, but if you ignore all the red squiggly lines, this is literally all that's going on. After you scan the environment, like I said, the native support does a lot of the work for you. Azure Spatial Anchors basically just collects all that data and just makes it understandable between the different platforms that supports. So all you're basically doing is once you have scanned your area, picked up a location and dropped appoint, it calls them anchors. It just basically takes that anchor and creates a new Cloud anchor for you, and then there's a second step involved in this version of the app where I save then I take that unique ID associated with that anchor and some I save in my Cosmos DB. Thereby, I can share those in all locations with you and then you can go hunt for those same spots. So the API docs implementing this is super-simple. It's really really straightforward. If you like your fantasy exactly what is going on with the adding and retrieving and sending the anchor itself, oops, what did I do here? Don't touch the screen. All right. So literally again, this part of the thing literally just grabs the response and puts it in my sharing services, grabs that Anchor ID that I was talking about. I'll be very careful here. The next bit, like I said, Azure absorbers or Cosmos DB, whatever your back end wants to be. You grab the Anchor ID, do all the magics to it and it's a rest call. So it posts it to the system and retrieving literally works the same way. So let's call admin and client. So the admin that I use is doing all of the posting and the finding aperture which I've shared with all of you, is the one that's going to go and find these Anchors. That's going to do the retrieving bit. What else. I wanted to show off something else. Yes. Now I'll show off something else. So why make my life easy? I've been talking about all this Native support. One day I asked myself, I wonder if I can do this in Xamarin.Forms. Well, good news. You can do this in Xamarin.Forms, you just make Native Page Renderers for each platform, because like I said, this is Native support, right? So fun fact, the client app that I use to drop these Anchors was that sample that I just showed you. So that's all Native, it's in Native iOS and Native Android but the apps that all of you have hopefully downloaded and will play along with me are the Xamarin.Forms version of what you're literally seeing on the screen right now. So what it's doing is again, really really simple. It's just taking that same Native logic but it's just all happening inside a Page Renderer. I didn't go very complex with it but like I said, since this is like just an alpha version, this is just some what I think could be a way this could be done. So all I do is, once I get the response that the user has put in Anchor IDs, say number 10. So I have retrieved all of the ASA Anchor information and now the system can process it and be like, okay, so this is a spatial Anchor, when you're moving the device and on the screen, if it finds this location matches where this Anchor is supposed to be, it's going to Render it on the screen. So that's all that's happening in Anchor looked up. So it like looks, it checks, I've received session, I have the frame that I'm getting from the camera, correlates the data it matches, it'll generate your Anchor for you. So I honestly thought this would be a lot more complicated. I'm going to be very honest. I thought I'll have to do rocket science, like figure out ML and go and learn what these Anchor points are and understand, it's reading all this frame data, what do I do with it. You don't have to do much. The team is amazing. All you have to do, just pull in the awesome [inaudible] package, it does all of it for you. If you're curious on how it's working, just go explore the package, go check out the docs, checkout the API docs, amazing, really, really well done. I was able to make all of these samples by just reading the documentation and following their Native sample reports, that's it. I didn't do any fancy rocket science over here. I'm not a great Xamarin delve, all of you guys are. It was amazingly simple how I was able to implement this whole thing, I loved it. So there's a lot more of the code and like I said, I cannot share these, the whole sample report with you right now. But MVPs you can have a look at it. But I promise you early August, as soon as everything is public, this whole report goes public and you can all pay along. Please give me PRs and fix my random models for me, it's my humble request. Let me see if there was anything else that I wanted to show. Yeah. So I had this question, right? When I was writing this whole thing, like I said, if you're new like me and have no idea on how the ARKit works or ARCore works or how a Xamarin support for ARKit and ARCore is, I legit just went and read these two blog posts and I have links to all of this. So this is Jon Dick in the Xamarin Android at Xamarin Component Team. This is his blog posts on how Xamarin works with ARCore. He walks through the whole thing, it's super simple, talks about how the API basics are. Like I said, I love the fact that was very similar between the two platforms and that's where I think the understanding was easy for me. But it literally is the sessions that is basically all the frames coming in through the camera. Each of the Native scates literally just like scans through the area, look for flat surfaces and you can see like cool little dots come up that show you, okay, yes it's figured out, these are the surfaces. So there's the HitTest, which is the same in both basically just does a mathematical calculation within the coordinates, to see this is where I'm supposed to place the Anchor. Yeah. It's super simple. This was the Android AR walk-through and we have a, let's see if, yes the website is open for me. This is the similar walk through. So we have an iOS 11 ARKit blog posts, but this is iOS 12 ARKit blog post, the updated one and same process. Larry walks through the whole thing, his sample code is amazing, his Rendered model is a lot more prettier than mine, because he actually got Xamagon's on it. If you notice the name of that app is Xamagon hunt. I failed you-all, I couldn't get my Xamagon on. But there are blue boxes, I promise you. Yeah. Again, I literally just read these two blog posts and I was able to implement the whole thing. So no rocket science. I'm super happy, I hope it makes you happy. Okay. So let me show you what the app does. All right. This is the moment of terror for me always. This is the first time I'm doing it with this device, so I guess that's why it's. Okay. Let's unlock this, all right. So this is my personal device, so please ignore all the messages my mother's is trying to send to me right now. This is the first sample that I showed you. So this is the sample in the report that will be provided as part of the Azure spatial anchor sample. This is in the same format as all the other native samples, so in case you're confused on what's going on, you can always correlate, this is how they did it in Native Android, this is what's happening in the Xamarin Android version. So I m going to go into the shared demo, because that's what we're doing right now. Let's see if I can get this to work. So I should have probably rehearsed this, I should have tried this out, shouldn't I? Okay, I'm going to hit "Create". I have faith. Okay. You can see the little progress bar at the top. That little animation goes away. Like I said alphabet with me, when you move the device it's supposed to just say like hey, move the device around, you know, just try to pick up data points and scan the environments so it understands where it can or cannot place objects. No, I reset my whole progress. No. Please watch me awkwardly not move from this area. Let me try the table. Awesome. Can I place it here? No. Can I place it here? All right. Giant blue ball right in front of me. So the changing in colors is just to show that yes success. Yellow the Native system. Yeah sorry. It goes away because it's done with the process but just if you remember that little yellow ball, it was yellow to say that the Native system has dropped the Anchor. The green means saving was a success. This is Anchor number 12. So I'm not going to use this to look at that anchor. We are going to play my game now. So let's switch over. I'm going to kill this. Let's move over to QuickTime. Anyone remember the Anchor number. Twelve? Twelve, yes. Okay. This is what I have to remember the Anchor numbers. You have a device. No, you don't have my face. Let's go to my phone. All right. Interesting. Did I go delete my own app? No, I did not. Okay. Here it is. So this is Xamagon Hunt. If any of you downloaded it right now, this is what you are seeing. It's so informative, isn't it? It only explains how this game works. No, it does not. The way the game works is, if you click on that little thing that says, "List of Anchors? Click Me". If you click in there, it goes to my GitHub repo right now. If I scroll it down, I have a list of Anchors and a little explanation on how the game works. But I'm going to show you how the game works right now. But if you forget, you can always go there and read it. The way it works right now is you hit "Start" here. You see a little text box that says "Enter Anchor Value". So I did number 12 right now, so I'm going to go put in number 12, and I'm going to "Tap" to locate. You can already see that it will debug buttons of ARKit in the background. It's already scanning the space. So I'm going to hit "Tap to Locate" and pray to all the gods that I know that it finds my ball. So let's move this. Guys, it found my blue ball, it's now turned into a sphere. But now, let me see your ball whatever. So this is supposed to be a Xamagon. I swear, I tried really, really hard. I need to go talk to my sister who is actually a graphic designer, learn how this works. But so yeah, that's the game. We're all doing it because in real, there's no marks on here. So apart from how cool and fancy this looks, all I want to highlight is, this is the power of Azure Spatial Anchors. So now, imagine all of you have that app right now, you can come up on this stage and you can look at this anchor right now yourself. Number 12 is the one on the speaker's podium. So it is really cool for all those actual enterprise applications that I was describing about. It's really cool to make Virtual Treasure Hunts for your friends. It's really cool to make Pokemon Go if you want of your own. But conceptually, it was really, really simple to implement. So if I were to just now look back and think what I had to go through to make this, it wasn't much. Go to those Spatial Anchors portal, start a new trial, open up a new Xamarin project, add in the NuGet packages. If I were to go by the native approach, which honestly I think was much more value than the Forms Approach was, now that I did both approaches to give you my honest feedback, I think the Native Approach was really, really nice. So going there, setup ARKit, have it render your frames, setup ARCore, have it render your frames, as soon as you can get Local Anchors working, that's it. Hookup Azure Spatial Anchors, set up a back-end to save your anchor points, and share with you friends, which all of you are now of me. So if you now want to genuinely play the game with me, to figure out whether Anchors are in around the conference space, so if you just look at this list, I went in and put in the number. You just have to go in and put in that number and be in that space, move the camera around a bit. I know of two known issues: Number 1, the app can crash sometimes, I apologize in advance if that crashes on you. The second issue as seen is, if you're too close to the object, sometimes it doesn't detect it, but that just might have been me because I can't do math and there's a lot of matrix math you have to do for this. So I might have just picked up a wrong X, Y coordinate and that's why it's not picking it up, but just walk around the area. So the descriptions are all in there. So the Blue Door Cafe was my first, first test, literally the day I landed just to see if the concept works and it did. So the other Anchors are all around the space over here, there's two with the Microsoft booth. So if you come and find them at the booth and come and show me a screenshot, you're going to make me really, really happy and I will appreciate that. Yes, so please play the game. Like I said, I promised as soon as a Spatial Anchors is live, my code will be live, and you can all join along. I'm going to switch back to the presentation. So key takeaways, because I love when everyone else has these in these slides and I want to be a cool presenter now. So the actual Anchor Object. Sharing it between the two platforms is not complicated because like I said, Spatial Anchors has been really smart about just trying to make our life easy, because they try to share it across a lot of different devices. So the two main bits you'll constantly be keeping track of in your code is the position, so that's literally where the anchor was dropped. Each native platform provides all this information for you. You're not doing any crazy things to get it. It's just something you have to knowingly extract from that local object. Just the information, what you wanted to render which can also be shared between the platform that you want. So for me, it's just the colors change based on what state you're in. I do yellow, you dropped it, it's processing the point. Green at success, it's saved your point. Another thing you need to remember is the Power Platform. Like I said, ARKit, ARCore, SceneKit, SceneView in Android, they already do a lot of the bulk of the work for you. So it's literally just initializing those views and just feeding and reading data from it. Then like I was talking about, you move around. You have to have a minimum amount of data before it can place objects for you, so that's a little progress that was showing up. So you move around the space that you're in and you collect all that data, then you can place and remove your object. In my ASA Common that I showed you, that common shared folder, that's where I have all the rest API calls to give the Anchor information to Spatial Anchors and also put it in my Cosmos DB right now. Like I said, shared service, you can go crazy with it. The samples give you an app service example and a Cosmos DB example. I literally just did everything that was in the sample code. I have not done anything special of my own over here. This is what will be available when this goes live. So like I said, you can add a login control if you want to do that shared Azure DevOps board maybe, and only give people in your team access to, "Hey, look at what's going on in the board?" Integrated IoT devices. There's a lot to do, this space is amazing. So I jumped the gun on this. But like I was saying in the beginning, our examined support is coming. It will be out early August. MVP is again watching online in the room right now. You can go to this aka.ms link, it's a form fill out. Azure Spatial Anchors team will reach out to you and give you access. So you'll get the early data bits that's literally what's driving these apps right now, that I demoed. You'll get access to that sample code, again, which is literally driving what I showed right now. So early August, everybody else will get access. All of this will go live. They're working really hard on the documents and on the sample. So MVPs, if you are opting into get access, please provide feedback. I've been giving feedback to the team. They love hearing feedback, they just make everyone's life a lot better. So the public launch for Xamarin.iOS and Xamarin.Android is coming in August. So questions? If you just want, come up to the mic so everyone can hear.  Does the SDK provide the custom renderers for the view in each platform also, or you have to do that yourself?  So are you talking about that Forms sample that I was showing?  Yeah, with the dots in the camera view.  Okay. Got it. So those are part of ARCore and ARKit that actually get shipped with Xamarin right now. So you don't even need the Spatial Anchors support. If you want to just make a simple Local AR-based app, the two blog posts that I showed, they talk about how we already have that baked in Xamarin right now. So when you have Xamarin.iOS and Xamarin.Android in your projects, you can literally go initialize AR view, and it'll get all those dots and everything, and you can just start dropping local Anchors. So that whole section of the app is driven completely by them. So it's already in there, we have support for it right now. Any other questions? Yeah, sure.  Does it use GPS at all so it can track everything? Because I know if you're in a building, I don't know how it would differentiate between the different anchors.  Great question. So I'm not sure about the intricacies of how and what data the actual ASA system tracks. But that Cosmos DB back-end that I was talking about, you could always add in columns and track that data yourself. That's how I would actually go about with version 2 of this game and tie it and have. So right now, I'm not storing GPS location because I don't want to GDPR, I would have to give all of you privacy, "Hey, I'm taking GPS location." But I think if I were to have to recreate a Pokemon Go type game, maybe it got at monkey hunt or something, but yeah, I will probably have track GPS location. Then I can actually give more notification than information in the app itself, "Oh, you're getting warmer or you're getting colder." This is close because yeah, I mean you can always correlate that data I would think, so yeah.  So when you're dropping the shapes, are you dropping them based on like you're physically dropping the shape with another device? So you're dropping the shape with another mobile device with that's running the Xamarin app, and then you're catching the shape with another device. Instead of dropping it with the GPS and some elevation data or whatever [inaudible]  Yeah. So that's Azure Spatial Anchors, the service tracks and figures all that out for you. So you don't have to think about, "Oh, for this X and Y coordinates, do I also need to capture depth data?" Or, "Is it on the floor versus on the stage versus on the sail?" They figure all that out for you. All you're doing and providing the data system is just the unique, they give you a unique string that they attach to each of these anchor values, and that's all you provide to the service. That's just a string and that retrieve and send API call that if you see in the code, that's all that does. So the system does all of the magics for you. You just literally just in the app itself, you're just doing touches and you just track where the user dropped it, and then the system handles all of it for you. Awesome. Anymore questions? Now, are you are going to play my game?  Yes.  Yes, awesome. Everyone online, I'm going to have a version up for you very, very soon. So again, thank you-all for coming. If you have anymore questions, you want to discuss some other cool ideas or how I could make this app behave better, please come and share it with me. I'll have the slides up to share. But again, just want to show that link for MVP access. If you want to sign up, please do go ahead and we'll get you access to those data bits. Early August, friends, early August, everyone will get access to the Xamarin bits. So yeah, thank you so much. 