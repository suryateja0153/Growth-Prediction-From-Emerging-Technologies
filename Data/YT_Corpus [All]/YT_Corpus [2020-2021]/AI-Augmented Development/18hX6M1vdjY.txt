 Autonomous driving has made enormous progress over the last decade, but the current generation of cars only work in predictable environments, out in the real world things start to go wrong. There are five levels to vehicle autonomy with the uppermost two requiring an unrealistic amount of computing power with the limitations of modern artificial intelligence. However, there is another way. We have built this autonomous vehicle prototype with the development team at T-Systems. We demonstrate how in the Fetch.ai multi-agent system agents representing people, objects and things can search, negotiate and transact with each other independently. So rather than a vehicle having to decipher a passive environment the signs and other vehicles speak to it. Vehicles have one or more software agents that search the local area for information relevant to their journey and they can see the world in a sort of augmented reality. What might start as a simple communication between signs and vehicles, opens up new economic opportunities as agents become more knowledgeable and signs become value centers in their own right. The upper levels of autonomous driving may finally be realized. With Fetch.ai true self-driving cars are just around the corner. 