 hello everyone my name is Alberto Rodriguez and this is robotics today an open series of technical talks in robotics I'm happy to be here to introduce the kickoff of this series with Andrew Davison but before that whoever you are wherever you are we hope that you and your loved ones are doing well and staying safe you know Kovac has challenged the way we live and the way we work and the way we relate to our communities including robotics and sort of that constraint is given as a great motivation to try to explore the the online format and see how how much we can get out of it for all of you that all those of you that are listening right now thank you if you're not we're recording all these and we will make the video available in YouTube later on so without more delay I'm pleased to introduce and welcome Andrew Johnson joining us I'm guessing from home I don't know and before that I want to welcome also Joe Leonard who will be with us and the panel discussion after Andy's talk and who we've asked to prepare a spicy question to kick-start that panel discussion so we are looking for that John thanks for joining from MIT as many of you might know under you he's a legend in the perception community is a professor of Robotics vision and director of the Dyson robotics lab at Imperial College London and he has developed several perception systems that have been widely adopted by a big part of our community maybe a bonus lamb and kinectfusion are the most famous ones and he has also deployed some of his algorithms in real robotics systems including intelligent robot vacuum cleaners but most of all I would like to highlight a couple things from his trajectory and the large influence that it has had on many of us right so on one hand Andy has been able to keep up with the huge transformation that sensors have had in the last couple decades right so if you look at the two most influential works or the two more cited works in his career maan Islamic electrician they drive from very different sensors and somehow Andy has managed to stay at the forefront and on both ends of that transformation which is quite remarkable and maybe the second thing is that Andy has also worked us to do another transition that the perception community has undergone in the last two three decades going from a maybe a more narrow maybe narrows a slightly narrow perspective with geometric perspective on what perception is to a more or brother sort of semantic understanding of what perception is or what how he calls it a spatial AI so without more delay Andy of course the cliche the the long-standing cliche says that slam is a soft problem but somehow stubbornly it seems like it remains to be a soft problem how is that why is that Andy thank you for joining us so say thank thanks very much yes so it's a it's a problem that I think has has continued to evolve into more and more interesting and challenging areas so as you mentioned Yunos sensors change you know different sorts of you know the background algorithms and tools change and our goals in applications change and you know one of the messages I want to get across in the talk is that I see slammers as a rolling problem that that's always changing into something even even bigger and that's what I'm now calling spatially I excellent thank you so I'll share my screen and start yeah yeah okay yeah sure you're sure give me a second okay so my slides are coming through okay yep good okay so thank you very much for the introduction Alberto thanks to all of the organizers of this fantastic new series so of course it's a great honor to be invited to be the the first speaker and I'm you know excited to watch the rest of the talks in what looked like it's going to be a great program thanks also to to John for joining the panel so John's someone I've known him and followed his work we know right since the time of my PhD okay so my talk is called slammed from slam spatially I and I've already said something about what that means so I'll go into a bit more detail on on that I'm going to talk a little bit about a little bit of history not not too much a little bit about my general thoughts about where this whole field is heading and then also show you in a various example was in particular from recent work in our lab or systems that I think are representative of progress in in this area so let's start by thinking about what's actually out there at the moment so having worked in this domain now for you know over over 25 years since I started my PhD when working on slam using vision was really just a bit of a and out there research idea we've now reached the stage where visual slam is actually presents in a number of products that you can actually buy so I've shown our selection of them here you know some of these are certainly what we would think of as robots so the dyson robot vacuum cleaner I'm showing they're drones such as companies like DJ or Skye do these you know run visual slam systems for localization and mapping but then there are also a lot of other devices that we might not think of as robots so inside mobile phones there's software like a architour or a our core that enables positioning and real-time mapping on those devices using using cameras and other sensors and then this emerging area of headsets for the virtual and augmented reality these are also slam enabled devices so this is you know kind of an amazing thing to me but this technology is really getting out there now but if we look at what's actually in products like this it's mostly at the moment positioning and perhaps sparse or semi dense reconstruction that that's the technology in inside this product so there's more and more levels of technology gradually coming in so things like dense and semantic mapping that I'll talk about later a bit based on vision or also entering these products but I think it would be fair to say that these are all still relatively you know early sort of products out there on the market so one way of thinking about slam is is to think of gradual progress in terms of adding levels of compatibility so this is a picture that comes from our our startup slam core where we really think of levels of capability in slam from localization up through dense mapping and then eventually to semantic understanding so I think of Slammers this really you know rather than saying slam is something that's done and then these levels sit on top for me all of this is is slam and it's gradually evolving into a bigger system and and for me that's a valid way of thinking about it because I always come back to it slam like ways of thinking whenever trying to add these new layers so every time you add a new capability it may affect you know the ability of the robot to localized for instance so I see it always as a kind of closed loop and hence map my argument that slam is evolving into something bigger spatial AI which I define here as as the online problem where vision is to be used usually alongside other the sensors as part of the AI which permits an embodied device to interact usefully with its environment so really we're thinking about embodied spatial AI in a general sense here so where might that take us in in the longer term so I've just got a couple of ideas here of products we might imagine in the longer term and the first one here is in robotics and I should first say very clearly this is just a mocked up image I found some on the internet and is any certainly not representative of any products that are specifically under development but let's imagine this products a mass-market general-purpose household robot so it may look nothing at all like this probably probably won't look humanoid it's all a lie I strongly expect but it's you know its job is cleaning tidying of complicated rooms and objects and it's going to need a spatial area a system that gives it all the perception that it needs to do all of that but let's bear in mind that the system that it has will be constrained by price aesthetics size safety power usage and all these things which must fit within the range of a consumer product here's another product that I think could really be a breakthrough and note note that I've called this a spatial iae product so this is a rather cute inversion of AI to stand for intelligence augmentation so I think of this as a sort of intelligent device but that's designed to work with a human in the loop so this is a concept or a future or granted reality system with the form factor of a standard pair of glasses it provides a robust and accurate real-time overlay in spatial memory of all places objects and people but it has very very strong constraints on how it can work so I think for this to really change the world it may have to weigh 65 grams and have all-day battery life so I'd like to make the point that you know I think both of these two types of products they probably require quite similar capabilities from a perception system I think either of these products would have a massive impact if they existed the reason they don't exist yet is not because there's no demand but because no one yet knows how to build them and especially within those very strong constraints of real products so I've just got a sketch here of how far I think we've got to go we've got current products on the Left we've got current research prototypes happening in labs and then we've got this big big gap to where we really need to get to this full spatial AI performance within the constraints of practical devices and I and I think we're talking about orders of magnitude of progress needed so thinking about this concept in general of a spatially ie so I think it's perception for it for embodied intelligence I've written recently a kind of discussion paper about this whole topic that anyone might be interested to read so it's called future mapping the computational structure of spatial AI systems it kind of defines more about what I understand this this problem to be it makes a few hypotheses and that my the key hypotheses are along the lines that you know a spatial AI system should build a persistent and understandable seeing representation which is somewhat close to two metric 3d geometry at least locally within that it will contain hierarchies of you know lower-level geometric information and high-level semantic information and I also think there's a generality to spatial area system so whichever of these applications this might be ultimately useful for I think there'll be a lot of commonality between the systems and maybe a fairly small number of performance measures that define the differences so I also think that by working on on this topic with the main goal of actually developing products we may learn learn a lot about general embodied intelligence I think that's something very interesting to discuss so what are the kind of computation and storage patterns that are required to do embodied intelligence efficiently so so that's I think interesting general research on AI and telogen s-- so i'm going to move on to a little bit of history here so really to guide you a little bit through these levels of progress that are that i've talked about in in SLAM so just very first briefly so probably john may be the only person here that actually remembers this work with this this was my PhD work which was in the act of vision lab with david murray in oxford back in the mid 90s so i worked on what turned out to be one of the very first visual slam systems so this was a stereo active head with two cameras mounted on top of a mobile robot platform and we were able to achieve localization and mapping with this system by having it drive around and and map a set of point landmarks in a scene so on the right you see a picture of the estimated trajectory of the robot and the estimated positions of a set of landmarks that it's that it's built into a map and this was all running in in in a Kalman filter for joint estimation that led after a few years on on to the mono slam work so let me just quickly show you a video of this so this was really inspired by saying let me just go back a second so it was inspired by saying can we do slam with the simplest possible vision based hardware set up ie a single handheld camera moving around in a room with no other kind of sensors and ideally no prior information about the room so the goal is to localize this camera in 3d as its moved around in a general way in real time and to build a map of the scene that it that it sees so here you'll see the camera's view of the scene and you'll see that what's going on here is we're detecting and tracking a set of landmark features in the scene and so these are salient points in the scene and we're building them incrementally into a persistent map so if I just skip forward slightly here here you'll basically see the result of that so this is the real-time 3d estimated map of the scene where the weather count the box flying around is the estimated position of the camera and each of these patches is an estimated feature in the world with a 3d ellipsoid so this is a sparse feature based map of the scene and really you'll see that we're not getting much information about the scene here but this is a map of the scene which is enough to enable localization of the camera so the main thing we're interested in here is is where is the camera and this is a map that's sufficient to do that so that system or something very much based on that is what enables the slam capability in in the Dyson 360 I robots or around the time of mono slam I first met some of the researchers from from Dyson and we we decided to work together on what seemed like a very ambitious concept at the time of building a product that did visual slam and that eventually led to the release in in 2014 a bird of the Dyson 360 eye robot which performs monocular slam so a few years after that and now we're talking around 2010 I think there was the next big kind of advance in real-time visual slam systems and this is the arrival of dense slam so the goal here was now starting to switch towards you know we're doing pretty well on positioning a camera how about this next thing of getting more significant detailed information about the scene so can we actually build these really detailed scene representations where we can see that their shapes of objects and so on so in particular an important person in this was rich in youcome who was a PhD students in my lab at the time and he works on several very influential systems including D term which which was a system just based on pure RGB images and was able to do dense real-time mapping and then the other important one was connect fusion which used a Kinect camera so you know what's really interesting here is it was the arrival of certain commodity technology that made these kind of things possible so in particular GPGPU so general-purpose and graphics cards that or programmable to do computer vision like things are behind all of the parallel processing that you need to do this kind of real-time dense visual slam and then of course depth cameras and especially connect and it's hard to remember now what a kind of revolution that was when it when it came along so depth cameras had existed before 2010 but they cost thousands of dollars and they weren't very good and suddenly you could buy this thing for $100 that there was there was great so you know Richard actually was doing an internship at Microsoft Research just as the Kinect was about to came out come out and we're and was able to work on that and really see what what can you do in real time slam if you have a stream of pretty high quality depth information so just to show you a video that's representative of dense slam systems that this is a system called elastic fusion which was a work from our lab at ice robotics lab about five years ago led by led by Tom Whelan so here we see a real time dense slam system here so the inputs of this system is what you can see in the bottom left let me just fullscreen this so in the bottom left you've got the live kind of image stream from a color camera and the live depth stream from from a connector depth measurements and then what we're doing is both tracking the camera and then fusing all of the information into a dense scene model in real time so there's many possible ways to represent dense scenes in this particular system we're using a representation called circles that means that their representation of the scene is basically a cloud of little disks so these are points that also have a kind of orientation and Ana size so they're representing you know small disks and if you kind of smush enough them close together it looks like a continuous surface so that's one of several options for representing dense geometry a particularly nice points about the the circle representation is it is fairly efficient and that enables the elastic fusion to do loop closure so this means that the camera comes right round and revisits old parts as parts of the scene he's able to make corrections to the map so that the map stays consistent which is something that wasn't possible for instance in the original kinectfusion system so I think the next and probably the most recent sort of real advance in real time visual slam is what's very much going on at the moment which is trying to add a semantic layer so the map built-in elastic fusion it's very detailed but it's still just raw geometry in the end it's it's a cloud of points so there's no meaning associated with it so of you know from from three or four years ago or a little longer ago of course the big kind of new technology coming into into slam is is deep learning and so enabled of course also by GPUs this capability to do real-time labeling and recognition so we built a system called semantic fusion which is aiming to do dense semantic mapping of the scene and if I show you this video [Music] this is this is what we're trying to achieve so map a scene densely in 3d but also apply labels to it which represents our best estimates of the objects or type of objects that are present and the way that this system works is actually fairly straightforward so we have our input data stream of color and depth images they're fed along two pipe ways one does slam reconstruction so this is in fact the elastic fusion what you just saw before it also feeds those frames to to a CNN which has been trained to four per image semantic segmentation so it labels every one of these input images with an estimate of what are the identities and then what semantic fusion does it fuses those labels into their dense map so every circle in the scene is being labeled with in each cell in the scene basically holds a distribution over what it thinks the most likely labels are for that point of of geometry in the scene and the interesting point of doing this in a slam style is that the label for each point in the scene can gradually change and improve so from one point of view you might get you know not a very good labeling of a particular elements of the scene it's quite ambiguous but when the camera moves to a better location that can actually improve okay so there's just one paper I'd like to to mention in in in connection with that so something that we've published or just just will appear in the Ikra conference in the next month or so so these systems that label scenes based based on CNN's I think there's two interesting ways to go about that one is the view based way that we took in in semantic fusion so you can label every view in a view based way as the camera moves around and then try and fuse all of those labels into the 3d representation another possibility that we're seeing a lot of nice systems but based on is build the whole map and then use something like a 3d CNN that can take in that whole 3d map and try and label it in place so there's very interesting pros and cons between those two methods one is one is a media it gets to work on the raw data whereas the other one gets to see the whole map in one go and can take accounts of whole context and that kind of thing the computational characteristics those two things are also very interested or also very interesting to to study so so how does it have a look at this paper here if you're interested in that okay so we've got these this range of systems that have gradually got more advanced one way of thinking about what happened there is that we've essentially been you know gradually throwing more and more computation at this problem and we've you know seen progress because of that so where does that actually take us so you know is this is this useful steps towards this spatially a goal that I thought I was talking about so I find this this picture quite interesting so this is actually a kind of more or less a a mock-up that I did but thinking about all of the computation that's happening in the system like semantic fusion so it's a pretty complicated system we've we've got data sort of represented in green and then blocks of computation represented in yellow and don't worry at all that this is a bit small to read or all I want to get across is is this is pretty complicated so there's labeling there's tracking this fusion there's math optimization there's all kinds of things going on here so how are we going to get this to run on tiny embedded devices that were that weighs 65 grams and this kind of thing that's what what I'm really wondering how are we going to kind of optimize this graph down to something a bit more manageable so I think there are two kind of interesting lines of attack that we've been following in our lab on which I'm going to take take you through next so one is to really think about representation so this really means if you look at something like semantic fusion we've got these huge representations of you know detailed things like we've got image maps of labels you know the site the size meant of an image with hundreds of thousands of of entries we've got maps of surf also I think the map of the room that I showed you in inelastic fusion earlier has something like 5 million circles in it so we've got lots of you know big amounts of data moving around this graph being worked on you know parallel computation where we can but if we you know the currently implementation of a system like this is also you know some parts of this work on the GPU some of its running on include err some of its running probably in intensive flow or or PI torch other significant parts are still running on a CPU a lot of data transfer and so on so what what if we could kind of simplify a lot of those representations so can we turn these huge you know dense blocks of data into much more simple things then we seem to get to something which would be much more efficient the the other line of attack which I'll talk about a bit later on is is can we use custom computing hardware to help us with this I'll come back to that later so firstly thinking about representation we've done various bits of work in our lab and and most of this has has increasingly been using elements of of machine learning so we're really trying to say are there you know representations or or algorithms with it within our complicated slam systems which can be replaced by something learned which will have kind of simpler and probably more more generally abstract and interesting properties so that brings me to a general discussion which I'll try to tackle briefly about slamming and deep learning so I think everyone working in in slam has been thinking a lot about this and in the last few years as in most fields of AI of course my points of view on it is that we have a kind of sliding scale here almost a continuum between things we could do where on one side we've got fully hand designed algorithms and certainly the early systems that I showed you purely geometric systems they're they're completely hand designed you know Manos ba no slam or elastic fusion and so on at the other end of this scale we've got this possibility of fully end-to-end learned algorithms so I think if you speak to someone let's say a deep mind that they'll probably tell you or you know they might believe that you know why would we ever hand design anything any any part of an algorithm why don't we just learn everything and end-to-end and I'm not saying that they're wrong I think in the long term that may well be exactly right because every time our human designs representation you put some limitation that that's an artificial system may not need but I think that the way that almost all of us are proceeding in trying to build systems that work is a modular combination of hand designed elements and learned elements and I think there are two key ways to do that either we can start with our hand designed system maybe the one I showed you in the graph just before and replace some of those blocks with with learned things which work better but also maybe have some sort of abstraction and efficiency the other is is to start out for that with a big neural network and say this is my system but gradually kind of impose structure on that network and say is this not a completely general perceptron it's you know there's no structure those things like spatial transformers and so on in there which force it to do certain things I see those things as very similarly related and well ultimately meet in the middle okay so let's so having said that let's talk about some of the work we've been doing on representation in our lab so the first sort of category of projects I'm going to talk about was started by a paper we had called code slam back back in 2018 and was followed up closer closely after by a paper called seam codes so don't don't worry too much about the details of this kind of network diagram I'm showing you here but the key the key idea is that we want to encode the variation in indents depth that we can see from a single camera in terms of a way that what are the main variations there so if you just think of you know a dense depth map in principle you know at VGA resolution that has 300,000 parameters for the depth of every pixel but if you point a camera around at real scenes there's actually a lot less variation than that so most real scenes tend to be made up of you know continuous objects and surfaces so there's a lot of sort of structure in in the space so M codes slam we reduce machine learning we have a we have a data set of in fact synthetic indoor scenes that were that we that we train this on and we learn an autoencoder which is able to go from a sort of small bottleneck to generating likely dense depth maps so with a smelly fairly small encoder of maybe 32 parameters we can describe the variation that tends to be present in in in dense depth maps so that there's more more details to it than that but what that essentially enables is when you have a camera moving around as seen from each single view you can make an initial dense depth prediction but as the camera continues to move you can then optimize multiple dense depth predictions against each other so by optimizing the codes of two of these dense depth maps we can adjust them to try and bring them into alignment but we will only be adjusting them within the space of things that we think are feasible given our our dataset so let me just show you a video here so in fact we've recently published a paper called deep factors and there's real-time open source research code available based on this paper which which is in an implementation and extension of the code slammer idea into a more general slammer system so just to give you an idea of how this works so remember that in this system we just have a monocular camera so there's no depth camera in this system so as the camera moves around you'll see that every every few frames we add in a new part to this representation so we've got a new keyframe we immediately get a dense depth kind of single view depth prediction from that frame but then we're able to optimize it against the other nearby frames to try and produce a larger and more consistent overall 3d map so we're here we're able to do joint jointly optimizable dense visual slam within this coded sub space and so just just for a monocular video input so of course subject to the things you would normally assume about a system that includes machine learning that it will work fairly well within the scenes that are similar to the scenes that it was it was trained in but please have a look at that and try the code out if you're interested I'll just very briefly show you this and the other video so this is from the scene code system so this is shown that we can use very much the same idea of code optimization not just to deal with sorry geometry but also to deal with with semantics so here we have a video where our goal is to jointly estimate both the the dense depth of this scene and also the the semantic labels but the way that we're basically combining multiple views now to do label fusion is not like in semantic fusion where we were just doing per per pixel averaging pretty much which gives us sometimes this quite noisy looking results here we're doing label fusion by code optimization so the labels in each we are able to change but they will only change within the space of feasible things that we are familiar with so therefore they'll tend to change in this much more sort of smooth and and coherent way so it will tend to change the label of a whole regions such as an object for instance okay so those systems you use learning just kind of encode the variation that's present in in a whole scene but you can see that there are some limit limitations though in some ways maybe we're asking learning to do too much there because a fairly you know a small encoding cannot give us that much accuracy in representing a whole a whole scene so what seems more sensible is to focus learning down on on things like objects so going back to about 2013 we've been interested in in this idea of can we actually build slam maps where the objects actually really are are the map so this system called aztlán plus plus it was this idea that we would come into a scenario where we're going to be moving a camera around and trying to map a scene but we're not going to do bottom-up mapping of this scene and then try and fit semantics and objects later we're going to have a database of the objects that we expect to be present and we're going to try and fit those as soon as we can so as the camera moves around the representation of the scene that we're building is that is a map that's directly built at the level of objects so you see that the camera will move and then down in the bottom right here you'll see objects kind of popping in to this representation so here either an object is present or it or it's not present there's no such thing as a sort of grep partially fused object so so this approach works in the case where you know the scene is obviously consists of a lot of instances of objects for which we have precise prior models so here we have no ability to deal with objects that we we don't know about in advance so I just like to show this graph so I think this idea of building maps out of these higher-level abstract kind of concepts like objects I think this fits very much in with a lot of thinking that that's out there in the community at the moment into building sort of layered scene graph type and representations of scenes so there's a couple of images here on the back from on this on the left from Facebook reality labs I'm on the right front from Luca Carlo nice group at MIT on building you know what I think is really the right direction in terms of layered scene graph type type representations so in our lab we've we've continued to focus on on object type representation so let me show you a couple more things that we've done so this one is is called more fusion so this is a new system just about to be published this this year at cvpr so this is really the direct follow-up from so you can just move to the right place or it really I direct follow up front slam plus plus in that we are coming to a scene in the situation where we do have prior high quality CAD models of the objects we expect to be present but you'll see that here we're really turning our attention to what could actually be done with that in that we're doing that now within a manipulation setting so here we we have prior CAD models of this set of objects the the camera now moves around so there's a depth camera on the motor on the robot arm the robot arm explores it does some initial volumetric fusion of the scene but then as soon as it's able to it fits precise 60 pose of object models to any objects it's really confidently identified so here you'll see it just found this so drill in in the center of the scene and then what you'll see on the right is is then it really jointly optimizes the poses of all these objects so we want to deal with really difficult situations where multiple objects are in contact there occluding each other so it's actually pretty hard to do single object pose detection in that situation but if we jointly optimize the poses of all the objects then we can do it so here is now an application of that for a kind of difficult grasping situation so here what the robot wants to do is actually moved the rather large red box in the middle of the scene into the into the brown box on the right so first it's it's scanning the scene it's fitting object models it's jointly optimizing them and then when it's confidence it decides on a plan of action and it realizes that to get to the red object it's going to have to move to other objects out of the way so first it takes this object here drops it in in the bin on the left and then also this yellow box and now it's able to get to the object that it's interested in and this isn't just grabbing and dropping it's picking it up in a well-known pose and therefore it's able to then precisely place it exactly where it wants it to be in the box so another system I'd like to show you briefly so it's very much along the same idea of an object-based slam system but in some sense out there at the other end of the scale here so there we were saying we had a precise CAD model of every object we were going to see this is kind of the opposite end where we don't have any prior CAD models we do have a really good neural network so we're using masks our CNN which is able to look at video frames and give us accurate masked regions of anything that it thinks is a separable object what we do in this system is for every one of these objects that we detect we add it to our slam graph so that's what you can see coming out here now so this is a graph of the positions the camera has been in and the positions of objects and then each object we make an independent fused 3d reconstruction of it so pretty much connect fusion running in a little volumetric region for each of the objects and then we've got each of these objects then really serving as a kind of landmark in in an overall slam graph and we can do normal slam things like loop closure so here we've come all the way around the room we find a set of objects that are consistent with a set of objects we saw earlier we enforce those constraints on the graph and solve the loop so this is a powerful system in that you can map all sorts of different objects in the scene but it has the downside that you know it's only using the visible information so when it reconstructs an object like a chair it's only able to reconstruct the portion of that that it can visibly see so that's at the other end of the scale of reconstructing you know of estimating the pose of a have am of a CAD object so the most recent work we've done in this space is really trying to find us a space somewhere in between so this system called node slam that we've also just recently put online is just placing the video so we're saying that we're somewhere in between so we're going to recognize objects in the scene but for a particular class of object we don't have a precise CAD model instead we have a kind of wore purple shape model so this is a model that we've we've learned using a deep learning method so it's a 3d volumetric auto encoder which we've trained on a data set of 3d objects so we've got you know hundreds of CAD objects of mugs and hundreds of CAD objects of bottles and we've learnt a kind of shapes based model for each one of those objects so now we can use those in in aslam context so that so there's lots of interesting work on this kind of idea of coded object models going on and especially in a computer vision literature what we focused on here is can we actually now use these sorts of models for quite useful things in a slam context so here you'll see that each of these objects in in the scene the first time that we observed it we basically optimize over the coded parameters of of the shape model or office of that particular class of object until they're the reprojected shape of that object matched well with the depth information that we've got so just from one view of or one depth view we can get quite a reasonable estimate of the shape of the object from multiple views we can improve that even more so this is now a full slam system where we are on each new depth frame we had jointly optimizing for the position of the camera the positions of all of the objects and then also these shape parameters of the objects and let me just skip forward a bit to show you an application of that so we've also just go to here so we've also looked at robotic pick-and-place using using this system so here we've laid out a few of these objects on the table we've got a depth camera on our robot arm the depth camera scans the scene in the top left you can see it gradually fitting the object models to the scans once it's fitted the models it can then do some you know simple planning and action and here what it does so it knows that there are four balls in the scene but it's able to accurately enough get an estimate of the size of the balls that it picks them up in order of size biggest to smallest and then is able to place them in that order to stack them precisely in the box so again this is an example not just of picking up an object and dropping it but picking it up in in us you know in us in the situation where we've got a very accurate estimate of the shape of that object and therefore we're able to place it very precisely so we're really interested in in this you know object based representations of a scene and increasingly in how those can actually be used for useful robotic things such as manipulation the last thing I'll show you in in that kind of area is his work from the Dyson robotics lab called the RL bench so this is a new benchmark simulator or dataset with which we've just recently put online so this is this is also available as as open source software so we're interested in enabling robots to do many different things so this is a data set where these are hand designed tasks for many of the things we might imagine that a robot arm might want to do like placing placing objects you know opening things closing things putting things on top of each other and within this simulator you can really do what you want so you say you can run run the simulator it includes simulated cameras of various kinds both I'm kind of off boredom and on board it includes simulation of so for instance you could train reinforcement learning agents to try and solve these tasks and some people are trying to do that but what we really wanted to do with this data set was make it deliberately pretty much out there in terms of what anyone's they're doing in reinforcement learning this is an example by the way of domain randomization so that's something that's been found to be very useful in a lot of reinforcement learning so you know know what no one has has shown reinforcement learning able to do these wide range of things and in a reinforcement learning I think has mainly been successful for fairly sort of local and and repetitive kind of kind of tasks so I would argue that maybe something more like you know a spatially I slam like object based representation of a scene may be the sort of thing that may enable AI to solve tasks like this so by putting this data set out there maybe we can motivate more people to be interested in those kind of those kind of methods okay so I mentioned earlier on that given this rather complicated looking computation graph we needed angles of attack in order to really make this computation feasible especially on embedded devices and and of the first thing that we really looked at was different sorts of representation that might turn some of these things from you know huge blocks of of data into rather smaller things like codes for instance representing objects or or scenes the other kind of key approach I've been very interested in is the last few years is is can we use new things happening in in hardware to help us with that so hardware as it relates to slam and and spatially I so I'm not really looking I'm not really talking here about actual robotic manipulator Hardware I'm talking about the sensing and perception hardware and in particular the computation hardware that would be involved so the main areas of interest are sensors and and computer processors so first of all in in sensors that so I've been very interested in what's been going on in in cameras so as our betterment mentioned earlier you know I see computer vision as a broad area and I would have always included you know whether it's a single camera a stereo rig a depth camera it's all part of computer vision but that is that isn't isn't an area subject to ongoing change and the most exciting thing that I've seen in the last few years is event cameras so this is probably something that a lot of you are not know about so this this is an image sensor that rather than capturing frames it captures per pixel asynchronous differences so the report from a from a depth for it from an event camera is a sequence of events which is each pixel telling you with an accurate timestamp when it's changing its its brightness so as soon as I heard about this I was immediately excited you know first of all because you know the data rate from a camera like this is much more seen dependent so if there's nothing changing in the scene then there's no advance and if there's a lot changing in the scene list there's a lot of events so we worked on slam like competencies using these these cameras and I'm not going to detail on on this but we you know we managed to it to achieve you know monocular 3d slam with it with a single event camera so here there's just a single event camera being waved around and we joint the estimating the color so that the intensity reconstruction of the scene which is not something that comes by default from an event camera the 3d motion of the device and and the depth of the scene that's how that work is very much on going there's lots of several labs now doing really really nice working in this area but for me it's it's all about this you know I think about this in terms of a computation graph and this idea that this event camera does write in Hardware on basically the sensor chip it does a very important job for you so you don't need to pipe video data down to a CPU because it's done some abstraction right there on the chip and I think that's the start of a more general thing that's going to happen where cameras will become programmable devices that are able to do a lot of processing right right there kind of within the chip so the other big area in hardware is processors and this is something I've also been really interested in and I'm really starting from you know going back 10 years where I saw GPUs coming into computer vision and what a massive difference that made I mean I think everyone's familiar with that now with we're deep learning that you need GPUs to do do deep learning but you also need TP used to do to do dense slam so the sort of computation where you want to do something at every pixel and every frame and it's very parallelizable GPUs are really good for that so we had a project this was a joint between several universities in in the UK over about five years which was really about investigating how does slam algorithms work on different sorts of processes so from a huge sort of desktop machine in the top left there with a big GPU down to an entire a tiny embedded board so can you pick apart as slam algorithm can you really think about which which bits work well on which sorts of processor and can you facilitate the sort of research on co.design of processors and and algorithms and and of course you know an obvious question there is you know can we build a chip specifically for slam that's that's as efficient as possible and that's a really good question and there's been lots of nice work and and some concrete projects are on that on that particular idea my particular my kind of current thought about that is it you know my my thinking about slammer is too broad at the moment to do that and I think when I was taking part in this people wanted to pin me down and say okay what's the algorithm and then we'll go and design the hardware but I was always a bit like world eyes this lot of possibilities and I'm not sure so I were I wasn't really willing to commit at that point and but then I heard about interesting things that were going going on in in kind of commercial hardware and in particular graph processors so they're the main workhorses of AI at the moment our CPUs and GPUs of course these are processes that are very useful but they're not really designed for AI or vision you know CPUs are designed for general-purpose compute word processors if you like GPUs are designed for graphics they've they've been successfully adapted for various things in envision but as I as I pointed out earlier you know we can use them for parts of a vision system but very far from all of it we generally have to have CPU still around too so and the graph processor is a new idea and in particular from that there are many related ideas here but I'm going to focus on a company in the UK called Graff core that's got a product called the IPU or intelligence processing unit which is a new processing chip which is designed to particularly enable what they think are the important workloads in in AI so what a graph processor is is a massively parallel chip so the IPU has about 1,200 processing cores on it but they're quite different from the cores on a GPU they don't all have to be doing the same thing in lockstep parallel they can all be running very independent programs and they're able to have very high bandwidth and almost arbitrary connectivity for message passing between them so what they're really good at is processing graph like workload so ad hoc kind of arbitrarily shaped graph like workloads and and really the sort of processing where the you know the computation and the storage is really kind of mixed together so for an IP you to work well you don't want it to be pulling any data from external memory you want anything that it's interested in storing to be stored in a distributed kind of alongside all of the cause of this processor and then to talk to each other by passing messages so so that's very interesting I can immediately see you know this can reduce the sort of data flow so the things that make you know processors use a lot of power are generally moving memory around bits times millimeters is what you want to reduce a graph processor by really thinking about jointly reasoning about processing and and storage can can really tackle that and yeah that this there's a lot of technical detail but you know an IP you divide devotes a lot more of its area on the chip to memory which is all about communication and local storage then then logic which is about fast computer so the actual cores on an IP you are not individually that fast but they're very very well interconnected so that enables new styles of of algorithms so when you compile something for an IP you and this is a visualization from from graph core you have a computation and and they're particularly focus on on optimizing neural network so this is I think alex net actually you take all the computation involved in in in training or using alex nets and you think of all the elementary sort of computations and and data movements that have to happen and you compile it down onto the the grid of processors on on an IP u and this is a kind of keep visualization that shows something about all that all the connectivity and you can identify the kind of regions in the neural network so having seen that I started to think well could we do spatially I in a similar way so you know this is really preliminary thinking and there's a lot more detail in in this paper I mentioned earlier if you're interested but let's think about all the computation involved in inner in a spatial AI system we've got a real time loop doing things like tracking and labeling there's clearly a graph like structure there we've got cameras which are kind of sending data there's clearly sort of graph like structure there in the regular pattern of pixels and so on and then we've got our kind of map store and anyone who's worked in slum for a while knows that you know graphs of how different nodes and factors connect to each other are the you know they're really the best representation for for slam so it's so those graphs everywhere in slam so can we really map those graphs onto onto a processing chip and and then use that to implement possibly you know some steps towards this very efficient vision vision of spatial AI that we think we need for products so we've taken some some first kind of concrete steps on in that in in the last in the last er few months and we've got a paper to appear at cbpr which is called bundle adjustment on a graph processor and there's also an another long discussion paper about the general idea of a class of algorithms which i think is really suitable for this which is belief propagation so certainly not not anything i've invented but something which I think might much very much come back into into interest because because hardware has has changed so we've been able to show concretely in in this paper will show at cvpr that we can implement a classical computer vision algorithm which is essentially you know the core slam problem of how do we jointly estimate the positions of a set of cameras and a set of points on an IP u chip by using gaussian belief propagation so if i just skip to the to the right part of this if you're familiar with factor graphs so on on the left here we see the factory graph for a bundle adjustment problem and we map each of those nodes variable nodes or factor nodes onto tiles of our IP U and then we were able to perform the optimization that we need to do in bundle adjustment by by message passing so we're we're going to send messages around between these tiles in in some pattern and show that we can achieve optimization of globally consistent things by purely local computation and message passing and show that we're able to do that very fast and very efficiently and then some very kind of preliminary results there in this paper we show we can we can optimize you know a reasonable size bundle adjustment problem to 20 times faster on an IP you then you can on honor on a CPU using series that the standard CPU algorithm for that ok so I think I'm pretty much out of time so let me just finish with some conclusions so spatially I researcher I think it's a fantastic thing to work on and I think we'll be working on it for for a number of years yet so so you know it involves computer vision and and robotics deep learning and representations but really focused on you know how can we put all of those things together to enable incremental efficient scene understanding taking into account also ultimately how we will lay these things out on Hardware so co-designer of processors sensors and algorithms I think is super important and as I've mentioned graph based algorithms for for estimation such as belief propagation and also graph based machine learning something I haven't really worked on much yet but I think is clearly going to be a key part of this so I just like to mention the my affiliation so so I'm leading the the Dyson robotics lab at Imperial College I'd like to thank all all of my colleagues there and also my collaborators at Dyson who support us so this is an academic lab with with postdocs and PhD students and we really do long-term research academic publishable research on slam scene understanding and manipulation and finally to mention slam Corso you slam core is a london-based startup which i co-founded with other previously academic colleagues and we work on an applied specialized solutions so thank you very much thanks and vo for the aspiring talk I loved lock the entire book I love the amazing results and videos of course but they also really really appreciated the vision and the broad perspective that you gave on the on the entire field of perception which I believe is a great you know service for the community so given this kind of vision and perspective so I'm sure that it in other circumstances we would have a standing ovation here but for now we'll have to settle for a beer and walk loose so thank you so much for for giving this presentation the planet for the final portion of the event is to have an interactive discussion with Andy and these will happen with the elbow our panel I am Luca Catalunya I'm a system professor at MIT and I will be moderating the panel together with Jeanette Bach from Stanford hi Jeanette I so joining us to be we have our guest panelists which Albert already introduced is a John Leonard from MIT John thanks for joining a pleasure and also we're going to have matt kokkonen from Stanford in Alberto Rodriguez from MIT Moodle will get cement from our amazing collaborators including Han yang from MIT around salut cinema Jackie from Stanford the nemeth Ezeli from MIT Rachel Holloway who will help us collect question from the audience so I invite the audience to post comments using the interface on different websites and I see that there will be like many questions flowing and I also seen by the panelists to use the reason function to ask questions on our site so of course I have a number of questions and curiosities that I would like to ask Envy but I will actually invite you only on earth as our guest panelist to start with remarks and quest don't take it away thanks Luka Andi wonderful talk I I have dozens of questions myself I I'm so excited to see this sort of really this three decades worth of work I remember I think it was in 1999 I was an assistant professor and Andy sent me an email he was a postdoc in Japan just and we started a conversation that's now gone for 30 years and I'm reminded of Amara's law which says that we tend to underestimate the impact of a technology in the short term hope we tend to overestimate it in the short term but underestimated in the long term and I think that's kind of really where we are with visual Simon spatial AI that I think in the very long term decades from now this technology really can be ubiquitous to make life better for lots of people and and it's we've come such a long way but there's a long way more to go and picking amongst the many sort of topics I might ask I'll go to the question of robustness you know I've always dreamed of having apt-get install slam so that say someone working in manipulation may be mobile manipulation they might just have this confidence of knowing what is where in the world that spatial intelligence that's something they could just kind of take for granted and Trust and so it's uh you know I'm guilty myself in the lab we make wonderful videos and and you know but how do you how do you make a system where you could just I'm you could be blindfolded in your lab and you would say John I'm gonna press go and I'm gonna leave or close my eyes and you knew it would just work where is the elusive robustness where you could truly test trust 3d you know object based slam for for say manipulation yeah great great question so of course yeah that I I agree there there's such a massive gap between you know the thing you see in a paper the the cool video of the style that I've showed you know I in my lab I've generally tried to go one more step than that which is the live demo so I've always been breaking on you know going to a conference I do you agree credit which I'm a big fan with a laptop and wave wave or come around and show something people can actually interact with but you know there there there's another huge a number of steps beyond that so it's actually real product and of course I've experienced that where you know with my work with with with Dyson so just the amount of testing and just basic you know engineering that needs to be done and and I I still haven't seen any indication that there's any magic bullet there I don't think you know massive amounts of testing finding all the the problems and and edge cases so of course we have some hope that you know think things coming in such as you know learning can can be very important parts of improving robustness so as as we probably all know you know the this general idea of image correspondence is probably always there you know date data Association as robots this quality is always the the hardest part of of slam and clearly we see now that that you know learned mechanisms are able to do a lot better in that but now now maybe we're in the bit this this trap of them you know how do how do you know that your dataset that you've trained on fits that with the place that you're actually going to to test in and that's something that I've actually got bit frustrated with recently so actually some of the things I'm most excited about at the moment are you know going toward sort of an unsupervised or or self supervised learning so actually for instance you know if you've got if you want in your network to to segment the objects in a room yes you could have a neural network you've trained for semantic labeling in some other scene and you can test it but it may not work that well I think what you really want to do is get data in the space that you're actually in and learn as best as possible in that scene of course you won't have high quality labels nests necessarily but I think you can go an awful long way without supervised learning and then maybe there's a fairly small amount of labeling needed at the end to actually relate to what unsupervised learning has learned which is more or less clustering and relate that to you know labels that a human would understand I agree with you just a quick follow up and then I'll turn it over to others and in his twenty nineteen year keynote Yoshio P NGO said quoted toe saying Nature doesn't shuffle the environment and I think something that we can offer the machine learning community is that this you know sub supervisor division or just that an agent that's controlling its own motions and moving through the world with a notion of space and time it's a bit like having the cards in a playing you know in sequence versus you know imagenet just pulling randomly from the internet yeah you know disparate and I think that's notion of spatial contexts that that that is the path because even though machine learning is obviously made incredible strides I worry about where we are on the ROC curve that were very hard very far from very high probability of detection and very low false alarm systems that we could use for example for autonomous driving in highly dynamic environments and so the robustness and I one is sort of the perfection point on the ROC curve where we really know we can trust it and so but I think a lot of these methods you've mentioned about object based representations and new types of processing the message passing it all fits into I think I think we've got much more great work to do and and thank you thanks thank you very much John for your question so I think we're gonna now take on some of the great questions and there were actually many from the audience so Hank you have selected one question from the audience can you give it to yeah thanks Andy for the nice you know talk about perception and vision and slam but I have a very highly ranked question from Carmen Purohit that asked that precise you know rather than solving slam as a standalone problem or other new developments in active slam in particular I was controlling planning in the loop as well yeah good great great question so no I I mean I'll put my hand up straight away and say it's not something I've worked on very much and for a lot of my career you know the output of the work has been you know this is the video this is the map this is the camera trajectory that word that we've estimated but very clearly that's not the end goal the end goal is embodied in intelligence manipulation and as you see where Willie's trying trying to work on those things now so it has to be said that the yet been much feedbacking in any of the things we've done from the action to actually you know improve the representation so we scan our scene we get the representation from vision and then then we act but but clearly you know once you start touching the scene understanding the physics of how things move then that should really improve improve your your your perception and and feedback on it so I'm really really interested in going in that that direction so for instance bringing physics simulation in into into the perception system is something I'm very interested in so in the system I showed you with you know a pile of objects that the robot was going to interact with so when we're estimating the positions of those objects we do take account of the fact that the objects shouldn't be intersecting each other but we could go further and and for instance say well physics says that this object can't be floating in the air so there must be another object kind of right underneath it which is supporting it so bringing that kind of physics in into the into the actual estimation of what's going on and then hopefully then rolling that onwards so versus the robot then starts to interact with with the scene and maybe poke want one of the objects and sees how it moves you know we probably we may get some kind of force some information from from the robot but we'll also get some updated visual information but then we've got to come up with a new estimate which is still consistent with with physics I think those things are very hard because simulating physics of contact is incredibly difficult and we've tried lots of you know different physics engines and and so on that have different strengths strengths and weaknesses but yeah those are directions I'm really interested in but can't can't claim too much progress so far okay okay looking forward to it that you have a have a follow up on this one yeah I wanted to follow up Andy so if you bring physics into the estimation loop would you'd still call it the spatial AI or would you have to change the name and I'd still try and claim claim that name but it's up to everyone else whether whether they whether they accept that I I think so I think embodied spatial intelligence involves all of those things it's it's about all of their sort of sources of information sensing and priors that that you know an embodied agent needs to to use in in order to understand enough about the scene to do intelligent things there so but based on that definition I think it still fits right um think so I was actually also following a question when you were grasping the objects they did stay in the clip or did you not take any visual information into account or track it so we you know to be on this way to doing something fairly you mean once they're actually grasped yeah and then they're moved into the box that were actually moving with the gripper they were wobbling a little bit they were just like being transported right but I wasn't sure if this was actually tracked you know so at that point we were assuming that you know once we grasped the object the object would stay in a fixed configuration relative to the hand and until we place it but certainly that might be possible right I mean it's difficult with a depth camera on their hand because the range of the depth camera is normally to you know the minimum range is such that you can't necessarily get an accurate measurement of something you're actually holding but you know cameras are cheap maybe we just put more cameras one right there kind of in the palm or something yeah that would be great rental ooh you picked another question from the audience yeah thanks for the talk Andy so I'm really excited excited about all the new developments and this seems to be an evolving field so we have a question from Carney is there an accepted grand goal for slam upon reaching which we can comfortably say slam is sold or do you think it is continually evolving or are there any like standard challenges for slam yeah I mean going back back to you know the question right at the start of the talk I yeah people have been saying slam is solved since 20 years ago and and I've and I think John have always been a mime I hope people that said hang on you know I'll immediately give you a way to make it more difficult like try and do it with a single camera rather than you know a laser rangefinder on it on a 2d robot for it for instance so I wouldn't want to set a kind of fake fixed target I mean I think for me the targets now are really about you know embodied intelligence that that's what we're trying to to get to and what we you know what we call slam has to be in the means to an end so you know it could well be that a lot of the things we've done are you know submit many of them are not good enough but some of them are overkill so you know for instance accurately estimating the metric shape of a building that's 100 meters long probably isn't really needed from many intelligent things that you would do with within that building or you know mapping a room with 5 million circles especially the parts of the room like the ceiling that you're never going to be interested in actually interacting with so I think the way that we should measure the performance of these systems should ultimately be much more about the end goal you know actions and and and intelligence that they enable and and then we should feed those back into into how the slam systems work if we can pull up on that adding that they in easy in quotes an easy goal is just because he has Lancie some spatial perception system to be as capable as a human perception system so whatever like a human can answer using sensor data it's only to expect for a robot for a smart robot to answer I don't know if it is a definition which is really actionable but I would love to be at that point I mean of course there's many things a current slam systems that can already do that are far beyond what humans like tell me the precise distance between those two wars and a new aslam system will tell you within a millimeter but yeah clearly many things that humans can do with their spatial perception slam systems can't can't yeah so I think that that's useful and of course it's all always in an inspiration and I think that I've realized more and more though I think I'd always thought you know what the only thing I'm interested in is making systems that work but I've realized more and more that I think I am also quite interested in understanding how something about biological intelligence works and I would never claim to have any expertise on that but I just have a feeling that if we discover you know representations and computational patterns that enable really generally useful slam systems to work that are also efficient and I think we may also learn some some useful things about others other sorts of and embodied intelligence very cool so that that is going to be like a much longer discussion but since we have many guests from the from the audience I will maybe call out on Numa to ask a question from the audience great thanks so we had a question regarding memory so as you get these more interesting more high dimensional maps or with the semantic information if you're having an embodied AI an agent that is supposed to move around and be untethered how how do you think that we can overcome the challenge of memory and capacity for maintaining yeah great great question and something we thought a lot about so I think the key is you know in introspection and abstraction so and this is actually something where we're concretely working on now kind of in in the graph processing work so you know we've got this computational resource we know maybe it's an IP you ship that is able to basically process a certain size of graph and clearly as if a camera keeps moving mapping more points the graph we're interested in is going to get bigger and bigger but I think that what we can do to overcome that is always be looking for ways to simplify the graph so for instance if we've mapped a hundred points and at some point we realize they lie on all I on the same wall and maybe that information comes from you know in your network that's telling us it's recognized a wall or maybe it's just pure introspection that we check we've keep finding all those points are really planar we could replace all of those nodes in the graph with a single you know clean node that abstracts em and captures all of the relevant information and then reconnected it to all their other necessary places so I kind of think that's that must be how you know scalable memory works so you've got a lot of detail about the things you're looking at right now and then the things you've you know maybe the things you're not looking at right right now they fade down to something more more abstract which is still sufficient for them to be useful but ISM at is much more efficient and I guess the semantic understanding can also guide you know what to compress and conform are to compress things in the past that's the value absolutely yeah yeah so I think Rachel it's a very interesting question from the audience I should do I want to read it so Lucas Manuel Lee asked how should the output of spatial AI systems be different if at all for different downstream applications enable a student versus self-driving and I think a related question from Immanuel Franz was about how would you integrate semantic integration with NLP or human machine interaction in the context of these different applications okay so let me take the first one so that this was about how much the how might the output be different for different applications yeah so it is it's good question so so clearly different applications need need different capabilities I think you know I think one of my hypotheses is they might not be completely different so this may be more and more of a sort of continuum so I imagine there might be you know a relatively small number of performance parameters that are defining how well your system works let's say the accuracy that you can measure pose might be 1 the density of reconstruction you know the the level of fineness of your semantic labeling their precision with within which you can predict the time to contact of that point that's directly in front of you or something like that maybe there are a rarely relatively small set of of kind of system defining parameters like like that which each different application would have different positions on those sliders you know so a drone needs to be able to react to really quick changes but you know you know you know that in some other regard it might be you know it doesn't have to do very much with objects or something like that whereas a you know a home robot has different so I I kind of hoped that there'd be really relatively general sort of architecture for spatial AI which then you would specify set of kind of performance requirements for a particular application and and the algorithm would hopefully can configure itself to fit for kind of optimal performance with regard to those parameters so you could could you remind me about the second question yeah the second one's about integrating in with things like NLP and human machine interaction yeah sure so another area that I really haven't done done any concrete concrete work on but is absolutely important if robots are going to be you know working alongside people so the only kind of concrete thing I can say is that I as I think we step up in spatially I towards these more semantic and abstract representations where we're going to have much more ability double toes to another representations meaning the Lotus dozen to a human so even when you when you watch a robot vacuum cleaner probably sometimes frustrating I must admit you you want to do things like a tell about to the kitchen you know it's still dirty over there and so you know going there would be fantastic understand so it's so yes I guess if you imagine a set of queries like that of of actions like you know go back to the you know the chair behind behind the sofa and clean under there and think about you know are are the representations that we're building which will hopefully be abstracting and hierarchical do they - these 16 graph like representations do they enable those kind of actionable queries thank you and the fascinating discussion fascinating talk I really loved your historical perspective on this lung problem unfortunately is time to wrap this up on behalf of all the co-organizers Luther Jeanette Alberto I would like to thank everyone that has joined us today for the inaugural talk for the robotics today toxeus next we we're going to have another exciting table from Leslie Kaplan from MIT or recent advances in decision-making and enforcement learning importantly the stop series is really an experiment on online robotics community building what goal here is to view beautiful talks not as an as a better option but really as a new opportunity to achieve a broader reach for excellent talks such as the one that Andy gave today but we want to get it right so it's very important to us to hear your feedback so please go on the websites and a couple of minutes and provide us with any comments you might have on how to make the seminar series more effective more useful in general great thank you very much and I'll see you next week you 