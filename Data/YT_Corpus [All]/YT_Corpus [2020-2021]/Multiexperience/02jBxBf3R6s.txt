 hi i'm robert sim i'm a principle applied scientist with the team called knowledge technologies and intelligent experiences otherwise known as KTX we're interested in scenarios involving assisting users with their tasks and especially those were we can leverage multiple devices and their complementary capabilities in order to help users so for example we might connect a Amazon echo smart speaker with a surface tablet or in this case an iPad and take advantage of the far-field capabilities of the speaker well using the screen as an interface or output device for the user to gather information in this particular demo we have a cooking set up and we paired the speaker with a recipe website and I'll just kind of step through the process and show how we can seamlessly go from speaker to screen and back to speaker so Alexa tell test chef let's cook okay let's get started okay so you can see that it transitions straight from she's gonna be confused about what I'm saying right now but it transitions from the the kind of conventional web page to a larger immersive view that you can see from a distance cancel give it a sec Alexa tell Tess Shep what's the first step put your paia pan over the largest ring on your stove so you get the idea the the user can interact with the speaker and the the screen will respond accordingly in addition we have question answering models and things like that so we can ask questions like Alexa asked test chef how much rice do I need 350 gram rice 1 and 3/4 cup and so yeah the the system is kind of aware of the content of the webpage can answer questions about it and can help the user accomplish their task so we we hope you enjoyed this demo and you can check us out online AKA ms / mdx 