{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "import requests\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium --user\n",
    "# conda install -c anaconda beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfSec10k(cik_inp):\n",
    "    # set the central index key\n",
    "    cik = cik_inp\n",
    "    \n",
    "    # pass the main url from SEC\n",
    "    url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + cik + '&type=10-k&dateb=&owner=exclude&count=40&search_text='\n",
    "\n",
    "    # grab the html\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "    except:\n",
    "        print(\"An error occured.\")\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    list_td = []\n",
    "    for item in soup.find_all('td',class_=[\"small\",\"\"]):\n",
    "        list_td.append(item.text)\n",
    "\n",
    "    list_td_2 = []\n",
    "    for item1 in soup.find_all('td',class_=[\"small\",\"\"]):\n",
    "        for i in item1.find_next('td'):\n",
    "            list_td_2.append(i)\n",
    "\n",
    "    df = pd.DataFrame(list_td)\n",
    "    df1 = pd.DataFrame(list_td_2)\n",
    "\n",
    "    df.columns = ['table']\n",
    "    df1.columns = ['date']\n",
    "\n",
    "    result = pd.concat([df, df1], axis=1)\n",
    "\n",
    "    # Data cleanup step - 1\n",
    "    result['date'] = result['date'].astype(str)\n",
    "    result['date'] = pd.to_datetime(result['date'])\n",
    "    result['year'] = pd.DatetimeIndex(result['date']).year\n",
    "    del result['date']\n",
    "    \n",
    "    # Data cleanup step - 2\n",
    "    result['table'] = result['table'].astype(str) \n",
    "    result['table'] = result['table'].str.replace(' ', '').str.replace('[', '').str.replace(']', '').str.replace(',', '').str.replace('  ', '')\n",
    "    result['table'] = result['table'].str.replace('(', '').str.replace(')', '').str.replace('-', '',2).str.replace('KB', '')\n",
    "\n",
    "    result['table'] = result['table'].str.replace('AnnualreportSection13and15d', '').str.replace('Amend', '').str.replace('notSKItem405', '')\n",
    "    result['table'] = result['table'].str.replace('34Act', '').str.replace('Size', '').str.replace('MB', '').str.replace('SKItem405', '')\n",
    "\n",
    "    result['table'] = result['table'].str.replace('AnnualreportSections13and15d', '').str.replace('Accno', '').str.replace(':', '',1)\n",
    "\n",
    "    result['acc_no'] = result['table'].str[:20]\n",
    "    del result['table']\n",
    "    \n",
    "    index_check = []\n",
    "    for index, row in result.iterrows():\n",
    "        if row['acc_no'][0:2] == '00':\n",
    "            index_check.append(index)\n",
    "\n",
    "    result = result.iloc[index_check].reset_index(drop=True)\n",
    "\n",
    "    # Data cleanup step - 3\n",
    "    result['acc_no2'] = result['acc_no'].str.replace('-', '')\n",
    "    \n",
    "    result = result.drop_duplicates(subset='year', keep=\"first\")\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "    result_r11 = result.head(11)\n",
    "    result_r11\n",
    "    \n",
    "    # URL filing link extraction\n",
    "    df_furl = []\n",
    "\n",
    "    for x,y in result_r11.iterrows():\n",
    "        # URL extraction step - 1\n",
    "        acc_1 = y['acc_no']\n",
    "        acc_2 = y['acc_no2']\n",
    "\n",
    "        url_2 = 'https://www.sec.gov/Archives/edgar/data/'+cik+'/'+acc_2+'/'+acc_1+'-index.htm'\n",
    "\n",
    "        try:\n",
    "            page_1 = urllib.request.urlopen(url_2)\n",
    "        except:\n",
    "            print(\"An error occured.\")\n",
    "\n",
    "        soup_1 = BeautifulSoup(page_1, \"html.parser\")\n",
    "        company_page = [item_page.get_text('\\n',strip=True) for item_page in soup_1.select(\"div.formGrouping\")][1]\n",
    "        company_page = str(company_page)\n",
    "\n",
    "        r = requests.get(url_2)\n",
    "        df_table = pd.read_html(r.text)\n",
    "        df_tp = df_table[0]\n",
    "        del df_tp['Seq']\n",
    "        del df_tp['Type']\n",
    "        del df_tp['Size']\n",
    "        df_tp = df_tp.astype(str)\n",
    "        df_tp['Document'] = df_tp['Document'].str.replace(' iXBRL', '')\n",
    "\n",
    "        # URL extraction step - 2\n",
    "        doc = df_tp['Document'].iloc[-1]\n",
    "\n",
    "        url_3 = 'https://www.sec.gov/Archives/edgar/data/'+cik+'/'+acc_2+'/'+doc\n",
    "        df_furl.append(url_3)\n",
    "        \n",
    "    # Text URL output to dataframe\n",
    "    df_furl\n",
    "    df_out = pd.DataFrame(df_furl, columns=[\"final_url\"])\n",
    "\n",
    "    result_r11[\"final_url\"] = df_out[\"final_url\"]\n",
    "    del result_r11['acc_no2']\n",
    "    result_r11\n",
    "    \n",
    "    return result_r11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullTextAll(link):\n",
    "    URL_text = str(link)\n",
    "\n",
    "    # Grab the response\n",
    "    responses = requests.get(URL_text)\n",
    "\n",
    "    # Parse the response (the XML flag works better than HTML for 10Ks)\n",
    "    souper = BeautifulSoup(responses.content, 'lxml')\n",
    "\n",
    "    text_b = []\n",
    "    text_r = []\n",
    "    for filing_document in souper.find_all('document'):\n",
    "        document_type = filing_document.type.find(text=True, recursive=False).strip()\n",
    "\n",
    "        if document_type == \"10-K\":\n",
    "            text = filing_document.find('text').extract().text\n",
    "\n",
    "            text = re.sub('\\n', ' ', text)\n",
    "            text = re.sub('\\xa0', ' ', text)\n",
    "            matches = list(re.finditer(re.compile('I[tT][eE][mM] [0-9][a-zA-Z]*\\s*[.|:|-]\\s*'), text))\n",
    "\n",
    "            # Business section\n",
    "            try:\n",
    "                start_b = max([i for i in range(len(matches)) if ((matches[i][0].upper().replace(\" \",\"\") == 'ITEM1.')|\n",
    "                               (matches[i][0].upper().replace(\" \",\"\") == 'ITEM1:')|\n",
    "                               (matches[i][0].upper().replace(\" \",\"\") == 'ITEM1-'))])\n",
    "                end_b = start_b+1\n",
    "                start_b = matches[start_b].span()[1]\n",
    "                end_b = matches[end_b].span()[0]\n",
    "                text_b = text[start_b:end_b]\n",
    "            except:\n",
    "                text_b = None\n",
    "                \n",
    "            # Risk section\n",
    "            try:\n",
    "                start_r = max([i for i in range(len(matches)) if ((matches[i][0].upper().replace(\" \",\"\") == 'ITEM1A.')|\n",
    "                               (matches[i][0].upper().replace(\" \",\"\") == 'ITEM1A:')|\n",
    "                               (matches[i][0].upper().replace(\" \",\"\") == 'ITEM1A-'))])\n",
    "                end_r = start_r+1\n",
    "                start_r = matches[start_r].span()[1]\n",
    "                end_r = matches[end_r].span()[0]\n",
    "                text_r = text[start_r:end_r]\n",
    "            except:\n",
    "                text_r = None\n",
    "\n",
    "    return text_b, text_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1291080']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read cik data from excel\n",
    "cik_in = pd.read_excel('S10K_Companies.xlsx')\n",
    "del cik_in['cik_short']\n",
    "del cik_in['name']\n",
    "cik_in = cik_in.loc[21:21]\n",
    "cik_in = cik_in['cik'].tolist()\n",
    "cik_in = [str(x) for x in cik_in] \n",
    "cik_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'linOut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-51a053c46be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"An error occured.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinOut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mcompany_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span.companyName\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mcompany_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linOut' is not defined"
     ]
    }
   ],
   "source": [
    "# Extraction loop and file creation\n",
    "for rw in cik_in:\n",
    "    cikNo = rw\n",
    "    \n",
    "    # Getting the company name \n",
    "    addr = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + cikNo + '&type=10-k&dateb=&owner=exclude&count=40&search_text='\n",
    "    \n",
    "    try:\n",
    "        linOut = urllib.request.urlopen(addr)\n",
    "    except:\n",
    "        print(\"An error occured.\")\n",
    "\n",
    "    s = BeautifulSoup(linOut, \"html.parser\")\n",
    "    company_name = [item.get_text(strip=True) for item in s.select(\"span.companyName\")]\n",
    "    company_name = str(company_name)\n",
    "    company_name = re.sub('[^A-Za-z0-9]+', ' ', company_name)\n",
    "\n",
    "    n1 = company_name.split()[0].upper()\n",
    "    n2 = company_name.split()[1].upper().replace('CIK', '')\n",
    "    print (n1 + \"_\" + n2)\n",
    "    \n",
    "    dataFrameOut = dfSec10k(cikNo)\n",
    "    \n",
    "    # Method to pull text\n",
    "    df_text_bus = []\n",
    "    df_text_risk = []\n",
    "\n",
    "    for i in dataFrameOut.final_url:\n",
    "\n",
    "        adder_bus, adder_risk = pullTextAll(str(i))\n",
    "        df_text_bus.append(adder_bus)\n",
    "        df_text_risk.append(adder_risk)     \n",
    "\n",
    "    # Final output to excel spreadsheet\n",
    "    df_text_out_1 = pd.DataFrame(df_text_bus, columns=[\"text_business\"])\n",
    "    dataFrameOut[\"text_business\"] = df_text_out_1[\"text_business\"]\n",
    "\n",
    "    df_text_out_2 = pd.DataFrame(df_text_risk, columns=[\"text_risk\"])\n",
    "    dataFrameOut[\"text_risk\"] = df_text_out_2[\"text_risk\"]\n",
    "    \n",
    "    pd.DataFrame(dataFrameOut).to_excel(r'Temp_Files_2/' + n1 + '_' + n2 + '_' + cikNo + '.xlsx', sheet_name='Sheet1', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
