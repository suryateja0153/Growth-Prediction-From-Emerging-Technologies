 thanks Fraser so okay so I did an internship at Microsoft Research and we explored a possible use for I guess input this is one of the applications we developed so we will have this user in districts of VR experience to solve the jigsaw it's not a bit can you see anything within this picture let me replay this anything off so we just changed this puzzle piece so as to have the user solve the jigsaw to make the jigsaw easier to solve we do this with respect to the eye gaze input so we only change stuff that is outside the fovea you see here so for example we changed now the piece on the lower lower right so we do this a bunch of times and we also apply other techniques such as visual masking the cat is moving for example and we also pay respect to the users and to analyze the spatial relationship between the objects which are the puzzle pieces here so why are we doing this the overarching question we want to answer is how do we make users not notice something in VR and I've given you one example and two others total will give you a couple of more and the answer to this is muse onsen which is the creation arrangement of a set for scenery and you have this in different sort of media productions such as theater or film you see a certain certain thing because the director controls the field of view and you don't notice the other stuff such as the boom operator the camera person etc and this approach doesn't work with the our productions because the field of view is on a user control so we see this phenomenon here where in VR the field of view is increased eventually even to the full 180 degrees of human vision and with hatch backing this is even more Aero we cannot trol so our ideas plan is kind of simple we just use I guess I tracking to secretly update content in the arm so we decreased the Arabic inner control to the fovea vision of five degrees or in other words we increase the range for updates to 355° there's a bunch of related work on this two works are find interesting here which I picked out a spot after proxy and dynamics a category direction we used I guess and measured seconds to reposition the user or change the demotion mapping to enable in the first case passive haptics in the second real walking there's other stuff such as auto pager may be automatically page turn when the user has reached the end of the page all of this work is somehow possibly rooted in this phenomenon of change blindness or inattentional blindness and you probably know this work on the right here with user where the watch of this video cannot see the gorilla walking through the scene when counting the ball cause of the white team so we can summer rephrase this question now and I asked initially so how do we make you just not notice something in VRS the same as how do we apply these cognitive illusions in VR and there's two people who can help us with answering that question ten years ago Stephan Susanna where keynote speakers at this very conference in Victoria one year before at the published a magazine article saying calling to delusions are not sensory in nature rather they involve high-level functions such as attention memory and causal inference so attention memory and cause interference and this is what miss unseen delivers an delivers computation most models of those very three things with the computation model of causal inference we can prevent anticipation of a change that is about to happen but the second we can prevent observation of the change and with the third we can prevent recall of the change that has happened and we use gas to process you process gas on in different forms to form these all of these layers to do exactly that and I will give you three examples like the Forge and the gallery and the jigsaw you have seen earlier which all use the different subsets of those of those layers so let's look at the gallery and the model of current attention this is kind of the naive approach you would initially do so the users looking at the modern art piece painting on the right we change the painting on the Left also to modern art so as to individualize the content so we do it like this we'll get the gaze point the current gaze point we compute the convex hull of the object we want to change compute the distance we have are in this distance is above our foveal vision of five degrees I mentioned earlier then you apply the change there you go we can get a little bit more complex than that there is this notion of covert attention so where you look it's not necessarily where you attend to for example can you look at me and not attend and not look at the slide but attend to the slide so look at me attend to the slide you might be able to see the gorilla there same thing this is called corporate attention and you can only consciously apply it 300 milliseconds after you've done this account we can measure circles so we measure this account with a time in of 300 milliseconds but we can apply the change on that's it so this is another layer let's talk about the forge and the layer of spatial memory so this relates to haptics so we see two weapons on the table in front of the user but physically you have only one prop which is on the desk on the on the lower right so which weapon do you imagine - a physical prop from the user's case we infer that the users interested more access than hammers so we match the axe or to the prop but you also move the hammer so tlemen so we maintain the spatial relationship between the objects the user has internalized so how do you do this similarly before we compute the convex house of the objects and we implemented a directed directed graph between those objects and we have the physical problem so under now the user looks throughout the scene and we adapt the way it's depending on whether the user looks so if the user looks at and the action that the hammer will change to the right between them etc so now when we decide to move the axe we move the hammer with respect to the right between the hammer and X all right so we can also use this model of spatial memory to trigger the change to prevent anticipation of the change so what we got is basically a rate vector we've got a scripted weight vector we can compare them and if this falls below a certain threshold then we trigger the change so the scripted weight somehow corresponds to our notion of the user deciding to pick up the action set of the hammer let's talk about the jigsaw so the jig's application uses random triggers so we change the puzzle pieces and randomly but there's also tons of other stuff like visual masking techniques so the cat moves as a callback function to to the mise-en-scène and triggering the change in the scene that's also a low contrast we use with the puzzle pieces crossfade stuff like that we evaluated these three applications and more and we found that also on the left side there's Likert scale noticeability ratings of the participants so we noticed that miss unseen is indeed able to induce those covered changes but there's important difference here which we add duck into so we did another psychophysics study on the visual masking technique so we had the baseline where we let the where we pump up this little dot which the user needs to identify right in front of the user then we have the a phobia condition where we have the ball pop up somewhere in the periphery so representing the importance of eye tracking then we have the ad cross fade bond the upper left like fades in with the two-second frame and time window and the other stuff such as low contrast we add the task of the user needing to read the text and we had motion distractors and what we see is that the detection rate decreases with each visual technique we apply I want you to pay attention to the second one to the afforded condition so the offeror condition is better than baseline which represents the importance of eye tracking but with each you know with the etre masking technique this gets better so it's both it's require eye tracking and you apply and we need visual masking techniques and other words this is some sort of signature not to noise ratio we need to decrease the noise we introduce into the gene into the scene but you also can add reduce the second Birkin but we can also add noise to master signal so we implement a bunch of other applications where this might be useful so on the upper right for example is our hacker riddle the user needs to find the right code sequence here but the user cannot guess when the user guesses the solution correctly we changed the hint the user needs to understand some user has figured out the hint on the lower left which we can track through our gate tracking then we allow for the correct solution on the upper left there's a real working scenario which is similar to the passive haptic scenario so whenever the user is not looking we can accommodate for less walking space by changing here the weapon reacts there's other stuff for example on the lower left we trigger this low fidelity explosion depending on users case so this relates the co-created rendering on the lower right here this is our idea of a virtual locomotion and teleportation where we basically reduce and the update rate in the periphery to one Hertz and keep the 60 or 90 Hertz in the middle to reduce motion sickness so to sum up we develop mise-en-scène which is a software system that induces covert changes into virtuality scenes by having by preventing anticipation observation and recall of a change these models and masking techniques reduce notice ability of those coverage changes to zero we've shown found in a bunch of different users for those corporate changes and virtual reality this is basically an argument - what's using eye-tracking for yeah for hiding changes and virtual reality scenes this work mostly provides without for with and II want to thank Christian MA and ale so that was a missin seen my name Sebastian on that scheme thank you for your attention [Applause] Thank You Sebastian I think you're ready for your patients the noise I'm hoping that's taken care of now there's all the time for questions for Sebastian aren't here very cool talk thank you very much I wonder if you a comment on how resilient the effect is I noticed for instance when you switched the hammer and the axe you maintain the spatial relationship and I imagine if you swapped out paintings in a room with only two paintings people would start to catch on to it eventually do you comment maybe on like how resilient the effect is it how much you can change or not right so I think so I talk about those different layers right you need to prevent the observation of the thing and it's correct like in the gallery users commented a lot on like I notice a change because I looked at the painting before and now I notice it because I haven't used or optimized our model of spatial memory and that within that application so if you do that you can then just not trigger the change of these let's look at this before so by having implementing all three stages to kind of prevent that and of course it sets the limitation right so if user really has looked at everything and everything is kind of internalized what can you change right so this is an obvious limitation to the thing yeah ali reza Behrman arizona state university this is a really cool talk sebastian I'm very fascinated this reminds me of redirected walking in ways I was curious for the examples you've shown it looks like you're fixed it's not look you're sitting down basically you're not free for moving around physically in the virtual space is that correct with the passive haptic so sample over each one with these scenes in particular so as I look around it's a matter of my orientation I'm curious how my physical movement would affect this is it whether or not you guys start up so we have infected this in of course we can factor in a lot of other stars such as physical movement we also can do what I wanted to do is like some include some video processing to have dance have another model of visual saliency to factor all this and it get a get a better model true we only focus specifically on on eye tracking in this work and so do you see this being capable of evolving into something to complement physical walking in virtual environments said that again do you see this for the system that she built being capable of complementing physical navigation in virtual spaces yes so one of the examples on the upper left is is basically a sketch right so it's the same notion as the passive haptics example where we accommodate or change the virtual world of the layout with respect to what the user has decided and we can do that for passive haptics all right with the viscosity of physical drops right but we can also do that with this constant your physical walking space all right the change is just bigger and this is harder to implement because it's compass more of your all of your view I have a better singer from K 11 so I was wondering the people who noticed the effect how did they react to changes alright everytime you cheat you break immersion right so if something is not believable it's not good for the application and I guess it's a probabilistic thing same is a change blindness stuff so on the user studies on change blindness from Daniel Simmons from the Simons from the 90s at the same thing we're like 20% I know I'm like I'm guessing here other participants still notice the thing and I unsure of what we can achieve here it's like 80% are 1920 this is a super application-specific but some people do notice that's true but this is what we want to prevent with this still have time for questions we have more beads if not the other team the other side is running a little bit behind so I'll ask a prickly question that I'd never want to get asked there's potentially ethical implications to this where you're having people look at something and then they look away and now it's not what they saw before have you thought about how this might be used for evil or is it some other part where humans start to doubt themselves and what they've seen interesting we could let a brainstorm on that yeah let's try something out later on yeah definitely we can like it higher but Facebook or something and then like iterate on that Thanks 