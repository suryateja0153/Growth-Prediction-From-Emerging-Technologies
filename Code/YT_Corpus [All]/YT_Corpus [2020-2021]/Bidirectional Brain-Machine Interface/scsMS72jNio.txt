 Hello everyone, my name is Pavel Tolmachev. I work at the university of Melbourne, and I am going to present our recent work about Discrete Hopfield Networks which realises so called content addressible memory, or simply put, associative memory Creating associations is an inextricable feature of cognition. No surprise, people were trying to understand how the associative memory works. One can easily describe the basic principles: one gives to a some agent a partial or noisy clue, and the agent recalls the whole object. To give a real-life example: your friend whistles a tune and you recall the whole song. But up do date, nobody knows precisely how an associative memory is implemented in the real brains. One of the first models of associative memory was popularised by Hopfield in 1982. Since the brain is an interconnected web of neurons, it is natural to model the substrate of an associative memory by a network of nodes-neurons. So the Hopfield considered a network of nodes which can have two states +1 and -1. Each of them connected to all other nodes with various strengths. Of course, one can see the drastic discrepancy with the biological reality, nevertheless, it was a start. The network then was endowed with a simple dynamics, described later. The Hopfield Network, if evolves freely, will converge to a some fixed point. These fixed points are determined by the interactions between the nodes. Thus, one can say that the network can "store" some binary vectors - which I will refer to as patterns. On this example the network of 10 x 10 neurons stores 4 patterns - binary images. Suppose the network has already somehow learned, and, thus, stores some patterns. The process of recall then can be described as follows: one initialises the network by setting the states of the nodes to +1 and -1 accordingly and then the system freely evolves from it, converging to a fixed point. If the imposed vector was sufficiently close to some of the learned pattern (like on the picture), the network converges exactly to this pattern. But what's the mechanism which allows the network to learn, store and recall the vectors you want to store, and not some other gibberish? Since the fixed points are determined by the weights of interactions, one needs to come up with some clever way of setting up (or learning) these interaction parameters. Thus the purpose of this research: find the good learning rules to store as many vectors as possible. Plus, given some sensible level of the information corruption, these vectors should also be robustly recovered. This talk will mostly be focused on the pure science, bypassing practical applications. Hopefully, It can stir up some discussion about the topic and eventually lead to some more practical ideas. The number of patterns the Hopfield Network can store while robustly recovering them depends on the learning rule - an algorithm of setting up the weights between the neurons. The first learning rule applied to store patterns was Hebbian learning (discussed later). To increase the storing capacity of the networks, various other learning rules were proposed afterwards. In our paper, we take a new look at learning rules, exhibiting them as descent-type algorithms for various cost functions. We also propose several new cost functions suitable for learning. We also discuss the role of biases — the external inputs — in the learning process. Furthermore, we apply Newton’s method for learning patterns to speed up the convergence, and, eventually, we experimentally compare the performances of various learning rules. Put more succinctly, here is the dinamics of the Hopfield Network. Here you see one of the N nodes with an incoming connections from the rest of the network At each time step, a neuron calculates a net input h, which is comprised by the weighted sum of all the states of other nodes + a bias term The state of the node on the next time step is then defined by a sign of this variable h . To give an example of a learning rule, let's consider the Hebbian rule. Intuitively it can be explained as follows. Suppose the network is initialised with all weights set to zero. And one wants to store a pattern, denoted by sigma, as depicted on the picture. Then the reasonable thing to do, would be to increase the strengths of the connections between the nodes with the same sign while decreasing the strengths between the nodes, which are in the opposite states. The essense of this rule is captured by the expression in the box. To aid our further investigations, one may find the physical analogy useful. In the case of symmetric interacitons, a Hopfield network corresponds to an Ising model: a set of atoms with magnetic moments, interacting with each other. Initialised in some configuration, the system quickly relaxes to a local minimum in the energy surface, defined by the expression in the middle. So the fixed points correspond to local minima of some function defined over all the possible configurations. Ideally, one wants to make the basins of attractions of learned patterns to be wider and rounder. However, it is exremely hard to quantify the properties of the basins of attractions. Instead, we simply focus on making the patterns to be stable fixed points of the iterative dynamics, so once the system arrives there, it stays there. Suppose the network is initialised in a pattern we want to learn. The state of the network on the next time step is defined by this expression. Then, we want that some distance between the pattern and a subsequent state of the network to be zero. The sign function in the formula doesn't allow us to do much, so we simply omit it. Thus, to learn a pattern, one needs to minimise the following quantity. For one pattern, one can then write a finction to minimise and find the minima in the weights-space with the gradient descent (just by taking the derivative). It turnes out that some of the previously known rules can be derived from the minimisation of the distance function. For instance, the hebbian learning rule is an instance of minimisation of the following function. The two rules proposed by the Diederich and Opper could also be derived by minimising l1 and l2 norms of the difference between the current and subsequent states. Previously I've been talking about learning one pattern at the time, which is called incremental learning. In this case, if the network is crammed with the patterns it starts to "forget" the patterns it learned before as the new patterns partially override the previous ones. The other approach is to learn all the patterns at the same time, by minimising the sum of the distances between the patterns and the subsequent states. In the case of the distance function being an l2 norm, this approach has an analytic solution, which was described by Personnaz and his coathors. One can attack the problem of learning from yet another angle. For the learned pattern to be stable, the following inequality must take place for every node: sigma_i h_i 0, That is because, in the stable fixed point the pattern state-value on a particular node has to coinside with the sign of the net input to this node. Each new pattern to be memorised imposes a linear constraint on the incoming weights of the particular neuron. So, the task of memorising a new pattern is decomposed to N independent problems, each corresponding to a particular node (since the imposed constraint depends only on the locally available information). And each of these problems deal with finding the right vector of incoming weights. This property allows the network to learn the patterns in a distributed manner, disregarding what's going on in the rest of the network. Here, it becomes straightforward that the biases could be treated on paar with weights, as if they are weights of connections coming from the node from the outside of the network being permanently in a "+1" state. To recap, for one node, each new pattern to memorise corresponds to a linear inequality constraint, which defines an allolwed half-space in the space of incoming weights. Learning multiple patterns at once corresponds to an intersection of half-spaces, forming a feasible hypercone. The deeper the weights are in the hypercone, the more robustly the patterns are learned. The natural idea to keep the weights far from the constraint-planes is to introduce barriers, so that the minimised function has greater values outside of the feasible hypercone. Introdusing exponential barriers, one gets the following optimisation problem, which then could be solved by a gradient descent, independently for each node. This introduces a new learning rule: Exponential Barrier Descent. We also need to impose a regularisation on weights to prevent them from an unbounded growth. Reformulation of the learning task as an optimisation problem allows us to employ more advanced optimisation techniques. One may want to use Newton's methods to speed up the convergence, in case if the learning is one-off instead of incremental. However, in this case, the update rule requires an inverse Hessian, which contains non-local information, and the learning becomes centralised rather than distributed. We have done multiple simulations to compare the obtained learning rules with the previously described ones. For each of the learning rules we created a net of 75 nodes, initialised with random weights. Then the network has progressively learned from 0 to 75 random patterns (with the probability of two states being equal). At each stage we have chosen a randomly learned pattern and have introduced from 0 to 37 flips (changing the state of the pattern at some node to an opposite state). Each time after the corrupted binary vector been imposed on a network, the system converged to a stable fixed point, and we have measured the overlap metric between the intended and the recovered binary vector. The overlap between the vectors is 1 if the vectors are identical. We then have run the described above procedure for each of the learning rules for 100 times. The performance of each learning rule was evaluated by the position of the curve, at which the resulting overlap was 0.95. (As you can see on the left picture, the thick line denotes overlap 0.95. Below this line the overlap is greater than this value). Thus the higher the curve lies, the better the performance of the learning rule. Here we have pitted against each other multiple learning rules, and the descent exponential barrier rules (red and green curves) perform better than any other rules, especially in the high-load-low noise region. The only rule which performed better in the middle region (the blue curve) is a modification of a rule proposed by E. Gardner. This modified rule employed the weak-pattern first update strategy, proposed by Krauth and Mezard. During the learning the network has been cyclically choosing the least robust pattern and then reinforced it. But this update strategy may also augment the exponential barrier learning rules, which would then make it perform even better. Now, to conclusion. In our paper, we have formulated a learning task as an optimisation problem. Based on it we were able to make a connection of previously known learning rules with a particular objective function being minimised. Following this optimisation framework, we have also introduced a new learning rule which performs quite well in comparison with other rules. And tt still retains the distributed manner of learning and not being overly computationally complex. By reformulating the task as an instance of optimisation problem we also were able to employ Newton's method for learning the patterns. THANK YOU FOR YOUR TIME! And I really hope you have enjoyed the presentation! 