 (music) - Good evening. I am Mycal Brickhouse, Director of Alumni Relations at Duke Divinity School. On behalf of the Duke Alumni Association, the Duke Divinity School, Law School, School of Medicine and Sanford School of Public Policy, I would like to welcome you to this evening's conversation, Being Human in the Age of AI. In this challenging season, we were able to take this planned live event and make it virtual for all of you to enjoy safely. For that, I would like to thank everyone who has made this evening possible. This conversation was designed with an interdisciplinary lens, seeking to address the ethical question, what it means to be human, In a Milia of artificial intelligence and machine learning. This panel represents a small slice of the diverse range of Duke experts, able to address this topic. After tonight, please look out for a follow-up email that will list additional resources Duke has to offer. Finally, thanks to all of you, who took the time to join us tonight. It is our hope that you will enjoy tonight's program. I'll now turn it over to Dean Judith Kelley. Judith Kelley is the ITT/Terry Sanford Professor of Public Policy and an expert in the field of international relations and Dean of the Duke Sanford school of Public Policy. Dean Kelley, a political scientist, earned her PhD and a master of public policy degree at the Harvard Kennedy school and her BA at Stanford university. She joined the Sanford school faculty in 2002 and became Dean in July, 2018. Dean Kelley is a dual citizen of her native country, Denmark and the United States. And she is a proud Duke parent. I'll now turn it over to Dean Kelley. - Thank you so much, Mycal and welcome to all our audience members today. And I'm pleased to introduce our panelists. Starting with Luke Bretherton. Luke Bretherton is the Robert E. Cushman Professor of Moral and Political Theology and senior fellow of the Kenan Institute for Ethics at Duke University. He's worked with a variety of faith-based NGOs, mission agencies, churches around the world, particularly in Central and Eastern Europe. And he's written several books, including a book called Resurrecting Democracy, Faith, Citizenship, and The Politics of A Common Life. And that focuses on the themes, including the church's involvement in social welfare probation, the treatment of refugees, radical democracy, globalization, responses to poverty, and the patterns of interfaith relations. His primary areas of research are supervision and teaching, Christian ethics and moral theology, the intellectual and social history of Christian political thought, political theology, the relationship between Christianity and capitalism, missiology, the practices of social political and economic witness. So welcome Luke. And our next panelist that I would like to introduce for you is Murali Doraiswamy. Dr. D, as I've also learned that he goes by, is a highly respected doctor, brain scientist and health technology researcher at Duke University School of Medicine, where he serves as Professor of Psychiatry and Medicine. He has served as a principal investigator on landmark clinical trials and played a key role in the development of many diagnostics, therapeutic and mobile health innovations, widely used in medicine today. Murali has led global efforts to develop ethical standards, minimize biases, and generate evidence that we need to deploy AI and digital technologies in several areas of health. He's been an advisor to leading government agencies, businesses and advocacy groups, and served as a chair of the World Economic Forum's Global Agenda Council on Brain Research. Welcome Murali. Finally, Jeff Ward from the Law School, is the Associate Dean of Technology and Innovation and serves as the Director of Duke's Center on Law & Technology which coordinates Duke's leadership at the intersection of law and technology with programs such as the Duke Law Tech Lab, a pre-accelerator for legal technology companies, and the Access Tech Tools initiative, a program to help students and Duke's community partners to employ human-centered design thinking and available technologies to create tools to enhance access to legal services. Jeff focuses his scholarship and professional activities on the law and policy of emerging technologies. The future of lawyering, and the socio-economic effects of rapid technological change. He has a focus on ensuring equitable access to the tools of economic growth and the resources of the law. Welcome Jeff. So before launching into my first question, I want to set the stage a little bit by providing just a couple of examples of recent developments in artificial intelligence, that directly interface with human cognition. And these examples come from a podcast called, Should This Exist, in case you were interested in hearing more about each of them. So the first example is, so what if your computer could read your face? Could read your emotions? There's a company called Affectiva that is developing an AI tool that can detect universal emotions, such that your computer could react to how you're feeling and that could make tech more human, but could it also invade our lives. Or another example is, what would happen if we could have therapy without therapists? If we remove the human therapists from the therapy interaction. There was something called Woebot, that uses artificial intelligence to just guide the user through a therapy session, anytime, anywhere, and this could increase access to mental health and also reduce the stigma around seeking therapy. No human therapists required, or what if you could learn as fast as a kid again? Would you? There's a headset apparently now that you can get called Halo and that can stimulate your brain as you're learning and be tailored to you. Right now, its use is limited to physical learning, such as athletes and musicians, but someday it could help anybody master anything. And would that be fair? Finally, I am speaking with my own voice tonight, but apparently there's a tech that could change your voice to anything you want. I could be Michael Jackson if I wanted to. You can modulate, it could help people use a voice that represent who they want to truly represent themselves to be. But what's real if you can't trust what you hear? So in these and many other ways, is artificial intelligence changing what it means to be human? So my first question, which I'll post to all of our panelists is, if one of the features that defines us as humans, is advanced intelligence, in what ways are our understandings of what it means to be human challenged by AI? Luke, I'm going to go first to you. - Well it's a delight to be with you all. I'll kind of set up things. I'm sure Murali and Jeff have much more detailed reflections on this, but as a broad strokes, to set the reflections off, I think it really, this question depends on what you think it means to be human. So there's kind of prior question, which this question is nested within. So if you think of the human as a calculating machine, and there's a long history of that going back to Descartes and others we could trace it back to, or if you think of the human brain via the image of a computer, you're going to see AI as a rival. You're going to see that there's a problem here. If you imagine and narrate the human in terms of that, we're desiring storytelling, meaning and purposing animals, then you're not going to think AI as a rival. It's going to remain within the realm of the tool, rather than a kind of equivalent to a person. So there is this prior question of how do we imagine and narrate to what it means to be human and how do we situate AI in relation to that. I think AI does kind of shine a light on questions about the meaning and purpose of humanity and how we understand what it means to be a human. We see that played out in philosophical debates around post humanism and trans humanism and these ideas that you can upload human consciousness to the cloud and this kind of thing. But I think that these are kind of questions begging moves that actually should force us to ask more fundamental questions of the meaning and purpose of the human. We've got a millennia of debates around that and many precedents for thinking about that. So I don't think there's a kind of step-change or point of rupture with previous generations, but I think AI does force us to ask these questions in kind more salient ways and presses them in particular ways. - Murali do you want to go next? - Sure. I really enjoyed what Luke said. You know, about 150 years ago when Darwin came up with this theory of evolution, a lot of us were like pretty upset probably back then, saying how can it be possible? You know, apes are so different. And then genetics revealed that, gorillas are only like 2% different from humans in terms of their genetics. And we said, that's okay. We still have our mind and our brain as you pointed out and that's what really separates us as humans. But computer intelligence is growing at such a fast rate and it's sort of beating humans or starting to beat humans in many of the areas that Luke mentioned. We thought emotion recognition was safe, but as you pointed out Affectiva, and there are a number of programs out there that can recognize faces, that can recognize various types of emotions. Even Alexa, at some point can probably recognize your emotions because Amazon has this huge price to make Alexa as sensitive to your emotions as any human. I think what's going to happen in the next 20, 30 years is that, the Turing price is going to change, right? So the Turing price is how can we design a computer that is so human-like that you cannot tell the computer from a human, the next price is going to be, can we find a human that's so human that cannot tell it apart from a computer. So being human, like a human, is going to become sort of a rare event I think. - Jeff, do you want to jump in here? - Sure. I'm just excited to be here. So first of all, I just wanted to say thank you. And it's a really nice opening question as well. There's so many ways in which I think, technological change challenges, as Luke said, are our conception, it's another mirror to look back upon what we've been doing for millennia. One way that I tend to focus on our cultural situation and one way that I think we've often understood ourselves, actually let me give two ways and one that's positive and one that's negative in this change. A couple of ways we've understood ourselves to be human is part of socially stratified cultures, right? I wouldn't have said this before, but I liked your example of Woebot, right? And how you use that as access to an essential service, mental health services. That AI may provide that wasn't there before. So in some ways we could imagine that, one challenge is that our highly stratified hierarchical sense of our place in this world could be challenged by greater access and equity. And wouldn't that be wonderful? But I think something that I feel more strongly about is that we've often understood our identity, our human identity as our place in a culture. And it's intimately tied with work and profession and those kinds of things. And to go back to Murali's comment about, the development over time and the increasing capacity, there was a time when we understood ourselves totally in the fields as part of our identity. And then machines came along, the steam engine, the cotton gin, et cetera, and radically changed our conception of ourselves there. And what we did was, we narrowed the category of tasks that we said, "Oh, this is what makes us human." Okay, a machine can spin cotton or whatever, but now it's the tools of the mind. And then what happened is over time we started to get AI and other tools that start to actually creep in that way. But we've said, "Oh, well, it might be able to replicate me for routine tasks, but it's my empathy and my creativity that is what makes me human." And we keep moving into a narrower and narrower category. And if you don't mind, I'll tell a quick story. If you haven't seen it, you already recommended a podcast. I'll recommend the documentary about AlphaGo. DeepMind, which was purchased by Google, had a tool called AlphaGo and it played the game of Go, which is much more complex than chess, people probably know about DeepMind and Garry Kasparov, but it's this artful game that's been passed on over millennia. And it's more potential moves in the game than there are atoms in the universe, and they played this AlphaGo tool against Lee Sedol, the world reigning champion in Go, and what's beautiful about the documentary is not only skip to the end, you know this from the very beginning, AlphaGo is going to win. Okay I'm sorry I did ruin it. But the part that is beautiful about it is this moment when the AlphaGo machine starts to put stones on the board in a place where no human would ever do it. And in fact, the announcers like the sport announcer said, "Oh my goodness, it's gone off the rails. This is it. No machine can beat Lee Sedol." Et cetera. But pretty soon the accumulation of stones, wipes Lee Sedol off the board and that very moment is a special moment that I hope everybody gets to see, because what it is, is it's a demonstration of a machine, teaching humans in a creative way, that no human creativity had ever accessed yet. There's an element there of machine driven creativity that I think teaches us that, the way we understand ourselves to have a special set of skills, that machines don't have access to, is getting more and more challenging to hold onto. And so I think that's a fundamental challenge. Where will we draw that line, in what differentiates us from the machines? - I mean, just a quickly, just to jump I think those are wonderful points. And I think that points to the way in which the distinctiveness of the human and the distinctiveness of the machine and the ways machines perceive and process is going to be different from how humans perceive and process. And we often conflate these two things or get anxious, because we insist on seeing them as equivalent or parallel. And so I think there's an important way of differentiating, but I think there's also a sense in which, the stories we tell about the human and about the responses to our environment, So kind of classic myths, like Prometheus's is a story which modern literature has used to think about its responses to technology. We steal fire from the gods and through technology make ourselves more godlike and release ourselves from the world of necessity and the imprisonment of nature. And this is a story of machines enabling human transcendence and how does AI threaten that, cause they are going to transcend us or is it Pandora? We open up a box of tricks, which causes mayhem and havoc. And in some ways, it's less the question of what it means to be human, but more, what are the kinds of stories we think and live by? How do we make sense of the reality that AI is bringing into being? And therefore kind of how as meaning-making creatures, we're actually interpreting the meaning of these things. And I think that's why we often get very anxious about kind of AI as a rival. I think we need to pay more attention in a sense to the stories we live by and the stories we situate AI within and therefore, what kind of meaning and sense of purpose of those setting up, how we interpret this technology. - Before going on to the other questions, I want to dwell a little bit more on this, because this is the central question of the evening. What does it mean to be a human in the age of AI? And so Murali and Jeff, I want to invite you to weigh in again on what Luke has said, but a couple of things struck me on what you said Jeff. You referred to machine driven, right? And then Luke starts to talk about, well, there's some point at which machines potentially drive themselves. But then Luke you also talked about purpose and I wonder, can a machine have a purpose? Or more ultimately Luke you're from the Divinity School, can it have a soul? So Murali, I don't know, as a doctor, maybe you can tell us whether machines have souls. Have you seen any in your dissections? - Well, I'm also a Hindu. Instead of soul, let me talk about another neuroscience concept, which is called freewill. Now, because freewill, could presumably be one other feature that distinguishes humans from a machine. And you might say, I have the free will as a human being to decide at any moment to do whatever I want. But it turns out actually that freewill might be a little bit more of an illusion. And there's a number of neuroscience studies that show that there are actually neurons in the brain that fire before you are knowingly aware of the action that you are going to take, and your smartphone in your pocket, can probably predict where you're going to be within like, you know, 100 meters, probably knows you better than most people in your family know you. So I think they issue of freewill could also be one of those chips that Jeff was mentioning might be knocked down and so we might have to put ourselves in a smaller and smaller bucket. And talking about Woebot, psychiatrists, we thought we were so human that patients would prefer us because we have empathy, we have this complex decision-making skills where we can integrate the human nature of it. Oracle just did a survey about two weeks ago of 12,000 employees. And 82% said they would prefer to talk to a robot than a therapist. I don't know if that's a reflection of psychiatry here or, you know, in this current era, but I just want to tell you even that has been chipped away. - Interesting. Jeff, any last thought on this topic before I move on? - Maybe one quick thought that ties together, both Murali and Luke. Back to this kind of reflection upon us, I do think that as technology moves, we rethink the way we've understood ourselves. And so you also mentioned Affectiva, which is this notion of, can we empathize? Can a machine empathize? And that question can be changed and then say, well, what does it teach us about the way we empathize? And we've often associated that with something like soul or human connection and both those things might be right, but it's also pupil dilation, right? Our perceptions of imperceptible heat changes in people's bodies and dilation of their pupils et cetera, things that could be mechanized. And that does change in the same way that a conception of freewill might be explained by neuron firings. It challenges us to better understand ourselves. And I'm not trying to take away a special place for us, but I do think it's important to be honest with what's actually going on when we do something like empathize or make a free choice. And oftentimes these things are more mechanical, chemical than we've previously acknowledged. - I do want to just push it back a little bit. In classical Greek philosophy and in Christian theology, the notion of animal's soul, isn't some hidden mystical bits floating somewhere in the boat. It's the idea that we're psychosomatic wholes, and it's the combination of will, desire, and agency. So it's just a way of talking about the ability to act as a psychosomatic whole. So there's nothing kind of mystical about it. And so I think that we're running into a problem when we have this knockdowns theory of bits of distinctly human things like will or language or tool making, either trying to separate out a specific category and then load that with what it means to be human. The more ancient idea encompassed in the notion of animal is that we are psychosomatic wholes. Is a physical, willing, desiring unity of being and the machine or an AI is never going to be that, cause it doesn't have a body in that sense and it doesn't have that kind of, we might even talk about it as emergent set of properties that come through being this particular kind of animal. And so I think that we get confused when we try and locate one specific area or thing as the human language will whatever. And we just need to think of ourselves. We're fleshly, sweaty, flatulence bodies that can do certain kinds of things. And when we think about ourselves in those terms, like we don't get confused that we have anything machine-like about us. So I just think we need to have a more humble, attend to our sweat glands, and we'll probably keep a sense of (mumbles). - As long as we don't get to flatulent machines. (laughing) We can continue to reflect on this theme throughout the discussion, but I want to move on to talking a little bit more about health, both mental and physical of humans, because we've been talking about things that machines might be able to do that make them more like us, but there are also lots of ways in which machines and AI is being applied to help humans who could be assistant in various ways, either to enhance capacities or to replace capacities that might be, you know, absent due to injury or other reasons, or even something like I was thinking the Affectiva if you could imagine people who have a hard time reading other people's emotions benefiting from something like this. So Murali, is AI enhancing health and wellbeing? And if so, are there any risks from AI when we are applying them to health? - Yes, absolutely. I think AI is starting to enhance health and has a huge potential to enhance health. I'll give you a couple of examples. We're all in the midst of a pandemic, right COVID. So would you like to know 72 hours before you actually develop COVID symptoms that you are going to develop COVID symptoms? And would that be so useful? Because you could isolate yourself, you could take precautionary measures, et cetera, et cetera. So there are a number of studies now that have shown that an AI powered smartwatch, can actually detect with almost 85, 90% accuracy based on variations in your heart rate and a few other metrics. So that's one example. The second example is, there are, I don't know, maybe like 50,000 publications on COVID that have been published since the pandemic began. No human doctor could ever digest all of those articles and come up with any meaningful conclusion, whereas an AI program can. And so this has being applied in multiple ways throughout medicine. And I'll just give you an example. So humans obviously, human doctors are slow, they make errors. The average doctor maybe spends about seven, eight minutes seeing a patient in a busy hospital. So that's what we call shallow medicine, right? So hopefully with AI, the machines are very fast. They can be standardized, they make fewer errors. Would you rather have a neuro I mean, I like neurosurgeons, but would you rather have a neurosurgeon with a tremor operating on you or would you rather have a machine that is so precise and it operates with something called the eye knife. The eye knife not only is able to make a very precise incision, it can biopsy the tissue, it can immediately tell what the pathology of the tissue is, and it can scan your computer and read 5,000 articles on a particular brain tumor and make some decision that pops up on the computer screen. So clearly I think for decision support purposes, for treatment purposes, for a variety of synthesizing information type purposes, it's going to be enormously useful. Now the risks of course, are not at the AI. To me, there's this old saying, "I'm not afraid of AI, I'm afraid of the human being programming the AI." So how good was the evidence? How well sort of, what does it tested? Are there biases? Are there privacy leaks? And last but not least, is there a black box algorithm that we don't understand? So these are some examples of some of the risks and yes, of course you write a bad line of code, you can kill someone. - What do you think Luke, is human flourishing possible with AI? We have examples now in Japan, for example, I know of robots keeping elderly people in particular company. So flourishing, not just in the medical sense, but in the mental sense as well. - Yeah, I think, in a sense is human flourishing possible, with AI, of course, and Murali has given some wonderful examples how it can contribute and we can multiply those exponentially. And so this question is, but I think there is a broader question when we look at it in the round, is AI contributing, will it contribute, we're trying to make a prediction, will it contribute or not? And here, I think we have to think through three points of kind of drawing out from philosophy of technology. The question of affordances, the question of inferences, if you like, and the question of agency. In public culture, we have this view that, guns don't do bad things, it's people who do bad things with guns. Technology is neutral, it's the user that which causes the problem, on the one hand. We have this kind of very deterministic view of technology and you get it, like there was a kind of very clunky history that somehow the stirrup caused feudalism or the clock generated capitalism. So technological developments change human consciousness, and that technology philosophy or technology has moved on from both of those. And we have this notion of affordances that technologies don't necessarily tell you what to do with them, but they do have drifts, directions of travel. They invite certain kinds of responses to themselves. So we have to think about what kinds of responses, what kind of drift does AI tend to? The other aspect is, what we might call inferences. And this is a kind of cultural anthropology idea. You know, if I take, AI as a technology, doesn't operate by itself, it is always situated in a culture and in a political economy. And so if we look at our political economy and our culture, it's a consumer's culture, it's got heavy concentrations of power in the state, heavy concentrations of power in monopoly, capitalism, things like Google. How is AI likely to be read in this kind of political economy and in this kind of culture? And then is it going to enhance agency? Is it going to create more distributed structures, more peer to peer governance? Is it going to enhance democratization or not? And I think on all those fronts, so give you one example outside of this, you think about a nuclear power station or a coal-fired power station, that inherently demands massive concentrations of power. It requires kind of states to build a nuclear power or state back structures. So it inherently concentrates power, both political and economic power. Is AI going to be more like a nuclear power station, or is it going to be more like the humble mobile phone, not smartphones, but humble mobile phones, which, you know, if we look in Africa, they've radically enhanced agency of lots of people have access to banking and health and been generally a boom on communication and enabled democratization. So we have these, the reception of technologies and the kinds of things they generate. I think the kind of early reports in, are not great. In this kind of culture, in this kind of political economy and with the affordances of AI, it does seem to lend itself to forms of surveillance capitalism and concentrations of economic power, and Google would be an example and concentrations of state power, social credit system in China being an obvious example of that. And so I think now AI doesn't have to be used that. I think the problem when we stack up the affordances, the inferences and how these are impacting agency, what AI, the challenges it poses is over and against its own affordances. It demands a huge amount of intentionality and counter design to ensure it does enable human flourishing and increase agency and increased democratization. My own view and Murali, and Jeff might have different views on this. I'd be interested to hear is I think it does tend to way from the level of political economy and can culture a way from enhancing human flourishing as it stands because of those contextual and affordances issues. But that's my view. - So we'll ask Murali about this in a second, but I want to ask Jeff because Jeff, Luke has opened up not one but two, multiple camps of ethical awareness and so the obvious question then is who should be monitoring, who should be providing the ethical supervision? - 15 minutes before the session, Judith, I just finished teaching the last session of my frontier AI & Robotics: Law & Ethics class. And I really should have just had them listen to Luke's last three minutes because he wrapped up pretty much everything, I hope they walked away with it. So nicely said, I couldn't agree more. And I especially agree and this connects to your question with that kind of epic intentionality that's needed in order to counter these drifts. Science and technology, philosophers and others like Sheila Jasanoff will say, not only is there a fallacy of technological determinism as if it marches on, on its own and that therefore our role in its development is diminished. That's a fallacy, we have a role. She also talks about the myth of technocracy. And this gets to your question. I think we assume particularly with complex topics and people's eyes, when you say machine learning, will glaze over and say, "Oh my gosh, that's for a special breed of human being to deal with and that's not me." They think that it's not for them. And I think that's really dangerous because the answer to your question then Judith becomes, not me, it's the technologist. And I would argue that we're seeding a tremendous amount of power to technology. So we already have the concentrating forces that Luke, so articulately expressed. And on top of that, we have the sophistication and perception of sophistication and technocracy, that requires a certain person. And I would argue so strongly, it takes everybody. Because every AI system and machine learning system is situated in a context, as Luke said, I call these socio-technical systems and we never talk about an algorithmic system, always an algorithmic system always. And an example would be, you know, Dr. D needs to be part of, a system that's trying to diagnose diabetic retinopathy. Because he's going to make sure that even if the overall optimization moves from 89% accuracy to 94, that there's not a subgroup that actually moves from 89% down to 48% or something, so that there's some group that totally misses. In that domain, maybe accuracy and non bias are our highest values. And he understands that because he's boots on the ground in that medical field. Move over to something that's more illegal in nature like, credit determinations, or what makes the news like sentencing, algorithms that help... There it's not only non-biased, which is important, accuracy is important, explainability in terms of intuitive explainability is required under our conceptions of due process. So there we have other experts that need to be part of building that process. So that kind of epic intentionality requires that the answer of your question be everybody, not just the technologist, if there are technological problems, we can't assume that there are technological solutions. It needs to be everybody. And I would add one additional element to that, that it can't just be professionals like us either. I think it's really important that we not bring to bear on this, an assumption that it's people of privilege who can be adequately representative of all the stakeholders involved, that we need to find new ways to build round tables that can bring all stakeholders, all communities to this, and engage in AI development. That would be my very strong feeling. And again, Luke, I wish you would have just given the last three minute talk to my class, that was excellent. - Judging from the amount of head nodding that I saw both by Luke and you Murali, rather than have you repeat the excellent answer that Jeff gave, I think I want to move on in our discussion. (background noise drowns other sound) No, beause I'm moving on. Because the next question, I think it's really interesting too. I want to make sure I get to it. We are beginning, what does it mean to be human in the age of AI? And we talked about us as human, what does it mean for us to function as human and how machines sit from us et cetera. But, what about when we start thinking about realities that are created by AI? So AI can now write stories by themselves. We have virtual reality that can be enhanced and you can have experience a reality that's not a reality. And this sort of takes me into the question of the whole notion of facts and truths and reality. So what does that mean? So Luke now I will let you say something and if you want to go back to the prior topic, you may do so, but if I have enticed you enough for the new topic, you can stick to that. - I'm thoroughly enticed. I'll get engaged with the question. Yeah, no, I think this is a really important question. And I think they, I mean, Jeff's already touched on it in terms of issues of bias and impartiality. And we see that, whether it's issues of sentencing, predictive policing, hospitals admissions policies, the data that's put in and the biases that get baked in, I'd be particularly interested in questions around credit scores and credit decisions and how the data it builds on action and Margaret who has a great article on this, talking about algorithmic Jim Crow and how you see-- - Luke, predictive policing just for our audience being when police is using artificial intelligence to predict where, what neighborhood-- - Crime spots happens. And that will build on prior datasets. And we know certain communities are overpoliced and therefore just replicates that over policing, but part of the problem, and this is a good example, it takes on the sheen of neutrality and therefore it appears more factual. And this is a good again modern philosophical problem, that is baked into the university system and baked into how we tend to think about these things of a fact value distinction. And so we tend to think, "Oh, well, that's just a fact." And then I'll impose my value on it. And we don't see how the creation of the fact, is actually resting on all sorts of prior values, whether that's prior credit making decisions, prior policing structures, prior hospital admissions policies, whatever it is. So I think there's that problem around how facts, the kinds of what gets kind of put out as factual because of the ways we absorb and approach technology, hides all sorts of biases. So that's one set of kind of questions. I think there's another set of questions around one of which is the kind of question of, if you talk about truth, what if we substituted the word truth for wisdom and how does AI enhance wisdom or not? And here, I think we face a very tricky issue and against the kind of affordances issue. A good example is the use of Google maps, very common use. My mother was complaining about my niece to me that she went somewhere, stayed there for a week and had no idea where she was. She just punched in the address in Google maps, and they have no consciousness of the kind of county of England it was in. My mother's obsessed with counties of England, but the ways in which AI is designed to lead to a more frictionless passage through the world, and it's supposed to make you not have to pay attention, not have to use your critical faculties, not have to learn the root and develop the memory to do that. And so the affordances of the technology, generate a lack of attention, in a sense that a lack of wisdom creation and habitual practices of coming to make wise judgments and discernments and discriminations. And so I think that's a much more tricky issue to get at. I think it's one that plagues a lot of technologies and how they're designed and how we experience them. But I think it's particularly pressing in AI where we're using it to replace human decision-making and therefore our ability to discriminate when should an AI be deployed or not. And what kinds of ways in which does AI undoes our ability to make wise judgments in the round. - So not only may technology be able to produce its own versions of reality, but also by depending on it, we become less capable of discerning the differences. - Yeah. And so I think exactly as we've seen in the current election cycle, and prior to that, 2016, when you have processes where we're essentially outsourcing decision-making, and we're increasingly through the tech, if I can just rely on Siri to come hold my memory, then my ability to decipher and discern, is this a factual statement? Is this a true statement? And the only way to do that in the current environment is you have to chase down sources, you have to make quite subtle judgments about news sources about does that feel right when that spam email comes in? Is that from someone in my firm or is that a fake thing? And we use all sorts of peripheral and intuitive and ways of knowing, but if we overlearn technology, we kind of unlearn, we deskill, technology can deskill us from being able to make wise judgments. And I think that's a more subtle issue, but I think a more profound issue on this question of fact and truth, what AI demands of us is a greater ability to discern. And yet the very technology itself, deskills us from the very processes of being able to make those discernments. - Jeff facts and truths-- - Judith if I may add something. - Sure. - I'm going to take Luke's argument to one next level. In fact, it's great to see a professor of theology and a neuro scientist agree. So that's great. So if you take Luke's argument to one next level, So basically the theory would be humans are going to get dumber in some ways, we're not getting wiser because, facts are being sort of chosen for us and we are not paying as much attention and we don't have the ability to discern and go deep. And then if the genes select for that trait, then successive generations will become dumber and dumber and dumber. I'm just taking this to a hypothetical next level so we could end up with a very dumb human race down the road. And in fact, there was a neuroscientist who wrote a very provocative editorial on exactly this topic. Saying that with the advent of machines and in the old days, people with lower abilities couldn't survive, they were less likely to be chosen as mates, but now a Instagram influencer, could be like the most attractive mate ever. - Jeff, did you have a comment on facts and truth? - Yeah. I mean, I think this is on everybody's mind right now. It's a great question because of the election and disinformation is plaguing us. If you look at effective machine learning itself, deep neural networks, about 10 or 11 years ago, when they started to get effective, there was a good gap of five years where there were almost no academic papers on bias. We've all mentioned bias here. And in the last years, if you look, there's been an upshot, everybody's trying to catch up. If you look back five years ago to generative adversarial networks, the Ian Goodfellow paper, which really was some of the primary science behind these fake digital artifacts, Judith, that you're alluding to the fact that we can send out into the world or GPT-3, which is a new technology that's really producing text in that way. We haven't caught up yet, the answer to your question, we're struggling to figure out how do we do that. On the faith side of it, or like my optimism side, there's the Masaccio's Holy Trinity hanging in the back of the church, I'm sure apocryphal stories were that people thought Jesus was behind the alter because they'd never seen double point linear perspective before, we caught up to that with the Renaissance. When I went and saw the what's the movie, the found movie, the Blair Witch Project, I remember being freaked out with my wife thinking, "Oh my gosh, they found this video. And these people were murdered in Pennsylvania." 20 minutes later, I understood, found video and I had the wherewithal to discern, it's getting harder and harder, but I do believe in a human capacity to discern truth, real from unreal, but it's getting harder. And it actually, I think goes back to the intentionality comment. It's happening so fast. We have to be deliberate in giving tools back to ourselves in ways that we can discern what's a fake versus what's a real digital artifact, because the risk is not just that we may make a mistake on one particular set of facts, is that we lose the ability to trust overall. And we feel the need to question. Once we have a lack of shared truth or the ability to reach a shared truth, a common truth, I worry about our ability to form communities, democracies, et cetera. And so there's a real risk here, I think with the power of generative tools to create fake digital artifacts. And we need to be, again, very intentional, deliberate, and aggressive about giving ourselves the wherewithal to discern. - Thanks if we have time we'll return to this question of democracy on so many people's minds, but Murali I'm supposed to ask you a question next about whether or not you believe that AI will overtake the human brain and the human IQ, but I'm now wondering given the previous comments whether maybe AI just needs to stand still and just wait for us to get tougher than it. But regardless, you can take the question in either format. Is it going to get out ahead of humans, is the basic premise of the question. And I think fear that like, basic science fiction writer has always been writing plots about. - So the answer to that question, depends on how you define the task or the challenge at hand. So obviously for narrow tasks, AI can already beat us. A calculator can beat us, a computer can perform 10 billion calculations. Human mind cannot even approach any of that. And interestingly enough, the standard IQ test, which humans have valued for a long time, if you look over the last hundred years or so, human IQ has gone up by about 20 points. And the prediction is over the next 40 years it will only go up by about four or five points because we have already maxed out nutritional gains educational gains. All of those gains that have happened over the last century. Now the Chinese have developed an AI program that can beat a segment of humans on the standard IQ test already. So if you're just measuring IQ as IQ, that can beat it, whether a machine will develop, what's called as general intelligence. I have no idea when that will happen. If you listen to Elon Musk, he predicts in the next 30, 40 years it'll happen and we're all going to be slaves, but I'm not so sure that it will happen because humans are programming AI in their own image. So it's unlikely to, and my prediction is that in the next 20, 30 years, we'll change the definition of AI so that it will no longer be just human AI, but it will be human plus machine. So an augmented AI will be the measure of human AI that we will use. - Interesting. So I'm going to switch a little bit since we have our alums with us today, and they're all sort of putting back on their student hats and thinking about the student days. So I want to ask you how each of you were all teaching and training students to prepare for this age of AI. And I want to combine that with a question we got from an audience member in advance, asking whether or not there's a greater role now, more than ever for a liberal and humanities based education for our students, or theology based education for our students. So who wants to go first on that? - I wouldn't mind starting. I love that question. And so whoever sent it in, thank you so much. Going back to where Mycal started us off about this interdisciplinary lens. I think one of the answers to that question is that, almost none of these questions, if you take serious that notion of socio-technical system, can be answered by one discipline. You have to have medicine and divinity speaking to each other. You have to have law, public policy and philosophy, speaking to each other. I think that's essential. The other is that on the humanities, I talk incessantly to the point that my students are probably so sick of it, about the narrative imagination. Here I am, I teach at a law school on a technical subject, and we talk all the time about narrative imagination. And we bring in the arts, we start the whole semester with a short story. I mean, I don't understand how we can prepare ourselves for a rapidly emerging future, without a really robust ability to imagine many paths forward, to do so creatively and to do so in ways that engage all of our human sensibilities. And so the arts for me, the humanities, asking questions about what it means to be human, these are absolutely essential, to our ethical decision-making going forward and the ways that we put demands on our technology. So I see a huge role. I think the walls of our institution, which are already so low and allow us to work together and do things like this together, need to almost go totally away. We need to find new ways to enhance that interdisciplinarity and think about problem solving over teaching a discipline. And I think we're well on our way to that. I think we're way ahead of other universities, to be honest in a lot of ways, but I believe very strongly. - Excellent. Murali do you have thoughts on this question? - I think we need to make medicine more human. So definitely the humanities are going to be essential for medical students and doctors. They are so busy and so caught up in technology that they forget that you are there to treat the person and not the disease. I would love to embed every healthcare AI startup with a humanities student, or have a humanities person certifier saying this AI is sort of meets the criteria. - Luke? - I would say that humanities always has this kind of huge anxiety problem in the university. So this is just, my soul is glowing at this, the importance of humanities. I just completely agree with it. There's a great story of a robot Stanford was designing and they'd put it into a mall in California and it was designed to avoid children. And of course, you know, children being children, they want to chase it, if you move away and it's a robot that's super cool. They want to play with it. And then the robot would move away and then the children would chase off and eventually they caught it. And the robot topples over and kind of hurts one of the children, there's a law case. And it's what a classic kind of classic case of, if an anthropologist or a child psychologist had been in the room and said, "Perhaps in your design phase, you want to design it as a playful artifact." That's actually going to keep children a lot safer than if you have this slightly anemic view of how it operates. So I think there are many examples of where folks in the design stages and training and imagining could benefit from the humanities. Whether it's the kind of historical consciousness or anthropology or questions about theology and philosophy, meaning and purpose. But I think also going back to what we said before, the question of kind of technological literacy and how folk in the humanities, not just it can help with humanities research, but actually I need to cultivate an imagination and an illiteracy about technology. So precisely this questions of governance, precisely are more distributed and inclusive decision-making. We have educated folk, who are able to make the interventions in these debates and informed way. So I think the traffic needs to go both ways. I think the other thing to say is, the humanities and as we're training often, and I get that at a university like Duke, the kind of drive to build up your CV, the drive to get work, and we're in classic kind of monastic terms, we're very much a vita activa. It's the active life, we get meaning and purpose through action, through achievement. And I think there is actually a need to recover the kind of (inaudible) the contemplative life, and actually take these moments, whether it's in a design process, whether it's in decisions about policy, whether it's in kind of judgements about, should this be an area of decision-making where AI can take over or kind of moments of heightened attention and contemplation. I don't think that's something very, the university per se does very well, and we're not very good in the divinity school either. We tried a bit of that, but I think that is going to become an increasing need across all disciplines of how do we cultivate habits of attention and contemplation, beyond the demand of the bottom line or beyond the demand of the next thing. Because that actually is crucial to this kind of wise judgment making, which I think is central, given the power of these tools and how quickly they can go wrong or make life a lot worse or their ability to make life better. So I think that's one that would be my plea and there are huge many religious practices, many philosophical practices, which we can draw on to think about that. But I think training folk to take time to kind of pause and contemplate in amongst the rush of activity is kind of counter-intuitive to a technocratic, technologically driven society, but I think will become increasingly important. - So gentlemen, the clock is ticking and we're just barely getting going, but I want to give, so Luke has had a chance to make his plea here. So Jeff, you have one minute to make your plea and Murali you will have one minute to make your plea. I'm just going to hijack what my students just told me in my last class, cause they're always smarter than I. So that is that we need to find ways to make wellbeing, the kind of values that are articulated by IEEE and other organizations around AI development, part of company culture, and part of company incentives. They determined that that's quite antithetical to a lot of the current shareholder, primacy and others, that determine decisions of these big companies that have an increasing role in our lives, even our speech. Our public square often determined by platform technologies. And we need to find ways to make human wellbeing a part of that. - Thank you, Jeff. Murali. - Just going to say two sentences. As a society, we need to invest in the humanities. We also need to invest in brain science because I think the answers are going to come from both. - Well, gentlemen, thank you so much. And my plea to all the people who have been joining us that you will continue to remember your home institution, Duke University, and join events that are being put on by the Duke Alumni Association. I want to thank everybody and the Duke Alumni Association and our partners around the school, the different schools who helped put on tonight's event. And I'm going to hand it over now to Susan James, from the Sanford School of Public Policy. Susan. - Thank you. I am Susan George James Duke class of 1990 and Assistant Director of Development and Alumni Relations at the Sanford School of Public Policy. As Judith said, thank you to the Duke Alumni Association, the Duke Divinity School, the Law School, School of Medicine and the Sanford School of Public Policy for bringing this evening's conversation to more than 500 of our undergraduate and graduate and professional school alumni. Thank you, Dean Kelley for being such a Def moderator and to Luke Bretherton, Murali Doraiswamy, and Jeff Ward for sharing your expertise with us this evening. As Mycal mentioned earlier, this panel represents a small slice of the diverse range of Duke experts, able to address this topic. Look for more Duke resources in a follow-up email tomorrow. And finally, thanks to all of you who took the time to send in your questions for our program and for joining us this evening, as we head into the holidays, we wish you and yours, a safe and healthy holiday season. Thank you. (music) 