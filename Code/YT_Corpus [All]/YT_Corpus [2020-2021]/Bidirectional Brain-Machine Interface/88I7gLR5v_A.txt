 welcome back everyone it's a great pleasure to uh introduce uh me zang she's uh um uh one of the top doctoral scholars uh in my group and she's going to do a very exciting presentation she has uh i don't want to spoil it but uh uh she's a person that has multiple faces so if you see uh one of these uh strange avatars here that's uh that's mainly and she will talk about uh something that's uh extremely cool and super exciting with a tutorial presentation for you to learn about uh deep genitive models so without further ado make me please yeah hello everyone welcome to the third tutorial uh yeah my name is mommy i'm a postal from gabriel's lab for those of you who have not sent me before uh just to clarify this is not actually how i look like since i'm gonna talk about generating models today i thought it might be fun to do a live demo of how we can use generated models in our daily life for example in the zoo meetings instead of mimicking fictional characters in the movie the generating models could also generalize to a real person so i'm gonna switch my faces uh just photo like this like steve jobs and also it could like extend to a personal pending and i can also move my head a little bit and make funny facial expressions yeah what is more interesting is that the generating models have been trained on celebrity faces and yet they have no problem generalizing um to for example animal faces like now i'm mimicking a monkey or a cartoon character like spongebob all right so for those of you who wonder what is the difficult technology behind the demo here is a pickup of the algorithm i also attached the link here and the paper for you to check them out here's the sauce image which is the hour how you want to be and then we can uh and this is the driving frame which is the real me and then we can calculate the optical flow which tracks the movement of individual pixels between individual uh between adjacent frames we can also calculate the occlusion map which denotes a part of the background that needs to be repented together with the features extracted from the source images these three components form a latent representation and then we could pass this to the generator and the generator could then synthesize new frames and broadcast to zoom i'm now presenting you an overview of deep learning frameworks in machine learning in the previous tutorial andrea and the rest talk about the basics of constructing a convolutional nets and uh establish connections between activations of the artificial neurons and the neurons in mouse brains in this tutorial i will move on to unsupervised learning in particular we will be focusing on um generative virtual networks and how we can leverage on this generating models to reconstruct images from brain signals here is the uh outline of the tutorial i will first talk talk about the basics of a virtual uh generator of restaurant networks in an advanced version of the uh afghan which is big big followed by a review of existing image reconstruction methods from brand signals before i go straight away to talk about gans i want to introduce you a very important concept in deep learning which is deconvolution in the past we have been talking a lot about convolution so the blue patch here is the input the sign patch is the output feature map we observe that the 2d convolution typically reduces the output feature map dimension in this particular example the future map and sizes decreases from five to five of five by five to three by three in contrast deconvolution does the opposite here again input is in blue and the output is sign the deconvolution operation up samples the input feature map from 2x2 to 4x4 in pi torch here is a function for deconvolution and here is an example usage below all right now let's talk about gans just a little bit of history gang was first invented by ian good fellow in 2014 since then gang has been a popular research topic so what is gain as this name indicates it's a plural term the gang consists of two and two networks one is the generator the other is a discriminator the generator takes a random noise vector and generates an image the role of the discriminator is to tell whether a given image is real or fake which is generated by the generator this two now works fine against each other in game theory it is called me max game in other words each of these two parties is trying to minimize their own losses given their opponent is perfect for example here if i'm a discriminator i'm assuming i'm facing a perfect generator which can generate realistic images to for me thus i'm trying to minimize the number of mistakes i'm going to make in misclassifying fake images as real images now let's take a closer look at the architecture of the discriminator same as other object recognition networks the git that you have probably seen so far it's just another one network consisting of stacks of convolution layers it takes an image as the input and outputs a probability vector indicating whether it is real or fake image a couple practice notes here so this is the code of python code loading the images to make the network usual basic variations you have to perform the image augmentation such as skating and rotation and since we have the value layers which uses zero as threshold we also want to normalize the image pixel values from zero to one to minus one to one in the end of the network to make sure the network outputs probabilities we add a sigma function which normalizes the values to zero to one by the way instead of polyconductor layers since this network consists of stacks of convolution layers we also named this scan as the convolution gain which is in short of the dc gain here's a diagram of the generator it does say exactly the opposite of the discriminator by replacing convolution layers with deconvolution layers the generator takes a uh a sorry so here what you see on the right is the snippet of how we constructed the discriminator and the arrows show here corresponding with the different layers of the convolution and take note that in the end we have to add in a sigmoid function all right so here's the diagram of the generator it does the exactly the opposite of the discriminator by replacing the convolution layer with deconvolution layers and the generator takes a vector of random random noise as inputs and that is for each element in this one dimension uh vector we randomly sample number from a normal distribution with me zero and variance one in pi torch this is how we implemented the generator outputs a tensor dimension of 3 by 64 by 64 however the generator itself has no notion about what this tensor represents thus we need a hyperbolic tangent function to normalize all the values in the tensor to be from minus one to one as a representation of the image this is a consistent with the real input image pixel values to the discriminator which also ranges from minus one to one take note that this normalization is essential since we do not want the discriminator to easily capture the difference offsets between the real pictures and the generated sensors here is the snippet of code constructing the generator the arrows to show the corresponding deconvolution layers in the end we added a tangent function now we have the constructor discriminator and generator let us see how we can trim this neural nets at the same time again to remind you that the generator and discriminator are playing against each other in a min max game thus we can divide the training into two parts so first we assume we have a perfect generator which generates bunch of images we can train a discriminator differentiating the fake images from the real ones just like training other object recognition network the discriminator takes either a batch of real images with the ground truth level is real it performs a binary classification of these real images and back property gradients like this similarly we could also label a batch of generated images as fake computes the laws again and then back proper ingredients the parts of the code shows the training of the discriminator with all the fake images labeled as fake this is the first part and then the second part shows the training of the discriminator with all the dual images labeled as real so far the training story has been very straightforward and let's now see how we can train the generators first without the real images we could simply concatenate the discriminator and the generator as one big network thus we can compute a binary classification loss and perform the gradient back propagation from the very end of the discriminator all the way back to the beginning of the generator since the objective is to trim the generator we want the generated images to fold the discriminator into classifying them into real images therefore the ground truths for ground truth level for the generative images are now going to be real from the point of view of the generator that is to see how much the generator needs to correct itself in order to make sure the discriminator outputs the uh the real labels for its generated images note here this is a important difference from training the discriminator where the ground truth label for the generator images have now changed from fake to real this is the code for training the generator and this is where the real uh labels have been assigned to the generated images since 2014 the study of gang has become so popular for the past five years there are many gangs out there and these are several examples there was one time that again becomes so fashionable that there was a joke in ai conferences that people say if you want to get your paper accepted in top ei conferences in 2017 you better put a word against somewhere in your paper in this tutorial among all this type of gangs i want to introduce big big in particular now we understand the basics of dc again however why do we need to study more against rather than dc games like why do we propose new games such as big big isn't dc again good enough here i summarize a couple of disadvantages about dc again so the first disadvantage is that the sizes of this generated images are typically very small it's even smaller than the sizes of imagenet images that we often input to the object recognition network second one generator is often responsible for generating only one object classes of the images in the lecture given by antonio teroba last week we knew that if we want to train and generate generating buildings it's very unlikely that these generators can give you dog images for example this is very unsatisfying typically we want a generator which can generate images across multiple object classes one of the reasons that we cannot do this is because we don't have constraints on the randomly sampled latent vector in the generator another disadvantage is that this images are typically of low resolution and its lack of high detail or high visual details not only that even training this again is very brutal for example sometimes the loss for the generator and the discriminator oscillates over amazon over numbers of e-pods as a testing stage it's common to see the generator collapse meaning they lost the ability of generating diverse number of samples and typically they will also they often generate like for example five five image examples from a class and then that's it since this is a battle between the generator and the discriminator it's easy that the discriminator always wins the game and thus the generator the gradients of the generator diminishes since they can obviously lose it obviously loses and never bursts no matter what so there are several empirical evidences suggesting that these networks are very sensitive to hyperparameter tunings during training that's why we uh here i want to introduce you an advanced version of again which is big big this is the word publishing nibs last year from demine as its name indicates it consists of two parts it is a big again and it is also bi-directional so let me first introduce the the part where it is big as its name literally suggests it is a big neural net it it it is trained with larger batch sizes and it has more network parameters the authors have also introduced several architectural modifications for example they introduce skip connections of the latent vector that is they bypass the latent vector to the next layer after the first deconvolution layer and so on cellphone tension model turns out to be useful in many applications the intuitive way to interpret this self-attention model is the following imagine your artist and you want to paint a dog unless you're sitting on the grass then this is what so the self-attention model does is to focus the to pay more attention to the dog regions instead of the grass so it this is exactly what this self-attention module does it helps you guide the network to focus on important regions to print such that we can generate more realistic images next let's talk about the bi-directional part which is the part i found which is very interesting so here's what we see in the dc again in order to uh control what is in the latent code authors introduce an encoder side by side the generator this is exactly the opposite of the generator which takes the real images and then causes latent representation of the head ideally if the latent vector carries the same essential information as the abstract information then the generated images should look close enough as the real images thus the discriminator has two extra jobs to do in addition to the objective of telling the generated images from real or fake it also has to distinguish the extracted representation of the real image from the data encoding the generator as shown here moreover it also has to distinguish whether the joint distribution combining the encoded representation and this image is real opaque which is it's going to take a pairs of the encoded the encoding as well as the the image with these two additional constraints um big gang can generate high quality images of larger image size up to five one two uh by five and two there are more tricks for constructing real estate images using gans for example researchers have found that updating discriminator more often during training is very helpful taking the average of the model parameters is also beneficial so here's what i meant for example in the first epoch you update your generator parameter and denote it as wg1 then in the second epoch you update the parameter for the generator again as wj2 and so on as the testing stage you compute the final parameter for the generator by taking the average of all the parameters over a number of big blocks here are more tricks we know that for each elements in the latent vector we randomly sample from normal distribution with mean 0 and the standard division one as the testing stage instead of sampling from a normal distribution again we are going to sample from the parts out of one sigma let's say this would help us input more extreme values into the latent vector and thus it contracts more realistic images but then there is a cavity so this would sacrifice the sample diversity since the space we can sample from become smaller during training the generator we typically want to regularize the weights to be orthogonal so here's an informal explanation but i would encourage you to check out the actual paper like they provide empirical and an analysis about this so imagine you have two weight vectors if they are orthogonal to each other then they will share less similarities compared with the two vectors shown on the right thus imposing this orthogonal regularization you actually push the network parameters to learn as many distinct features as possible all right we finished the machine learning part of the tutorial let us now move to the brain reading part in neuroscience with all the basics of deep generating models in mind we would could ask ourselves the following question if this random vector can generate images can we actually plug in any brain codes into the generator and reconstruct its uh corresponding images one could imagine that the brain code could be any type for example it can be measured with any techniques in neuroscience and recorded from any brains of animal species and in the future instead of restricting ourselves into vision we could also extend it to five senses for example sound touch and a smile and we could also extend this to high-level cognitive functions such as emotions memories and languages before we uh completely switch gear to review image reconstruction methods let me now stop here for a couple seconds and take one to two questions if any great we've got a question from anthony chen can you explain why having relu activation uh means we need to normalize pixel values to negative one comma one question mark uh what sorry can you repeat the first problem what is iou activation uh can you explain why having re uh so capital r little e capital l capital u r e l u activation means oh i see got it yeah so typically we have this uh linear rally activations in the network so that's where we zero out all the negative values in the layer so imagine now if you don't normalize your pixel distribution from minus one to one let's say we stick with zero to one then you your your value function basically so uh so basically you can treat value as a uh as a thresholding um mechanism that decides whether the value is gonna be above zero or uh below zero and if it is below then i'm gonna zero out all the values therefore you probably want to shift your image to pixel distribution um from minus one to one hopefully uh that this would provide a satisfying answer to your question uh the next one is uh anonymous is asking uh why is it called self-attention instead of just attention oh well that's uh because the self-attention module like the the network itself learns attention so typically when people try to uh say attention you sort of like provide human supervision or human judgments into the model like which parts of the image that you think are important but here the network actually just automatically computes the tension values and the the tension maps so yeah thanks and we've got one more from sumik faron uh can you explain uh more about the tricks for image reconstruction i mean what could be the strategy if you want to look at the output of some patterns well i don't quite understand what you mean by strategy of the patterns but i i would imagine so let's say if you want to interpret a certain target unit in those generating models you could maybe we could i i haven't done this before but i would imagine let's see you we could use similar ways as what deep dream has been doing which is you try to compute the loss of certain features of the target unit in certain layers then iteratively amplify take the gradients and iteratively amplify it with respect to the generated images all right so this is a relatively new and fast growing research area after literature review what i found was only papers on image reconstruction from brain signals unfortunately i didn't find any literature about reconstruction of other modalities for example music in this later half of the tutorial i will briefly go through some of the representative works on image reconstruction methods i divided the rig construction method into two groups one is based on gradient back propagation which was mainly inspired by deep dream and texture synthesis first i should clarify that this greedy and backprop methods are not generating models since this tutorial i'm gonna focus on generating models i will only briefly introduce the key ideas behind this algorithm but i strongly recommend you to check out these papers by itself so the first two pieces of work shown here are inspired by the idea similar to similar as deep dream this algorithm is used for visualizing the patterns learned by a particular unit or layer from a neural net it over interprets and amplifies the patterns it stays in your image deep dream does so by first forwarding an image through the network then calculate the gradients of the image with respect to the activations of a particular layer then the image is modified to increase those activations enhancing the patterns is sent by the network resulting in dream like images another piece of work was mentioning is the study of neural responses in we one where they use this uh texture like pictures what these ulcers did is first they take a natural image pass it to a pre-trained neural net it takes another white noise image and pass through the neural net again then the texture synthesizer computes the squared mean mean squared area of the ground matrix as a target layer for example constraint in this case and it back propagates the loss with respect to the white noise image the generated image has this texture looking like so to put this in layman's term the goal of the texture uh synthetic synthesizer is to preserve the structure content of the target layer in neural nets what is interesting is that these ulcers found that this synthesized texture images share similar view and neural responses and what has been observing the natural images okay now so now let's uh move on to the generating model based method one of the key problems we need to solve is how we can translate the brain signals into those latent random codes which uh the the generator understands one of the straightforward solution is we don't need to perform translation between the brain signals to these latent codes instead we could directly take these brain codes as inputs to the generator and fine-tune the parameters of the generator to make it adapt to our new brain codes so here is a representative work where the authors ask participants to see a bunch of images and obtain their brain codes here each latent vector represents this piece of code corresponding to an image then the generator takes the brain codes directly as inputs and reconstructs the images just as a standard gantt training officers impose discriminator losses to tell whether the images are the real images or reconstructed lines from the brain signals in addition the ulcers introduce another two losses to impose constraints at both the pixel level and feature levels that is the reconstructed images should look as similar as possible as the real image as a testing stage the generator is fixed and then we could take directly the braincodes and plug it in to construct the image on the right the two bottom rules shows the reconstructed images from the two subjects using fmri signals again i want to emphasize that one could easily imagine that we can replace the fmri signal with other type of wrinkles and adopt similar approaches for image reconstruction of course the generator already contains many parameters and this method would require intensive amount of data in order to fine tune the generator so next we will explore alternative approach which is to fix the generator parameters and instead try to look for the best latent codes that would drive the neurons to fire as much as possible here's how the algorithm works so let's start from a bunch of noise vectors js says what we did before the generator uses this old latent codes to generate a bunch of images and then we present this generated images to monkeys and record their neural responses in the form of spike trends just a quick recap from ethan's tutorial last week on processing your data we could just treat this uh spec trans as a binary vector and compute its average fire rate for example we could take the mean of the this spectrum and we get a number for example this is 30 hertz for the first image and 20 hertz for the second image since our goal is to drive the neuron to fire as much as possible we use those computing mean firing rates as the finished score for each image code in this paper authors will propose to use the genetic algorithm to select the best parents then crossover mutate them to get the next generation of codes after the optimization we have a new population of code and then the generator could generate a new batch of images and this process iterates so here uh so what i'm showing here is a single synthetic image involving over generations this method is a closed loop system because it involves the monkey brands in the loop and then it requires many iterations in order to find the best stimuli next i will introduce a simpler way of establishing the mapping function between the latent code and the brain signal via linear regression again the generator are fixed in this case just a quick recap so in big gain we have an encoder which encodes the abstract representation of the real images and after training this latent representation carries similar features as a latent code z thus we could make use of this relationship to help us find the best mapping function so let's see we have total n training images for each image we could pass them through the pre-trained encoder from the big and big big end let's concatenate this feature vector from all the images as a big matrix called generator matrix it is of dimension 120 by n next we could similarly represent these images to animals and extract those brain signals concatenate them and name it as brain matrix it is of dimension and v by n since we have the generator matrix and the brand matrix we could easily perform a linear regression between two of them specifically this is 12 establishing linear mapping by computing the weight matrix which is of the dimension and v by 20 and then um we also have the generator matrix which is 120 by n so we could calculate the weight matrix uh here are some practice uh practice uh issues that you have to take notes for example w might not be invertible so we could take the pseudo-inverse and the brain matrix size might be very big and we can perform dimension reduction using pca to pre-process the data since the scale of their brand signal might be different from the latent code we also want to perform normalization of the brain signal first all right so as does the testing stage we could simply make use of the weight matrix we just computed and transformed this brain signal to latent code and then pass this little code back to the generator in big by again for image reconstruction on the right uh so yeah so on the right what you see here are the reconstructed images using fmri signal again the linear regression method could be generalized to other forms of brain signals as well then after reviewing all these image reconstruction methods from brain signals you might have your own personal judgments about visual brain reader is the best in order not to be subjective the next question we want to ask is can we come up with a quantitative evaluation matrix to evaluate all these brain readers and the answer is clearly yes so similar as brain scars proposing jim d carlos lab evaluating the relationship between state-of-the-art object recognition network and the neural responses in the brain here we propose to adding additional metrics to evaluate to the deep generating models and report the relationship to the brain signals if uh yeah i think we still have plenty of time so let me just introduce you each of this metrics individually so first inception score it measures both the image quality and diversity here is how the score is computed first it reflects the image quality so let's see if we have a generated image and then we pass it to object recognition that works here via using inception 3 but it could be any other recognition network it outputs the vector indicating the classification probability for each object classes if the reconstructed image is of very good quality that is a network has higher confidence recognizing what the object is then the level distribution would be unit model resulting in a lower entropy score for those who don't understand what entropy is it basically reflects the uncertainty of a distribution conversely if the network barely recognized what's the reconstructed images then we will end up with a uniform label distribution which results in a higher entropy score in this case the lower entropy score the better the image quality is moreover we want to evaluate whether the brain reader has enough image diversity for example if the network of is generated dog images no matter what split latent codes you give to the network then i would say this network is very bad because it fails to capture the diversity of object classes thus if we want to if we can take the sum of the classification probability for all the reconstructed images we would expect to end up with a more focused distribution since the generator always generated dark images in contrast if the generator could evaluate multiple object classes like showing here we have elephants cats dogs etc then the sum of all these classification vectors would give us a more uniform distribution therefore a higher entropy which is good overall we want a more focused level distribution for better image quality but a more uniform distribution for diversity in order to combine the of both aspects together inception score finally computes the ki divergence between these two distributions the higher the kl divergence score the battery reflects the model's ability to generate good images and obvious diversify the class object classes note that this inception score discards the information about the real images because you only calculate the image quality and image diversity based on the generated images it has nothing to do with the real images therefore uh we are proposing a new metric which is the fractured inception distance shuffle fid for both the reconstruct and the real images we could use the instruction net to extract their feature vectors thus for each real and generated image pairs we have a pair of feature vectors the fid calculates the distance between these two distributions a better brain reader gives a lower distance score so here's a mathematical formulation of fid which basically calculates the distance between the mean vector and the trace of their covariance differences so here is the third metric for evaluating brain readers again this is a big buy again architecture i want you to focus on the encoder part again so for each generated image the encoder could take them as inputs and outputs their feature vectors if this encoder representation reflects the information about subject categorization then these features from the generated images should look good enough for classification of image nets we therefore could train a simple classifier predict their cost labels on imagenet and report their top-line accuracy if the brain reader constructs meaningful natural looking images which corresponds with the real images then the top one classification accuracy would be very high in the big by gun paper i'm quite surprised that the also reported 61 top one accuracy on this and using the purely unsupervised learning all right so we could also conduct human behavior experiments to evaluate to these brain radars here is the experiment paradigm let's see we have the input image presented to the humans and then as targets we could then randomly choose one reconstructed image based on the brain signals from that target image as a negative sample we could randomly pick another reconstructed image from a non-target image from this this three images we proceed to conduct the behavioral experiments using mechanical amazon mechanical turk this is how it looks like in an example trial so in the experiments the subject is instructed to choose either option a or b that is small visually similar with a targeted image after we collected human choices on many trials we could simply report the accuracy of human judgment now i'm showing you now i want you to pay attention to the bar on the right highlighted in red here the dashed line is the chance level which is 50 in the paper also reported there are about 90 percent of people choosing correctly that is they prefer the options which are reconstructed images based on the brain signals generated after the subject this is the target image instead of the non-target image all right at last we could also assess the controllability of this risk constructed images at the neural level so here's an illustration of what happens we first record the average neural fire rate of monkeys after they see the real pictures then we put test monkeys on their corresponding reconstructed images and record their frame rates next we show a skater scatter plot where the x and y axis indicates the frame rate of the real and reconstructed images we put a dot for each pair of the average neural virus between the real and the reconstructed images and here are more dots for the second pair of images and third pair and so on now we can fit a line and report their linear correlation so the higher the linear correlation implies reconstructed images capture more essential features all this brand score assessments what uh we discussed so far could also be applicable with other types of brain signals all right so just to conclude nowadays that have been fascinating progresses about deciding brand encodes using machines and interpreting the machine codes using brain responses hopefully not far from the future we could find a perfect bijection which could translate between these two types of codes with this bijection it's possible that one day we could stimulate and control any part of the brain last but not least i want to conclude the tutorial visa illustrative figure showing you a hypothetical diagram of the brain machine interface for the visually impaired with the joint efforts of the neuroscience and ai researchers one could imagine that one day we could help the visually impaired by insert a camera in front of the eyes and the the neural networking or pocket could be embedded in pocket processor it extracts the machine codes and translated to brand understandable language and this could be task dependent and it could be tax reading or face identification of obstacle violence in the end it could translate the brain understandable codes while a uh wireless transmitter on the scalp and uh stimulus use the electrodes embedded electrode arrays and body in the brain to stimulate the brain a general question how do we know brain signals contribute to the generated images the generator also can generate similar images by inputting a random latent code space right yeah that's a very good question but if let's see is i mean i'm not sure about the dc gang case because it always generates generates uh images from one object classes but if let's see for the big bi again case um so if the generator ignores whatever in the latent code it's probably gonna generate images from different type of giant classes compared with the image that is actually presented to the animals right so for example if i'm presenting the monkeys with the dog image and if i'm just randomly plugging another random noise vector to the generator and the gener the generator is probably going to generate a cutting image instead of that and that's why in the evaluation metrics we introduce the top one classification score on imagenet and hopefully that could clarify your question thanks uh next one's from jad uh is fmri more adapted than a eeg to this problem can we expect more precise reconstructions with eeg is there a way to combine input from both emi or from both eeg and fmri uh that's an interesting question i'm not so just to clarify i'm not from neuroscience background i couldn't say too much about it but i would imagine uh yes and why not i mean for me any type of brain signals is just like a series of numbers and then if we could interpret them correctly and analyze them uh normalize them correctly then i don't see the reasons why this wrinkles cannot be represented in one way on another why does increasing the activation create dream like images so the the name is deep dream i think it's probably just saying you know this weird looking dog is looks like a dream that you just had right so but then from that's the algorithmic level uh what it basically does is just to amplify the the activations of the target units in the target layer so i don't i mean this is the subjective card it actually depends on how you interpret them right 