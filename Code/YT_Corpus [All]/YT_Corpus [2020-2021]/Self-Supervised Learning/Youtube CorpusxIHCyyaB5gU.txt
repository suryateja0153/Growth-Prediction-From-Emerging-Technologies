 In this work we define and solve scene de-occlusion problem in a self-supervised manner What we have is merely a typical instance segmentation dataset including RGB images, modal masks and category labels. The modal masks are instance segmentation masks that represent visible parts of objects. We have no annotation of occlusion ordering or amodal masks. Scene de-occlusion decomposes a real world scene into intact objects with invisible parts and the background The objects are organized by an ordering graph the edges of the graph indicates the occlusion directions The scene de-occlusion problem can be broken down into three subtasks. First, we recover the ordering between occluding object pairs Next, with recovered ordering we obtain the occluders of an object. It facilitates us to perform amodal completion that is predicting the intact mask of an object given its occluders. Finally, the intact amodal mask contains invisible regions of an object We further perform content completion to fill in the invisible regions with RGB contents. To quickly catch up with the basic idea, let's start from amodal completion. Amodal completion aims at predicting the intact mask of an object given its occluders Let's have a look at the blue objects and their gray occluders Could you tell the original shape of the blue objects? It is not a difficult task for humans. For machine, given ground truth amodal masks as supervision We can train a network to perform full completion of objects, that is, completing object masks at once. However, what if we don't have the ground truth? Here we introduce partial completion. We show that full completion is equivalent to a sequence of partial completions We denote the modal mask as M0. At first we consider one of the occluders, the gray circle, and perform partial completion to complete the occluded part by this occluded to obtain M1. Next, we consider the other occluder, the gray rectangle, and perform partial completion again to obtain M2. By considering the occluders one-by-one, we break down full completion into a sequence of partial completions. However we still have no ground truth to train the partial completion process. M1 and M2 are still unavailable What we have are merely M0 and its occluders. Next, we present our key idea We further trim down M0 with a surrogate object to obtain M-1. The surrogate object is a random modal mask chosen from the dataset. Then we perform partial completion on M-1 to recover M0. This process is trainable The trimming-down-and-recovering strategy is the foundation of our  self-supervised solution of scene de-occlusion. Let's summarize what we've done so far. we propose partial completion mechanism to complete part of an object occluded by a given occluder without using amodal annotations. Next, we naturally raise the question, who are the occluders of an object. As shown in this picture, without ordering recovery we cannot tell, among 4 grey objects who are the occluders of the blue object. To solve ordering recovery, we need to introduce the regularization for partial completion. This figure shows the original partial completion process. In such case, partial completion always encourages increment of pixels. Next, we introduce the other case. In this case, we put the surrogate object below the blue object and we encourage the partial completion process to retain the original mask of the blue object. The partial completion process is trained with case 1 and case 2 randomly In this way the behavior of partial completion goes as follows if the target object looks like to be occluded by the surrogate object as in case 1, then complete it If not, we keep it unmodified. Now we obtained a regularized partial completion process. It has such property, that if the target object is partially completed by partial completion we know it is occluded by the surrogate object Otherwise it occludes the surrogate object. Partial completion is implemented by Partial Completion Network-Mask, abbreviated as PCNet-M. Given an instance A, we randomly choose another instance B from the dataset. Note that we only have modal masks for both A and B. For case 1, we put B above A and train the PCNet-M to recover A. For case 2, we put A above B and train the PCNet-M to retain A. The PCNet-M is a U-Net The inputs randomly switches between case 1 and case 2. Hence, PCNet-M has to discover cues from the inputs to distinguish between the two cases, and determine whether it should partially complete A or retain A. Besides, PCNet-M is also required to recover the occluded shape correctly, if it determines the situation as case 1. Next, given a trained PCNet-M we show how to recover the ordering between two neighboring objects. For example, in this image, we denote the doughnut as A1, and the coffee cup as A2. At first, as shown in figure (a) we regard A1 as the target object and A2 as the surrogate occluder. We observe that PCNet-M keeps the mask of A1 unchanged. Conversely, as shown in Figure (b) we make A2 be the target object, and A1 the occluder. Then PCNet-M increases the area of A2. Since the incremental area of A2 is larger than that of A1, it is concluded that A1 is above A2. This simple strategy achieves surprisingly high accuracy. In this way, we could recover the pairwise ordering between all neighboring objects, resulting in a directed graph. With the predicted ordering our next step is amodal completion that is predicting the amodal mask of each object given its occluders. To this end, we propose ordering-grounded amodal completion. Now we have recovered the ordering among objects to construct a directed graph. Taking object 3, the donut as an example, we search all its ancestors nodes in the graph as it's true occluders, which are object 2 and 4. Recall that partial completion considers the occluders one-by-one, and complete the target object step-by-step. However, we find that the trained PCNet-M is generalizable to use the union of all occluders to complete the target object at once. Hence, we take the union of modal masks of object 2 and 4 as the input occluder mask. Given these inputs, the PCNet-M predicts the amodal mask of object 3. The process is iterated for all objects whose indegree is larger than 0. Since the completion of each object is independent of each other the process does not need to follow a certain order. In amodal completion, we mentioned that we need to use all ancestors in the graph as the occluders. For example, in this figure, for object 1 all ancestors in the graph include object 2, 3, and 4. But why not just use the first-order ancestors since higher-order ancestors like object 3 do not directly occlude object 1. Here is the answer If we only use the first order ancestors that is object 2 and 4 the completion is limited in the union of modal masks of object 2 and 4, that is, the gray region. However, higher-order ancestors might also occupy the invisible region of the target object. In this case, part of object 1 is indirectly occluded by object 3. Using only first-order ancestors results in incomplete amodal completion. When we use all ancestors, object 2, 3, and 4 we can obtain correct amodal completion. With amodal completion, we delimit the invisible region of each object. There remains one last task that is content completion. Content completion fills in the invisible regions of an object with RGB contents. It comes with a very natural question. Is it the same as image inpainting? Let's first have a look at how we train the content completion network PCNet-C. Similar to PCNet-M, it partially completes an object given its occluders, while this time on RGB contents. The surrogate includer B is randomly chosen in the same way as in PCNet-M. The inputs include the modal mask of A erased by B and the image patch erased by the intersection of A and B. The intersection of A and B indicates the missing region to fill in, and A\B indicates which object the missing region belongs to. The PCNet-C is trained to recover the original image. In inference, now that we have the amodal mask of an object, we take the intersection of the amodal mask and the occluders to obtain the missing regions. We use the missing regions to erase the image as the input, and also feed in the modal mask to indicate the object that the missing region belongs to. The PCNet-C fills in the missing regions with RGB contents, and the output is multiplied with the amodal mask to remove the background. In this way, we obtain the decomposed intact object. The difference between PCNet-C and image inpainting lies here. Image inpainting does not require the modal mask of the target object as input. It does not care about which object the missing region belongs to. If we directly apply image inpainting the missing regions will be filled in with the content of other objects. For example, part of the coffee cup in this case. This result is reasonable for image inpainting but not for content completion of an object. While with the modal mask as an auxiliary input content completion is able to generate correct contents. Up to now, the scene de-occlusion problem is well solved even without manual annotations of ordering or amodal masks. In summary, we proposed a partial completion framework that is trained in a self-supervise manner. With the training PCNets we propose a progressive inference scheme for ordering recovery, amodal completion, and content completion. We evaluate our method for ordering recovery and amodal completion on two datasets. We achieve very close results to supervised methods. Our method can be used to convert modal datasets like COCO and KITTI into amodal datasets. Specifically, we train the PCNet-M on the training set using modal annotations, and again apply it on the training set to perform amodal completion to infer pseudo amodal masks. Such self-supervised conversion is intrinsically different from training a supervised model on a small labeled amodal dataset and testing it on a larger modal dataset, where the generalizability between different datasets can be an issue. To evaluate the quality of the psuedo amodal annotations and compare them with manual annotations, we perform comparison experiments on amodal instant segmentation task. We train Mask R-CNNs to predict amodal instance masks from images. The experiments use the same framework and hyper-parameters for training and testing. The only difference is the annotations during training. With the pseudo annotations yielded by our method we achieved the same performance of 29.3% mAP as that using the manual annotations. It is an encouraging observation that maybe in the future, we do not need to annotate amodal masks anymore. Here we show some qualitative amodal completion results. Please pay specific attention to these yellow objects. For these objects, the results of our method are potentially more natural than the ground truth. Our scene de-occlusion framework allows us to decompose a scene into background and isolated completed objects along with an occlusion ordering graph. Therefore manipulating scenes by controlling ordering and positions is made possible. We show four kinds of manipulations deleting, swapping, shifting and repositioning objects. The baseline method, modal-based manipulation is based on image inpainting, where modal masks are provided ordering and amodal masks are unknown. Our method produces natural manipulations, while results of modal-based manipulations contain lots of defects. Here is another example. Here is an interactive demo for image manipulation. When you open an image and perform scene de-occlusion, you can freely re-organize items in the scene. You may move the toilet roll, change its order, and put it somewhere else. You may pull out a carpet under the toilet with its shape recovered accurately. You may also move away the vase to see the occluded boundary of the cabinet, and repositioning furnitures Moreover you are allowed to insert objects extracted from other images, and put it behind existing objects With self-supervised scene de-occlusion we can do a lot more things than before. For example, we can perform data augmentation for instance segmentation via randomly re-organizing objects in images It may be better than InstaBoost that cannot handle the occlusion problem. For the other example the predicted ordering might facilitate the mask fusion process in panoptic segmentation. Scene de-occlusion also potentially benefits augmented reality. For example, in the figure below, scene de-occlusion enables us to insert a 3D character behind some real objects in the scene. all of the things are done without extra annotations. Here are some discussions. First, can it solve mutual occlusion? No for mutual occlusions, the ordering graph in our method cannot be defined. As shown in these images fine-grained boundary-level de-occlusion is required. It leaves an open question to the scene the occlusion problem. Second, can it solve cyclic occlusions? The answer is yes. Our order recovery method recovers pairwise ordering rather than sequential ordering. Therefore cyclic occlusion is not a big issue. Thank you for watching 