 is been an influential figure really in in uh in computer vision in general i think every most of us definitely myself and my generation when when we grew up and went to cvpr wanted to be like aliyosha so really it's a it's an honor to have him here he was advised by jitendra malik at berkeley now also faculty at berkeley after being at cmu and he'll be speaking about why it pays to read perception literature confessions of a computer scientist so alyasha the stage is yours take it away all right thank you very much thank you for for uh inviting me to speak here so i'm well i'm the last dog and so i'm gonna keep it you know light and also i am you know i am a computer scientist i am computer vision scientist i i i don't actually do any uh perceptual experiments but i'm a huge fan and indeed i was thinking i would do something a little bit different and just kind of look at all of the ways that reading you know perception cocci psychology literature has shaped my career and kind of guided me in in in doing my work okay so basically yeah this is going to be kind of a little bit of a rambling love letter to to you guys okay um so i started with texture this was the first thing that you know jitendra malik told me i should look at the texture and and it was immediately i got excited because it is this magical problem that's not just a computer program problem right it's a it's a perceptual problem so you have a bunch of of of of of of samples that don't look like each other it at all on the on the pixel level you know it computes like l2 distance they're all very different and yet a human looks at it and they all look like they are drawn from the same distribution okay and so even the definition of texture really is a is a perceptual definition and and things like texture analysis is basically really it's all about you have some true infinite texture out there in the in the in the you know in the sky and all you have is you have a couple of samples or some samples and then your question to uh you want to ask is is it are they drawn from the same texture or different textures and so really my first influence in in in psychology has been of course bella ulis who is really the father of texture and and i mean he also did you know the the random dot stereogram thing but you know i don't have stereo so i don't care about that but texture really he was really um very influential for me and so you know for those of you maybe who don't remember he was really the one who focused on this idea of of of pre-attentive discrimination so he he noticed that there are some patterns that just pop out at you and are easily discriminatable from the background and other patterns that are very hard to discriminate even though in terms of you know in terms of you know ink differences they are just as as as as you know as different uh and and so he basically argued that these pre the things that you cannot distinguish pre-attentively are what constitutes the same texture okay and and and so my first uh my first work was on texture synthesis which is kind of a related uh question of if you have one sample from this you know infinite texture in the sky you know how can you generate another sample that a human would think is drawn from the same texture and you know we did you know a a paper on on this nonparametric texture growing and and you know we got some good cute results um and you know one thing that kind of i i found interesting is that by mistake i ran it on a on an image that turned out not to be a texture it was like a real image and this was like an extrapolation result and i was kind of surprised that that worked you know it still kind of worked okay and then um and then you know reading further i i happened upon or maybe i heard it actually at one of the workshops uh simon thorpe came and gave this wonderful talk on on recognition uh uh on on on visual classification and i think his the task was is there an animal in this image and he was doing it at something like 150 milliseconds so really like pretty attentive exactly like uh bella ulisha's regime of pre-attentive like feed forward not much time to do anything really right and what what what simon's lab has shown is that people were really good at this like surprisingly good at doing what you would think is like a very fundamentally deep and you know uh uh semantic task and they were just doing it like nothing okay and and of course nancy cumbershire also did uh cool uh follow-up work in that uh and it could have also reaffirmed these results and so uh so there was already this feel feeling that that that it there was something about there was something about texture processing that was that was kind of uh uh more you know more that means i and and in my in my lab back when i was a grad student uh uh laura uh laura uh walker lenninger and jadandra malik were looking at scene classification and they were basically their idea was to do some really stupid texture discriminator and so what they did is they just did you know basically a dictionary of patches and then they looked at the histogram of these patches text ons and then they basically just the nearest neighbor in the histogram of patches this discrimination so really very basic very low level texture kind of question and they thought how far can they go how well can they do and what they found on this kind of scene discrimination task is that they were oh i don't have them well these in the x-axis there are different scene types like bedroom forest beach et cetera and what they found is that compared to human uh subjects they were basically doing pretty well on on the you know just on the texture they were pretty much on the level of like humans at 50 milliseconds okay and and and so this really got me thinking and then later on there was a another work uh also in just doing texture this is notice this this is 2001. okay and i'll just play this this this work from uh from the microsoft lab and they said this is john nguyen and uh and colleagues and here they basically just do they they they compute texture within this boundary and then they just do kind of nearest neighbor in in terms of the text on the representation okay so the stupidest simplest thing you can think of right it's not it doesn't know anything about the boundary of the cow he doesn't even know about the the whole idea of a cow it's basically doing like cowness or grassness right it's it's totally doing texture recognition but you show it to an undergrad and we'll be like wow you know it's you know the computer is recognizing objects right uh and and this was this really made me realize how amazing i mean look at this like this is you know it's yeah it just it just works and look you don't even need the full but like it has no idea that this is like many bikes it's not why buy it right but it doesn't matter because it looks good and and so this really opened my eyes and i realized you know a lot of what we think of as object recognition is it's really just that it's really just texture recognition and so when later on uh you know uh there were all these examples from gaddis of neural networks you know you you scramble your image into some texture mumbo jumbo and it just works perfectly fine you know i wasn't surprised to me this was obvious because of all this all of this texture perception work that i have read about you know 20 years earlier and and and so this is why i was really focused on thinking about going away from just texture recognition and really thinking about um about scene understanding right not just recognition but really parsing the scene into into into some meaningful uh uh parts um and it it is really all about you know learning complexity because if you look at the kind of a natural scene it it it's like you cannot do it without dealing with occlusions you cannot do it without reasoning about 3d you cannot do do it without really trying to understand what is it that makes a c and luckily i found that psychologists again were thinking about this so for example hawken colleagues were talking about different types of ill-formed scenes so you know type one is a well-formed scene and then there is a different a bunch of different ways it can go berserk right and then irv bieterman had this wonderful page so you know biederman of course most people think of biederman they think of geos and you know i think psychologists are really into geo and it's like for from computer vision perspective you know i mean they're nice in theory but like it's just impossible to actually do them in practice but but the his the biederman's work on on on you know on on understanding well-formed scenes that's that's a beautiful paper if you haven't read it just beautiful beautiful paper and so bienerman basically had this whole uh theory about what is it that makes a well-formed scene what is it that that that that you need to have for uh for a scene to to hang together to to do to be a coherent thing and so here he hypothesized five parts support size interposition and uh and likelihood of appearance and luckily i was reading this paper and i was also in in in in in brussels and i i went to uh to a magritte museum and i realized that magritte apparently read biederman because he had all of these things in his painting so here is a violation of support okay here is a violation of size here is a violation of interposition and here just everything goes bad you know position probability size everything okay and and so then then i i you know i i realize that really we need to deal with all of these things and if we can get there then we can maybe try to understand a scene and and this is what i convinced my very first phd student derek hoyme to take on and and he was super brave because what you know i i thought that was like a you know 20 20 year worth of effort and and he didn't know better so he took it on and and uh he basically looked at trying to understand the 3d spatial layout uh from a single image okay and kind of try to uh figure out what is it that that that the the um basically think about the three-dimensional structure of the scene like the support surface the vertical surfaces their orientations uh and also connect it with occlusions uh uh figure ground relationships and also object sizes and and interposition all try to do all of this together okay and of course the main challenge is that from a single image this is this is you know this is ill-defined right so given this image you know it could be this you know it could be this it could be this it could be an infinite number of interpretations and the trick is of course to get the right one to find the right interpretation or the likely the most likely interpretation right so you know how is it that we can figure out that this is not likely and this is more likely uh and so you know i'm not gonna just i'm just gonna briefly summarize his his his work and he basically talked about basically labeling these geometric classes uh of of different types of services and and then we had a way to pop it up like a basically paper pop-up and and you know given this geometric labels kind of find where to fold and where to cut and make it into like a little very coarse plain or 3d model and then a be able to kind of basically walk into an image from a just a single image okay and it's you know it's it's a very coarse model but it it captures the the precept of 3d in a way that you know a lot of the more complex more sophisticated models do not so here are some more examples of that this is really old stuff this is like yeah 2005 but you know still cool and it's it's an interesting it's an interesting kind of historical note that basically at the same time uh andrew engen students were also interested in kind of understanding uh uh 3d from a single image but they went a completely you know quantitative route they were basically predicting depth at every pixel and i would say that the kind of the the perception and foreign view was uh i think was a more right way to do to go and in fact if you look at uh at at uh ashodor saxena who was the the first author of this this other competing work in a couple of years he actually came around to our view and he basically added the the qualitative 3d understanding to his system uh so so it was kind of a you know we got ahead because we you know we read more perception stuff um now you know this is not always going to work uh so here is an example of things not working and it's not working because the previous model did not really deal with with with occlusion boundaries okay and so next derrick thought okay we needed to figure out you know depth ordering and occlusions again in a qualitative way and of course then you have to go to all of the all of the literature and figure ground and of course i made derek read all of that literature and basically uh boundary ownership and and all that stuff and it's interesting again to kind of connect this to the old classic work and computer vision uh something like the volts algorithm where the idea was that you're going to label your boundaries you're going to find your boundaries you're going to label them and then there is this beautiful gorgeous junction propagation algorithm for those of you who are in computer vision i'm sure maybe you have or in older generation you probably had to do it in your class so there is a different type of junctions and then you propagate labels for the junctions and you get this beautiful result okay and it just it's a gorgeous beautiful story okay the only problem is it just never worked it worked on these you know hand-drawn things it never worked in the real world and everybody was kind of puzzled why doesn't it work in the real world so we you know people try to like find all these t junctions and then blah blah blah and i just happened to stumble upon a little wonderful paper by george mcdermott which basically kind of made me realize we are just on completely the wrong track so basically he was looking at t junctions and real images and looking at uh how people perceive t-junctions and he showed that locally from a little patch humans could not tell t-junctions or any other kind of junctions really it was really only when they got the whole context they were able to to realize that those were you know the the the correct type of junction so what we were doing in computer vision was completely backwards we were starting with trying to find junctions and then trying to understand an image from there whereas it looked like the humans were doing it the other way around they were first getting some notion about uh regions and and and and and boundaries and then kind of the junctions were the output not the input and so that made us just go this direction and so basically what we started with is we started with uh getting finding slowly find going from from uh from edges to boundaries and then trying to figure out which were that were the occlusion boundaries in conjunction with also reasoning about qualitative depth and these kind of surface representations okay and we were able to slowly slowly kind of converge upon a representation that would give us both uh boundaries with with with uh uh with the labeling of you know foreground background and also with uh with a range of depths okay and the final work of derek's thesis was basically just put everything together with surfaces occlusion boundaries and objects and their scales and and and and basically put it all together uh uh and it was actually very much inspired by kind of intrinsic image work from uh uh uh barrow and tenenbaum um so er that was all fine but the problem was that you know surfaces were just not good enough to really represent the scene because they were just like you know all these paper paper cutouts you know there was no meat in on on this thing right and so we realized that we really need to go from from surfaces to volumes okay and again this is where kind of a we were very much inspired by by biederman and and colleagues and we tried to basically figure out how to inject both geometrical geometric volumetric constraints and also physical constraints like stability all in the same framework and this was a a work of uh abhinav gupta who back then was a was just a starting uh uh postdoc in in my lab and so we had a we had a kind of a very cute algorithm that kind of starts with little blocks and then basically makes these like like lego blogs makes them to represent a given image again from a single real image and and so we were able to do these kind of 3d pars graphs uh uh with with with the relationships like in front of above of uh heavy light um and and again this is like you know 10 years ago i'm kind of surprised how well we were able to do and we were able even to even to get a hold of some sort of like 3d uh renderings from real images i think i have actually not seen any other paper that's able to do something like this so so this is still uh and a hard problem and i think we'll make we're able to do as much progress as we would have because we were we were we were aware of all of this uh uh work from from earth bitumen and colleagues um at that at around that time i had my the next inspiration uh and this one came from odd oliver okay and and this is her wonderful work on on uh on the capacity of of a visual memory i know that odd gave a talk here but unfortunately she doesn't talk about this stuff anymore and the young kids they don't know it and so i'm just going to i'm going to i'm going to make sure that public service here to make sure that everybody knows because this is beautiful beautiful work so of course you know in 1973 standing already showed that humans have this amazing uh capacity for remembering images something like 83 recognition of a hundred ten ten thousand images okay but standing was no he was you know he did like you know forest versus beach right so it wasn't clear how much information was being uh stored in the in the memory it you know is it just one label or is it like every pixel right and so what odd did is that she basically redid this experiment but really tried to push the humans harder by showing a lot of very similar objects and also looking at the same object and different states and also having uh uh similar uh classes of objects but you know not the same instance and and the the results were spectacular so she replicated standing result okay for kind of the same uh the same object class uh but she also showed that even if it's different if even people were able to distinguish one member of of a class like you know they were able to tell apart one remote control from a different remote control or they were able to tell apart different states of the same image so it was just incredible how how much how much we remember visually okay and so this really was inspirational to to my data-driven kick where james hayes and i you know thought okay let's just let let's try to do something with lots and lots of data we thought okay let's how about whole filling so here is a here is some thing we don't want we are going to get rid of it in photoshop and now we are going to uh you know download two million images from flickr and this is 2007. so you know this is this is a long time ago and and we're just gonna you know try to see if we can use some other image to fill in the hole and ta-da you know it actually works uh and and the cool thing is it really it works because because of all this data okay so because first james tried it on 20 000 images which back in 2007 was like huge amounts of data and it just totally didn't work here are the nearest neighbors and you know they're not that near but then he just kept downloading and you know when he got to two million boom it just just worked and so here are some other examples uh yeah and we also did it for geolocalization with the same idea okay uh but but do humans really remember every single pixel i asked aud when we were in paris we happened to be together in paris on our sabbaticals in 2012 and we had wonderful time drinking wine and talking about philosophy and um and and odd was like huh funny you should mention i actually have some experiment on this but i didn't think anybody cared so i never i never published it so this is such a cool experiment at least to me i'm i'm just going to play it god gave me the slides so i'm just going we're actually gonna do it okay i think i still have a little bit of time uh so this and you know it's hard to do in on zoom but so the idea is audi's gonna show you lots of images or a bunch of images when you see the same image twice you're gonna clap okay it really works when you can hear other people clapping but you know okay here we go okay so of course you notice that some of these images are basically just kind of random textures that don't mean anything and i'm you know when when you do it in an audience it's very clear basically people do really well on normal images and they are basically a chance on these random meaningless textures it's a it's a very very clean result very powerful result and so this made me think wait a minute it's not just that you're remembering every single pixel you are really remembering something that's that's meaningful to you so you have some you're you're remembering on some natural image manifold okay and and that really got me into this whole self-supervised representation learning so the whole self-supervised learning i got this idea from from odd in 2012 in paris and then like all of these papers that our my lab has since put out uh all result of this realization that we really need to learn a good representation that kind of that that sits on the data manifold because otherwise it's you know we're not gonna we're not gonna uh make it work okay so i do i do i have more time or am i done you can keep going yeah okay let me just you know one last thing is you know there's all these self-supervised stuff um this is probably the one that may be most relevant interesting to you guys this is a work we did with andrew owens and and and this was a self-supervised learning work again inspired by a perceptual experiment the mcgurk effect i'm sure most of you know ba but actually the audi is exactly the same it's just that the video is different okay so this this is just a beautiful illustration of a very tight coupling between the audio and and visual processing and and of course in computer vision it you know we never does this right it's always separate and so we decided we're going to do a a learn a joined audio visual representation and the way we did it is our usual self-supervised trick we say okay let's try to do bring the the real ones close and the fake ones apart so it's okay let's take let's take uh you know real let's just take a video and it's corresponding audio and fake sorry [Music] and fake we just take some random other audio from okay now the problem is this is too easy no not not gonna work very well because it's just it's too easy to tell so what we did instead is we took the same audio but we just shifted it a little bit okay and then see now the system needs to really work hard to try to figure out that there is a shift it needs to find correspondences and figure out that there is a ship and so we train the system and and and after you know several weeks of training were able to get a representation that was that it knew where the sound was coming [Music] okay and we even even did a little cute demo of separating on-screen from off-screen sound so this is the united states show to the rest of the world unshakable japan u.s alliance so much and then we can just do only the thing that they're only the only the audio that has evidence in the pixels so much or you can do the other one we're able to show to the rest of the world the unshakeable japan us alliance okay and i have many many more examples but i'm out of time so in conclusion you know in in in computer vision i have this reputation people say oh he's very creative he has lots of out of the box ideas and frankly this is all the dirty truth is i just read and get inspired by human perception research and and and you can be too and so this is this is basically that that that's how it is so thank you and also thank to all the wonderful uh all the wonderful psychologists that that i got inspiration from thank you 