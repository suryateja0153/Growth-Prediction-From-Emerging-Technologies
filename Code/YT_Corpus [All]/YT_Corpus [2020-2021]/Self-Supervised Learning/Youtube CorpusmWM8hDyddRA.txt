 [Music] we present Cola we're prediction learning to model what matters deep model-based reinforcement learning has shown promise in a number of settings ranging from challenging robotic control to learning directly from pixels these methods exhibit numerous favorable properties including sample efficiency stability and interpretability however scaling these methods to diverse visual scenes poses numerous challenges the primary of which is underfitting even with very large models in these settings the learned model cannot perfectly model the scene as a result learning algorithms must bounce how best to learn the model to achieve good performance on downstream tasks specifically in many cases there exists an objective mismatch where better performance fitting the model does not directly translate to better performance using planning in the downstream tasks motivated by this challenge of objective mismatch we ask the question by conditioning the model on the goal can we focus the model capacity on the task relevant quantities enabling better performance on downstream tasks we assume the self supervised setting where we do not have access to any test specific rewards at train time at test time the aim is to plan using the learn model to reach on-seam goals to this end we propose goal aware prediction or gap gap encodes the current observation and goal into a lane space captain reconstructs the difference between the current state and the goal from the Leighton encoding while simultaneously for dynamics are learned in the lane space critically since we're in the self supervised setting we don't have access to true goals instead we leverage hindsight relabeling to acquire goal reaching trajectories the benefits of gap are twofold first the objective encourages only capturing the parts of the scene relevant to the task allowing the model to more effectively use its capacity second gap biases the model towards higher accuracy near the gold particularly in multistage tasks as the agent gets closer to the goal the total information about the scene that the lane space needs to capture is reduced furthermore we show in the paper that for sampling based planning model accuracy near the goal is most closely tied with selecting the optimal action sequence and as a result downstream tasks performance in our experiments we examine three primary questions first how does gap redistribute model prediction error second does using gap lead to better downstream performance on visual control tasks and lastly this gap scale to real cluttered visual scenes first we examine how gap redistributes model error we do so by sampling a thousand random trajectories for an unseen task and ranking them by their true task cost we then measure model prediction error over the prediction horizon averaged across the top K trajectories where smaller values of K indicate better trajectories we observe that on the best trajectories are the ones with lowest true cost gap in Blue has prediction error that is significantly lower than the baselines which include a standard lane dynamics model in purple a standard lane dynamics model which predicts residuals from the current state termed residual model in green a method which conditions on the goal but keeps the learning objective unchanged term GCP in red and a method which also conditions on the goal and upwards the prediction laws on States closer to the goal as measured by pixel distance turned cap-weighted in orange next we examine if gap improves downstream task performance on unseen tasks in a simulated manipulation domain in the four tasks the agent must either push one box to the target position push two blocks to their respective target positions or open or close the door where distractors on the table all tasks are specified by a goal image we compare to the standard and goal condition model-based techniques described in the last slide as well as two additional baselines first action reconstruction which uses an inverse model loss to learn the lane space and reinforce learning with imagined goals a rig a sub supervised mode free RL technique gap is the top-performing method in three out of four tasks in particularly in the more challenging to block pushing task gap significantly outperforms the baselines lastly we the distribution of mall errors of gap to basins on the more challenging data from the Robo net data set we observed that the trend holds and that for goal reaching trajectories gap maintains lower bottle prediction error and generalizes to unseen trajectories more effectively the key takeaways are objective mismatch is a particularly important problem and vision based domains our goal aware prediction framework helps mitigate objective mismatch by improving model accuracy on task relevant States and using gap leads to improve downstream cast performance [Music] 