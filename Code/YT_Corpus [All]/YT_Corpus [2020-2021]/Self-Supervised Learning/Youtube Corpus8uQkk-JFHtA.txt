 okay we will wait until there's a few more people trickling in but we might as well get started now since it's 11:10 hi everyone welcome to CS 287 lecture number 17 Peter's not in town so I'll be filling in for a lecture on imitation learning so to outline today's lecture we're gonna start with some problems set up then talk about supervised learning of policies inverse optimal control and then of course an analysis of both of these approaches and then at the at the end we'll see some examples of more recent work in robotic imitation learning so to set up the problem we have as input or what we're given is our state and action space and transition model of the MDP that we're acting in but we may or may not have action we may or may not have access to the transition model then we are also given demonstrations which are samples from some expert policy PI star as opposed to the usual set up that we've been kind of operating under for the from since the beginning of the semester so we've assumed so far that we have access to this cost function which is specifying kind of like the goal of the robot but maybe this is harder than it seems to provide so in the homeworks we've been giving you this nice reward function that is usually a pretty simple straightforward function of the state but sometimes in more complicated situations it's not as easy to specify the task or communicate the robot its objective so for example we can think about trying to get a robot to clean a room for us it's harder than you might imagine to that's my reward function for this especially when the observations are the states are really high dimensional such as using images so we might try to use this like general perception by using images but you know it if we try to learn a mapping from pixels to reward that's already pretty difficult maybe we have to use some secret techniques or some heavy engineering in order to get a lower dimensional representation that we can learn or that we can specify a reward on so this sounds kind of hard so maybe you know it's probably easier to use a lower dimensional input that we might have access to right so maybe we think about using some internal sensor that we have that tells us how much dust we pick up intuitively if we want to clean a room a pretty good indication if we have done our job is if we've picked up a lot of dust but even using such a simple representation and reward function that if we communicate it to a human might make sense it's easy for our robots to optimize for the wrong thing so if we want to pick up as much dust as possible the robot can maybe learn to cheat by just picking up a sufficient amount dumping it picking it back up and keep on with this process so that I can keep collecting reward trivially without actually completing the task so this is something called reward exploitation and that's something that people in AI safety have thought a lot about so this is kind of a silly innocuous example but we can imagine that in high-risk scenarios this could have pretty harmful repercussions okay so what we might want to do is reduce the chance for misinterpretation of what the robots objective is and to do this we can use demonstrations so these demonstrations are presumably demonstrations of good behavior and these demonstrations can capture nuances in how we want to test to be accomplished so in our cleaning robot example we if we show the robot how to clean the room it can you know kind of glean from our demonstrations hopefully that we want to also obey like social norms we don't want to be disturbed disrupting the humans during cleaning we don't want to be like breaking things just so we can pick it up so this is just a way that we can reduce the misinterpretation and this is what we do in imitation learning is we learn from demonstrations so in this lecture we'll be covering two broad angles from which people are approaching imitation learning one is direct estimation of the optimal policy and the other is inverse optimal control or inverse RL where we try to estimate the reward function and then use that to learn pi-star are there any questions on the setup okay cool so we'll dive in by talking about imitation learning manifested as a supervised learning problem so again here's our input and our demonstrations are actually these input/output pairs of Pi star because pi star is just a function and because we have these input/output pairs we can just use supervised learning to approximate this function so we can just pick our favorite model sum we can use a neural network decision tree SVM whatever we want of course subject to the constraints of whether we're in a discrete or continuous action and stay in action space but we can then use these to learn pi star this sounds really great because supervised learning works and we get demonstrations of optimal behavior so weird we should be good to go right but it's not so simple in this case because we have we only have data from the optimal policy and in and I'm since we're operating in an MDP the distribution of states that we encounter is determined largely by the actions that we take so in supervised learning a common assumption is that our training and test distributions are the same and we get iid samples from these but the the distribution of states that we see under an optimal policy will definitely not be the same as if we just run out a learned policy right off the bat so maybe we can see an example of this by thinking about trying to teach a self-driving car so maybe I want the self-driving car to drive on this nice hillside road and I expert drivers so I demonstrate how I want the car to drive so here the dotted line can represent kind of the mean trajectory that I took when I was demonstrating how to drive on this road so now we train our supervised learning algorithm provides learning model and then we run out our policy so maybe in the beginning it's doing okay and then it starts to wobble a little bit and maybe it's these states that I didn't see when I was demonstrating so we're not guaranteed guaranteed to do well but maybe it still outputs something and by chance we get back on track no baby when we're turning or turn slightly too much and then we see this we kind of get out of distribution and we're not so lucky this time we take a little bit more of a wrong action turn even more and now we see this nice lake over here and we're totally out of distribution our car doesn't know what to do and then we you know drive off the cliff so this is because we have these compounding errors that come from like maybe a small error initially that takes us out of distribution and into a region where we don't know what to do okay so in 2016 and Vidia had a self-driving car project and they kind of they recognized that this is a problem so what they did is kind of they augmented their data set their training data with data that could help the self-driving car learn how to correct itself even though the expert never made a mistake necessarily so there here we they added extra cameras to the side so that we get an angle that kind of simulates if we drifted a little bit and then they manually adjusted for the shift and rotation and then they trained so here are their results [Music] so we can see that it's doing a pretty good job of following the lanes even on this kind of off road area but in this project they kind of used a very problem specific trick or augmentation kind of like maybe one would call a hack but something that's more generally applicable maybe is this algorithm called dagger and dagger as is short for data set aggregation so Before we jump into the algorithm the high level the high level goal of this algorithm is to bridge the gap between our PI P PI star and P PI theta and how are we going to do this we'll ask the expert what they would have done when they encountered the out of distribution States that we encountered while running out our policy and now since we have these expert labels for what went wrong or what happened when we went wrong we won't make the same mistakes hopefully and we'll start bridging the gap between the expert policy and our policy so here's the algorithm we start from a behavioral tone policy usually and then run out our policy get a bunch of data and then ask the expert to go in and real able for what actions they would have taken in those states so we throw out the actions that we took and replace them with the expert actions and then we train on the aggregated data set so that we get this corrected information so what's nice about this is that we have some theoretical guarantees if we make certain assumptions but as you might imagine this is pretty expensive and it might not always be possible so here in example we had before maybe like I saw that the car went off road or off veered off a little bit but I could tell it to steer back on track and then hopefully it would learn to have a more robust behavior so here's some follow-up work that uses dagger to draw to fly a drone in this forest and it is able to fly in an area that it has never seen before and this sounds really appealing because we can kind of bridge this gap that we were talking about and we really like supervised learning since it works but this is not always possible we can we not we can't always have access to an expert that is going to be ready to be able to answer the queries for all these observations that we encounter so that leads us into inverse optimal control and the question is can we do better with the expert data that we're given from the start the aerial cloning we just mimic the expert and we don't have any notion of intention so we're blindly kind of parroting what the expert is doing so and this is kind of the root of the problem that we just talked about if the expert never makes any mistakes when we inevitably make mistakes we won't know how to correct for it so maybe we can see why maybe having an understanding of what the expert is trying to do might be more helpful than just blindly following what we've seen the expert do so in not all cases where we have perfectly optimal expert but within this behavioral cloning framework we don't have any way to kind of account for experts involved in Melodi if the expert has a slightly different embodiment and has like different actions or a different action space than our robot we not might not even be able to use be a real cloning but if we can try to understand what the expert is doing maybe we can still accomplish similar tasks and maybe what's more applicable is the idea of robustness so again if we're making mistakes or we encounter things that the expert never saw some things that are unexpected then if we have an understanding or conceptualization of what the experts trying to do maybe we can still succeed so in summary finding out what the teacher is trying to do can potentially enable us to do better than the demonstrator okay so in inverse optimal control we have as input the same things data in action space perhaps access to a transition model and the demonstrations and what we're trying to do now is to learn the reward function and then use the reward function to learn pi-star yeah okay so to start we'll make some simplifying assumptions so that we can kind of analyze what's going on so first we're going to assume a linear reward function on a feature I state so we have these states that we're going to map to some feature vector that will kind of hopefully capture what is important about the state and this is something we can manually define and then we're gonna parameterize the reward by this weight vector W and under this formulation we have that the value function with respect to a particular reward function and policy is given by the classic expected discounted sum of rewards on the left so under a certain policy and evaluated with the particular reward function so if we just sub in on the right here we just subbed in our here to get W transpose Phi and then because W is not random and it's linear we can just pull it out of the expectation and now we have this w transpose times this expectation of the discounted sum of feature vectors so and we'll call this the future expectations which will become relevant in just a bit but first the takeaway from this right now is that we can represent kind of the Pala performance of a policy under some reward function as this w transpose mean of pi term okay are there any questions on this so far okay cool so okay so if our goal is to learn a reward like how do we know what a good reward function is or like what characterizes an acceptable reward even so by definition we can see that under the a true reward function by definition the value or the performance of the optimal policy must be greater than or equal to the value of any other policy and then if we remember that the value is just the W transpose mu term we can express our objective as this okay now in 2004 Peter and Andrew Inge had this paper that kind of gave us this really nice insight that maybe what's not important is maybe finding the one true reward function that we might assume like underlies like the intentions of what our expert is doing maybe it's good enough or not maybe but it's good enough to match the feature expectation and this will imply that our policy is matching the expert policy so if we hear we're just saying if the feature expectations are sufficiently close then the policies performance will be sufficiently close and that means we're doing as well as the experts doing under the reward function that we care about okay so to sketch out the algorithm that they propose we start with our assumption and initialize some reward function some linear reward function and some initial policy then what we'll do is continue iterating by alternating between guessing a reward function and then finding the optimal policy with respect to that reward function and the high-level idea here is that we're getting closer and closer to the experts performance yeah but here there's this gaping hole where we're trying to guess the reward so how do we guess the reward in the initial IRL formulation by entering and Stuart Russell back in 2000 we get this idea of degeneracy so what this means is there exists many reward functions that might be optimal or for which the expert policy is optimal so for example kind of a pathological case is if we have a constant reward function that is so you can think of just a reward function of 0 everywhere it satisfies this inequality so that means our expert policy looks optimal under this reward function but maybe this is like clearly not the most useful reward function it's not capturing any kind of like intent of the expert but this is not just limited to like kind of pathological cases for example if you see me walking towards the door maybe maybe I'm trying to go talk to someone or maybe I'm trying to inspect the door handle because I think it's interesting or maybe I want to go out and get lunch it's not really clear what the reward function is or what kind of objective I'm optimizing for so in short we have a lot of reward functions that could possibly be optimal so how do we pick one because here we want to guess the reward such that our guess a reward that makes our policy optimal how do we resolve this ambiguity we can kind of take inspiration from kind of classic classification problems so if we have this binary classification problem and we're using a linear separator there are many possible solutions that are that satisfy the constraint that we are separating the two classes but I guess the inside that kind of people had was we want to maximally separate these classes and how do we do this we find the or we try to maximize the margin where the margin is the minimum kind of distance from the boundary so they formalize this with the hard margin SVM and we can use a similar idea here kind of the intuition is that we want to maximally separate the optimal policy or the policy that is induced by our learned reward function from suboptimal policies and so we'll call this this is like analogous to the margin because here's the policy performance of under the optimal policy and any other policy and then we want to separate it maximally with this we want to separate via gamma so this looks eerily similar to the hard margin SVM objective and so you can just use the same trick where we get the signed distance to the boundary and use and turn this into a QP and then that makes it attractive all to solve okay so we slap this objective in here and solve it using a QP solver I need generics QP solve our and then this is finding the reward function again such that the demonstrators policy maximally outperforms the previously found policy so we're going to be comparing against like PI 0 PI 1 and so on until our current PI looks close to the optimal policy or the demonstrators policy okay so using this max margin formulation is one way to break ties but we're still not guaranteed to capture the demonstrators like underlying true reward function or what we might think of they're true underlying reward function that kind of describes fully what the intent is and it's hard to optimize with more expensive reward functions like neural networks and kind of what I would alluded to earlier is that in behavioral cloning we don't account for expert sub optimality or the chance of expert sub optimality one way one extension of this is to use the same SVM and analogy and use a soft margin SVM so we're kind of allowing some leeway to account for some extra sub optimality ok and if you're interested in looking at that you can see maximum margin planning uh-huh this paper from 2006 okay cool so are there any questions on yeah I'm not sure about in this case actually yeah I can look okay so next I'll talk about max entropy IRL so kind of the problems that we just introduced with this feature matching using max margin is that we're still not like fully addressing this ambiguity problem and kind of brushing under the rug this expert sub optimality but this paper by zero at all in 2008 introduces a probabilistic framework in which we can think about modeling the expert behavior and this employs the principle of maximum entropy so basically what we want to do is pick the least committed distribution subject to the constraint that we still need to be matching the feature expectations so basically the way that we're going to break ties is by choosing the distribution that gives us that uses the least assumptions we're not committing to any one in particular so kind of in the max margin formulation we are choosing arbitrarily this this objective but it might not be the case that this is what we really want and maybe what we want yeah maybe what we want is something else so we're going to try to use the least assumptions in picking our distribution okay so what we're going to do is assume again a linear wave function and known dynamics and model the experts are the probability of a probability of a trajectory under an optimal policy as proportional to the exponential of the reward function so we can interpret this as is if the cost is really low or if the cost is really high it's exponentially less likely that it came from an optimal actor but here we're allowing the optimal actor to kind of not be optimal all the time it has low probability of having these low reward trajectories okay so we can show or it can be shown that if we model it like this and in when we have linear reward function this is equivalent to modeling the expert as minimizing this objective so this objective is minimizing the expected cost under the policy while maximizing the entropy of the policy and that's way this principle of maximum entropy is coming into play okay so to write this out we're gonna assume this cost function parameterize by theta and since it's linear we can write it as this where F is the features of the state and then we're in assuming assuming that we know T or T is the transition probabilities so we know our dynamics and we're gonna use maximum likelihood to maximize the likelihood of these demonstrations actor ease the observed demonstration trajectories so we want to maximize with respect to are the parameters that we're learning the likelihood of these observed demonstrations okay so this likelihood we can write as the normalized yeah the normalized distribution here where this little towel is a trajectory from the observed demonstrations okay so now we're going to plug in for P or towel this so we said it's proportional to this because we have a normalizing factor which is just going to be the sum over all trajectories of this so for now we'll call that Z and we know that this is the same thing as minimizing the negative maximizing this is minimizing the negative and to make our lives easier because we have this product will use will optimize the log instead so we're going to minimize the negative log of this guy okay Hey uh-huh well minimizes a normalize log okay so we can kind of expand this out and get the this is just the sum okay so now we can break this down further by saying that this will be negative log of Z so yeah this will just become 1 over M log Z the sum and minus 1 over m sum over this we cancel out the log in the e and get negative cost of the trajectories and then we can distribute the negative out okay so now I've written too big and I ran out of space so let's just keep in mind that this is our objective or maximizing likelihood and remove these assumptions ok so if this is our objective what we can do is just take the gradient and set it to zero or we're gonna use gradient ascent sorry so we're going to find the gradient with respect to theta from here we have one over m gradient with respect to theta of log Z and this one over m they respect your theta over the sum of the cost okay so if we sub in the linear function for this cost and we note that this is constant for this log Z is constant over all the trajectories because this is precisely this sum over all of the trajectories possible it's not right that here sum of all trajectories of this function we can cancel out this one over m term oh where m is the cardinality of so this is just the gradient of log Z and then this is 1 over m again since theta transpose F so the gradient with respect to theta is just F okay so using chain rule we can simplify this to negative 1 over some so this is just one over Z let's see skinny and then we pull the gradient inside for chain rule and then keep this term around okay so we note that this is just the probability of this particular trajectory so because we have this proportional probability divided by the sum of all possible trajectories so this is the probability of this trajectory given theta and teeth now why is it given theta and T so our cost function is what parameter R is what gives rise to the policy and the policy combined with the dynamics will give us the trajectories so that's what this comes from and since it's all linear you can kind of just turn this into a sum over the states because the summary trajectories will be intractable but we can figure out how do you compute this and then using the same observation here we can just get that this is the features of the state okay all right No so this is our initial objective okay and we've turned it into this so the gradient is this term Plus this term and we can compute this but we don't know how yet to compute this so in order to find actually compute this will use a DP algorithm so we note that the probability of this state is the sum over all time steps of the visiting that state at that time step and how we'll do this is well kind of do this forward computation so we'll say that this is the marginalization over the joint of all the previous states and actions that I could take or that could get us here so that's T plus 1 is this times this is getting messy but times are policy and then times the transition probability that we get there I'll write that out here so s prime is we're marginalizing over the states at time T and the actions that we can take from those states in order to get the probability of this state at the next time step ok so now we know how to compute this and we can formulate this in an algorithm so here's the algorithm we initialize some theta and gather some demonstrations so we have our data set and solve for the optimal policy with respect to the current cost function and that's where we're getting this by the way we have to solve for the optimal policy and when we have a tabular setting we can use something like value iteration like we've learned earlier so we get this policy and then solve for this using the algorithm we just described and then compute the gradient as follows and then update with one gradient step and then we repeat this process so once you updated your theta your cost function changes so you'll have a different policy okay so are there any questions on this okay so now we'll see some examples of more recent work and imitation learning in robotic applications so the first that we'll look at is motivated by the fact that we don't have existing control interfaces for robotic manipulation like we do for self-driving cars or piloting drones so earlier we had some examples of self-driving cars and this is pretty easy to provide demonstrations as humans because the observations that we see are exactly those that the robot will see and we can measure the actions that we took the steering actions that is and similar to piloting drones we have a remote control and then get these same observations that we would if the drone was flying itself but in robotic manipulation when we have especially when we're doing things from vision things get a little complicated because usual methods of providing demonstrations involve taking the robot and physically guiding it through the actions we want it to take and this is called kinesthetic teaching but when we have visual input we as humans if we are guiding the robot this will cause some visual obstruction or some hardships for the perception algorithms that we're using because they don't have to ignore the human in the observation so in this paper they thought how can we provide demonstrations high-quality demonstrations while also keeping it inexpensive so they developed a cost-effective cost-effective consumer grade VR teleoperation system so that the human can control the robot without actually having to manually guide it physically so there's no vision there they use a single neural network architecture that performs all the tasks from vision and for the imitation learning algorithm itself these behavioral cloning augmented with an auxiliary loss to make it goal-oriented so we'll talk more about this auxiliary loss in a bit but the idea is if you add some source of self supervision we can kind of make incorporate some ideas from inverse reinforcement learning and get better for performance so we'll talk about the auxiliary loss shortly but here's a depiction of the VR system and someone controlling the robot with it and then on the right the efficacy of the auxiliary loss okay so in this work the inputs of the policy were raw images concatenating are concatenated with on the end effector position after some learned function was applied to it so now we get to the auxiliary loss and during training we have access to kind of the tasks and the full demonstration that was given per per rollout so what they did in their experiments was have two to auxiliary tasks each with this auxiliary loss so it's just a regression loss so what they're trying to do is predict the current end effector position given the raw image as well as what the final end effector position will be at the end of the demonstration so this is kind of where the goal oriented miss comes from we are predicting the final state of the robot from just these observations and in this sense we're kind of like inferring what the goal is yeah and that will influence the features because we'll be back propping through this function with this zillion loss and they show that it helped with learning so we can see a demonstration of their results so the human is demonstrating via VR teleoperation how to do the spec in place tasks and then the robot execution the learned behavior so they show it on a fairly wide range of robotic manipulation tasks are there any questions on this paper okay so if you're interested this paper is called deep imitation learning for a complex manipulation test from virtual reality teleoperation from Peters group back in 2018 yeah okay so in the previous example what was nice is that we have this control interface now from which we can control provider ID demonstrations to learn from but we still need quite a few demonstrations so still not too much but in order to be very effective we need a quite a bit of demonstrations that brings us to the fields of study or a line of research and imitation learning that has to do with few shot learning so well we want to do in this setting is use prior experience in order to learn from a single demonstration or few demonstrations so I'll talk about the four the first such successful result and this paper is called one-shot imitation learning back in 2017 so ideally we can learn a task from a few demonstrations and furthermore we want to be able to generalize to any kind of instantiation of the task so what they look at in this paper is building towers of blocks so blocks kind of induce this combinatorial space so we can kind of combine them in any way we want and we don't want to just use a ton of demonstrations just to learn how to stack some blocks in a particular configuration we want to be able to build any configuration if we're just shown an example so in order to accomplish this they use a metal learning approach so what they do is chain on pairs of demonstrations try to use one to predict the actions of the other and one key contribution of this work is that is their proposed architecture which consists of these three networks and if you're interested you can check out the paper but here is a high level of kind of schematic of what their architecture looks like and here's a sample video of some of their results so they're able to accomplish this really really long horizon tasks using a single demonstration okay another line of research is third-person imitation learning so so far we've assumed that the observations and actions that we get from the demonstrator are aligned with the robots and this way we can use like super super visor learning approaches like behavioral cloning and this is what's called first-person imitation but what humans do or what humans are capable of is observing other people maybe with slightly different or observing others or even things with different morphologies and able are able to imitate just by observing them so in this paper what they do is try to translate between different contexts where contexts are differences in viewpoint surroundings and objects in order to learn from just observing so in this paper they learn a context-aware translation model using multiple demonstrations of the same task and then when faced with a new context they translate the demonstrations and follow a trajectory so this means that we don't necessarily need the robot itself to be guided through in the first person in order to imitate so on the left is a figure from the paper that shows their context-aware translation model and I guess a very high-level view of what this is is we're just mapping to some shared latent space where the features are aligned and then we can decode into the two different contexts or two contexts that were given at the time and then a schematic of their entire approach is on the right so we have a demonstration you get a current target context that we that the robot is acting in and then translate the demonstration and you use this for an RL logarithm so here the robot is using has to use a tool in order to accomplish the task because we explicitly are in this paper they explicitly avoid a handling domain shift and this is one like kind of big limitation of such approaches so so what they're varying here instead of the actual embodiment of the robot and having to resolve differences in morphology that induces like a really hard vision problem they use things that a human demonstrator and a robot demonstrator can both use and then we learned we learned some optimal behavior based on observing the observing the human okay so we saw one-shot imitation learning or a few shot learning and we saw a third person imitation learning why not combine both so we can imitate others by observing a single demonstration so can we get robots to do the same so this is a really enticing prospect because it's one really natural and intuitive wave if we want to teach a robot how to do something is just to demonstrate how to do it ourselves but just like the previous paper it's really difficult to resolve differences in morphology and again in this paper they use tools so that a human demonstrator and robot diamonds and robot can use the same or yeah they don't have to account for differences in morphology they can just use the same tool okay in this paper there they acknowledge some previous work which explicitly uses some manual correspondence using pose detection to kind of align like the robots end effector with the human hand for example instead they take a data-driven approach and they try to overcome the differences yeah sorry they use a data-driven approach because one additional problem is maybe we can't even find a mapping between a robot and a human because our robot might not be a humanoid robot so instead they try to infer the goal from the demonstration instead of trying to like find this mapping between human and robot if we were just trying to infer what the human demonstrator is trying to do no matter the morphology of the robot we can hopefully accomplish the same thing so this is also a meta learning approach so what they do is build a rich prior on structurally similar tasks so this is like the meta training phase and what they do is head in for the policy given a human demo so what this requires is human aligned robot and human demos so it learns from all these training samples it should learn how to infer the corresponding robot demo when given a human demo ok and another thing they do in this paper is use temporal convolutions to integrate temporal information in the demonstration because the demonstration is a video if you're interested you can check out the paper that's called one-shot imitation from observing observing humans via domain adaptive meta learning also known as demo ok we can see some of their results here so not only is there a difference between the demonstrator and the robots so a human is demonstrating we they accomplish kind of circumventing problems with a like other domain adaptation so the background for example is different and then they show also on another robot that they're able to achieve good results okay yeah that's all I have do you guys have any questions otherwise you can be set free early okay thanks [Applause] 