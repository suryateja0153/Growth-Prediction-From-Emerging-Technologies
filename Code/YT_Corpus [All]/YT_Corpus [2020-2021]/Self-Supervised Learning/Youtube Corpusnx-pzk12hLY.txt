 Hi, everybody! The title of our paper is Robust 3D Self-portraits in Seconds. We propose an efficient method for 3D self-portraits using a single RGBD camera. The input is a single-view RGBD video, in which the performer wearing loose clothes only needs to self-rotate a circle. We want to reconstruct a detailed and textured 3D self-portrait conveniently and efficiently and can handle general clothes. The equipments of our system are only a computer and a single RGBD sensor. After the subject turns around in front of the sensor, our system will reconstruct a 3D self-portrait in a few seconds. In the related work, we will talk about three categories of 3D human reconstruction. The first category is the learning-based method. These methods infer a parametric model or body mesh from a single RGB image. However, these methods suffer from depth ambiguity and lack accuracy. The second branch is fusion-based method. These methods reconstruct non-rigid objects from a single RGBD camera. But they cannot achieve robust tracking or handle general cloth. The final branch is bundled adjustment. These methods can generate robust 3D self-portraits, but they are too heavy for applications, while our method is in seconds. Our goal is to generate robust, efficient and accurate 3d self-portraits. And our insight is that learning-based reconstruction can provide a shape prior for more robust double-layer tracking, and the proposed non-rigid volumetric deformation refines the shape prior simultaneously, so that we can fuse sparse and large partial scans for bundle adjustment, which reduces the optimized variables significantly. Finally, we propose the lightweight bundle adjustment that involves key frame selection for efficiency as well as joint optimization and live terms to further improve accuracy At the first, we utilize the adapted PIFu to generate an inner model for more robust double-layer tracking. Then we perform the proposed PIFusion to fuse partial scans, while refining the inner model by non-rigid volumetric deformation. Finally, the lightweight bundle adjustment is proposed to loop these partial scans in reference frame and fit them with key input in live frames to generate a detailed and textured 3D self-portrait. Now let's see the details of our method. We incorporated the depth feature into PIFu to generate a more accurate inner model. Then we construct inner and outer correspondences to achieve robust the double-layer tracking, while the proposal non-rigid volumetric deformation is refining the inner model continuously. After PIFusion, we can obtain several partial scans in the reference frame. In the reference frame, the partial scans should be deformed to construct a loop. Moreover, they should be warped to fit with outer observations in live frames. So we jointly optimize the bundle deformation and live warp fields. In the formulation, there are four terms. The loop term guarantees that the partial scans should construct a loop in reference frame. And the live depth and silhouette terms make sure that the partial scans should fit with depth point clouds and silhouettes in live frames. The smooth term is a regularization term of non-rigid deformation. Here are some example 3D self-portraits generated by our method. We compare the proposed PIFusion with DynamicFusion and DoubleFusion, and our method can achieve more robust tracking. We also compare our method with the state-of-the-art bundle adjustment method, Wang et al. And our method can reconstruct more detailed and accurate 3D self-portraits. QR code is our project webpage. Welcome to visit. Thanks for watching. 