 Wei-Chiu Ma: Okay, so everyone it's we start recording out Wei-Chiu Ma: Okay, so, um, hello everyone. It's a great pleasure to have Shubham here today at presenting at our vision seminar. So he is currently a research scientist at Facebook AI research located in Pittsburgh and he's going to become an assistant professor and Carnegie Mellon University next year. Wei-Chiu Ma: So in the past few years. He has done a lot of amazing and really interesting work in 3D vision. So one of his work was recognized by the CVPR best student paper award into 2015 Wei-Chiu Ma: And a lot other of His work has inspired, like many follow up works from various groups in the road. So, just to name a few. Wei-Chiu Ma: And we all know that lots of reasons for some reason we're having trying to make the graphics rendering pipeline differential right Wei-Chiu Ma: And for instance, like the volumetric rendering work from Geiger, and the volumetric rendering techniques used in NERF Wei-Chiu Ma: But actually, Shubham has done very similar thing already into 17 in a CVPR or paper and one of the reason I remember this is because that ears up I was hosted in Hawaii and I spend most of my time in a beach and that's an enormous talk was one of the few one that I've attended Wei-Chiu Ma: Is Antonio here. Okay, he's not. Okay, great. Wei-Chiu Ma: Okay. And another thing is on recently we saw a lot of things on Twitter called 3D photography right where people try to send this is normal view from single image. Wei-Chiu Ma: And a lot of those techniques are based on a representation card maybe layer that image. Wei-Chiu Ma: And this was originally introduced in graphics, but she bomb was the first one to bring it into the normal use in this context, and since then vision community has do a lot of things with this thing. Wei-Chiu Ma: Okay. So as you can see she has done a lot of really cool stuff. And that's see what he's going to tell us today. Okay, achieve and we are very happy to have you here. It's all yourself. Shubham Tulsiani: Yeah, thanks a lot for the introduction. It's great to be virtually here. Shubham Tulsiani: In fact, I was Shubham Tulsiani: There in person, a couple of years back, so Shubham Tulsiani: This talk I've tried to only talk about the stuff that sort of nuisance thing. Shubham Tulsiani: But as it isn't a lot of the slides are made at like 3am yesterday, so forgive some things on our college debt. Shubham Tulsiani: And, you know, before I start, feel free to stop me at any point, I sort of in zoom. I don't think I'll be able to see you raise your hand or something. So just speak up if you have any questions at any point in between. Shubham Tulsiani: And yeah, I think just interrupt if anything is unclear if you have any questions. Shubham Tulsiani: So today I want to talk about sort of as the title says self supervised reconstruction in traction and Shubham Tulsiani: You know, at a glance, you might think these are two very different things that have one is reconstruction, which is Shubham Tulsiani: Something we're getting the CD aspects of the world and interaction is that affecting the world. Shubham Tulsiani: But, you know, over the talk I have to convince you that they are very closely related. And that one is just about Shubham Tulsiani: Representing the world and then subsequently using that sort of every presentation you do something meaningful and advisors, making the slides. I kind of realized that Shubham Tulsiani: A good subtitle for the talk is learning with Newton in the loop where you can call supervised learning. We're learning from annotation as learning with human in the loop idiot annotations from them. Shubham Tulsiani: And a lot of what I'll talk about today tries to sort of do away with that by instead using principles from physics and geometry. And so this cheekily learning Shubham Tulsiani: So yeah, before I start, I want to maybe take a couple of steps back and just note that for all of us who are doing AI or in some sense subfield of AI. Shubham Tulsiani: One of the goals has always been to build these general purpose robots. Right. So, for example, Shubham Tulsiani: Here's the robot shown from the movie volley, and we want to have these kind of intelligent agents that can do something meaningful across different scenarios, for example. Shubham Tulsiani: Hopefully I can have a robot, such as this, which cleans my kitchen and living room maybe makes me lunch or coffee or something like that. Or, alternatively, you know, hopefully the same robot if there isn't coffee at my home can go to the restaurant to my house and go fetch coffee. Shubham Tulsiani: And even looking at these very mundane daily life examples we realized that for this particular agent to accomplish these kind of tasks. It's really needs to understand the world. Shubham Tulsiani: At a very fundamental level, it needs to understand where people are needs to understand where it can move around understand how things will change how people around the agent will walk and so on. Shubham Tulsiani: And so sort of towards this more or less grand goal of building intelligent agents that can meaningfully impact in the world. I just want to take stock of where we are and how is it that we can sort of eventually get there eventually build Shubham Tulsiani: And well, any such agent will in some sense need to have two basic capabilities. Right. And one is that of perception. That is such an agent should be able to understand what it is that it seems Shubham Tulsiani: And the second capabilities that of interaction that is this agent should then be able to accomplish certain goals or manipulate the world to achieve certain names. Shubham Tulsiani: And if you look at both of these individually, the tasks of perception or the field of perception in the field of interaction. Shubham Tulsiani: These are actually been to great success stories across the decades. If you look at sort of perception or interaction on so Shubham Tulsiani: For example, we have these amazing systems that can Shubham Tulsiani: Look at an image and classify what they're seeing or classify or sort of even go ahead and live in the pixels or what they're seeing Shubham Tulsiani: For example, here are some results from originally were given this image, you could easily recognize this as a major container ship and so on. Shubham Tulsiani: And in parallel over the decades. He wants to make a remarkable amount of progress in building robots that can interact. For example, can do very dexterous manipulation tasks. Shubham Tulsiani: And actually very, very useful. These days in industrial settings for example in assembling cars or kicking things off from conveyor belts and placing them certain bins and automatic packing and so on. Shubham Tulsiani: So if we look at both progress in perception and interaction separately. We realized that we have made great amount of strides. Shubham Tulsiani: But unfortunately, the things I'm not quite come together, we don't quite have robots that work in the real world. And I'd say that's because Shubham Tulsiani: When we're looking at perception give so far. Probably just focused on naming and labeling pixels global systems that tackle perception for perception sick. Bye. Recognizing images and pixels. Shubham Tulsiani: And in contrast Shubham Tulsiani: The approaches we have looked at for interaction typically work only in controlled environments where, in some sense, the problem of perception has been obstructed away because Shubham Tulsiani: These are environments are controlled to be in a setting where we already know the underlying state of the system. Shubham Tulsiani: And so what I'd argue is that to get towards these more general purpose robots that can function diverse settings. Shubham Tulsiani: As a computer vision person what I'd argue, is we need to look at perception that is more directly useful for the kind of interactions that we want to do. And one way of looking at the Shubham Tulsiani: Techniques, you have developed in the interaction literature is that we have learned how to Shubham Tulsiani: Go to actions and given some sort of world model. So, given an understanding of how the world is and how the world works. We have developed many techniques over the decades that can let us leverage the sort of world models. Shubham Tulsiani: To get to desired actions. And so, in this context, sort of the role of perception is then to go from some sort of sensory input to build some sort of value models that can then be subsequently used for interaction. Shubham Tulsiani: And so in this talk today, I'm going to roughly focus on how it is that we can go from these sort of sensor inputs to build some sort of world models that can then subsequently be used for interaction and through Shubham Tulsiani: At a high level, what we hope that these World War one should capture is described to us how the world is and how will the world change as a result of my actions. For example, if I pull this block away the tower that is standing will fall under Shubham Tulsiani: And sort of one of the main questions in this quest to go from sensory input to our models is Shubham Tulsiani: You know, how is it that we can acquire supervision for this. So we would of course want this sort of a prediction to work in knowledge scenarios that you've seen for the first time, we would want to be able to infer world models. Shubham Tulsiani: When we will serve as the new house. So we clearly need some form of learning that relies on previous data. Shubham Tulsiani: But the question is what enables this learning how can we direct supervisory signal to go from the sensor inputs to some sort of world models. Shubham Tulsiani: And there's been one very, very successful paradigm for learning that we've used across many tasks in the computer vision community. Shubham Tulsiani: Which has been to learn from many annotations. So for example, we can look at this task of image classification that I discussed earlier. Shubham Tulsiani: And when you want to classify whether an image is a container ship or a leopard or so on. Shubham Tulsiani: You can collect these millions of examples that for each image sensor input have the corresponding output annotated by a human. And you can use that a supervisory signal to learn the prediction function. And this is, of course, also work for slightly richer tasks such as Shubham Tulsiani: For instance, segmentation using similarly large data set, such as Microsoft local Shubham Tulsiani: But in fashion. If you want to use this paradigm for this task of predicting world models, this becomes a bit problematic. Shubham Tulsiani: Because, you know, figuring out I notation for what is the underlying world model given a particular image extremely challenging and I'd argue that Shubham Tulsiani: Getting such annotation is extremely costly and often impossible if you want certain details such as the to the mass of the object. Shubham Tulsiani: And sort of in parallel. This is uncertain ecologically plausible. This is not how we humans learn the humans just Shubham Tulsiani: Go around and interact with the world and learn to predict these sort of learn and understanding of the world through our interactions. We are not Shubham Tulsiani: Given supervision by our parents that for this particular object. This was the specific masters for the chip and so on. Shubham Tulsiani: So there must be a slightly better way that we can learn to go from our observations to some sort of mental models of the world without relying on these tedious manual annotations and Shubham Tulsiani: The way I want to talk about is learning by prediction consistency. So what I mean by that is, given our visual inputs or sensory inputs. Shubham Tulsiani: Our goal is to predict some sort of mental model of the world. Shubham Tulsiani: And because it's sort of a mental model of the world, it needs to have some predictive capabilities that are consistent with the laws of physics. So Shubham Tulsiani: Given a predictive model and our knowledge of let's the laws of physics and laws of geometry. Shubham Tulsiani: You couldn't use certain predictions from the squadron this model can make certain sort of verifiable hypotheses or verifiable predictions which we can match against instead of us observations and this can allow us to learn, even without using supervision for the truth. Shubham Tulsiani: And so sort of the picture that I'm showing here at a very high level tries to learn a prediction function that predicts some sort of world models. Shubham Tulsiani: That are just consistent with available observations when viewed through the lens of physics are viewed through the lens of geometry. Shubham Tulsiani: And what I want to cover in my talk today are a few instantiate of just this idea Shubham Tulsiani: So I sort of covered a few projects that instantiate just this idea of learning by prediction consistency for a few different us sort of learning single view 3D prediction. Shubham Tulsiani: Or sort of understanding physical predictions. From video. And lastly, learning some sort of followed models that are moderately useful for interacting with the world. Shubham Tulsiani: So, and sort of to begin with. I want to focus on the single view through friction dusk. Shubham Tulsiani: And before I go into the method I do again motivate why it is that we need to go beyond to the reasoning, why it is that we need some sort of a 3D representation of the world. Shubham Tulsiani: So if I'm seeing this particular image, then the current recognition systems that we have will give us very accurate representations of this image in the form of these labels associated two instances are two pixels. Shubham Tulsiani: That they'll identify that there's a couch on the left, identify the pixels that are associated with it and so on for all of the other objects that are in the scene. Shubham Tulsiani: But this the presentation itself is not very actionable. It's not very meaningful on its own. What it ignores is that this particular image is actually sort of a window to the underlying 3D world behind it that Shubham Tulsiani: This image is just something that a particular robot is observing from a specific perspective and it's just Shubham Tulsiani: Sort of a canvas that points to the underlying physical reality that is behind it and if this robot needs to go around and interact with the scene or go around and clean the scene or Shubham Tulsiani: Go and sit on the chair. It really needs to understand this physical reality behind the image as opposed to just the labeling of the pixels of image. So you know if Shubham Tulsiani: As an alternative, all he could understand on the image where these colored pixel groups that we saw earlier. Shubham Tulsiani: Than just data presentation is essentially like having a painting, which is not really sufficient to be able to interact with the world. And so Shubham Tulsiani: We really need some notion of the underlying physical reality, given this image that they've given an image that as this one, we need methods that can understand what's really behind this particular image. Shubham Tulsiani: And so just as one possible representation of this patient structure what we have been sort of Shubham Tulsiani: One possible approach to infer the spatial structured underlying elements that we've been following is Shubham Tulsiani: That we want to be able to infer the 3D structure for all the objects in the scene. Shubham Tulsiani: And then given this presentation compose a 3D scene. So what it may look like is we can infer the 3D structure of the chat, place it at the appropriate location and similarly for the couch and Shubham Tulsiani: Sort of the pillows and place them on the couch and similarly for the chairs, tables capacity to and so Shubham Tulsiani: And I'll not really talked about the composition part of how do we compose a scene and given all the objects, but I want to what I want to focus on is how do we acquire the capability. Shubham Tulsiani: Of inferring 3D for all kinds of objects that we might see which is a very diverse set for example, even in the scene, things like lamps boulders and the sculpture, etc. So how is it that we can acquire the capability to predict 3D for any particular object that we see. Shubham Tulsiani: So the more completely. The goal here is given a single image of an object that has this one we want to be able to predict Shubham Tulsiani: It's 3D shape and corresponding camera pose and sort of perhaps optionally texture and here I'm just showing the sample result that one of our recent works gets on this particular image and also maybe on some other images. Shubham Tulsiani: For these animals, for example, and again before I go into details. I want to place this in context of a lot of prior work that has looked at this task of single view 3D influence and Shubham Tulsiani: In fact, you know, the problem of single view 3D influence actually traces back to the very first computer vision PhD thesis ever written, which incidentally was at MIT malaria robots were given an input image we wanted to Shubham Tulsiani: Figure out whether the blocks are pyramids, or two boys and where they are. So this was more framed as a 3D detection task will be a prayer. He knew the shapes of the objects, but just wanted to find them in an image. And this actually Shubham Tulsiani: Culminated in very impressive demo, such as the one you see on the right or more complicated template shapes. It has the ones you see on the left and sort of this Shubham Tulsiani: Same approach of finding known shapes and an image is what continued over many, many years towards more complex shapes, such as, you know, the cabinets on the right or the razor on the left. Shubham Tulsiani: With all of these approaches initially focused on identifying the known 3D shape and an image, whereas not really predicting the shape for a new object of the scene. Shubham Tulsiani: And sort of that to some extent changed with the advent of deep learning and the availability of synthetic data where the common approach then became to have an input image and learn a prediction function to infer some sort of a 3D presentation. Shubham Tulsiani: But across all of these approaches the way learning would work is we would require annotations for the ground to 3D associated with each training image. Shubham Tulsiani: So all of the approaches that I'm showing up actually required synthetic training data for learning and that doesn't generalize very well to real images and the Learn systems are not easily transferable to other mentors. Shubham Tulsiani: And so to maybe do away with the requirement of Shubham Tulsiani: 3D supervision some approaches then looked at using mighty view supervision instead. So, for example, even one input image we could predict some of the sort of the 3D structure. Shubham Tulsiani: And say that it should be consistent with the normal view of the same object. And so instead of requiring ground to 3D would require multi view data for me. Shubham Tulsiani: But unfortunately, this is also slightly constrained because it's not very easy to get multi view data even for a lot of object categories, for example. Shubham Tulsiani: You're thinking of birds. The birds will just fly away before you move around and capture the same word from a different perspective. Shubham Tulsiani: So, you know, while reducing supervision to requiring multi views is helpful. It doesn't enable large scale learning, which is what we want to do when we want to deconstruct all kinds of object categories. So the form of Shubham Tulsiani: training data that we want to be able to use these image connections that is we have many, many images, but only a single view per instance and for each instance, though, be a zoom some approximate Shubham Tulsiani: Mask segmentation as additional labels that are available to us, but using this form of data, a supervisory signal can allow us to really learn very scalable systems that can learn 3D across many hundreds of thousands of other categories, hopefully. Shubham Tulsiani: But the question is, so again we want to be able to learn some sort of a prediction system that given an image of an object. Shubham Tulsiani: can predict the underlying 3D structure, but only has this one viewer supervisory signal rate. So the question is, how do we learn from this and Shubham Tulsiani: One possible solution is we can say that the predicted 3D should just be self consistent that is whatever 3D we predict it should be consistent with the input image from which it was productive. Shubham Tulsiani: And that is certainly useful signal, but unfortunately that's not sufficient by itself. Shubham Tulsiani: And just to highlight this is sort of an example from sin, I needed some seminal paper where we're seeing the prediction of some 3D structure on a plane. Shubham Tulsiani: And this 3D structure could be something like cuboid or it could be something far more arbitrary. So there are essentially infinite possible 3D structures. Shubham Tulsiani: That can explain a single view. And all of these are sort of self consistent with the 2D projection that you're seeing. So we need some additional way to be able to distinguish these many degenerate solutions about Shubham Tulsiani: So to be able to learn from these image collections. We can enforce dramatic self consistency, but that is not sufficient by itself, because there are many possible Shubham Tulsiani: Solutions for 3D structure that can explain a particular view. So, in addition, what we might need to enforce is some notion of category level possibility that the shapes across the category. Do not very operation. Shubham Tulsiani: And so in sort of a recent paper that we put out an archive, a couple of weeks back, we will operational using these ideas. Shubham Tulsiani: And given an input elements we predict what we call an implicit memory presentation using this sort of Shubham Tulsiani: An image connection or superficiality signal. So we have many images of birds, but for each image eaten since we only have a single view. And importantly, we don't have any key point or post supervision with several private zoom and we just only have an approximate focus on segmentation. Shubham Tulsiani: We want to be able to learn 3D influence using this sort of data. Shubham Tulsiani: Though in addition we also relied on a single template shape or category so across all birds. We have one template shape that guides us in learning the shape space and sort of illustrate that come in a bit. Shubham Tulsiani: So before that going just to clarify what this implicit misrepresentation is and we to contrast it with an explicit mystery presentation so Shubham Tulsiani: Typically when we present a mesh we represented as the locations of a fixed set of verses. So the fixer topology, such as a sphere. Shubham Tulsiani: And the parameters the sphere by 600 verses. So the representation of a mesh is this 1800 dimensional vector which specifies the location for each of the witnesses. Shubham Tulsiani: And so, for example, we can pick a particular vertex and predict its corresponding location and so on. For all the vertices. And this gives us the resulting message of the bedroom here. Shubham Tulsiani: In contrast, and implicit representation doesn't Parliament tries location of specific word is. But it's Parliament raised Shubham Tulsiani: By sort of an arbitrary function that maps and arbitrary point on the original apologies that are the sphere to corresponding target coordinate. So it's so much more Shubham Tulsiani: Smoother and continuously presentation compared to an explicit misrepresentation. Shubham Tulsiani: That given an arbitrary point, there's a function that maps into a corresponding location. Shubham Tulsiani: So this is sort of the representation that we use in this work, except we specialize it to operate at a category level as opposed to an instance level. Shubham Tulsiani: And sort of what I mean by that is across the categories as houses we have different 3D shapes. It has this horse. Here are the other horse shown here we go into an implicit shape space that can capture these different shapes in a category. Shubham Tulsiani: So instead of having different implicit functions that correspond to different instances we instead. So the parameter is it as Shubham Tulsiani: Being a latent variable condition implicit function. And this is actually very similar to this Atlas paper if you're familiar with it that Shubham Tulsiani: Each ship is represented by this latent variable and then given any particular point on the sphere condition on the latent variable will get the shape of the appropriate us Shubham Tulsiani: Except what the additionally do is factor is this implicit shape space to have a category level mean ship and only an instance level information that is across all instances. There's a common mean ship. Shubham Tulsiani: And then this differences, specific to the instance are only deformations from that means shape to the particular shape. And so having this factorization allows us to Shubham Tulsiani: Wear this means shape we actually initialize by the template that I talked about earlier. But essentially having this factorization allows us to enforce category level consistency. Because now across all instances in a category we enforce that visit common mean shape of the common fiber. Shubham Tulsiani: That is shared across the implicit free presentations across on ships. Shubham Tulsiani: So given this sort of implicitly presentation that pilot can parameters, the shape space of a category. What we want to learn his prediction of this Shubham Tulsiani: Sort of. We want to be able to learn the shape space as well as be able to predict the corresponding shapes for images in a category so remote completely Shubham Tulsiani: Given an input image. What we want to be able to predict is the corresponding camera, that is, where is it that we are viewing this particular object from. And also, what's its corresponding shape. Shubham Tulsiani: And sort of this time as I described earlier for each category we have this template shape which can get deformed based on the predicted latent variable. Shubham Tulsiani: And then projected back to the image coordinates using the predicted calendar. Shubham Tulsiani: Where sort of all of these functions fiber and delta are actually things that are being learned on the fly with respect to the objectives that are described Shubham Tulsiani: So given the particular image we predict the camera and the corresponding deformation parameters which allow us to Shubham Tulsiani: Transform a predictive predicted 3D shape on to the image frame and then we just say that this render tradition should match the foreground mask that we have available for this particular moments. Shubham Tulsiani: And in addition, what we also do is make per pixel predictions for what spherical coordinates is being rendered at that pixel. So for each pixel you we predict what point on the sphere gets deformed and projected back on to this particular pixel Shubham Tulsiani: And the way that helps is it provides us an additional regularization to learn better 3D infants so completely for each pixel you know prediction of what spherical coordinates predicts to that. Shubham Tulsiani: And we say that this particular predicted coordinate when map to the moon shape or when deformed and predicted back to basically come back to the location that it started from. Shubham Tulsiani: And this prediction see is being made only using sort of very local evidence. So you can think of it as Shubham Tulsiani: Implicitly encouraging you know pixels along the eye that looks similar to predicts similar spherical coordinates. So this cycle consistency loss. Shubham Tulsiani: Just implicitly encourages semantic consistency that similar points on the sphere map to semantically similar reasons or regions that have similar appearance. Shubham Tulsiani: And this sort of allows us to learn good poses without explicitly requiring keyboard supervision and this was actually based on sort of a previous paper we had called canonical surface mapping and I'd sort of encourage you to look into that for some more details. Shubham Tulsiani: But just a combination of these two losses. When we enforce that the predicted shapes should explain the foreground and that the purpose of predictions. Shubham Tulsiani: Should be cycle consistent allows us to learn 3D in France, using just these images connections. Shubham Tulsiani: Whereas in addition we also extend the variable z to not just capture the shape Shubham Tulsiani: But also capture the texture that gets overlaid on the mesh and I'm skipping some of the details there. They can just do this by extending the last also take into account the color values at the foreground mixes. Shubham Tulsiani: And then sort of what this framework allows us to do is given a new image make 3D influence citizen for the 3D shape. Shubham Tulsiani: For this new image that we're seeing for the first thing, and also overlay the PREDICT texture on it and maybe visualize it from normal views and we see that both Shubham Tulsiani: The shape and the texture that we predict is actually quite meaningful and we do this across many categories, because this approach is really scalable. Shubham Tulsiani: It only requires approximate for Grandmaster supervision to be able to make this sort of prediction. And so, and it can even handle some sort of an articulation. So does the head of the husbanding down Shubham Tulsiani: And in the paper we showed is also about 30 of the categories, because it's extremely easy to apply this approach to a new category. All we need is approximate segmentation for all the for some a few hundred instances in the category. Shubham Tulsiani: And just one 3D template shape downloaded from the web. And this allows us to get through meaningful reconstructions across many, many of the categories. Shubham Tulsiani: And we also evaluate this sort of both directly and indirectly. So, you know, given these two different images. Shubham Tulsiani: The way we are reconstructing them is actually by deforming the same sphere, right. So there's a common sphere which through the implicit function and corresponding latent variable. Shubham Tulsiani: Gets deformed and projected back onto this image. So because they're being deformed to a common sphere we can Shubham Tulsiani: Infer correspondences between these two different images by figuring out which point on the sphere gets projected to which pixel Shubham Tulsiani: So for example, this coloring here highlights that same points on the sphere get projected onto this location. So given a query pixel on the image on the left. Shubham Tulsiani: You can find the corresponding pixel on the image on the right by finding which pixel had the closest point on the sphere project back at that location. Shubham Tulsiani: And this allows us to transfer key points from one image on to the other and provides us a way of evaluating this and Shubham Tulsiani: One other way that we also evaluate it is by annotating some semantic key points on the template shape. So just for testing. Shubham Tulsiani: We noted certain points on the template that this particular 3D point is week this particular 3D point of sale and so on. And we measure whether, when our 3D shape is projected onto the image do those key points land at the locations of the expected them to and we evaluate Shubham Tulsiani: Our deconstruction's individually using the key point transfer every prediction accuracy and Shubham Tulsiani: Maybe I'll skip over some of the details, but Shubham Tulsiani: Yes, I think they just want to highlight that. Shubham Tulsiani: The in the top row. The first approach the density gradient approach is that approach that does not use any 3D structure at all. Shubham Tulsiani: And the two approaches in the middle of the CSM and a CSM or approaches that do not allow shape variation. They use some 3D structure, but do not allow the shapes across different instances to be there is an approach which allows predicting a Shubham Tulsiani: Which allows for instance deformation gives us much more expressive power and is more accurate across both these metrics. Shubham Tulsiani: And we also evaluated our approach on this 3D intersection of A union metric on a different data set. Shubham Tulsiani: Very clearly compared to previous approaches that used more supervision in the form of key points and we observed that our results are actually quite competitive to the previous approaches. Shubham Tulsiani: As well. But unfortunately, there weren't any prior work which could get 3D reconstruction using just the amount of supervision that we were using in this book. Shubham Tulsiani: But sort of yeah this is going back to some more isn't Shubham Tulsiani: As I said, the approach we have actually worked remarkably well across diverse set of categories. So on the left. I'm showing the input elements of you're seeing for the first thing Shubham Tulsiani: The second row is a predicted shape you from the predicted viewpoint, and then it's textured, and then we see Shubham Tulsiani: The predicted textured shape from random normal viewpoint and, you know, we see that the shapes are actually quite meaningful. We can sometimes captured articulation. For example, the head of the bear is a bit bent down, whereas the head of the calculus of it bent up Shubham Tulsiani: And similarly works across different categories. One thing I've noticed too is we're not very good at. Shubham Tulsiani: Explaining very precise articulation. So while the head bending down is typically accurate. Shubham Tulsiani: The location of the legs is not very accurate and to some extent that's because we only use approximate for grandmasters by the signal and those are often not precise enough to Shubham Tulsiani: Give us very detailed leg estimates and here are just some more. Shubham Tulsiani: results showing the predictions of rotated from 360 degrees and we again see that both the texture and shape are actually quite meaningful though. Shubham Tulsiani: Not perfect given the limited amount of supervision that you're using, and you're just someone zones across different categories like planes boats and hippos. Shubham Tulsiani: Yeah, so Shubham Tulsiani: Now would be a good time. If anyone has questions on this part of the work before I move on. Shubham Tulsiani: Pause for a few seconds. If someone has any questions. Shubham Tulsiani: Not okay i am i continue Shubham Tulsiani: So, Shubham Tulsiani: I did a very high level, what I talked about so far was just this sort of a system that given an image. Shubham Tulsiani: Could learn to predict some sort of a 3D model of what's under like that image. And what we saw was to be able to learn this 3D model. We didn't need ground truth supervision for the 3D models. Instead, we could just say that this model. Shubham Tulsiani: You know, when Shubham Tulsiani: When you to the lens of geometry or, you know, viewed through the lens of includes optics should give us some sort of predictions that is how does this Shubham Tulsiani: How should the image look and render from a particular view and we could just check whether that is consistent with respect to what we know. And in addition, some trials as the shapes across a category. Don't be arbitrarily could allow us to learn so David Bau: I actually this David bounce right i do have, I do have a question. David Bau: And I'm not gonna be able to stay here. I'll hold off by want to get it and before David Bau: I lost you. David Bau: So, so when you're developing at this very impressive methods. So you're showing David Bau: Some results on natural photographs. I'm just curious about your development process is you're trying to get your methods to work. David Bau: Do you do. Um, as you're developing these methods do you work a lot with synthetic images as well to to test here methods and get them to work first or do you just work directly with natural images and get them to work in that domain. Shubham Tulsiani: So I think this work this specific work that I talked about was actually the culmination of maybe three, four papers before it on the line. Shubham Tulsiani: So for this particular one we were already well familiar with sort of the data, etc. So, this we more or less tried in the real world cases, but the initial few papers. Shubham Tulsiani: Were done in very, very high settings. And in fact, not even 3D settings but just more to the settings where there's a square and you're observing a 1D image of it. Shubham Tulsiani: And so first, a lot of the methods are debugged in this very dry setting. And over the years. Then he became confident that these things actually do work and David Bau: Cool. Shubham Tulsiani: Putting suddenly it's more directly on the relevant data, but Shubham Tulsiani: That wasn't always the case to begin with. Shubham Tulsiani: Does that answer the question. Shubham Tulsiani: Okay, good. Shubham Tulsiani: Yeah, I'm happy to take more if there are any more questions. Shubham Tulsiani: Okay, if not Shubham Tulsiani: I guess. Moving on, and maybe get Shubham Tulsiani: Some more time for questions at the end. Again, Shubham Tulsiani: Yeah, so one view of what I just showed you so far was given an image, you could predict some sort of 3D models that are consistent with the laws of geometry as well as some trials that we have about how the world works. Shubham Tulsiani: And as I alluded to earlier this framework is applicable beyond just 3D reasoning, so Shubham Tulsiani: You know, instead of just falling Euclid laws of geometry. We could also follow Newton's laws of physics and predict some sort of physical 3D models. Shubham Tulsiani: That have some physical presence and are consistent with certain let's say Newtonian laws and Shubham Tulsiani: The as an example of this. I want to just talk about this project where we were trying to basically understand interaction videos so Shubham Tulsiani: Given a video, such as actually given many videos, such as this one, where in all of them. It's actually I think me who's interacting with this particular skill it Shubham Tulsiani: In different random this and to some degree, if we were just to label semantics for all of these videos, we might call at least half of them as lifting Shubham Tulsiani: But we really know that the interactions are actually different. We're affecting the object in different ways across these different clips in one Shubham Tulsiani: Sort of the way I'm holding them are the forces. I'm applying to these are really different. And so we wanted to have an approach that given sort of an interaction clip of a human. Shubham Tulsiani: Interacting with an object could understand that interaction at a physical level and sort of go completely. The goal is given an input video where we are depicting Shubham Tulsiani: Sort of interaction with unknown object. And when I say known. It's like, we know that it's this precise kill it with this precise 3D model and be aware that Shubham Tulsiani: And there's some it's even known mass distribution of the scale at which you're able to view. Shubham Tulsiani: So, given an input video depicting interaction with unknown object. We want to be able to infer for each time step, what forces were applied to that object. And wherever they applied at every step. Shubham Tulsiani: So, Shubham Tulsiani: This is a very challenging task for which supervision is extremely difficult to come by. And so what we did to address this task is plus to the collected data set. Shubham Tulsiani: Where we took objects from the YC data set. So these are data. These are objects with 3D scans that exists. So we know precisely the structure of each of these objects and Shubham Tulsiani: Sort of just me and I've interacted with these objects for a few hours, and we collected data set of interactions with these objects in diverse rent movies. Shubham Tulsiani: But our goal is to be able to infer the physical forces and contact points for each of these objects. Shubham Tulsiani: What was the force applied. At which location at each of the timestamps. So one possible way to collect annotation for this would be to just Shubham Tulsiani: Apply many, many false sensors on each object and somehow get the ground truth data for what forces were being applied to this object at each time. So, Shubham Tulsiani: Unfortunately, that's not really quite practical because for sensors, you know, are not very accurate and they would change the way you interact with the object because they're not very small as well so Shubham Tulsiani: Having four sentences would give us very precise measurements and it would make the interactions that we do much more unnatural as well. And so instead of requiring sort of ground truth for supervision, we instead realize that we could just use indirect supervision, so Shubham Tulsiani: Instead, the kind of data annotations that we got on this data set with these key point annotation. So for each object. Shubham Tulsiani: We manually defined a certain number of key points are, for example, about nine to 10 key points for the particular drill object that I'm showing here. Shubham Tulsiani: And got people on Amazon. Turn to annotate the key point locations, let's say, in every 10th frame of the interaction data set. Shubham Tulsiani: And what these key point annotations allowed us to do was they cover some sort of 3D object pause for each of these Shubham Tulsiani: time frames. So for every 10th timeframe in the interaction interaction video we have annotations for what the approximate object poses. Shubham Tulsiani: And environment. We also good annotations for read the human fingers touching this particular object. So for each of the fingers, there's some sort of color code that I'm showing. And we asked people to annotate which pixel Shubham Tulsiani: The finger is in contact with if they can predict it, and Shubham Tulsiani: By making an assumption that across the interaction video the contact points don't change and the object frame, you could actually live these 2D annotations to 3D print annotations by the structure from motion. Shubham Tulsiani: So each of these contact points we actually could recover in 3D. In the object coordinate frame by just assuming that they don't change significantly over the course of interaction. Let's maybe a limiting assumption. Shubham Tulsiani: But essentially, we can collect this data set we have approximate object poses and some sort of contact points for each of the clip and you can use this for learning. So, Shubham Tulsiani: At a very high level, what we have is given an input video, we learned a few prediction branches, one that predicts Shubham Tulsiani: The contact points in the frame of the object. So because it's known object we just predict the coordinates 3D coordinates of the contact points and use the annotation that we have a supervisory signal that the predicted contact point should match the annotated contract. Shubham Tulsiani: With remote interestingly would be also predict is a bird time step force that is applied at each of these contact points. And as I mentioned earlier, it's extremely difficult to get ground truth supervision for this course. Shubham Tulsiani: But what we can do is just say that the timestamp forces that be predicted when Shubham Tulsiani: given as input to physics simulator would give us a simulated state for the object at each time step. So given the predicted forces, you have an estimate of how we expect the object to change across time. Shubham Tulsiani: And we can just say that this expected change of the simulated change that is a result of these forces should be consistent with the image evidence that we have that so that Shubham Tulsiani: The key points that we expect as a result of assimilation better match the key points that we have annotated in the ground. Shubham Tulsiani: And again, note that this annotation will not be there on a buffering basis. This can be there at a very sparse level or sort of every 10th frame in our and you can still use the stream framework to learn first prediction without having grown to supervision. Shubham Tulsiani: Because what this request is only ground truth labels for these annotated to the key points as well as just the ground truth have entered in contact points and the ground two key points, but not really annotation for what precise forces building Shubham Tulsiani: And through one neat thing. I just like to note is when he started the project, we thought Shubham Tulsiani: Being able to do the differential physics simulator or set up a differential physics simulator that handles these Shubham Tulsiani: relatively complex missions would be challenging. But what we realized we could just do find a different thing with respect to Shubham Tulsiani: A standard simulator slip in this case by bullet and use that as some sort of differential physics simulation. And the reason we could do that. Was that the Shubham Tulsiani: state space was actually very limited. So the role of the simulator is to take in the current state and the current force and the contact point and predict the next time state. Shubham Tulsiani: And the state space actually extremely low dimensional because it's known object, the state space is simply sort of the rotation translation and the corresponding velocities Shubham Tulsiani: And so it's a pretty low dimensional system and given Rafi just 20 calls to the blender simulator, we could compute a gradient for this and actually sidestep the issue for how do we get very good differential simulations for this, which Shubham Tulsiani: Ones. Yeah. Wei-Chiu Ma: So just want to make sure and in this setting, you have the 3D model for these objects, right. Shubham Tulsiani: Yes, it's Wei-Chiu Ma: You have Shubham Tulsiani: It's actually not just 3D model one thing, maybe I'd like to notice we have the 3D model. And we also have some assumptions of the density of the 3D model, which actually may be inaccurate, but we have whatever is required to simulate this object. Shubham Tulsiani: Okay, look at me, sort of assume Shubham Tulsiani: Some fixed sort of fiction coefficients some fixed fixed density, etc. I mean, which are still questionable questionable assumptions. But given that it still I'd say interesting that we can learn some sort of meaningful force prediction. Shubham Tulsiani: And and just like to show some results. Xinchen Yan: Chabad another question to this interest. So another question. When you Xinchen Yan: Do the physics simulation you, what do you mentioned, it's basically complete the funnel difference Xinchen Yan: Yeah, you're good in using the pilot difference, but I just wonder how efficient it is to run the thing I bought it is a very efficient. Shubham Tulsiani: Yeah, it was actually surprisingly fast as him. Shubham Tulsiani: I'd go so far as to say that Shubham Tulsiani: The simulation wasn't really a bottleneck for us. Xinchen Yan: I see, I see. Okay. Shubham Tulsiani: Maybe that was because behind the machine with a lot of CPUs. But yeah, because it's only 20 calls to Shubham Tulsiani: The pilot simulator for each palladium step that we want, which is Shubham Tulsiani: Not terrible Xinchen Yan: Yeah, yeah. OK. Shubham Tulsiani: Cool. So yeah, here that maybe just some results, but on the left. Shubham Tulsiani: Input videos that we have. We didn't see during training. So these are the new interaction sequences of well either me or have been of interacting and in the middle you see that simulated trajectory that we get Shubham Tulsiani: As a result of the predicted forces and it's not precise, but it's actually quite accurate. And one thing I'd want to note is Shubham Tulsiani: He didn't really add any additional trials of what are good forces and that contact points should not change. Shubham Tulsiani: Sort of that forces at point should not change arbitrary. So, which is why the forces you see on the right are not very smooth. But maybe incorporating certain priors like this would help. Shubham Tulsiani: As well. But he didn't really want to Shubham Tulsiani: Go down that line yet. Shubham Tulsiani: But still sort of the simulator trajectories that we see quite accurately matched what we see in the video. Shubham Tulsiani: And one sort of result that was interesting to me specifically was that we could learn contact point prediction using just the annotations that we got from humans or he could also give the contact point prediction branch gradients through the simulator so Shubham Tulsiani: Sort of in the column in the middle, the way we get learning signal for contact points is only through the manual annotations. Shubham Tulsiani: And on the right, the way we get supervision for contact points or gradients to contact when prediction. Shubham Tulsiani: Is both through the sort of physical simulation branch, as well as the manual validation and using contact points in the simulator actually significantly improve the Shubham Tulsiani: Quality of the key points. And here I'm just showing that have some visualizations, but in the paper. We also have some more quantitative evaluation of this so Shubham Tulsiani: Being able to simulate through the predictions improve not just sort of allowed, not just force prediction, but also improve the fidelity of where we are holding the object because maybe Shubham Tulsiani: We needed to hold it till a plausible ways that some forces could be applied to the object. Shubham Tulsiani: And Shubham Tulsiani: If there are any more questions, I'd be happy to take on this one. Wei-Chiu Ma: And so I just want you to confirm. So for now, your physical inferences single frame based right. Is there any temporal smooth. Shubham Tulsiani: It's actually an ordinance of the importance of video and then we make a yeah I didn't really go into the details of the network, but it takes in a video and and for the timestamp forces by looking at the whole video Se Shubham Tulsiani: Okay, so I have maybe like eight minutes maybe skip a lot of the details. Next one. So what I talked about so far was Shubham Tulsiani: Actually, yeah. So when I talked about so far is how we could use the laws of physics explicitly. So we are predicting these forces that are applied to the object. Shubham Tulsiani: And we hard coded the equations of physics in the simulator itself to be able to learn this prediction. Shubham Tulsiani: Sometimes that's slightly difficult to do. For example, the objects are unknown. We can't exactly simulate them precisely Shubham Tulsiani: And so what instead of sort of using these explicit laws of physics we could learn implicit physics models that sort of make the prediction of what will happen as a result of our actions in maybe slightly more general scenarios where let's say the object is not known precisely Shubham Tulsiani: And these sort of models can let us still learn prediction from images. Why, also in some sense learning the effect Shubham Tulsiani: Of the actions with some parameters as opposed to requiring that everything be known and we I'll maybe skip a lot of the details here, but we just looked at this task. Shubham Tulsiani: sort of looked at this idea in this very simple setting where we want to plan for long horizon tasks. So there are just some blocks. Shubham Tulsiani: That are present in an image, and we want to take a sequence of actions, such that the configuration changes to something desirable and so each action. Shubham Tulsiani: Is very restricted that we are only allowed to push objects. So in this particular case. Shubham Tulsiani: We should be able to push the blue object around the red one without actually disturbing it so we need to realize that will be just, namely the blue block. Shubham Tulsiani: To the left, then it will sort of hit the red one and lead to something that's undesirable. So what we really need to do is move it around and not just push it to them. Shubham Tulsiani: And so it's really important in this case to have some mental model of how the world is and how the world will change as an actor, and I will maybe skip. Most of the details and just note the fact that Shubham Tulsiani: Sorry, just know the fact that we need a forward model. So if we have some forward models shown and sort of purple lines here. Shubham Tulsiani: That, given the current state of the world can predict the effect of actions in it, then you can just use the solid map model with classical tools like modeling predictive control. Shubham Tulsiani: To sort of plan for a long term action sequence. And so in this world. The focus was on how we can learn a good forward model. Shubham Tulsiani: For this particular scenario. And I will unfortunately not have time to go over a lot of the related work, except to maybe just highlight one thing, which is that the following model could actually also be learned from random interaction. So Shubham Tulsiani: Given Shubham Tulsiani: Lots of blocks on the table, we just let the robots that have interact with these blocks for two days. Shubham Tulsiani: Randomly push them around and this sort of naturally led to many interesting scenarios where things are near each other, pushing one block will push the other block and so on, but just randomly interacting for a couple of days. Shubham Tulsiani: Allowed us to get a lot of interesting data and then we could just learn this forward model in a very simple way that Shubham Tulsiani: Given the current image and an image that I know is the result of my action. I just want to follow the model. Shubham Tulsiani: Who's effects can explain the resulting sorts of, given the current image. Shubham Tulsiani: And my known action at time t. I want my forward model that I'm skipping one of the audience here to give me back an image that is accurate and is representative of what I actually observed Shubham Tulsiani: So just using these random interactions in a sensible way that here is my current image. My forward model should be consistent with the investments that I eventually see as a result of this action allows us to learn this predictively in some way and Shubham Tulsiani: I will skip a lot of the details, but maybe also just note that can also be don't need supervision for every particular timestamp, but we could just read currently apply this model. Shubham Tulsiani: And only have supervision, that is, let's say keyframes apart. So given a current image an image that as a result of a sequence of key actions as opposed to a single action. Shubham Tulsiani: also allows us to learn and that is actually nicer to preventive so rather high level we pick random action sequences and the supervise the following model to make meaningful predictions that are consistent with the resulting image of this action sequence. Shuang Li: I have one question. I'm showing me versus the actual or Randall is impossible to find the shortest path from one observation to another region or you don't care about Detroit, just from this day to the st Shubham Tulsiani: So they're just learning of forward model right so it's more like here, but the five actions that I took, and the result of this action sequence, the predicted results should match the actual result and it's fine of the actual sequence. I took was suboptimal so we're not really Shubham Tulsiani: So the forward model just needs to predict the result of each action, one at a time. It's only when we are searching later through this model predictive control framework that we care about optimize Shubham Tulsiani: The fall or more than itself doesn't care about whether the actions are optimal. And if that makes sense. Yeah. Shubham Tulsiani: Yeah, and Shubham Tulsiani: Again, this may be headed one desert here that for this particular case when we compared our approach to some prior pixel based forward modeling work, we see that it could actually make the blue block. Shubham Tulsiani: Go around the red block and Shubham Tulsiani: At the end, you'll see some more random actions because it sort of things it does achieve the goal. So it's trying to take an action that has no effect. But you can see that the following model we learned allowed us to sort of plan in this way. Chuang Gan: And I Shubham Tulsiani: skipped some orders over again but again just stepping taking a step back and showed all of these three Shubham Tulsiani: Results, which, you know, in some sense of tackling very different problems that are from deconstruction to making a robot interact with sort of blocks on the table, but I'd really like to highlight Shubham Tulsiani: That all of these Shubham Tulsiani: Essentially at the same exact same learn can ism that given an input elements. We are predicting some sort of models about the physical world and saying that they should be consistent. Shubham Tulsiani: Sort of the predictions that we get out from this model to really mad. Some sensor observations and incorporating set of laws of physics that connect the model to predictions really allows us to learn and bypass the need of supervision and Shubham Tulsiani: One last point I'd like to mention in a minute is Shubham Tulsiani: I really motivated this as trying to do perception that is useful for interaction. So having these sort of old models can allow us to act better Shubham Tulsiani: But sort of visit the way we were interacting to learn these words, what is for sort of random and then sort of an orthogonal question of how is it that we should add to get better sensor input to then learn better world. Shubham Tulsiani: Models and no one very popular answer in literature is to take actions that just maximize our prediction enter that Shubham Tulsiani: Make the world models be as wrong as possible and sort of take actions that are in this way, maximally informative. Shubham Tulsiani: And in some recent work to argue that that notion is maybe a Shubham Tulsiani: Bit sort of tool restrictive or Shubham Tulsiani: It's extremely difficult to learn good word model. So you will always find interesting actions sort of Shubham Tulsiani: You'll always find actions that give you high prediction error. Shubham Tulsiani: So we've looked at. Shubham Tulsiani: A couple of different ways of defining interesting actions. For example, if you have two hands. Then take actions when the effect of the sequence of actions is different from Shubham Tulsiani: Doing the actions together or, for example. Shubham Tulsiani: If we have access to multiple modalities then really prefer actions that lead to normal multimodal associations that if you see an image and a sound that you've never seen the combination of not is something interesting to do. Shubham Tulsiani: With that, I'd like to Conclude Shubham Tulsiani: We're happy to take any more questions. Wei-Chiu Ma: Any questions. Xinchen Yan: No question. So I think this is very nice talk. So I think in the first part you you show some maybe nice results like reconstructing the 3D shape of maybe animals. Xinchen Yan: From, like, a single image. I think one one claim you mentioning the target. So you want to make sure that the Xinchen Yan: Maybe the reconstructive reconstructed so you shave. Are you for for visual useful for robotics Division I just wonder Xinchen Yan: Do you have any like Xinchen Yan: Oliver ideas like how, how can we make sure that these like sweetie animal for useful for robotics application. Xinchen Yan: Because I think Xinchen Yan: I think for 3D like it like a shape that objects. I think straightforward, we can. This can be useful for navigation and manipulation tasks, but for animals because they are like actually agents. I just wonder you have any for Shubham Tulsiani: Ya know, so I think that's a great point. I don't have a direct answer for you know how precisely the animals that you are constructing useful, but my main Shubham Tulsiani: Goal. There was to get a method that can let us reconstruct maybe through all kinds of the categories. So what we started with was, you know, we should be able to really reconstruct any category image. Shubham Tulsiani: And sort of the animals and actually we also show some origin categories that we're just sort of examples that this approach can handle general categories. Shubham Tulsiani: Yeah, we don't really have rewards that right now, go out in the wild and interact with animals. But, you know, maybe just to construct a use case. Shubham Tulsiani: You know, if there's figuring out whether the line is running towards you are not in so even also learning pose in France. In that case, for example. Shubham Tulsiani: Would be produced, but but maybe just taking a step back. I think the aim of the project really is to enable reconstruction across generic categories and that I definitely think will be useful, especially if you look at a few more indoor categories. Xinchen Yan: Yeah, thanks. Thanks. Wei-Chiu Ma: I guess I have one also a bit. I'm kind of high level question because like I think she bomb you're mentioning that you are arguing that it's important to do perception for interaction, right, not just perception only Wei-Chiu Ma: Seems like currently the work that you presented with all of them you define the road model manually. Wei-Chiu Ma: For instance, you define that. And you want to know the speed or whatever. So have you tried like I'm trying to hook the whole system like end to end, and then using a downstream cast to survive and see if there's any difference on the road model. Yeah. Shubham Tulsiani: So that is something I to constantly keep sort of thinking about and going back and forth on because in some sense the into anything that you mentioned is actually the way if we have a lot of interaction data for one specific tasks to do so for example, if you look at any deeper Shubham Tulsiani: Than that is precisely how they do it. That Shubham Tulsiani: Both perception and interaction. I learned into and without any meaningful modularity. Shubham Tulsiani: I don't necessarily think that approach is very scalable, or if we want systems that can do a broad set of tasks, then that is the way to go. So to me, actually, it's extremely important that Shubham Tulsiani: When you think of perception, we put in the right kind of biases that, you know, we understand that the interaction will happen with the physical world. Shubham Tulsiani: So the role of perception, to some degree, is to represent the physical reality and sort of that has been one of the guiding principles for the kinds of things we pursue Shubham Tulsiani: And I think you're right that if there's a specific task for which we can interact sort of infinitely then we might not need the separation, but at least my infusion is that if you want to tackle many different tasks or a Shubham Tulsiani: Common system that can tackle many different tasks, then Shubham Tulsiani: I think this demarcation Shubham Tulsiani: Okay. Wei-Chiu Ma: Does anyone has more questions. Wei-Chiu Ma: Okay, then that's thanks Siobhan again. Yeah, thank you so much for giving such an interesting talk. Shubham Tulsiani: Yeah. Wei-Chiu Ma: Let me give you a clip on the zoom Chuang Gan: Good. Wei-Chiu Ma: Thank you so much. Yeah. Wei-Chiu Ma: So I guess I'm in a spreadsheet that chubby share with you. There'll be like the zoom link and Wei-Chiu Ma: I think English will be waiting for you. Okay. Wei-Chiu Ma: Yeah. Yunzhu Li: They're using to Syria. Another Lu Lu 