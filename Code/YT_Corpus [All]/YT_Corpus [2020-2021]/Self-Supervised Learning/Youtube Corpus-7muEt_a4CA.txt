 Hello, and welcome to the second episode of AWS DeepComposer: Train it again, Maestro. Today, we're going to learn about machine learning and how it powers AWS DeepComposer. I'll introduce you to common machine learning terms and the machine learning life cycle. Once you have a basic understanding of machine learning, we'll introduce generative AI and generative adversarial networks. We'll wrap up by going under the hood of AWS deepcomposer to look at the AWS architecture that makes music generation possible. What is machine learning? Most of us understand the process to build an app. We use programming languages like Java, Python, Javascript, C sharp, et cetera, to write code, to tell the computer what to do. Machine learning is different than machine or computer can produce output without being explicitly programmed. So here's how it works. A learning algorithm, studies data looking for trends and patterns in the data. This process is called training and it's how the developer teaches the machine. There are several ways that machines learn from data. We call these techniques. So there are different techniques like supervised learning, where machines learn from data that are already labeled with the answer you're trying to predict. There's also reinforcement learning where feedback is given to the machine during training. This is used a lot in robotics. If you've ever played around with AWS deepracer, then you've experienced reinforcement learning. There's unsupervised learning where the data is not already labeled and the machine has to determine the relationship. There are several flavors of machine learning. You'll hear a lot about classification and segmentation. This is where the machine tries to classify the data into groups. This is useful for segmenting and understanding your customers or even detecting fraud. The next flavor of unsupervised learning is where the machine doesn't just try to find patterns. It tries to mimic them. The ability to mimic something is at the heart of generative AI. And the expectation is that the mimic produced is indistinguishable from the real thing. Now we are not talking about an exact replica here, but something totally new. This gives the computer something it never had before creativity. So you can see why this advancement is so exciting. And our use of generative AI, the machine finds the pattern distributions in music to generate new music. After the learning algorithm has finished studying the data during training, the trends and pattern distributions it finds in the data are then stored in a mathematical model. This mathematical model is simply referred to as the model. So when you hear data scientists talking about the model, that's what they're referring to it. Container for the trends and patterns found in data. Now, once you have this model, it can be queried or consulted by giving it new data that it hasn't seen before, which causes the model to provide you with output. The output comes in the form of an answer or a prediction, or in our case, a brand new original song based on the data, the melody and the music genre that we provide the process to create this model does have additional pre and post training steps. When you put all of those steps together, you have what's called the machine learning life cycle. The multistep process to create a model. The first step is to obtain and prepare the data. Next, you train your model. Then you evaluate your model to make sure that it's performing well. Next you productionize the model. And this is where you put it in an environment where it can be used. And lastly, you actually use the model or generate what's called an inference when working with machine learning, there's the training process to produce the model. And then there's the inference process where the model is used to do something like obtain a prediction or in our case generate a new song based on a melody. We provide in our music genre model. The model is trained using music samples from that genre, we then send our melody to that model. And the models output is a new original song with accompanying instruments. This is a good example of inference. Generative AI is used during the training process to produce the music genre models. So the jazz, pop, symphony and rock models that will be used to add the accompanying instruments to our melody to form a new musical track. You've probably seen generative AI in use before AWS deepcomposer. You may have seen it in image generation, video generation or voice generation. How do you think all of these deep fakes are made? Deep fakes are manipulated videos or images produced by AI that produce fabricated images or mimics of a real person. Have you seen those time lapse images of what a person could look like 10 years or even 20 years from now? Generative AI is able to generate photos that are so realistic humans can't tell the difference. So how does generative AI work? Generative AI uses the generative adversarial networks - GANs architecture, which is an algorithmic architecture that allows you to generate new data samples based on the dataset fed to it. Before I deep dive on the GANs architecture, you might be asking yourself, so what is a neural network? A neural network simply referred to as the network is used during the training process to produce the model, neural networks have multiple layers of learning and are modeled after neurons in the human brain. Neurons are cells that transmit information for data to other cells. There are multiple layers of learning between the original input data and the final output. In our case, a new musical track. Let's dive deep on GANs. I mentioned that GANs architecture uses two neural networks. It actually uses two convolutional neural networks - CNNs. CNNs are neural networks that are often used with the image data represented as a matrix. In our example, the melody can be represented as a matrix or 2D patterns in the dimensions of note in time. So CNNs work well with music, but why do we need two networks? Two networks are used so that they can be pitted against each other during training. Thus the term adversarial. The networks at first work against each other, but then given enough iterations or ethics or passes over the data they stabilize and come to a common understanding. The common understanding is called convergence. This common understanding actually produces music that is pleasing to the air. The two networks each have a role in this process. One network acts as the generator, the one that generates the music and the other network acts as the discriminator. The one that is evaluating what the generator produces, think about it like this. You have a generator who is like a counterfeiter responsible for producing fake money. And you have the discriminator who is like the bank teller responsible for allowing legitimate money to be deposited into a bank account and rejected. If the money is fake, the counterfeiter must succeed at generating fake money. That is indistinguishable from genuine money. And the expert must succeed at telling a real from a fake. The discriminator gives feedback to the generator, which in turn helps it produce a better fake. The feedback the discriminator provides to the generator is based on what's called a loss function. There are two loss functions, one for the discriminator and one for the generator, the two loss functions measure how close the sound of the music comes to the desired output or a product that closely mimics real music. We'd like the loss function to eventually converge to zero, which means the output is close to the real thing, but getting all the way to zero rarely happens. Just know that a higher value from the loss function means that the outputs are deviating significantly from the original musical sound. So let's look at the data flow in this diagram, you have the generator, the model that is used to generate new music based on the input, melody, you then have the discriminator, the model that is used to classify music as real or fake. The output of the discriminator goes into the loss function and feedback is given to the generator, which helps the generator improve. When the discriminator successfully identifies real and fake music samples, it is rewarded. So no change needed to the model parameters. The penalty in this case is that major updates are made to the generator's model parameters. Now, when the generator fools, the discriminator, it is rewarded or no changes needed to the model parameters. When it doesn't the penalties that the discriminator is penalized and its model parameters are updated. This process is repeated over and over again, maybe thousands of times until convergence. The final result of the training process is a final GAN model that can be used to generate an original music track based on a given genre when given a new melody in our case, the final GAN model is the generator. The generator will be used to create a new song by adding a company instruments to a melody. The discriminator model is discarded. Let's peek under the hood at the AWS architecture used to train this model. In this diagram first, the user selects to train a model from the AWS console. This launches the training job by selecting the various hyper-parameters and dataset filtering tags, then calls API gateway, which calls the Lambda, which then writes a request and its status to dynamo DB. This then triggers a Lambda function to start the training workflow, which uses step functions to launch the training job on Sage maker, the step functions, monitor the status and add status updates in dynamo DB, the deepcomposer console polls, the backend and displays updates on the console. Once we have the final model, we can use it to generate a new song based on the melody. This step is called inference where we actually use the model to do what it was trained to do. The model takes the original soundtrack and adds the accompanying instruments in this diagram. You'll notice that we're using the common building blocks of the AWS ecosystem, API gateway, Lambda, dynamo DB, and Amazon Sage maker. There's even an integration with SoundCloud. Let's look at the flow. First, the input melody is entered into the console. Then the deep composer console calls to API gateway to invoke the controlling execution Lambda, the controlling Lambda inserts records in dynamo DB, and in parallel calls out to Sage maker. Sage maker hosts the music genre model. We conduct an inference by passing in the input, melody to the genre model. The genre model generates a composition and returns that back, which is then available for listening in the AWS deep composer console. The inference container is running on an EC2 C5, which supports processing intensive workloads for machine learning and even deep learning. Well, that's it for this episode, we've learned more about the AI that powers AWS deep composer. We've learned about GANs, the architecture that uses a generator and a discriminator to produce a model that can generate a new song. Join me in the next episode where we will get hands on with generating your first original track using one of the preloaded genre models on the AWS deep composer console. 