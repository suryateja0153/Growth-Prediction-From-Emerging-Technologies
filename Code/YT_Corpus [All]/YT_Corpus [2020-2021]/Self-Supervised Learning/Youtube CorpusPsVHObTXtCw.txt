 hi everyone i'm baihan ling from columbia university i'm presenting my work online semi-supervised learning in contextual bandits with episodic reward in the online learning setting the agent or the contextual bandit usually receives a context and makes a decision based on it then an expert or user would give a feedback to the agent as a reward which will then be used to update the agent the bandit feedback can either be a positive reward or a negative reward however more often than usual in real life the feedback could be missing in another word the rewards usually comes in episodes of different frequencies this is especially true in many real world applications for instance in a recommendation system the user might be sticking on a product page for a long time because he or she is away from the computer and this inactivity shouldn't be treated as negative feedback to this product in personalized medicine or longitudinal studies some patients might stop showing up for follow-up checkup or drop out of the study these missing data points shouldn't be treated as negative data point as if they passed away or otherwise in many interactive systems these days the system learns continually through interacting with the users however when the users are not around given feedback the system shouldn't interpret it as a positive or negative signal in on and off seasons the feedback to certain advertisements will come in different frequencies but that shouldn't be treated the same way as if some ads are more popular than the others we can formulate this problem as the contextual bandits with episodic reward for each round the agent makes a choice given the information from a context x however the reward feedback r for its chosen action is only revealed in an episodic fashion in this work we model the episodic reward as the probability p from 0 to 1 suggesting the probability this feedback being revealed to the agent to solve this problem we propose the solution based on the standard linear upper confidence bound algorithm which we call background episodically rewarded linear ucb or berlin ucb the major difference between berlin ucb and the linear ucb is at the episodes where no feedback is given we use two strategies in such a case we either only updates the covariance matrices or we create a pseudo-reward from some sort of self-supervision mechanism which enables us to update the agent as if there is a reward under the proximity assumption we chose our self-sufficient modules to be three online clustering algorithm gaussian mixture model k-means and k-nearest neighbors we empirically evaluate the performance of our algorithms and the standard linear ucb algorithm in the online classification task with banded feedback we chose mnist and warframe as our data set and generated six synthetic non-stationary environments we created non-stationary contacts either by varying cluster distribution over different episodes or inversing the images into negative images we create non-stationary reward either by shuffling class labels over different episodes or switching between mnist and wall frame classification tasks over different episodes as a multi-task environment we also consider the case where the number of arms is fixed and known from an oracle the alternative would be the contextual bandit doesn't know the number of arms and it extends its arm during the learning when facing new classes lastly we vary the frequencies of the reward prevailing over different episodes shown here are the average classification accuracy in this experiment it is important to note that the classification with banded feedback is far more challenging than the traditional classification problem because the bandit feedback only reveals whether the prediction is correct but it doesn't tell the agent which arm is in fact the right arm we see that both the stationary case and the case of non-stationary articles our methods demonstrated a clear advantage over the baseline even in the extreme case where only one percent of the reward is revealed in the scenarios of the non-stationary context and rewards we still observed an overall benefit of our proposed method over the baseline we also provided a proof of concept in an application of online speaker recognition task we consider this interactive system that receives a continuous stream of audio input from a conversation of multiple people the system would output in real time the name of the person who is currently speaking andy learns from the user that tells it whether it makes the correct prediction or not however as we can see the reward is very sporadic because the input stream and the prediction are continuously provided without interruption but our user won't be there providing feedback 24 7. we evaluated on the minivox benchmark and demonstrated robust performance over the baseline if you are interested in this specific application feel free to check out the extended version of this application reference below in conclusion we proposed an oval problem setting inspired from practical online learning application called contextual bandit with episodic reward we consider it as online semi-supervised learning problem and proposed a noble solution that introduced self-supervision to provide pseudo feedbacks to the agent in episodes without feedback to evaluate the performance of our algorithm in this setting we introduced an oval non-stationary benchmark that includes six types of synthetic stationarities in context reward and oracle from the experiments we observed that updating the representation structure when no reward is revealed improved performance of contextual bandits adaptive learning with context-dependent clustering modules is much better than learning without self-supervision ongoing directions include improving the self-supervision modules for instance with graph method incorporating the online clustering module into the online learning problem itself providing the theoretical work of this problem and extending it to more applications thank you for your interest if you have any question feel free to reach out to me you can also check out several of our related work in this direction which are referenced here the full paper can be accessed on archive and the codes are available at our github repository thank you 