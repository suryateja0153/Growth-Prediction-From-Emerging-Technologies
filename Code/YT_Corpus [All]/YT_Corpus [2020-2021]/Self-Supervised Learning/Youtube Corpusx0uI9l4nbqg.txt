 - Waiting in the wings are our next two rockstars and, this one is all, it's called making use of all your data, labelled or otherwise. Jessie Wang, Brad Nguyen, now I'm going to classify these two as hard working, touring musicians on our great concept today. They are polished by long experience in the field and like a great touring band, the sum of the two is even greater than the individuals and I'm going to call, and everyone's going to be, all you millennials are going to be rushing to Google because I'm going to compare you two to the Eurythmics. Now the Eurythmics was a great band from my era and actually you know what, I think we might sneakily, Fiona I don't know if you'll forgive me, give away a book to who on the channel, in this talks channel, can tell me which Australian town the Eurythmics were founded in. That's a really obscure piece of music trivia. First up to get that will win the book. Brad is senior consultant here at Thought Works. Recent experiences included data strategy advisory work, he has got the hottest letters in town, CD for ML. So, continuous delivery from machine learning. Oof, that is so hot right now. Does that kind of work? Chat bot building. Machine learning proofs of concept, that's a very, very sexy sort of resume. Pockets of experience in other old technologies like web apps, Micro Surfaces and app development, dev opps, oh, so last Thursday but, pride of ThoughtWorks, he earned his stripes as a game developer for EA Games, data science role in all sorts of places. Sportsbet, red bubble, amazing. Jessie, a talented data consultant at ThoughtWorks here. Another person in hot demand. Passionate about optimization and operational research so making it real. Machine learning, all those area. She's been both an academic and an industry assistant researcher at the University of Melbourne and solving real world problems for the local council, bottlenecks of transport. Well, they're not out there at the moment. Jessie said we're pretty glad to have you instead. Work for all sort of our clients. They've been wonderfully lucky to have to and today, you're going to talk about supervised learning, the problem starting with a labelled data set. So even I know what that's about. It's like data comes in all shapes and sizes. I think, Matt Kelsey on our podcast, pressed two, talked about poor old data engineers and scientists are probably going to spend at least half their time just looking for the data, let alone whether it's well sorted or well labelled so, I'm going to let you talk about that today. You're both experts and I thank you so much for coming and, we'll see everyone else over in the channel. - Thank you Nigel for the introduction. So as Nigel mentioned, Brad and I will talk about how to make use of all your data, no matter if it is labelled or otherwise. So, so before we go to introduction, I wanted to make it clear that in order to 100% understand the context, you may need some single-background in machine learning and you may be familiar with term like loss function and labels. So now, we will talk about our motivations. There are three reasons motivate us to do this talk and they're all based around our previous experience. So in our previous work, we normally have a very large amount of unlabeled data that only have a little bit of label data. And if we build a model only based on this label data and then it's kind of like a waste of the labelled data. So it raises the question like, can we make use of both limited data and unlabeled data to build a machine learning model that is actually functional and working pretty well. And, Yeah, and then the second motivation is, is link it back to the label data. We want to reduce annotators time. So because every label data, we need two men to actually label them and this involves label cost. So it's a way to reduce and reduce time as in the same amount of time, we can label more data or, we always label the more important data point first. And then, the last one is, when we're working with different clients, things change rapidly. They always have new products onboarding. Is there a way the model can quickly recognise a new product, and then digest it and then, in the future, we can actually use the model to recognise them. So that's the three motivations. We have, now I will pass to Brad to talk about our approaches. - Yep. Thank you Jessie. So, here, I would like to give a very quick explainer, very high level overview of the whole landscape of machine learning so we can follow this talk. So we would start from the top row there. This is the, where traditionally, people look at in machine learning, where you look at supervised learning, so that's the case where you have on the data and you tell the model what to expect to classify or predict, so that's okay we have the labels. And if you look at the other stream of it, if you look into unsupervised learning, so that's the bottom row there, where you only have the data, you don't have any labels. So in that case, normally people can only do either abnormal detection or you can group together similar things. That's the only thing you could do. But, if you pay attention to the middle area there, it's what we think, we often see a lot in the real world, so in fact, that's probably the most common in the real world, where you have very limited label data, just a few label data because we know labelling data is quite a human-intensive process and you have lots of unlabeled data that you can also learn from. So, what are the ways that we can actually combine the unlabeled data and then you add a little bit of label data to actually give you the same power as the supervised learning model. And... Yep. So for the next slide, I'm going to just very quickly introduce our running example for this talk. Which we'll use throughout our presentation. The data set we are going to demonstrate here is the fruit increasing the data set. And, they are really just that there, really just a bunch of images of fruits. They are to pass that off so in the first graph, we are not going to use any label at all, so the only label we use are the raw images of those things. We don't even know the names of the things so, we only see the image of the fruit. In the second about, Jessie will talk about free shop learning where she use a little bit of the label. So, basically you only, you also tell whether it's an apple or whether it's an orange. But with only few of them and then the rest is still unlabeled. All right. - Yep and, yes. And now when we, at the start we want to, tell you what we want to achieve. So here we want to, at the end, to build a classification model. So if we put an image into the model, the model can see what's this image. So in the first example, if we put a green roundish thing into model and then the model will say a-ha, it's apple and if we put a red sparkly-ish thing into the model and then the model may say uh, it's not an apple. So that's what we want to achieve at the end. - Right. So that's the goal, what we want to achieve. So how do we get there? So in this case, I'm going to give, like a real of how our top is structure so you can follow. So it's divided into two parts. In part one, we are talking about this thing called representational learning. With totally unsupervised so, we do not use any labels at all. We just look at a bunch of raw images and we want to train a thing called embeddings model. So, this is really just a way to represent things. And in the second bit, we talk about official learning where you use that embeddings model and then you can apply to train a dash screen model within this case it's the apples versus non apple or apples versus lychee as Jessie mentioned. So that's an overview. Right, so, next, I'm going to go into a little bit of detail now to talk about representational learning. So, just to recall, so the goal of this step is to be able to learn to represent different things, right? And it's unsupervised learning so we do not use any labels at all. The only input we receive is just a bunch of raw images with no thing. I wouldn't know whether this one is banana, or that one is a strawberry. I wouldn't know that information. I only know these are a bunch of different fruits and my goal is to be able to tell that if I see two bananas, I want to be able to know that these are the same thing. If I wanted to, if I'm shown a banana and a strawberry, then the model should tell me that these are two different things, right? And you may ask, how does it actually work? But if you look at the way that babies learn for example, if you have babies in your family or in your extended family, that's actually how they learn things because they can tell whether a cat is different from a dog without even before learning those words, right? Or you show them another cat, they would probably know there's still a cat without even knowing the word cat, so that's not how they learn. You just look at the data first and you try to, derive some features to be able to represent them. So that's the goal of this step. So if babies can learn that, then the machine learning can also do that. We'll show you how. Right. So, this is one example of how you can train a model without telling it what it is. In this case, the input is an image of the banana. I wouldn't even tell it it's a banana, right? So you can, from that same banana, you can run it into two different augmentation functions. And it's really just a rotation of the image here, so that's one example. And we basically arrive to that point now. So those are the two data points of the same object. So if you feed them through the same neuro network, with the encoder, and at the end of it, you try to ask the model to try to maximise the similarity because they are really the same thing. So you try to teach the model to keep learning until they are really similar. Then, what happens is, if you extract the last layer here before the projection, is what they call representation. So what's in this representation is really a set of, you know, features that the model has learned that allows us to recognise that those two are really similar things. That could be the colour, that could be the shape, or the contour of the images that allows them to learn this are really similar, right? So that's really just a pair-wise way that you can learn unsupervised, so, you have noticed that we haven't even tell them what it is really just to give them an image. That is one way to do it. Now, back to our fruit example, we probably can't even do it better now. Not only we ask them to recognise similar things, we can also ask them to contrast things, right? I would like to introduce three innovations here. So the anchor, the anchor could be any object. The positive is something really similar to the anchor and in this case, you probably could guess that we probably just take the same anchor, we rotate them or augment them, just a little bit. That's the positive and if you take a random selection outside of the anchor, you probably get something else entirely different. That's a negative, right? So the goal here is, now this treating is called a triplet and it's one data point that we want to ask the model to learn that well, I want you to try to learn that. The two apples are really similar and the apple and the lychee is really different. So, in terms of the machine learning model, how would you do that, right? The core concept of any machine learning model is a loss function. So if you have known machine learning, you would know that at the core, is a loss function. Let the machine learning try to minimise, right? So I'm just going to pause for a very quick, just two seconds here, just so everyone can take a very quick look at the loss function. Right, and then we'll just try to explain what it is. How do you write a function that the machine learning can really try to separate the lychee and the apple. So, if you look at the notation, a stand for anchor, b stand for positive and n stand for negative. So it's really, just try to take the difference between the, it's L2 num of the distance between the positive to the anchor and the... the distance from the negative to the anchor. So one way for me to understand this formula is, right, you mentioned that building the training of the model, so what we want to achieve is the distance of the negative, should be a lot larger than the distance of the positive to the anchor, right? So if during the training, if for some reason the positive distance is still larger than the negative distance, if you look at the formula, what will happen is, that subjection is still greater than zero. So in that case, the loss still can be minimised and the model still have to keep updating. If the distance of the negative and the positive is the same then the formula will become the alpha which is the margin here. Enough case is still a positive term so the model will still have to keep updating. And it probably stop when the distance of the negative to the anchor is exactly greater than the distance of the positive to the anchor by the amount alpha. So that's the way it actually goes up and that's when you can tell that the negatives at least separated by the positive distance by alpha. So, this talk, like, we wouldn't be able to go into a lot of details into this function. So just, play back one, the direction out there for the audience if you want to understand this really beautiful formula, if you want to understand more about it. Think about the role, for example alpha, what happened if you removed alpha from this formula? What do you think will happen to the model? You can either try to experiment, run a thought experiment and let us know what do you think is the role of that alpha here. So in the next case, we talk about this, now, the quality of the model is as good as the quality of the data that you feed to the model. And how do we at least sample the data so that we can train the model? In this case, we show you how we actually select the example that you can train the model. The columns, the columns they add the data points to the model. So, if you look at the first column, for example, that's really just one combination of the anchor. The positive and the negative and the anchor and the positive again, they are really variation of the same image and the negative is something different else. So, a lot of work has been done on actually just how to select the data so you give the model the best shot of training and converging in faster time as possible. For example, if you just give a very obvious example, so that's very useful for the model at the start but if you keep feeding it very obvious example, it would not learn any new or useful information. See in that case, people have to really focus on the thing they can't have, negative example. So those are really, you know, tight and very difficult to classify and then the model can begin to pay attention more to how to better separate those example, right. So again this is not the forecast of this talk that we just give you some overview of how people can actually do online mining of these new sampling to train the model. Right. And, at this point, we fully train the model with annual labels. We get embeddings and a lot of people actually ask, at this point is how do we know if its working because it's so abstract. It's not, you know, easy to see, right? So one way is, remember that embedding is in indimensional space, so they are a vector, in indimension you could be in space, right? So one way you can do that is to project them now into three dimension, which is what we are doing in this case. The coordinates given in this visualisation is given by the model and the colours is the actual labels. So we cheat a little bit, we use actual label here to then check the model to know whether it's working or not. So in this case, if you look at the visualisation, you will see that, like, the thing, with the same colours, they tend to group together so the model is telling us that, that based on the model, things that are actually the same thing, actually consider to be really close by the model. That's one way you can really then check embeddings. So next I'm going to hand to Jessie. She will talk more about with the embeddings that we train, how do you actually take a product to train something else. - And now, we will take a step further from embedding to official learning. So we will start from higher level concept. What is official learning? So, in a lame term, it's actually just a classification problem, but, we have n different classes and then in each class, we only have m labelled data. But, having said that, they may have many many unlimited but we don't have m labelled data. So here is an example. We have three different classes and each of the class, we only have one example and then we use these to train the model and then once the model is trained, if we give another input, and then the model will say, oh, this input is likely to be the first one which is like an apple, green apple, and the official learning is widely used in different areas. For example here in m class, image classification is also used in segmentation and mutual language processing. But in this talk, we will only focus on image classification. Just a quick recap. Brad already introduced what is embedding, now just a quick recap. So in embedding, one way to train the embedding model is we randomly take two examples from our database. I take the first one as our anchor and then we do some augmentation to create our positive example. So, just emphasise that m, the anchor and the positive apps, they are actually created from the same example and then we take our second example as our negative and if the model is well-trained, then the model will map the image to the interdimensional array, and then we can see this indimensional array as indifferent features. And if we plot these three indimensional array into indimensional space, what would we see? We would want to see that this tense between anchor and the positive is actually closer than the distance between anchor and negative. So this embedding model. Now you may ask, what does embedding model to do with official learning? So one way is we can use the output of embedding model as fishers to train a classification model. So what I mean here is, if I have a image and I have a very trained embedding model, I put the image into the model and the model will output as an indimensional array. And then we use this indimensional array as indifferent features of this image to train our classification model. Basically use map and this an indimensional array as features into a class. So, why this is helpful? Because, normally training an embedding model is a much, much heavier and complicated work compared to the classification model. Why? Because it involves a lot of parameters or weights. For example here, this is already a super simple model for embedding. Suppose it was set up for embedding model but it already have so much parameters. So each line over here cannot reason as one parameter or waves, and then compared to the classification model, we only have a few and if we can make use of embedding, take it as is, what do we need to adjust? We only need to adjust this parameters which is a much simpler work. So it can save a lot of time and effort and how does that work in the whole flow is we put image in, so this example is an apple, and then the embedding will map to indimensional array and then classification model will keep this indimensional array, map it to our class, which is the apple and if we have a different instance, same process. Embedding model texts to indimensional array and classification model and can embedding much, indimensional array to the class which is not apple, this way. And then there is another way to do a few shot learning is, we still train a classifiction model end to end. But, instead of starting from random parameters, we start from the parameters of embedding model. What do I mean is, so, this square in the this square. So this is the embedding, they have a lot of parameters. Normally, if we want to train a model from end to end, all these parameters is generally start from random. So is random ways. But instead of from random, we can start from using the embedding model's parameters. Because it's already trained so it's already have some idea about how the parameters should behave. So it means when we train end to end, it saves a lot of work. But, this way compared to the previous way we introduced is still more heavier, more complicated and more work to do, but the good part is, in this way, the output model will be more close, more tightly fit into our specific problem. So in this way, if we have an apple, it will instead of map the indimensional array it was eventually mapped to the class, which is apple. And if we have a lychee, we take input in the end to end classification model, it will directly output to a not apple. So that's the two ways of doing official learning model. Now, as of now we introduced some techniques about semi-supervised learning. And, and next we will talk about a little bit about machine learning of opps and, operational cousins. So, first we want to link back to our motivation. One motivation we had, the first one was, we wanted to make use of our limited data and unlimited data which you already done. We know how to make use of them and then build a good enough model and then the second one is we want to reduce the annotators time. How can we do that? Easy, because, once we have the model already, instead of, as annotator, instead of go through each individual unlabeled data point and then label them, what we can do is, we take a bunch of unlabeled data and then put into the model and the model will, different classification say this is an apple or not? And then we take all the apples and then show it to annotator at one go. Say, oh, these are all the apples classified by the machine. And then, as annotator I was like mmm, this one and this one does not look like apple. So I just circle it and say, these two are mis-classed there. So in this process, I actually labelled multiple examples as one go. So, in other words, in the same amount of time, I am able to, as annotator, am able to label more data so we can reduce our labelling cost. - Yep, so next I'm going to talk about some of the practical considerations that we reflect on when we, these some actual project with in this area. So, the first thing like Jessie mentioned, ML Ops is certainly one of the most, it's probably the most important things when you set up your ML project for success. So what we mean by that is, Jessie has mentioned that you need to iterate a lot of the data and the model. And the question we ask ourself is, how do we measure them can do that is, low cost as possible so, the fact that you can continuously develop a chain of data, retrain the model again, get a matrix and then monitor it, it should be able to go through that cycle in a matter of minutes, not hours or even days, right? So you can set that to happen them, you have a lot of chance to iterate a lot more times on the model. It's not the focus of our talk as well but there's another great talk coming up at 2:00, 3:00 pm today by Henny and Kiro. So if you would like to hear about ML Ops, please head to that talk and, quite specific to, unsupervised learning. So just want to raise some of the issue that we have seen. So, what happens when you've seen you're like, new unseen data, model drift, and those things only happen with ME machine learning models anyway. But, if you pay attention to our talk, they actually, is probably much hard doing in our case because they are two different components so you got the embeddings that you train in the unsupervised manner and you got the final classifier model that you want to train in a supervised manner. So how do we be couple to two components? Because if you consider about, let's say if you have the classifier model has been deployed to hundreds of different operations, operational stores and now if you only update the embedding, you probably do not want to deploy everything again. That's probably a lot of our time and a lot of cost involved in that, right? So, in the same paper they introduce tripping loss. They also talk about things like how many data links, the fact that when you train the embeddings, you do not want to just to everything from scratch again. But rather for example, you could mix some of the, when you sample the data to train the model, you could re-use the embeddings from the previous embeddings. And, you mix some example of that into the training in that way you actually got a better chance to have a compatible new embeddings so you can still re-use the same classifier, even with the new embeddings. So I think those are the few concern that we, we think it's quite important. And the other one is obviously how you, you know, do mining, whether you do online mining or offline mining to seek out the sample to train the data. Where should we focus on the hack negative, my example, or the semi-hundred examples, those are really important consideration to train in official learning and representational learning. - Yep, and so last thing we want to say is, iterative improving, improving, as Brad mentioned. Now we can close the feedback loop. We have a healthy, iterative improving cycle. So, what do we normally have in real life is we normally have some data asset and then we want to have some, we want to have something in production. So, in this case, we start with some, some limited data and a large amount of unlimited data. We can build a model to recognise our product. Now, this model at the start, may not be very good. It may not perform very well but it's okay. We can still put it into production, maybe in your hidden ways of outcastment, will not see it, so it will not effect our customer experience. But, if it's in production, it can start to generate more data and then it will grow our data asset. So we will have more and more unlabeled data. And then if we use the smudge, annotated way to label more data, it means our label data set will also grow. In, once we have more unlabeled data, more label data, that means our model will have better performance. So our performance is improving as well. So in this way, once our, when we, go through this cycle multiple times, our model is in a good shape, we will be happy to release it and actually be customer facing. Now, as, think of example like if we are working for a, supermarket, we, our model can recognise all the existing items like, banana, apple, orange. And then one day, the creative farmer, they have a dino-fruit come out and then of course, this diamond fruit, the model never think of it before. They don't know what is the diamond fruit. So what the model will do, maybe the model will classify it as a, item I don't know. And now as an annotator, what I do is, you know, regular base, I can check the class, I don't know, to see if there any new stuff and then if I, notice the diamond fruit and say oh, this diamond fruit is a new product. So I will unlabel them. I will find all diamond fruit and then label them. And then so that our model will also recognise, will also quickly recognise what is the diamond fruit. This means we can faster onboarding new classes. So now, our cycle can helps me keep improving and then digesting more and more different products. - So that brings us to the summary. Here we would just like to come up with some real high level takeaways for you. So what we have demonstrated here is really how you can train a model with very little label data. So if you pay attention to the way that we sample the data, there's no label at all. You really take the same apple and randomly pick another lychee and then train, how to separate them. So that's how you know that there is little label data. And, yeah, that's first summary. - And then we also introduced what is semi-supervised learning. We introduced some techniques like official learning and then we also introduced the embedding models. So as of now, we are more equipped and more comfortable with working with unlabeled data and little label data. - Right, and the final thing is, we cover a number of operation concerns. You know, ML Ops, have you ensure that you can train embeddings successfully, how do you visualise them, those things will hopefully give you a better experience the next time you want to try with some supervised learning. - Thank you so if you have any questions, please feel free to ask. We will still stay online and keep asking questions in Slack, we will be there. And if you want to find us, also you can find us on LinkedIn. - You sure can, I've put some of your contact details in the channel so you might regret that because judging by the activity in that channel, you are going yo have a lot to look at later. There's a chance to look at it during the break and during the afternoon. Speak of it also remind everyone that we've put together a virtual speakers lounge for later in the day. And, at the end of all the talks, everyone is going to pile into their channels and a few of us are going to go into the hallway track and have a chat about how you thought it all went, who you enjoyed, all those kind of things. Now, I am going to pick, a couple of questions out for you. So, 'cause there's no way you'll absorb that in giant conversation which I have to, you'll be pleased to know that Matt Kelsey pitched in with a giant rant about one, elements of machine learning so, you'll have to unpick all of Matt's work this afternoon. But, gee, when you put up that triplet gloss slide, you lost me. I think that's where those of us who are not technical fell out, I thought, I was reminded sorely of the three body problem, which I don't think was actually a solvable, mathematical equation and, thank goodness we got you here to solve that sort of thing so, I think a lot of people were interested in just the how, getting it done kind of thing. ENG asked this question, what do you think of how machine learning algorithms analyse visual data versus simply text-based data or even more complex, a combination of the two data types. Imagine trying to tease out the text of a document, pictured in a image, and how do you, in his experience they tried that a lot of times for work and, he's found it very technologically complicated. If you just buy a package, you just go to Amazon and get a subscription to recognition. Do you start there from scratch like you're overselling it Brad and Jessie, is it a really hard problem to solve? - Yep. - Who wants to take that? Is that Jessie, you want to have a crack at that one? Would you just buy a package to do a complex analysis or would, how would you build something for that in the real world? - So, in our work, we just did a simple example. We used tense flow, so build from scratch by, Matt Kelsey and us, yep, so, it's not heavily with that package. It's mainly start from scratch. - Brad is it a solved problem? The mixed media problem of both pictures and words in the same documents? - Yeah, it is a well-known problem I guess and I guess the decision whether you buy or you build thing, I think is a decision whether how much you want to iterate, improve and customise your model because, it could be some new instances in the model that recognition may be not really good at doing that and I think there's sort of a lot of techniques out there in the self-supervised learning space. You can do Redux training, you can just try to, you know, take one part of the picture to train and predict. It again, and you try to recover the text from the image. So a lot of techniques I think can be done. I think the advantage of when you build your own model is, you probably can cover some of the new instances that is practical to your used case I guess and you can't always come back to recognition from it, at least as an incumbent. You could have a chamber and challenge your model and see which one works better for you I think. - Well, we live in an amazing world with the availability of technologies and, you know, the whole cheap storage, cheap compute, and image recognition as a service. It is truly and thank you two for taking us to that future. We'll see you over in the channel but it's time for a break at this point and lots more information over there. I put all your extended references, et cetera in the channel and, go and have a look and see what sort of controversy you've all created. Thank you so much. 