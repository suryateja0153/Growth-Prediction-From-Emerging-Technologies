 [Music] hi everyone and welcome to the learning from limited label data session in the frontiers of machine learning event my name is ahmad awadallah i'm from microsoft research ai and today i will go i'm going to be talking to you about how do we try to bring ai experiences to everyone overcoming the challenges with limited label data as an information worker you have to deal with a lot of sources of information in order for you to be productive from your documents and your slides to your email and calendar if you are a developer you also have your code and your pull requests and bugs if you are a salesperson you also have your customer data and leads and so on so forth in microsoft we have been working very hard on trying to leverage ai in order to help everyone be more productive whether that be by understanding the content of your email messages to better prioritize them or recommend the actions for you to take on them or by better recommendations that tries to predict what are you trying to accomplish right now and sharing and making available the right information for you at the right time and we strive to build intelligent experiences quickly and efficiently that allow us to reach more markets languages domains and tasks however recent machine learning techniques specifically deep learning require large amounts of training data and if you look at the figure i'm seeing i'm showing on the screen you would see the performance of one of the glue benchmark data sets with a birth based model as we have more and more training data available to us and we have seen that curve over and over again where even with large-scale pre-trained language models we still need a lot of training data for the task at hand in order to achieve the desired performance so if we were if we want to reach languages and markets and domains what would be the best way around that can we just annotate all the data that we need if we are only interested in 100 tasks and each task require a moderate amount of training data like in the order of tens of thousands but we also want to support 100 languages for 100 different organizations the numbers add up pretty quickly and we find ourselves faced with the need of collecting hundreds of billions of annotated data this is not only hard and time consuming and expensive but it's also in many cases not feasible because of the private nature of the data that will not allow us off creating manual annotations for it so let's think about the different phases we go through as we are developing an ai based experience at the beginning we typically have limited amounts of annotated data for the task we are interested in but we might have large amounts of eyes of data and associated metadata size of data is data that's accessible to us but we cannot see or annotate so they are always unlabeled and the associated metadata could be metadata related to the tasks such as user behavior signals and so on in that phase techniques like weak supervision where we can leverage low quality labels based on user behavior or other metadata as sources of supervision for machine learning can be very helpful additionally techniques like semi supervised learning that can leverage unlabeled data by using structured assumptions about the data itself could also be of significant help once we have a version of our experience working in a limited set of languages or domains we can start thinking about expansion to new low resource languages or domains and in that phase cross-lingual transfer learning techniques that try to leverage knowledge available in rich resource languages to help low resource ones are very useful but also domain adaptation techniques that try to do the same but for new domains when our experiences are deployed and and users are interacting and using them that provides us with a very valuable source of knowledge which is user interaction data that comes in the form of implicit and explicit feedback about how humans experience our systems this data could be leveraged in order to learn from implicit and feedback implicit and explicit feedback directly to improve the system and also to build learning to correct models that are able to recover from mistakes that our models are doing finally all of these techniques are built on top and leveraging recent advances in pre-trained language models such as pert turing and gpt so now let's double click on one of them and talk a little bit more about weak supervision i'm specifically interested in the case where we can leverage user behavior data that might be readily available to us as a source of weak supervision this real world data beyond content could still be very helpful for us as we think about building natural language processing models many such applications have a lot of information like user actions current context and so on so forth if we take office 365 as an example we have hundreds of millions of active users creating and interacting with trillions of entities like emails and events just like modern web search engines have been very successful at efficiently leveraging user behavior data such as queries clicks sessions and so on so forth we think we can leverage user behavior session in order to build better machine learning model operating on user content so going back to the assumptions i was describing earlier we assume that we have a limited amount of annotated clean data but we can use user behavior and interactions to generate a much larger amount of weekly annotated data and our objective is to leverage both sources in order to train task specific models specifically we have been finding a lot of success in using a meta learning framework in order to concurrently leveraging the noisy data and the clean data to improve our models more specifically you can you can imagine a setup where we are co-training two models at the same time our main model that's trying to solve the task at hand and another model a meta model that's trying to take in the noisy data and correct or reweight it in a way that makes it more useful for the main model when we co-train these two models concurrently we are able to significantly improve the way we leverage the noisy data in order to improve our systems we have applied these techniques to multiple applications for example when applied to the task of email intent identification we can collect a small amount of annotated data in the orders of hundreds or small amounts or thousands but we can collect a much larger amount of weak label data derived from behavior signals such as reading an email or flagging an email creating a calendar item attaching a document and so on so forth when we concurrently leverage the clean and the weak data together you can see that we can achieve much better performance than using any of them in isolation the weak data tends to be noisy and deep learning methods tend to be very good at modeling the data including the noise so co-training the models on both clean and weak data at the same time allows us to extract the signal from the noisy data in a way that improves our own overall performance without being significantly affected by the noise and you can see that we can achieve pretty good gains in different scenarios even when we have clean data as small as one percent of the total data or larger amounts of training of clean data as well there are some scenarios where you don't have a single source of weak supervision but you can have multiple sources providing multiple labels for the same instance we applied that to this data set of fake news detection where weak labels are derived from behavior signals such as sharing commenting sentiment and so on so forth where we have multiple weak supervision signal for the same aniston and you can see that we can also effectively leverage the multiple sources of supervision and that having multiple sources of supervision can further help now going back to our picture i also wanted to spend some time talking about another scenario learning to correct machine learning models will always make mistakes and they will never be perfect if we are able to build collaborative ai systems that enable fast and progressive interaction between the human and the model allowing them to collaborate in order to improve and solve more complex tasks then we can work around these limitations one way to do that would be to allow the human to interact with the model to identify and correct mistakes we studied this problem in the context of semantic parsing where we have a scenario where you are trying to ask questions against the structured data such as data in a database the semantic parser's job is to take in a natural language query and generate a parse in sql that it can execute against the database what we see here is a case where the user asks the question and the system came back with an interpretation of the question but the system made a mistake in its interpretation if the user is able to provide open form natural language feedback describing what the mistake is and how it can be fixed it can significantly help the model narrow down the scope of where it's looking in order to fix the mistake and we have found out that in this application actually many of the mistakes can be addressed and corrected by seeking feedback from the human many of the mistakes were conditions that were dropped or added by mistakes or some information that should have been included and has not been included or mistakes about deciding how to order the results so how to aggregate over and with and in order to do that we had to construct a data set that allows us to go beyond the parsing problem where we take in a question and generate a parse but think about the correction problem where you take a question specifying an intent from the user an incorrect interpretation generated by a system an actual language feedback provided by the human trying to correct the mistake and finally the correct parts that we would like to get at and we can see that we even with simple methods we can correct more than 25 percent of the mistakes using just one round of the feedback our model here leverages existing models in semantic parsing where we basically try to take the semantic parts that has been generated and try to edit it in order to correct it leveraging the feedback that we have collected from the human 25 percent of mistakes is very valuable given it's only one round of feedback but also we still have a long way to go because as estimated by a human performance we can see that we can correct up to 80 percent of the mistakes leveraging the open forum feedback going back to this picture you see that there are different ways that could allow us to alleviate the challenge of label data scarcity and we just briefly talked about two of them and how we are applying them to some of the problems we are interested in at microsoft such as productivity applications that help you with test completion or question answering and semantic parsing systems that can help you quickly interact with data finding the right information but there are so many other interesting directions that we can pursue and i'm very excited about the three invited talks that we will have in the session where we will hear from experts in the field about different directions related to the theme of our session dr marty hurst from uc berkeley will start by describing some of her lab's work on building text summarization system without text summaries leveraging techniques like reinforcement learning that can learn directly from external reward signals dr graham nubik from carnegie mellon will then talk about how we can expand our models to the long tail of the next 1000 languages finally dr alex ratner from the university of washington will describe some of the work that he has been doing over the years for building machine learning models with weak supervision and applying it to a diverse set of domains such as medical applications or knowledge-based construction we're very excited about this session and we hope that you will enjoy the three invited talks and you will participate in the discussion thank you so much well thank you ahmed for that uh wonderful talk and thank you everyone for joining the session um as a reminder if you aren't paying attention to the chat window uh we will actually be available in the chat window to answer questions um i'm paul bennett a researcher at microsoft research and co-hosting the session with achmann and next we're going to hear from marty hurst marty is a professor in the school of information and the eecs department at berkeley her primary research interests are in search engines information visualization natural language processing and moocs she's been very active in writing on such uh topics as search user interfaces as well as a fellow of the acm and has many rewards throughout her career today she's going to be talking on this theme of limited data about self-supervision vision and summarization is also going to provide some context about summarization challenges in general so with that uh let's go to marty's talk i'm marty hurst and i'm very happy to talk about some research we've recently completed that we're calling summarization without summaries this work was done primarily by phd student philippe lavonne from uc berkeley and with two other collaborators uh andre psy from bloomberg and john kenny from uc berkeley so here's a brief outline of what i'm going to talk about first the umbrella project is the newslends project for which the summarization was one piece then we'll talk about automated summarization in general and then we'll talk about our particular approach to abstractive unsupervised summarization so the overarching project is called newslends and the goal is a better news reader it's also a platform or jumping off place for natural language processing research including summarization like i'll talk about today news lens is available online for you to try and currently consists of a chat bot in a mobile app as well as a web-based interface that incorporates events event-driven stories over time and this incorporates the automated summaries that i'm going to talk about today the newslends data set is really quite large it's been gathered over more than 10 years it includes more than 40 sources from around the world and over 7 million articles and we use this data set for part of the research i'll talk about today so next i'll talk about automated summarization in particular extractive versus abstractive summarization and then prior approaches to abstractive summarization and what our particular definition of a good summary is so if you're doing summarization typically your goal is to take a long document and shorten it in some way but still retain important information from that document so consider this following news article that i'm going to read through a bit since this is our running example and say we want to reduce it to 20 to 25 words of length so the article is about chilean president sebastian panera who announced on wednesday that his country which has been paralyzed by protests over the last two weeks will no longer host two major international summits clashes at demonstrations in the capital of santiago have left at least 20 people dead and led to the resignation of eight key ministers from panera's cabinet the president has now canceled the hosting of the economic apoc forum and cop 25 environmental summit which are both due to take place later this year this was in 2019 so if you're going to make a 20 to 25 word summary what would you want to include in this if you're doing extractive summarization you have to pull out text verbatim so you pull out one or two sentences that's pretty much all you can do so you have to pick between these three sentences among these three sentences and decide which one would make the best summary so it's extractive summarization you pull out a sentence and that's your summary the president has now canceled the hosting of the economic apec forum maybe not the best summary with abstract summarization by contrast you identify key concepts keywords that you want to include in the summary and then you create a brand new summary by taking those terms and interweaving them with other glue words that make for a fluid flowing comprehensible summary chilean president announced his country will not host the apec forum and the cop 25 anymore due to protests in santiago this was actually generated by our system not perfect but i think you might argue but it gives more information than the extractive summarization summary and in a smaller instill in ace brief form abstracted summarization is quite appealing you can tailor the length and keep and often the length of abstractive summaries are shorter than extractive ones because you don't have to pull out existing sentences verbatim you can pack in more key content into a short space and also it can count as derived work which is helpful for intellectual property issues but the challenges are it's much harder to automate abstractive summarization and only recently has there been progress in this area and furthermore it's quite subject to error and especially summarizing news we don't want to make false statements about what happened in the news so what are current approaches to abstractive summarization well the standard approach is to use a seek to seek model where you encode the document and you decode out a summary and what people do for use abstractive summarization is they use an existing very large data set of abstracts and documents summaries and documents and they train using a seek to seek model with teacher forcing this reference see it all use a pointer generator network to do this so the benefits are that the model learns what's in the data and so actually they don't have to focus on summarization as a task it's just a standard kind of approach but the limitations is that the results tend to be more extractive when you actually see the results rather than abstractive and you can't actually control for the length you can't say you want a 25 word summary so much it's really based the output you get is based on the input that you trained on and you need very large collections in order to make this work in the training as they are supervised methods another approach recently came out well in 2017 by paula said all which is to use the rouge metric which is a standard evaluation metric uh for summarization and actually optimize on that evaluation metric so it's rouge is easy to compute because it's just n-gram overlap between the summary and the reference document are between also between reference summaries so how long does your summary overlap with reference summaries so the idea was what happens if we directly optimize our summarizer with the reach score so what happened was it got very high root score so it was successful on that metric but unfortunately since rouge is only an approximation to what a good summary is the summaries were poorly rated by people so here's an example output from that work where the first sentences read kind of well but they're extractive and the sentence or the text outlined in or highlighted in red is really not fluid and doesn't really make a lot of sense so these got poor ratings from people so we propose an alternative we extend police at all's work by building a better evaluation metric and optimizing for that instead of reach and so our approach is we define what we think a good summary is and then we train a reinforcement learning algorithm to optimize on those metrics so what are our metrics what is our definition of a good summary we define a good summary as a brief fluent text that covers the main points of the original document and we'll be emphasizing these three points in the remainder of this talk so now i'll talk about how to summarize without summaries talking about how we get coverage via masking how we retain fluency all within a reinforcement learning loop and we'll present some results so step one is we need to compute what we call coverage so summaries must contain keywords in order to be a good summary of a news article so what we do is identify keywords from the document so here i've highlighted some important keywords that we want to have appear in a summary so we use a tfidf type measure to select terms and notice that all forms of the same word are identified so if host occurs in the original document we also want to identify hosting and all occurrences of that term and we also want to identify entity names and we we basically identify about 30 percent of the documents terms although this is a trained uh hyper parameter and then what we do is we mask out those selected keywords we create a version of a document with those terms masked out then the algorithm must figure out which keywords were in the original document but it has to extract them from the generated summary and this is different than how masking is typically used so we generate a summary and then the algorithm has to figure out what the blanks are in the original document from the summary so this incentivizes the algorithm to put keywords into the summary so if we have this generated summary this fills in 10 of the 15 slots that we blanked out highlighted in green there's more than 10 in there's not 10 words highlighted in green in the summary but that's because host covers host and hosting in the masked out keywords so i'm going to start building up a diagram of the algorithm overall the architecture of the algorithm so first of all our goal is to have a brief fluent text that covers the main points of the original document so we give as input to the summarizer a length a target length which helps us enforce brevity as well as the original document and the summarizer just generates a summary then we mask the document in the manner i just described and we feed the mass document into this coverage model into a coverage model then the coverage model generates a document that's been filled with its best guesses as to what the blanks should be filled with and those blanks are assigned a coverage score so a little more detail about that so those of you who know about bert and its mass language model might see some similarities between that and the coverage model that i'm talking about right now so in the bird mass language model it blanks out a random percentage of tokens of usually 15 and it fills in the blanks using the rest of the unchanged unchanged tokens from the same document whereas what we do is blank out all occurrences of a set of key tokens it's not random it's motivated and it's every occurrence so with bert you might blank out one occurrence of paris and not another occurrence of paris but we blank out all occurrences of paris and then we fill in the blanks using both the unchanged tokens from that document and the unmasked summary and our algorithm does use a burp model and we fine-tune it on our coverage scores so here's that in a bit more detail where the input is a summary followed by a separator followed by the masked document and the output is the fed into a fine-tuned vert whose goal is to identify the fill-ins for the blanks so you can see here chile was the wrong guess for the first mask president was the right guest for the second mask and so on in this case say the algorithm gets 33 of these right and it gets a covered score of 0.33 all right so that's the what we've got there this then the next step is to retain fluency so as we've talked about coverage and what does that do it incentivizes finding keywords or content words but this can lead to generating just a list of keywords which isn't very appropriate for reading so our goal is to balance content and fluency and our approach is to optimize for both simultaneously so we add to this model to this architecture a fluency model which generates a fluency score and finally this is all incorporated into a training loop uh in particular a reinforcement learning training loop and we use the sc st optimization procedure the self-critical sequence training which originally was applied to image captioning and we directly optimized the summer summary score which is a combination of the fluency score plus coverage with two parameters that are learned the fluency model only sees the summary and we use a language model that's fine-tuned on news on the large news collection from news lens actually to obtain a score and we you see here we do some normalization to put the fluency model within a certain range and so the final summary score is a weighted sum of coverage and fluency and in more detail how the training works is we actually generate two candidate summaries s1 and s2 these are generated with two different sampling methods and the details are in the paper we compute a summary score for each of these and then the gradients for update are based on the reinforced algorithm based on the difference between these two scores r1 and r2 and there's uh some details about how if one is lower than it should have been by by default then that causes a change in the expectations so we're in essentially essentially increasing the model likelihood of the summary with the higher reward which increases the expected reward here are a few example training runs or one screenshot of example training runs and what we're seeing here is a trade-off between the fluency score and the coverage and then the summary score altogether uh trained over several days and what you see is at the very beginning we can get pretty high fluency using a language model and very low coverage there's a big spike in fluency which then rapidly drops off as the coverage increases so there's you very much see a trade-off between the two and the summary score shows the two being balanced against each other and then very very slowly increasing over time now i want to show the effect of varying the target length what kind of summaries you get depending on the length of summary you get you are outputting so these are summaries generated by the model so if the target length is 10 we get panera canceled the apex summit at santiago if we make the length 24 we get panera said chileans have been canceling the hosting of the apex summit which was scheduled to take place in november we give it more space 45 words we get sebastian panera announced wednesday that his country will not hold the apex summit which was scheduled to take place in santiago panera said that chileans have been paralyzed by protests over the last two weeks so much better summary it has more space and you see the coverage score increases as the length gets longer which makes sense there's more room for content words while retaining fluency and you can see the dynamic nature the algorithm is able to generate different qualitatively different summaries depending on the length so let's do some results and compare this algorithm to others using the standard measure of rouge we first show supervised methods the top ones pointer generator pointer generator plus coverage and the bottom up algorithm get rouge one and regel scores as shown here of the unsupervised methods of which ours is one text rank which is extractive gp2 zero shot and summary loop um 45 length 45 summaries we're doing better than the unsupervised methods and actually better than pointer generator as well and this is with no training data also we can combine our algorithm with supervised data and we get even better results so if you initialize a supervised algorithm with the summary loop model and then train on only 10 percent of the data we do as well as gpt2 on 100 of the data for the cnn daily news data set uh if you give some really 100 of the data we actually do better than all of the other approaches now if we want to see how abstractive are the summaries generated uh some of these tools some of these algorithms generate rather extractive summaries so let's compare them so we look at the measures of how many errors are made and what sort are they inaccurate these are manually assessed are they inaccurate or are they ungrammatical and then how many abstraction techniques are used so these include compression sentences merging sentences novel sentences and entity manipulation so bottom up has some more errors but it also uses more techniques more abstraction techniques and the summary loop has some a few more errors it's a trade-off there but far more abstraction techniques than the other two with a 57 technique application success success rate so much higher than the other two and with far more abstraction here's an example of a note structure summary generated where the red shows sentence merging and the blue shows sentence simplification it's a sentence merging is taking the words from the left in red and making a new sentence and the blue is a shortened sentence you can see it's doing a lot of abstraction to make a shorter document so to summarize we have some next steps we're actually extending and adapting the approach to other text generation tasks including text simplification and summary style adaptation and this is actually in collaboration with microsoft research and actually philippe labon is doing an internship this summer at msr we also have the chat bot that i mentioned this is also a paper in the acl 2020 demo track and i encourage you to check out the youtube video it uses question answering and generated uh abstracts isn't it so in summary our contributions are summaries that do not require training examples are highly abstractive especially compared to state-of-the-art have configurable length and incorporate key content from the articles and a new approach to reinforcement learning using fill in the blank with motivated choices for terms that balances coverage and fluency that makes use of special techniques to fortify against degenerative cases that i did not talk about here better in the paper and there's code available on github if you'd like to try it out so thank you for your attention i want to we want to thank our sponsors of bloomberg amazon nvidia and now microsoft research for our work going forward and i hope you check out the research in more detail thank you so much for the very interesting talk marty a reminder to everyone that marty is available online as well as all other speakers so please feel free to submit questions in the live chat our next speaker is graeme newbig graham is an associate professor at the language technologist institute in carnegie mellon university his work focuses on natural language processing specifically multilingual models and models that allow us to best natural language interfaces for humans to communicate with computers in their own language he publishes regularly in top venues in natural language processing and machine learning and his work has won several awards including at eminent esel neckell and others most nlp work focuses on few resource-rich languages such as english and french in his talk today graham will talk to us about how can we expand that to the long tail of the next 1000 languages hello my name is graham newbig from carnegie mellon university and i'm very happy to present today about lessons from the long tail methods for nlp in the next 1000 languages so as we certainly know natural language processing techniques have made great progress in the past several years especially for languages like english or other high resource languages like chinese french etc however there's over 6 000 languages in the world and for the great majority of these languages we have very little to none of this great language technology that exists for other languages so why is this the reason for this is because the machine learning techniques that have led to great strides on english or chinese or these other high resource languages rely on large amounts of data for training and if we look at the amount of data available for most of the languages in the world we don't have anywhere near the amount that we would need so this is a graph of all the articles in wikipedia and we can see that very quickly the number of articles drops off with the top 30 languages having many more articles than the remaining ones and once we get down to 300 languages we see that all the languages after that have no articles at all so this is a dire situation and it's even more dire if we look all over the internet where more than half of the articles are in english so why should we worry about these long tail of languages that don't have very much data so one very important reason is that language is an inexorable part of our culture and preserving these languages is very important to preserving and legitimizing the culture and language technology can be a tool for this and a single signal of importance of the culture in addition for humanitarian aid even rudimentary natural language processing can help us understand things in crisis situations so for example in the current coronavirus crisis we are working on creating translation systems that would allow people to understand related information in the languages that they speak finally i think it's just the right thing to do people prefer to interact in their own languages so we should let them so one very strong tool in our toolbox to help scale to these languages is multilingual training and what we do here is we basically take many different languages and we feed them into a single natural language processing model and i'm going to talk about three case studies of work that we've been doing in this specifically tailored towards the languages on the very low end of the resource spectrum specifically universal phone recognition linguistically motivated models for cross-lingual sharing and balance training for multilingual models in all of the sections there are a few takeaways so all of them are based on multilingual training methods in addition to scale down to these languages with very few resources we need to use intuitions from linguistics or advanced machine learning techniques and i'll outline these for each of the sections so first universal phone recognition so one thing to know about very low resource languages is that speech is paramount and the reason for this is that most languages in the world are purely spoken and thus the technology we use will need to go through speech on the other hand even for the least resource languages we are often able to obtain speech so this is an example of a speech collection effort by stephen bird where he has created a simple app that allows you to go to speakers of languages one example of this is augustine a speaker of tembe a language in brazil and have them speak stories or other content in their languages into the app and then you can take this and have another speaker for example emilio who's a portuguese speaker go and translate that into another language such as portuguese however while taking this data or collecting this data is possible this can also result in data graveyards and these data graveyards are basically speech data that's locked up in speech and it's never transcribed so linguists spend a lot of time transcribing this data but unfortunately human effort for doing this takes a lot of time and this wastes precious time leads to poor relations with speakers when a linguist goes in and then can't provide anything to them for a long time and speed is of the essence in these situations so phonetic transcription is the first step in building resources for a language and this is basically where we take in a speech waveform and write down the sounds of the speech this is an example from english last time i used the steel button these sounds can be expressed in a number of ways one way being phonemes which are language dependent units of sounds denoted by slashes but at the same time we can also write down phones which are language independent sounds denoted by square brackets and to give an example of this one phoneme might correspond to multiple phones so time steel and button in north american english all are written with ts for parts of their phonemes but actually these are different sounds and you can try this yourself to see how they differ so transcription is usually done in phonemes because this is easier to think about for linguists but this can also be a problem for cross-lingual transfer is phonemes are language dependent so to give an example of a universal phone recognition model we use some techniques to handle the fact that phonemes differ across languages in multilingual asr training so one way that we could do this and it has been used in previous work is a private phoneme model where basically we predict the phonemes for each language separately but the problem with this is that there's too little sharing between the languages so each language is trained basically independently another thing you could do is you could have a shared phony model where basically you predict all of the phonemes at the same time but unfortunately this gives has too much sharing because the shared phonemes actually are language dependent and differ from language language so what we propose instead is to use a little bit of our linguistic knowledge and say okay first we would like to recognize universal phones and then we have a simple transformation to convert these into language-specific phonemes so we tested this model on 11 high resource languages and then we also took the trained model and applied it to two new languages that had never been seen in the training data and tucson which are actually very low resource endangered languages we evaluate the model on phone error rate so the lower is better and what we were able to find is on the 11 languages we were able to do just about as well as any other model but on the very very low resource languages our method in yellow was able to do much better than all of the others and in addition if you have some idea about what phones tend to appear in the language you can further improve these results so another issue in multilingual models is lexical sharing and what i mean by this is even in very similar languages there might be small spelling variations this is an example of belarusian in russian where each of the words are very similar in their spelling with only a few small variants unfortunately for computers even these small variations can cause them to think they're completely different words in addition there are script differences so for example turkish and wigar are languages from the same language family but turkish is written in latin script and wigar is written in arabic script and also there's morphology or conjugation differences where different languages use different suffixes to indicate grammatical features of the word so we've come up with a few methods to resolve these issues one is a method that's kind of a general purpose method for lexical sharing between languages and it's particularly suited for cross-lingual transfers so the way it works is we take words we decompose them into their character engrams and what this allows us to do is this allows us to kind of find words that have similar spellings and ensure they have similar embeddings in embedding space and this allows us to handle spelling similarity we then have a language specific transformation and this allows us to handle consistent variations between languages so this transform is different across languages and finally we have a semantic embedding uh library where we predict a particular embedding for each word and this allows us to capture latent concepts and we do this essentially to model when uh ling words have very different spellings but correspond to the same concept so in machine translation for low resource languages we found that this was significantly better than other options such as using character engrams only and also perhaps more importantly it significantly improves over subword-based encoding methods such as those used in multilingual vert widely used model here to take this a step further in how we can incorporate linguistic information into our models we note that a skilled linguist for example david is a linguist at cmu can create a reasonable morphological analyzer and transliterator for a new language in new in short order so basically what this can do is this can take a script that's not written in the phonemes like i talked about before and convert it into its pronunciation and assign its morphological tags so we then take these linguistic analyses and represent our word with phoneme engrams uh the lemma of the word in its morphological tags and we were able to find uh that this gave good results on name density recognition and machine translation over languages that were in different scripts and with different morphological features finally i'd like to talk about balancing training for multilingual models so as i mentioned before we have very large problems of data imbalance when we're training multilingual models one solution to this data imbalance that has been used before is temperature sampling and basically what this does is this down samples the most frequent languages and upsamples the least frequent languages when training our models in terms of data size and this is also a method that's used very widely in multilingual training such as multilingual burke or multilingual neural machine translation models what we ask in this work is instead can we learn the data sampling strategy directly in order to maximize our accuracy to do so we turn to a method that we are going to be presenting at icml very soon which is a differentiable data selection and this is a meta learning method that allows us to learn a weighting of training data to optimize a held out development loss and what we mean by this is we essentially have a data score that tries to predict how frequently we should be sampling data and this data score is learned to minimize the development loss on the data set that we care about so the main idea is that the score should upload data that has a similar gradient to the development data so we calculate a reward for this data score based on the cosine similarity between the sample data and the development set that we would like to optimize accuracy on so how can we apply this to multilingual to learning multilingual data usage so the existing approach as i mentioned before is temperature-based uh heuristic sampling and the way this works is basically we take the size of the training data for each language we exponentiate it by a temperature value and use this as our value for the sampling data from each particular language and how we use differentiable data selection to do this instead is we directly parameterize the data score over the standard data set sampling distribution so basically instead of sampling by the data size and heuristic temperature we directly learn the sampling probability itself we then optimize the model over a multilingual development set to make sure that the model learns to be good at processing all of the languages in the development set so we performed experiments on multilingual neural machine translation and we display here gains over a single language baseline where we have temperature sampling the kind of state of the art method here we also have proportional sampling where we sample each uh language according to its overall frequency in the data and the bars here are basically two different data sets and many to one and one to many translations so what we can see is in many to one translation where the target is always english proportional sampling works better and in one-to-many translation proportional sampling does not work well and what we see is basically there's no consistently strong strategy with respect to this on the other hand multi-dds our proposed method and another method that tries to stabilize training with some tricks do significantly better than these baseline methods so i've given a very brief overview to some of our work and i'd like to talk about what we know and what's next so basically we are currently building a powerful toolbox for cross-lingual learning this is a very active research area and as i mentioned data is a bottleneck but in another way human resources are a bottleneck as well so this is an example of a paper count at 2018 nlp conferences by the country of the person who was publishing the paper and what i think is really important to see here is for example africa and south america are not despite their linguistic diversity are not well represented on this map so i'm really excited by efforts such as masakane nlp which is an african initiative to try to get people from africa working on nlp on the languages they're interested etc so thank you very much and i'll be happy to answer any questions thanks everyone uh first thing with us and as you notice if you're paying attention to the chat we're having a few uh difficulties with posting messages um our speakers are here and we'll get to some of these questions in the q and a if we can't respond to them there hopefully we'll also get be able to get this fixed during this session um and uh after graham's great talk our next speaker is alex ratner alex ratner did his phd in the computer science department at stanford he's now moved to uw and is an assistant professor there uh he's focused on real world problems applied to many spaces but in particular uh very much around taking methods such as weak supervision and making them more formal and scaling them out with systems like snorkel and he's going to talk to us about some of those challenges today so with that let's take it away with alex let's talk hey how's it going so i'm alex ratner and i'm going to be talking today about some [Music] kind of practical notes observations and some of the techniques that i've been developing through the snorkel project at uw now and also previously and i'll be covering work from when i was doing my phd at stanford so hence the pastiche of logos there all around uh one approach to handling the the lack of of labeled data that that is often such a bottleneck to machine learning progress today via programmatic approaches to weak supervision and i'm gonna have a special emphasis in this kind of more casual high-level chat uh and and preface to the q a coming up on notes from the field from from lots of practical deployments both at stanford uw and out beyond that so i'll start with that and here i'm going to uh go a little bit deeper than uh then perhaps usual into the motivation of the problem because i think there are some interesting practical notes about how practitioners actually are using weak supervision both via systems and techniques like the ones i've worked on along with a myriad of other ones so i'll start with the the 40 000 foot level motivation which is that and again i think this will be redundant for this crowd but it's that machine learning development really has a new bottleneck today and it really centers around the data that these models learn from the so-called training data and so you have you know at a high level for a standard let's say iid classification problem you have three main ingredients you have some labeled training data that's labeled according to the annotation schema that you want to train the model to to output according to you have some kind of model architecture and obviously algorithms to uh to train it and then you have the hardware and the infrastructure that this rests on and it used to be that the model and the features and the the structure of the model and model architecture and all the hardware and infrared this was where teams spent their time on and got stuck on when deploying machine learning you know five ten plus years ago one of the most remarkable things that's happened over the last you know five years or so even is the increasing availability accessibility and and power of these last two steps so i often use the phrase commoditization and i think that's meant as a stunning positive for what the field has accomplished what open source offerings have accomplished in that if i want to get say you know a state-of-the-art solution to what often used to be a grand challenge problem machine learning like classifying images i can do this in several lines of python to get the latest and greatest uh model or a wrong example given my python code on the screen but you got my point and i can you know pick your pick my favorite or second favorite cloud provider whatever it might be and and uh spin this up and get a a really state of the art solution but of course this all relies on having the training data and the trinity that's carefully labeled and curated and managed according to the problem objectives and so i'll give an example from a paper that we actually just published in the in patterns based on work with several teams at stanford medicine and stanford hospital and this is just one of many examples that highlights not just that training data is is a bottleneck but it's that it's it's really a uh you know a very strident one that has orders of magnitude uh difference so in this example the goal one of the several goals of the different data sets was to classify chest x-rays for triaging so chest x-ray comes in should it be read urgently or can it sit in the queue and be read later by a human radiologist and uh given a label training data set that had taken in this case about eight person months to label uh the modeling took a couple days the vr collaborators they downloaded some of the state-of-the-art you know cnn and other image classifier models and the variance amongst those models was under a point in in the metric we were optimizing for rca you see on this binary classification task conversely the the training data again as i just mentioned took eight person months and the spread and according to how uh you labeled and denoised and managed it which i'll get into in more detail coming up uh accounted for an order of magnitude greater you know variance in the quality so i think this is just one of many many examples that highlights how training data is not just a necessary ingredient it's one of the highest leverage points in determining ml success and it's extremely difficult to get in the real world so um everyone from you know uh journalists and scientists and researchers to the largest organizations in the world are often uh blocked not by the people or the algorithms or the infrastructure but the labeled training data so i guess that's that's all stuff that uh you know if you're in this session you're probably already uh well-versed in but i want to go a little bit deeper into some of the practical pain points that that we've gotten to see out in the wild working with a range of organizations all in all different you know sectors and areas so first is the issue of data privacy and i think in machine learning we're very used to the idea of problems on public data sets that can be labeled through some kind of crowdsourcing operation where you take some data it's either accessible in the open source already or you ship it out to some external org and pay to have it labeled and i should note that even this takes a long time so imagenet again hopping to an image rather than nlp example but imagenet uh one of the foundational data sets that fueled a lot of the modern cambrian explosion of ml progress took two years over two years to label and i was just public image data and there are similar stats for other data sets that other training data sets to get labeled for many many organizations in healthcare in finance in government in any tech company that has user data in telecom and the list goes on this is just a non-starter you can't ship data out of the organization and often there's actually internal data access issues so even you know one team of data scientists or developers may not have access to the data that um they want to train their models on internal to that organization due to privacy constraints for say user data or or cleared data or etc so again nothing new that data is often private but i think it's often underestimated how much this is a a practical blocker to building training data sets and therefore to really leveraging modern machine learning techniques subject matter expertise is another huge one that i believe is undervalued we're used to data sets that involve um you know labeling cats or dogs or stop signs or pedestrians on the image side or you know labeling sentiment or or uh you know named entities or parts of speech on the nlp side uh most exp you know most annotation out in the real world requires some kind of subject matter expertise a doctor a bioinformatician a legal analyst a government expert etc and annotation schemas how you label the data for the model are very org specific and finally what i think is the most underestimated factor in how modern machine learning is conducted today is the fact that you really need to be able to iterate very rapidly and that iteration has to include iterating on the training data and this is just not something you can do with massive hand label training data sets at a high level you know you have input distributions that change in the real world you have upstream components preprocessor everything from how you're tokenizing words in an nlp pipeline to how you're sub sampling the data all of this changes constantly and every time something upstream changes you often need to alter the training data which again if it's all hand labeled you can't do and then finally output goals change all the time you know do you want to label these documents eight ways sure label a whole training set uh over weeks or months to do that and then someone downstream of that machine learning model decides they actually need a a 15 way classification so suddenly you have to throw that training date out and start over again so again i'm kind of dwelling here longer than uh maybe necessary for many of you but i just think it's these are often under appreciated pain points that really beg the question uh how can we do better than just relying on these massive labeled hand-labeled training sets i'll throw in one other higher level motivation which is kind of an engineer's or more kind of toy theoretical motivation which is in a sense if we have subject matter experts that have high level domain expertise we used to have ways to inject that into uh data processing systems with expert systems they would write a rule uh with uh feature engineering based machine learning modeling approaching approaches they would make a feature now each of these approaches had their drawbacks rules are brittle and expert systems didn't generalize well enough feature-based approaches it's not exactly the most direct way to inject this knowledge either because you know it's it's statistically tricky but still there were ways to get subject matter expertise in the system and the way modern ml works today is great if you have lots of label training data but it often feels like kind of playing 20 questions you know when you could just ask for something directly today when you don't have that massive training data set in other words imagine you're a um an analyst at a bank and you want to classify contracts and you have a legal analyst that just wants to say something dead simple like if you see the word employment in the title classify it as an employment contract right sounds extremely simple but getting that specific heuristic or that specific domain bit of domain knowledge into a modern machine learning approach via just this input port of labeling data points individually is near impossible uh or at least it would require this is ridiculously uh difficult you know labeling thousands of of data points um uh you know something roughly inversely proportional to the sparsity of the feature space just to communicate you know one little common sense bit of information so um so contour classification spam classification you know again a lot of the question uh we asked in the work i'm about to give a little bit of an overview around weak supervision is why is hand labeling the only interface why can't we let a subject matter expert especially when you need a very time constrained and expensive subject matter expert to provide the information why can't they just directly inject that information so that was kind of a whirlwind tour of of these very real world pain points that i think are often underappreciated but that really make label training data a a very problematic uh paradigm not just a slow or expensive one but really a non-starter for for many people who want to use machine learning so the key idea that a lot of my work and and the work of the teams i've been fortunate to work with has focused around is this idea of of building and managing training data programmatically and a lot of this work uh overlaps with um what's often called weak supervision the ability the idea of using something that is faster cheaper higher level than label than individually labeled data points but also noisier and and messier in other ways to supervise machine learning models so i'll just show an example of this and this is um an example of a system called snorkel flow it's based on uh some research that we've done over the years out of out of uh stanford and uh it's kind of an end-to-end platform around the idea of programmatic training data labeling and so the first idea is directly coming from the motivation before let users directly express their domain knowledge and express them in functional forms so for example you know look for the phrase credit agreement in the title if you see it label the contract a certain way otherwise just abstain and i'll get back to why that abstention part of the semantics is actually very important uh now actually i should be positive before being negative with the red text the positive part which should be apparent is that providing supervision this way kind of bridges the best of rules-based approaches with modern statistical learning approaches as i'll get to it allows you to inject domain knowledge directly in an interpretable and auditable and versionable way and obviously in a way that can be much faster like in that radiology example the problem of course is that these labeling functions are what's often called weak supervision they're noisier especially when you're writing them in these functional forms they might be correlated or redundant in various ways you might write two labeling functions from two different developers that are near duplicates of each other you don't want to double count their votes they can disagree with each other and have varying levels of noise or accuracy so a lot of the work that we did over the years and are still doing is is working on ways to via you know latent variable modeling approaches to denoise and integrate the supervision with with theoretical consistency guarantees some of the latest work is you know the intuition is based on looking at agreements and disagreements between the labeling functions and either using matrix completion style approaches or triplet based approaches to even when there are tangled correlations between these labeling functions denoise their outputs um and then one key idea here is the ability to generalize so um i'll use an example around contract classification this is actually an example being used in a major u.s bank you might ask very reasonably if you uh you know labeled the data and you train this this latent variable model to combine the the different labeling functions you now have something that can be applied to new inference time data and label it uh why what do you need the model for and the answer in general although there are actually some more nuanced ones i'll get into a little bit later is to generalize beyond these labeling functions the the major down side of role-based approaches is usually especially with things like text that they're very hard to write in a high recall and high precision way so you may label with your labeling functions you know some 20 30 40 slice of the data the idea is to rely on modern statistical learning approaches to generalize beyond that so one example was even a very intensive effort of writing labeling functions for a very high profile problem got about 84 85 accuracy and uh the model was able to generalize beyond that and get 97 accuracy so that extra boost was done just via leveraging a a model architecture over a lot of data no additional inputs of of from the expert users and then finally one thing that's actually crucial i won't have time to go into some of the work that's starting around this but the high level concept that i'd like to highlight is the ability to iterate and i think in you know one of the biggest things that we've tried to accomplish with projects so far and i think is one of the most important directions for uh study and weak supervision moving forward is this ability to rapidly iterate and to take machine learning development from something that often today looks like this one-off one-and-done process that always has to start with a massive upfront investment in hand labeling for weeks or months and turn it into something that looks much more like a a rapid iterative error analysis driven development process just like how we build um you know the rest of software and uh this is something that you know uh in this snorkel flow approach is accomplished quite you know simply and practically by uh you know looking at error modes in the data and then going back and editing or writing labeling functions and turning this crank very quickly but i think stepping up a level of abstraction this idea of being able to iterate with weak supervision uh is extremely and with machine learning in general is extremely important and a really fascinating area of study skip over this i mentioned a couple published case studies with just one quick takeaway for each so and i'll move kind of quickly here but we did a deployment a little while back at google with three teams there we deployed a version of some of the open source tech around this the snorkel idea uh we found that that this approach could replace tens to hundreds of thousands of hand labels which had a big you know impact the bottom line the cross and now there's a bunch of deployments but i think the thing that to emphasize here was that it actually wasn't about the cost of labeling a hundred thousand hand labels for these applications because these were these were you know high profile ones that had budget for this it was about the fact that they changed all the time and that it wasn't a hundred thousand labels once it was a hundred thousand labels every couple of days and even for an organization like google this made ml impractical um another example intel i'll skip over the details because we're getting close to time but uh they just do a great uh we did a collaboration with them similar uh type of deployment they did a great kind of rundown of how even you know hand labeled solutions have all these hidden costs of of validation and updating that that just cause major blockers compared to a weak supervision approach and the final thing i'll highlight which recently came out in in patterns uh was the radiology example i highlighted and the one k takeaway that i want to just briefly mention in addition to the the exciting performance of of actually going from you know literal person months to person hours with the same quality was this idea of cross modal weak supervision where you can write these labeling functions or express signal over a set of features that may not be available at inference time and in this way you can essentially do cross feature distillation or transfer so in this case i'll just briefly give this example and then wrap up the idea was that we had at training time hundreds of thousands of unlabeled data points but they had both image and text they had the x-rays they had the the transcribed notes from the attendings we had the our subject matter extras write labeling functions over the text part but then we trained and we used these these weekly supervised signals to train a model just over the image part so that way at inference time when only the image came in the labeling functions were inapplicable but the model we trained could be could be run so i think this is one other idea of training data and weak supervision not just as a way of getting around this this training data labeling bottleneck but also as a way of of transferring domain knowledge across modalities form factors this was also actually in the google paper if you read it it was about transferring it from slow internal features to high performance public sla compliant features so i think training you know weak supervision and uh these approaches are more than just about getting around labeling bottlenecks but also about this this interesting kind of transfer and i'll just kind of briefly wrap up and say there's a bunch of other operators that i think are interesting that we have worked on and are still working on not just labeling data but trying to abstract other operations like augmenting or transforming data sets slicing or structuring or partitioning data sets and i think in general there's a very exciting research pathway forward and practical pathway forward to say not just how can we label data better but how can we abstract and automate the other operations the very diverse other set of operations around labeling building managing versioning and just maintaining a training database systems in a more you know programmatic weaker higher level and ultimately more practical ways so that i'll wrap up and i'm excited for the q a so thank you so much graham alex and marty for the very interesting talk uh now we will move on to the live q and a session uh so we uh we ask everyone to submit their questions on the live chat paul and i would be monitoring the live chat and we'll be reading out the questions uh to our panel and they will be answering it here uh so uh we uh let's start actually with a question uh to marty paul do you wanna get ahead with that uh yeah i think right before we jump to that um there was a question for alex just in the chat window that he was trying to type to and having trouble so let's just handle that first before general discussion question so there's a question about is this a move towards rule-based systems when we look at things like snorkel um are there uh you know problems with that treatments of them in terms of constraints and other principles so alex do you want to quickly address that yeah great question can you hear me okay by the way my internet is a little spotty but uh yeah i got flagged by as in something in my response was inappropriate and i'm trying to now adversarially guess what in the language models picked up because i think i gave a very stayed and nerdy response but um great question i think at a high level definitely a return to um you know some of the the beneficial aspects of rules or expert-based systems for input right so the benefit of you know uh just you know extend i guess intentionally stating some knowledge as a rule rather than extensionally stating it via you know lots of individually labeled data points is that it's more direct it's interpretable it's modifiable it's auditable which people care about um the the downside traditionally is it's often very brittle it's hard to cover you know the long tail of patterns and unstructured data like text so one view of what we try to do with a snorkel-like approach or programmatically supervision approach is you know let users directly express this knowledge and then try to generalize beyond it using stats learning approaches so i'd say the attempt at a high level is to kind of bridge the two second thing i would say about um i may be a misunderstanding so correct me if i am but this idea of um principles versus constraints uh you know one way that we're working on and that's not that big of a extension is to have the labeling functions that go into snorkel output over labels as the the generic format and in this way you can at least incorporate both what i think you're referring as principles of you know labeling something which is the default semantics right now and also express you know constraints um say by having a uniform distribution over everything but one label for example uh and that would be one simple way to step towards this broader semantics for the the input so we're definitely thinking about that right now and if you have further ideas we'd love to chat thanks alex um and yeah let's uh switch over to a question to start with marty uh's response from and so marty when we look at the work on summarization and think about how people consume information uh why bother to go through the effort of building a chat bot how do you think about information consumption and the role that these technologies play in that more generally there it's being a little finicky thanks for the question at first i just wanted to i saw there's some more questions in the chat about how do we choose the terms to join math school since i was not able to type my responsible answer a little here someone asked if we uh do some training for that and uh actually we use a measure that doesn't require training it's a tf idf measure term frequency divided by document frequency uh roughly and so that sort of inherently finds the most important terms it's a standard term and a standard approach in information retrieval and so that's the approach that we're using for that it's not all that sophisticated and and we could definitely pursue more sophisticated approaches it worked pretty well for our purposes at least so far now to answer paul's question about the chat bot yeah so we're actually really excited about this approach we're really interested in having summaries are great for getting a quick overview of what a news article is about but we'd actually really like to have people engage more deeply with the news and not just and go beyond the headlines and the idea behind the chatbot is to allow people to ask questions about an article with the hopes that that will allow them to engage more deeply so there's evidence that if people interact with information and ask questions that they learn more about that information so that's the fundamental idea behind the uh chatbot okay so it's more sort of self-directed information consumption and learning to ask questions pretty deeply somewhat self-directed but actually the chat bot proposes its own questions as well so people uh in our small study that we performed people would actually click on those questions to get ideas of what to ask and they would and additionally ask their own questions yeah that's great great thank you so much marie so a question for graham um some of the some of the limitations of working on low resource languages is a lack of benchmark data sets do you have any thoughts on how we can encourage the research community to work more on the resource languages given that challenge so that's a great question and and thank you for asking it um i i think there's a number of issues here and it depends on what your definition of low resource language is also i i think if you even look beyond english there is a very large lack of benchmark data sets so if you look at how much the research community and especially the ml based natural language processing community focuses on things like squad or glue or super glue these are all data sets that exclusively um include english recently there's been a move away from this both by my group in collaboration with google we created a benchmark for multilingual learning called the extreme data set x-t-r-e-m-e and there's also a parallel effort by microsoft called the exclu data set and both of these what they tried to do is they tried to kind of expand this benchmarking approach over many different tasks to at least more languages 40 languages uh for example in the extreme benchmark however um that's still you know the top languages uh and if you want to go down even further you can't just collate the existing data sets but you actually have to think very seriously about where you can get data and for some uh for some tasks such as machine translation there is actually some amount of naturally occurring data and for example there's the opus corpus opus that collates a huge number of data sets which makes it very easy for machine translation research however if you if you go even further to like other tasks that aren't machine translation where there's not as much naturally occurring data there's really a paucity of datasets that being said i don't think this is because of a lack of um interest in working on these data sets i think a lot of people who speak languages are interested in working on them so i think one way forward is a concerted effort to kind of gather together resources about what you need to do to create data sets in particular languages disseminating them to people who are interested in working in languages and as i said at the end of my talk i think community involvement is very very important i think involving people who speak the languages to kind of get involved and build data sets is one way forward so sorry for a long answer to a short question but i think it's a very important and interesting one absolutely thank you so much um thanks and uh i'm going to go to a question from uh trumpet bensell which has a question about um when we think about learning from limited data what are thoughts about using methods like gpt 3 for pre-training approaches and how it applies to learning from limited data and pros and cons and i guess before throwing this over to the panel which i think there might be multiple opinions i have my own but i'm going to reserve those um i guess i'd separate out two parts this question they're sort of pre-training with large models in general and then generative models in particular and so like there are other large models um uh like burt uh that are not necessarily generative and so i think we can probably separate those two aspects of just large models in general pre-training does that solve everything for limited data and then generative so i'm going to just open it up in general and see who wants to jump in on this one i could say one quick thing this isn't even related to well it's motivationally related to my work but it's just actually a note on another researcher's uh work and not specific to gpt3 but just around some of these pre-trained language models there's a paper um by some researchers out of facebook recently showing how you know a pre-trained model compares to just uh training on a large amount of of data and uh and then that could be weekly or or non or strongly supervised data and so i think it's it's interesting to note that it's um you know there are many cases especially when we bring some of these limited label data techniques into play where people actually do have sufficient data to with a basic model like an lstm you know come within a point or even do better than on their particular task then you know an approach leveraging a massive pre-trained model and so i think it's it's obviously exciting uh um all of this progress around new transferring techniques is extremely exciting but you know there's no one tool that's right for every job or universally better and there are many cases where we do have tools that could be better for especially for custom tasks uh with weakly supervised or other types of data available that can perform better so there's one one paper kind of showing the other side of this discussion i think is interesting yeah um and uh and just my own personal experience with this i kind of will throw on top of that so i i agree with alex's uh comment that i haven't found one technique that university is great pre-training in general definitely reduces the problem for the label data it does not make it go away in my experience and it's you often have to be aware of how the pre-training was done the particular type of data what you were predicting using um you know techniques of masking or self-supervision actually ends up being entangled with your problem in not obvious ways uh when you apply it to certain cases and so separating those things out some of the the recent work uh like i think at iclear there was a paper from um out of uber ai on the plug and play language models is an interesting way of adding more control on some of uh these massive models that highlights where the different kinds of uh issues that come up so uh i i would also like to add uh something on top of this which is um i think pre-training methods there was actually also a question in chat um but pre-training methods are the development in nlp the past you know three four years or maybe 10 years if you go back to word devec which is also a pre-training method but the um the advances are amazing um they're great i don't think they will solve all of your problems but they'll solve a lot of them including domain robustness scaling up with very few examples i think gpt3 in particular is interesting because they took a very different approach which is that you're not taking the model and tuning it you're taking the model and prompting it to try to do something and there were also some interesting uh works on demonstrating how this can be used to for example predict factual knowledge in things like this uh before the gpt-3 thing the problem with this approach it's a great approach the results are very interesting the problem with this approach that i see now is that what if your model is demonstrating undesirable behavior um what if your model is for example saying uh very offensive or racist things like some people pointed out about gpt3 how do you fix it and the um the answer is more clear for a previous paradigm of you know taking a pre-trained model and fine-tuning it because then you could fine-tune the parameters of the model to you know heavily penalize it whenever it tried to do something like this it's much less clear how to do that when you're just prompting the model and asking it to to do things so i think it's very interesting a very interesting development but there's some things we need to think about seriously there okay great thank you so much everyone um so there's a related question actually to graham from fatima and the question is what are your recommendations on including information about the structure of the language when doing multilingual models and also how does that relate to work not only on human languages but in more structured languages yeah so that's a great question i actually uh very briefly answered that in the chat also but um we we have a little bit of work on this um one way that people have been incorporating structured of language structures of languages is through tree or graph structures um these uh are particularly useful for programming languages because the syntax of the language is very clear and um we know what it looks like precisely but it's less easy for human languages where the structure is not so not so clear so um so one of the questions that that came up was on expertise for labeling so uh alex when you think about domain experts and how to get them to write weak supervision functions in general and the space of weak supervision functions um how do you think about bridging that gap between experts and their domain knowledge and you know getting them to actually write things that reduce to functions that are useful for labeling um do you have insights to share there of particular experts and how to make that work more generally in other domains yeah well i mean i'll start at a high level and i think it's a fascinating question and we've played around with several different you know modalities of hey you know write write rules or or more passive ways of collecting data give natural language explanations is something that my colleague braden hadn't had talked to the paper on a little while back data augmentation is another way of kind of injecting domain knowledge about um you know invariance into the data that is becoming quite popular now you know express which you know parts of the data can be perturbed or transformed and should not affect the class boundary um even so there's i think a whole bunch of rich ways that people can interact i think one of the common things maybe this is redundant to express but is that you know the the paradigm of uh if you have a subject matter expert the only way they can interact and inject their information is by labeling individual examples it seems kind of comically inadequate when you actually move into real world settings where these people are you know have very minimal time and you know and and uh asking you know asking questions um instead of just having a direct answer you know if you wanted to get a subject manager to communicate one high value feature you know think about how difficult even theoretically that is to inject in a modern machine learning model these days uh you know um and so i i think that's just motivation for being you know for more creativity here and with respect to some of the approaches we've explored obviously asking um uh folks to write down rules um that they have uh is one interface that seems to be fairly intuitive you know it's a way that you can have highly technical subject matter experts whether they're uh doctors or um or you know uh technical analysts and a government agency or or or you know legal analysts etc um that are not programmers but do have very technical knowledge express things flexibly often you need to provide some kind of primitives or building blocks they express these heuristics or rules over so there is sort of a kind of new type of maybe feature you know future engineering but you're just providing some basic building blocks like bounding boxes and images or features and images that they can then express heuristics over that seemed to be one promising technique of kind of priming that process and then also the ability to leverage existing knowledge resources which especially in in both the open web and in large organizations are very rich sources you know people often have in both medical and government areas for example there are there have been massive investments in ontologies and being able to leverage these ontologies for example to provide input signal to modern ml methods uh seems just like a no-brainer given how much information and investment has been stored in those so uh yeah both both both new ways to accelerate expression of smes um of their knowledge as well as just leverage the knowledge that's already been expressed and codified um thanks uh alex it sounds like there's some opportunity for hci researchers then to also think about how they design uh that space of bounding box and other kinds of inputs uh ahmad interactivity yeah is a huge huge thing that i think we should see more of an ml of turning into more of an iterative development process rather than a you know label huge training set one and done type thing that's a broad area lots of folks are working on i think it's fascinating great thank you so much alex so uh our last question uh i have a question for for marty uh do you have any thoughts about what would be the most interesting problems in the summarization space or in the building summarization systems or other systems like summarization without having access to large amounts of label data so i was just wondering if you have any thoughts about what would be the most interesting next steps or spaces that you think we should be thinking about uh thanks good question i i guess um i think i'm not very good at most but i think an interesting problem is really that many people have stated to us to our team is finding the different threads within a story so we actually build stories new stories from different events and stringing them together but what are the different threads within that story so if there's a story about say i don't know a worldwide pandemic that's a not a monolith right so there's a lot of different strands within it and it's very hard right now to even identify what those are and to classify them and talk about them they have different angles to them there's an economic story there's a political story there's a health story and many sub-stories within that and how do you detect those and then how do you display them in an interface to help people better understand the news and understand the nuances within the news yeah i said thank you so much i think i think that makes a lot of sense and in many times we just like think of summarization and producing like a short paragraph summarizing something and that leaves behind a lot of cretchiness and therefore means underlying information of what we are trying to summarize great i think we are yeah i think i think we are running out of time so i would like to thank our speakers again thank you so much for the very inspiring uh talks i personally learned a lot from them we'd also like to thank all that indecent audience for engaging and asking questions and sorry for the glitches about censoring out some of the comments we have to figure out offline why the term tfidf is deemed offensive that concludes our session about the frontiers of machine learning event will continue today we will have a 26 minutes break now and then we will come back with the last session on the climate impact of machine learning so hope you can all join us there again thank you again so much for everyone that attended and thank you so much for our speakers thanks everyone for joining thanks to the speakers thank you you 