 We present a quadruped locomotion controller that can tackle motion synthesis in dynamic environments. Our controllable agent can effectively synthesize reactions to dynamic environments without the need for any labels Common ways to solve motion synthesis require extensive labeling of the actions and transitions. However, because of the movement complexity, it is impractical to capture and label agent’s reactions to all plausible external perturbations. These complex reactions are best modeled with a physics-based agent that interacts with the environment. The interaction with environment makes the agent hard to control, often producing unnatural and jerky movements. We propose a GAN control adapter, that enables our agent to produce natural movements while following user control accurately. Through deep reinforcement learning, our agent gains the ability to recover from unseen scenarios, making it suitable to synthesize meaningful reactions to external perturbations. Our method consists of three stages: Imitation Learning, GAN Control Adapter, and DRL Fine-Tuning. In the first stage, our agent learns to perform natural movements by imitating the reference motion, this prevents the agent from performing unnatural or awkward movements. In imitation learning, we use the policy network of Peng et al., which consists of a Primitive and Gating Network, by imitating the reference motion we obtain the natural action distribution. We then adopt user control by using GAN framework, here we consider our gating network’s output as fake samples, and the learned primitive influence as real samples. During inference, we replace the reference motion to enable user control GAN Control Adapter allows the agent to follow user control, but when we unexpectedly change the target speed to zero, the agent falls. Moreover, the agent breaks when we introduce external perturbations. After fine-tuning the agent with deep reinforcement learning, it can handle drastic changes in user control and recover from unseen scenarios. Not only that, our agent can even withstand multiple external perturbations at the same time We now compare our approach to other baselines For the first comparison, we consider the user control as a goal to optimize in deep reinforcement learning. We then visualize the action distribution in the 2D space. From the clip, we see that the agent can follow user control, but produces a different action distribution and perform trot at unnaturally high cadence. Adding a control adapter by minimizing the standard L1 distance to adopt user control, leads to a better estimation of both motion range and action distribution. However, the agent still performs trot at a faster speed instead of canter. Replacing L1 with GAN, our agent preserves the natural action distribution learned from the imitation learning. The wider movement range and similar action distribution shows that our agent successfully adopt user control. Using GAN Control Adapter, our agent produces movements within the natural action distribution. This is visually shown by our agent automatically switch from trot to canter when the target speed gets higher. A more recent approach controls the agent through an existing kinematic controller Despite its high-quality motion, its ability to accommodate unseen transitions is limited by the availability of action labels and motion data In contrast, our method learns to accommmodate the unseen transitions by exploring the action manifold, hence does not require action labels or additional motion data With our agent’s ability to follow user control, users can directly attach navigation modules such as path-finding. The navigation module then drives the agent by emulating user control over agent’s move speed and heading. Other navigation modules such as ray-sensor also drives the agent through the same approach. This allows our agent to retain all desirable properties while accommodating different navigation modules. To synthesize motion clips in a dynamic environment, we only need to first loosely model the environment, record the agent's movements, and finally replay it as animation. In this simple animation, our agent tumbles around to compensate for the boat's movement and slide when walking on an ice patch. In summary, our physics based agent learns the natural movements through imitating a reference motion, enable user control through GAN Control Adapter, and recover from unseen scenarios through deep reinforcement learning. As a result our agent can produce natural movements, controllable, and adaptive to external perturbations. Making it a powerful tool to tackle motion synthesis related tasks. 