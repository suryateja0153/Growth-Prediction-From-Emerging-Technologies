 Thank you very much. It is my honor and pleasure to be here today and to speak to you about our research  at UC Berkeley towards AI. I have to say that I haven't been in an audience that's so impressive in quite a while. I rarely have to give a talk both in front of the leading young graduate students and senior executives of a major corporation at the same time, so I have certain jokes that work with one set and jokes that work with the other set and  I don't know if it's going to go over. But I was told I should give a technical talk, and so that is what we're going to do today. And I have to say, much of what my perspective and what I'm going to talk to you about I think really amplifies and is aligned with the excellent talk that Yoshua gave earlier, so I think we'll see a lot of common themes arise. Let's start with what works, a cliché of what works, of course. This is what most people focus on--the IID, supervised learning case. Everything is great if you like to recognize cats and dogs on the internet, we know how to do that. We can train models, we can train convolutional networks with backprop, we can train them on ImageNet. If you'd like to recognize a dog, you can probably get it to work it arbitrarily well with enough training data But then I actually try and build a product or do something in the real world. Like I want to build a doorbell that lets my dog in and not the raccoon that otherwise enters my dog door--and that does actually happen in my house, unfortunately. Actually, it's a cat door. I didn't actually have time to build this, but if I had, I probably would have discovered that--excuse me. I would go off and just plug in the doorbell camera, run it through the ImageNet model that was trained on ImageNet and, you know, I mean, we get perfect performance on ImageNet, so what do you expect would happen as I-- and I'm ready to start selling this because there's a need for this raccoon-proof cat door or dog door. It's just, it's probably not going to work, right? Because even if you take a regular video, run it through a model that works perfectly on ImageNet--you know, we were told vision is solved, deep learning is here, it's great. But you make a small little change, like just trying to take a model that you trained on images, and run it on the frames from a video-- and frames from a video are images last time I checked, usually. What's going on here? Many of you are familiar with where I'm heading and this how to learn in non-IID conditions is the key problem of our time and has been around for awhile as well. The models we're training, that are exciting and so successful, are still far from anything that I would call a human vision system. They do exactly what they were told to do, and they're very good at it, but they don't actually learn the way that people learn. If I give you an example of a new category, and I give you the examples on this slide, and I tell you I call this a 'blicket' in some foreign language, well you'd have no trouble recognizing blickets in these images, in black and white images, in videos, probably you could even recognize it from all sorts of signals. But conventional machine learning approaches suffer and they do exactly what they were told. So they were told to learn that these are the category dog in this case, and if you showed them this image, well, to a human vision system, it looks like the same object. Or this image. Or even this image. That's because human vision systems have learned to generalize over all these kinds of variation that by default conventional deep learning systems aren't able to do. So we often call this data set bias, or robust learning. We want to have machine learning methods that are able to match human-level performance and not be surprised by new environments, new weather conditions, new datasets, and so on and so forth. So that's a bit of a teaser just to enter into the main part of what I want to talk about today: beyond supervised AI. And there's three themes that I'll articulate until I run out of time, and then I'll have to just skip to the end of my talk probably. And the first, as I've already highlighted, is how do we build models that have the power of deep learning, that work well in IID settings, but avoid surprise and avoid the problems of domain shift and different environments. That's gets to how can we avoid needing full supervision in every future condition that we have. Even more beyond supervision, how can we learn without and labels or without any goals, especially now in a setting where we want to learn to act using what would conventionally be called reinforcement learning, but really if you think about it, if we want to have human-level quote unquote reinforcement learning, we need to maybe fundamentally reassess what we mean by goals. Or we need to have a real generalized notion of goals. Goals need to exist over very long time scales and there may be purposeful exploration that doesn't even have explicit reward. Things like that. So I'll talk about work in that direction. And last but not least, pushing a little bit towards that c-word that Yoshua is so boldly articulating, or consciousness. I do think there's a relationship between explanations, models that can tell you, perhaps in words, why they think they're doing what they're doing, and that you can maybe even influence and instruct with words, or advise with words. I think that's one of the more exciting future directions of AI and work that we're doing at UC Berkeley. So in the time that I have left, I'll try to give you some depth into each one of these areas and see where we go. So adaptation. The key challenge that we've articulated and have been trying to face in the last several years, is how can we learn and exploit the power of models that perform a certain task? Ok I'm going to switch now from dogs and cats to bottles and cups, but it's still equally simple. You know, we can train a model that can classify dogs and cats or bottles and cups, but then I go out and test it in a new environment and I learn that, unfortunately, in the representation that I learned in the first dataset, which looks, from what I can tell on this slide, it looks like I scraped it off of Amazon, or it's ImageNet, or something like that And then in the real world, my robot goes out and experiences a new kind of sense or a new kind of weather condition, and maybe even a new kind of label, just different kinds of animals in the environment than were there a year ago. All sorts of things can lead to this subtle shift, but somehow humans can overcome these shifts. And so we want machine methods to do so as well, and I don't think we've solved this problem by any stretch, but I can at least give you the hint of one of the main themes that we've developed over the years, and other labs have been working on this as well. And it ends up having a close connection to GANS and adversarial methods. We call it domain adversarial optimization. Cause we'd like to somehow reduce the discrepancy between these two conditions, the red domain and the blue domain in this example, and how can we do that? And how can we do that in particular without having any actual labels, any additional target labels in the target domain. I don't really want every week to have to go and draw bounding boxes around images  from my doorbell camera so that it doesn't let the raccoon in anymore, even though it's changing from fall to winter or something like that. That's not very acceptable. And there are many different versions of this problem statement and other related work. I'm just going to tell you the one perspective that we've been looking at most recently and over the last few years using domain adversarial optimization, or sometimes we call it domain confusion. So we don't have labels in the target domain, and some people think, then, you're lost, or all you can do is just try and hold your hands and not be more conservative in your classification. But somehow people do notice when the sensory environment changes, they can project into this new environment, so how can the machine learning method do that as well. Also, we're inspired by very classic methods for metric learning, which say, if I knew ahead of time that there are corresponding instances in two different projections, well I can learn a representation that puts them together. And the deep learning equivalent of that would be Siamese networks. So how do we do this when we don't have labels in your target domain for the task, or paired instances. Well, the general idea is you want to align distributions, and we have to make certain assumptions that are feasible, and there's a lot of future work or ongoing work for cases that it's questionable. But let's just assume for now that we do essentially have a view of the entire distribution of the data and the classes are reasonably balanced on both sides. Well if I could figure out how to align the whole distribution, I'll probably come pretty close to aligning instances, and if I can do that, theory and practice tell us that I'll generalize better in new domains. So how do I do this alignment without labels? Older work would often fit parametric models, or kernels to the data, and the most recent work, again with aligned inspiration pardon the pun, to GANS, we use an adversarial technique. We say, well we do actually have labels in the target domain. What I said is we don't have any task labels in the target domain, we actually have the red and the blue labels. Now we could assume that there are variants of the problem statement where we say that that's not the case, but I think it's pretty reasonable to say I know which camera I got my image from or I know what month or what dataset it comes from. So we actually can use an adversarial optimization on the domain classifier, where we actually take the loss from the domain classifier and inversely optimize it using, again, style approach and update the underlying representation to maximize the confusion of the domain classifier while still maintaining performance on the source classifier. So this is a line of work that suggests that we can have the power of deep learning methods and apply them as well. It's like the solution to the limitation of deep learning, is often more deep learning, I like to joke, but done the right way, not just blindly adding more data, but trying to structure the problem in ways that give you the right inductive biases for the future. So this method we've developed over the last few years, applied it for classification, semantic segmentation, most recently object detection, and the idea is indeed to align these representations, align these domains in representation space. Here are some examples of what it looks like for semantic segmentation. In autonomous driving you might want to train on computer graphics in simulated environments because it's much more fun to crash your car in the game world than in the real world. But it's notoriously hard, and surprisingly, performance can be quite poor if you train a comnet to recognize, for example, images in GTA, Grand Theft Auto, or CARLA, and apply it on the real world, the performance drops significantly, the models overfit to weird quirks in the rendering algorithm, for example, that we don't quite understand. But our method can automatically discover that and overcome it. So, you'd like to have the kind of results we would see in the ground truth in the upper right, and if you just naively train in simulation and test in the real world, for example, in cityscapes, you get the image on the lower left and our adaptation model automatically aligns the Mostly it's manipulating the kind of textures of this model. One of the main limitations and avenues of future work is how to have this kind of transformation not only work essentially on the texture of the scene, but to also have it on the structural properties of the scene, whether they're people or writhing crowds, or singletons, and things like that. I don't have any slides on that, but that's work that we're hoping to be submitting quite soon. And we've recently applied this also for detection. A particular technical detail for those of you who might be familiar with this work is to simplify the Cycle GAN and add a discriminative loss to optimize its performance. And also detection performance has been notoriously bad if you don't do anything. If you just train on synthetic data, and try and test a detector on real images, your performance can drop from 70 to 30, which is really remarkable. And the methods that we're reporting--a baseline method from Zurich last year, was reported that got 39 mAP. Our methods are now into the 50s. So you can see, there's still work to be done. We're about halfway to solving this problem, and I think that understanding not only these low-level transformations, but also the high-level structural transformations between scenes. Some environments have no trees or are very crowded, those sorts of things I think will get us to the end of the performance. So this was a snippet of building models that leverage deep learning to work in novel conditions, to adapt to new domains, and avoid surprise. And that's important because it means we don't need so many labels, right, because I think in industry now the solution is fully supervised labeling. This is why we have annotation companies with billion-dollar valuations right now, because people think there's no alternative but to label all of the--you know, that there's going to be some annotator in India or China or even the US labeling the images off my doorbell camera every month when the weather changes. I mean, I don't think that's how it's going to work. So we need to build self-supervised and semi-supervised learning agents. But I'm going to go even further and say that the whole notion of supervision is sometimes wrong and I'm going to make this argument especially in the context of reinforcement learning. That we don't really know often what the goal is in life, and yet all of our at least recent deep learning, reinforcement learning work-- if you go back far enough, certainly in AI, the broad problem statement has been articulated and was clearly addressed. But most of the recent work in RL assumes I have a very very short term goal signal, and my task is to optimize against it. And that doesn't characterize much of life, like human life or human-level AI. So we need to push towards learning agents, agents that learn to act, perceive and act, that are driven, we argue, more from exploration than from an explicit reward signal. So let me tell you about that. So as I just said, reinforcement learning is great and we've all been excited by the progress that was shown in the recent results, but the methods that actually work on the leaderboards all assume, or generally all assume, dense rewards. They actually get a signal almost at every time stamp, or they have little shaped rewards that give nuggets of feedback as they go through the environment. At least I would say that characterized the first and second generation of methods of deep reinforcement learning methods in the field, and we're not the only group who's articulating this perspective that rewards are sparse in the real world. Here I am sitting on the stage. What reward function am I optimizing at this very minute? There's no little dollar sign here that's increasing, that my funding is going up or it's going down or, I mean the lunch will probably be very good. I'm looking forward to that. There's going to be food and probably they'll feed me if I give a good talk, so I like that as well. But in general, humans--or consider the choice of going to Berkeley for graduate school, or to a fine university anywhere in the world. Is it worth it to commit, you know, five to seven years of your life to get a PhD. How do you choose to make that decision? Your reward is going to come much later in life, and I think it's really not that we're optimizing a specific reward. I didn't come here with a single goal in mind to give a talk. I make decisions. I have a lot of hunches about where I should explore to find new ideas, or what skills I'm going to acquire, or what general relationships I'll build that will be good for my career and for my group. And I mean, this is how cognitive scientists also have said children learn. If you look at, if you have children, this is where it's useful to be a parent. Often being a parent is a trade-off between time commitments of personal and professional lives, but having kids actually reminds us every day of the limitations of conventional machine learning methods and how easily few-shot learning-- And I often have to tell my grad students who don't have kids that this isn't how people learn. They don't learn from ImageNet. So we want to have models that are inspired by the same philosophy. This is not a new idea in AI, that exploration should drive action learning. There's a long history of great research and our work sort of is in the right-hand side of that list of uncertainty-based models. The key idea is that we've explored over the past few years-- this is the work of Deepak Pathak and other colleagues at Berkeley, and Deepak is now on his way to be a professor at CMU--is to drive reward from uncertainty of prediction. You actually like to explore and discover new things, at least within limits. So let's see how that plays out, this curiosity-driven action learning-- representation learning driven by action. So we have a video game, we have an agent. The agent sees something in the world, the agent knows that it can move around. We're going to be looking at Mario Brothers; I'm sure everyone's seen this. You go down, nothing happens. So that's something. So what does Mario think is going to happen when he observes this environment and he chooses to pick the up button? Well, he goes up, and he comes down. In this case, the prediction actually was correct, matched reality, but now here, he's going to predict going up, but he's actually going to get stuck up there. And that's interesting, and children find that interesting when they experience it. They attend more to events like that. Oh, surprise! That's a signal that is prediction error, that is curiosity. So we've explored in a series of papers starting a few years ago, ICML '17, how to use curiosity intertwined with deep representation learning, and that turned out to yield models that were very high-performing on a lot of the conventional reinforcement learning benchmarks that could now be solved without any reward at all. So here's how this proceeds. We start off with a current image and we want to learn a policy that will pick an action and that of course will act in the state, in the game, and we'll get a next image and we'll continue from there. We'll be building a prediction model that's going to take the action and the current image and try to predict, "What does the agent think is going to happen next?" And the lack of success of that model is going to generate a positive curiosity signal that will drive learning in the model. So we're going to pick models that learn-- we're going to pick actions to maximize curiosity. Curiosity is this internal reward signal, this curiosity reward. But people had talked about doing that for a long time. So what's new about what we did in the last few years-- The key thing is to learn a representation in which you discover what parts of the world are relevant and then be curious about them at the same time, because in essence, it's almost, it's a kind of attention, but it's focusing a representation that says I need to learn what parts of my environment are relevant to my task and for this we have an auxiliary task here that's to predict the action that will be chosen from the current image and the next image. That drives learning of the feature encoder in the model, and in that encoded representation, the curiosity on the prediction model gives us the reward that's sufficient for learning. If you did it without the feature encoder, without the auxiliary task, the model would end up always looking at some small random variation in the world that wasn't important to your task. So these two ideas together and the extensions of them that we've been exploring in the subsequent papers that I can refer you to are sufficient to actually get a lot of the same kinds of results that made such a big splash with deep reinforcement learning and highly-supervised settings. So here's how the performance of this game is without any reward at all, just with curiosity. And I'm going until 11, right? I think. That's my counter. Yeah, good. So, there we go. It starts off not doing too much. Kind of bouncing around, curious, looking around, and then all of the sudden, POOF! Moves forward. Now, after a random experience like that, here's what it looks like. It actually learns to play the game pretty well. And it's surprisingly learns to go through several levels. And for awhile we were wondering, why does it not like to die? We didn't actually tell it not to die, but it turns out it has what many of us seem to have that we call, it's an English expression called FOMO--fear of missing out. Like being alive is useful, you get more curiosity the longer you're alive, and it seems like this agent is as well. So we never actually programmed in that it would get a penalty for dying or benefit for not dying, but the longer it's alive, the more curiosity rewards it gets to experience, and so that turns out to be a sufficient justification to stay alive. So in this past ICLR, we took those initial results and scaled them up and showed that not only did this work in this Mario game, but it does across a wide range of challenging environments, these Atari games, works on all of them as well. And so it's not just a specific single result, and not just video games, but also robotics. Here we have simulated robots and real robots. So here we have these ant-like creatures that are trying to learn to walk. They can of course learn with a reward signal on locomotion, but even just being curious about your motion is sufficient to generate walking-like behaviors. Or navigation in virtual environments. The model can learn to explore because it like to see new things and discover its way around a maze. And once you've discovered all of these behaviors that are sufficient for exploration-- I'm not going to have time to cover this in my talk-- but you can certainly use that to then build up little skills and the skills will end up being useful for solving a certain task. So at some point, there's going to be a reward. I mean, somebody's going to tell me that lunch is down the hall and to the right in the lunchroom and I better get there quickly because all the good stuff is going to get taken quickly by the other executives. They're going to go there and get the food quickly. No, the graduate students are going to go get the food quickly. And I will have explored this environment and then I'll know very quickly how to get there cause I'll have these skills that I created through exploration that are useful to actually then solve a task in a few-shot condition. What are some of the limitations of this? Well, it turns out the model might get confused by fake news. If you have an environment where there is structure in the environment and it can control the structure, such as this television in this unity environment, the model just goes there and watches it forever. So we still need I think the sarcastic versions of curiosity to handle this case or maybe this is like humans that get stuck in gambling environments or something like that. Here's an example of this working extension of this method that was in collaboration with CMU on real robots using exploration to learn underlying behaviors that are then sufficient to drive more structured exploration. And then the last example in this line of work is moving toward multi-agent exploration. A very simple and interesting example where we take pong and actually run two curious agents against each other and see what happens, and they actually end up learning to play pretty well. But what's interesting, they never actually learn to win very well. They actually learn to just keep playing over and over and over. In fact, at some point it breaks the simulator because they're both perfect. So nobody told them that the score mattered, so they don't actually optimize for score. They just keep playing because it's fun. At least, that's my interpretation. Ok, I think I'm just on time. I'm going to spend the last part of my talk now diving into explanation. We covered adaptation, how we can not need supervision in new domains or avoiding surprise. How can we have sort of self-supervision and exploration in robotics and RL in action? And now let's really push up towards the integration of language and vision and compositionality and say how can we take these models that are often criticized as being black box models and see if we can have them provide more trust and ultimately acceptance? Because we can see that they're learning for the right reasons and have intuitive structure. They match our expectation of how methods should act. And XAI research I think is quite interesting right now. The first thing to say is there's lots of different meanings of the word explanation, and I actually sometimes-- you'll see in the work I present here--will refer so several of them. So there's explanations that are assessing causality, explanations that might be useful if you really want to debug a system, and then there's explanations that are more--maybe more justifications that are more--like what human explanations are like. Again, if I say why am I here? What's the purpose of giving a talk? Well, I'll say I'm here because I want to share my group's research and increase its impact by getting all these great ideas out in front of such important people. I don't know if that's really why I'm here. I don't actually know how my brain works. But do know that these stories are very useful for humans to tell each other and I'd like to have machine agents that can also have that kind of capability. Another interesting thing about XAI research is you can often get into very heated arguments with theorists about XAI research because many of them think that interpretability or explainability is a very well-defined concept. And it is if you choose that. For example, in an IID setting, interpretability means that I'm selecting between different model families with different levels of complexity. Classically, we have old methods that are rules and new methods that are deep models, and there' always a trade-off between accuracy and explainability, and maybe we can try and push the frontier out a little bit, but you always have to pick one. Do you want your system to really work well, or do you want it to be explainable? The famous debates. Yann LeCun I think always says "Do you want your surgeon to be explainable, or do you want him to work well?" I'd probably pick the surgeon that actually works well if I had to choose, - but I really think it's a false choice, and what's interesting about explainability and next generation AI and deep learning is not the IID setting, and not the setting where I assume essentially a closed-world without a human in the loop, but the setting where we're actually building models that have to perform in new environments- they have to generalize outside of their capability-- and that they're interactively being trained with, by and for humans. So when I think of XAI research, I'm not thinking of a strict model capacity interpretability trade-off of old, I'm thinking of models that have to show their work, that can be trained on whether they showed their work. That's more like how my kid learns. When he has to do a problem set, he has to show his work and the teacher grades his explanation. And often gives him feedback, saying you got that part wrong, or you got this right but you got it right for the wrong reason, here's some additional loss. Please learn. So once you get into that world, where explanations actually provide even more training signal than the original problem, that of course means they can outperform. So in some senses this is very simple. This is just saying that a certain class of multi-task learning is really explanation-based. So if we have saliency or additional privileged task training, that that models can outperform, and those are explainable models. Another theme that I'll show you the recent work at Berkeley on is as we build XAI models that allow us to take black box, deep learning agents and relate it to language--sort of the extension of all of the great work on image captioning that we did over the years, that's now useful to provide, essentially, captions for decisions. These are justification style explanations. But we can actually turn that around and build what we call advisable AI systems. We started off learning to explain, and now we're moving towards a paradigm that we call explaining to learn. So that humans can give advice to systems, and the capacity for the system to explain itself, become useful directly for that supervision. And last, but not least, I have a few results to support this as well. Explanations reveal model uncertainty. If we're thinking of an AI system as not being a purely autonomous creature, devoid from all human interaction, but indeed AI systems are often agents and tools that work for people and that human managers have to decide whether or not to accept or agree with or scrutinize, the explanations are useful to reveal a kind of model uncertainty. At least a useful qualitative model uncertainty that if the model shows an explanation that just doesn't quite make sense to a human, well the human's going to be more accurate at knowing when to accept or reject the prediction of that model. So I think I have enough time; I'm hopefully going to be able to touch on all three of those and then have a few moments for questions. So let's start with I think the simplest kind of explanation model that I think's very, very popular in the field now. That's salience. The idea of salience, or introspection. I'm fond of the RISE model from colleagues on our DARPA XAI team at Boston University. It shows which parts of an image, what was the evidence, which pixels were used to make a decision. That's useful if you want to look at an image and see well, if I was going to classify that as sheep, which part of the image would matter? If I was going to recognize it as a cow, which part of the image would matter? And strictly speaking, these already get us off that kind of curve hypothesis that said that adding explanations will always reduce performance, which is what the theorists would argue if indeed this was just a model selection story, a model capacity story. Because here, these are added onto the existing model. The model doesn't change its performance at all. If anything, if you do a few smart things, you can make the performance to go up, but I'm not going to talk about that. Single image salience is definitely the most common paper at ICCV on XAI research right now. There's just an incredible industry in this. I think it's useful, but I'm not terribly excited by it. I find it pretty limiting to say the explanation is just a heat map of the image. And here are pixels that matter, and here are pixels that didn't matter. Because for most interesting tasks in AI--certainly future AI--it's not enough just to say here's a single heat map. You know, I'd like to have--so for example, if we have a really complicated task, like this type of visual question answering task from the CLEVR dataset-- what number of other objects are there of the same size as the grey sphere? If I said, "Hey, I have an AI system that can solve that problem. Look, here's its heat map. Here's the things it was paying attention to. It kind of looked at almost everything that was useful." Do you understand how this thing works? Just from that heat map? I mean, I don't. I don't really know whether it's going to work well in a future condition or if I should trust it or not. I'd like to have a notion of an explanation or salience that revealed the internal processing of a model. And I'd like to have the model actually have some sort of structure and compositionality for me to actually think it's a useful explanation and a useful model, actually. And so this part of the talk is really both about explanations and an argument for multi-step saliency for explanations, but even more fundamentally, it's echoing some of the points I think that Yoshua made earlier, maybe in a slightly different form, that we want compositionality and sequential processing in these deep models. There's probably going to be a lot of work that still needs to be done in this to find the ultimate solution, but here we're going to see methods that learn to reason about how they should answer questions and have compositional structure in the model that ultimately answers these questions. Because if I'm going to--I think most people in this room, if they're going to solve this problem themselves-- they're first going to look for the grey sphere and then they're going to have a second step where they say "What are the other things that are the same size as that first thing?" And then they're going to hopefully come up with the answer, maybe count the things that were attended to. So this compositionality, I believe, is a hallmark of next-generation AI and the kind of future topics that are going to drive great results going forward. Just to reiterate this point, the current approach of deep learning is often a monolithic network. I have a single--and it does have compositionality inside of it-- I certainly do agree with that in the ways that have been put forward-- but not necessarily enough compositionality to really handle all of the kinds of variation that we have in the real world. I don't want to have to have training data for every possible structure of objects in the world. I don't want to have to have training data for the power set of all the conditions, for example, in this CLEVR dataset. And that's what CLEVR was designed for. Training a monolithic network on this generally is quite limiting. So we want models to be able to generalize compositionally-- language has that property; I argue vision has that property-- so that we can train on certain configurations and then at test time to see completely new conditions and have a model that's going to perform well in that setting. Now in the old days--I mean, like, before almost all of you were born who don't wear suits and work for Samsung--like in the eighties, a computer vision graduate student would have been expected to write a computer program to answer this question. It would be like a visual routines. So we have a small program that will look and find the blue cylinder, find all the objects that have the same size, and then describe the answer. And what we're arguing for, and what we've been developing in a line of work that we call neuro-modular networks, is that we want deep architectures that have this same property, both for explainability but also because we fundamentally believe it will generalize better in the real, complicated world. What does it mean to have a module or a program in deep learning? It's pretty simple. It just means you have a layer or a part of your network that performs that function. In code, we have texts. In deep models, we have layers, for example. But if we had two different tasks. Each image here is a different task, is a different question, and from the question a human or our learning-to-reason agents decides a different program is necessary to run, that just means we have different layers. We have layers that are dedicated to comparing the size of things. We have layers that correspond to different colors and shapes. That means that we just assemble them together and learn them jointly. This is actually not harder than learning a monolithic network. There are fewer constraints but there are enough constraints to actually learn all the weights of this network end-to-end. So not all of the weights are tied together all of the time, as they would be in a normal monolithic network, but enough of them are that we can learn. To really put this together, not only do we need to learn to ground the modules, we need to learn to reason which modules to use in different settings, and that's what we've done in our most recent papers. I think this was ECCV 18, called End-to-End Neuro-Modular Networks, where we have models that look at the question or look at the task and have to reason about what are the series of modules that are going to be useful to solve this task. So the method looks at the question and decides, "Ah! First I should find the ball. Then I should check to see things that are the same size as the ball. And then I should count." And when this thing is trained and running at inference time, it also outputs those attention masks that are the bottleneck between the layers. So you can treat this as an explanation and say this is really what the same size module was looking at. So we can debug whether or not the first module got it wrong, or the last module got it wrong, and we can show this to a human and say, "Here's how this model works. Do you think it's doing the right thing? Is it generalizing well?" And so on, and so forth. So these models do perform well on the CLEVR dataset. They also perform well on natural datasets-- the plans don't turn out to be quite as exciting as CLEVR because of CLEVR's designed for this. But we think it's really moving in the right direction, that we can have deep learning models that have internal compositional structure, that therefore are explainable, and reveal what their processing is to humans, who can then decide whether or not to correct part of it or make sure it will work better in the future environment. Alright. So I still have a few more minutes and a few more things to talk about. Actually, that was just half of our XAI story, to a first approximation. That was the compositional, causal, neuro-modular story, and also very excited about the very complementary perspective, which is-- and eventually we have to combine them, and we will, but let me show you where we are right now-- which is, I would just like to tell you a story about what my deep learning agent is doing. I would like to make a decision, like labeling an image with a category, and then tell you what would an expert have said was the reason to do this and where would they have grounded the evidence in the image. We call this generating visual explanations over the years. And of course, it starts with just potentially a deep model that doesn't even have any of that great compositionality that I outlined earlier. And of course, we can take deep models and transduce the internal state of that model to text, right, and train it-- and here we're actually going to train it in a fully-supervised setting, with fully-supervised, expert rationales-- and have the text that's generated not only be relevant to the image--well, that would be a caption--but also relevant to the class. So a visual explanation or a visual justification is text that's relevant to an image, and also to a decision that was made on that image. And I'm going to skip over this in the interest of time, but you can train this both using essentially image captioning losses and also discriminative losses that ensure that the text actually matters to the decision. And so we can get output where we have a decision that this is a mallard. We can tell you what a human would have said would have been the right reasons to say that. And if you see that it's relying on something that just doesn't make sense, well that gives you good evidence to say that maybe I shouldn't accept this answer. And of course, then the model can go back and rescore itself and say, "Wait a minute. I should find evidence that's grounded in the image." I'm going to skip over this in the interest of time. And we can also extend this model to not only provide an explanation but also counter-factual: why do we not it's something else. Well it's not a scarlet tanager because it doesn't have--I can't find the evidence in the image for what an expert would have told me would have been the reasons that I would have called it a scarlet tananger. I have 90 seconds left. I'll say we do have some user studies that show these types of explanations are useful to humans. And I want to close, or at least do the last minute, on going from explainable AI to advisable AI, and we're doing this in the context of autonomous driving. First, I just want to say that the ideas from the first two parts of this talk now weave themselves into this part, into our work on autonomous driving-- this is work led by John Canny and some of his students at Berkeley-- where we have simple versions of the neuro-modular architectures. We're really going to rely on object detectors and have some causal flow of object-based attention in the model for autonomous driving. And also, text-based or heat maps. This example doesn't show an object detector. It's just a heat map, but we have both. And textual explanations, where we have datasets of essentially driving instructors who explain why a human driving decision would have been made in a certain way. And so we can have the model also output that text, and it's interesting. It's hard to say what--it's hard to prove this is valuable or not valuable. I'm just going to say it's interesting. But I'm really interested because we've achieved this, and now we can achieve this-- and this is the latest work the John reported at CVPR this summer-- of going from explanation to advice, where because we have something that transduces the internal state of the deep architecture into texts, we can now turn that around and have users provide text that are going to influence the internal state of the deep learning system. So for example-- I'm out of time, but let me give you a quick anecdote-- It turns out that training data, the model by default didn't learn to pay attention to pedestrians very much, which is really surprising. But it turned out in the data the model was training from, pedestrians always were in crosswalks. So it just relied on the fact that it should stop at a crosswalk. A jaywalking pedestrian would be in trouble with this model, because it hadn't paid attention to that. So we showed--or John's group showed--that you can give textual advice, literally of the form, "Pay more attention to pedestrians here," and that caused the model to increase its attention on the pedestrians, even when they weren't in the crosswalk. And that seems like a good thing in general. This is obviously not the final story on this altogether, but I refer you to their CVPR paper for more details. That's what I wanted to cover today: adaptation, exploration, and explanation. And I'm looking forward to lots of great research in this area, both with Samsung as a partner and with all the great researchers I know in the audience. Thank you very much. 