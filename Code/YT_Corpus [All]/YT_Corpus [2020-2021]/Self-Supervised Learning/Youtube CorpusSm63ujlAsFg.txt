 Well good afternoon everybody. Welcome to the Georgetown Global Virtual Fintech Seminar Series. My name is Alberto Rossi. I'm an associate professor of finance at Georgetown and associate director of the Georgetown Center for Financial Markets and Policy. Today the Center, in conjunction with the Ripple University Blockchain Research Initiative, is delighted to bring together scholars to present their machine learning-related research. We have three fantastic papers. we're going to start with “AlphaPortfolio for Investment and Economically Interpretable AI” presented by Will Cong. We'll then going to move over to “Forest Through the Trees: Building Cross-Sections of Stock Returns” presented by Svetlana Bryzgalova and then we're gonna finish with Léa Stern presenting “Selecting Directors Using Machine Learning.” So the speakers will have 20 to 25 minutes to present their research and will be not interrupted. We will have then 5 to 10 minutes for questions at the end of the presentation. Please raise your hand or use the chat feature to ask questions. I'll be the person handling the Q&A and before getting started let me just thank Anna Kormis and Reena Aggarwal and John Jacobs for making the event possible and I will turn over the baton to Will Cong who's be the first presenter. He's gonna be presenting the paper “AlphaPortfolio for Investment and Economically Interpretable AI.” Will, you have 20 to 25 minutes. Thank You Alberto. It's a great honor to be here. I'm just going to share my screen here. That works. Okay, now you should see the full screen here. Okay. Great honor to be here and today I'm going to talk about my joint work with Ke Tang from Tsinghua University and Jingyuan Wang and Yang Zhang from Beihang University. Jingyuan and Yang are actually from computer science department so this is very much a interdisciplinary piece of research. The title of paper is “AlphaPortfolio for Investment and Economically Interpretable AI” even though it sounds quite trendy, but what we look at is an age-old classical problem in financial economics; that is portfolio management. What we do know is traditional portfolio management approach that involves two steps. The first step is about estimating the distribution of moments of return distribution and the second step is either we take the market with a path of mean variance optimization or, in practice, many traders do characteristics sort based on some estimates and then do a -what I call- a little bit ad hoc of weighing either equal weighted or value weighted, so and so forth. But we do know, due to the lack of long time series, you know from Martin 1980 onwards, and and also the fact that we just don't have many long tail events return is hard to estimate variance, covariance, matrices are usually not very stable and invertible. There are many traditional fixes to that problem but even those achieve very moderate success just because of the nature of the problem. What we want to take here is really a different approach. We want to directly optimize the portfolio. So by definition it's not going to be a general equilibrium model; it's not going to give you the pricing kernel and all that, but the goal is to achieve a portfolio performance and we're not the first one to do this. Well, there are several studies by Brent and co-authors who have look at the issue before; they want to get to the optimal ways of assets in the portfolio directly, either through a nonparametric approach or parametric approach. But the parametric approach is still prone to miss specification and nonparametric approaches limit in the sense that the the model can only reliably implement and parametric estimators after two predictors, in their earlier work. So we want to take a data-driven version of that. The underlying assumption is really the relationship between portfolio weights and predictors is less noisy than that between returned moments and predictors and by getting rid of the intermediate step of estimating the moments or return distribution, we are also subject to less miss specification of the intermediate model or return model and why why would this work? Well, think about how we construct a portfolio. If I estimate the first moment, so suppose I'm constructing a long portfolio, the first moment of an asset is -suppose I estimate that very accurately, is really negative it's not going to be in my portfolio. So rather than using the statistical powers to reduce the pricing arrows of this asset and by introducing by finding the right kernel to use, we may as well focus on the essence that I'm going to include in portfolio and really estimate their higher moments better in order to reduce variance of the portfolio, for example. Okay, so this direct approach is going to be a reinforcement learning based is essentially dynamic programming once we see the model we'll get a sense of that. So that's that's one thing we want to do. We didn't do it in the paper and I probably wouldn't have time to touch on that. There are further extensions which are related to how we can incorporate dynamic learning and dynamic budget constraints that are related to robo-advising, but that's another benefit of using a reinforcement learning approach to the problem. There's the second kind of line or contribution want to make is really to illustrate the utility of cutting edge AI or deep learning models in finance. We know there have been much advance in AI and machine learning and their applications in finance. We can roughly label them under either dimension reduction or semi-parametric of distribution free approach to the problem but we are typically still in a two-step approach. The goal there is to estimate the risk premiere, find the right pricing kernel, is not directly optimizing the portfolio performance in that regard. Recently, there are several applications of a neural network approach but still within that framework. What we want to do is to take this new approach and we also don't want to pull statistical packages directly without, you know, tailoring it to our particular application. So if we think about the data generating processes in sciences, a very engineering, are very different from social sciences. In finance, we know the data that we have to share: high dimensionality, non-linearity, that science data also face, but at the same time our signal-to-noise ratio is really low and studies have also revealed their important interaction effects rather than the sparsity problem that computer scientists typically face. In this study, what we want to emphasize in addition is really the kind of the fast dynamics of the data generating process. On our genes, physical laws don't evolve a very high frequency, but policies change all the time. You know, investors preferences, risk aversion might evolve over time as well. So historical data in that sense is not really historical data, so we need to teach the model how to learn with new data that's coming in and that's why we're taking this AI model and build upon it and we're also facing panel data where we have multiple assets. So, you know, if you look at the machine learning and AI models developing computer science, it's typically for machine translation. The computer vision is a single sequence learning and by sequence learning it's essentially a neural net version of our traditional time series econometrics. But we face multiple sequence learning so we have to make adjustment to that. We also want to demonstrate that and our findings are robust to economic restrictions and we want to interpret the model. So that's that's where we want to come in and make a contribution. There are papers in computer science that use reinforcement learning to look at finance problems but typically the researchers are not focusing on, you know, the economic motivations, restrictions, and interpretation of the model. Okay? The second half of the paper is really- now suppose we have this model that works great, this alternative approach to portfolio management that that seems to work well. It's a black box. We don't want black box. We want to understand the economics of of the model, of our approach. So the second half of the paper is really devoted to developing procedures to interpret, to distill the model, to figure out what are contributing to the model performance if there's out performance. So this is very much related to interpretable AI in computer science. The difference is over there studies either use feature importance analysis or construct surrogate models and we're the first one to take a hybrid. I'll be more specific on what we do here. Basically, we use two approaches. One is polynomial sensitivity analysis. The other one is textual factor analysis. The basic idea is to project the black box model onto a linear linear regression model space. We can put it higher order terms, but but we can use the feature importance analysis to figure out which features are important put their higher order terms, interaction terms into a linear framework. The textual factor analysis is projecting the model onto natural language space. Yet hope of looking at the words that correspond to, you know, the portfolio selecting certain stocks to take long positions with short positions; does that correspond to what the firm is discussing in their disclosure? So hopefully through interpreting the natural language we can can gain some understanding, a narrative of the model. Okay? So these are the areas that we hope to add to the literature. So with that I'm just going to start on the model but one thing I want to emphasize is, you know, our key contribution -our emphasis- is really not on the fine tuning of the parameter or some people might say, "well your model is so complex when you have so many degrees of freedom. Of course you're going to do better" and all that. That's not always true. Right. We look at our example task; we need to guard against model selection bias and the like. So that's not where we are adding the most. What we really want to contribute is this approach, this direct optimization approach and hopefully I can convince you the choice of this particular AI model is very much motivated by what we need to do, right. If we are maximizing I say the Sharpe ratio of the investors, supervised learning traditional regressions are not going to work. There is no right or wrong. Sharpe ratio, per se, right. But reinforcement learning is very natural. When we get a high Sharpe ratio in the training that we provide positive feedback to the model. So that is context for parameter regions where they might have the highest gain in model performance and hopefully all these would become less abstract when we come to the model. Okay, so so here is what I plan to do. I'll talk about model methodology and show its empirical performance before I touch on economic distillation, the interpretation part. So this is the model. I don't think given the time constraints you are getting to fully comprehend what we are doing here, so let me just break it down into a few steps. The model we use is a transformer. That's the latest sequence learning tool used in machine translation and computer vision and also self-driving cars. So what do we want to do? We want to use neural networks to represent the asset information. It could be financial ratios, it could be market signals and in our representation we know we're facing panel data. Past dependence, history dependence could matter. So that's why we want to use sequence learning. As earlier sequence learning models such as RN suffer from certain technical issues and limitation, so basically the candidates we have our long, short term memory and the latest one is transformer. So we do both in today's presentation. Our only report what we find with transformer because that turns out to guard against certain gradient explosion problems that computer scientists have identified better. Okay? So here's what we do. For each asset, if I label it by eye, I'm going to look at a certain window of their features such as book to market ratio, their earnings growth, whatever future we have in mind- that we feel are relevant we're going to put it there and using Xi to represent it and then we're going to go through this transformer encoder to further compress the information embedded into a neural network. I have to skip the details. There is the standard neural networks transformations, linear fast and nonlinear parts, but in the end we basically have a matrix representing the asset i over time their features were presented in a their space that's what we get in this r. Okay? So that's that I said that what we add to the model transformer encoder it's really a cross asset attention network. The idea is, "okay I have representations of a time series of information for the asset." Now suppose I introduce some parameter matrices all this W query key and value vectors. So far there's no economic meaning to it. It's just made matrix multiplication. But what I want to do is, okay, I have a really flexible setup where I introduce these matrices. I'm going to have them multiply the prime series representation of the assets and then look at the cross assets' relationship. In machine translation, there is something similar which is self attention. Basically looking at the earlier sentences of a passage with the later sentences of a passage and take your dot product once we represent them using vectors and I haven't told you why we do that, right, but that's just the operation we introduce for a moment because these operations is going to allow us to generate what we call a winner score and the the formula shown here is the self attention kind of function that we typically use and the goal is to generate some numbers that captures interactions of assets as well as their time series representation. So far there's no economic meaning to it. It's just representing the information. Okay, well and this is basically what we have for each TE stands for transformer encoder that represent a time series and by doing putting on top of that the cross asset attention network, we're going to generate is a scalar product that's something we call winner score. Why do I need this winner score? I'm going to rank the asset based on winner score. I'm going to long the top-ranked ones and short the bottom-ranked ones. Now if I just blindly run this, I'm going to perform really badly because there's no economics to it yet. I haven't trained the model. I didn't specify the model based on economic reasons other than the input features that are, you know, relevant for that is we traditionally believe are relevant for asset returns and portfolio performance. This last step training the model using reinforcement learning is very important. So what is reinforcement learning here? It's basically a dynamic programming approach. We acknowledge the world is too complex. We are now going to be able to fully estimate the distribution of asset returns very well. So what we want to do is really like a child we don't know how to use a remote control. We're going to let the child try some buttons. If they see cartoon channels, maybe not their best or favorite channels. They are going to explore the buttons in the area more until they gradually learn through dynamic interaction with the environment how to operate a remote control. Here is similar. We set up a really flexible model that generates portfolios based on the winners form. It's going to suck at the beginning if I randomly initialize the parameters. But as I train the model, what I get a high Sharpe ratio, less a Sharpe ratio is my metric for performance. I'm going to give a thumbs up, that's positive feedback. If it performs poorly, I'm going to give it a thumbs down and then the model is going to explore parameter regions that are going to give me a high Sharpe ratio, out-of-sample. Okay? So that's what a reinforcement learning implementation of the model is. The trick that the numerical approach for the model to converge is a standard from Barto and  Sutton's reinforcement learning book. So I'm going to skip that part. Now let me just show you some performance, empirical performance of the model. We look at U.S. equities. For illustration, dinosaurs very standard will train the model pre-1990 until the model converges. We test the model from 1990 to 2016. We do allow monthly update of the model. So it's a rolling test. We also look at a subsample post 2001, after many changes in the market microstructure and so forth. The input signals are very similar to what Freyberger and co-authors have in their nonparametric approach. Basically, the imposed fall into a few categories: investment, signals, quality signals, value of signals, trading frictions and we also lag it by 12 months. So it's 12 times 51 characteristics. That's our input feature. For the textual part, textual factor analysis we use a firm's filings. Okay? So here is a key table from the paper. This is the performance of AlphaPortfolio on the test set In terms of Sharpe ratio, return turnover maximum drawdown we can see it does pretty well. The Sharpe ratio out-of-sample is above two, typically, especially after we act through the small stocks. Be burden q10 means we're excluding the bottom 10% based on market cap. Those are typically illiquid, hard to trade. They turned out to be driver so many of the earlier anomalies or even earlier machine learning strategies but AlphaPortfolio is robust to that. Now, if we control for various risk factors, here I list CAPM all the way to Hou-Xue-Zhang 4-factor model. The Alpha is still very significant. Okay? We also look into where the out performance is coming from. Here I'm just on the left panel. I'm just comparing it to the very very closer paper using the parametric approach. That's one of the best performing machine learning strategies in recent years and NP stands for nonparametric, AP stands for AlphaPortfolio. You can see MP also performs very well but then once we take out the smaller stocks the performance tend to drop quite a bit. AlphaPortfolio is not suffering from that. This is not meant to be a critique to these other papers. These other papers have a different goal. The goal is to figure out what's the right count factor we shall have. Can have a low dimensional representation of SDF? Can we estimate risk premium more accurately? That's not our goal. Our goal is to maximize portfolio performance. Okay, and the right-hand side: is suppose I use the different models. Suppose I use transformer encoder, but I do it in two steps: estimate asset returns and then construct a portfolio using equal value weighted. It still outperforms many traditional strategies but we can see reinforcements learn the dynamic one staff director optimization is contributing to the out performance. This is the time trend for Sharpe ratio. The red line here and the axis alpha over time. It fluctuates some anomalous, some signals might be traded away but overall is still statistically and economically significant. The performance is not driven by the particular time here we look at. No particular industry is driving the performance either. It's not coming from short lack of the portfolio. One way includes transaction costs is still the yielding similar performance. Now we can put in a few other economic restrictions such as excluding the unrated of downgraded stocks because they are hard to trade. They might be going through some corporate events and other remove and co-authors have a recent paper looking at machine learning strategies and how they might be affected by imposing these economic restrictions and here we do see the performance drop quite a bit. We're still a very significant and the job is mainly coming from the fact that in a training sample, we didn't really exclude. We didn't train the model excluding, you know, downgraded or unrated stocks. Okay? So so that's not so surprising to us. This is how the model performs and the different market condition, different sentiment periods. VIX market liquidity is not driven by, you know, crisis episodes where we have high sentiment no liquidity. During regular times it is giving similar performance. So let me just touch on this economic distillation a little bit and then I probably have to wrap up. So now the model works, it seems and again it's not about a specific setup of the model, it's really this approach- reinforcement learning the direct authorization approach. Now, how can we interpret the model without a good understanding? We can't rely on it. It's just another paper pulling machine learning packages. That's not what we want. We want to interpret the model. How do we do it? Well, we first figure out important features in the model based on how they contribute to the objective that we're optimizing and once we have the features we add their second order term we add their interaction terms you can do it for different degrees for the polynomial. Then we go back to a linear model and if you are unhappy with too many variables, we can do a variable selection. So this is different features how important they are over time for AlphaPortfolios' performance. Let me tell you what these symbols are. Basically, it turns out that the profit margin is very important. Cash holding inventory change turns out to be the most important feature and the inventory change, squared. That's also important term we didn't really capture interaction terms in that regard. There's also rotation patterns. You know, financial variables and market trading variables tend to rotate in and out over time and these are things that we can explore further to gain some interpretation and finally we also project this onto the firm's filings. When firms take long positions, the the firm's filings are typically talking about loss-cutting profitability and when firms are when AlphaPortfolios are taking short positions in asset. The assets tend to discuss real estate, corporate events, mistake recent mistakes, and I don't have time to go into more detail but this is a first tax base interpretation of AI machine learning models that I think offer some promise to develop narratives is now going to be causal a notification. But it's a initial status interpretable AI in social sciences. So that's pretty much what I have. Basically, we want to provide this illustration of the utility of reinforcement learning. It's really a different paradigm compared to supervised versus unsupervised learning and we have to make sure is economically motivated by financial data, the nature of the data generating process and we also want to be able to understand what the model is doing and for that we need this procedure so economic distillation and the procedures are quite extendable it doesn't have to be portfolio management. Okay, so with that I'm going to conclude here. What did we do? Well in terms of economic approach to portfolio management, we take a direct optimization approach acknowledging the complex environment that we face and we also illustrate the efficacy of AI machine learning applications. The key is really to make them tailored for our particular economic application and finally we provide this procedure for interpreting AI or machine learning models. This is a, you know, interdisciplinary emerging area. I personally find it very exciting also working with computer scientists. Very different perspectives so with that I'm just going to conclude here and happy to take some questions.  Thank you so much, Will. That was a fantastic presentation. We had one question from Cameron Pfiffer. I think it's Pfiffer and in the question is, "I'm curious about the models heavy reliance on inventory changes. Do you have a good interpretation for that? Is it maybe some kind of orthogonality to the typical factor that makes the performance so good?  Yeah, that's a great question. I'm happy to go into more detail offline as well. So regarding the inventory changes. Inventory changes scale by total assets. It turns out to be a anomaly identified in an accounting paper in 2001. It didn't garner too much attention, but there are economic channels where, you know, it's related to certain accounting reporting issues that could lead to firms being under price or over price so that's one way that we can go deeper to really figure out what's going on, economically. But an initial stab that we want to do is just be able to identify them and see which ones are more important. Okay. I actually have a question related to the application to robo-advising I know you didn't touch on it but, like, is your idea would be simply that you would imagine kind of individuals setting their preferences and then the algorithm constructing optimal prefer locations for whatever their preferences are? Yeah, so there are actually two kind of directions related to what you imagine. One is can we use a reinforcement learning to figure out their preference? There are reverse reinforcement learning approaches to figure out, based under dynamic action to figure out how their preferences are changing and evolving. That's one aspect. The other aspect is really, you know, they might have dynamic budget constraints, right. If I make more return today, I have a bigger opportunity set because shorten I need to put in and you need to put some capital with the brokerage and all that. So that is something dynamic programming can easily incorporate whereas the traditional two step approach is really hard to incorporate. So that's where reinforcement learning could be good hold some promise for robo-advising. honestly where I had the robo-advising 1.0 and you're the experts. There's a big room for the that in that regard. Yeah, I think I mean you're I think you probably have it in your paper you maybe I think it's I don't remember but so I think that yeah robo-advising is really at the stage of the rotary phone right now, right, so it's still kind of very very non sophisticated in many cases the applications can be improved dramatically. But yeah, we have a couple of more questions. I'm gonna ask you one and then you can maybe answer the other question through the chat so that we can move on. So I think Charu Agarwal is asking, "is it correct to say that machine learning models helps to select the significant factors from the whole range of possible factors?" Yes and no. Yes, in the sense that it does perform very well. So some features must have played a bigger role in there, but by machine learning packages themselves, is a complete almost complete black box. It's really hard to know which one is more important. Occasionally, we can identify all these are the important features because based on the gradient numerical optimization, they tend to contribute more but we still don't know: are they contributing in a nonlinear way? Are they contributing interaction with other features? And I think that space is wide open. Computer scientists are doing some cross sectional plots just to see, you know, how they are contributing. But I think just coming back to what we all understand. Linear models, variable selections, is one seems a reasonable approach in that regard. Okay, well thank you so much, Will for the great presentation. I think you have more questions coming in but I think you can take them through the chat. Let's move on to the second presenter, Svetlana. Thank you. Yeah, thank you. Svetlana, are you there? I don't think we see you and I think you're muted. But now we can see it and we can hear you, yeah. Perfect. Okay. Thank you so much Svetlana. You have 20 to 25 minutes for the presentation and you can take it away. Alright, thank you so much for putting the paper on the program. So this is a joint work with Markus Pelger and Jason Zhu both from Stanford and if I'm not mistaken Markus is actually with us today so he can also help with some of the questions that have been asked in the chat, for example during the talk. Okay. Let's start. Many, if not most, empirical applications and asset pricing have essentially two ingredients. There are test assets, which are usually a set of portfolios and then there is the model that tries to explain why their expected returns are different from each other. Over the years, we have largely been focusing on the model side. In particular, we found a lot of new factors and built hundreds of nonlinear structural models. However, there was surprisingly little attention to the left hand side of this equation. But the outcome of the test is essentially going to be only as accurate as informative is the data that is used for it and so many factors themselves are also built from exactly the same sorting based portfolios that have been used as a test assets. But why is that the case? Maybe all these years we have been focusing on their own object to begin with. So this is essentially was the start of our paper and in this work, we are trying to build a cross-section of portfolios that efficiently reflects the information contained in a given set of characteristics. In other words, we project SDF on the characteristic space and find a small number of interpretable base assets that really represented well. How do we construct these base assets? All portfolios and are going to be based from the conditional portfolios place. So we call them AP-Trees because they come from the decision trees with characteristics driving the construction of a particular attest asset. Then, we use an economically motivated procedure to prune them. That is to select a small set of potentially overlapping portfolios based on different splits and their depth. The outcome of this procedure is a cross-section of test assets that are easy to interpret. That is, we're going to know  exactly what kind of stocks go inside each portfolio at every point of time and why the end up there. This approach allows us to study the impact of nonlinearities and interactions without having to formally model them and it massively elevates the curse of dimensionality. For example, I will show you a cross-section of 20 or 30 based based assets which are going to be reflecting ten characteristics at the same time. Yet, despite having only 20 or 30 portfolios, this cross-section is going to be several times more informative than stuck in 100 individual deciles sorted together in one giant cross-section. Our discussants have high out-of-sample Sharpe ratios analysis. On average 30% higher and the more characteristics are involved, the better they're going to behave relative to the alternative. They're not spanned by the usual longship factors and obviously pose a much higher bar for existing models. Finally, you can easily customize the procedure to include various constraints. For example, on liquidity, number of portfolios, and the rest. So, one thing that often kind of emerges is the question is why do we still use portfolios? Let's say why not individual stocks? Unlike individual stocks, for example our test assets are going to be balanced. They're going to be relatively stable in their risk exposure. So you could easily use them with structural models and GMM. If you want to get a small test of portfolios, test assets that reflects only some characteristics but not all, you can easily do that. If you want to have a smaller or larger number of portfolios because you're estimating a smaller or larger larger type of model with larger number of parameters, for example, again you can easily do that. That is going to be part of our setting. I'd like latent factor models or for example different SDFs, that are directly based on reflecting characteristics. Our portfolios are going to be fully interpretable and they're going to be easy to use with most tradable and non-tradable factors are the reduced form or structural nonlinear models. So basically, it's a kind of a general solution to the problem. Now, in the interest of time, I'm going to skip the literature review and one of the reasons is because there have been, of course, a large number of effort from people both from traditional empirical asset pricing literature and more recently machine learning approach that try to use efficiently the information contained either macro data of stock specific characteristics to kind of figure out what is the data generating process that describes our financial returns. However, we are going to be looking directly at one question that we think has been largely overlooked in the previous papers. Which is exactly the cross-section of portfolios, the selection of the base assets. So what have people largely been doing up until now? People have been using sorting and here is an example of a cross-section consisting of 16 doubled sorted portfolios based on size and value. Just so that we're all on the same page, I'm going to explain how it is constructed. We first sort all the stocks into 4 equally spaced groups based on their size and here you can see bottom and top 25% of the stocks. In a similar way, we can serve them based by book to market or volume. So the intersection of these assets then is going to form 16 double sorted portfolios. However, it is also immediately clear why this approach leaves much to be desired. There is a massive curse of dimensionality. With 3 characteristics, you're essentially already going to be running out of stocks. Why did we use 4 splits resulting in 16 portfolios with 2 characteristics? Why not 25 or 50 or 75 or whatever is your favorite number? Why they have to be equally spaced? Maybe we should keep everything in the middle for example one big asset let's call it more or less market and then add some splits in particular tests, after all we do know that most of the anomalies are concentrated towards the end of the cross sectional distribution of stocks. How to officially reflect the impact of nonlinearities and interactions among these collectors, characteristics. So all of these issues led us to rethink existing approach and while our solution of course all has its' limitations, we hope that it's definitely gonna start the much-needed discussion. A simple generalization we propose in this paper relies on decision trees or conditional sorts, conditional splits to form portfolios. Here is an example of one such tree. It starts with all the stocks being divided 50/50 into 2 big groups based on size and then each of these portfolios could be sorted, say by value and then for example by size again. As a result, we get a tree of depth 3 based on 2 characteristics. By nature of these conditional splits we will never end up with empty portfolios. Focusing on the conditional splits here is also important because it allows us to look at the joint distribution of characteristics and study its full impact on stocks. Which is actually crucial because of 2 fundamental reasons. First, most characteristics are dependent and this dependency structure is not linear and is often hard to capture by, for example, just correlation alone. Conditional splits implicitly take it into account and create balanced packets of securities. Since characteristics are generally dependent, the order of the splits will matter as well. For example, the left panel shows you the average from discrete distribution in terms of market cap and book to market ratio. So size and value. And just looking at it it, is immediately clear, but if it first sort by size and then by value but by value and then by size, it will give me very different portfolios and this is actually true for almost all of the variables we have looked at almost all of the firm's specific characteristics and this also matters because the characteristic have a very rich conditional impact on asset returns. Here is a textbook example of again size and volume. Obviously, an average high book to market firm tends to have a higher rate of return than its growth counterpart, which is the purple line on the graph. However, this effect is almost absent in large caps and it is particularly strong in the small caps. There are many other examples where once some form of conditioning is taken to count. For example, based on accruals or asset turnover the effect is going to change in magnitude, shape, or even sign compared to its unconditional counterpart. So it's really important to understand these type of complicated interactions as well. So let us consider our figure at cross-section based on size and value which is probably the most popular set of best assets used in the empirical literature. How would the tree based portfolios look like in that case? It will probably look something like this, where depending on the firm distribution and their effect, you can have more granular splits in some areas rather than others and I suspect that some of you are smiling at the moment both because they actually recognized this picture they have seen me actually making this joke and for example communicating some other conference because what you're looking at is the painting from the Museum of Modern Art in New York. But the reality is actually not that far from what you see on this painting. On the right panel you see the familiar 16 double sorted portfolios based on size and volume, in the space of cross-sectional quartiles and going from 0 to 1. On the left, I am plotting all the portfolios that could be created with trees, both final and intermediate notes. I'm limiting their depth so that each of them includes no less than 1/16 of all the stocks so they could be on equal footing with what we get with double sorts. Note how really interesting and complex could be the return generating process captured by the portfolios on the left and how much we could be missing with the usual sorts which are shown on the right. You will also know that I have for example relatively few splits in some areas for this picture in particular in the southwest corner and this is because there are simply very few stocks that allow on those size and value and so the trees endogenously take that into account. This is something which is well-known for practitioners. For example, mutual fund managers because there is almost no mutual funds that are actually holding the bottom 20% of the stocks sorted by volume. They are simply too small. Of course, using all the final and intermediate nodes from all the possible trees where the fit steps is not really feasible for empirical analysis. There are simply too many impossible portfolios. So we need to find a ways to select those which are really key. Those basis assets and this is the process where effort is pruning. How does pruning usually works? In a standard application of decision trees or random forests, people tend to use the bottom-up approach. So for example, consider the set of these 3 nodes. So usually one could try to compare let's say the return on the bottom 2 portfolios and if they're different enough, we keep them separate. You do the split. If not, you merge them into the parent node but this approach is clearly not going to work for our setting because remember our goal is to select blocks for the SDF. The best set of basis assets. That means that together that should contain as much information as possible that should then form an SDF with a highest Sharpe ratio. If I want to use portfolio logic to this process of pruning or selecting the portfolios, the question at hand is whether I assign the same weight or not to these 2 child nodes in my overall asset allocation and clearly the answer to that question depends not only on returns, but also on variances and covariances of these portfolios not only between each other but also with all the other assets that I have at my disposal and this implies that pruning based on some one-dimensional local characteristic cannot really be done here. Choosing the right set of portfolios that really span and form the SDF is a global optimization problem and this is why usual techniques from random forests and decision trees are going to be really good let's say for just return prediction but they're probably not the right 2 if you want to understand the building blocks for the SDF. We have to solve a global optimization problem. But as with any global optimization problem with a really large set of variables, of course, it is going to benefit immensely from different way of shrinkage. So we take all the final and intermediate notes from all the possible trees with a given depth and construct an efficient portfolio frontier. You will notice that there are essentially 3 tuning parameters here. There is the target return (the my portfolio), there is the loss of shrinkage (bit lambda1) and then that can be used to control how many portfolios we want to end up with. For example, select 10, 20, or 50. And then there is the ridge part. That's the lambda2 so that is needed to stabilize the variance on portfolio weights within the overall SDF. As a result, we can select the best, robust set of basis assets from both the final and intermediate nodes. That is kind of very narrow, less diversified, and very wide, more diversified portfolios that form the SDF and this is another aspect in which we differ from the standard pruning of the trees. In the end we're going to retain only particular nodes colored in red in this picture. Not all the portfolios along the whole tree pass. In a paper, we study the properties of this procedure and in particular we can show that one can think of this optimization as actually building a robust mean-variance portfolio from the point of view of an investor who is very ambiguity adverse. That is it simply doesn't trust the numbers from the in-sample parameter estimates due to their instability and would really like to protect herself against the worst possible case within a certain range of parameters and it turns out that the optimal portfolio allocation under this estimation uncertainty and Sharpe ratio, expected returns and their variances is actually going to coincide with our problem 1 to 1. While the loss of ridge penalties are well-known in the finance literature and in particular due to the recent work by, for example, Serhiy Kozak and Victor DeMiguel whose papers also have been cited actually by the previous speaker. It is the use of the target return as a tuning parameters that I would really like to comment on. When you bury the target rate of return for your portfolio, what you're really doing is moving along the portfolio frontier between the tangency portfolio and the minimum variance one. In other words, you're going to be shrinking expected returns towards their cross sectional mean. Why is this important? This is important because there is massive estimation error and expected returns. When you consider a set of 25 or 50 portfolios and choose how much to invest in each of them, it's very likely that the asset with the highest rate of return in sample actually got there simply by chance. Just because it's the best out of 50. So if you want to use this usual in sample estimate you would overweight it relative to its fair value and this is where shrinkage to the cross-sectional mean really helps. Of course, the optimal degree of it is going to depend on a particular set of assets and one sample analog of this procedure is very well known to all of you I'm sure and this is the so-called adjusted betters which have published by Bloomberg because it they're also being shrunk towards their cross-sectional average which is one and you can see it for almost all of the tradable securities like stocks where they have the actual estimated better coming from the data and they adjusted one, which applies shrinkage to the mean and here is a final illustration of how pruning could work empirically. So instead of splitting all the stocks into let's say 8 equally distance portfolios, you could actually end up with just 5 as a result of this pruning procedure with one of them being the market, the second including half of all the stock so maybe a quarter and then maybe only one of them going really into the tail into one eighth of the stocks. So you could end up with portfolios actually more diversified than the standard sorts to begin with and it really all depends on the data. The selection between both final and intermediate nodes allows us to understand how granular one should actually go with these splits in the portfolio. Now, let's look at some of the empirics. Our data is going to be standard. So we're looking at monthly returns and some of the most famous characteristics used in the previous literature and in particular I'm going to take a list of 10 characteristics. Why? Because these are the characteristics that have been used to construct portfolios available from Ken French website. So those that everybody essentially is using in academic research. The standard application and familiarity would also help us to establish natural basis for comparison. We are going to be focused on the properties of 36 cross-sections. So each of them is going to be based on using size and two other characteristics. Why do we always include size? Again, because this is exactly how portfolios are constructed, those available from Ken French website, so again this is a natural benchmark for comparison. We will use the same portfolio approach to different assets. So all of the results that you're going to see, all of these different properties of cross-sections based on double or triple sorting versus trees or conditional sorts, all of them are going to be coming really from trees versus standard sorting based approach. So, it is important to know that all of our empirics is going to be done out-of-sample. We're going to use the first 20 years of data to estimate model parameters. We're going to use the next 10 years to find the optimal tuning parameters, such as the degree of for example shrinkage for each of the cross-section. So they're gonna be cross-section specific and then we'll fix that selection, portfolio weights and just let the data speak, let the model run for the next 23 years. All of our portfolios are going to be evaluated and we specifically are going to exclude extreme sorted portfolios. So for example, whatever you're using the same characteristics 4 times. In fact, our results would have been even better if we were to use them in our analysis, but we just want to be completely fair with the triple sorts and double sorts because it is impossible to achieve that degree of granularity using 3-part-sorted portfolios and so we wanted to kind of have these two methods to be completely on equal footing. So actually, if you relax this condition, you would see the trees perform even better. On the triple-sorted sides, we're going to be using the cross-sections of 32 or 64 treatment sorted portfolios which corresponds for using either one or two cards based on size. Okay, so here are the monthly out-of-sample Sharpe ratios that could be achieved by combining an optimal set of tree based portfolios together relative to that of triple sorts. For the sake of comparison, we are here retaining 43 based portfolios for each of the cross-sections and I will comment on this choice a little bit later. In this graph, I'm sorting all the cross-sections by the Sharpe ratio that is achieved with the trees and it is clearly much higher than the alternative. But Sharpe ratio could of course come from either high loadings on some sources of risk which come with a reward or it could come from actually alphas something which is not spanned by traditional risk factors and as we show in a paper, to a large extent they're actually coming from the alphas. These SDF alphas, again specific to each of the cross-sections, are not spanned by Fama-French 3 or 5 factor models. It doesn't help if you use cross-sectional specific factors. So, illustrated by this picture, where for example you're always include momentum factor or whatever. Momentum has been used as one of the characteristics to construct strategies and portfolios. Knowledge changes if we're actually going to be include all 10 long-short factors in addition to the market to try to explain the performance of the strategies. Naturally, conventional models also going to have a hard time price in this portfolios and within each of these cross-sections. While conventional cross-sectional specific long-short factors have no problem at sheering and are square of something like 85 or 90% on triple-sorts even 64 of portfolios of people sorts. The ability to explain our base assets is going to be much worse and in some cases you can get only as much as 40% of the r-square. So to be honest, people sorted portfolios again even as many as 64 are actually quite easy to price. Let's look at the particular cross section to get a better sense of what is going on. Here, a portfolio sorted by size, investment, and operating profitability, and in this particular case there is almost no need to use ridge. They're also some moderate shrinkage towards the cross-sectional mean it was just like 15%. So the left panel is going to show you the Sharpe ratio as a function of the portfolio numbers. Those that we retain from the whole set of trees. Which is then used to select the optimal value, right. And before we just said the equal to 40 for almost all of the cross-sections. But as you can see, actually, almost all of the information is contained in just ten portfolios. So if you use just 10 basis assets, that's already enough to get like 90% of alphas and Sharpe ratios and everything else and these turns out to be a very stable result across all of the cross-sections. We see essentially the same picture out-of-sample for almost all of them. This feature of the data is extremely robust. You don't need a lot of portfolios to reflect data well. What you need is to just choose the right ones and here are some of the summary statistics where I'm highlighting the properties of the cross-section based on 10 tree based portfolios. As you can see, they have higher Sharpe ratio. Not massively, but then again it's just a representative example. They have more pronounced IFAs and they have really low cross sectional r-square compared to triple sorts. Your factors it's very easy to get an r-square of 90%. A much better measure would be looking at the Sharpe ratio which is achievable out-of-sample. Or how much the date is actually spanned by your factors kind of like an out-of-sample GRS which is exactly what we do here. We don't think that our results are driven by micro caps we have done the same analysis by cutting out different types of data to kind of safeguard against this. Which is, by the way again, very easy to do when you're working with trees. So for example panel B shows you what happens if we include only stocks with a market cap and ball 0.01% that of the market. Just to give you a kind of as a side comment, right now there are only about 600 stocks that are gonna satisfy this requirement. So there's a really really large caps. This is almost just S&P 500 and we still get higher Sharpe ratios significant alphas and lower feed from the standard models. In the paper, we also did a bunch of other robustness checks, experimenting with various cut-offs, rebalancing frequencies, we looked at the time variation, base asset selection is always very stable even though their weight within the SDF could for example change over time depending on expected returns and volatilities and the last picture that I really want to show you is essentially the structure of the SDF in the portfolio and the characteristic space. While some of these patterns are going to be similar across trees and triples sorts others are going to be very different. For example, the whole set of stocks with high investment are going to be basically endogenously treated as one big portfolio and you should be buying not just small packs but actually large ones as well and it really feels like this picture kind of illustrates the strengths of our approach. Information contained in the conditional distribution of firms and the characteristic impact matters for decision making because it affects both kind of the return aspect but the risk, the volatility aspect as well and I think at this point we're only starting to understand how that translates to risk loadings and returns. But triple sorts are for sure are just two cores for addressing this type of problems. Finally, just as a very quick side note, I want to show examples of portfolios which are based on 10 characteristics at the same time and here at the bottom you see the green and blue lines. So these are going to be for example various combinations of deciles sorted portfolios based on 10 characteristics. So you start with like 100 decile-sorted sorted portfolios which usually is assumed to be a really challenging as thing to price. It is not. Out-of-sample, that type of test assets achieves maybe a 30% of the Sharpe ratio that you can actually get if you were to use tree-based portfolios. So using decile-sorted portfolios and pricing them is actually quite easy and you're going to miss out a lot of the information which is contained in the returns. In fact, it is enough to help again a small number of assets. But, rightfully chosen number of assets in order to get almost all of these results. Okay? I would really like to finish this talk by kind of talking a little bit about the overall role of machine learning and finance and so recently there have been some people kind of getting skeptical about the potential and the reason it happens is because there are essentially two types of problems that we are facing and I think Will also has been talking about them in this previous talk. There is the simple prediction. Say, returns risk premia as an unknown function of some variables and this is where standard machine learning typically really really shines. It is designed to work there. However, our ultimate goal in many applications is going to involve prediction output that is then used for other problems, for building portfolios, for finding SDF, for getting accurate estimates of some structural parameters and what works best for the standalone prediction is not necessarily what's actually gonna give you the best answer to all these other questions and so this is why off-the-shelf statistical tools and machine learning tools sometimes can be quite disappointing. It is very important to keep the economics inside the mode on data techniques and then I think we'll be able to do a lot more in different areas of economics and finance and there is so much potential in the tools that we're just starting to understand. Thank you. thank you so much, Svetlana for the fantastic presentation. Do we are a little bit behind but do we have any question for Svetlana before we move on to Léa? And, you know, if questions come up a little bit later you can always write them in the chat so there is always that option as well. Going for once, twice, okay well then let's move now to Léa who is going to present the paper "Selecting Directors Using Machine Learning" there you have 20 to 25 minutes. Alright. Can you hear me? Everything okay in terms of logistics? Okay perfect. So thank you very much Alberto and all of the organizers for putting together and you know this Machine Learning Day. This is this is great and thank you for inviting us to to talk about our paper or research. So this is joint work with Isil Erel, Chenhao Tan and Mike Weisbach and Isil and Mike are actually in the audience today. So they're with us today and so the question of selecting directors is a long-standing question in corporate governance and so what we're really interested in in this paper is understanding the decision-making process that goes into selecting directors. Snd so we've known since Adam Smith here since Adam Smith and Berle and Means that we can expect this process to be filled with agencies and the reason is because CEOs are typically those that are, you know, de-facto in charge of the nomination process for directors and the thing is that CEOs today have just as much control over who gets a seat at the table as they did back then. Right, and when I say back then you can take your pick 1776 or 1932. It's pretty much the same. So really things have not changed much, right. CEOs are still in charge of that process. What has changed though is technology and so what we do in this paper is we're going to use the predictive power of machine learning techniques to help make some progress in the central area in corporate governance. Okay. So visually, here's what we do. Okay, so suppose that this is the hearing board and the company, so the firm has a board seat, an empty seat to be filled here and what we're going to do in the papers to visualize what we do is we're going to employ supervised machine learning algorithms to generate predictions for us for how potential directors would perform on the specific board for this company. So we're going to be taking into account who's currently sitting on the board and what kind of company it is and the idea is going to be to compare the director that was in the end that was eventually selected by the board and by the CEO, right, to the directors that the algorithm would have recommended for this position and by the way those faces are actually not really where AI generated so I can use them my slides. Okay. So so this is this is the idea of the paper. So why do we use machine learning and debate in this paper? So what's the point? So the reason we use machine learning is we think that the problem of hiring directors is essentially a prediction problem, right. So boards have to make some kind of prediction or how they think a particular candidate is again going to perform and machine learning techniques are simply superior to traditional econometrics when it comes to predictions- to add a sample predictions- and there are a number of reasons for this. You know, the big one is that they're they're designed to maximize out-of-sample predictive accuracy by avoiding overfitting, for example, and by not being constrained by your specific permit resumption so functional form or by being restrained in the number of covariances that they can use. Okay and so the reason why we really need what we really want accurate predictions in our exercises because we want to use those predictions. So the prediction is not the end exercised here. We want to use this prediction because you want to use them to compare the performance of algorithms versus management selected directors and we think that this is going to allow us to have something to say about the quality of director hiring decisions and we can also use those predictions to look not at the performance, but at the attributes of algorithm versus management selective directors and that's going to speak to the features that are overrated or underrated in the in the process for selecting directors. Okay. So what do we mean by director performance? So in any algorithm design, a really key component, what's really crucial, is the choice of the label. This is absolutely crucial. So what are you asking your algorithm to predict and so this is something we've really given a lot of thought and it's actually not simple. The reason why it's not simple is because the board acts behind closed door and the only thing that we observe is the collective action, right? So the board acts as a group. It makes this really important decision such as hiring or firing the CEO or approving a merger bid but we only observe the economists as a group. But what we really need is an individual measure of performance and so to guide us in our search for a good measure with metrics to measure directed performance, we go back to the Mandate of the Board and we share the view of Hart and Zingales, that directors fiduciary duty is actually to represent shareholders' interest and that means that the decision making process for hiring a director, well it should involve some kind of prediction for how he or she is going to fulfill that that mandate. Right? So we really would like our measure of directors performance to be tied to how well she's going to represent shareholders interest and so that's why we're going to use the level of shareholder support. It's tends to be a really natural metric for director performance when we go back to what the mandate of the board actually is and that's what we're going to use as our main measure. To be more precise, we're going to use excess votes. So we're going to use two devotes or at annual director re-elections and we're going to subtract the average that other board members that were who were up for election that same year the average for the board. And so we end up with the market-based individual measure of director performance. And so we know from the literature on director elections that there's typically really no surprise in terms of the outcome of those elections. Directors get reelected pretty much no matter what. There's no surprise in the outcome and they typically gather really really strong support. So I think the average of is around 95% and that's also what we find here. What's interesting is that in the recent literature on director election, so even though there's no surprising the outcome, the variation in the votes does matter and the variation really does reflect market perceptions of director quality. So for example, Fischer et al. finds that the level of shareholder support predicts surprise reactions to subsequent CEO turnover. Cai, Gordon, and Walkling find that low support for directors leads to more CEO turnover and more actions to improve corporate governance down the road and Reena and co-workers find that receiving low support, you know, low shareholder support has really negative consequences for directors. So again, even though there's you know really any surprise in the election outcome, the level of support really does contain meaningful information. So our paper is related to that literature. It's also related to the new literature or emerging literature that these days but is growing quickly in machine learning, in econ or finance and there are a multitude of new papers and I simply here listed some of what I think are seminal papers. It's also related to the older literature on discretion versus algorithms which recently received renewed attention because of theadvancements in machine learning. So one of the reason that may not seem obvious or why we really like our measure of performance so using shareholder votes as a measure of performance is that it somehow mitigates the concern the potential concern that the algorithm might act as a biased propagator. So you may have heard, you know, the concerns that you know we hear you may have read about this in the popular press or any like in very serious journals that algorithms may recreate our worst biases as human beings and this is actually a real concern. So I'm not downplaying the concern, but here our measure of performance kind of helps in that regard because we have the decision-maker and the evaluator who are distinct entities. Right? So what we really wouldn't want for example is we wouldn't want those who select a candidate also be the ones whose standard performance as either good or bad. Right? So for example, not so good measure of director performance would be promotions on the board, right? This would be horrible in terms of replicating biases. So here we have the boards who that includes the CEOs like the directors generate our right hand side variables and then we have shareholders separate entity that generate our left hand side variable. Okay, so in terms of data, we have roughly 25,000 new directors who are appointed over a 15 year period so between 2000 and 2014 so we have the shareholders  data for these 25,000 directors. We're going to split this data into the training set and the test set. So the training set is going to to be comprised of directors appointed in the first 12 years over sample period and then the test set is going to use the last 3 years of director appointments and which may we split the training and test it this way to avoid fitting a model to predict director performance using directors that are going to be appointed in the future. You don't want to do that and so we the predictions that we make you know our ex-ante predictions so we're being very careful not to give the algorithm any information that the nominating probably would not have at the time of the decision and so for a vectra of inputs again, we use a bunch of firm board and director level characteristics that we give to the algorithm to generate the predictions and again our outcome variable of the main the main level is this excess shareholder support measure. We also do the same exercise using two low support so without subtracting the average for the state of directors and we find very similar the same conclusion. We also run some robustness using firm profitability and cars. Okay. So what we do in this table is we'll simply report the fraction of new directors so in our test at the fraction of new directors that receive much lower support than other directors on the board and we break it down by treachery state. So overall this is considering roughly the bottom 10% of new directors, but you can see that it varies quite a bit by some director and board features. So for example, we have about 11% of new directors who are male that receive this low support, this bad outcome and that that number is is 8% for women and if you go down the table a little bit you see that for busy directors it's 14-14.5% so 14.5% of new directors were busy so they sit on three or more boards, they received this really bad outcome which is only 9% for directors that are not busy. So it does this suggest that there are some characteristics that are associated with lower shareholders support. The thing is, it's really hard to know which functional form of which attribute is going to lead to good out-of-sample predictions, right? So the space of careers that matter is potentially really big and how to combine them to get to arrive at strong at our sample predictive accuracy is just really not clear. So this is just simply another way to say why we're turning to supervised machine learning in this case because those techniques are designed to pick up signal and avoid overfitting in order to generate out-of-sample predictions. So again the algorithm is going to use features about the director, but also about who else is currently on the board; features about the firm so I see all of this and and fit the model for appointments between 2000 and 2011 and then is going to generate predictions for how directors are appointed between 2012 and 2014 are going to perform. Okay. So results. So here's one way to show the results and, you know, the result that I'm going to show you only use data on our test sets. So those are all out-of-sample predictions. So on the x-axis you've got predicted performance and then the y-axis you have actual performance and so what we would like or any model that tries to make performance predictions, we would want directors predicted to do poorly to, in the end, to end up doing much worse than those predicted to do well, right? So we want actual performance to be increasing in predicted performance and what we find so we try various algorithms that are widely used in this community we find that for all of those machine learning algorithms that's the case. Right. So directors predicted to do poorly, they do end up doing much worse than those predicted to do well and we also try to run an OLS model, Nevada Tech, as a benchmark but it's clear that the OLS model has issues with overfitting and so out-of-sample the predictions are just simply not very good. Now, from a governance perspective, we're really interested in knowing more about -I don't you can see my mouse here- we're really interested in knowing more about these directors. The directors at the bottom of the predicted performance distribution. So in the bucket one and two. Alright, so we want to know more about these directors because those are directors who predictably received no support from shareholders. Before we can do that though, we need to take a little detour and this is kind of a technical detour that we need to take to ensure that we can trust these predictions. So what this crop here is telling us is that the algorithms do well and they make good predictions in our test set. However, if ultimately the goal is to show that the algorithm can improve on Board's decisions then this is not sufficient to show that you can predict well on the test set and the reason why is because we can only observe how well the algorithm does on cases for which we see the actual outcome. So for which we have their labels. But this is not a random set. So if boards rely on unobservables in their hiring decisions, then the distribution of outcomes for hired directors can be very different from the distribution of outcomes for those that were not hired even if they have exactly the same observable features. So in other words, among those with the same observables who gets a label and who doesn't is not random and that's to select the labels problem. So a way around this involves designing a pool of potential candidates for each new board seat. So in your test set, right. So we're gonna ask who else could they have hired but didn't and so for directors in your test set, we're going to look at other directors that they could have hired and so those are going to be directors who around the same time joined the board of a nearby smaller company and the reason we designed the clinicals this way is that we're zooming in on candidates who we know were available to join a board at the time. They signal, right, if I revealed preference that they would have been willing to travel to that location for meetings and they would have been willing to accept because it should be pays more and more prestigious to sit on the board of a larger company. We also run so really the rest of tests around different ways of constructing this candidate pools and we find similar similar results. Okay. So for these candidates, we don't observe how they would have done on the board, right. So that's the essence of the positive of the selected labels problem. But we do observe is what we call their quality labels, right. So their performance on the board that they effectively joined and so we're going to use those quasi labels as an indication for how those possible candidates would have performed on the board. On the on the focal board and so here's an illustration of that quasi-labels procedure. So the first step is we want to rank all hired directors. So again, in our test set directors that were effectively hired by the local firms according to their predicted performance. We're going to zoom in at the bottom and say well those are candidates that the algorithm would they would have the algorithm would have flagged them and said those are predicted to do poorly and then for these we asked who else was available but even if they didn't hire and so we're going to for each of these we're going to grab your personal candidate pool and look at all of those candidates and rank those candidates according to their predicted performance on the focal board. We're going to focus on the top also the promising candidates and for these promising candidates we're going to rank them according to to their quasi-label. So according to their performance on the board that they effectively charm and then the question becomes where does y, so y is the actual performance of those directors that were at the bottom here. So directors the algorithm flagged them as those are going to likely to do poorly. Where does y so their actual performance sit in that distribution of quasi-labels and intuitively if y ranks high in that distribution. What does that tell us it tells us? That well, the algorithm, you know, flagged them as potentially poor performers, but when we look around and we see who else was available and how they did turns out that those directors actually did pretty well. So whatever unobservables the board used in making its hiring decision, you know what's that signal and what's a good call. On the other hand, if y ranks low in that distribution of quasi-labels, then this is telling us that those directors were identified as directors would perform poorly and they indeed turned out to do poorly. So whatever unobservables the board the board used was not signal but it was noise or it was issues with agency issues and so what we find is that if you look on the right-hand side of this graph what we find is that using extra boost one of the algorithms that we use when we look at directors predicted to do poorly, or directors in the first style of predicted performance the median rank in the distribution of quasi-labels is the 27th percentile. So really low and directors predicted to do well, they rank at the 78 percentile and when we contrast that to the performance so it looks like OLS just has a really hard time in discriminating ex ante who's going to do well and and who's not. So in this graph, we see that the rank in the distribution of quasi-labels increases across the deciles of predicted performance and so and this is when we compare with all potential candidates and so what this does is it really confirms the validity of the predictions beyond testing performance on the test set and gelling with the selective labels to problem. Okay. So now, you know obviously, that those quasi-labels are not a perfect substitute for labels, right. So we know we're very aware that those those quasi-labels might be inflated because the the match between the board and the director is an endogenous match, right. But from our perspective, and for the exercise that we do here, this is simply a nicer benchmark than the ideal benchmark that we'd like to use and so what we simply need is we need that the we need the distribution of quasi-labels generated by candidates in the pool to approximate the distribution that they would have generated had they served on the approval board. So it's really the distribution that mattered and the second assumption that you need is when is that the error is not systemically correlated with white hat, with predicted performance and remember also that those candidate pools and the positive of procedure all of this we only use it to evaluate the the predictions from the algorithm in order to deal with the selective labels problem. But in practice, you know, the algorithm can still make a prediction for any director and invoice can still rely on on factors matching but we really think that this quasi-labels procedure can be used in other prediction contests in which labels were missing and kind of as a way to go along that selective labels problem which is pervasive if we agree we want to apply machine and social sciences. Okay. So as I said earlier from a governance perspective, what we're really interested in is understanding understanding a bit more about those directors that populate the bottom bucket, right. So who are those directors and why were they hired if there were predictably unpopular for each characteristic? Here we're going to show the mean for directors who are hired but we're predictably bad and the mean for directors the algorithm would have recommended instead and so what we find is that some overrated features in the director selection process is being male, having a large network, having a finance background, and lots of current and previous directorship and an underrated feature is actually the number of qualifications. Okay, so no point clear so maybe I should have said this a little bit earlier but we're currently actively revising the paper and have very exciting new results coming soon so stay tuned. But to back up what we do in this paper, is we use 21st century technical tools to confirm an observation that really dates back over 200 years. Namely, that the board selection process leads to directors who offer and not necessarily the best choices to serve shareholders' interests and we strongly believe that predictive algorithms just the one we use in the paper hold promise for improving boards' choices of directors and for improving the way in which governance serves a shareholders' interests. So thank you. Thank you very much, Léa. That's fantastic you're co-authors already were taking care of some of the questions, but you still have a couple of them, yes I think Mike was very very active but so we had the first question is from Jikhan. I think Mike didn't had the time to answer this question yet but he's a Jikhan is asking "how do you know so how do you think about considering industry or sectoral fixed effects because it may be possible that increasing trends in a EBITA for example may and be higher in certain sectors and specific areas?" Right, so we do take that so the algorithm does see which industry the the companies operate. So there is one of some, you know, it does take industries into account also we do take industries into account when we do that exercise of looking at other potential candidates who could have been hired we at some point we restrict to only candidates to only other directors who join the board of a company in the same industry, right, and so we do take industry into account. That's absolutely I think important, right. And I think here's a second question which is a more technical is "what about can you talk a little bit about the hyperparameters that you set for the XGboost. Are you taking, like, the standard settings that the factors have or do you modify?" Yes. So that's a good question. So so we do use the standard, you know, default parameters and XGboost and and really the I like this question because, you know, the the innovation in the paper is really to kind of flip the question of the the quality of boors and on its head right in fittest as it has a prediction problem and so the innovation is not going to come up with absolutely the best algorithm. We're kind of a little bit agnostic about which algorithm does best or should do best and so in that sense we're very we're not trying to just say okay let's let's try XGboost which it's a very powerful algorithm and then, you know, in terms of the hyperparameter, you know, which we tried with the with the default and it works really well and so we're not kind of training but but so that's also goes back to the idea that you're trying to the way we view this paper is really as a first-class exercise. You kind of show that there are directors who, you know, we can predict that they're not going to do so well and but again we're not I'm sure they could be better at, you know, if you had better data so privately, you know, obtained data and if you had a better algorithm you could make even better predictions, right but for sure, yeah. And I think on the flip side, it is kind of a moral curiosity I have so when you use OLS do you use just a kitchen sink OLS or do you put all the regresses it or do you do a model selection because in many cases whenever you start using model selection criteria like AIC or BIC, see you start seeing that even or less performs reasonably well in this kind of exercises? We try we tried different OLS models, you know, and and it kind of didn't really matter which one it's just it did well in sample but other sample did not do so well but again like be, you know, I think if OLS had worked well then that would have, you know, again goes back to what is the innovation of the paper is like if OLS had done well it's still this predictive exercise, right and and so yeah but it, you know, it it really did turn out that whatever we tried with OLS it didn't do well at a sample. It really could not discriminate exactly who's going to end up at the top and who's going to end up at the bottom and when we consider that quasi-labels perceived the variable well. Okay good. Thank you so much. We are right on time it's at 1:30 and let me give another a couple of seconds in case someone else has a any other questions and otherwise let me just take this opportunity to thank very much the presenters they all did an amazing job thank you Svetlana, Will, and Léa once again. We have a number of initiatives here at the Center so please make sure to check our website and follow us on Twitter @GUFinPolicy and so just a reminder we will not have an event next Friday mainly because the NBR is having this household finance sessions that are going to be covering a lot of aspect of fintech, so the next event will be on July the 24th and so you will receive an email with all the details over the next couple of weeks. Thank you so much for logging in and have a wonderful weekend and thank you guys there was awesome. 