  RICHARD CUTLER: Well, greetings everyone, and welcome to this workshop on machine learning methods in SAS for regression and classification. I'm going to start by loosely defining what I mean by machine learning methods. I think many people would agree that they're a group of highly computational methods for extracting information and characterizing structure in high dimensional data. The term machine learning is used almost synonymously with statistical learning. The term statistical learning was first used by computer scientist Vladmir Vapnik as the title of his book The Nature of Statistical Learning Theory in 1995. Vapnik was the co-developer of the support vector machine methodology. The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman has rapidly become the state of the art in terms of textbooks for this kind of methodology. When people talk about predictive analytics or predictive modeling, they're often talking about the application of machine learning methods to data, particularly two very large data sets. Some people conflate the terms machine learning with artificial intelligence, but machine learning is only a subset of artificial intelligence. There are many aspects of artificial intelligence which are not machine learning. So two very important types of machine learning are supervised and unsupervised learning. Unsupervised learning includes very familiar statistical methods for dimension reduction, such as principal components analysis and multidimensional scaling. Unsupervised learning also includes cluster analysis, so the k-means algorithm and the many hierarchical algorithms under this general umbrella. Supervised learning is the other major area of learning theory, and that includes the statistical methods for regression and classification. Roughly speaking, the difference between supervised learning and unsupervised learning is whether or not you have a response variable. Over the past 35 years, there has been tremendous development of classification and regression methodologies, and it has really been something of a quiet revolution. The two main areas of development are regularization methods for estimation and model selection in the multiple linear regression model, and nonlinear methods for improved production and alternative interpretation in regression and classification situations. We usually date the origins of regularization methods to a paper by Hoerl and Kennard in 1970, in which they introduced ridge regression as a method for getting sensible regression estimates in the presence of very extreme crude linearity. LASSO is due to Tibshirani, first published in 1996, and it has become, in a relatively short period of time, the standard approach to model selection and variable selection within regression, and particularly with regard to very large data sets where p-value methods are simply not appropriate. The LASSO has been the subject of considerable discussion and development. The group LASSO was introduced in 2006 by Yuan and Lin. Often, we like to include categorical variables in a regression, and we do that through the use of dummy variables. What the group LASSO allows us to do is to group together variables so that they come into a regression or a lever regression together, and so that's useful for the categorical predictors. The adaptive LASSO was first proposed by Zou in 2006 and further developed by Wang and Leng in 2007, and it has some very nice asymptotic properties. Least angle regression, due to Effron and a number of his colleagues at Stanford University, is more of an algorithm than it is a new methodology. The LAR algorithm allows one to compute LASSO estimates very efficiently, and other estimates for other regularization methods too. Elastic net, due to Zou and Hastie in 2005, was developed to deal with the kind of situation where we have very wide data sets, perhaps genetic information. So we have many, many more variables than we have observations. And it overcomes some of the problems that other methods, p-value methods, criterion methods, LASSO, have in selecting variables in that rather difficult situation. The elastic net combines elements of ridge regression and LASSO, and the elastic net estimates may be computed using the least angle regression algorithm of Effron et al. So non-linear regression methods and classification methods which have been developed over the last 35 years or so include classification and regression trees, sometimes called decision trees, developed by Breiman and Friedman, Olshen, and Stone, who wrote the monograph in 1984, and provided some of the first software for fitting this class of models to real data sets. Breiman and Friedman also developed the alternating conditional expectations algorithm, which is one of the first algorithms for fitting non-parametric regression. ACE was quickly superseded by the generalized additive models of Hastie and Tibshirani in 1986, which are now extremely widely used and implemented in almost all computer packages for doing statistical analysis. Jerry Friedman combined elements of generalized additive models with classification regression trees and came up with the multivariate adaptive regression splines algorithm, usually abbreviated to MARS, in 1991. The origins of support vector machines go back to work by Vapnik and Chervonenkis in the early 1960s, but the algorithm which is widely used today was developed by Cortes and Vapnik in the mid 1990s. There are at least two flavors of boosted trees. Adaboost was developed by Freund and Schapire in the mid 1990s and is still fairly widely used for classification. One of its advantages is it works well with very little tuning. Perhaps the state of the art in boosting is the gradient boosting machine developed by Jerry Friedman and published in 2001. This is the algorithm behind the tree net classification package. And in many applications, both for regression and binary classification, gradient boosting machines have been shown to be among the most accurate classifiers or predictors, if not the most accurate. Finally, random forests was developed by Leo Breiman and first published in machine learning in 2001. This has rapidly become a very popular algorithm particularly for classification, but also for regression. Part of its appeal is it requires almost no training. One can just simply apply the algorithm straight off the shelf and get very sensible results in a vast array of different kinds of problems. And there are additional features which make random forest appealing, variable importance measures, and graphs to characterize the relationship between the predictor variables and the response variable. As Leo Breiman liked to say, it's peeking into the black box. So our agenda for this workshop begins with a quick discussion of principal components analysis, in large part because principal components analysis is foundational for methods which follow. Then we're going to talk about regularization methods for multiple linear regression, and we'll start with principal components regression and partial least squares, both of which are very heavily dependent upon and closely related to principal components analysis. Then we'll go to the so-called penalty methods for regularization, which include ridge regression, LASSO and some of its variations, and the elastic net. And all of this should take us roughly half of the workshop. The second half of the workshop is devoted to non-linear methods for aggression and classification. We'll use logistic regression as an entry point, because many statisticians are familiar with logistic regression. And we'll quickly move into generalized additive model, which are an extension of logistic regression and related techniques to non-linear relationships between predictor variables and response. We'll spend quite a bit of time on decision trees for classification and regression, because they find a lot of application within business modeling. We'll talk about random forests, and I'll give an example which demonstrates its extreme accuracy for prediction. And then I'd like to finish with just a taste of the application of support vector machines. First topic that I would like to cover is that of principal components analysis. The purpose of principal components analysis is to reduce the dimensionality of data by identifying linear combinations of the variables that explain most of the variability in the variables. In some cases, the principal components analysis can reveal some substructure within large complex data sets. If there is colinearity amongst variables, then principal components analysis can eliminate that core linearity. Mathematically, principal components analysis involves simply computing eigenvalues and eigenvectors of either the correlation matrix or the covariance matrix for the variables in the data set. To illustrate the process, I have an example of some weather data that forms part of one of my ecological studies. These are monthly average temperatures at 840 widely spaced sites in the Pacific Northwest. This is the correlation matrix that you see in front of you. And if you look at the correlations, particularly the correlations for consecutive months, you see that they are relatively high. So between January and February, the correlation is 0.99631. Between February and March, it's 0.98670. Between March and April, it's 0.99285. Indeed, there's a lot of very high correlations here, indicating very extreme colinearity amongst the observations and amongst the variables, and the fact that really, most of the information is in a lower dimensional subspace than the 12 dimensions mapped out by the 12 months. So the question that we need to answer is, can we reduce these 12 variables to a smaller number? And so I carried out principal components analysis in SAS to do this. By default, PRINCOMP uses the correlation matrix. You can specify that it should use the covariance matrix, but in this example, I felt the correlation matrix was probably more appropriate. The variables that I've specified, tave1 through tave12, that's the average temperature for January through the average temperature for December. And here's the output. The largest eigenvalue is 11.1. So that first eigenvector, that first principal component, explains over 92% of the variability in the data. And you could be forgiven for saying, OK, that's enough. I'm done here. We'll settle for one principal component. The first two principal components account for almost 99 and 1/2% of the variability among these variables, and that surely is enough. So based upon this tabular output, we would say that at most two principal components which are linear combinations of the variables are needed to explain all of the variability in the data. Scree plots and variance explained plots are two graphical devices to help us decide how many principal components to keep. And we see here in the variance explained that by the time we've gotten to the second principal component, we have explained nearly all of the variability. And so this reinforces our earlier conclusion that only 1 to 2 principal components are needed to completely explain the variability within these 12 monthly variables. Component profiles are correlations among the principal components with the original monthly measurements. You see that for the first principal component, that correlation, that's the blue line at the top of the plot, and most of those values are close to one. You see for the second principal component, which is a dashed red line, some are negative and then some are positive, but for the most part, they're away from zero. And then for the remaining principal components 3 through 12, the component profiles are hugging the zero line, indicating they're essentially just noise. So this reinforces our earlier decision that only one and perhaps two principal components are necessary to completely characterize the variability within these 12 monthly variables. So here are the eigenvectors and these are labeled, you can see, Prin1, Prin2, Prin3, Prin4. The eigenvectors are the principal components. They are the directions, and geometrically and algebraically, they are the coefficients for linear combinations, for the variables for the different months. And we look down-- the first column here, so for the first principal component corresponding to that very large eigenvalue, and these coefficients are nearly all equal. And so that suggested to me that I could create an index, which I have called AveTempAve over here on the left. And that's simply a straight average-- not a weighted average, but a straight average of the average monthly temperatures for January through December. The second principal component has got a mixture of positive and negative values. The negative values are for October, November, December, and January, February, March, technically April too, although that one's very close to zero. And so those seem to me to be winter months. And for the summer months, we've got positive coefficients. So this led me to creating a second index, which I've labeled AveTempDiff, on the left hand side. I take the average of the April through September, average temperatures, and I subtract from that the average of the average temperatures for January through March, and October through December. And so that is a contrast between the summer average temperatures and the winter average temperatures. The sites in which there is a huge difference are qualitatively different from sites where there is a very small difference between these two sets of averages. So in that particular example, the principal components analysis seemed to work very well. It revealed some structure. We were able to reduce the dimensionality of the data. I started out with 11 sets of monthly variables, and so that's 132 total variables. By reducing each group of variables to just two, that gave me 22 variables, which was much more much more suitable for all subsequent analysis that I carried out on the data. Here's another data set where I can apply principal components analysis. These are crime data for the 50 states and Washington DC in seven different categories. So we've got murder, rape, robbery, assault, burglary, larceny, and auto theft. I might have thought that the violent crimes, murder, rape, assault, and perhaps robbery could be combined into one principal component, and larceny, auto theft, and probably burglary into another one. And so we might be able to reduce the dimensionality of the data here to two from seven, and that would also reveal some structure in the data. You notice, these values are very variable. So for the murder rate in Connecticut, it is 4.2 per 100,000 people. In Alabama, it's 14.2. The larceny rate in Arizona is 4,467 per 100,000 in Arizona, and in Alabama, it's less than half that. So there's quite a lot of variability within these data. So I applied principal components analysis using PROC PRINCOMP. Here's my correlation matrix that is part of the output. And you can see that there are relatively high correlations between murder, rape, and assault, and perhaps to a lesser degree with robbery and burglary. But things like larceny and order theft do not seem to be correlated virtually at all with things like murder. So PROC PRINCOMP, I obtained these eigenvalues. And we see that somewhat less than 60% of the total variability is explained by that first eigenvector, that first principal component. Contrast that with the 95% or so that we got from the weather data. The first two eigenvalues account for about 3/4 of the total variability. The first three for about 87%. And there's a real question as to how many principal components that we would like to keep in this particular data set. Here are the scree plots and the variance explained plot. And I'm looking at the variance explained plot, and I see it's a relatively smooth curvature from that first one. And if I look over at scree plot, it suggests that maybe two would be enough. That's only explaining 75% of the total variability, or maybe four would be sufficient. That's explaining quite a bit more. That's getting up into the upper 90s in terms of percentage of explained variability. Now let's take a look at the principal components themselves. And the first column here, so this is the first principal component. That looks like similar values, coefficients, for all different variables. In the weather data, you know that you could argue that that made sense. For these data, crime data, it's really hard for me to accept that we should be lumping together murder with larceny, with roughly the same coefficients. And so I have questions about the sensibility of that very first principal component. The second principal component looks to be a contrast between the murder rate and perhaps the auto theft and larceny, right? Although the assault has got a relatively large coefficient, at least in terms of magnitude. And so that detracts a bit from the interpretability. And principal components three and four and thereafter are increasingly hard to interpret. So in this particular case, the hope for separation into violent crimes, and non-violent crimes, and perhaps something in between certainly doesn't seem to be apparent from the principal components analysis. And I would say this is an example where the principal components analysis does not reveal anything particularly useful to us. Principal components analysis is very commonly used, and I would say abused, within statistics and data science for reducing dimensionality and eliminating colinearity. It certainly makes sense when you've got a bunch of variables that are highly correlated and which are in the same units. And then you can significantly reduce the dimensionality of the data and reveal interpretable combinations of variables. That's what we saw with the weather data. In other situations, principal components analysis may be less effective for dimension reduction and interpretation, and really, it may not reveal any interesting structure within the data at all. In these cases, it simply is not an effective data analysis tool. Now, although principal components analysis reduces the dimensionality of the data set by reducing the number of variables, it doesn't actually reduce the need for all of the variables. So it doesn't effectively select subsets of variables. All of the principal components are linear combinations of all of the original variables, so you still need all of the original variables. OK, moving on now then to principal components regression. The basic idea is this. We've got some kind of colinearity that we're trying to deal with, and so we want to carry out principal components analysis on the correlation matrix of the predictor variables, and then select a number of principal components to retain. The resulting transform predictor variables are going to be orthogonal, and hence, colinearity will be absolutely, totally, and completely eliminated. Then we want to regress the response variable on the transformed predictor variables. Once we've done that, we'll back transform to the original scale to obtain coefficients for the original predictor variables. So here's a motivating example for this kind of analysis. This concerns the physical fitness of 31 subjects, and it's a data set within SAS PROC REG. The response variable is oxygen uptake while running, which is a measure of physical fitness, and there are six predictor variables, the age and weight of the subjects, the time it takes them to run 1 and 1/2 miles, their rest pulse rate, average pulse rate while running, and maximum pulse rate while running. What we would like to do is to predict the oxygen uptake using a multiple linear regression model. And so I've carried out this analysis within PROC REG, an initial analysis, at least. And you'll notice that I've circled there the options collin and VIF, which stand for colinearity diagnostics and variance inflation factors, which are also colinearity diagnostics. Some sample results from the analysis, you'll see that the R-square value is just a smidge under 0.85, and so that's a point of reference for all future calculations and analysis. And here is my initial output. You see, down in the far right hand column for the variance inflation factors, that the two largest corresponding to run pulse and max pulse are 8.4 and 8.77. Usually, we use a cutoff of 10 for identifying variables which are colinear, but I would argue that that's getting close enough to 10. Among the colinearity diagnostics, we first look at the condition indices and see if there are any exceptionally large values and jumps in the condition indices. And I would say the jump from 82 to 196.8 is a fairly large jump. The way that we read proportions of variance is, you look along the row for which you've got the large jump and condition index, and you see on what variables the proportion of variance is really large. And you can see here that that's on run pulse and max pulse, and in this particular problem, that's bang on. And so that interpretation works well in these data. If I were to compute Pearson correlation coefficients amongst all of the predictor variables, we do indeed see that the correlation between run pulse and max pulse is nearly 0.93. So that's very high and that's where the colinearity lies in this particular data set. So just to remind you, here's the output. And now I want to focus on the column which is parameter estimates. So we've got run pulse and max pulse, both of which are measuring roughly the same thing, because they're very highly correlated. And yet you see in this regression that they have coefficients which are opposite in sign. The run pulse, it's minus 0.37, and for max pulse, it's 0.3. And that doesn't seem very sensible. We would expect that these two variables should have coefficients of the same sign. This is a symptom of colinearity. This is what we often see when we have colinear variables, is that they will have opposite signs, and those coefficients will be larger in magnitude than they should be. So I decided to carry out principal components analysis, and then I'm outputting the scores from the principal components analysis. And I look at the scree plots, and I look at the eigenvalues of the covariance matrix. And certainly, by the time we've got four principal components, remembering that there are only six variables, we have explained about 98% of the total variability among the predictor variables. One could argue that three is enough and perhaps even two. Looking at the scree plot and the variance explained, particularly the variance explained, by the time we get to four, that's most of the variability, and so that's why I chose four variables. So now what I'm going to do is to go back and run the regression, but instead of the six predictor variables we began with, I am going to use the first four principal components. And so you can see that in here, I've got Prin1 through Prin4, and I've labeled this model PCREG. I've carried out colinearity and t-diagnostics for this new regression, and I have to admit, this caused me to chuckle. Because the principal components are orthogonal to each other, there's absolutely no colinearity. And hence, the variance inflation factors are all one, and hence, the condition indices are all one too. So principal components analysis-- principal components regression will 100% eliminate colinearity in the regression. So in order to obtain the coefficients of the original variables, it's easiest to use PROC PLS within SAS. That stands for partial least squares. But if I use method = pcr, then that will do principal components regression. I'm specifying the number of factors to be equal to four. That's what NFAC = 4 is doing. And then with the option solution, I'm asking for the estimates of the coefficients for all of the original variables, which require some back transformation. And if you would turn your attention to the right hand side where we have those parameter estimates, then I see that the coefficients for run pulse and max pulse are now both negative, which makes sense. The higher the pulse rate, the less fit the person is, and so the lower the oxygen uptake. And they're about the same magnitude, and that magnitude is much, much less than it was in the original least squares fit. So these coefficients have been regularized or stabilized by the application of principal components regression. So the next topic, which is related to principal components regression, is that of partial least squares. And unlike principal components regression, partial least squares seeks to explain the variation in the correlation matrix of the predictor variables, and simultaneously, the response variable. It works by successively extracting optimal linear combinations of the predictor variables, and it's related to canonical correlation analysis and factor analysis, which you may have encountered in the context of multivariate statistical analysis. Partial least squares is commonly used in chemometrics and in the analysis of high dimensional data in genetics. So I carried out partial least squares within SAS using PROC PLS. This time that's the default, so you don't have to specify method is equal to anything. And you can see that I have asked for a solution, which is coefficients, at the end, and then I've left everything at default settings within SAS. And this is the output that you get. And so we're trying to simultaneously explain the total variability amongst the predictor variables and the response variable. So that's-- the model effects is the predictor variables and the dependent variables is the response variable. So if we take two partial least squares combinations, we're explaining about 80% of the variability in the response variable. If you remember from an earlier slide, the maximum R squared that we get from ordinary least squares is only 0.85 or a little under, and so this is the vast majority of the variance in the response variable explained. It only explains about 64% of the variability in the predictor variables. If we were to move down to, say, four numbers of extracted factors, then we're explaining well over 80% of the variability in the predictor variables and in predicting the response variables. Now, on the plot on the right hand side, these values of R squared have been plotted. And my selection of the value four for subsequent analysis doesn't really have anything to do with the fact that the two cross over there. It has more to do with the fact that this is the smallest number of factors for which both the R squared for model effects and for the dependent variables are above about 0.8. So now I'm going to fit partial least squares models with four factors and with two factors, these being the two candidate models that were thrown up by the initial analysis, and I want to compare the results from both of these. In particular, I want to take a hard look at the solution, which is the coefficients, the estimated coefficients, for the original variables. So this slide is pretty densely packed, I'll own up to that. So if I look at the partial least squares fit with four factors, that explains 84.7% of the variability among the predictors and 83% of the variability in the response. And so that seems to be doing a little better than the partial least squares with only two factors, which explains only 64% of the variability among the predictors and 80% of the variability in the response. However, when we take a look at the parameter estimates, we say that for the partial least squares with four factors, the coefficients of run pulse and max pulse have opposite signs. They are much smaller than the ordinary least squares estimates, but they are of opposite sign, and we don't think that should be the case. Whereas if I look at the partial least squares estimates for two factors, then I see that both run pulse and max pulse have got coefficients which have the same sign, and that sign is negative, which makes sense in the context of this problem. So in this problem, we would prefer partial least squares solution with only two factors because it's simpler. We're really primarily interested in explaining the variability in the response variable, that's our prediction error, and the two factor partial least squares does almost as well as the four-factor. And then finally, the coefficients of run pulse and max pulse have the same sign in the two factor PLS. And so that indicates the regularization has been effective. So some brief conclusions about principal components regression and partial least squares. Both principal components regression and partial least squares may be used to fix colinearity and regression problems with relatively small numbers of variables. Both of them form orthogonal linear combinations of the predictor variables. Principal components regression seeks to explain variation in the predictor variables, whereas partial least squares seeks to explain variation in both the predictor variables and the response variables. Neither principal components regression nor partial least squares identifies important variables or subsets of variables. The principal components and underlying factors are all linear combinations of all of the original variables. So if you're trying to reduce not just the dimensionality of the regression, but actually the number of variables, to do some model selection or variable selection, then neither principal components regression nor partial least squares will do that for you. As we continue our discussion of regularization methods for multiple linear regression, the next group of methods that I'd like to discuss are so-called shrinkage or penalty methods. The granddaddy of these is ridge regression, which was developed in 1970 and was really way ahead of its time in terms of regression regularization. And so here's a brief description of how it works. The first thing that we need to do is to standardize all variables to have mean zero and variance one. That includes the predictor variables and the response variable. When you do that, you'll find that the intercept term drops out. In ordinary least squares estimation, the way that we would obtain estimates of the coefficients for the predictor variables is to minimize the residual sum of squares with respect to beta 1, beta 2, up to beta p. In ridge regression, we minimize a penalized version of the residual sum of squares. You have the residual sum of squares and we add on a multiple of the sum of the squares of the regression coefficients, so the beta j squared. As the ridge penalty, the parameter lambda increases, the parameter estimates get pulled towards zero. Parameter estimates that have been inflated because of colinearity are drawn disproportionately towards zero. The goal is to select the value of lambda at which all of the parameter estimates have stabilized. The purpose of ridge regression is to obtain good estimates of the regression coefficients for all of the different predictor variables. But in terms of predictive accuracy, it does pretty well too. Ridge regression does better in general than traditional variable selection methods, such as stepwise methods. So I'm first fitting ridge regression using PROC REG within SAS. And so you can see that I've done PROC REG and I'm going to output the coefficients. And I've chosen the ridge parameter to be between zero and one in steps of 0.4. Selection of the range for the ridge parameter is sometimes a bit more art than it is science, and this was not the first range that I tried for these particular data, but this seems to work quite well. So this part of the process can take two or three attempts. Of course, it runs very quickly in SAS. And then I've done the modeling, and I'm obtaining the variance inflation factors, which we're going to take a look at. So here is the output that you get for ridge regression in PROC REG. And so the variance inflation factors are up above the plot, but I'd like to turn your attention to, is this one here, with the standardized coefficients in it. And you can see that all of the regression coefficients are drawn towards zero, but in particular, the coefficients for RunPulse and MaxPulse are disproportionately pulled towards zero. So they are the ones exhibiting a greater degree of change. And in fact, the coefficient of MaxPulse actually changes sign, from being positive to around about here, around about ridge parameter 0.6, to being negative and then staying negative thereafter. Certainly, by the time we reach lambda is equal to 0.8, it looks like all of the parameter estimates have stabilized, and so that might be a reasonable value of the ridge parameter to choose. And here's the output that we get. And so I come down here to observation 42 and 43. And so this is corresponding to the ridge parameter being equal to 0.8. And I've highlighted the coefficients for RunPulse and MaxPulse, and you can see that both of them are negative, which is what they should be. And they are much, much smaller in magnitude than the original ordinary least squares estimates, which were of the order of 0.396-- 0.369, excuse me-- and 0.3032. So the effect of pulling in the coefficients using the ridge regression seems to have been rather good. So ridge regression can be a very effective tool for stabilizing regression parameters in the presence of colinearity, when there are modest numbers of predictor variables and we don't care about model selection. We don't want to reduce the number of variables. But just to reiterate that point a different way, ridge regression does not select subsets of variables or identify important variables. So it's not a model selection procedure per se. It is a regularization procedure to obtain sensible regression parameter estimates. So the least absolute shrinkage and selection operator, very widely known as LASSO, is the next topic. The motivation for this actually came from some earlier work by Leo Breiman, who liked the idea of ridge regression, but wanted to do some variable selection. And so he developed a methodology that he called the non-negative garrote. In his paper, Rob Tibshirani acknowledges Brian's work, and that it gave him the motivation to develop the LASSO. So in some respects, it is very, very similar to ridge regression. Step one, we standardize all the variables to have mean zero and variance one, and the intercept term drops out. As we said for ridge regression, in ordinary least squares, we minimize the residual sum of squares with respect to the unknown coefficients, beta 1 through beta p. For the LASSO, we minimize a penalized version of that residual sum of squares. It's lambda times the sum of the absolute values of the coefficients, not the squares, but the absolute values of the coefficients. So as with ridge regression, as the LASSO penalty parameter increases, the parameter estimates beta j hat get pulled toward zero. But in contrast with ridge regression, some of the coefficients are pulled exactly to zero. And so that's effectively deselecting the corresponding variables from the regression model. One can use the LASSO to select variables and then carry out ordinary least squares estimation, or to select variables and shrink the coefficients of the selected variables to improve the stability of the coefficient estimates. So there are several variations on the LASSO now. One of the things that we often want to do in a regression is to fit categorical variables and to put them into a regression. We create a bunch of dummy variables, and we would like those dummy variables to all be in the model or out of the model at the same time. And so the group LASSO allows us to do that, and is actually more general. You can take-- specify any group of variables to come in and go out at the same time. The adaptive LASSO imposes some additional weights on the LASSO penalty term. And in doing so, it has some very desirable asymptotic properties. And so there are many circumstances in which people recommend the adaptive LASSO over just the ordinary traditional LASSO. So how do we fit LASSO in SAS? The appropriate procedure is PROC GLMSELECT. And the plots that I want to show you are of the coefficient estimates as we increase or decrease the penalty parameter. In the model statement, the option is selection=LASSO. And then you'll notice here that I have chosen Schwarz's Bayesian Criterion as being the thing to select the best model. We'll discuss that a little more a little later on. So if I wanted to get out of this analysis, the least squares coefficients from variables selected by the LASSO, then I add another piece in the option down here. So in the second bit of code, you see LASSO, open parentheses, LSCOEFFS. And that's saying, print out the coefficients-- least squares coefficients for the variables selected by the LASSO. So here is the output. And just reading the table in the right hand side and looking at the graphs on the left hand side, we see that the SBC criterion, Schwartz's Bayesian Criterion, selects a model which has got three variables. Those variables are RunTime, RunPulse, and Age, and therefore Weight, RestPulse, and MaxPulse are not included within the model. So since we know that MaxPulse and RunPulse are highly correlated, including only one of them in the model makes a whole lot of sense. So in this rather congested slide, on the left hand side, I've got the LASSO parameter estimates for the variables retained in the model. And of course, they're slightly shrunk towards zero. And I've got the ordinary least squares parameter estimates. And across the board, the LASSO coefficient estimates are smaller in magnitude, by which I mean closer to zero, than the ordinary least squares estimates. The difference is small for RunTime. The differences between the ordinary least squares and LASSO estimates are a little larger for our Age and RunPulse. Recall that the ordinary least squares coefficient for RunPulse when MaxPulse was in the model was minus 0.36. And here you can see it is-- the ordinary least squares estimate when there are only three variables is minus 0.13, and when we use the LASSO parameter estimates, minus 0.08. So much smaller in magnitude, because those parameter estimates for RunPulse and MaxPulse were very substantially inflated by the colinearity of those two variables. We could use cross validation to select subsets of variables. But I want to point out that cross validation is more related to prediction error and really, in regularization, we're more interested in getting sensible estimates of the parameters. And so there is nothing to guarantee that cross validation will give us something particularly sensible. In this particular example, for the oxygen uptake data, I ran the LASSO and I used what SAS calls external cross validation, fivefold cross validation, in order to stop the LASSO process. And what we find is that it chooses a model which has got all six of the variables in. You can see that in the right hand side here. And that the coefficients for MaxPulse and RunPulse have blown up at this point and are opposite in sign. And so it would seem that cross validation has not chosen a very sensible model in terms of regularization of the parameter estimates. So in this particular case, the LASSO parameter estimates are identical to the ordinary least squares estimates, because all of the variables are in the model, and I think it's fair to conclude that in this case, cross validated error is not a good criterion to use for model selection. No variables were removed and not enough shrinkage took place. This has been my observation, is that cross validation and the LASSO do not necessarily go well with each other, and cross validation error is not necessarily a good criterion for using with the LASSO. I typically use things likes Schwarz's Bayesian Criterion, or AIC, or the corrected AIC. I'd like to give you another example. So this is modeling the quality of white wine, and it was published in 2009 in Decision Support Systems, and the authors used data mining to look at the different physical properties. Response variable in the quality of wine data set is a measurement of the wine quality on a scale from 0 to 10, with 10 being the highest. The median value of the score of three experts is what is used, and there are a total of nearly 5,000 observations in the data set. The predictor variables are chemical and physical characteristics of the wine samples, which include pH, density, alcohol content, as a percentage, chloride, sulfates, total free sulfur dioxide, citric acid, residual sugar, and volatile acidity. So in this case, I did try cross validation. And it turns out, whether you use cross validation, or SBC, or AIC, you get essentially the same thing. So you can see that I've done selection is LASSO, and cvmethod is-- split is equal to 10, which means do tenfold cross validation. And once again, we'll look at a plot of the coefficients. So the cross validated error drops off dramatically with the addition of the first two variables. So looking at this bottom plot here, first two variables, and then it sort of flattens out. The cross validated prediction error is minimized for a model with nearly all of the variables, so 12 of the 13 variables. Only citric acid is not in there. If you were to use other criteria such as AIC and SBC, you would get the same group of variables. So this is a data set where there are lots of observation, as it seems like you need most of the variables. And if I were to show you the correlation matrix for the predictor variables, you would see that there's not really any strong evidence of pair wise colinearity. So here are the coefficient estimates that we get, and we can interpret those. So increased quality is associated with the variables which have positive coefficients. And so that's larger values of alcohol, for example, and residual sugar, and pH. Values of pH, higher values of pH, associated with what the experts thought were better quality wines. Increased quality is associated with smaller values of density, of chlorides, and of volatile acidity. So the LASSO and variations of LASSO have quickly become the standard methods for regularization and variable subset selection in multiple linear regression. The LASSO combines the stabilization by shrinkage of ridge regression with the ability to deselect unimportant variables. LASSO works particularly well in large problems with many variables, the kinds of problems in which p-value and other traditional methods may not be helpful. The LASSO does breakdown when the number of variables exceeds the number of observations. In this case, the LASSO becomes quickly saturated and stops. But for many applications, the LASSO is now the standard methodology. When we go into select variables in a relatively large data set, we don't use selection is equal to backward, and set a criterion like alpha is 0.5 or 0.01. Usually, we will start with the LASSO, and then we'll move from there. So the third of the shrinkage or penalty methods for regularization is the elastic net due to Zou and Hastie. And it combines ridge regression and LASSO. The coefficients are shrunk towards zero. And like LASSO, the elastic net can deselect variables by setting their coefficients to zero. As with LASSO and ridge, we standardize all the variables to have mean equal to zero and variance equal to one. And then we minimize this residual sum of squares, which is penalized by the sum of the absolute coefficients, and-- which is the LASSO penalty term, and by the sum of the coefficient squared, which is a ridge regression penalty term. And so it's really interesting that you can get a better result by combining these two penalty terms. It's also interesting that the same least angled regression algorithm which you can use to select the-- or to estimate the parameter estimates, the parameters, the regression coefficients in the LASSO works for the elastic net. So here's a particularly nasty data set for an example. This was used in the original paper by Zou and Hastie, and it's a SAS example, in PROC GLMSELECT. And so there are training and test data sets. The response variable is the type of leukemia. So we've got type 1 and type 2, and these have been coded as minus one and one for the analysis. There are 38 observations in the training data and 34 in the test data set, and there are a total of 7,129 predictor variables, which are genes. When I first started out as a statistician, the notion of having several thousand variables, predictor variables, and only 30 or 40 observations in your training data set was just laughable, and we couldn't do anything with it. And so it is a measure of how far we have come, and how far-- how much these modern machine learning methods have helped us, that we can now consider these kinds of problems and get sensible answers. So the unresolved issue is the selection of the penalty parameters, lambda 1, lambda 2. One approach which we follow here is to select a very small value for lambda 2. So that's the term which goes with the ridge penalty, the sum of the beta j squared, and then use cross validation or a validation data set to estimate lambda 1. That's the penalty term for LASSO. So we can fit the elastic net in PROC GLMSELECT. And you can see that I've specified a training data set and a validation data set, which determines when to stop the process. And I want to see plots of the coefficients. Initially, I've done a LASSO here, just to show you what happens to LASSO. And then I've done elastic net, so that's the second piece of code. And I've chosen the second parameter term to be equal to 0.001. And I'm using the validation data to select when the process should stop. So here's what happens with LASSO, OK? After 73 steps, the model is completely saturated, by which I mean the residual sum of squares has dropped to zero. At that point, the LASSO just stops. It can't go any further. The optimal model is selected by the average squared error on the validation data set, occurred at step 68. And you can see that on the validation data set, the squared error there was 0.1531. That is indeed the minimum. Looking at the top part of the plot on the left hand side, you can see, the LASSO effect as coefficients enter the model, as the penalty term is decreased. And then the selection model, the model which is selected, occurs towards the right hand side of the plot. But beyond that, you can still see that the coefficients are changing quite substantially in some cases, as we add and drop additional variables from the model. So now we look at the output for the elastic net. And so you notice that on the right hand side of the plot, the coefficients smooth out very much. This is a lot like what we see in ridge regression. And so there's a great deal of stability in these coefficients. The validation data set stops after-- it says that the optimal value of asymptotic-- of average squared error on the validation data set is minimized at 105 steps. But you can see that I've gone out to 250, and that the coefficients are just sort of smoothly changing. The traces have sort of smoothed out somewhere between 150 and 200 steps, and we're not seeing much in the way of new variables coming in and out of the model. And when they do, they don't change the other coefficients very much. So other choices of validation, cross validation, with the data and other choices of lambda 2 yield very similar results. There's nothing special about the plot that I got for the choice of parameters that I made in this particular example. And so I think for these particular data, the case can easily be made that the elastic net works better in terms of identifying variables which might be important in stabilizing the parameter estimates than does the LASSO or indeed, any other kind of variable selection method. So now I'd like to move into nonlinear methods, moving away from the multiple linear regression model to other things, for regression and for classification. Some of these methods are direct generalizations of multiple linear regression. And so included in that category are generalized additive models. Others offer a completely different approach to nonlinear regression that allows for the inclusion of high order interactions. This includes all of the tree-based methods. All of these nonlinear methods have much higher predictive accuracy than traditional linear methods in certain situations. And all of these methods are available in various procedures in SAS. Generalized additive models are available in PROC GAM and PROC GAMPL. Decision trees, classification or regression trees, are implemented in PROC HPSPLIT. Random forests is implemented in PROC HPFOREST. And support vector machines are implemented in PROC SVM-- HPSVM. So I want to give an introductory example and talk about visualizing relationships using generalized additive models. So a very special case would be when we could assume that the response variable is at least approximately normally distributed. And then we have the additive regression model as listed here. So this differs from our ordinary multiple linear regression model in that the functions s1, s2, up to sp, are not simply coefficients creating a linear combination of variables. Instead, they can be continuous, so smooth, but highly nonlinear functions of the individual predictor variables. And so for example, the kinds of effects that we might see would be a quadratic function or an upside down quadratic function. The s's do not have to be monotonic. We could see threshold effects, where initially the value of the response increases with the predictable variable, then it plateaus out at some point, or the opposite, that it's plateauing at some point and then suddenly drops. That's another kind of a threshold effect. These are the kinds of things that you simply cannot see and cannot characterize in an ordinary linear model. So the object of fitting a generalized additive model is to try and estimate these functions, s1, s2, up to sp, using scatter plot smoothers, so non-parametrically, as opposed to specifying some parametric form, such as a cubic or a quadratic, for these particular functions. So I thought I'd illustrate this by the white wine quality data that we've seen previously. And the procedure that I'm using in SAS to fit the generalized additive model to these data is called PROC GAMPL. And I'm going to do scatter plots of these functions s, so that's what the plot is asking for. And I'd like to have separate plots, so I'm going to unpack them, and then I can repackage them as I wish. So as it's highlighted down here, you can see, distribution is equal to normal. It says that for this particular generalized additive model, we want to assume that the response variable is approximately normal in distribution. And then you see, around each of the variables which are predictor variables, which were included in the model statement, I have this function s. So s, open parenthesis chlorides, close parenthesis. So the s tells PROC GAMPL to consider this variable for a nonlinear fit. Now, one of the nice things about PROC GAMPL is that it works very well off the shelf, and in particular, it can make a determination as to whether an effect in a model should be just a linear effect or whether it should be considered as having some curvature to it. And so we'll see that in the output. So I want to make all of the six variables which I'm including in this model statement here candidates for nonlinear estimation. And here are two of the plots that I got, and I chose these two because in some sense they seem to contrast. So the first one, I would say, is almost linear. And so that is saying that on average, the quality of the wines as judged by the experts seems to increase or be associated with increased amounts of alcohol in really quite a linear way. I think the thing which surprised me about this plot was the range of alcohol content, from 8% all the way up to 14%. So this is a case where PROC GAMPL selected fitting a curve to the data, but it's not really that far from a linear fit. On the other hand, the free sulfur dioxide, that is a long way from a linear fit. And in particular, starting at about 100, on whatever scale free sulfur dioxide is measured, the quality seems to go down quite rapidly with increased amounts of free sulfur dioxide. And so that seems to suggest that the wines which have too much sulfur dioxide have a taste that contributes an element to the taste which the experts do not like. Here's another example. So this is a data set for which the LASSO works extremely well. There is data on air pollution and mortality for 60 jurisdictions within the United States. For the most part, they are large cities. The response variable is the mortality rate per 100,000 in population. The predictor variables include a bunch of socioeconomic variables, percent of the population over the age of 65, average educational level, percent in good housing, percent minority, percent below poverty level, and so on. There are some climatic variables, so average temperature in January and July, total precipitation, and average humidity. And then there are the variables which were of most interest, which are pollution variables, measures of hydrocarbons, nitrous oxide, and sulfur dioxide. These variables are very, very skewed, and so they have been warped transformed to improve their distributions. So I do an initial fit of these data in PROC REG, and I want to explore colinearity. And I find that the R-squared value that I get is approximately 0.8. That's the measure of the quality of fits, and that's pretty good but not outstanding for these kinds of data. I put as x in front of each of the predictor variables, and that allows me to use this very compact notation. So the x: means all of the variables which begin with x, and of course, that's all of our predictor variables. And so here are the colinearity diagnostics, and the parameter estimates, and so on. And so as I look down here, I see most of the variance inflation factors are not too bad until I get to the last two, for the log of hydrocarbons and the log of nitrous oxide, and they're up around 20. And if I come over to the condition indices, they're sort of gradually increasing, but then seems to start jumping after about 12 variables to 103, 133, 147, and then a huge jump up to 452. So it's evident that there's some colinearity within these data, and we would like to fix that, but I would also like to know, is the relationship between the individual predictor variables and the response variable linear, or would it be better characterized using some nonlinear functions? So here is a scatter plot of the relationship between the log of the hydrocarbons and the log of the nitrous oxide. And we can see that that's very close to linear, with a correlation nearly 0.95, so clear evidence of colinearity. In the previous slide-- so we could go back. We would see that most of the variables were non-significant, with high p-values. And so this is a data set that we can use the LASSO to help us out and select some appropriate variables. And that's what I do. I'm using PROC GLMSELECT again. Selection is LASSO, and I'm going to use Schwarz's Bayesian Criterion once more as a stopping criterion. And this is the output that I get. So the number of variables which are selected by the LASSO using the SBC criterion is eight. And so that includes a mixture of variables, such as non-white, percentage of non-white people in the district, population density, with nitrous oxide, and log of sulfur dioxide. And then some weather variables, precipitation and the average temperature in January. And there are a lot of variables which have been removed. So there were 21 predictor variables originally, and now there are eight. So 13 have been removed. If I look at how well this model fits the data, up at the top right hand table, the R-squared for this is 0.746. If you remember, for the multiple linear regression model with all of the predictor variables, it was about 0.8. So actually, the predictive accuracy of this model has not been decreased much by removing eight variables. And you can see in the plot, at the top, at the left, the effect of the colinearity with the hydrocarbons. So as soon as both of those variables are in the model, their coefficients go soaring off positive and negative. And so you can see the negative effects of colinearity there. So we can interpret the coefficients and parameter estimates on the right hand side. The mortality rate is lower in places which have a higher average educational level and which have a higher percentage of good housing. And mortality rates are higher with higher population density and with higher precipitation, interestingly enough. The variables sulfur dioxide and nitrous oxide come in with positive coefficients. So high values of these pollutants lead to higher values of mortality. Perhaps the one that I thought was most interesting was that the average temperature in January has got a negative coefficient. So the warmer it is in January, the lower the mortality rate, and that's suggesting that the warmer states in the South and the Southwest have got lower mortality rates related to the average temperature. And as someone who doesn't like shoveling snow, I can certainly sympathize with that particular conclusion. So now I want to take the same data set. I've done some variable selection using the LASSO, and it worked out, I think we would agree, very well on that particular data set. And what I would like to do is take some of the most important variables, not all of them but some of them, and use PROC GAMPL to determine whether the effect should come in as being linear, or whether there is some curvature, and what the nature of the nonlinear relationship might be. And so you can see that I am fitting splines as my smoother, and I get to choose maximum degrees of freedom that can be considered. And rather than the default value, I've chosen the relatively large value of six. And then a PROC GAMPL will go ahead and select as many terms as needs, up to this maximum degrees of freedom. And this is what we get. So these are the six plots of the estimated spline functions for those six variables, which I included. And if we look at education and precipitation, they both seem to be coming in as linear effects with opposite signs, so decreasing with increased education-- mortality decreasing with increased education, and mortality increasing with increased precipitation, and they're coming in as linear. And then I would say that if we look at the percentage of minorities, so non-white, and the log of nitrous oxide, then these two are pretty close to being linear effects. PROC GAMPL selected to fit curves to them, but if I look at the confidence limits here, I think I can probably fit a line through each of them. And so they are very close to being linear effects. For the average January temperature, that's a decreasing function and it's kind of wiggly. But even that, that is not too far from a linear effect. And for population density, I think one can almost fit the zero line through that. So there are some data sets for which there are very clear non-linear effects. And PROC GAMPL is good at identifying the variables for which the relationship with the response is nonlinear, and then estimating the nature of the response. But there are many data sets for which the relationships are at least approximately linear, and fitting a much more complicated predictive procedures such as generalized additive model really results in no benefit at all. And this is one of those data sets. So this is a data set in which the relationship between the predictor variables and the response variable are close to linear, and no additional interpretation or predictive accuracy is gained by fitting a generalized additive model. In many other applications, we do see some gain in accuracy by fitting a generalized additive model. Next topic that I'd like to cover is that of classification and regression trees, sometimes called decision trees. This group of methods, I have been using since the early 1990s, and I have found them to be helpful in understanding data and in predicting both categorical and continuous response variables in many different kinds of situations. And so when I get a new data set and I know I'm going to be doing classification or regression, one of the first methods that I turn to is a tree. So regression and classification trees are fully non-parametric. There are no distributional assumptions on either the predictor or the response variable. They naturally segment the data into increasingly homogeneous groups. Small trees are easily interpreted. And so when I introduce this topic to my classes and in other talks, I often talk about, imagine that you are going into a bar, and you meet someone that you've never met before, and you want to explain something about what you do and what kind of analysis you've carried out. Well, I can tell you that explaining a tree, which is-- divide a data set into smaller pieces, is a much easier thing than trying to explain logistic regression or even multiple linear regression. Trees can handle complex high order interactions. We're going to see that. They effectively handle missing values through surrogate splits. Many statistical procedures really struggle with missing values. You either have to impute data or you have to eliminate all of the observations which have got some missing values in some of the predictor variables. You don't have to do that with trees. And then in some situations, we find that trees are accurate predictors of numeric and classification variables. In the classification case, sometimes trees are about the same accuracy as logistic regression, and sometimes they are more accurate than logistic regression. And again, we'll see an example. So how do trees work? They work by recursive partitioning of the space spanned by the predictor variables into increasingly homogeneous groups, with respect to numerical or categorical variable that is being predicted. So this schematic to the right is supposed to illustrate the process. The two axes represent two predictor variables, and then the different rectangles are subsets of the observations. And so the first split, I can tell from this graph, occurs just before the value seven on the variable which is represented on the horizontal axis. And I know this is the first split because it goes all the way up through the rectangle, which represents the entire data space. So that was the first split. And then the second split occurred on the second variable, which is on the y-axis, at a value about somewhere between six and 1/2 and seven. It could have been the rectangle to the left that was created by the first split on the variable in the horizontal axis, or it could be the horizontal line on the second rectangle, the right hand rectangle. It doesn't really matter. And then the partitioning continues, and the bigger rectangles are generally subdivided into much smaller rectangles. The numbers that you see in each of these rectangles represent the number of observations. Firstly, they're not equal, not even close to being equal. And secondly, they're not proportional to the size of the rectangle. So at the bottom left corner, we have a relatively large rectangle with only 22 observations. And near the middle of the plot, at the bottom, we have a tall, thin rectangle which has got 33 observations, and certainly hasn't as much area as that bottom left rectangle. So why do we need such a methodology? It seems very strange to be subdividing data. It's sort of like a digital approach to statistics when we're used to doing linear and smooth things. And to partly motivate it, take a look at the graph on the left hand side of this slide here. So there are two predictor variables, which are labeled x1 and x2, which are here on the horizontal and vertical axis. And this is a classification problem. And there are two classes coded as zero and one. In this plot here, you can see that the ones are in the first and third quadrants and the zeros are in the second and fourth quadrants. If you try and apply-- this is a standard problem in computer science. It's a test problem for new techniques, and we picked it up in statistics. It's quite a useful way of looking at things. So linear methods such as logistic regression or linear descriptive analysis do incredibly poorly in this problem in terms of prediction, whereas classification trees which are subdividing up the space, you can kind of see that it would work well here. And indeed, classification trees and methods related to classification trees do extremely well. Now, I'm not suggesting to you that there are real data sets out there which look like the XOR problem here, but what I do maintain, and which I think we see evidence of in one of the later examples, is that there are real data sets out there in which the boundaries between the different classes are really complicated, and for which linear methods are not going to work so well. So to illustrate the efficacy of trees over other methods, I want to start with an example about credit card application data. And so they concern credit card applications to a bank in Australia, and the data were first published by Quinlan in 1987. The response variable is coded as yes if the application for the credit card was approved, and no if it was not. There are 15 predictor variables, which are denoted by A1 through A15. Some of those are categorical and some numerical. For proprietary reasons, the nature of the variables is not available to us. We note that variables A9 and A10 are coded as t and f, which we take to mean true and false. There are a total of 666 observations with no missing values, and of those, 299 persons were approved for credit cards and 367 were not. And so the split between approved and not approved is actually pretty even in these data, and it's got a mixture of categorical and continuous predictor variables. And so it's quite an interesting data set. So as a preliminary analysis, I applied a standard technique, logistic regression. So in the class statement, I have the categorical variables A1, A4 through 7, 9, 10, 12, and 13. And then in the model statement, I've got A1 through A15, so the variables which are not in the class statement are continuous. I've chosen to do some backwards selection of the variables to eliminate unimportant variables, but I'm going to do that using backward elimination with a significance level to stay of 0.05. We could use LASSO, we could use group LASSO, we could use other methods, but I chose to just take a simple solution here. The ctable statement within PROC LOGISTIC produces a classification table for a particular cutoff or a group of cutoffs, and part of that table are some of the usual metrics for what we use for evaluating the accuracy of a classification, sensitivity, specificity, percent correct, and so on. So given that there is a relatively equal split between approvals and non approvals, I've chosen to use a standard cutoff of 0.5 here. The ROC curve is a good way of summarizing the predictive accuracy of a classification, a binary classification, in a graphical way. And so I always request the ROC curve when I am carrying out a binary classification, as we are here. And here's some of the output from PROC LOGISTIC. So let's start on the right hand side with the ROC plot. Each time we remove a variable, it recomputes the ROC curve, and we see that when it removes eight total variables, all of those ROC curves are practically overlaid. You can scarcely tell there are eight different curves there. And so that suggests that we lost absolutely nothing in explanatory power or predictive accuracy by eliminating those eight variables from the model. Indeed, the AUC criterion goes from 0.9506 for the model with all the variables down to only 0.9463 after we eliminate eight variables. So there's no question that removing those variables is a good thing to do. It simplifies the model but doesn't affect the fit or the predictive power of the model. Now, in terms of accuracy, the overall percent correct is 87.4%. So that means the error rate is 12.6%. That's a number worth remembering for a future discussion. The sensitivity was a smidge higher than the specificity, 90.7% versus 84.8%. Overall, that's a pretty good fit, we would have to say. So now, the real question is, what of the important variables and how are they affecting the response? And so to answer that question, we might look at the logistic regression coefficients, and here they are. And the question that I pose here is, what can we learn from these parameter estimates? And there are so many of them. And really, without some getting down in amongst the weeds, it's very hard to conclude anything about the effects of different variables from this output. So now I want to fit a classification tree to these data. And the procedure within SAS for fitting classification trees is HPSPLIT. It's a relatively new procedure within SAS/STAT, and it has been updated, and it is very nice. So I want to evaluate the accuracy of the classification using cross validation. And you can see here that I've specified cvmethod=random(10), and that says, please do tenfold cross validation. The only plot I want from this initial step, which is determining how big a tree we should fit to the data, is this plot which is labeled cvcc, and that is a plot of cross validated area as a function of the size of the tree. So that allows us to choose an appropriate tree size. The criterion by which I want to measure how consistent the observations in the group are is the Gini index, and so I specify that with grow gini. The default with an HPSPLIT split is entropy. If you try entropy and then you try Gini, you'll often come up with very, very similar answers. So there's not much to choose between these things. I just happened to select Gini. And here is the plot that we get. And it turns out that the minimum cross validated error occurs at a tree with five terminal nodes, five leaves. These are the subgroups of the data when the process is stopped. That's a pretty small tree. That's a very easy tree to interpret. Breiman et al.'s 1-SE rule says, you don't choose the one for which you've got the minimum cross validated error. You choose the smallest tree for which the cross validated error is less than or equal to the minimum plus one standard error. And that's a much smaller tree. That has only got two terminal modes or two leaves. So that's really saying, in some sense, the best tree is just subdividing the data into two pieces, and that just seems kind of insane and inconceivable. Well, here's what the tree looks like. So in the full data set, there are 299 applications that were approved. That's 45%. And there were 367 that were not, and that's the information that you get in the node which is labeled zero. And so the majority case is one, which is really zero, so that is the ones which were not approved. Then we carry out a split on the variable A9, and 352 observations go-- have t as being the level of variable A9, and 314 have f as being the level of variable A9. If we look at the observations for which t was the value in variable A9, then almost 80% of them were approvals for the credit card. If we look at the 314 observations, for which f was the value in variable A9, then almost 94% of those were not approved for credit. Now, we don't know what the variable A9 is or was. I suspect it was something like, have you had a credit card before? And true would be yes, so those people are people who have already developed a credit history, and would be good candidates for a new credit card. And then those people who've never had a credit card, the bank might decide to be conservative and say, you know, we're not willing to take a risk on most of those people. It could be something else, but that's my guess at what variable A9 is. What I'd really love to point out is that this tree just split the data set into two pieces, and from that, we were able to get a pretty good idea of what was going on. Can we do much better in terms of cross validated error rate? For that tree with just two leaves, the cross validated error is 13.74%. Remember that for the logistic regression with seven or eight variables, it was 12.6%. So that's not a great deal of difference for a very much simpler model, just two subgroups of the data. The cross validated error, if we increase the tree to have five leaves, actually increases to 14.36%. And for 10 leaves, it's about the same, at 14.44%. I applied to state of the art classifier to these data called random forest, and I found, about the best you could do was 12 and 1/2% error rate, about the same as you get for the logistic regression, and only a tiny bit less than this very, very simple tree. So pretty much all the information in this data set about whether an application will be approved or not is actually in this variable A9, and it partitions the data into just two pieces of approximately equal size. I think that's a spectacular demonstration of how well the tree worked. Now, we've talked about the wine quality data and we applied both the LASSO and then generalized additive models to these data. And now I want to apply a regression tree to the data set. Remember, the response variable is the quality of the wine sample as judged by three experts. We take the median value. It's on a scale of 0 to 10, with 10 being the highest quality, but the vast majority of values are in the mid range. There are nearly 5,000 observations here. The predictor variables include chemical and physical characteristics of the wine, including the pH, density, alcohol content, chlorides, sulfates, total and free sulfur dioxide, citric acid, residual sugar, and volatile acidity. And so I get my cost complexity plot, which is plot is of the cross validated average squared error on the vertical axis, against the size of the tree. The minimum cross validated error occurs at a large tree with 57 leaves. However, the 1-SE rule of Breiman et al.-- so that's choosing the tree which has cross validated error less than the minimum plus one SE-- that occurs at a tree with just five terminal nodes, five leaves. And so that's a very small, very interpretable tree. So now I'm going to refit a tree with just five leaves. And so the way I do that is prune Costcomplexity Leaves=5. You'll notice that I've circled this intervalbins=10000 here. The original algorithm for classification and regression trees said that you should sort all the values of all the predictor values into ascending order. And then you would choose as candidate cut points all of the midpoints between consecutive observations. The algorithm implemented in SAS doesn't quite do that. It takes the range of each predictor variable and splits it up into 100 bins of equal widths. I wanted to get a little closer to the original algorithm, and one way of doing that is to choose interval bins is equal to a very large number, and then most of those bins will only contain a single observation. And the results should be very similar to what you would get using the original classification tree algorithm. I'm using cost complexity, so I'm going to prune back to a tree with five leaves, because that's what the 1-SE rule tells me I should do. And here is the tree. And it may be hard to read these values. I'm sorry, I don't know how to enlarge those values. But at the root node, there are nearly 5,000 observations, and the average quality score is about 5.88. The first split is on alcohol at a value of 10.801. For the 3,085 wines with alcohol less than 10.8%, the average quality score is 5.6. Whereas for the 1,813 wines with alcohol greater than or equal to 10.8, the average score is a little higher, at 6.34. For the wines with alcohol content less than 10.8%, the next split is on volatile acidity, at a value of about 0.25. The 1,475 wines with volatile acidity less than 0.25 had an average score of 5.8725. And the 1,610 wines with volatile acidity greater than or equal to 0.25 had a slightly lower average score of 5.3. This is consistent with a negative regression coefficient for volatile acidity. The second split for the wines occurs with alcohol greater than or equal to 10.8% is on free sulfur dioxide at a value of 11.012. This is much less interesting, because only 114 out of the 1,813 observations end up in the node, the terminal node or leaf corresponding to free sulfur dioxide, less than 11.02. How well does the tree fit the data, compared with the regression models that we had earlier? If we use a tree with just five terminal nodes, five leaves, then the cross validated prediction error sum of squares is 0.5892. And that compares reasonably well with what we got for using the LASSO with multiple linear regression, where the CVPRESS was 0.5679. If I were to fit a much bigger tree, so getting closer to the minimum cross validated error, a tree with, say, 57 leaves, then I can decrease the cross validated prediction error to 0.5485, which is better than anything we got from linear regression. There is a cost to that improved accuracy, and that is you get a tree which has got 57 leaves or terminal nodes, and that's a much harder tree to interpret. I would like to do one more example on classification and regression trees. This is another bank example, and it has some elements in common with the earlier example we did on credit card approval, but there are also some interesting differences in the analysis of this data set, and I think it's worth going through. So I actually want to show you how I would analyze these data using SAS OnDemand. I'll introduce the data with a couple of slides, and then we'll switch to SAS OnDemand to actually conduct the analysis. So there was a direct marketing campaign by a bank, and what they were trying to do was to encourage their customers to take out term deposits with them. And the campaign was a mixture of direct contact by telephone calls, and by direct mail, and simply advertising on TV, and other aspects. The response variable in this data set is coded as yes if the customer did subscribe to a term deposit, did make a term deposit, and no otherwise. There are 45,211 observations, so that's quite a big data set. When I first saw that, I thought, boy, this is going to challenge some of the techniques, especially if we're going to do tenfold cross validation. There are 16 predictor variables in the data set. Some are categorical and some are numerical. And here is a list of those predictor variables. So the age of the customer, the type of job he or she has, marital status, education, does the customer have-- is the customer currently in default on some credit line, average yearly balance, presumably in all accounts, in euros, does the customer have a housing loan-- so I'm thinking that's probably a mortgage-- Does the customer have a personal loan, the last contact type, the day of the last month of contact-- I can't see why that would be relevant, but I could be surprised-- and the last month of contact-- that could be interesting. There could be some months which work better than others-- duration of the last contact, telephone contact-- this is in seconds-- The number of contacts with the client in this campaign, number of days passed since the customer was contacted in the last campaign to get people to take out term deposits, the number of contacts with this client previous to this campaign, and the outcome of the previous campaign. So success is that the customer took out a term deposit and failure is that the customer did not. OK, so that's the setup, and now let's skip to the analysis in SAS using SAS OnDemand. I have set this up in SAS OnDemand, so I've already written the code. And let's take a quick look at the data set. So in particular, I want to look at the outcome variable, which is yes if they took out a term deposit and no if they didn't. I suspect that many more people would say no than would say yes, and so let's actually run that and see. And yes, the answer is that only 11.7% of the customers did take out a term deposit. And so that means over 88% did not. This number here, this 11.7%, I want to keep this in memory because we're going to come back and refer to it. There's a huge imbalance in the data here. So there are almost eight times as many people who did not take out a term deposit, who did. And this has some profound implications for prediction. Usually, we find that the class which is most common is predicted very accurately and the class which is less common may be predicted quite poorly. So given that it's a large data set, it makes perfect sense to partition it into training and test pieces, so that we can get a very objective estimate of how well the predictions are doing. And so I'm using PROC HPSAMPLE to do that. And so I specify all the variables in PROC HPSAMPLE. In the class statement, we put the categorical variables, and in the var statement, the continuous variables. I'm then going to take the-- oh, in the sampling, I'm going to choose 70% for the training data and 30% for the test data. And the way that HPSAMPLE does this is that it creates a variable called _PARTIND_, and there are two values, zero and one. It'll be-- 30% of the data will have value one on PARTIND, and 70% will have value zero. And so we can split off those two data sets, and you can see that I have done that here. I've created a data set called BankTrain, which is the 70%, and another data set called BankTest, which is the 30%. So I will run this. And it's made the split. And so the 30% corresponds to 13,563 of the original 45,211 observations. If I go to the log file, I can see that the data said BankTest has got these 13,563 observations and the data set BankTrain has got 31,648 observations. So that's exactly what we hoped for. Well, the next step in my analysis is to fit a classification tree and to try and determine the appropriate size of the tree. And so that's what the next piece of code does. Exactly as it was in previous examples, in the first pass through, I just want to find out what the appropriate size should be. And so that requires this CVCC plot, which is a plot of the cross validated error against the size of the tree. And in the class statement, I put the categorical variables, in the model statement, all of the variables. And then I'm going to use the Gini index to grow the tree. So we run it, and it's going to take a while because we've got training data set of 35,000 and we're doing tenfold cross validation. And so that's going to be 10 data sets of about 31,000, and it just simply takes a bit of time to do these calculations. OK, and there it is. So that took a couple of minutes, I think. And here are the results. This is the item that we want to look at, the cost complexity plot. What you see is the minimum average misclassification rate occurs for a tree with 73 terminal nodes or leaves. That's actually a very large tree. But if I apply Breiman et al.'s 1-SE rule, that selects a much smaller tree. What I do is I look for the smallest tree for which the cross validated area is less than the minimum plus one standard error, and that's probably five or maybe six in terms of tree size. And for future analysis, I chose a tree with six terminal nodes. So returning to the code, this is-- this next block of code here is fitting a tree with six terminal nodes. So I'm going to grow the tree using the Gini index, as before. But now I prune using cost complexity, and specify that there will be six leaves or terminal nodes. The next statement with code, file is equal to, is placing-- is creating a destination for the code that we can use for scoring new data sets, and in particular, the test data set for this analysis. So let's run the code, fit the tree with six leaves or terminal nodes. That too is going to take a little while because of the tenfold cross validation to evaluate the error, but not that long. And what we find amongst the summary statistics is that the overall cross validated error rate is about 10%. So that means 90% is correct classification rate. The specificity is fantastic. So that's for the no category, and that's at nearly 97 and 1/2%. But the sensitivity is really, really bad. It's down at about 1/3, and really, I think that's completely unacceptable. So in terms of looking at the tree, we can certainly interpret the node-- nodes in the tree. And we'll notice that the first split is on a variable duration. So this is the duration of the phone call. And the cut point takes place at 540 seconds, which is about nine minutes. If I come down the right hand side of this tree, then the next split is also on duration. And so there is a group here for which the duration of the call was greater than or equal to 836 seconds, which is about 14 minutes. And that group of people, which is quite a large number, 1,228, the majority of them, so more than 1/2, elected to take out a term deposit. And so the phone call was a very good indicator. The length of the phone call was a very good indicator as to whether the person takes out a term deposit. Presumably, that's because the customer is discussing with the agent over details about the term deposit and making arrangements to come in and sign any paperwork that's necessary. If we take the left hand branch here, so this would be the duration of the call, between 540 seconds and 836 seconds. But then we take the right hand side down here-- so p-outcome is the outcome of the previous campaign. And so if that outcome was successful, so if the customer had previously opened a term deposit, then the customer is much more likely to be willing to do that again. The probability or percentage is 0.83 here. But this is only a tiny group of people, whereas the group up here in duration greater than 836 was 1,228 people. This group of people here is only 83. OK, so here is the RSE curve for these data when you've got such a small number of terminal nodes. Then it tends to look this sort of piecewise linear effect. So what we'd like to do now is to predict onto the test data set, to get a completely unbiased version of the accuracy of our prediction. And I've chosen to do it in two ways. Firstly, I'm going to use a 0.5 cutoff, the traditional cutoff, for the probability of the outcome being yes. And then I'm going to use a smaller cutoff. So this is a value of something like 0.117, which is the number that we saw earlier as being the proportion of the data set for which the outcome is equal to yes. And then once I get the nodes, I'm going to construct the confusion matrices on the test data. So OK, so let's run this piece of code, going from here all the way down to here. OK, so predicting onto the test data set using the usual cutoff of 0.5, we see that the sensitivity is 34.29%. That's very poor. So that's the same as we saw with the cross validation, whereas the sensitivity, so that's the proportion of noes who are predicted as no, is about 97 and 1/2%. If we use a cutoff of 0.117, then the situation is much better. So the sensitivity is now over 50%, at nearly 55%, and that's entailed a small loss in the sensitivity, from 97% down to 92%. I still would like to do better with the sensitivity, and I'm wondering what one could do to do better. And so it occurred to me that I should look at the predicted values that are coming out from the decision tree. Because there are only six terminal nodes or leaves, that means that there are only going to be six predicted values, and here are those predicted values. So by changing the cutoff from 0.5 to 0.117, I put these 842 observations and 85 observations into the predicted outcome is equal to yes category, but that's still a tiny number compared to the 11,807 which have got a predicted value of 0.6, and therefore are predicted no. And so as I think about this, the fix has to be to break up that group into smaller pieces with different predicted values. And the way that we can do that is by fitting a tree which has got more terminal nodes or leaves. And so that's the next piece of code that I have down here, is I'm going to fit a tree with 22 terminal nodes, and everything else is the same as for the tree with six terminal nodes. And then I'm going to create the two confusion matrices using a cutoff 0.5 and 0.117. And it worked really quickly, once again. So once you prune the trees, even with the cross validation, the fitting process is very, very quick. So you can see in this graph here that all of the nodes which are terminal nodes, which are pink, are ones where the outcome equal yes was in the majority, and all the blue ones are outcome is equal to no in the majority. And I come down here to the summary statistics, the cross validated error rate is 9.63%, which is actually a little less than for the tree with six terminal nodes. But the sensitivity is still terrible at 44%. Then the specificity is still very high, at 96%. When you add more terminal nodes, the ROC curve is going to look smoother, and that's what we see here. And now we can look at the confusion matrices for predicting onto the test data. And if we use the usual cutoff of 0.5, even with a tree with 22 terminal nodes, the results are pretty terrible in terms of the sensitivity. That's 40%, so we're only correctly predicting the yeses at a rate of 40%, and that's absolutely terrible. But if we change the cutoff to 0.117, well, that makes a huge difference. Now I see that we're correctly predicting over 80 and 1/2% of the outcome is equal to yes, and about 81.7% of the outcome is equal to no. So the sensitivity and specificity are almost the same here. I think most of us would regard this as being a much more satisfactory result. It's been carried out-- the improvement in predicting outcome yes has been-- taken a toll on the accuracy of the prediction of no. So this number here has dropped from about 97% down to 81%, but then the two-- sensitivity and specificity are about the same. I'm sure that the bank is much more interested in the customers who said yes and the characteristics of those customers, and they may want to further change the cutoff so that they could increase this number, the accuracy of prediction of the yeses, even more than for the noes. OK, so that's the analysis that I carried out in SAS. And as you can see, it didn't take very long to get some sensible information out of the analysis. So returning to the PowerPoint slides now, I have a summary here of what we've just been talking about in the slides. There are almost eight times as many customers with outcome as no as outcome as yes. And so we'd say the data are imbalanced. This is a fairly common problem in many applications. Certainly, in the ecological statistics that I am very familiar with, I encounter imbalanced data almost all the time. Most machine learning classifiers will predict the more abundant category very accurately and the less abundant category much less accurately. We saw that, right? So the noes were predicted very accurately, perhaps 97% or more correctly predicted, whereas the less abundant category, the yeses, the sensitivity was less than 0.5, and some cases, 0.33 and 0.4. So there are several ways of fixing imbalance. One is to up sample the less common classes, and you could use PROC HPSAMPLE to help you with that. Another method is to down sample the more common classes. By far, the simplest method is to adjust the probability cutoff for the less abundant class, and that's exactly what we did when we fitted a tree with a few more terminal nodes. That seemed to work very, very well. Here is the code for partitioning the data set into the 70% and 30% pieces. So in PROC HPSAMPLE, I had specified partition, and then samp percent to be 30, to split it into a 70-30 split. And then I specified all of the variables. And then the next piece of code is splitting the data into the training and test pieces on the basis of the variable _PARTIND_ that is created by PROC HPSAMPLE. And in fitting the classification tree, you saw this code in the live session and here it is again. I used cross validation, tenfold cross validation. And initially, I just want this plot of the cross validated error rate against the size of the tree, in order to determine what an appropriate size would be. And I used the Gini index as a measure of homogeneity of the observations in the leaves. And here's our plot. And we selected a tree with six leaves. The 1-SE rule would say five leaves perhaps, but six is only one more leaf, and it's still a very small and interpretable tree. And the overall accuracy was 90%, but as we saw, the sensitivity was terrible. And so that's when we decided to change the cutoff, and that improved things a bit but not enough with the tree with six leaves. So then we fit a tree with 22 leaves, and for the cutoff of 0.117, we got this confusion matrix, with the percent correct sensitivity and specificity all about the same, at about 81%. And so this seemed to be much better than the previous results that we got using the traditional cutoff of 0.5. So the reason I did this example was twofold. Firstly, it's actually a large data set, and so things take time, and it can be a challenge to analyze larger data sets. The second reason was I wanted to discuss the imbalance issue and how one might fix it. Of course, the simplest way is just to adjust the cutoff, and that seemed to work quite well here. I've also, in other applications, used up sampling. I've never had good success with down sampling. Usually, I get much less accurate estimates of the sensitivity and the specificity. So in terms of the interpretation, we talked a little bit about this. If the phone call, the last phone call, took more than about 14 minutes, that was a good sign for the bank in terms of the customer opening a term deposit. And also, if the duration was between 9 and 14 minutes, and the previous outcome was that the customer made a term deposit, that was also a positive outcome. So finally, some conclusions about classification and regression trees. They are a completely different, highly nonlinear method for predicting interval-valued and categorical response variables. They work by recursive partitioning of the data into increasingly homogeneous subgroups. The predictive accuracy of classification and regression trees is comparable to, and in some cases greater, than multiple linear regression and logistic regression. And the fitted trees, especially the small ones, are very easy to interpret and can reveal structure in the data that is not well characterized by other linear methods. The example that I would point to is the credit card application data, where a single split of the data into two subsets was as good as logistic regression and much easier to understand. So random forests is an absolutely state of the art machine learning, statistical learning, methodology that was originally due to Leo Breiman in 2001. As the name suggests, what it does is combine predictions from many classification or regression trees to construct a more accurate prediction for each observation. Random forest's origins lie in some observations made by Breiman about classification and regression trees. He noted that the trees were very unstable with respect to the data. Small perturbations in the data could yield completely different trees. And secondly, he was disappointed in the accuracy of classification and regression trees. In some problems, they work very well but in many other problems, the accuracy of trees was not substantially better than that from linear methods, such as multiple linear regression and logistic regression. So the algorithm, the basic algorithm, for random forests goes as follows. Many random samples are drawn from the original data set. The observations in the original data set that are not part of the random sample are said to be out-of-bag for that sample. To each sample, a regression tree is fit without any pruning. So it's a fully grown tree and it does indeed over fit the data. The fitted tree is then used to make predictions for all the observations that are out-of-bag for the sample tree, for the sample that the tree is fit to. This is very much the cross validation idea. You fit on some portion of the data and you predict on the other portion. In the case of random forests, the other portion are the observations which are not part of the random sample. For a given observation, the predictions from the trees on all of the samples for which the observation are out-of-bag are then combined. In regression, this is accomplished by averaging the out-of-bag predictions. In classification, this is accomplished by voting the out-of-bag predictions, so that if class A had 50-- if there were 50 predictions of class A and 40 of class B, then we would choose class A. Class A would be the prediction. The devil is in the details, so they say, and certainly, there are a lot of details in random forests. The number of samples to draw from the original data may be determined by the user. The default in PROC HPFOREST in SAS is 100. And in the vast majority of problems, the error rate does not change much if more samples are drawn and more trees are fit. Samples are drawn without replacement, and the size of the samples as a proportion of the size of the data set may be selected by the user. In PROC HPFOREST, the default is 0.6, and that turns out to be a pretty good value. In the few times that I have tried other values, 0.6 or something close to 0.6 has always turned out to give the most accurate predictions. The number of variables available to be split on at each node in a tree may also be determined by the user. For regression, the default is 1/3 of the total number of predictor variables. And for classification, the default is the square root of the number of predictor variables, which can be a very small number compared to the total number of predictor variables. The reason for restricting the number of variables that are available to split on is that it ensures that the trees fit to the different data sets are very different from each other, and it's in those situations that one derives benefit from combining the predictions of many trees. So I'd like to illustrate the use of random forest to you, combined with other methods that we've talked about, with an example from my research, and this concerns invasive species in Lava Beds National Monument. The Klamath network of national parks and national monuments in northern California and southern Oregon had identified invasive species as being its most pressing problem, and they were looking for research help in this area. It has been noted before in other publications that the US spends perhaps 40 billion a year dealing with invasive species. And so it is a financial concern, and it's very much of ecological concern, because invasive plants and animal species can often displace the native plants and animal species. We chose Lava Beds National Monument to illustrate some methodology because they had the best data. The most common invasive species in Lava Beds National Monument was something called common mullein. That is the picture on the left, and its scientific name is Verbascum thapsus. So that was a primary interest. Park personnel, when they detected growths off mullein, would go out there with bottles of Roundup, and exterminate it. But it can take several years of applying Roundup to fully kill a patch, and at that-- while that's going on, the mullein may be coming in in different places in the park, and they wanted to know where to look. There are other invasive species in the Lava Beds National Monument, that include nettles, two or three different types of thistles, and something called white horehound. So what does the data look like? Well, the response variable is presence or absence of mullein, coded as one for presence and zero for absence, on 30 meter by 30 meters sites. So we digitized Lava Beds National Monument into 30 meter by 30 meter sites, and we had data from a little over 12,000 of those 30 meter by 30 meter sites. We had a lot of predictor variables. So they include topographic variables, such as elevation, slope, and aspect. And they include what we call bioclimatic predictors, so temperature, precipitation, moisture index, potential global radiation, vapor pressure, humidity, and degree days. And then we also had the distances from every single one of these 30 meter by 30 meter pixels to the nearest road or trail. I've put that in red, because that turns out to be very important in these analyses. The expert on invasive species who was working with us said that the literature is very clear on the fact that the vectors by which invasive species get into national parks and other areas is through the boots of people who are hiking in the parks and on the wheels of the cars and trucks which come into the parks and monuments. So I carried out an initial analysis using logistic regression with all 31 of the predictor variables. And then I did some variable selection, just simple stuff, by backward elimination, with a significance level to stay at 0.1. And you can see the results in the table below. They're pretty much exactly the same for the full model, and for the model with 15 variables, so 1/2 of the predictor variables, percent correct is about 77 and 1/2 to 77.8. And the specificity and sensitivity are of similar values. So that's an error rate of about 22% to 22 and 1/2%. Since we've been talking about LASSO as a method for regularization and for model selection, I decided to apply the LASSO to these data. You can apply LASSO to logistic regression, just as you can to ordinary multiple regression. The procedure in SAS which you can use to do this is PROC HPGENSELECT. And so I did that, and the PROC HPGENSELECT selected just eight variables. So that's 1/2 as many, again, as the backward elimination did. There's a cost in the simplification of the model. The percent correct dropped through about 77.8 to 73.8, so a drop of 4%, and corresponding drops in the sensitivity and specificity. But if you're interested in model simplicity, then eight variables is a lot better than 15 variables. So I've done some baseline calculations here with logistic regression, and now I'd like to apply some tree-based methods and see if they can do any better. So the first thing I did was to fit a classification tree. You see, it says there, classification tree with 214 terminal nodes. You might think, boy, that is absolutely crazy. There's no way in the world that you can interpret the tree with 214 terminal nodes. That's true if you want to interpret the whole tree. Well, we can certainly look at the first few splits on a tree and do some interesting things. And if we're interested in prediction, then this is a tree which gives very high accuracy. The number 214 came from applying Breiman et al.'s 1-SE rule to these data, and following exactly the same process that I have done with earlier examples, when I was fitting regression trees or classification trees. So this classification tree its cross validated percent correct is 87.4%. So that's an error rate of about 12.6%, and that's only a little bit more than 1/2 of the error rate of logistic regression. In other words, the classification tree absolutely smashes logistic regression out of the park in terms of accuracy on these particular data. So the next stop is random forests. Can random forests yield even more accurate predictions? So the first thing to do is to talk about fitting random forests, and the procedure within SAS for doing that is PROC HPFOREST. You can see that I specified the maximum number of trees as 500, when the default is usually 100. I did that in part to show you that there's very little change in terms of the out-of-bag accuracy between 100 and 500 trees. Now, scoreparole=oob is probably not an option that would have jumped into your head. So what this is saying is that for constructing predictions for the observations in the training data set, then one should use the out-of-bag predictions, and that's exactly what the algorithm says that you should do. So PROC HPFOREST is not part of SAS/STATS and it has a different syntax. To specify the predictor variables, we use input statements. And you can see here, I've got input, and then a list of variables, and / level = interval. So that's saying that all of these predictor variables are continuous interval valued variables. If I also had some categorical variables, I would need a second input statement, and I would do level = nominal for those other variables. The way that you specify the response variable in PROC HPFOREST is using the target statement. So the presence or absence of mullein, coded as one and zero, is the response variable. And I want PROC HPFOREST to understand that this is a classification, not a regression, and so I need to specify level = nominal. Finally, the score out= is giving a name to the output data set, which contains the scored values for all different observations. So I ran PROC HPFOREST forest on these data. You might think with 500 trees, this would take quite considerable amount of time, but it does not. FOREST is very fast and efficient compared with comparable machine learning algorithms. So it only took a few seconds, and here is some of the output that I got. So on the left hand panel, we've got loss reduction variable importance. One of the features of random forests and other tree-based algorithms is that they give you estimates of the accuracy of-- the estimates of the importance of various predictor variables. And you can see here, the column to look at is the out-of-bag Gini, that relative humidity diff is the most important variable, followed by MinTempAve, all the way down here. Among these variables are DistRoad, the distance to the nearest road, and DistRoadTrail, the distance to the nearest road or trail. And among the other variables are difference in summer to winter precipitation, and average precipitation, average temperatures, and maximum temperatures, and so on. Now, what we're particularly interested in this example is predictive accuracy. And so the two columns to look at have been highlighted in red. The first column on the left hand side of this table contains the number of trees, and I've heavily edited the output table, which includes the information for 1 up to 500 trees, and I've just done 1 to 5, and then 10, 25, 50, 100, 250, and 500. And then the out-of-bag misclassification rate is equivalent to cross validated error, and that's what we're interested in. You can see by 100 trees, it's about 6.59%. If we go all the way out to 500 trees, it only drops a little bit more, to 6.47%. But that's 6.47% is a very impressive error rate. That means the percent correct is 93 and 1/2%, and that's what we see in this summary table here. So I've added a row for random forests, and it is a good 6% better than classification tree, which in turn was 10% better than logistic regression. And across the board, random forests is spectacularly accurate. The last topic for this workshop, I want to give you just the merest introduction to this support vector machines. As stated in the introduction, the original idea is due to Vapnik and Chervonenkis in the early 1960s. The modern so-called soft margin algorithm that is in widespread use is due to Cortes and Vapnik in the mid 1990s. And in his book The Nature of Statistical Learning Theory, Vapnik discusses support vector machines. Support vector machines became very popular in the computer science literature in the 1990s, and support vector machines are related to logistic regression. And there are many problems where it is roughly as accurate as logistic regression, but there are some problems in which support vector machines just about blow all other classifiers out of the water. Support vector machines can be formulated as constrained optimization, and in the textbook, Introduction to Statistical Learning by Edwards, Witten, Hastie, and Tibshirani, there is an excellent discussion of support vector machines. So I'll lay down some terminology and give you some idea of the geometry. Suppose that we have two classes, so that's filled-in circles and the hollow circles, and they're completely separable. So we can separate them by just fitting a plane between the two data. The optimal hyper plane maximizes the distance between the two groups. That's the so-called margin. The largest distance between the two groups, rather, is the margin. Now, the points which fall on the boundaries of the margin, these are the support vectors, and these are very important in determining where the classification boundary should go. So in practice, the boundaries between classes can be very nonlinear, as this picture strives to show. And so what support vector machines does is it takes a situation where you've got really a very highly nonlinear boundary between two classes. It projects the data into a much higher dimensional space. And in that higher dimensional space, a linear separation of the two classes may be possible or at least approximately possible, in the sense that the misclassification rate is low. Now, the trick to projecting into the higher dimensional space is in the choice of something called a kernel function. So fitting support vector machines within SAS is done using PROC HPSVM. As with PROC HPFOREST, this is not part of SAS/STAT, and the syntax is a little different to what we are used to seeing in SAS/STAT procedures. You can see down at the bottom here, the response variable is specified as mullein, and the input variables specified using-- the predictor variables are specified using input statements. The kernel, the RPF there, stands for a radio kernel. And in the vast majority of problems I've ever applied support vector machines to, a radio kernel has outperformed other kernels. In most packages, the radio kernel is the default because it is usually the most accurate. And there's a parameter that goes along with it, and you can see that I've specified that it should be equal to one. Now, with PROC HPFOREST and with PROC HPSPLIT, we have specified tenfold cross validation or something like it, and that is specified using the select statement. Fold is equal to 10, so it's tenfold, and the CV is equal to random, and saying, do tenfold cross validation here. So here are the results for fitting support vector machines to the Lava Beds National Monument data. And you can see that if I just use the default linear kernel, actually it does about the same or even a little worse than logistic regression. If I use the linear kernel with the parameter equal to one, then I do a little better than logistic regression, but not as well as a classification tree or certainly nowhere close to random forests. If I spent a lot more time trying different values of k_par, I might be able to improve this, but that's one of the costs of using support vector machines, is that you have to tune them, and that tuning process can be time consuming and potentially frustrating. In this particular example, the support vector machine did not do nearly as well as random forests or even a classification tree, but there are problems that I have encountered in which support vector machines performed the best out of all of these very good classifiers that we have available to us now. So some final conclusions about this section on nonlinear methods for prediction in classification and regression models. Generalized additive models, classification and regression trees, random forests, and support vector machines, are methods that are generally more accurate than traditional linear methods, such as multiple linear regression and logistic regression. And they can also provide insight into the structure in data sets. Generalized additive models are particularly good for visualizing the relationship between predictor variables and response variables. That's these s functions that we graphed in a couple of examples. Classification and regression trees often provide very succinct summaries of the data and yield insight into discontinuous nonlinear structure. So we saw this in particular with the example on credit cards. By simply dividing the data set into two pieces, one could see exactly what was going on in the data, and that was nearly all the information there was about whether or not people were approved for credit cards. Random forests is one of the most accurate classifiers in existence and is very good for regression too. Unlike competitors such as boosted trees, and support vector machines, and neural networks, random forests requires almost no tuning. Perhaps it is for that reason that random forests is extremely popular, both with SAS users and with users of other packages, for analyzing data in a wide range of different scenarios. Indeed, in their book, Hastie, Tibshirani, and Friedman devote a chapter to random forests, and they comment on the fact that just pulling it off the shelf and applying it to data sets with no tuning very often yielded very accurate predictions and good results. That brings this workshop to an end. Thank you for your attention. I hope that you have learned something from the content here. And should you have any questions or interest in pursuing things, please don't hesitate to contact me through my email address. 