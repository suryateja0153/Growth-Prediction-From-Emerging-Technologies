 - It is with great pleasure that I introduce the next session. So this one's called Designing Machine Learning For Humans. I played a special track in the morning break. I played Gary Numan's Cars for them 'cause that's part of the story, but that will be giving too much away to say why. So Diana Adorno and Karen Davis. In our great big musical adventure today, I'm going to call them the craft work of ThoughtWorks. In this case, it's very it's techie, but it's human and presentation is everything. And design is so important, which is natural in this case because Diana comes to us as head of design at ThoughtWorks. Looks at design across the enterprise, you know, using service design and evidence based approach, designer researcher strategist and loves producing digital product. Continually experimenting, she's got all sorts of qualifications in research. Cognitive anthropology, worked all over the world and a big supporter of ThoughtWorks work in diversity, in thought and action. And that's part of her everyday life. And you'll see it in her work as well today. Now, the other part of our duo today is Karen and Karen is a data engineer at ThoughtWorks. She began her career as a software engineer before moving into the data space and has worked as a data scientist, a data engineer and a machine learning engineer. She's passionate about all things data, makes a great compliment to Diana today, as they talk about what machine learning opens up as the possibilities for organisations to do things faster, to do things better and with greater insight. But in that rush to automate, things can go wrong. And the humans around those machines can be forgotten or overlooked or the impacts on them dismissed. People need to be at the centre of every design, that is, the philosophy here, and you're going to hear more about that in some stories and principles and techniques. In another incredibly entertaining set of metaphors as to how it all works. We've had Lego this morning, we've got more stories now. And so I would duly love to hand over to my two colleagues and I'll step away. Thanks for coming team and looking forward to the next 20, 30 minutes. - Thanks Nigel. Yeah, we don't have the Duplo, the Lego, but we've probably had something similar to a photo album, I suspect. So, we wanted to just share a few stories. So Karen and I have worked together and as Nigel did that great introduction, so we went on a project together and we realised, obviously we're coming from different perspectives. And Karen, obviously from that data and machine learning technical perspective and made very much from that design and applied psychology and looking at it from the human perspective. And so we wanted to sort of share that our thoughts today and what we've learned from working together. And yeah, how do we design for people, both people and that machine learning. So we're going to share a story today. We're going to share a complex story, but everything in the story is true for any product design and build, but it's also a story about failure. And by both looking at that complexity and failure, this can give us clues into what contributes to success, of course. So we're going to look at a few factors, like including language that we use as a team and also that responsibility or that failure. So who does that really sit with. And what's that right balance of interaction between that human and the machine. So how do we work that out? We're going to give you a few things to think about for whatever you're working on or planning, and we'll leave you with eight principles to take away. I didn't give Karen a chance to say hello, actually. - Hello. - Right, so to share my stream and we'll start the photo album, okay. So imagine this, so you are driving home after work, so you've just managed to get in there before rush hour. So you've managed to get away early enough. So what are you thinking about when you drive? Are you thinking about the drive itself? Are you thinking about the car? Are you thinking about the traffic or the people around you? Or maybe none of these things. Maybe you're thinking about your day, your favourite TV show, maybe listening to music, but the night also suggested, but there's a lot to do and think about when you're driving. So how are we doing that? So you're using your senses. So you're using your vision, to look ahead, you're also looking even further into the distance, some shapes that you can see in the distance, but you're not really sure what they are. So you're also looking maybe to the side. So using your peripheral vision, which is tuned for movement and maybe you're looking behind you. You're also using other senses, you're using hearing. So you can hear things. So you can hear things on the side or something that's coming up behind you, and you can also use touch. There's some feedback that you get from the vibrations in the steering wheel. Maybe you can tell that the condition of the road, or maybe you can have a bit of a sense of the speed you're doing perhaps by the sound of the times on the road as well. And then there's also back to sort of vision, but now we're looking inside the car. So maybe there's things that you're looking at that are there to support you driving. So like a speedometer. So you can tell you how fast you're going. And so you're switching constantly between these sensors and trying to sort of balance out and continue to drive the car. Now, a lot of these things we don't really even think about, they become unconscious if you've been driving for a while, but there's actually a really a lot going on. And as that information is coming in, your brain is processing it to make judgments, to interpret things and then to take action. And so we do a lot of these things all the time and we don't even think about it. It's actually really a dangerous pastime. But obviously we just take it for granted. - Now we're going to see the cars that see the world as the car sees it. So obviously the car doesn't have the same kind of senses that humans do that Diana just described, but it does have information coming in from sensors and other hardware on the car. So it's got cameras, that's bringing in video. It's got Lidar, which is sending out little light pulsars, and it's using these to build up a treaty representation of the world. It's using radar, where it's trying to measure the other objects and their philosophy. It probably has some form of GPS especial location, so it knows exactly where it was. As well as other equipment like accelerometers, gyroscopes, even things like tyre pressure. So it's taking in all of this information and it's also starting to try and make sense of it and process it. So it's got a computer vision system. So it's using the information coming in from the sensors and the cameras to try and work at what objects are around it. Is it a car, is it a bicycle, is it a pedestrian. It's got a planner unit, that's kind of taking in all of the information and trying to make decisions on what it should do next. So obviously it doesn't quite work the same way as people does and doesn't quite think the same way as people do. And dealing with ambiguity is pretty difficult for it. Now, is that a parked car in the lane up ahead? Or is that a car that's moving at the same speed as the car then that I'm curbing? And now to a real story where things went wrong, and this is a story from 2018. It is tragic and it can be uncomfortable to think about, but there are many things for us to learn from it. We are now in Tempe, Arizona, it's a place wait wide roads, fast cars. It's almost 10:00 PM on March 18th, 2018. Elaine Harrisburg is crossing Mill Avenue, wheeling her bicycle, shopping bags hanging off the handlebars. And a prototype Uber self driving car is travelling north. There's a single driver or rider, as you would call them behind the wheel. At approximately 9:58 PM, the car hit Elaine. So what happened? So it happened at night, this is a view of the road from the cars camera. The visibility is okay, the road is well lit. So at the time of impact, data from the drivers phone showed that they were streaming the voice. Had they looked up, they would have easily seen Elaine. The driver's hands were not hovering over the steering wheel, which was required as part of the Uber's policy. And uber had recently changed a policy. Prior to this, they had two riders or drivers in every car and they had swapped back to just having one. So let's take a closer look at what happened. So 5.6 seconds before hitting her, the car's radar detected Elaine. 5.2 seconds and she was picked up by the lidor. So the vision system of the car hadn't been trained on pedestrians who weren't crossing at a crosswalk. It had only been trained on pedestrians crossing at the crosswalk. And so it didn't recognise Elaine when it picked her up on the lidor and the radar. It first predicted her to be a car or unknown object, then I think a car and a bicycle, and each had a different path. 1.3 seconds before the crash, the system determined that emergency breaking was required, which is normally performed by the vehicle operator. The emergency braking had been switched off or disabled because when it was turned on, it led to erratic behaviour. Sometimes the car would just slow down, give it so a plastic bag. So 1.3 seconds before impact, it detected an emergency. The emergency brakes were disabled and it had no mechanism to alert the driver. One second before impact and the driver turns the wheel. And then one second after impact, they applied the brakes. - So that's a really tough story, but we have to ask this question, who is responsible? So I just want to ask you that question. So if you could just take a moment, jump on to the chat there in zoom, the Q and A and just put down who you think is responsible. Let's take a moment to do that. There's a few going in there. - Yeah, yeah, I got a few. So it looks like the driver. So lots of people are saying the driver, Uber, algorithm and the driver, Uber CEO, everyone. - Everyone? - And my model, the algorithm. - Oh, nice, yeah, that's good. Okay, so I think this is a really really important question to learn as we go from these sort of stories. So when we looked at this, there's actually quite a few, as people are putting in, it sounds like there's a range there, which is who is responsible. And there's a few groups here. So there's the pedestrian, so there's Elaine. A lot of people that are talking about the rider. So Elaine was crossing the crosswalk, but not at a cross road, she's crossing the road. The rider was looking down, watching the voice, not watching the road. The government is also has a part to play here. So the design of the roads themselves, that force pedestrians to cross multiple lanes of highway. There's also the governor who gave the okay to go ahead with this trial in this form. Then there's also the team and there's lots of different roles in the team. So is there any one role in the team that's more responsible than the other one? Who decided to the turn the brakes off? So who was doing the testing? So who were making the design decisions? So how was that happening? So thinking about the team. Then there's also obviously Uber themselves, just pushing to do this trial and doing it in this way. And as Karen was saying, it's the policy, they went from two drivers down to one. And then there's the car itself. So a few people have sort of talked about the algorithm there. So the car seems to be a player, and there is a certain point that is worth talking about there. The fact that the emergency breaking was turned off, is obviously important. So there's a lot of different groups to consider. So when we're looking at who's responsible, we have to say, who has the most choice? So who could choose? And that's worth looking at it and looking at Elaine. So she could have walked further away to cross at a cross walk, but they didn't. It was quite a walk to do that. So she didn't really have a lot of choice, she had to cross the road. The rider had a choice to be, well, potentially, had they looked up, then they would have seen her. Then there's the roads department, had they thought about pedestrians and had more crosswalks or bridges or tunnels some way for people to cross and not designed everything around cars. And if Arizona had learnt from the fact that California had said no and agreed not to proceed with the trial in this form, perhaps. So there's a choice there. The team has choices. They could've considered people who were not on cross walk. So check their own bias and made sure they were considering a number of different scenarios. Maybe knowing that the brakes they were playing up, is maybe think about a safer way to run the trial. And Uber, if they had kept two drivers, so the change of policy and again proceeding with the trial. And the car, it's interesting the car. The car doesn't actually have any choices of its own, but it takes the action based on other people's choices. And so it's really is a bit of a combination, but I think if we're going to focus the responsibility, we were sort of saying that sits here. So definitely the agreement to go ahead with the trial, but also just the design of roads, they're designed for cars, they're not designed for people walking and people walking in bicycle across the road even more so. The design team had a lot of choices. They had a lot of ability to change the way they were working, looking at the data that they were looking at and also looking at the data that consider people not on crosswalks. And Uber obviously had choices as well. So pushing for the trial, but if they had kept the two drivers rather than going down to one. So I think that's important to consider where that responsibility lies. Sorry, can I say one last thing, is so this is important, this is something that is important for us to learn as Microsoft products, is to take that responsibility really seriously as we start to put those products out. - So what can we learn from this? And I think an important questions to ask as well is, are we learning from this? Because it's still seems to be happening. This is 2018, where it shows like car in autopilot crushing into special police car. More recently in the last two weeks, it has an autopilot crashed into a police car again while driving. And the driver had admitted that at the time of the crash that they were streaming a video on their device And this is a YouTube video tutorial, on how you can watch Netflix while driving your Tesla. So this is an iPad hanging over the console and Tesla at the moment don't allow you to stream Netflix or other streaming services on their console, but they are talking about it maybe at a point in time when the cars are fully autonomy. But this is a hack basically. - Yeah, this is amazing to see this hack. And it gives us a bit of a clue as to what people want to do when they're in the cars. It also shows that they have enormous trust in the car itself to drive it. They feel confident enough to actually look at some sort of entertainment when they're in it. And so it also gives us an idea about a clue to their mental model. So, which is just that absolute trust. Is like they are not thinking about it as a car, they're not thinking of themselves as a driver, I think that's really interesting. We need to take that into consideration and design that experience based on that understanding. - Yep, as long as things we can kind of take away is don't assume a data have all that will need. And typically when we start building a machine learning algorithm, we're looking at what data is available to us, and that is really critical to the success, more data equals generally equals a better algorithm. One of the things that led to this happening and in this story we saw that the efficient system of the car had only been trained of people crossing at a crosswalk. So when it saw Elaine on the side of the road, it didn't identify there a person in time to stop the collision. We had data that was missing from the data set that the algorithm was trained on. And this is a common problem machine learning. The difference between data that the model was trained on and the data that the car used in production. So we can do things like shadow release production, where we have the modelling production process in the data, but we just processing and monitoring it. We're not doing it for real. - So the other thing we need to do is look for extremes and disasters to help shape our thinking. So if we look at those extremes, and what we saw in that story is that the team hadn't considered this particular occurrence. And so we need to really push ourselves to say, look for those different scenarios. I always like to think of disasters just to help me show the shape and think from a design perspective. How can I think about this problem in a different way? And disasters are a real clue because their behaviour often radically changes, in a disaster. And it can just give us insight about how to design something. So in this instance, maybe something that's extreme, this car is designed with no driver whatsoever. Would that change the way we tested it? Would that change the way we ran the trial, or even how we set up the car itself? Or maybe something we look at other external scenarios, so a child running across the road, would that make a difference? Like a smaller person or the opposite, like a truck dropping their load or a hole in the road opening up? So thinking about things outside of the car, or maybe increase the distraction inside the car. So we need to sort of think about those different scenarios, and that can really help us sort of think it through. The other thing is also look about those near misses. So the aviation industry and the oil industry do this really well. So they realise the consequences of making a mistake and so they look for things that are near misses and they want to report those and learn as much from those as well. So I think that combination of extremes and looking at those close calls can really, undertaking them really seriously can help us. So what we're saying here, is that driver experience is key. And a lot of people, when we ask, whose responsibility, a lot of people obviously think that it's the driver. It's not the driver at all. So the driver is someone we're designing for and we have to be quite deliberate if we're going to design that driver experience. And what we call them is really important. Do we call them a driver or a rider? If we think of them as rider, well, they think of themselves as a rider. Then they've going to see their responsibilities really differently. They think of themselves as a driver, then again, they probably will take that as a different responsibility. So that driver experience looks something like this. So this is a Tesla and this one is a video of this car driving with a driver in it. and what we see is there's a lot of interventions that are needed every few seconds it seems like. An intervention is needed by the driver, because there's lots of objects in this street. There's a , there's poles, the lines in the road and the car gets confused quite often and so it's not safe. So that's how a rider experience is very much an intervention. And they obviously don't have their hands on the wheel there, but they were bringing their hands up to adjust and they were actively looking at alert. So we need to sort of really design for that experience. If we contrast that to the rider experience, it's much more like a passenger, it's much more like on a bus. You can see this person doesn't have, and probably this is a future looking one, which doesn't have anything in front of them. So there's no option really to intervene. So it's designed very differently. And we have to be quite deliberate because we'll get different behaviour depending on the clues that we put in front of that person as they are designing. So this is something as a concept called distributed cognition. So we have to choose where is the task? Is it distributed between the person in there and the car? Or is all of the thinking and decision making in the car itself? And so we have to make a deliberate choice. This is another one, which is sort of an amazing photo. This again, it's a clip from a video and a motorist saw this. So two people in a car that was moving and they're both asleep. So what's going on? So both the driver and the passenger are asleep. You can see on the far side, the drivers they're slumped forward as if they were on a train or a plane and it's not clear who's fallen asleep first. But I'm just wondering why the passenger looked very much like a passenger falling asleep is leaning against the window. I'm just wondering what's going on? Is everyone exhausted? Are they everyone really tired? Is it just too much to ask of people to be in an autonomous vehicle to stay awake? And is the experience just too boring? So it's sort of a car, but you don't have enough to do. So what's going on? We really need to think about the design of it and be quite deliberate. So when we're doing it, we need to think about everyone that's involved. Obviously the passenger and the driver or the rider, the people outside of it. We also need to think about the team. So the team, there is a bit of a tendency to be optimistic and maybe to think that the people outside the vehicles are all being rational. And so we need to think about the mental model that the team has. Again, if they're thinking about the person in the car as a driver, they're going to have expectations. But if they think of themselves as a rider, they'll have different expectations. So we really need to be really conscious of the language we're using when we're talking about this problem. And maybe there's also a bit of pressure on the team as well, to go out early and do a live trial. But we need to sort of be really conscious of what are the different groups looking for? What are the consequences we really need to be really clear about what the impact is. I was trying to avoid to use impact that this move for. But there's a technique called consequence scanning. So we think about all the people and then ask those questions. It's about what is the consequence for that group? Both intended and unintended. We have to then also go into that realm of what if, so that we really need to think around the problem. So we have a paradox here. We've got a paradox of between safety and danger. So as we've seen boring is dangerous. So if it's boring and they want to go to entertain themselves then it actually introduces a lot of danger. Whereas actually a little bit of danger can make it safer. If you think about if you're driving at 40 kilometres an hour, or you're driving at 180 kilometre an hour, perhaps on an Auto mode, you're focusing attention is quite different. So you want to introduce some danger, obviously not too much danger, but to keep that person or the people, the driver in this case, alert enough so that they take that seriously and they can maintain that focus and not just fall asleep or well look at other entertainment. So there's a real balance between that danger and safety and that boredom is actually is introducing more risk. - So we need to consider all of the consequences. As people we have a bias towards optimism. We can oversimplify things. We sometimes have a narrow focus and consider only the individual components or models. So we need to take a step back and look at the bigger picture. Look at the interactions between individual technical components and then take another step back and think about the interactions with the technology in the environment we'll be operating in and with the people who will be using it. Think about downstream consequences and how things can go wrong. Not just where the technology can fail, but where people might fail to behave in a way that we expect them to, or that we have assumed that they will. And it's really important to have diversity of opinions, different opinions, different backgrounds, different experiences. You need to have these different perspectives in the room. It's not a mode of thinking that comes naturally to people, so look for techniques to support your thinking. So I think our original placeholder for this slide was test the heck out of it and really just test. Once you've started to look at all the consequences, make sure you're testing them, testing individual components, test the interactions. Obviously you need to make sure you're testing the technology like completely as well, but it's not just testing the technology, it's testing the technology and interactions with people and the environment. - Yeah. Well and I think we also have to just really common gun variety, sort of usability testing, make sure if we're introducing something in the car, people know what it's for and that they can use it. Here's an example of one where this is a random green light, but it's not really clear what it's for. So you don't want to add confusion into the car and make sure what people can understand every aspect that's been designed in the car. - So working together. One thing Diana and I found really useful when we started working together, was to really come up with a shared language. Like she really understand each other's mental models and where we were coming from. It's quite a similar process that we go through. We've got experiments that we were not getting validation and feedback, but really, it was really important for us to get that shared language, that common understanding. - Oh yeah, yeah. Sorry continue - I was going to say, I kind of tended towards optimism. Like of course we can build these things, of course this is possible. - Yeah, that's a good point. And I tend to be sort of the sceptic. I think it was really good to understand that we sort of had similar starting points. We started with questions. I tended to go much broader and I remained sceptical all the way through and never believe anything. I'd believe any of the data that's coming in and always looking for multiple sources of information. Because when you're looking at human behaviour and motivation, it's not absolute and there's lots of things that influence it. So I'm never going to trust the data, certainly not from a single source. And so once we worked out kind of our different perspectives that actually worked really well, 'cause we were really clear on what those different perspectives were. And so what we could see, what was helpful, is those natural tensions to balance out. So as we've sort of said, that language revealed our mental model, the words we use as a team really shapes our thinking. And we need to make sure that we're not using this same word and meaning quite different things. And as Karen was saying, there's a tendency to start with optimism and I'm kind of more of pulling towards being sceptical of the data. And bringing that balance of you want some certainty, but I think keeping that doubt all the way through, is really important. And we've sort of heard a little bit about a discussion about that big data, but this idea of that small data and that qualitative research is really, really key. 'Cause you just don't understand what humans are capable of. And that's super important for the success of any of these products. - So when you talk about shared language, this is a diagram that helped us get a common footing. It's kind of just a really high level of what machine learning is. And so like to venture it quickly, there's three kind of main categories, supervised learning, reinforcement learning and unsupervised learning. Supervised learning is really where you have a data set and you have a set of labels. So you might have images of different types of vehicles and they have labels like bike, car, truck, et cetera. And then you use that model. You use that data to train a model so that that can then make predictions on the unseen data. Unsupervised learning is where you have that same data set, but you don't have any labels. So you just kind of like asking the algorithm to find patterns. Reinforcement learning is a different style of machine learning. So that's where you set up an environment and you give the algorithm rewards when it successfully completes a task that you wanted to. It's kind of, if you see any examples of machine learning, learning to play games, it's generally using reinforcement learning. So the algorithms involved in a self driving car are quite complex and they might come and have different components EGPs, like the classification of identifying what type of objects. Might be classification, algorithm and it might be some unsupervised learning to do segmentation. The planning part, the decision making, could be reinforcement learning, or it could also be some type of statistical model as well. So it is a complex problem. - Yeah and this is really good. So when Karen and I developed this together, it was really good to understand sort of how to think about machine learning and what it was doing. But even when we take sort of one thing, like the image classification or the image recognition, it was really good to understand what the accuracy would be. So what is it that we can see automatically or with the machine learning? And what does that mean? And what are the limitations of that? 'Cause I'm applying a cognitive and behavioural model to this. So if we look at the image classification and these are limitations, how'd you quite easy to design around it, if you know what the limitations are. So absolutely and again with that scepticism is never trust any data completely. So we also want to build in some areas of forgiveness in that product so that we can keep everyone genuinely safe. - And so this is a complex problem. And as we highlighted during the talk, there are lots of different aspects to consider, apart from the technology. They are the things like the context, what the extreme scenarios are. What the interactions are going to be? What people will be involved and how they might behave? And what it means to be a good citizen. What ethical decisions should we be thinking about and wanting to legalities do we need to be aware of? - Yeah, so I think just as Karen says, there's just a lot to consider in all of these things. So just want to give us a few kind of takeaways from this. So these were just the principles from our working together and sort of thinking through this. We sort of thought that, well, some of the key things, as a cross-functional team, as we're saying is, all of us is different ways of thinking, that diversity of thinking is really everything we're thinking about this type of problem. So avoiding that danger. And really learning about each other's mental models as a team and considering all of the people connected and developing that shared language is really important for working together. - So it's important to pair frequently, having different experiences and approach the same problem is really beneficial. Share often and look for ways to how you can share within a team. Don't be reductive in your thinking, it is complex. Really importantly is to stay sceptical and expect disasters. And I think one thing we've seen is that there is a lot of trust implicitly, in this technology and machine learning technology, where we seeing people asleep in their cars. They're literally putting their lives and the lives of other people in the hands of the technology. So I think as the people who are building these products I think it's on us to stay sceptical and just test as much as possible because we are the people who have a lot of choice in how they're implemented. - Right? Thank you. We've got a lot of resources that we're going to share on the channel. But I'm handing over to you for questions. - And my goodness haven't you generated the interactivity on the channel. And I think one of our original plans is that you just scan the Slack channel and pick them out, but I'm going to do that for you because it was so active today and so much. So some really good questions was, Diana, there was a moment in a room and a multidisciplinary design team room where they somehow managed to miss the Jay Walker scenario. What was the human element? What was the human situation there? How does someone miss a scenario that in hindsight seems incredibly obvious to plan for? - Yeah, I think it's around how you, what's your starting question and how do you start to understand the context that you're going to be operating in is really important. So rather than missing something, I think what's missing is a few activities around. So you might do ride a longs for example. You might ride around specifically scanning for things outside the car. You might actually look at accident data, you need to really look really closely. Maybe you'd use drawing sort of surfacing over and watching just movement of people and things and objects from afar. I think some of those things you really got to reach out to sort of start to actively look for those different scenarios. What are the different aspects of things we need to think about? And I think that's the way I would sort of think about that. It's very hard. I think when you are looking at very focused and you've got a data set and you start there's a tendency bit of a trust in that, that you're not thinking about it. So I think you rely on people. I part sometimes split the teams into people who are prefer standing up to sitting down. I'm definitely standing up and getting out in the field. But it's that real field work, that's actually going to bring some of those different problems into the room. - And question for you, Karen. It's surely not every machine learning project, that there must be projects where we don't have to worry about this kind of thing, ethical behaviour, et cetera, et cetera, just straightforward it's data. - I think maybe you've got a classify on your system that you are identifying cats and dogs, and it's just going to be for your own benefit, maybe that's example. But I think any point in which we start to interact with people and we start to have an impact on people's lives then we need to take a step back and consider all of the principles, consider all of the consequences. And just really make sure that we're getting it right. Because as we said, people just seem to implicitly trust these algorithms. So I think it is the team to be sceptical and to just really, really test it and make sure they get it right. And I think the stuff we talked about, we talked about a complex problem and we've talked about an extreme example, which is like the self-driving car. But I think, a lot of the principles and the things we talked about could equally apply to any machine learning project. - Diana, someone asked reductionism on that last slide. So avoiding reductionism. Quick definition of reductionism, what is that? - Yeah, what we just mean is that don't simplify too soon. So I think as we go through it, we tend to let go. I think we understand the problem and complexity, and we're reducing down to just the simplest parts. What we know we have to do is challenge our thinking and be conscious of actually opening out that complexity and then reducing down so that we can act on it. So it's actually around being conscious of the way we're thinking about this problem. So being reductive means to reduce it down and we need to be doing both opening it up, considering complexity and reducing it all the time. And this is also where that diversity of thought, people who think differently, who see different things. If I take that as an example, it's like, if you've got surgery going on and you've got a surgeon looking at that, or you've got a cleaner looking at that, they're going to just see different things and they're both important things to consider. And so that's probably one that diversity of experience, of ways of thinking. And that's going to be important. - Any examples from banking and insurance? And I think a lot of people online probably work in financial services, software developers, all sorts of different roles. This is a big issue there at the moment. - Yes, I think all of the stuff we talked about, the problems like missing data, any data set. Your data set and not matching, your training data set not matching production and not like being really confident in the model as you release it into production. I think these are all principles that can apply no matter what the sector is, no matter what the underlying sector. Whether it's finance, whether it's insurance. whether it's a self driving car. - Well look, we're coming into lunchtime. I think people would be exhausted after a morning absorbing incredible talks. So thank you you two, Diana and Karen for bringing a multidisciplinary way of thinking to the table. I know that's classically Thoughtworks, but it doesn't happen easily. And the meeting of the minds, I think it's in those margins, those overlaps of knowledge and I've experienced where new stuff is created too. New approaches and new ideas. And we're very lucky to have such a diverse community to work within where lots of new ideas. So cheers, I know you're going to pop over to the Slack channel and have a look at the conversation that you've generated there. It's pretty massive, I've got to warn you upfront. It's sort of cool. There's your lunch all gone and we'll move into the lunchtime session. 