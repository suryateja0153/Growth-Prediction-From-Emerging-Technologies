 good morning everyone and welcome to the first session my name is susan dumay i'm a technical fellow at microsoft and the director of the research labs in new england new york and montreal in this session on machine learning conversations i'll be joined by four researchers from microsoft who are really pushing interesting new frontiers in reinforcement learning and natural language generation this my colleagues will be katya hoffman from the msr cambridge lab in the uk akshay krishnamurthy from the new york city lab asla silik gilmaz from msrai in redmond and dan klein from microsoft's semantic machine research group in berkeley katya and akshay will lead us off by talking about some recent theoretical advances in reinforcement learning that enable both more efficient and adaptive systems to be generated and asan dan will talk about new results in language generation for both creating coherent long-form stories and for composing sequences of actions in the context of conversational ai systems each speaker is going to give a short talk followed by questions from me and you the audience it the audience can use hub to ask questions we'll be monitoring that throughout and without further ado let's take it away with the first talk by katya welcome to my talk on one of the most exciting frontiers in machine learning learning to adapt advances in deep meta reinforcement learning to motivate the problem of learning to rapidly adapt i want to give you a little bit of context on what my team and i are currently working on within the game intelligence group at microsoft research in cambridge our mission is to advance the state of the art and machine learning in order to enable new applications in the gaming space in particular that means that we envision new gaming experiences for players and for developers we create new tools and capabilities so that they eventually will be able to use machine learning to shape those new and engaging experiences within this space we see one front here that is particularly exciting and that if solved will have a lot of applications both in gaming and beyond and that is the frontier of developing machine learning approaches that can rapidly adapt to new scenarios or in our case to new players just to give you two examples um if we think of a game like minecraft there's an enormous amount of diversity and so if we wanted to use for example machine learning based agents in order to solve some tasks or collaborate with with players within minecraft worlds they would have to be able to very quickly adapt to new situations that they might not have encountered before just to give you an example here there is one way in which people play minecraft which is called a parkour so it's essentially a speed run where the agent has to very very quickly run through a parkour that in many cases human players have created for their friends or possibly for agents now if just looking at these few examples that i'm showing you here in the slide you can see the enormous diversity of those kinds of tasks and if you wanted to imagine an agent that is able to solve all kinds of parkour challenges it might have to deal with situations that it has never seen before um just based on the creativity of the people who create those kinds of power core challenges very similarly in a game setting we may want to have agents that can very quickly adapt to the enormously varied play style of the diverse player population that play xbox games every single day and so a key challenge that we're looking to address in our current research is to understand how machine learning based agents or models could very rapidly adapt to for example new game scenarios and players in particular in settings where we have limited data available for each individual player or for each individual game setting now to put this in context at the moment we have seen a lot of rapid progress in reinforcement learning and imitation learning so we are now able to train machine learning based models that can very effectively solve a given task just to give you one such success story um here is the task that we post to the research community last year at the nerves conference and which will be repeated this year at nerfs 2020 which is the mineral competition where we collaborated with partners at universities for example carnegie mellon university and other partners in order to create a challenge that would really test the ability of current reinforcement learning and imitation learning approaches to create agents that can show complex behaviors and in this case the challenging task that we picked was to obtain a mind in minecraft a diamond and so in case you're not familiar with the game minecraft in order to obtain a diamond a player or an agent would have to go through a quite complex sequence of steps for example gathering wood and other raw materials create tools and eventually it needs to start digging find a cave and find a diamond ore in order to actually mine a diamond and when we set out to kind of pose this challenge we were concerned that this task was too hard and that it would take a very long time for people to actually solve this we were very positively surprised to see that within the first iteration um the competitive competitors really rose to the challenge and they were able to develop approaches that lean on a combination of reinforcement and imitation learning to effectively solve a task or for almost this complexity with a very limited amount of data and experience available to the agent and so unfortunately i won't have time to show you the video here in the in the talk but if you head over to mineral dot io slash competition you can see a video of last year's winning agent which is able to very effectively mine resources construct tools and it gets very very close to actually getting that diamond so we have all um hopes that this challenge will be completed um in this year's iteration but now the key limitation of these types of approaches is that for each task for each jump kind of scenario um current reinforcement learning and imitation learning approaches will have to be trained specifically to those tasks so the limitation is here that once trained they cannot rapidly adapt to new tasks or for example players or collaborators as we set out as our next frontier the approach that i want to talk about today is one that i was very proud to collaborate on with my amazing collaborators at oxford university that you see here in the slide and together we developed a new approach which is called very bad which stands for variational deep reinforcement learning the key idea in this approach is that we develop an approach that can learn to learn so for example if you imagine an environment where it's quite easy to simulate a wide variety of settings you essentially pre-train an agent or an algorithm so that it learns to learn and at deployment to very very rapidly adapt to this to a new situation that it has not seen before just to give you a motivating example i'm showing a kind of toyish environment to illustrate this particular challenge over here on the slide so you see here a kind of research environment that we use to test our approach which is a grid world where you see the starting position of the agent over here and in every kind of every so often there is a switch in the goal position in terms of the goal that the agent needs to reach so here hitting to the agent you can see the currently selected goal position but in principle every couple of rounds the goal would reset and the agent has no idea where the goal is and what task it needs to solve in the new iteration so you see this as an illustration to really rapidly adapt to a new task attend um in line with the variability of the agent about the environment that it expects to interact with now what we're aiming for here in terms of the solution concept is to train agents that learn what we call base optimal behavior based optimal behavior means that an agent would maintain a belief over its current task so in this example it would maintain a belief over possible goal positions and you can see here if it has already visited some potential positions its belief that the goal is there will be reduced and so it will now focus its exploration on these remaining possible goal locations an agent that is base optimal shows the optimal behavior assuming that its current belief over the environment is correct and so we can contrast based optimal behavior with a previous state-of-the-art approach called posterior sampling where instead of learning based optimal behavior the agent at each time steps samples the possible belief and then x optimally according to the sample belief and this can lead to so-called differing where in each step the agent may switch between possible goal locations without really learning how to explore the environment systematically and actually act based optimally in contrast we can see that the approach that we propose is actually learning this systematic exploration so it systematically seeks out goal locations that it has never visited before very effectively sweeping the space of potential goals and we can also see this in terms of quantitative results and mapped out here on this plot is the theoretically possible base optimal behavior and we can see that our proposed approach is very very close to actually achieving the space optimal behavior meaning that it has learned to systematically seek out information in order to complete a task now how did we actually do this the key idea in our approach is that we aim to flexibly learn base optimum behavior so that the agent can rapidly adapt online with minimal assumptions and the approach we propose has three main components first there is an encoder which learns um to embed uh trajectories so a state actually rewards topics that the agent has encountered before and it encodes this in what we call a belief which is a variational representation a belief over the kinds of tasks that the agent might currently be in the second component is the decoder and it learns to decode the belief the embedding in order to predict what will happen in the future and so the key insight here is that this decoder needs minimal assumptions that every reinforcement learning agent makes in order to predict the next state and the next reward function so it's essentially learning the dynamics of the environment that the agent is interacting with and because it um learns to decode this variational representation this belief that we post it is able to decode all the different variants in which the environment may transition in the future the final component is our policy and this is actually a model-free policy trained using standard reinforcement learning standard deep reinforcement learning approaches but the key difference is that we condition this policy on the belief that the encoder has learned to encode and so this means that the policy can learn how to optimally trade off for example how to explore the environment with directly using the information that it already has the so-called exploration exploitation trade-off in line with the belief and the uncertainty over its belief given by the current belief representation on this slide i'm showing some of the results we'll be able to achieve using this approach and these are obtained in the so-called mutual kutas which is a commonly used and physics simulator you can see here our comparison with standard baseline approaches and in particular we'll see in yellow that our approach behaves near optimally in line with oracle performance i can also show qualitative results here and you will be able to see that the agent briefly moves in the wrong direction before figuring out the actual task and then moving backwards as intended in this particular task so you can see that it is indeed able to very very rapidly adapt to this task so in this talk i introduced one particularly exciting new frontier of machine learning namely the task of rapidly adapting to new tasks and players a challenge that is particularly important in video game applications that we are currently working on we introduced a powerful new approach called very bad which flexibly learns base optimal behavior in a wide range of settings next our team will focus on applying this and similar approaches to gaming and other applications and if you would like to learn more i invite you to visit our website at aka dot ms game intelligence in order to learn more about our research and the exciting applications that we're envisioning thank you so much for your attention hi thanks for tuning in so this talk is about generalization and exploration and reinforcement learning let's start off by just recapping what reinforcement learning is so reinforcement learning is a sequential decision-making setting in which an agent interacts with an environment to optimize some sort of long-term reward function and in the modern era we've seen um quite a flurry of interest in applying reinforcement learning methods to complex settings like gaming robotics and dialogue so in all of these settings the agent operates on raw sensory information something that i'll refer to as rich observations and has to um accomplish these long-term planning sort of tasks now um we've seen quite a bit of success in the context of gaming but somewhat less so in in robotics and dialogue and the main reason here is that um the modern reinforcement learning methods are highly simple and efficient so they can only be applied to settings where we have a high fidelity simulator like gaming where sample inefficiency is a bit less of a concern now since we're talking about sample efficiency i think this is quite an opportunity for theoreticians because we can think about designing algorithms that are provably sample efficient and so that's our goal we're going to design provably efficient sequential decision making methods that scale to these complex domains and before getting into the technical part of the talk i just want to pause for a moment and think about why this problem is difficult um so these uh reinforcement learning methods must sort of demonstrate three core capabilities these are exploration credit assignment and generalization so exploration is all about planning to navigate the environment to collect the information you need to learn further credit assignment is about uh this causal question of understanding what decisions that you made in perhaps a long sequence of decisions are relevant for the outcome that you are observing right now and generalization this is really the purview of supervised learning so this is about using information you've seen in the past to make good decisions in a situation you've never been in before so the the research community has been thinking about um this problem for some time and um one way to achieve design algorithms that achieve all three of these goals is to first start by designing algorithms that achieve two of three and we've actually done this with quite some success so on on the left-hand side addressing exploration and credit assignment we have these methods called tabular reinforcement learning methods we also have policy search that addresses the other two and contextual bandits that addresses exploration and generalization let me just take a moment to describe these tabular methods because it'll be relevant so tabular methods are um algorithms that do exploration and credit assignment but not generalization and they work in these settings where the agent does not operate on rich observations instead the agent operates on a discrete finite state space with a finite action space and what you can show here is that there exist algorithms that's in polynomial time and polynomial and with a polynomial amount of experience provably find near optimal policies in these um problems with finite state in action spaces and so these three surgical lines of work are relatively mature and these things are deployed in production today but when we talk about getting algorithms for all three this area is still relatively underdeveloped and just to drive this point home let's consider this a very difficult exploration problem we call this the diabolical combination lock so here the agent starts uniformly in two states from these two states there's one good action that transits you down the good chain so you end up uniformly in the next two good states and nine other actions transit you to the bad to the bad state uh this repeats for some time for 100 time steps and if at the end of the if at time 100 you're in the good state and you take the good action you accumulate high reward on the other hand if you ever transit to the bad state you accumulate some low reward so this is the environment and it exhibits many challenging aspects for reinforcement learning algorithms first of all there's a long horizon you have to make 100 actions exactly correctly there's stochastic dynamics which preclude the use of open loop policies you really have to look at the state you're in before you choose which action to take there's also anti-shaped and sparse reward so this is actually encouraging the agent to go into the bad the bad chain until it realizes that there's a the good chain has some reward that can be accumulated um on the other hand so far there's only 300 states there's only 10 actions per state so this is still in the purview of these tabular methods it's a finite state space finite action space we can't actually run these algorithms that do solve this problem and really there's no generalization required here so i'm going to bake in some generalization and the way i'm going to do this is i'm going to um instead of showing the agent the discrete state i'm going to show the agent uh some high-dimensional signal maybe an image that's generated from the latent state and every time the agent is in a state it's going to see a slightly different image perhaps there's some noise in the process and um okay so now the agent really does have to generalize and this um this is sort of the environment that we're thinking about i'll show you this in a moment but these um these algorithms that we know from uh the empirical successes really do fail quite spectacularly here okay so we want to solve problems like this and thinking about this problem a bit further a natural sort of solution strategy emerges and this is um via representation learning so that problem there's this uh the agency is this high dimensional signal on the other hand the dynamics are governed by this discrete state space and i told you that we actually have algorithms that can solve this problem if they knew the state a priority okay so maybe a natural strategy is to learn how to decode the latent state from the observations and this is a form of representation learning because the latent states are a representation of the environment and that's the kind of algorithms that we're going to develop here and to formalize this kind of approach we need to think about a model in which the latent states are actually governing the dynamics and so this is what the block mdp is doing so here it's a rich observation problem with the discrete latent state space the agent operates directly on observations and here's how it goes so at any time the agent is in a state you can think about it as the position of the agent in some grid world the agent does not observe this position instead the agent observes some high dimensional input for example image from a first person camera the agent takes an action sends us to the next state and so it repeats and so we want to design algorithms that provably explore and find near optimal policies for these block mdps okay the key challenge here is actually that the latent stage is never observed if it were observed we could just do supervised learning learn a representation of the world and apply a tabular method but the latest states are not observed so what can we do so borrowing some inspiration from the empirical literature we looked at a couple approaches so one is unsupervised learning a popular approach is to use an auto encoder another one is just model based learning you just learn the whole dynamics of the world which actually pass through this latent uh discrete latent state space and another approach is based on auxiliary supervision so here maybe you would train a predictor to predict the action that the agent took on the basis of the observation so we looked at these a little bit and in the paper we actually have some failure cases for a lot of these approaches and i'll refer you to the paper for details on that i will tell you what we do so we use a form of auxiliary supervision and this is um what we call contrastive learning so here's how it works the basic idea is for contrastive learning you're trying to predict uh real events that could happen in the environment from fake ones you're trying to distinguish between real and fake events and the way we do this is we um we collect triples by rolling into some observation with a policy taking an action and seeing a new observation we do this again so we have this x1 a1 x1 prime x2 a2 x2 prime those are real events that could happen in the world real transition triples and um then we're going to splice these two things together so we're going to create a negative example which is x1 a1 x2 prime and this is an event that's very unlikely to happen in the world and we train a classifier to distinguish between these two things and somehow this classifier you can think about it it recovers the dynamics of the process and because the dynamics go through this discrete bottleneck um the this contrastive learning problem also sort of recovers the latent states and so we use this contrastive learning problem in an algorithm that we call homer and what we can prove is that this algorithm covers the states and finds a near optimal policy with the sample complexity that scales polynomial polynomially with all the relevant quantities but it's completely independent of the size of the observation space so this really can be used to scale to rich observations the algorithm also runs in polynomial time whenever you can solve this contrastive learning problem in a tractable manner so let's return to this hard exploration problem um so we ran a bunch of methods and um some from the empirical literature and of course homer and here what i'm showing you is the visitation frequencies of these algorithms along these three chains um when i run the methods for one million episodes so ppo which is this very popular policy search method it um it only makes it to you know time step five and then it sort of always falls into the bad chain and never achieves near optimal reward r d this is the random network distillation strategy which is an exploration strategy this is pretty good but somehow it's quite high variance so we could not get it to move past time step 28 and i wouldn't be showing you the slide if homer did not succeed but we found that homer actually has quite low variance it consistently explores the whole environment recovers the latent model and achieves near optimal reward so that's all i want to say about homer let me just switch gears and tell you about a richer model so this is a low rank mvp here um you have this transition operator this this thing that governs the distribution of the next state next observation x prime given uh the current observation x and the current action a and we're going to assume that this up this um transition operator admits the low rank factorization where each each entry of this t matrix can be written as the inner product between a latent representation 5xa and another representation mu of x prime and i'm not going to make any assumptions about the size of the observation space so this this matrix on the left could be arbitrarily large but i am going to assume that the embedding dimension this dimensionality of this five vector is relatively small that's the embedding dimension d i really like this model and just to motivate it a little bit more um there's three remarks i want to make so first of all the latent uh this lowrance mdp actually generalizes block empties but it's actually substantially more general so so we can show that there are low rank mdps with embedding dimension two that cannot be written as any non-trivial block mdp the second remark is that actually um learning these low rank mdps is tractable whenever um when someone a priority tells you the latent representation phi this is this remarkable paper by gin at all from last year and so this sort of gives us hope that um if we can learn the phi then we can sort of apply this this previous algorithm to explore and do this planning and reward maximization the last remark is actually that this setting is tractable statistically even when the latent representation is not known a priori but unfortunately the algorithm from this paper is computationally inefficient so the goal here is to develop a computationally and statistically tractable algorithm for this more general model and the way we're going to do it is by trying to learn the representation phi even though it's not given to us a priority okay in the interest of time i'm just going to give you a really whirlwind tour of what the algorithm is the algorithm is called flambe it stands for feature learning and model based exploration and it's a model-based approach it's just it's very simple at the high level it just alternates between fitting the dynamics and model-based planning and the way we learn the dynamics is we learn a low-rank mdp model via maximum likelihood estimation with function approximation so maybe we use a neural network to maximize the log probability of the transitions and we architect the neural network so it has this um low rank factorization at the last layer and then so that the neural network will actually give us a learned feature map hatfi and we plan to explore using this algorithm of gen at all and our learn features so that's like the high level the details are fairly complicated but i do want to tell you what we proved so we proved that flambe learns a low-rank mdp model m-hat that universally approximates the true model in the sense that for any policy any reward function the value for that policy and that reward in the model is about the same as the value for that policy and that reward in the true world up to an error tolerance epsilon and this happens we can do this with a sample complexity that scales polynomially in all the parameters independent of the observation space and with the statistical statistical complexity of the class of functions we're using for the maximum likelihood estimation as before flambe can run in polynomial time assuming we can sample from the models and solve mle problems and just to mention about the representation learning aspect of this we also prove that for any reward function the neural optimal policy is linear in our learned features five so the features that we do that we get are actually a good representation in the sense that they allow us to approximate the things we care about so that's all i wanted to say um just to wrap up so the conceptual approach here is to decouple the dynamics assumptions from the observations and this yields sort of tractable reinforcement learning settings where where we still have to use nonlinear function approximation and this sort of allows us to scale it to rich observations in a provably efficient manner um in recent work we've been able to show that we can use this approach effectively to learn environments like block mdps and low rank mdps and the way we're doing this is using the strategy of representation learning so in some sense i feel representation learning provably enables generalization and exploration and reinforcement learning with that let me just uh thank my collaborators alec michael sean john dependra gwen and thank you for listening hello i'm associated i'm a researcher at msrai deep learning research team part of my research is to focus on learning or controlling discourse in long text generation in this talk i will show you our recent work on controlling discourse via the use of explicit memory in decoding texture machine is one of the most important topics in nlp you can build models like machine translation which is a text generation system and can encode the utterance of a speaker and translate it into another language instantly perhaps in many languages instantly or it can build systems that can take multiple documents and generate short summaries that contain important and organized aspects selling points in order we can ask a system to summarize a multi-party meeting for us there are several other tasks that i'm listing here that require automatic text generation system so in essence we need the text generation systems for several reasons because there's an abundance of information like meetings emails online search results etc if summarize well they can help improve the efficiency also ai robots can converse with humans and help speed up the learning for them they can help improve productivity as well for instance online shops require large quantities of product descriptions which ai systems can actually generate summaries for before i move on i would like to mention a very recent talk by josh standing balance acl 2020. in his talk he defined three different ways of teaching meaning for ai agents the first one started with statistical language models another one was to building models that try to ground meaning in perception and action and the third one was semantic parsing which supports like logical reasoning and thinking my talk is about building statistical language models that can teach machines write texts like humans so i'll start with talking about one of the recent methods we propose to control the discourse with explicit memory in language models i will then conclude um talk about some new open research questions for better text generation i would like to start by talking about language models first since text generation is a type of language modeling problem links models assign probabilities to sequences typically they express this probability via the chain rules that i'm showing here as the product of probabilities of each word conditioned on that versus antecedents so why do we actually need this probability well a speech recognition a phrase like i saw where when is much more likely than the phrase like eyes of fn um or in speech recognition or like sorry spelling correction we see a phrase like 15 minutes that's more likely to be a mistake than minutes so a language model can actually quantify this true sentence probabilities which plays a role in several other tasks like machine translation summarization question answering so they're really everywhere so in text generation applications mostly we use this language model and condition it with a prompt and prompts can come in many forms here's an example of a constraint generation problem the goal here is to generate a paragraph or a multi-sentence text that describes the people the setting attributes animals mood etc uh in blue you notice that i'm showing core references relational information lots of prepositions like behind under etc so while being extremely powerful these language bonds can actually suffer from a number of issues for example if you tune your language model with an encoder decoder architecture and keep the decoder as an rnn based language model you might end up with text like this i'm showing on the right some of the issues you can see are repeated engrams the model contradicts itself the text is not really specific or sensical so there are fundamental issues not just without any language models but in training of the text generation models in general some of the issues related to the next generation um are uh for instance exposure bias which is related to the discrepancy uh between the training and the test time because at training time the model is conditioned on its ground truth while the test time is conditioned on its own generations there are other issues related to the compounding errors like caused by this label bias problem that i'm showing here is related to the fact that the previous bad decisions can actually hugely affect the current decisions which cause small areas to accumulate also large vocabularies can cause soft max bottleneck issues image to learn latent representations is mapped to a multinomial probability distribution over the vocabulary which is usually very large um last but not least we train the genetic models using a surrogate objective function which is a cross entropy without considering and and goal but then if you consider the end goal and add training data with things like reward based auxiliary losses it may also additionally cause inconsistencies one might say that most of these issues actually to a degree result with today's more sophisticated as well as complicated neural language models so let's look at some of them so this picture actually summarizes several node language model methods that i proposed recently so the basic idea of these matters is to train a language model on a massive amount of unlabeled data and then use the internal representations of the language models on subsequent end tasks this was a form of transfer learning our expectation is that these highly advanced today's language models should actually result in significantly more coherent than realistic simulated content so let's see some examples um so the context i'm showing here is from jpt2 paper from 2018 it's about someone making a chocolate cake explaining how that is made it's a bit different than a recipe um actually so if you give this to a gpt too it decides that the text needs a proper recipe and generates one almost perfectly this is actually an amazing performance but most often than not we see some issues with these type of generations again using the same recipe with a different sampling strategy we immediately notice things like incoherence in text for instance like this i'm showing here in bold but still these are not that easy to differentiate between how like a human written text so what is happening here well if you look at the core of the reasons we see that we're trying to model the meaning of a word by looking at its context for instance at the top as a decoding from left to right which is a jpt2 language model down is a birth style language model so we train these generic language models with no explicit planning with no information about the narrative flow topic structure or you know any other additional information and we expect that they should do well at writing long texts or stories so we claim that by just learning by predicting the next token given the context is not enough to generate multi-paragraph long text so this brings us to the discussion of how to control the long text generation for better narrative experience so there are many ways we can control the language models for better generation here i will discuss only one of the one of the ways which is controlling via discourse so our goal is to teach the language models about how to plan before and during the writing by mimicking the way the humans write stories specifically humans write down these plot elements or key events that they want to occur in a story when they are writing a long form story our goal is starting with these outlines teach a machine to order these outline story elements which are plot elements and then write a consistent story out of them that makes actually sense this is the the work that i'm going to be explaining so we have three goals we would like to generate the fluent multi-paragraph stories we will start with a large pre-trained language model like gpt we also want to keep track of the various plot elements in an external memory state and make sure that they are being combined in a way that seems possible lastly we want the generator stories to have a good narrative flow we're going to introduce various discourse-based features to allow us to generate more discourseable stories okay so in order to generate more coherent paragraphs that follows the context of the previous paragraph better we're going to take the outline that we're using as input we're going to send it to a gpt2 to generate a single paragraph and um and we're going to generate a paragraph because this is a supervised structurally conditioned learning task is going to learn to generate a paragraph given these unordered outline points now we hypothesize that different portions of the narrative are written stylistically differently so for instance an introduction paragraph or a setting paragraph of a story should actually slightly be different than a body paragraph or a concluding paragraph so what we do is we um introduce these discourse markers like i meaning introduction um into the gpt2 as as to as as a prior information so we can then continue this paragraph generation uh as like a incremental generation task after we generate each paragraph we continue generating other paragraphs using the same outlines in order to control the narrative flow between paragraphs we are going to introduce the next paragraph representation and send it to the next paragraph generation as an input we're going to also adjust this discourse marker because we were generating an introduction and now it's going to be a body paragraph which is the following is following the introduction pack so notice how we keep changing the checklist markers on the left of the outline every time we generate a new paragraph so the idea here is to keep track of various plot elements such as events objects relations that are already measured in the paragraph so we generate the next paragraph given these aspects that we didn't get to mention before this is a way of dynamic plot state tracking and the way we implement is through defining an explicit memory so we want to have a memory unit that is keeping track of all the plot elements that we're discussing we're going to have an explicit memory matrix such as this which is going to be split into two uh yes and they will carry a different information so the first piece is initialized with all the different plot elements coming from the outline we're using as input and the second part of the memory is going to keep track of the global document state by keeping track of the latent topics that have been discussed so far now using this memory metrics we actually alter the gpt transformer blocks to include this memory component as an additional attention mechanism emission decoder not only attends the previous tokens but also the memory component that keeps updated every time we generate a new paragraph also these components injected into gpt architecture now constitutes our model which is called the plot machines now down to the experiment to see how our language model differs from other language models uh before i do i actually i'm showing here a neural architecture for those of you who are interested in but i'm not going to go into details of that so we compare our model to several baselines one of which is a very strong controllable baseline which is grover it's one of the large language models which is another controllable generation in which the control is based on the attributes such as entities related to the context that is provided at the prompt time so for the experiments we conducted um a new outline labeled document data set this data set consists of articles such as wikipedia articles or newsfire articles or stories and corresponding list of key points and we fine tune our models our model and the baseline models in a supervised way with these outlines as input and the documents as output instead of showing accuracy numbers i want to show you um a somewhat more reliable way of evaluating text generation which is the human evaluation we conducted human evaluations to benchmark the model's outline usage narrative flow ordering of aspects and i'm showing here a comparison with just with grover which resulted in the outline condition generation and that the the plot machines can actually learn to keep track of the context better and it yields stories that have higher narrative flow compared to the grower and this was like evaluated with humans all right to conclude i presented an outline generate a condition story generation which is a new task for generating stories for outlines keeping um which represent the key point elements we call these plot machines which generated paragraphs using higher level discourse structures and dynamic plot memories to keep track of the outline and the story and our quantitative experiments and quality experience show that these plot machines are effective in composing tighter narratives which are like focus narratives based on these outlines compared to the competitive baselines uh we also release our code if you're interested in trying it out um lastly i would like to mention a few key points and future directions we should consider for long text generation firstly textual systems need to be able to perform common sense reasoning specifically regarding character's goal and their associated actions and we need a better way to incorporate common sense knowledge graph into long text generation for better text human looking text secondly building accurate language models that capture meaningful long term dependencies is actually a core challenge in natural language processing so we need to be able to learn well calibrated language models and track and manage the content uh that is conveyed in in there through better evaluation metrics and even share tasks we should also look at the way we do decoding and find better ways to mimic human generation process lastly i want to say a few things about evaluation um so training a powerful language model actually relies on evaluation metrics that can measure the model quality from different perspectives versus um it's very important to build evaluation methods that can determine whether a text is generated by human or a machine to prevent any potential harm also to me it's concerning that neural language models can generate open-ended texts that are fluent but not grounded any real world knowledge or facts such as fake news so uh we should be investigating uh this um uh in the in the future um so um we have a new uh evaluation uh paper out there um it's a survey paper um and i would like to close by uh self-advertising uh so i'd like to invite everyone to join our tutorial at em nlp this year with my collaborators in which we're gonna be discussing more about these text generation and evolution and several issues thank you let's uh proceed right now with with qa for the talks by katya and akshay and we have a few questions uh with one question for me and and several from the audience so let me uh start with an initial questions for tatia and akshay so you both presented some really impressive new theoretical analyses of rl systems that use some form of abstraction to identify related states that are used to support a more efficient exploration and adaptation so actually you use kinematics state abstractions and katya you used uh meta learning over simulated states to share knowledge from related tasks um you described some really impressive initial results of these systems and uh but i also know that you've both previously worked on much more complex real-world scenarios and so my question for the the two of you is what advances will it take to move very bad and homer slash flambe to get to some of the the broader complexities that we see in real world scenarios um maybe i can start with the take on this um i think this is a fantastic question and uh sue as you know my work um i'm really passionate about solving hard research challenges with in mind really getting to the point where we can enable new real-world applications and i really see very bad as a kind of case that that really makes this point we started from this general concept of base optimal behavior and then try to rethink how we can use domain expertise about a certain domain in order to utilize simulations and training that can be done in simulations before a system is deployed and actually needs to very rapidly adapt to a real-world situation so if you imagine kind of a scenario of let's say um patient flow optimization in the hospital we can imagine building very complex simulations where we can incorporate a huge amount of expertise that already exists about those settings with all the different kind of variations that um such a system might encounter in a real world environment we would then pre-train the system in simulation using as much data and compute as necessary and also with the ability to test the system and really understand whether it's learned appropriate appropriate decision making and then at deployment the system would be expected to generalize as long as the kinds of parameters of the rear-world system have been covered in the simulation so this would give us a setting to use utilize massive simulations but then at runtime very very rapidly adapt and optimally adapt to the context at hand what we proved out in the system or in in very bad is that we can use a very flexible encoder decoder architecture that uses latest advances in deep learning to learn a representation of how different settings might be related to each other in a latent state and with this kind of work we remove important bottlenecks of um kind of using very flexible state-of-the-art mortars for doing this so we are very excited to see how far this can already be pushed into real-world applications and we're actually examining at the moment where we can take a next step to prove out how far this can be taken along this road um we don't know yet what challenges will come up but i think that is a really exciting journey thanks katya um yes susan this is a great question and um yeah the way i kind of think about homer and flambe are kind of a very like blue sky kind of approach and there's i think a lot of steps that we need to take before these things are like something we can actually deploy let's say in like production um one thing that i i'm quite interested in doing in the near future is actually starting to sort of incorporate more of the ideas from the empirical literature so when we're working on these projects we're definitely taking more of a theory first kind of approach where we start with like basic foundations of learning theory and trying to apply them to rl settings and i think to make these things even like something we can implement and run on the standard dprl benchmarks i think it's going to be really important to understand what are the empirical approaches that are working effectively there and how can we sort of like close the gap between what we prove and what is kind of already working reasonably well in practice but i think actually if you want to move to let's say like more high stakes much more data limited settings that might be relevant for various production applications like robotics and these kind of things i think there's many other issues that are also kind of relevant so one of my pet peeve issues that kind of keeps me up at night is the fact that we actually don't know how to do cross-validation when an agent is making decisions about data collection and this this is a huge problem because cross-validation i feel is like some bread and butter procedure that everyone uses when they use machine learning and we don't know how to do it very effectively even for contextual bandits which is a much much easier problem than the things we're talking about in here so i think if we actually want to get these things to be productizable or whatever we do have to think about how how can we do some of these other things that are very important for getting machine learning into production okay um thanks let me follow up on on that um a little bit actually you know the the the work that uh you know you've done on homer um and and flambe do this beautiful theoretical analysis of improved guarantees when you have a very simple assumption of only low-rank mdps and so you really are starting to tackle fundamental issues i think in in representation learning in rl assuming that there's only a low rank state transition matrix in practice can you imagine adding additional structural assumptions for particular applications has have others tried that in in the literature yeah so maybe since the um audio video is a little bit strange let me just summarize briefly what we prove about flumbe yeah so we're considering a mdp model where the transition dynamics has admits a low rank factorization but we make no assumptions about the size of the state space we only assume that the the rank of the transition matrix is relatively small and what we prove is that you can do this model based function and production function approximation based algorithm that does strategic exploration while learning a dynamics model that admits the lowering factorization and gets a sample complexity that scales independently of the size of the state space and this is sort of uh and let me just add briefly the algorithm also sort of identifies a good representation which is sort of the factors of the dynamics model and this lets you do efficient reinforcement learning with linear function approximation afterwards um now to get back to your question uh i haven't seen too much i'm not the most familiar with empirical literature so okay on the theory side i have not seen too much but i think it is a super important thing that we need to think about so one model that i think is from a complexity standpoint quite difficult but from a practical standpoint quite important is like a factored um mdp model where there are many sort of objects in the world and they're evolving relatively independently and because the evolution is independent the complexity of the system is actually quite low um if all these factors were interacting all over the place this would be very very hard thing to to learn and to model but when you have these factors evolving independently it's something that seems relatively simple right and that i feel is quite a good fit for the real world right most people go about their daily lives and just bump into a few other people and so on and that's a model that other folks in the msr nyc lab are working on and how to sort of learn a representation that admits sort of a independent factors kind of decomposition and i'm also wondering if somehow the low rank structure can be thought of as maybe can be generalized like tensor structures which also can maybe capture some of these many factors evolving kind of independently so i think this one is quite an important one i have not seen much theory work on it but i think if we want to sort of push to capture very complex systems this is something that we might want to try to think about okay thanks actually i think that's a first of all i think it's a fabulous result and you're right that there is a lot of structure in the world and um you should be able to capitalize on you know a lot of that so i hear from our uh crack crew in the back that we are ready uh to resume live the live um feed so let's go back to that and then we can come back as a group of four and take questions from again me and and the audience so thank you ashley and katia for starting the uh the q a early hi everybody i'm dan klein and today i'm going to be talking about the semantic machines approach to conversational artificial intelligence in particular i'm going to be talking about task-oriented dialogues which are designed to be trained from data and which are grounded in services in the real world all of this work is joint with the amazing people at semantic machines shown here in a photograph from back when such photographs could be taken language is so important because it's a universal interface not just in the sense that people already know it but in the sense that it promises to unlock the entire universe of capabilities in the technological ecosystem that's around us let me show you what i mean right now we live in an ecosystem with tons of services and capabilities that surround us in tons of devices however the kinds of things that we do with them are often easier said than done for example if i'm using my calendar i might want to know when are megan and i both free i can figure this out in my calendar but it might take a lot of work and it takes barely any time at all to say it similarly if i'm preparing slides i might want to delete the footer of this document it's easy to say but it might be hard to do in this case it might be hard to do because it's a piece of functionality that hasn't made it to the short list of what's exposed in the gui as another example i might want to know what the weather is going to be like during my morning run now in this case i can get this done but it involves multiple steps and i have to do the orchestration and as a final example maybe i want to set the photos from yesterday's party as my tv screensaver it's really easy to say but it might take me a long time to do what we really want is for the computer to do this orchestration for us so what are the standard approaches to language interfaces and how do they solve these problems well the standard approach is about intents and slots and let me show you how it works if i say something like turn on the lights the idea is the world has been sliced up into a bunch of domains like calendaring or home automation and in each of those domains there is a list of functionality that is supported each square here represents something the system has been programmed by hand to do in response to the user utterance we pick the one that's appropriate for example maybe the one that turns on the lights and this choice is done using machine learning it's basically classification and this can be extended for example if i say set a timer for five minutes i can use machine learning to classify that to the set a timer function i can also extract out the five minutes as an argument to that function so how far can we push this what if the user says something like what time am i getting coffee with megan we could probably just about make this work though it might get a little creaky but let's imagine that we found the appropriate functionality and we answered 12 30 pm but now what happens when the user asks a follow-up question like how long will it take to get there well now we're in a little bit of trouble because one this is a contextual question where is there and two it's not really just about calendaring anymore because this connects up to multiple different services in multiple different domains and that doesn't really fit the paradigm so what's the semantic machine's view well instead of viewing the world as being composed of a bunch of pre-bundled units of functionality instead we imagine there's a whole space of things the system might be able to do and in response to an utterance from the user in a context instead of picking a pre-bundled unit we're going to synthesize a new program which combines multiple actions dynamically this program we synthesize can hit multiple backend services and isn't confined to a single domain in this talk i'm going to focus on five key ideas of our approach the first is that dialogues are actually programs and this is going to involve techniques of program synthesis the second is that complex tasks are actually built up out of simpler ones and this is going to involve compositionality the third is that meanings are going to inherently depend on context and we're going to handle this with techniques of meta-computation fourth things will go wrong and so i'll talk about how we approach this through a lens of exception handling finally we think systems should tell the truth and that's going to require new techniques of dynamic generation where we take as a primary focus the truth of the system and not just its fluency so now let me tell you what i mean when i say dialogues or really programs and what this has to do with program synthesis even for a simple utterance like turn on the lights we could view this as synthesizing a program here the program would be something simple like calling the function turn on lights if the user says set a timer for five minutes we could see this as again calling a function set timer but now with an argument which is extracted from the input this is about what the normal standard setup is designed to handle in terms of intents and slots this can be pushed a little bit so for example if i say what time am i getting coffee with megan you could imagine having a time of calendar event by title and attendee in which you pass the title in the attendee this is where it gets a little bit creaky but this can be made to work and in fact in standardly deployed systems you do often see things just like this to push just a little bit further into the tail the problem is you're trying to handle a combinatorial problem by enumeration so what could we do instead well instead of synthesizing this single function call in response to what time am i getting coffee with megan i could instead break it up into a sequence of simpler steps first i could find a person whose name is like megan then i could search for an event with a title like coffee that includes that person and finally i could describe its start time breaking this up into multiple pieces has several advantages first of all it's now easier to learn and easier to cover the long tail of combinatorial interactions between these atomic pieces secondly the structure of the input language sometimes but not always provides a strong clue to the structure of the program itself once we frame dialogue as a program synthesis task this gives us a nice treatment for compositional behavior both inside a turn and across multiple turns for example within a turn if somebody says something compositional like what's the temperature going to be for my coffee with megan this causes a big problem for a classic system because the slots and fillers approach doesn't naturally synthesize or integrate across domains however here we can start out with the same program that finds that event but instead of describing its start time we keep on going once we found the event and do the computation needed to get a weather forecast for that event's time and place and then finally describe the temperature this lets the system answer the compositional query in a compositional way notice that the information flows from one part of the program to another via variables in the structure of the program what happens in a multi-term context well if somebody says what time am i getting coffee with megan and we synthesize the program to answer it and the system gives a time and then they follow up by saying what's the temperature going to be well again we can simply handle this multi-turn interaction by continuing the program again the flow between turns is now along the computation structure in a nice way that parallels what happens in the single turn compositional case behind all of this is actually a computation graph where each computation we do represents a node in the graph and this graph is persistent and represents the context for future parts of the dialogue as we'll see when i say that information flows along a variable what this actually means is a node in that computation graph is shared between the older computation and the computations that continue it another major limitation of standard approaches is that meanings depend on context we handle this through meta computation so for example let's look at the case of reference maybe the user said what will the weather be like and it's easy enough to synthesize a little program that finds the weather forecast at here and now however what happens if we've just been talking about coffee with megan at 12 30 and then the user says what will the weather be like well we could synthesize a program that finds the weather forecast at that event's location in time what we do instead is unify them into a single analysis which in both cases says get me the weather forecast at the salient place and time what this does is the computations appeal to a meta computation which involves place and time which are now to be extracted from the compute graph in a dynamic way another big contextual challenge is revision let's say a user says do i have many meetings today here's a fragment that computes that now what happens if the system says no and the user says what about tomorrow well of course we can synthesize a fragment which finds meetings tomorrow but what about tomorrow might occur after asking about the weather and now what about tomorrow needs a program that finds the weather tomorrow or what if it happens after what's the weather going to be like during my third meeting today and you say what about tomorrow let's see how meta computation on the data flow graph can handle all of these in the same way let's say the user says do i have any meetings today and we synthesize a program to find today's meetings and then they say what about tomorrow well of course we could just synthesize a program to find meetings tomorrow but remember that the data flow is still around from the previous computation and so what we're going to do is rather than synthesizing a new program from scratch instead we're going to synthesize a revision to the previous computation it's going to look something like this whereas before metacomputation found a value in the compute graph now we're going to find a node in that graph itself and then we're going to do a program transformation on that fragment to build a new compute graph which we will then execute and describe its result now what about tomorrow more or less means do the salient computation except this time do it with tomorrow substitute it in for the salient date next what happens when things inevitably go wrong here we have a representation for exception handling which handles not only true errors but actually quite a wide range of things that can happen in a dialogue for example let's say the user says set up a meeting with megan and we synthesize a program to find a person like megan and then to create an event with them what could go wrong well maybe the system finds more than one megan or maybe it finds no megans or maybe there's an api that has an error during this process or maybe you can't create a calendar event without a title and so we get a missing slot error of some kind in each of these cases the program will not completely execute and will instead raise an exception let's imagine that there's more than one megan that is salient well in this case a multiple matches found exception will come back into the system and will trigger a new round of program synthesis this program synthesis will most likely be a revision of the previous program using meta computation to transform it into something that for example asks for a clarification before continuing this turns out to be very powerful not just for handling true errors but for elegantly handling a wide range of things that can modularly go wrong finally i said programs should tell you the truth of course they should in standard systems the way this is accomplished is by having a list of templates of what the system can say this doesn't scale to a case of contextual program synthesis because if the system's constantly synthesizing new programs it would constantly need new templates so here's a sketch of how we let our generation be dynamic while keeping it grounded when the user utterance comes in we synthesize a program and at the end of that program is a call to describe now what we pass in to describe is not just the value that we've computed but rather the entire computation graph that led to that value the generation module then uses the structure of the computation graph to formulate a response because the generation module has access to this computation graph it's able to stay grounded to the values and computations inside and say something truthful like in this case it will be 74 degrees tomorrow at noon at rosie's cafe now of course it's also possible to say other truthful things but hopefully we've avoided the system lying for example saying something like it will be 99 degrees when in fact it will not in summary all these techniques have been about reaching the long tail of things people can say things they want to do and context that they can ask for these things in there is a long tail and standard techniques focus on the head although standard techniques are actually quite good at lexical robustness they've limited functionality by assuming that there is a fixed inventory of actions that have been manually curated and they limit context by assuming that most of the time little or no information is retained between turns in contrast the semantic machines approach is designed to reach that tail it's based on compositional program synthesis so that we can build complex new programs out of simple building blocks it uses meta computation to manipulate past data flow graphs it uses events and exceptions in order to react when things go wrong and it has a dynamic but truthful generation system let me show you what this looks like when it's up and running this is a demo of our system trained on data from multiple domains like the ones seen in the talk today even though we didn't get into the predictive modeling here the program synthesis revision and generation are all trained from that data first let's make sure that there's actually some program synthesis going on so we'll ask it something that's much more specific than any hand-built intent or slot classifier would be able to handle what's the weather going to be like three hours after i meet with brees manager next tuesday in order to deal with this query the system has had to figure out which brie we're talking about who's her manager finds the event and send that off to a weather service now let's show the program synthesis working across multiple turns of a dialog using the compute graph reference machinery when is my afternoon run and the system looks that up and now what's the weather going to be like then and here that weather computation has used the information computed in the afternoon run query finally let's look at a longer interaction that will show the meta computation machinery doing reference and revision operations show me my day tomorrow it starts off by fetching the day is jason coming to the research group and it's able to find that event and figure out whether jason's coming what about diana invite her please which her invite her to what that's been figured out using the reference machinery rename it to research and engineering discussion here the system's done multiple things with program synthesis it's very easy to do multiple things in a row you just concatenate their programs okay great that's a quick tour of what the system can do i've talked about a way to view conversational ai as a problem in truthful compositional and contextual program synthesis there's a lot more to say here about the data about the modeling how this interacts with speech and a whole bunch of other challenges that i won't have time to get into today if you're interested in the details a paper and a release of a data set are coming soon and if you're interested in the system then stay tuned for that as well thank you thanks everyone and we are now ready to start the the unified qa session before we do that i know there was still some some challenges with syncing of audio and video so i want to start by asking asla to quickly summarize some of the key takeaway messages for for people and we'll post the appropriately sync videos later today so asa yeah thank you susan so um um i presented an outline condition story generation um and um it's actually a new task for generating stories from outlines that represented key plot elements and so given an outline for instance as a set of phrases or engrams which can describe key characters or events in a story the task is to generate a coherent narrator that is consistent with the provided list of outlines so this task is actually challenging because the input um only provides just a sketch of uh of the plot that we are asking the model to write uh so because of this the model actually needs to generate a story by weaving through all these key points and trying to find out which come because these are we're not assuming that these key points are actually sorted um so it requires this model to keep track of these dynamics states of latent plot uh conditioning on the input outline while generating the full story so uh without going too much detail uh well i i linked the uh the paper that corresponds to the talk on on this on my website and and this this page and and the code is also online i've seen a a question and i'll put a link here as well um i just want to also mention that the the the methods that we the method that or the model that we came up or this approach uh we just applied it on a story generation given this list of conditioned outlines but um we have been working on some other downstream tasks that can this approach can be useful and efficient uh for other long generation um tasks um yeah okay great thanks sasa for the the really coaching summary and for posting slide and code i want to start with a question to asla and dan who who both had really exciting new results on data-driven generation asla in going from key points to a coherent long-form story and dan in going from questions to generating a sequence of programs through synthesis that compose actions and then not so much about generating answers that the human consumed after that composition of actions but uh still generating um interesting uh summaries of the the state of the world a key component in these systems is really maintaining memory or context throughout the course of either a long sequence of text or a conversation um also and dan are there similarities or and differences in the approaches you took and what can you learn from each other yeah i think uh that's a great question um i i think in some ways my first reaction is that they're very complementary so one of the things that i talked about but didn't really get into much detail is eventually the system does to render text or speech that a human can understand and at that point you have this problem where you've done a computation and there's a lot of things you could say but you need to make sure it's it's consistent and coherent with what you've done and not just with what you've done but with the dialogue so far and i i i think um that what ashley's is talking about in her work really dovetails very nicely with how you might go about doing that there's always been a tension in generation between um guaranteeing that something says exactly what you want but not having much flexibility versus these modern large-scale neural models that can say anything but but might lie to your face even if they're attending to the true information and the ability to um make sure that when the system says that your package is coming on friday it's not actually coming on thursday but friday's more common um these are going to involve techniques like what she was talking about so i see these things as very very compatible and i think that's uh that's really some great work i just want to add um thank you dan it was a great great summary and i do agree i just want to add something on that is is that like for instance our models um unlike dance is not doing any much factual consistency or um you know uh or you know conditioning on on in a knowledge graph for instance or knows about um common sense reasoning or things like people uh would think oh you know this is nonsense uh we could you know technically generate things that are that have some ethical issues or some some bias because the bias is based on the what this uh the pre-trained language model has learned um so i guess that's especially when generating longer texts we probably is what i'm currently these days i'm thinking is to focus on evaluation aspect and how to actually incorporate that into this generating generation models and someone has already asked like if these models like gpt3 can be deployed okay if i if i do see these gpts um i don't know it depends on on the infrastructure and and they're like huge models uh they're potentially really useful because uh they're really good language models and we do really need language models to complement uh our our systems and i i really hope that we do i don't know if dan has any comment on that i i totally agree i mean even in a system like ours where we're so concerned with making sure that um the system shows its work and tells you something that's that's true and drinking the provenance of that information there's still all kinds of other uh there's all kinds of other decisions to make pragmatic decisions you say more or less we saw a little bit of that in the talk there's decisions about being consistent from turn to turn and and how you convey information and i i think these kinds of longer distance dependencies um which really are an important part of making a conversation natural and effective i i think are are very important and are um the kinds of techniques that you're talking about are very very applicable yeah thank you thank you you both raised this really interesting issue of going from uh generating sequences of words to larger constraints and pragmatics that govern how we generate text over through throughout a conversation or over a long-form story so assa are there aspects of storytelling other than kind of the content or the outline that you showed that you think really need to be incorporated to to do the you know the kind of long-form story generation that you talked about um there are a very good question thank you susan um there are um um a lot of i think they're working in rl uh in you know these multiplayer games and others uh uh also in dialogue as well that um you know you need to ground the generation uh in order to uh get what you really uh want from the system for instance you don't need to ground it but and in cases where uh you actually need the consistency uh or you know factual consistency or or like for instance in dialogue you really want the system to know about some certain list of um you know aspect or like you know entities uh or or constrained in such a way then um uh incorporating some sort of a knowledge graph structure into the generation would probably help um generate more um grounded i'm going to use the same word here but um and also more consistent generation especially for the longer um text but then this longer sequential learning uh also appears in not just for text but also like i said in katja and actions um systems as well um where you know the the issues that i listed in my beginning of my talk which you probably didn't get to listen well but i hope you will later is is these um you know labeled bias problem or compounding error so if you do make an error it's really hard to uh you know come back from that error so how do you actually build your model so like you know we have these beam search algorithms they're not that efficient but i i i think like we need to efficiently like think more about these longer term uh generations and how do we actually maybe using reward based systems are or like uh complementing the the current generation texturizing systems with those would probably a way to go which are grounded in the knowledge graph forces you can actually see if that that reward uh can be aligned with with the information that you provide like the structural information so um actually do you want to comment on that because i know a lot of your work is also contact grounded in uh games or akshay right now in in terms of the uh the lower dimensional manifold that you use for uh representing state transitions and yeah as aslam mentions i think a lot of the kind of challenges that we're encountering are quite well aligned and the kind of scenario that you that you mentioned there thinking about grounding language in interaction with some railroad environment is something that's fascinating to me it's not something that we've been able to um really dig our teeth in so far um but i think we can learn a lot from as london's work uh when we when we get to that point and i'd love to work on that in the future let me just add one quick comment i think actually these things are also quite aligned on like a more technical level so it feels like rl agents should also have attention and you know these like grounded knowledge graph type systems to help them reason about what decision what decisions they should be making and so there's like i feel a real opportunity for maybe like all of us to actually like kind of try to do something kind of maybe briefly coming back to the discussion um that also really nicely aligns with the discussion of structure that came up earlier in the q a with akshay in particular right so language provides a really interesting structure in terms of how we reason about the world connecting that to interaction and abstraction could be really really promising fabulous um let me we're nearing the end of the the session i want to give you each 30 seconds to say a little bit about the problem that you are most interested in i think we've heard some of this come up in the in the q a but why don't we do it alphabetically so so this could be a problem that you think new phd students would benefit by working on so actually i'm sure yeah so i think one thing that i would encourage phd students to do is with regards to rl i would encourage them to find say a real problem that they can try to uh model and understand and figure out what kind of structures are present in that real problem and then work on studying how to design algorithms that capture those structures asla my advice would be if they are interested in learning especially long text generation long or short text generation is to focus on methods where evaluation is the key in my opinion the next thing that we need to work on explainability explainable uh reasoning um and finding out like why this sentence is generated uh i don't know how we none of us do at this point we have some methods that we we know how but we need to work on that so that will be a very very interesting direction also um ethical issues there's a lack of systematic methods for evaluating how efficiently we uh with these nlg systems and algae meaning natural language energy systems can actually avoid these improper or offensive language how do we actually build these systems especially important for production systems yeah great point when you generate language you will hear dan let's see i i i'll i'll say three things for 10 seconds each the first thing is grounding thinking about how we can relate all of these latent representations that are learned in current models to grounded contingent facts inside a database that may change and need to reflect uh truth the second is modularity we build these very large systems that are very powerful and end to end but one of the first things we learned in computer science is to build big powerful tools you need to be able to modularize and abstract how do we get that uh how do we get that power on top of what we already have and the last thing to think about is data when you have a phd student often you think about the data as being it appears as if by magic and it's static but in the real world you start with no data and then you have a little bit of data and then you start getting more data but your early data starts to decay because specs are changing and the world's evolving and there's a lot of challenges here that i think go a little bit unnoticed but are very very real research problems uh that impact uh uh very greatly on real systems okay fabulous thanks dan katja you get the uh next to the last word i get the last word now i have a very challenging task on on following amazing speakers and i very much agree with all the aspects that you've already brought up um so maybe i'll just add one more thought there are applications that are possible today also think about the kinds of applications that you would like to enable and what's missing in the current uh research directions to enable exactly what you would like to work towards okay thank you um all of the the speakers and to the to the audience as well we've reached the end of today's technical session again i apologize for the technical difficulties in syncing the audio and video across different platforms but we will post the synced videos for attendees hopefully later today and for the public uh shortly thereafter thank you all for your attention and engaging with the engagement with the speakers the frontiers in machine learning event will resume tomorrow morning at 9 00 a.m pacific daylight time and the theme tomorrow is going to be security and privacy in machine learning there'll be sessions on confidential computing security in the face of adversaries and moving beyond fairness to social equality so an amazingly broad span of topics that all talk at the high level about security and privacy i hope you can join us again tomorrow and thank you very much for attending today 