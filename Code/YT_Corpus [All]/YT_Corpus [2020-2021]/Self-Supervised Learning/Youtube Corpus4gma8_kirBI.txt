 hi mitch wenger back with another video on data analytics and machine learning in this video we'll discuss supervised segmentation hope you enjoy okay let's get started in data mining we look for patterns in the data that we can use one way we can extract these patterns is to segment the population into subgroups by attribute where each subgroup has a different value or different average value for the target variable than the other subgroups do members of the same subgroup by contrast have similar target values of course we have to use attributes which will be known at the decision time even though we won't know the target value this segmentation allows us not only to start generating a predictive model for our target variable but the approach also lends itself to developing a very understandable set of rules for defining the segmentation process so for example we end up saying something like young professionals who live in seattle are going to have a churn rate of about 14 so young professional and seattle would be attribute values that we use to define the segment while that churn rate 14 in this case describes our predicted target value so how do we determine whether an attribute is important vis-a-vis the target variable and this is the basic concept we use in supervised segmentation how much does the attribute tell us can we automate the selection of the best attributes and then rank them maybe and as you might guess the answer is yes let's look at one useful way we can select those attributes and how we can continue the process to create a full supervised segmentation model the approach we'll investigate is quite useful but it's important to remember that this is just one of several approaches to identifying useful attributes nevertheless it's a good solid approach that you'll want to keep in your bag of tricks and something you can think about conceptually as you approach a new data science problem so once you run into your next huge data set with dozens or maybe even hundreds of attributes it'll be useful to think back on this idea of identifying informative attributes in order to help get started so let's say we do have a large data set how do we start figuring out which attribute gives us the best the most informative partition and in this example we can start with a binary classification problem yes or no so in the example you see on your screen we have 12 objects each object may have one of two colors orange or blue one of two shapes square circle and one of two borders green or pink so each object is labeled with a y or an n which represents the value of our target variable so you can imagine this as the label for indicating churn or for indicating fraud or for indicating a potential loan write-off or whatever now we can describe the feature vector for each object as the set of values for those attributes color shape and border and of course in this practice data set we also have the target value available so our first question becomes okay we have three attributes which of those three attributes best segments these objects into groups that differentiate y and n values in other words which attribute will give us the purest segments possible now in data science pure means homogeneous target values within that group the closer the segment is to all members having the same target value well then the purer the segment obviously if even one member has a different target value then that segment is impure real-life data rarely has a single variable that creates fewer segments instead we look for ways to minimize impurities within each segment this gives us a couple benefits first we have an opportunity to learn more about our data set and hopefully the greater population you've had a chance to do this hopefully with some exploratory data analysis this goes hand in hand with that process we could also use the attributes in a predictive model here we're going to try and predict the yes no value of our label attribute we can then make an offer give better terms investigate the transaction further and so on when it comes to selecting which attributes to use we have a number of considerations as we mentioned just a minute ago an attribute rarely splits the group perfectly even if one of the groups ends up being pure the other or others might not be so consider our example that we just looked at with the objects let's say we eliminate that first circle we'll imagine the data set doesn't include that first circle so we could then use the color orange to create a pure group with value n of course we only have one member in that group now right the blue group of course would still be impure so perfect splits are rare next if we do split into two groups one with an n of one and then the other group has an n of ten well is that a good thing maybe a different type of split that produces a more evenly distributed pair of groups even if each group isn't totally pure might be better as long as we still have significantly improved the purity of each group and of course some attributes categorical attributes are going to have more than two values so if we have different attributes that each have a different number of values how do we compare the results of those splits how can i compare a split into two groups for one attribute versus a split into five groups with another attribute and then finally continuous attributes well what about those numeric attributes whether they be a continuous real number or an integer do we create a category for every numeric value well short answer is no luckily we can address all of these issues for classification problems mathematically and the mathematical formula is based on the concept of entropy and its cousin information gain these concepts came around in the 1940s let's attack them in reverse order in information theory entropy is considered a measure of how much disorder exists in the data set in supervised segmentation we can consider the whole group but if we break it into segments we can also consider each segment separately with respect to how pure i.e low entropy or impure high entropy that data set or segment is so we calculate entropy like this we represent entropy with this h so h of x for our data set equals the negative sum of each target value i for every possible value i of the target the probability of that value times the log of that probability for the value and we can use natural log or base two we generally use base two and data science that works out pretty well for us so the graph to the right might give you a better feel intuitively for how it works so we start over there on the left and you can see all of the data points have negatives so if we're looking for data science with a plus sign well we aren't going to find any there there's no uncertainty they're all minuses so we have zero probability of finding a plus sign we have zero disorder therefore zero entropy all values are the same we know what the value is going to be now as we move up that curve we start to switch out some of the values and we can see we have a mix of minuses and pluses moving up the curve until we reach the top of the curve in which case we have an even split of pluses and minuses so we have a 50 50 chance of selecting a plus that's the most uncertainty we can have in this data set therefore we have the most entropy and that entropy value for that most uncertainty is one now as the distribution continues to add more pluses you can see that we start to become more and more certain of our prediction again because the probability for pluses is getting higher and higher finally in the lower right group we have all pluses at which point we have a hundred percent chance of selecting a plus no uncertainty therefore no entropy the value is zero all right we can calculate that using this formula and it works like this if we go back to our shapes that we were looking at before we've got some no's we've got some yeses those are our two possible target values i if you will so the probability for no is 5 over 12 41.67 percent probability for y is 7 over 12 58.33 so then we just plug those into the formula and we only have two values so we're going to have to do two calculations for those two possible values probability of no times the log 2 probability of no and then the probability of yes times the log 2 probability of y we'll add those two results together so we end up with 0.4167 and then you can use the log function on your calculator you can use the log function in excel or whatever to calculate that log value and we end up with negative 1.2630 and then for 0.5833 we end up with a negative 0.7776 and so we can add those two together the product of those two we end up with minus 0.4356 minus 0.5263 we flip the sign on that result we end up with an entropy of 0.9799 we're not quite at 6 and 6 which would give us an entropy of 1 but we're pretty close so our entropy is pretty high there all right so that's entropy now what about information gain how can we determine which split is the best so let's look at this formula information gain for a training data set basically the full training data set t's with all of the labels marked for us we already know what the target value is and then we're going to evaluate one of the attributes a and partition on that and then we'll do this for the next day and for the next day and for the next day however many attributes we happen to have so the information gained for the data set by partitioning on attribute a well basically we're going to calculate the entropy for the whole data set on split there on the left side of the right side of the equation and then we're going to add up the entropy values for all of the possible values of the attribute based on the probability of that value in the split so that's basically it it's a weighted value of entropies post split so how do we do that well let's take a look at our example we can split our data set three different ways we can split it on the color of the object is it blue or orange we can split on the shape of the object is it square or is it round we can split on the color of the border is that border green or is it pink and then we calculate for each of those subgroups the entropy and we just work our way through it mechanically so if we split on object color you can see we end up with two groups the small group has two but we have a 50 50 split so we have a probability of 0.5.5 the larger group has 10 items in it six yeses four nodes so we have a 60 40 split next we've got a split on shape so the round group we end up with three two of them with yeses so we have a 67 33 split on the square we have nine so we have five and four fifty six forty four and then finally down at the bottom we have a split on border color and we have five yeses and one no for the pink border so that's 83 17 and then we have four no's and two y's for the green border 33 3367 for the s's all right so intuitively we can look at this and say well it looks like border color gave us the best split because we now have groups that are more pure than the original group and the calculation will bear this out so we're looking for which of these combinations of groups is going to give us the best improvement in weighted entropy so now we do our calculations we're going to multiply our probability times the log of that probability for the group so in the top we have 2 out of 12 so 0.1667 and we have 10 out of 12.8333 times the entropies of those various groups so we end up with 0.9758 next we have three 25 percent versus 9 75 times the entropies for those two smaller groups and we end up with 0.9729 finally we look at the border color we have a 0.5 probability of getting in either one of those groups so we multiply that 0.5 times the entropy for each one and we end up with 0.7880 so keep in mind our original entropy was 0.9799 and now we just subtract that newly calculated entropy for each attribute split from the entropy score of the unsplit group voila information gain so in this example we can eyeball the subsets like we just did to see if things make sense keep in mind that our original group had a seven five yes no ratio so we had about a fifty eight percent chance of randomly picking y in the split on object color our chance of selecting the y are fifty 50 for orange 60 for the blue objects both pretty close to our ratio for the entire group and as you can see our information gain is pretty small if we split on shape we get that similar outcome we have 67 percent likelihood of choosing correctly for the circles 56 for squares again both pretty close to the full group ratio so we get another small information gain and as we've already discussed our split by border color gives us 83 for pink borders a much higher likelihood a much higher percentage than we had for the full group and 33 percent for the green border a much lower value than we had for the original group so both of these give us a more definitive split for each group that is we have a better likelihood of guessing correctly so accordingly the information gain is much greater and that's the one we would choose first border color now as a plus the notion and calculation of information gain addresses a lot of those concerns we just raised about segmenting the data we can determine the information gain even if the resulting partitions are still impure so we can still make that calculation we can also calculate the information gain no matter how many categories we have that entropy formula can be calculated for however many values of your attribute value you have and you can split and calculate the new entropy and the weighting of that entropy for any number of values of your attribute it also considers the number of instances that end up in each subset and weights each subset accordingly so that just leaves the numeric attributes we have talked about as well well we can do what we call binning the numeric attributes by dividing the attribute into a discrete number of value ranges and you see this often with income so you may have income groups zero to fifteen thousand fifteen to thirty thousand thirty to fifty thousand etcetera once we've done that then we have ourselves a categorical attribute that we can move forward with so we've completed the process of identifying the attribute with the most information gain now this process can be performed independently for each of the subgroups with our remaining attributes of course attributes with more than two values do add a bit of complexity but the basic approach holds so note that once we've split on border color we can now split each subgroup using the most informative attribute for that subgroup in this case our next split for the pink borders well it could be on shape or on color as each of those would give us two completely pure groups on the green side though the splits would be different and we would split on shape next as it would also give us two pure groups or if we split purely on color of the shape we would not end up with two pure groups so you can see we might end up actually splitting on different things and result with a tree that has different attributes at the same level on different branches so calculating entropy and information gain is not particularly difficult but even so our data mining tools do help automate the process for us it's still vitally important to understand how the process works because you may be asked to explain a model or in particular one of the decisions it made at some point in your career fortunately if you can visualize the approach we just presented for identifying informative attributes you're off to a good start that's it for this presentation i hope you found it informative as always be sure to check out the other videos in this series you 