 AND ENJOY YOUR NEXT SESSION.  DID YOU KNOW THAT A. I. MODELS WITH BILLIONS OF PARAMETERS ARE TRAINED AT SCALE? WOULD YOU LIKE TO SEE THAT IN ACTION? MOREOVER, WOULD YOU LIKE TO ASK THEM A QUESTION AND EXPERIENCE IT BY YOURSELF? WELL, IN THIS TECH MINUTE YOU WILL MEET THE VICE PRESIDENT OF SEARCH AND A. I. AT MICROSOFT AS HE SHARES A BEHIND THE SCENES LOOK AT MICROSOFT A. I. AT SCALE INITIATIVE. USING HUGE PRETRAINED LANGUAGE MODELS AND HOW THEY'RE TRANSFORMING OUR USE OF A. I. AND MICROSOFT. WATCH THIS ONE. HI, I'M RANGIN VICE PRESIDENT OF SEARCH AND A. I. AT MICROSOFT. I'M REALLY EXCITED TO TALK ABOUT A. I. AT SCALE HOW WE USE LARGE, PRETRAINED LANGUAGE MODELS AND HOW IT'S TRANSFORMING USE OF A. I. IN MICROSOFT. EVERY STORY ABOUT A. I. AND MICROSOFT STARTS WITH BULL GATES AND MICROSOFT RESEARCH. WHEN HE -- BILL GATES. HE HAD A VISION THAT COMPUTERS WILL ONE DAY SEE, HEAR, TALK, AND UNDERSTAND HUMAN BEINGS. OVER THE LAST FEW YEARS, MICROSOFT RESEARCH HAS HAD MANY BREAK TLOOURS FROM VISION IN 2016 THE FIRST COMPANY TO REACH HUMAN PARITY TO SPEECH A YEAR LATER WHERE WE REACHED HUMAN PASHITY ON SPEECH RECOGNITION CHALLENGE, THEN MACHINE READING EXTRA E HENGS. -- XREE MENTION. TRANSLATION, SPEECH THEY SIS AND FIRST COMPANY TO MEET THE GENERAL LANGUAGE UNDERSTANDING BENCHMARK. THERE'S A TOUGH BENCHMARK WHICH NEEDED US TO UNDERSTAND LANGUAGE ACROSS 10 DIFFERENT TASKS. SO DEEP LEARNING AND MICROSOFT RESEARCH HAS BEEN REALLY, REALLY POWERFUL IN TERMS OF SOLVING THESE EXTREMELY DIFFICULT CHALLENGES. IT'S GREAT THAT WE'RE MAKING SO MUCH PROGRESS ON THESE RESEARCH BENCHMARKS, BUT WHAT MATTERS IS TAKING THIS TECHNOLOGY AND BRINGING TO IT CUSTOMERS. WHAT WE NOTICE WAS WE HAVE A LOT OF DIFFERENT A. I. GROUPS ACROSS THE COMPANY, FOR EXAMPLE MICROSOFT OFFICE, WE HAD A TEAM WORKING ON A. I. FOR BETTER READING AND WRITING ASSISTANCE IN WORD. WE HAD A TEAM WORKING ON A. I. FOR BETTER SEARCH DEVELOPMENTS. AND WE HAD A TEAM WORKING ON A. I. FOR BETTER, DO UMENT MANAGEMENT AND SHAREPOINT. THEN WE HAD A TEAM IN OUTLOOK WORKING ON A. I. FOR BETTER E-MAIL MANAGEMENT. WE SAW A BIG OPPORTUNITY WHERE WE CAN HAVE THESE TEAMS COLLABORATE, SHARE, AND BUILD FROM EACH OTHER RATHER THAN WORKING ON A. I.. AL GO MYTH MANIES INDEPENDENTLY. TO AK ACCELERATE, WE TOOK ADVANTAGE OF THREE TRENDS. THE FIRST TREND IS TRANSFER LEARNING WHERE CAN YOU TRAIN A. I. ALGORITHM ON ONE SET OF DATA AND SET A TASK AND REUSE THAT SAME MODEL AGAINST A DIFFERENT SET OF TASKS. THE SECOND TREND IS AROUND LARGE SUPERVISED NETWORKS. YOU CAN ACTUALLY TAKE, TRAIN A VERY LARGE NEURAL NETWORK OVER MASSIVE AMOUNTS OF DATA AND SELF SUPERVISED WAY. YOU DON'T HAVE TO GIVE IT SUPERVISED LANES FROM A HUMAN, KIT LOOK AT THE DATA AND THEN TRY TO STRAIN TRAIN ITSELF. WITH LOTS OF DATA AND WE CAN TRAIN REALLY MASSIVE MODELS. THESE MODELS ARE POWERFUL. AND THEN THE LAST TREND WAS A CULTURAL SHIFT, WHERE SATYA SAYS AS A COMPANY WE NEED TO COLLABORATE AND SHARE MORE. AND THE RESULTS HAVE BEEN PHENOMENAL. IF YOU THINK ABOUT WHERE WE WERE LET'S SAY THREE YEARS AGO, THE BIGGEST MODEL THAT MICROSOFT COULD TRAIN WAS ABOUT 10 MILLION PARAMETERS A YEAR LATER, 100 MILLION PARAMETERS. EVEN IN EARLIER THIS YEAR WE TRAINED ONE OF THE LARGEST NATURAL LANGUAGE GENERATION MODELS AT 17 BILLION PARAMETERS. WITH EACH ORDER OF MAGNITUDE WE SEE THE MODEL GET BETTER AND BETTER. THIS IS WHAT WE ANNOUNCED EARLIER THIS YEAR. JUST RECENTLY, OPEN A. I. TRAINED A MODEL, 175 BILLION PARAMETERS, ON TO AZURE INFRASTRUCTURE SIMILAR TO WHAT WE WERE ABLE TO DO. LET ME SHOW YOU AN EXAMPLE OF WHAT'S POSSIBLE. THE FIRST IS PREFORM GENERATION. HERE, I START AND GIVE IT AN EXAMPLE INPUT LIKE HOW WAS THE UNIVERSE CREATED. THIS MODEL JUST STARTS TO GENERATE THE TEXT THAT COMES AFTERWARDS. SO YOU CAN SEE I GIVE IT THE PROMPT OF HOW IS THE UNIVERSE CREATED IT TALKS ABOUT THE BIG BANG THEORY, AND SCIENTIFIC FACTS. NOTICE THAT IT'S DRAMATICALLY CORRECT, FLUENT, THESE ARE THE THINGS ENGRAINED IN THE MODEL INCLUDING THE KNOWLEDGE THAT THE BIG BANG THEORY IS ONE OF THE MOST ACCEPTED THEORYS AROUND HOW THE UNIVERSE IS CREATED. THIS IS NOT JUST COPYING AND PASTING FROM THE WEB. THIS IS ALL KNOWLEDGE THAT'S ENGRAINED IN THE ALG MODEL FROM LEADING LOTS AND LOTS OF DOCUMENTS. FOR THE TRANSFORMATION WE HAD TO RETHINK THE ENTIRE A. I. STACK FROM THE INFRASTRUCTURE WHERE WE HAVE TO GET THE BEST A. I. ACCELERATORS, GPU, INTEL FPGA, BAND WIDTH, WE HAD TO GET REALLY HIGH BANDWIDTH NETWORK, NV LINK, SO THEY CAN TALK SUPER FAST. THEN SOFTWARE WHERE WE ACTUALLY INNOVATED AND BUILT SOFTWARE CALLED DEEP SPEED ALLOWS TO US TRAIN WAY BIGGER MODELS. MUCH FASTER THAN IF YOU DIDN'T HAVE IT AT ALL. AND THIS IS ALL ON TOP OF THE SAME INFRASTRUCTURE BECAUSE WHAT HAPPENS IS, THESE MODELS ARE SO BIG THEY CAN'T PIT ON A SINGLE GPU, YOU HAVE TO SPREAD THEM OUT AND THEN THE GPU HAS TO TALK TO EACH OTHER. DEEP SPEED DOES THAT. AND THEN WE HAVE HONEST RUN TIME WHICH CAN SPEED UP BOTH YOUR TRAINING AND YOUR INFERENCE. IF YOU HAVE THIS MODEL AND YOU WANT TO BRING TO IT PRODUCTION YOU CAN USE HONEST RUN TIME FOR INOFFENSE THE MODEL. AND THEN FINALLY, WE HAD ONE CENTRALIZED TEAM THAT CAN COORDINATE ACROSS THE RESEARCH GROUPS AND PRODUCT GROUPS. BUILD THE REALLY LARGE PRETRAIN MODELS AND THAT'S THE TOURING GROUP BUILDS THE TOURING NLP MODELS. THIS ALLOWED A LOT OF CUSTOM CAPABILITIES THROUGH REUSE. NO LONGER DID WE HAVE EACH OF THESE TEAMS WORKING INDEPENDENTLY. BUT ONE TEAM BUILD REALLY MASSIVE NLP MODEL, AND THEN EACH OF THESE GROUPS WOULD TAKE THAT MODEL AND FINE TUNE THEM. SO THEY CAN USE THE MODEL FOR THEIR BING SCENARIO, SHAREPOINT SCENARIOS, OR IT CAN USE IT FOR THE WORD SCENARIOS AND SO ON. AND THIS IS WHAT WE MEAN BY A.I. SKILL. FOR EVERYONE. WE HAD TO BUILD A. I. INFRASTRUCTURE AT SCALE WHICH INCLUDES MASSIVE -- THE MOST POWERFUL GPU THAT CAN RUN OVER TONS OF DATA WITH REALLY POWERFUL NETWORKING. WE BUILD REALLY LARGE A. I. MODELS AT SCALE, BUILDING BILLIONS OF PARAMETERS SUPER POWERFUL. THEN DEPLOY THE A. I. MODELS TO SCALE. WE NEED TO DEPLOY THEM ACROSS HUNDREDS OF MILLIONS OF CUSTOMERS, THEY NEED TO BE ROBUST, THEY NEED TO ACTUALLY RUN REALLY, REALLY FAST, AND THEY HAVE TO HAVE LOW COST AND HIGH THROUGH PUTT. LET ME SHOW YOU ANOTHER EXAMPLE OF WHAT THEY CAN DO. HERE I'M GOING TO SHOW YOU SUM ARIZATION. WE'VE GIVEN A PROMPT, THIS IS THE SUMMARY OF ROMEO AND JULIETTE. THE MODEL TAKES THAT TEXT AND SUMMARIZES IT IN A COUPLE OF SENTENCES. I'M GOING TO SHOW YOU ANOTHER EXAMPLE OF SUM ARIIZATION. WE GIVE IT THIS REALLY LONG PIECE OF TEXT, AND THEN THE MODEL IS ABLE TO SUMMARIZE IT IN JUST A FEW SENTENCES. FEEL FREE TO TRY SUMMARIZATION ON YOUR OWN. THE FIRST IS BING WHERE IT CREATES INTELLIGENCE ANSWERS THAT SUMMARIZE THE WEB. CAN YOU ASK A YES LIKE HOW MUCH IS THE BLOOD TEST FOR DOG ALLERGIES, AND WE READ BILLIONS OF DOCUMENTS IN REALTIME TO SUMMARIZE WHAT'S ON THE WEB FOR YOU. ANOTHER EXAMPLE WE HAVE IS SEMANTIC HIGHLIGHTING. CAN YOU SEARCH FOR DEFINE R AND N, NOT ONLY GIVING YOU DOCUMENTS WHICH ANSWER YOUR QUESTION, BUT WE'RE ACTUALLY ABLE TO HIGHLIGHT THE MOST IMPORTANT PIECE OF TEXT IN THE DOCUMENT. SO FOR EXAMPLE DEFINE R AND N, WE SHOW A TYPE OF ADVANCED NUSHL NETWORK THAT INVOLVES DIRECTED CYCLES AND MEMORY. THIS HAS BEEN AN AWESOME COLLABORATION ACROSS TEAMS IN MICROSOFT WHICH ALLOW FOR FASTER INNOVATION. WE HOPE IT DOES THE SAME FOR YOU. FOR THE LATEST ANNOUNCEMENTS LOOK AT THE LINKS BELOW. WE'RE EXCITED TO SEE WHAT YOU BUILT WITH THE TOURING LANGAGE MODELS AND A. I. SCALE. THANK YOU AND GOODBYE. NOW LET'S HEAR FROM OUR GOOD FRIENDS OVER THIS ONE OF THOSE THINGS THAT WARMS 