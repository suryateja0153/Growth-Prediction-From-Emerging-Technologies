 - Hello, everyone welcome back to the Stanford HAI Fall Conference on Triangulating Intelligence: Melding Neuroscience, Psychology, and AI. We had great talks and discussion this morning and are ready for our next lineup of super speakers for the afternoon. I'd like to remind everyone that you can submit questions for any of our speakers on the event page on our website, down that right column, where there's both opportunities for questions and for just chat comments. And on Twitter you can use the hashtag #Haineurohai H-A-I-N-E-U-R-O-H-A-I, but without further ado, I'd now like to introduce our first speaker Sanjeev Arora, professor of computer science at Princeton University. Welcome Sanjeev, please unmute yourself and turn your video on and we're really looking forward to your presentation. - Good afternoon, today's talk is titled How to Allow Deep Learning on Your Data, Without Revealing Your Data. But after some consultation with the organizers, also, they thought it would be useful to add some thoughts on mathematical understanding of deep learning, which will be the first few minutes of the talk. So, this introductory section concerns the ongoing effort towards mathematical understanding of deep learning and given the complexities of deep learning, you may well wonder if that is the oxymoron of the day. So in the next few slides, I'll try to give you a very brief taste of this growing area and give you three reasons or motivations, I think it will be essential in the long run. And then the rest of the talk, I will talk about how to do deep learning on your data without revealing your data. Just to lay all my cards at the table, I believe that at its best theory should surprise and challenge practitioners instead of just formalizing their entrenched intuitions, using some epsilons and deltas. So without further ado, the three motivations, why we need mathematical understanding of deep learning. Motivation one, to get very far in AI using deep learning we need better mathematical understanding of properties of the trained nets. And the main reason we lack such understanding right now is the current black box approach or viewpoint of deep learning. All you need to do deep learning is to settle upon an architecture, define a loss function and start doing gradient descent on this loss. But we lack any significant insight into happens during this gradient descent, which in turn means that we lack any insight into the final solution that this grade descent produces, namely the trained net. In other words, this trained net is a black box to us and this whole process is a black box to us. Now if you began to understand something about what gradient descent is doing, you might start getting some of the good stuff that we want, like explainability and intepratability of the nets behavior, and some properties that we can't just verify using held out data, such as robustness to distribution shift, or adversarial perturbations, something that some people call strong generalization. Generalization beyond the training data. And this theory is now starting up. To explain further how the black box view of deep learning is holding us back, let me give you an example. You've all seen language models like GPT-3, which have caused the revolution and natural language processing in recent years. The training loss of these consists of performing well on what you can think of is fill in the blank tasks. The model is given a partial sentence, like Rob went to the cafe and ordered a blank, and it is trained to produce a distribution on missing words that predicts the next word well, such as distribution, like probably of latte is 15%, probably of danish is 10% and probably of dog is 0.01%, which is very unlikely. So I'm assuming you've seen language models before and the phenomenon I'm interested in is the following, that the same model which is producing these distributions of next word is also along the way, giving us very powerful semantic representations of texts, which are helpful for all kinds of other NLP tasks and revolutionized NLP in recent years. So specifically when you input this piece of text into the model, its internal workings involve a vector representation of the text. And this vector representation is extremely useful as a representation of the text, in dozens of downstream tasks. Now to a practitioner, this doesn't seem very surprising because of course the language model is pretty darn good, and so it makes sense that it must have developed good understanding of the text. But at a mathematical level, this is not clear at all and for example one could wonder, why is their representation so broadly useful? Is it something to do with something special about language modeling? You know just predicting the next word. Or mathematically if it arises from some property of current, deep architecture is being used. And as you can imagine, one can make a whole lot of progress on such basic questions, given that deep learning is a black box. So now people are starting to open this black box and in a recent paper with two students, oops, Nikhil Saunshi, Sadhika Malladi, which is now in our archive. We give a mathematical exploration for why such representations from such models are useful in downstream tasks. It turns out the answer is that both of those are important. The fact that it's language modeling, and that it's also a property of a grid and descent and local optimality in there. Okay, so that was the first motivation for why we need mathematical enchanted, deep learning. The second motivation for mathematical understanding is that surprisingly enough, deep learning doesn't fit traditional optimization theory, meaning the stuff that we've been taught in courses for decades. Now there's basic optimization theory is something that both theorists and practitioners know, but it turns out that it can be quite misleading when you drill deep down into deep learning. And let me illustrate today by showing that something even as simple as learning rates in optimization behave very differently in today's deep learning than what we learned in optimization classes. So learning rate is this parameter ADO in gradient descent, how far do you step in the direction of negative gradient? That's the learning rate or the step size and the usual classes have taught us that you should start with high learning rate and slowly make it lower. But in a new paper last year, we show that actually this traditional understanding, it does not fit deep in today's state-of-the-art deep net models at all and by state-of-the-art, I mean those that use some form of normalization at layers. So what this paper with a grad student, John Li showed was that these state-of-the-art nets can be trained using exponentially increasing learning rate. In other words, at every iteration of gradient descent, you multiply learning rate by one plus C, for some constant C, bigger than zero. Now all of you know, what exponential increase means. So this indeed means, because it's being done at every iteration that very soon the learning rate is in the millions or trillions. It's gigantic, you're making gigantic steps, but it still works. And the reason we came up with this crazy idea was that we had a mathematical proof that it would work. We were exploring the mathematics of learning rates and that's what we came up with. We had a proof that existing training can be done with exponential learning rates and therefore the experiment was natural,` and it works. So after the surprise, we continued to explore learning rates and rule of learning rates, and in this new paper that we just put on Archive, we tackled another widely held belief. Which is that as you see in this figure, the learning rate, the belief is that learning rate needs to start out high and then it should slowly, should be decayed and that this is actually not just need for optimization, but also for good generalization. And the story vaguely is that the loss landscape is non-convex that has hills and valleys and this large initial learning rate leads to some kind of a Langevin diffusion process, which is fairly stochastic and it helps explore this loss landscape, the hills and valleys and finds a broad valley or minimum. So that's roughly the theory. And in this paper we show that this intuition is actually also incorrect for state-of-the-art nets. So you don't need large initial learners, so okay in the previous paper we show exponentially large learning rates are fine too. And now we're showing that actually you don't need large initial learning rate or a large learning rate at all. And it still a trains to good accuracy and we don't find any evidence of this diffusion theory. And in fact, we show that going forward what we should look at is function spaces. And we start developing a theory for that. Okay and now the third motivation for why we need a mathematical understanding of deep learning. And this is maybe a little bit speculative and I'm suggesting here that current learning paradigms may not be sufficient to lead to flexible interactive agents. Okay, so basically the issue is that when you drill deep down, most, if not all machine learning today is relying on the idea that you're learning on a fixed distribution of data. Training and test data points are assumed to be independent samples from the same distribution. So we all know that if the distribution then shifts at deployment time, then the learner is not guaranteed to work. Now, a flexible agent has to be able to do all sorts of different tasks, meaning perform well on all sorts of distributions. For which people have invented various paradigms, which I would simplistically describe as distributions on distributions and you'll see that a lot, I think, in Joshua Tenenbaum talk. So for example, in -learning, you explicitly think about your learning on a distribution of tasks. In Bayesian frameworks, you're assuming that the parameters of the task distributions are drawn from a prior distribution. So in other words, the paradigm of machine learning, needs inputs to come from a fixed distribution, which is the top turtle in this picture. and the top turtle may still be resting on another turtle, which is another distribution. Which in turn may possibly in turn rest on another turtle which is a distribution, et cetera. But at the bottom, the bottom turtle that's holding this all up is some base distribution where you're taking independent samples from a fixed distribution. So that's the dominant paradigm, I'm not saying everybody in the field is doing this. And it's possible that this can get us all the way to flexible interactive agents, but maybe not. And that's what I'm suggesting today. And it's quite possible, there is a middle position in the debate that maybe this paradigm doesn't capture intelligence when training data is limited as it is for humans, but maybe it does it for machines since it can learn from all this if you want more data. So I'm open about this, but I'm just suggesting that maybe we need a new paradigm. Okay, so that was a initial part of the talk, why we need mathematical understanding of deep learning. and now I will get to the main part of the talk, which is how to allow deep learning on your data without revealing your data. And this is joint work with this team, grad student Yansibo Huang, our colleague Danqi Chen, post Zhao Song, colleague Kai Li. And the first paper appeared at ICML. All the authors except Danqi, and the second paper will be appearing in EMNLP-Findings and includes Danqi, okay and I hope that this work illustrates what I said at the beginning of the talk. That where I hope theory can be useful is by being inspiring and thought provoking coming up with radically new solutions, rather than merely solidifying existing practice. Okay so today's broader topic is artificial intelligence, and it is clear that in the near future, it's not due to Androids or robots of movies, but the devices that surround us. And these devices that surround us are based upon a Faustian bargain, if you will, that we have to handle our data to a handful of dominant companies. And then we'll enjoy a world customized for you. Now, this raises a fundamental question about privacy, which I've taught, which was in the title, which sounds oxymoronic. How can deep learning be done on my data without revealing my data? But that is indeed the question we'll talk about. That there's all these forms of data that we are interested in. That's going to be input into the deep net and the training here, which is this little hardworking blue man is doing all this stuff, grading and backprops with the deep net. And we want deep learning to be done on our data without making us reveal the data. And some of the candid applications you can think of are, for example hospitals want to train deep nets on the pooled patient data, lots of hospitals. And privacy laws prevent them from actually pulling the patient data. So they want to somehow train the deep net without revealing the data. Or say, you want to customize the Gboard, the Google keyboard for user groups and the user groups have their own chats, private chats. And you want to customize Gboard on that, without revealing the chats to Google. Universities now want to customize the slack forums, using private conversations, et cetera. And of course you may want to hold onto your data instead of giving it to Alexa and Google and so on and Amazon, et cetera. Okay, so there's a lot of prior research and federated learning is a conceptual framework would think about it. For example, imagine that the hospitals we should train this deep net collectively using the data. And in federated learning, the deep net is trained at a central server, hospitals hold onto their data and actively participate in the training. Everybody maintains a copy of the deep net as it's being trained, and using the local data and that propagation, they compute a gradient of the loss and send only the gradient updates to the server. The server aggregates updates from all the hospitals, aggregates the gradients from all hospitals and sends back the new parameters of the net and the training and waltz is back and forth between the hospitals and the server. So it's just a framework for thinking about distributed deep learning. Now turns out, even though the hospitals may appear to be holding onto their data, and this framework, Information leakage is occurring from the gradients that they are transmitting about using their data. It was shown by June around last year that any eavesdropper on the protocol can use an efficient gradient matching attack to reconstruct the hospital's data from their gradients. So in other words, we need some explicit method in this picture to protect the data. There are two standard solutions existing before. The first is differential privacy, which adds carefully chosen noise to the gradients. But it shortcomings are that the accuracy of the trained net plummets by a lot. 20% on CIFAR10, and the accuracy drop is actually catastrophic on image net, more complicated datasets. The second shortcoming is that it gives a very restrict notion of privacy. It is concerned with privacy loss due to the release of the trained model that is proper use. And the reason has to do with the fact that differential privacy was invented in a different setting, namely privacy of databases. So differential privacy gives no guarantees about site computations using the shared gradients, which is what's going on in this gradient-matching attack. So the second solution that has existed for several decades is cryptographic. Just as cryptography can protect our online transactions, it can also protect every arithmetic operation in any computation, including the computations done in deep learning. But the shortcoming is that it does this encryption operation by operation and deep learning just has billions or trillions of operations and it's not very efficient, although there has been some progress in making it somewhat more efficient. But also the other shortcoming, is that it needs finite field arithmetic which doesn't work very well with the real value arithmetic. In deep learning and request special set ups, such as public key infrastructure. So it's currently not a very practical solution. So now we come to the new idea Instahide. It gives a way to encrypt images. So you take normal training images and test images, and you can encrypt them. Now you can input it into the same frameworks if you have the same code, Pytorch whatever Keras. And it can be used in the same training, same little blue eye, and you train and test on encrypted images, and doing this training on these encrypted images has very minor effect on final accuracy as we'll see. And very little effect on efficiency, unlike cryptography. So it's better than differential privacy that has minor effect on accuracy and much better than cryptography or existing cryptography, because there is almost no effect on efficiency. And we think it reveals nothing about data, but it's a new form of encryption and violating privacy here requires solving some computationally difficult problem this analogous to existing photography, but it's a new form of encryption. Okay so I'll give you some idea of what's inside this new encryption. It's inspired by a technique called mix-up. This is a form of data augmentation introduced by Zhang et al. a couple of years ago. And we've been trying, the theorists been trying to understand what's going on with Mixup because it's very counterintuitive. Let me describe what it does. Mixup inter, well the training images, the training is on images, but Mixup interprets them as vectors and the labels can also be treated as vectors. Now with vectors, you can take linear combinations and that's called Mixup. So you take 0.6 times first image and 0.4 times another training image. And you get this composite image, pixel wise edition. So you get this composite image and Mixup in wards training on these composite images. Now the labels themselves are vectors, so this is standard and deep learning that the label is the one hot vector. It has zeros everywhere, except one in the location, corresponding the label in the first case, but in the second case airplane. So the label of the Mixup image is 0.6 bird and 0.4 airplane. So you train on these Mixup images with these Mixup labels. And surprisingly enough, this improves training, deep net is never trained a normal images. It's only trained on the Mixup images and this improves training. And in theory, you've been trying to understand this because it's kind of bizarre that the nets which are so nonlinear, have been trained to behave linearly. Anyway, I won't say much more about why mix-up works, but we're just going to use it at Instahide. So one idea is that we'll do Mixup, but that's not turns out is not enough as we show in the paper to do, to ensure any kind of privacy. So what you also do is you mix it up with images from a big public dataset, like Imagenet or Instagram images or whatever. And those also, so they are these coefficients 0.6 0.4, et cetera. These are chosen from some simple distribution and you mix it all up. So in this case, four images, you get the composite image. And then on that composite image, you flip the pixel signs randomly. So pixels are in this case scale so that they are say between minus one and one, and you'll flip the sign randomly, and you get something like this. Remember that pixels are colored. And so when you flip the sign, you will really change the color a lot. So you basically get this greyish looking thing and that's encryption. Why the large public dataset, well, because it's off the shelf, no special setup assumptions. You can just take any images out there and large, because this gives more security. This is, formerly it's because an encryption key is a choice of images and there are lots of choices for this in the public data set, and that gives more security. So formally the general insight strategy is that you're going to take Mixup of two private training images as you're doing Mixup and then K minus two K is some parameter like six or four or eight, and K minus two public images from this public dataset followed by pixel wide random sign flip. And you're gonna train on these Mixup images, which look like blobs. So that's it. And you also do the same thing at inference time, and do the inference that way. And the conjecture underlying the security of this is that this's based upon intuition from the classic subset sum problem, that extracting significant information about private images from the encryption. This blob takes into the K minus two time, where N is the size of the public data set. So since N is pretty large, say millions or billions, this is fairly secure we think. and we put out a challenge data set, which you can try to attack. So here you can see that Instahide has fairly minor impact on accuracy. MNIST, CIFAR-10, CIFAR-100 and ImageNet are standard image datasets, and you see that competitive Vanilla training Instahide doesn't have a very dramatic effect on accuracy. Computers have differential privacy and the running time, isn't too different from Vanilla training. So there's just a forest Scott maybe it can be improved. Robustness to attacks, this scheme has been out for a few months and people have suggested various attacks. We came up with some attacks, we've run them. We can't seem to break them. But as I mentioned, there's a challenge dataset out there, which you can try your hands on. Now I'll talk briefly about text datasets and how to apply similar ideas to text. So there are several complications. Number one, as you all know, images can be thought of as real value vectors, but texts is a sequence of discrete symbols. The second big difference, which is why it took some thinking and it's a separate paper, to do it for texts, is that the current paradigm for machine learning on text is by fine tuning one of these language models that I mentioned earlier. So you take this language model like BERT, which is pre-trained on a large Corpus, and now to make it do some classification tasks, you take the embeddings produced by this language model, and you're going to input it into the classifier net. And you're gonna train this classifier net using the label data and this text embedding. And so now gradients are flowing back and forth and training is going on, but there's two types of training going on, the classifier net is being trained from scratch. And the top net, the language model has been fine tuned. So where do you stick the encryption in this picture? You stick it in between the language model and the classifier net. Okay so just in picture, going back to the hospital picture. So the BERT or the language model is sitting with the hospitals and they have the secret text and the classifier net is at the central server and the hospitals are going to send over the text embedding and the classifier net is going to send back the gradients, and the Instahide kind of ideas are going to be used. What we call Texthide to encrypt these text embeddings and the gradients that are being sent. Okay, it's similar to Instahide but analysis of security is completely different because it's been stuck in a different place in this pipeline. And it also has minor impact on accuracy on the standard benchmarks, as you can see on this. So I've concluded here, we've introduced these methods Insahide and Texthides which are substansive advances on important technological and societal problem. How to allow deep learning on my data without revealing my data and their potential applications to medicine, Alexa, Gboard, internet of things, self-driving cars, et cetera. You could easily imagine a sort of new kind of company built around privacy, which provides this kind of service and acts as an in between between us and say, Google and Amazon. All these methods are direct plugin with a few lines of code to existing frameworks on existing softwares, like PyTorch and Keras and so on, with minor effect on accuracy and an efficiency. And the power of these ideas may cast new light on other open problems on security, privacy, and robustness. Hopefully this fresh take will stimulate other works. Thank you very much. And I'm looking forward to more theoretical understanding of deep learning. - Thank you very much, Sanjeev. That was a wonderful talk. Let me now welcome Yejin Choi an Associate Professor at the university of Washington had a Senior Research Manager at the Allen Institute for Artificial Intelligence. Yejin Please unmute yourself and start on your presentation. - All right thanks everyone for having me here and joining the talk today. So I'm excited to share our recent adventure in cracking common sense Intelligence. So why common sense Intelligence when today's deep neural networks seem to solve everything starting from object recognition, machine translation and so forth. Looks as if we've done solving AI and we can all go home, or not. So the state-of-the-art models are not very robust when presented with adversarial examples or out of the domain distribution. Not only for computer vision applications, but for NLP tasks as well. So it looks as if right now we know how to solve a data set without really solving the underlying task. So why is that the case? Well, there's a fundamental difference between how humans understand the world and how machines mimic that understanding, only through pattern matching in datasets. So how do we bridge this gap? It's good to think about how human condition really work and then see what sort of major things we are missing out of that. So for that discussion, there's a system, one system, two reasoning comparisons became quite popular, thanks to "Thinking Fast and Slow" due to Daniel Kahneman the Nobel prize winner. But the downside of it is that it seems that there's a major misunderstanding going on in the field that perhaps we figured out how to do system one reasoning with the deep learning. And we only need to figure out how to do system two reasoning. I dare say that that's a major, major misunderstanding. So Kahneman's "All Your Work" actually talked about three cognitive systems, Perception, in addition to system one, system two and perception and intuition together are the sort of the processes that are fast associative and automatic. Whereas system two is the one that is slow and effortful. And then what's really interesting is that both the system one and system two require conceptual representations and require reasoning about past the present and future. And the both are important with respect to language. We cannot really do things that far without language. So we can think about what sort of applications correspond to these different cognitive systems. So for reasoning to system two, we often think about solving puzzles, or writing programs or solving logic theorems, but there's those reviewing or writing scientific papers will require system two reasoning as well, but hopefully reviewers that do involve system two reasoning when reviewing our papers. So what we seem to be doing really well right now seems to be more belonging to this perception that system like object recognition, image segmentation, and speech recognition and so forth. In between lies this something really quite interesting, which is all about intuitive inferences. Reasoning about preconditions, postconditions, what happens to before and after people's motivations in touch with their mental States. We do this almost a subconsciously every waking minute. So this is going to be the focus of my talk and to make my point a little bit clearer, let me show you this Roger Shepard's "Monsters In a Tunnel." What do you see here? It's not just two monsters in the tunnel that you see, you see stories, like two monsters are running as opposed to standing still on one foot. And one is chasing another as opposed to copying his body movement or the chaser is hostile. And the chaser is afraid, even though two faces are actually identical. We almost hallucinate all these stories, explanations, and imagination about what's going on in that particular image. So two important observations here, A, none of these inferences have to be absolutely true. The inference are statistic in nature. Everything's a divisible with additional context, more importantly a lot of these intuitive inferences are sort of common sense inferences and a great deal of that can be best to describe the natural language. And when you do that, we can not just do it with just words or a graph of the words, but we need the full scope of natural language. I already showed you three examples here that are in natural language, but let's look at another example. So this is a snapshot from the movie Titanic. And so what do you see here? Well, if we ask computer vision, state-of-the-art systems, you might get some set of objects that are correctly identified. But when you look at this, a set up objects, you don't really see what's going on. It doesn't really capture the gist of what's going on in this image. So then we might think, well, let's add more interesting words and then even created some relations between the two. And certainly we are heading to more informative direction, but still, when you look at this graph, do you really see what was in the original image? So that's where the necessity for a full sentence description comes in. But if we just do a state-of-the-art image captioning, again it doesn't really explain why this person is doing what he's doing. Is he having fun and trying to save statue in the water or trying to save himself from drowning? In addition, we almost to see dynamic context of this image, for example, what might have happened seconds ago or what might eventually happen down the road. So in order to address this sort of a reasoning about the dynamic context of a still image, we presented the Visual Comet at ECCV this year. And the gist of this given an image we want to be able to predict everything, all this octopus tentacles at once. And this is really the nature of a human common sense inference. So we on the theory based not only one image but for many others, and what it culminates to, is these connected graphs where you can see notes like a screaming for help, and then see in what sort of a visual situations you might imagine to hear that auditorium sound that you cannot actually hear from those still images. And then crashing the car. So this is what might look after crashing the car. And this is the moment perhaps before crashing a car without really looking at that particular moment of a crushing the car we can still reason about before and after. So we have this new resource, which includes 1.4 million common sense inferences describe the natural language, over 60,000 complex visual scenes. Whenever we have this sort of data, we can now throw in our favorite neural net architecture, I'm not going to go into any of this detail. Let me just highlight one particular qualitative example, what this sort of resource might enabled. So here a person one is putting up a letter on the table at an outdoor restaurant. So this person one, if you look closely, it's a serving person and what's his intent, well, he wants to serve other people, have them meet. And then afterwards he probably will get back to the kitchen to get more food. So by having this sort of a resource we can train you on how to work to mimic the sort of intuitive reasoning, or counter common sense of reasoning that we do and in doing so, it's real important to realize that we need the full scope of language, not just the words or graphs of the words. And if you buy that now, interesting correlary to that is the reasoning becomes generative task as opposed to discriminative task. What I mean by that, you could insist that I'm going to consider 1 million possible sentence descriptions of common sense inferences, and then try to take it as a discriminative task, evaluate all 1 million choices and then rank which one comes the best but that is certainly how we speak and reason in real life. We tend to almost generate token by token, word by word in real time. And I think this is really interesting aspects of the efficient reasoning capability that humans have, that we need to investigate more in current AI. So we have prior precursor works to visual Comet so Atomic and Comet, that I'm going to briefly describe. So Atomic was presented about last year, it's "An Atlas of Machine common sense for If-Then Reasoning. And so this is all about text and it concentrates a little bit more on social common senses. So given a center event like X repels Y's attack in the middle, what we can infer is like causes or preconditions of that event. So perhaps she wanted to protect either other people or herself and we don't know which is true for sure, but these are reasonable predictions about her intent compared to, you know, she wanted to sleep, that's completely unreasonable. We can also reason about effects or post conditions such as, as a result of she might feel angry. She might want to file a police report, her heart might race. He might fall back and then either he might want to run away or attack back. So there are different types of reasoning we can do for the dynamic state changes of the world. Now some of these inferences can be about static attributes. Like a personality being skilled, brave and strong that do not change minute by minute. So this is more static in nature. So we had a lot of this. And in order to build this graph, you know, you might wonder, can we just extract this knowledge from a lot of web data because there's ton of that? The problem with that is that there's a reporting biases. So people do not say obviously things and AI might just think that well, humans, murder each other four times more often than we in exhale, because we don't really say how often we do exhale. And so it's not going to be accurate enough. We decided to fully crowdsource the knowledge graph in order to see what happens. Before I tell you that super brief remarks on how this differs from what you might heard about. So for example, there's a Cyc, something called the Cyc, and then there may be other favorite resources that you are aware of. So there are two fundamental key differences that I want to emphasize. A lot of the prior resources focused on knowledge of what or taxonomic knowledge, that penguin is a bird, but that's really not enough. We need knowledge of how knowledge of why, which corresponds to more of an inferential knowledge. We do a lot of reasoning almost by memorize the pedal matching even as a human and why not AI shouldn't do that. So another major difference is these logical forms. So, so not all, but a lot of the prior resources were in this form. And when you look at this, even as a scientist does this make immediate sense, that it aligns with your common sense understanding about how the world works. It certainly does not speak to me right away. And so in our resources, we are going fully natural language. And this really aligns with what Sanjeev said earlier about how natural language modeling can be very beneficial for downstream applications. And as it turns out, that's exactly the case with common sense modeling as well. In any case, we focused on causes and effects in order to better address this concern against the deep learning. Even though we still like deep learning. So speaking of the deep learning, here comes to common sense transformers, which starts with this question. So atomic 900,000, if-then rules, common sense rules or Rule of Thumb about how the world works. Is this a really large enough? Or is it just in this tiny little corner in this universe of knowledge that you and I share? I speculate that the answer is likely to be the letter. So then what do we do? Do we try to somehow extract the more knowledge and store all of them explicitly? And my speculation is that we don't need to. So it's a better to build a model that could generalize for knowledge that's not even stored. For example, if I ask you whether elephants are bigger than butterflies, you don't have to store the knowledge apriori. You can just think about butterflies and elephants and then, and sorry on the flight. So that's what we want to do. In doing so we are going to benefit from transfer learning, from language to knowledge, by starting with the pretrial language models, and then continue train that your favorite transformers, on our atomic knowledge graph, after serializing Atomic knowledge graph into a long string of text. So now this transformer keeps reading common sense descriptions, and then here's an online demo you can play with, but it turns out this Comet generalize much better to out-of-domain examples, then we realized at the time of a publication. So here's some fun examples. Sanja rides into the sunset on a motorcycle after solving AI, atomic doesn't have anything about solving AI. Also Atomic doesn't have these people's names, Sanja. It's all about person X and person Y, but Comet doesn't mind. And it predicts seemingly reasonable predictions. Like maybe she was having fun. And then she might be seen as someone daring, brave, adventurous and so forth. How about Gary breaks the world record for most controversial Tweet? So atomic doesn't know anything about Tweet or controversial Tweet. Doesn't know which Gary we're talking about but it produces fairly reasonable inferences. Like maybe he wanted to be famous. Maybe he wrote a book, wrote a blog post, and might be seen as very famous and influential. Maybe you feel is very accomplished afterwards to celebrate, tell everyone about it. So Gary Marcus, as we know, has some concerns about deep learning. So this is one of my favorite Tweet messages that he wrote about how is GPT-2 is not up to his expectation. So he gave this prompt, "what happens when you stack kindling and logs in a fireplace and then drop some matches is that you typically start a" and then GPT-2 goes on to say some nonsense, but that's it because the GPT-2 is really language models t=so try Comet, which is knowledge model. And it turns out it just works right away. So Gary wanted to start the fire, which is pretty good. So let me now shares closing remarks and open research questions. So if you search common sense, the word common sense from ACL anthology. So this is a repository of all the NLP papers. Most of them are either from eighties or from the past a few years. So there's this curious void, significant void in between. And indeed, when I was thinking about studying common sense several years ago, many people in the field advised me, don't do it. If I want to be taken seriously, don't do it. Don't even speak the word as a matter of fact. But the more I think about it, the past failures are inconclusive when they were based on recomputing power, not much data, no crowdsourcing, no strong computational models, like no transformers available. And then there are conceptualization and representation choices who are not quite right. It was too much of a logic forecast whenever you play with the logical forms, there's a serious loss of information. So language is the way to go. Fast forward, it seems that the new generation doesn't know this past failures, so they're much more optimistic. So we held this first to common sense tutorial at ACL this year which to our big surprise was second-most popular. So it seems that there are a lot of interest for common sense or growing interested for common sense. But what is the path forward really? This is sort of like almost a moonshot research goal. And well, I personally don't know how to get there quite precisely just yet. But one thing for sure is that brute force larger networks with the deeper layers will not do it. By analogy you don't reach to the moon by making the tallest building in the world one inch taller at a time. So let's just step back and really think about the current paradigm of you know how neural network solves leader boards. So oftentimes it's, you give this lots of exam problems and then let it learn. But the problem there's a really big problem with that multiple big problems with that. One is that neural network inevitably latch on spurious biases in the data set, and pretends that it knows how to solve it without actually learning the fundamental concepts correctly. But let's really ask this question. Are we even able to do this? Like imaging taking a deep learning class, professor doesn't teach you anything, but just to provide you with a lot of multiple choice questions with the correct answers, and you're supposed to figure everything out on your own. I don't think even a human can do that. Alternatively, let's just say, the professor just provides a lot of deep learning code, so that it's a little bit more like self-supervised learning now. And, you know, I don't think humans can really learn efficiently that way either. So we like taking classes, reading textbooks, learning from tutorials very, very early on in our education. And I think we really need to think about how to teach machines AI with a declarative knowledge in a similar manner. The other question to think more about is this evaluation paradigm. Multiple choice is very attractive because it's easy to evaluate, but what you cannot, you know, as Richard Feynman said "What I cannot create, I do not understand." The one way to interpret that is that, well, unless you can generate the answer, you cannot really prove that you really truly understand. And it's also like assuming that there's some Oracle always provide us with like a four magical choices to choose from over which one of them is guaranteed to be true. That's just practically a not very useful assumption to make. So I'm beginning to think more and more that we need somehow a way to teach machines, some concepts in a more direct way. And what I just presented in this talk is a one way to get there. It's not the most perfect way of getting there, but Atomic is basically a textbook of common sense knowledge in declarative form, which contrasts with the observed knowledge, that's implicit in lots of raw text that language models are trained on and in doing so, we use language as the symbolic representation of knowledge instead of logic, because logic can state just so much. A lot of examples that I was using today are deliberately chosen to demonstrate how hard it would be to translate all of them down to consistent logical forms. We need to focus more on causal knowledge. And the neural networks are really great for generalization to competitional, and previously unseen events. I've kind of felt like Atomic and Comet together are so 2019 though, so we have this 2020 version coming up. Which I'm not going to go much into details other than to say that, well, it just has a lot more new relations, even including how money might be used not only for paying repairs, but also fold into origami, which is kind of cute, or you know what sort of events can be hindered by. So, you know, you cannot get your car repaired if the cost is too much, or the car is totaled so that it's just not amenable to repairment. So keep going on this, so that we have plenty more. But skipping over all this, let me just highlight one thing about, you know, this likely questions that people have in mind. doesn't GPT-3 by any chance to solve all this on its own. So let me give you a little bit of context to consider though. So our model Comet now it's all based on BART is much smaller than GPT-3. So it's about 400 times smaller, and GPT-3 is so big, so that it's not easy to download it and fine tune it to customize it for your own downstream anything. So with that in mind, GPT-3 is doing really great considering how it's unsupervised off-the-shelf with a few short examples, but still A, there's considerable gap, B, it's too large and whenever you can use a smaller one, why not, but this is for some of the simpler common sense knowledge that we evaluated on. But what about morality? Because we do worry about AI safety. So I give this prompts last night, running a blender at 5:00 AM is a rude because you can wake up the entire neighborhood. You can only do it if you're making a thick smoothie and need to incorporate some ice. Wow that sounds nice. I can do that, if I do it. So it's a little bit confused about the bed, but how about, it is okay to post the fake news if, and then GPT-3 says it's in the interest of the people. So I was thinking, I don't know about that. And then I tried to once more and then says, Ooh, it's even worse. And so don't say that to me. But so here are the takeaway message, by the way, we have a new papers, social chemistry one-on-one, for reasoning about social and moral norms to appear at EMNLP soon. And the paper will be, might be on archival, or will be archived within 24 hours or so. But basically the idea is that we need to teach AI with descriptive ethics in the form of declarative knowledge. It's extremely unlikely that brute force larger networks with some fence instruments, they self-supervised learning, will be able to discover all this magically. Another real important point is that social moral norms cannot be really, really done without the full scope language incorporation. Compared to descriptive ethics, there's another more prescriptive direction where you might just to focus on a few central dimensions of immortality, and then focus only on that. But if you do it that way, you cannot really translate that to natural language descriptions of a particular situation, like the way that I showed you here today. So we really do need to embrace language, which was sort of the recurring theme in this doc. Let me add to just one more thing on top of this, generally a reasoning angle, it's not just me. It turns out that there are cognitive scientists who are talking about similar things, so the enigma of reason is one of my favorite books that I discovered the past year. And in it, it talks about how intuitive inferences are a great deal about extracting new information from the information already available, like a common sense inferences. But here is a reasoning, system two reasoning are used to primarily not to guide oneself here guide means making an important decision, but it's used in order to justify my decision in the eyes of other people who will convince other people. So are the condition of these authors is that really reasoning serves the purpose of a communication language is really, really, really deeply involved in system two reasoning. And, you know, it's a similar to how, you know, I might have made this intuitive decision already that I'm going to work on common sense, but how do I now justify my decision? Well, now I need to really involve system two reasoning. And in doing so human reasoning is a mechanism of, system two reasoning builds on top of intuitive inferences. It's not a separate thing. They're really integrated which with each other and conventional mathematical logic plays at best a marginal role, which also repeats in this Phillip Johnson large book on "How We Reason." Okay I a stop here, I'm exactly on time, thank you. - Okay, thank you very much Yejin for that thought provoking talk, we'll see you soon for more discussion in the panel session. Our next speaker is Aude Oliver, MIT Director, at the MIT-IBM Watson AI Lab, and Co-Director of the MIT Quest for Intelligence. Aude you may now unmute yourself and share your presentation, thank you. - Good afternoon or good morning, everyone. Thank you very much for giving me the opportunity to speak today and attend the panel later. So while I was putting together this talk on incorporating insight from cognitive science into AI, I realized that since my freshman year at the university, which was over three decades ago, I have always combined I've always combined knowledge from different discipline at the time there was mathematics and philosophy, but particularly cognitive science, neuroscience algorithm. And it's something difficult for me to separate those domains where an ID come from, as I really tend to think of a task or solution really melting all those domain. And how to become a bio trilling role in different domain of science and technology is a training that I try to bring to my students. So I think it can give a richer and different view on the question of intelligence. So today I will present a few example of the work we are doing in my lab at MIT, trying to use good ID coming from cognitive science into AI model. So first I will describe some very concrete example of how we can leverage cognition for machine learning model. I will only speak about two or three, but if you open a textbook in cognition and neural science, you will find hundreds of useful concept. I still want to write a sort of integrative Human-AI cognition textbook, we'll see. And I will also share some neuroscience visualization about the human brain, which shows how the human brain is at work during seeing and hearing, which I think could really inspire the next generation of cognitive architecture. So one of the main project we have been developing in my lab at the core of cognitive AI is how do we build a system that can recognize the visual world? And you can see here a few of the milestone from seeing an activity recognition to common sense reasoning that actually are all inspired by a cognitive concept. I believe this milestone or needed step to bring a cognitive capabilities of the model to the next stage and have the model being an interface that people can understand and relate to and communicate with. So, which kind of cognitive concept did we concretely use in our study? So imagine the world, just looking around, they unfold in time and they unfold at different temperal scales. So the question we ask is, well, if we want a model that is able to understand and recognize what's going on, what is a meaningful moment, a meaningful event that the model could learn and capitalize on? How long is a meaningful event? And then how long should the video be to try and model that are going to be able to recognize what's going on. So we created the Moment in Time project at MIT that is led by Mathew Monfort , which is a large scale data set with a million snippet video of three seconds. And over 2 million LaBelle currently. And this one project started three, four years ago with a simple observation. Three seconds, this is just enough time to size up the scene to size up what's going on in an event. And three seconds is the short term toggle window that all the meaningful action between people, object, phenomenon, things that are happening in the world. It's also a window of time evolve as human face the physical and corporeal constraint takes time to move, pick up an object and so on. And in the experiment we did across a neural network, three seconds seems to be enough to learn very robust video representation in order to transfer the representation learning model of a task. So if you take a long random video, When you cut that long video and you cut that random segment of three seconds, you will have a very high probability that in that segment, there is a meaningful event and you show it to people and people will say, oh yeah, this is what's going on. So in the moment that I said all the cut were actually originally selected at random, and then we'll label them in crowdsourcing experiment, which gave a lot of diversity to the data set. So if I'm an action of an event is more than three seconds, well, three seconds is kind of enough to have the gist of it. So to train the model, and if an event is less than three seconds, then it's good because you have the event in context and you know little bit what's come before and after, which is going to be important for building model that will rely on causality at some point. - So here are some example of whether the model that were trained on those three seconds snippet for activity recognition, are able to do when they are faced with new video. And here is like the real output of the model in reddish are the zone of attention where our activity recognition model focus more in order to tell you what's going on and give you some of the words that you can see at the top left of the screen. So let's keep running this boating skiing. So you can access the work on our website or papers as well as download the data set. So this was very encouraging project, because if you have a backbone basic event recognition model that can give a high accuracy and costly localize what's going on, it means that, well there's basic recognition of the activity that is checked. And our idea was well, when this abilities develop, what can we do with it? What can we build on our own? And we actually build model that can perform abstraction by linking events that look very different. Like the one you saw here, the event of cutting the tree or separating the papers. They both belong to the class abstraction and on what I'm going to show you, we were able to reach that level of abstraction in our model, because we had a very strong backbone. So I do think that when you think to what's going to a high level goal, cognitive high system, it's certainly worked for us and our team to think into milestone and see when we achieve a milestone how it can be useful, the next step. So I speak about abstraction, but what does it mean abstraction for an AI model? So let's play a game. So here is an example of a game and we had our model and people playing the same game, which action best match the reference set? So if you look at the top floor, there are three videos of person sleeping and resting or doing some yoga. And the game is you're given another set of video and you have to choose which video best match the original set here represented under the blue. And when we gave this to people, most of people selected as you may see watching this, the little cute dogs. So now the example on the bottom left. So we have going upstairs or also climbing. So that is the match set. Game is given a set of every video here too, which one does fit the best, the same concept, abstract concept of the top one. And when we run this, most people selected the chimpanzee clinging. So now look, and on the right are those additional example. So the matches set here shows, there's some actually water that is falling down and on the other one is as well, a cartoon also falling down. And so which one of the two video in the bottom, does match the reference set. And if you look closely, it's actually well, it's a tricky questions, but most of people we gave the task to responded in this way. So not a trivial task but a game that, now model and people can play. So let's now have another game, which is very related. So now you have a set of video, three, four, or five. So let me play them. This one, this one, this one. And the game is which video or event does not fit to the two other ones. So which one is the other one? Well think a moment, look at those, a neural task, most people have selected this one as being the outlier. Let's see another example. So always water. Again some task, which event could be the odd one? Actually also difficult. Now, given this task, the higher responses that we got from people was to select this one as the odd one. So in all of the example I showed you here, we had people and our new model of abstraction playing the same game. And they choose the same image most of the time, about 75% of the cases. So what did we do to make the model playing pretty well at those two games? Well, we told the model, the notion of abstraction that we execute the operations here as what is the common concept that connect the set of video, a set of two, three or four video. So here's an example. What do those three videos have in common? Pick a moment, what can connect to them at a higher concept than abstraction will, a description of the activity. So people will easily understand that those actually depict the same some sort of competing. Human machine learning model struggle at inferring, this abstract connection of whom we are going we are seeing very fast progress almost every week or month, there's some nice breakthrough out there. So in the framework that we propose, this is a project that is led by Alex and at MIT, that was just presented at ECCV in August. The abstraction framework, combined the visual features with natural language supervision to generate a richer representation of the action video we've in common. And think about how you teach a young kids about animals or things going on, or concept, you actually speak to him or her all the time. So it does make sense that in order to build up for presentation, that are going more far far away the exact depiction of the object and the scene on the things into video, you speak to the person, you have that long range representation. And this is what we did in this work. So for instance, the model has learned that exercising can take many forms, running, lifting, and boxing. And its feature representation, mixed with language can be used then to play those games, those two new task, which is find the event that fits so set completion or find the one that doesn't fit. So what I'm showing here is actually still a time model. We actually produced two models, one based on momentum time dataset, and one based on kinetics for generalization. So there are multiple dimension that you could imagine people focusing on when they have to play those games to find what fits or doesn't fit. In this particular paper, we focus on activity. And we also told people when they were playing the game to focus really on the activity. So here actually a very interesting sort of the overall output is basically what was the type of information that the model of abstraction were using when it was taking the decision. So here are a few example of just the who abstraction output of the model. When we can also ask the model, well, if we give you those two videos or those three or four video, what is the abstraction that this comment does. and here it shows some of the responses of the model and some are kind of, they make a lot of sense and some are really interesting how you could make link with things that we might have not make links to and the output also gives you the sort of focus of attention, which is where the model can focus to produce a response regarding the type of activity. And what I think is a very interesting in model that can take decision at this more abstraction level. And we start seeing quite a bit in the literature is that it's a framework, in order to perform few short learning or even zero short learning, which is generalizing to new classes with a few or no example, as well as getting a little closer to human lag abstraction abilities, which has been a new experience into conceptual categories. So I just gave you an examples of a couple of project into cognitive principle, they are hundreds out there, and it will be a really fabulous journey, if all the cognitive, the computer scientist out there were starting reading the textbook in neuroscience and cognition because there's a lot of gold, literally hidden in every page. And there is more good that you can find when you start working on the neuroscience of the human brain. So let's now look at what we could learn from the human brain that could start inspiring, Two more of those models. Models that are more complex, which are architecture of cognition because they could start taking into account a lot of the perception and thoughts that we do when we recognize, or when we move in the world and so on. So one of the most important aspect of the brain besides being a deep learning network, it's a dynamic deep learning network. The information that comes to us in the world and it's processed into the brain is processing time. So top world sequences, things that happen or that are processed in parallel or one after the other is a very important element of human intelligence. And we do not know very well how to deal with timing and the right sequence of processing in artificial neural network in order to reach some more cognitive level of AI and task that unfold in time. And if we want to understand human intelligence and see how human and artificial system could interact better, I believe we need to really have a good understanding of how things unfold in perception and cognition, in the human brain. So capturing the temporal sequence of when perception is not easy, there's a human thought out there, but this is another line of research in my group at MIT. And then one or two decades, the slide will really look like the cover and age of cognitive neuroscience, but these are the tool available to a lot of the neuroscientists today. So in our experiment, we have people doing the same exact task and experiment twice in two machine. One in MEG, and then they go in fMRI or the other way around. So MEG or Magnetoencephalograph is a functional neuroimaging technique for mapping brain activity by recording magnetic fields produced by the electrical currents that naturally occur in the brain. When you look or hear things, and it's using very sensitive magnetic two meters. FMRI or functional magnetic resonance imaging, measures the brain activity by detecting the changing associated with blood flow. And these techniques relies on the fact that the cerebral blood flow and neural activation are related. So when our area of the brain is in use the blood flow to that regional also increases. So when this line of work initiated with, we combine the data from those two very different sensor, using different machine learning techniques. And now let me show you an example of the brain responses. Two images shown for only half a second. So here is what half a second feels like. You see a lot, but it's kind of fast. All right, so here is our brain model and you can see the timer. So when I stopped the movie, it's going to go fast. At some point, you're going to see some of the region lining up with their font colors. And this is what we recorded, we put here together about 15 people, 15 grand to show you this. When we were showing a series of images for half a second each, as you saw in the little demo, all right. So we have this started and we can start seeing some responses that stop coming around. You see 70 milliseconds over here and the start in the back of the brain, that's normal, we are showing images. There's no sound in the scanner and it shows that while it's perfectly expected, information goes into the occipital and without seeing some responses. So that sort of study that was published initially in 2014, actually we did replicate a lot of the work. And since more, I've been with replicated, recording things we absolutely know about visual processing. Oops, which is that as we see and time unfolds, information is envisioned. And then it flows on both sides and on the temporal side of the brain and then so on and so on. So I show you here a large visualization, but what the neuroscientists do is that they go and dig into the responses of every single brain region. It's a map. We have a lot of modules and we have a good idea of what each brain module does. So the one we will see the one that will be used for imagination and so on. It's really a journey to the result, but it's a way to track down what's going on. And it could really inspire a new architecture when we show all the type of activities. So in a similar vein we also ran those neuro-science experiments with sound, and we put people into the two scanner and we had them listening to snippets of sound for half a second. So sound were represented, voices, animal sound, object like the telephone, `toys, cars and environmental sounds like wave, storm and so on. So now let me show you what we observed. There's a lot to see here. And there is a few really nice new nuggets of information. So now it's sound. So right here our ear. So normally all noise, expected it start on the left and the right in the temporal lobe. So this means here that the information regarding the sound has been perceived. And then many of our brain regions are going to start responding. And I'm going to stop here and you see on the Y representation of the brain you have in the front this region, this sort of isolated region. And this is actually a region that is more specific to process voices and animal sounds. So when we go through those results, we see that within 150 milliseconds there are many areas that are processing sound, and then those are the area processing sound, start really spreading to the area that are more visual area processing sounds. So what does sort of visualization show you is that actually processing sound from the brain when you compare it to visions is much more complex. There's a lot of regions that are being engaged either in a sequence or parallel and manner. And we really are in the process of segmenting all this in order to have some more clear guidelines about how sounds are processed in the human brain for some future architecture of cognition. And I'll show you here, vision and addition, that you can play the same game with tactile information or memory and so on. So ill show you a new example of this incredible engine that's the human brain when it process sensory input. But it's also important to characterize the bandwidth and the boundaries of human perception and cognition. Let me show you what I mean. So let's say you're in the streets, you're just waiting a few seconds to cross the scene. You're not looking particularly anywhere, maybe you have been lost in your thoughts. Oh well did you notice anything? Let me show you. So here's basically what happened in front of your eyes. All those elements were changed literally in front of your eyes and cognitive science has developed method to quantify what we can see and not see, here and not here. To characterize our perceptual bandwidth, the boundaries on how flexibilities. And I think that many of the concepts of the cognitive bandwidth will be very informative to have systems that interact with people. Here is again the demo, if you just missed it. Yep, you did miss a lot. So to conclude, biological and artificial neural networks, they are like, to me, the two faces of the same coin. Both biological and artificial can differ a lot in their architecture, their secretary, their learning rule, the objective functional task, but they are both systems that use iterative optimization process to pursue an objective, given a lot of data. And there are many common ground and principle between them. So I think of characterizing the bandwidth of human perception and cognition is very critical. And when you start digging into thinking about intelligence system, you're going to need to go and look at what's going on in the system. And that may unveil a new field of investigation, cognitive, clinical, social, computational, experimentalists it's the sort of sensitive neural scientist that's going to really dig on and understand what's going on in the model. So as we see more interactive system, it's going to be important to study the implementation that works best for performing specific task will help to explore the alternative that have not been taken by a biological system. And most importantly, I think that in all the system we are building at the heart of this endeavour, we need to integrate decision about ethics, social responsibility, and the risk of AI into our pipeline, thank you. - Thank you very much Aude for that wide scope from neurons to large scale activity recognition. Now I'd like to welcome the final speaker of the session. Josh Tenenbaum is professor of I can't say it. Josh Tenenbaum is professor of Computational Cognitive Science at MIT. Josh you're already up see, so please start your presentation. - Okay, great, thanks. It's a great honor to be here, to talk to you about scaling AI, the human way. I should say that, you know, like many of the other speakers here I've been working on of this triangle and the bi-directional interactions between the science and the engineering of AI for my entire career. It's what I got excited about. Originally it's what got me into the field. I'm gonna tell you about the work that we're doing here at MIT in the Brain and Cognitive Science Department, as well as Csail and in the Center for Brains Minds and Machines and the quest for intelligence under MIT. But part of why I feel like it's such a great honor to be here as I really got my start in this field as a visiting undergraduate at Stanford in the summer between my second and third years as an undergraduate where I worked with Roger Shepherd and David Brunohurt, two people who have both been mentioned here before. And that's really where I got my basic idea about how to think about the mind, why it was so exciting and interesting, the connections between mind and brain and how computation can enable those connections. Like many of the other speakers here, my work is fundamentally motivated by asking why do we have all these great AI technologies, but no real AI. So why do we have machines that do things we used to think only humans could do, but nothing that has the flexible general purpose intelligence that you use to do each and every one of these things for yourself, what's the gap, right? And again, like I think especially some of the discussions that came up in the early session and some of the things that Yejin was saying, especially to resonate with this. I think part of the gap is that, we've made a lot of progress in AI technologies, using techniques from machine learning, deep learning and reinforcement learning that originally came from, just as Matt gave a beautiful history, right? From the psychology basically of animal learning. So studying very basic equations of like Pavlov's dogs, reinforcement learning in animals or rats in a maze, right? So these, I think we all agree that these can become powerful tools for finding associations and recognizing patterns when you scale them up in big datasets, but it's just one part of intelligence and maybe just one small step towards real understanding. So I'm interested in taking the next step, which comes more from a human driven point of view to say well, in human intelligence, we don't just find patterns, but we build models. We are about understanding and explaining what we see. We are about imagining things that we could see maybe, but we haven't yet, maybe that nobody has. And then planning actions and solving problems, then come up when you want to make those things real. And then learning as building new models of the world, as we experience both our successes and our failures. So to reverse engineer this, to write down essentially the equations of model building, we're starting with young children because they are the original model builders. And again, like many of the other speakers were inspired by could we build human like AI that starts with just the basic kind of common sense that you see in these one and a half year old children. We're far from that. But imagine if we could do that, and imagine maybe longer term, if we could build intelligence that grows all the way into adult human intelligence, starting from the way babies started, right? This might not be the only route to human level AI, but many of us think it could be the best bet. Because if you think about it in our known universe, it's the only example we have, of a system that reliably, robustly grows into human level intelligence, starting from much Loveless. And we know that even small steps towards that for as this, could be big. So we're starting with the kind of most basic common sense that you can see in every one and a half year old, but no AI yet, like the intuitive physics that you see in this kid here stacking up cups or playing with blocks. What is the knowledge that lets him say, make a stack of two cups there and put it on top of a stack of three cups to make a stack of five cups. That is knowledge of objects, but also multi-object multistep plans. Or the intuitive psychology that you see here in the famous experiments of Felix Wameken and Michael Tomasello. The understanding that let's that one and a half year old kid in the back there, literally read someone's mind. See this adult doing something that he's never seen before, but figure out what he's doing and why, and even how to help him. And you can see that here when the adult pauses and the kid does, well exactly what you see them doing. It's an incredibly sweet and cute moment. And it's also remarkably intelligent. Just think if we could build robots with this kind of common sense and helpfulness around the house, it would be amazing. Now we're far from this. And I think in order to engineer this kind of abilities in AI, and to understand in engineering terms what's going on in these kids' heads, we have to face some hard problems of common sense. And here, you know, it's going to resonate again with especially some of the things you just saw from Yejin, but this is partly my way of introducing what I mean by common sense okay? I also wanna contrast our approach with the approach you saw, especially in the morning from Matt, Daniel and Chelsea, I should say, you know, I'm a big fan of all of them, and I'm grateful to have collaborated with all of them actually on papers, especially with Dan in some recent work that I'll mention here okay? But I think, you know, in each of their talks, you saw fundamentally a sort of bottom up learning driven approach, which really takes as the core driver of intelligence. Of the tools of deep learning and reinforcement learning and add some other stuff to it, maybe okay? But I think these hard problems of common sense, just show us what some of the limitations of a purely bottom-up learning approach might be. Some of the ways in which though we're making great progress there we might be more, you know, climbing trees or ladders and so on rather than getting to the moon. Not that those aren't really interesting trees and ladders. And not that those, not that these techniques are also, as I belief, part of the bigger picture, I think very much they are. But think about these, these problems of generalization, like as Chelsea put it, you know, we want strong generalization, We might want to build systems that get broad training data so that they can generalize just as broadly. But the problems of generalization, aren't just the strong generalization as Chelsea was calling it, or Sanjeev was talking about, where we have like say distribution shift. But what I might call super-strong generalization. So generalizing two environments that are vastly different from training with essentially no retraining or fine tuning. Or generalizing to an infinite range of novel tasks with essentially no retraining or fine tuning or examples of somebody doing the same task, because maybe you just invented the task yourself. So just to illustrate one of these strong problems, super-strong problems with generalization think about this scene understanding task. Understanding what's going on in a basketball game. So here's a typical image of a typical basketball game from the viewpoint that somebody who doesn't play basketball, but might be watching from the side might see, can you see some images like this or some scenes, and maybe some people tell you some things and you understand what's going on. The objects, the balls, the players, the hoop, the goals and so on. The teams who's home, who's helping, who's trying to hinder or block others, okay? But whatever understanding you get about these events from this viewpoint, generalizes to this viewpoint, which you know, until you saw a camera put at the rim of basketball hoop like this, you never would've seen this activity here, but you have no problem generalizing to this, even though it's very far from the distribution. But this is actually still an easy problem of generalization. Because think about all these other kinds of ways of rendering the scene. And for example, in a video game. So here's a sports video game where it's fairly realistic graphics, but different at all sorts of low level pixels statistics, as well as some, not so low level things like that circular hoop you see on the player being controlled, which is just something that comes up in a video game, but you have no trouble sort of factoring that out. Or take this video game here with less realistic graphics and much less realistic physics. You see somebody making a slam dunk going much higher than any human basketball player ever jumped. But you have... In some sense, the physics of all of your experience, or think about kinds of things, which again are much more different at the pixel level. And this, again, it resonates to me with what Matt was talking about, where in some sense, common sense, isn't just about pixels. So take Minecraft, where you can see a basketball game, but the pixels are so different. And indeed, even the objects are quite different in their composition, but it's the same thing going on. Or take this really old classic Atari game of one-on-one basketball. So different, right? And yet you have no trouble understanding it. Or lastly, the really classic Atari game. This is the Atari 2,600 basketball where, you know it's ridiculous graphics by today's standard. And yet you still have no trouble and kids can play it just as well. And the first time you see it, you understand exactly what's going on, basketball, hoops, players, ball shooting, in some sense, these are all the same things, the same activities, events, objects, agents, places. So I wanna understand what is the common sense that allows me to perceive and make sense of this. Okay now I, you know, again, I think this poses a severe challenge to a data driven learning approach. And this is a challenge that has been highlighted since the beginning of the field, right? I mean Turing in his first paper. I mean, arguably the first paper on AI, the paper that introduced the Turing test and which also famously introduced the idea of building in a machine that grows into intelligence like a child or that's taught like a child emphasize this idea of basically starting with as little as you can build in. Little mechanism, little knowledge, lots of blank sheets. But we've learned now, and this is where this is at least in the work that I do. A lot of the inspiration comes from developmental psychologists and developmental cognitive neuroscientists. Like the people that you see here, Liz Spelke, Rebecca Sachs, Alison Gopnik, Laura Schultz. Hu Wang who was a student of Laura's also worked with me at MIT is now a recently tenured faculty member here at Stanford and one of the participants in the hire center. And, you know, if you look at them and many others in the field, they've shown us all these ways that the starting state and the learning procedures, are much more sophisticated in their structure, their content, their mechanism than we might've thought. There are basic core knowledge systems as Fauci has shown for objects, agents and places. Rebecca and others have shown how the brain systems that underlie these are largely on taking the same shape, even in very young babies babies just a few months old. Where Alison, Laura, Hu have shown how the learning algorithms we heard about the child of scientists in the morning right? And I think some of the work that Dan told you about is starting to take steps towards this, but the ways children build models, do experiments, form theories, get into an experiment a theory building loop like a scientist, go way beyond what we've been able to do so far in sort of traditional machine learning driven AI. So I wanna talk about the ideas, the formalisms that we've been trying to develop to capture these ideas, like what we call the game engine in the head, or the child is coder or the child is hacker, okay? Now we do this in our work in the context of several technical tools. One is the idea of probabilistic programs and probabilistic programming languages. These are new AI programming languages, which integrate the best ideas of multiple areas of the field. You know, again, I, Surya gave a really nice history of the field. Which talked about basing and approaches as well as neural networks, but I think both of those are really important in the kind of work that we do, basing inference for reasoning about unobserved causes from sparse from certain data and hierarchical probabilistic models, and Sanjeev formulated that very nicely, as well as, and also how that relates to the neural network and having recognition function approximation toolkit. These are complimentary and work nicely together. But I think what also gets a little bit of short shrift is the original and maybe best idea in AI, which is symbolic languages for abstract knowledge, representation, and reasoning. I agree with you, Jen about almost everything (chuckles) except maybe I think that there's more value in actually having underlying knowledge representations that use symbolic languages like programming languages or abstract knowledge, representation, and reasoning. And I think by combining symbols, probabilities and neurons effectively, or differentiable programs, then that's a very powerful tool kit together. Another idea is this slogan that we, or the slogan I use for it is the game engine in your head. And they had the idea that the very fast approximate simulation programs for graphics, physics and planning that are at the heart of modern gaming, and that have been increasingly used to train certain AI systems, especially in the SIM to real paradigm. I think it can be extremely powerful if you think about those as prototypes or examples of the kind of cognitive architecture, that the kind of common sense core knowledge systems that would be built into our heads, into baby's heads by evolution. So the difference is, this isn't about some surreal approach. It's about a child who sees an object. See some objects, thinks about a plan, thinks about a goal. Like maybe I can roll this ball and knock those things over and actually has the simulator in their head, which they use to perceive, think about, predict, and make their plans, right? So that's why it's not the game engine as a training ground. The real world is the training ground but the game engine is in your head. It's the basis of the models. So over the last 10 years or so, we've used these tools to capture a number of aspects of basic common sense in quantitative terms. So with Pete, Natalia and Jess Hamrick, who are now part of Matt's team are core members of Matt's team at the mind, and continuing in all sorts of interesting ways that are related to this, we've used these tools to build models of intuitive physics. For example, in blocks world scenarios, to answer questions like, will this stack of blocks fall and to capture here, quantitatively people's judgments about how stable a stack of blocks are. But the same tools can answer many different questions without any retraining or really any training at all, okay? In this system so far, there was no learning. There's just probabilistic inference, running a few approximate simulations in a game style physics engine. But that can let you answer questions like which way will the blocks fall? How far will they fall? What will happen? How it would be different? If one material is much heavier than another, working backwards, if something surprisingly isn't falling, can you infer just a heavier material? Or to do interesting kinds of counterfactual reasoning, right? In some sense, this connects to some of the themes that were raised when we're talking about Judea Pearl's work. But now the causal model, isn't a graph. It's a physics engine program, which is necessary to reason about flexible open-ended world of many objects and complex interactions. There's no single graph that describes how blocks interact. But if I want to answer a question like what will happen if I knock one of these tables hard enough to knock some of the blocks onto the floor? Will it be more red blocks or yellow blocks? Well across these scenarios and an infinite range of others, we can capture people's quantitative graded intuitions of more or less red or yellow by the following sort of mechanism. We infer a 3D scene description an object based scene description and run a simulation. That's a simulation of a small bump. Here's a simulation of a big bump. And we just have to run a few simulations, one or a few simulations. You don't have to run a high precision simulation, and you only have to run it for a few time steps to be able to capture people's intuitions on the grain of a kind of quantitative precision and probabilistic approximation that you see in the behavioral data. In some particularly recent work, Kelsey Allen and Kevin Smith and our group. And I should say, Kelsey is also about to join Matt's group under Jess Hamrick at Deepmind, in a few months just finishing up her PhD and a core part of that is this virtual tools game, which you can see here. I'm just illustrating a few people playing this game where the idea is have you have a goal, which is to get the red object into the green region. And you try to achieve that goal flexibly by picking up an object and dropping it somewhere into the sea. And here you can see a few people playing different versions of this game, okay? You don't have to learn to play this game. You use the physics knowledge that you have of the world, and then you can play any one of these levels, not perfectly but across many levels these are just a few of the ones we've studied, and here are some of the within level learning curves. People typically take two, three, four, maybe five, six or seven trials. It's a few short, if you like reinforcement learning, but people can learn to solve one of these levels on average and just a few trials and a simulation based few shot re-model based reinforcement learner using the approximate physics engine that we've talked about, can capture pretty well what people do here. The same kind of models can capture and provide actually some of the first quantitative models of intuitive physics in babies. So baby shown scenes like these ones over here which again it's, this isn't really like anything that the typical 12-month-old has seen, but seeing just a little bit of dynamics of a few objects bouncing around in one of these gumball machines, and then with an occlusion of either a very short period but like on the pure timescales that Aude was talking about in the moments dataset zero one or two seconds. In this experiment, you vary the number of objects at different colors, how close the objects are to the exit at different points as they're moving around randomly and the duration of occlusion, and those have predictable effects on how likely an object of one color or another is to come out of the hole after reclusion and infants look longer integrated way when something is more surprising or lower probability, according to our model. So this probabilistic physics simulation engine was really the first model that we had that could quantitatively project infant surprise okay? Bringing things back to the brain over the last couple of years together with Nancy and several great researchers in her lab, Jason Fisher, who's now at Johns Hopkins and Sarah Swetman, who's finishing up her PhD have identified regions in the human brain that seem to underlie these kinds of judgements, including the predictions that we talked about. But also for example, in variant representations of mass, this is a paper that recently came out in "eLife" where you can, for example, cross decode across all these different ways of perceiving, whether an object is heavier or lighter based on its physical dynamics. It seems that this part of the brain is inferring and encoding that, and it's also an underlies the kinds of predictive judgments that we saw here, okay? These brain networks in premotor and parietal areas, it's not like nobody's ever studied them before. They've been previously identified and studied in the context of action planning and tool use. I hope you can see here that that all makes sense now in a sense that I can tell evolutionary just so stories and I'd love to try to study these in a serious way, but the idea that brain systems that originally developed probably to move our own bodies around and then to use other objects as tools and figure out how to use them to move other things around, they now can be taken offline and used in this very, very flexible way. I'd really love to understand at a neural circuit level, how these systems work and to do this, I've been working with a number of researchers, including especially people in Dan Gaiman's his lab. So here, these are just figure summarizing two of the projects that Dan talked about, Damian's work on these hierarchical relation networks for particle based physics and Dan Bear and Chaofei's work on these learned physical scene graphics plantations. And though we don't yet have direct evidence that the brain works this way, these are at least somewhat plausible models, especially when you confront the challenges that Dan was talking about. Like the fact that graph networks are not exactly like neural networks, but you have to have some way of rendering them into a fixed finite sized neural population. But this is the research program. And what I'm showing you is the top down perspective on what I think is you saw as the bottom up perspective before. So the optimist in me is that these perspectives can converge. Though we don't have evidence for those kinds of models yet as being in the brain. You know, we've identified where to look with fMRI. And the next step now is to see if those models actually can describe the encodings there, for example, and then it may be even the neurocircuitry. But one example of this kind of approach, work that Yoka Yildirim recently published in science advances is just another idea, basically using a neural network, a deep neural network, that's been trained to invert, not a physics program, but a graphics program. And he showed that when you apply this to the well known face patch circuitry, that Dora Tsao and Benbrook Frewald have studied in macaques and in further temporal cortex, this kind of neural inverse graphics program provides really a strikingly good account of for example, the patterns in similarity of representation of faces in different viewpoints and different identities. And here's the efficient infographic network of IL Durham. And it does much better job of accounting for multiple stages of representation in the primate brain then say VGG face, which is one of the, you know, standard state-of-the-art open neural, deep neural face models that was trained not to invert a graphics model, but to do classification. So in some sense, this also gives a different interpretation of the function of the visual system. Something potentially more generally like inverse graphics. Intuitive physics has been used in, and recently, these are just several collaborations we've worked on to try to put intuitive physics into robots. Some of these systems do various kinds of learning. So for example, Yunju Li's system in the lower left, that's kind of making the sushi rice uses a very similar kind of hierarchical relational graph system to the work from Dan Yemen's his lab, but then uses it for model predictive control. But the systems, for example, that Mark Toussaint built that looked at that like humans, and even like some other animals, smart animals like crows can learn to use tools that they've never really encountered before in flexible ways, even to use, for example, a tool to get another tool, sorry. Or Neema Fazeli's work with Georgian Wu who's recently joined Stanford as a grade assistant professor in computer science, building a system that can learn to play Jenga or Yang Minku's work on using soft body physics engines to learn to walk. There's relatively little learning here, or very, very simple focused model based learning. And, you know, again, I think this is represents the more sort of top down structure driven approach to how we might use intuitive physics in robots that can flexibly engage with a range of tasks that are not much like the experience that have come before. I just wanna leave you with two highlights of where I think the future is. So one is I think to really make this approach, you know, go to the moon, it's gonna need some rocket boosters. And for this, I've been collaborating with my longtime colleague, the Kashmen Singa who runs MIT's probabilistic computing project, especially the work of Marco Cusumano Turner, who recently got his PhD there based on the system Gen, which is one of the newest state-of-the-art probabilistic programming languages. I really think of it as a AI programming platform that really nicely integrates probabilistic, symbolic, and differentiable, or if you like neural network modeling and inference. This is an open source platform. I won't go into the details, but it's Julia based. It's also sits on top of PyTorch, and it really gives us ways to build flexible, scalable, very efficient, real time generative models and range of specialized inference programs that can let you do for example, you know, take the ideas of common sense object systems, but also agents systems and places systems. These are sort of the three classic spell key core knowledge systems and take game engine inspired, symbolic geometric representations of scenes, but now across, across very different scales, ranging from, you know, abstract symbolic scales to fine scale 3D structures and many things in between and do real time probabilistic inference over these. I think that's, what's gonna allow us, for example, to really make good on this as a approach to scalable AI. I'll skip because I'm just about out of time how we're using this in the context of action understanding, but we can come back to this in the discussion I've mostly focused here on intuitive physics, but understanding, for example, you know, what makes a scene like this? How do you understand what somebody is reaching for? And that somebody else is trying to help them versus a scene like this, where, you know, you understand that what somebody is doing is the opposite of helping, right? We think that these, well we already have preliminary examples of these kinds of probabilistic programming systems being used in a multi-agent context with generative action programs and agent models, where the agent can basically try to maximize its expected utility based on its expectation of another agent's expected utility. That's a simple version of helping, where you make it a negative dependence, and then it becomes hindering. This is a way to try to do the kind of action agent-based scene understanding that we saw, for example, in the Warnick and a trauma center base. - The last direction, I just wanna talk about is learning, which is if common sense is based on these kinds of simulation programs, then learning common sense has to be something like a program learning program. So we think of this as, you know, what I sometimes call the hard problem with learning, because unlike learning in a neural network, it's not just about, there's not a smooth air landscape that you can do gradient descent on. The space of programs is much more complex. The search problem is much harder yet somehow children solve it. Starting a few years ago in our work in our group, we've been working on something that we call Bayesian program learning. So for example, in the thesis of Brendan Lake, he developed together with Russell Kadinov this Omni Glock dataset, which for us is, you know, for many people have used it as a kind of one shot or few shot concept learning domain, where if you look at all these characters in many different alphabets, each one represents a visual concept. A simple one that you can learn from one example. But the way we approach it is as a simple warmup problem for program learning, because every one of these concepts can be described by a generative probabilistic program. Something like a drawing program or an action program for how you put ink on the page in terms of strokes and sub strokes. And that makes a template for how you would draw our new characters and then perceiving or learning a character from one example is a kind of Bayesian inference. Running that program backwards to infer the evaluation trace that led to one example, and then you can sort of rerun it to produce render, imagine, and classifying new instances. And this idea of Bayesian program learning is one that we've been applying in a number of different settings going beyond handwritten characters, like three D shape programs or programs that can interpret images, or even kinds of reinforcement learning, learning simple Atari-like video games. This is from the thesis work of Pedro Sunita's, where he's shown that the idea of Bayesian program learning, and then using that in a sort of a model based planning approach can learn to play new Atari-like games very, very quickly, you know, hundreds of times faster than even the best deep reinforcement learning systems and almost as fast as a person. So where this leaves us though, is still with much harder problems of essentially program SymSis. For me I think the rocket boosters that we need for learning programs is best encapsulated in this phrase, the child as hacker, or the child as coder. Hacker as you know, around MIT, we're thinking of the good kind of hackers, the ones who just like spend all their days, making their code more awesome. But you could just think of any really good creative software engineer whose job is to produce code. What are all the activities of coding, all the ways you make your code better on a daily basis? So tuning parameters is one of them in existing code, right? And that's kind of the coding version of what we do when we were running stochastic gradient descent or backprop on a fixed neural network. But there's many other activities which are structurally modifying the code, adding functions, extracting functions, debugging, finding errors, profiling, refactoring code to make it more generalizable, maybe more efficient, more elegant, adding new types, writing new libraries, writing holding languages. And we think all these activities of coding have analogies in children's learning. And so if we want childlike learning algorithms, then we need coding algorithms or automatic program SymSis algorithms that do these kinds of things. Now we're very far from doing that and this child as hacker ticks paper just kind of lays out, sketches out the idea and makes the proposal. It's sort of a white paper. I'll just leave you with one example of another recent PhD thesis from our group. This is the work of Kevin Ellis who just graduated this year and he's gonna be starting next year as assistant professor at Cornell. In the meantime, he's working at a startup called Common Sense Machines, which came out of our group also, and is trying to explore some of these ideas in some AI sort of model building contexts, that's a story for another time. But what Kevin did in his thesis work is built the system among other things called DreamCoder, which is you can think of it as a system that doesn't just learn to write programs, but it learns simple programming languages to capture a domain of learning. It does this with a kind of wake-sleep learning in what is often these days called a neuro-symbolic framework. So the basic things that at the heart of what it learns is libraries of coding primitives that then let it write new programs more quickly. But it also learns a neural network to guide the search. And it jointly bootstraps through two different kinds of sleep learning phases, the symbolic program library, as well as neural networks, which represent a kind of procedural expertise that help to guide the search for new programs. And just to illustrate this on some kind of, you know, more graphical examples, there's many different applications of this system, but like, let's say we wanna learn to make various objects out of blocks or learn drawing programs. So for example, we can start off with a very simple, minimal language for drawing like logo, basically, right? Where you have a pen, you could pick it up and put it down. You can go in a line or you can turn, but then we can learn a library of higher level drawing primitives, like circles, semicircles, spirals, polygons, or higher order ones like take any shape and repeat it, radially symmetrically, some number of times okay? And then that now lets us draw many... learn to draw many new shapes that we couldn't draw originally. The neural network is trained on these simulations that are drawn from the basic. Basically there's a probabilistic generative model. That's this learned library. So we draw over iterations. Like we simulate random programs drawn from our growing library. And so over the course of learning because the library gets more complex, the random simulations or the dreams become much more interesting, and that provides much better training data for the neural network, which helps it solve more problems, which helps us learn yet more new, good primitives. So that's the positive virtuous cycle of bootstrapping that you can see in this algorithm. We think that's one way to start to capture some more interesting ideas of children's learning. There's a lot of other cool things like using this to learn languages of vector algebra and physics, or even learning basic functional programming languages, but check out the dream coder paper on archive, if you want to see more of that. So just to conclude what I've tried to sketch here is both the toolkit we're using and a roadmap for how we might start to fulfill AI's oldest dream, building a machine that grows into intelligence the way a person does. And from the science side, we thereby come to understand better how our own minds are built. And it starts from revisiting the classic questions that come up in any approach to learning, which is what do you start with and how do you learn the rest? And recognizing from multiple decades of developmental psychology and cognitive neuroscience that human brains really do seem to start with a lot of structure and learning procedures have to be, we have a range of learning procedures including things which are... something probably like neural networks or like stochastic gradient descent in a differential program, but also much more structural kinds of learning and inference as we see in probabilistic programs and program SymSis methods. And when I talk about a roadmap, what I mean is something like this, okay? And I think this is where, especially the work I've been talking about, meets up with what Yejin is doing. Multiple phases of knowledge that start with that one and a half year old stage, and then go into the next key stages of learning. So what's the next big stage after one and a half? Well, it's basically one and a half to three, when the most important thing that every child in our world learns is how to read or language and then how to read, right? And once you get that, then you have the stage of three on up, which is using language to learn everything else first by talking, asking questions and answers, and then by reading. And that allows you to tap into all the knowledge that humans have built culturally, that accumulates across generations. That's the real singularity. I think that's the basis for being able to build machines that humans can interact with the way we've always interacted with each other in the true human centric way. Machines we can talk to trust and teach. And I think building, you know, the kind of human centric AI that hire is all about. And when I say this as a roadmap, I mean this quite seriously, I mean that by seriously studying each of these stages of children's cognitive development, trying to draw the kind of reverse engineering insights that we've tried to do in the earliest stages in babies, but doing that all the way along, building up with the engineering toolkit and the science together, I think that really is our best hope to build the kind of AI that we all would like to see, thanks. - Thank you very much, Josh, for that well, integrative panorama. So at this point, I'd like to and ask all the other speakers, Sanjeev, Yejin and Aude to come back and join us for the panel discussion. And I'd like to remind our audience that you can submit questions for our speakers on the event page of our website, okay. I think we've got everyone back in live now. So let's start at it. As a pro you'd remark Josh already observed that in some sense, the morning speakers were more of a lump in that, I mean, if I very crudely group them, in some sense, they're all happier with the learn forward from pixels perspective to life. Whereas the afternoon speakers are a much more diverse crowd, but we really do hope and believe that there are a lot of different perspectives and a lot to learn in different ways and actually understanding intelligence, both human intelligence and how to build it into machines. So I thought I might ask first Sanjeev for question relating to the first part of your talk, but it's a little bit of a tricky question, but I'll try you on it and see if you have a thought about this. In the morning talks, our morning speakers basically said, all neuroscience is useless for understanding how learning works or learning algorithms, right? That we, we just can't really see the details of the learning algorithm or come up with anything useful from what we can do with neuroscience. So if we're gonna make progress in learning algorithms, it looks like it has to come from artificial intelligence and also the three motivations you listed. So basically the second and the third ones were both related to learning algorithms. The second one sort of more narrowly on optimization methods, the third one is to what we actually need, to understand how we get flexible interactive agents like human beings. Do, do you think you right. So in the neuroscience context, you know, there's been lots of discussion. People say that there doesn't seem to be anything like back prop, but there are certain kinds of feedback loops that are going on. Like, do you think there are possibilities for a theory first perspective to help us get more insight into how human learning works or is that too much of a leap to hope for? - So I think one insight that we got out of some of the theory we did in the last couple of years is that the gradient based learning that we do is at the parameter level. And what we are interested in is the end to end functionality, the functions. And that seems to be some very different process. And you can't really infer the properties of the latter from the former. So right now we just understand that those can be very different and you don't know in an easy way to understand the trajectory of the end to end function. So I guess my question about the neuroscience would be that the data is, as you say, very low level, and it's very fragmented, you know, there's all kinds of things going on and you see bits and pieces of it. And so indeed yeah, what we have an intuition about is end to end function that's going on and the mechanics of it, it's not clear that that's so important. I mean, there's some local update and there's a network there, but not clear that that should be very important. So maybe we are on the right track in sort of inventing our own architectures. - And is possibly part of the answer the direction that Josh you were heading in on wanting to emphasize the structure that is present in the brain. So rather than I guess, most of modern neural networks, deep learning, is more of there's some structure the networks, but in some sense, the role of that is minimized and the role of parameter learning is maximized. - I think there is a connection and I think it's one that I'd love to get a theorist perspective on because again, since you've mentioned this also there's been a long tradition of learning theory that shows that if you wanna generalize in strong ways, from small amounts of data, you have to have some kind of inductive bias or something built in. And the question is sort of in what form is that built in or are there ways that in a -learning sense, you can get other kinds of data that can inform what's going on. But one, one idea, I mean, here's a very relatively concrete theory question. I know actually we've talked about this with Surya. I don't know if Sanjeev we've talked about it, there's a thing that we see we in the let's just say the deep learning community sees, I see this in some of the deep learning projects I've worked on where when you take a complex deep network that achieves some state-of-the-art or great performance on maybe an engineering task or on some brain modeling task, the training matters it doesn't matter as much as you might think, if you take the architecture and just kind of randomize the weights so start off with small random, weights often, it kind of still works, not as well, but often still kind of works and networks you know, if you compare architectures different architectures with their initial weights, there's often the same relative performance. The ones that are really good after training or better after training were also better before training. And one more thing that we see, and this I've seen this in vision projects we've worked on as well as language projects. Like there was this project that was mentioned earlier by Martin Shrimp and Ed Federinko, which I played a small role in that project on using GPT and other transformer models for modeling language in the brain. We see the same thing there also, which is that if you take, again an untrained sort of random weight network, but you do just a little bit of training, like on the last layer, like you build a linear decoder that often works pretty, pretty, pretty well. So I wonder, you know, what is the possibility or what are the prospects for an approach to understanding how brain development, and inductive bias might work, where, you know, most of the inductive bias is in the architecture with a little bit of learning kind of at the end. So you don't need to do back prop. If all they're doing is putting a single layer on at the end, or you know again it might be different brain networks that effectively act as maybe very structured layer looking back at another one, but the amount of learning could be local and limited, and there could be a lot of inductive bias in the architecture. Can we understand theoretically, like why that often works pretty well? Does that resonate with you guys at all? - Yeah so I guess I'll mention a few random things and see if it works. So there is a study of over primary tries nets. And one of the things people have studied are these neural tangent kernels. That you take an architecture and take its infinite length and width. And that's sort of like what you're saying, that you're basically training the last year in some sense. And there's a strong inductive bias, which is a neural tangent kernel for that architecture. And those actually do reasonably well. And these will always call, I mean, these are related to what the Europeans used to call liquid state machines. They've been stating those for several decades. - But these could be, but those were like where there's like basically random, random weights and even like a random-- - Right, so what happens is that when you go to the infinite limit, the training is basically very minimal as well. Basically you're seeing something related to the architecture, not so much the training. - It's interesting that like, you know, again, like the transformer architecture was clearly a big step forward, right. Even if it's not enough, it's clearly a big step forward. And GPT has some nice architectural properties that are different from the bird style models. And it's interesting that maybe what we see going on there is like a cultural evolution, recapitulation or analog of biological evolution, right? Where like the culture of deep learning driven by the massive energy of the tech industry and all of the things you were saying we want from our technology leads to a process. Again it's a kind of code evolution process that produces architectures that have really interesting properties. And maybe that's kind of analogous to what biology did, where again, a lot of the work is being done by evolution, coming up with a certain architecture, which has certain inductive biases baked into it. - So I want to actually make a quick point about that. So what do you call architecture actually comes with an algorithm, training algorithm. - That's right, just as in biology, right? You're saying they're both exactly right. - There's like a simple paper, like a cute paper we have out as well. And like the difference between convolutional nets and fully connected nets, like how do you explain that the convolutional nets are better and you cannot do it by traditional learning theory because a fully connected net contains a conclusion net. You could always train it by ignoring some other connections and enforcing weight sharing. So somehow the algorithm training algorithm plays a role in the difference. So now actually we have some theory that shows it, but. So yeah, there is definitely an algorithm at the back of your mind already when you talk about architecture. - I wanted to dive in. So Sanjeev we've also looked at these neural tension kernels, and we actually showed that in a narrow networks that are used in practice, if you train for a little bit, you have a trained tangent kernel that picks up a lot of important features and data. But then if you learn with that one, you'd out perform the usual neural tangent kernel quite a lot. But so I wanted to go back and ask a question in general. I think an integrative question about learning, right? So on the one hand we've been talking about common sense learning Yejin and Josh talked about that. And then on the other side, we've been talking about self supervised learning. So these are kind of two related questions. I'll just ask the first one for now, you know, a key theme and, you know, arising in learning powerful representations and spans multiple talks is sort of self supervised learning without labels where you try to predict one part of an object from another, like predict a word from the rest of the sentences, or predict different views of an image, whether they're the same image or not. And in AUde's talk, you know, whether three action sequences were, which one was the odd ball action sequence. So why is it that, you know, these simple auxiliary tasks self supervised tasks actually yield powerful representations, you know, can there be a theory for that? Can we have the rational design of exhilarating tasks for bootstrapping powerful representations. From a neuroscience perspective I find this very interesting because the first thing the retina does is parsley the world into 20 different visual representations. For example, one of them is black and white, and one of them is color. And we know from AI work that predicting color from black and white yields somewhat powerful representations. So maybe evolution figured out a way to pasalate the world into different sub-parts. And then it predicted the parts from each other in a very powerful way. And what we haven't quite figured out is what should we predict from what else? So I just wanted to see that and see what you think. Reach out at Aude first, for some thoughts. - This was inspired by Auden's talk actually. - Yes so to me, if a model is self supervise or whatever literally the learning method you use to build your model as a cognitive scientist, I will ask the model to show me be beyond the performances of recognizing for instance object is actually the error and the mistake that the model can make. So what we do is we go by your life by actually comparing something with something else and so on. So in a very simple way, we have those distance metrics and the distance between things represent the type of mistake we do. So we could take an Apple or an Orange. So there's a long metric that can characterize the mistake. Now, one of the open question in neural network, whatever the learning method you use is let's say you have two networks with very different way to learn that produce the sense sort of mistake. Does this mean that the representation they use is the same or not? And a lot in human cognition, we have this, I put, this as an assumption that when the mistake are very similar, then the underlying collection of representational calculation also similar. Does it throw them out? Those are assumptions. But I see with the very fast development of a world species of neural networks that we could be in cases where the output looks like something we could do. The range of mistake makes sense, take the tiger for the cat, that makes sense, take the tiger for the car that does not make sense kind of, but have different representation. And that will be very informative, to show how artificial network with very different principle end up in the same result. And that will be a way how AI can actually make cognitive scientists back to think. - Interesting, yeah that's interesting. I just wanted to contrast that with like the second question I was thinking of, and I do want to return to the self supervised learning versus common sense reasoning. So the second question was about common sense reasoning inspired by actually Yejin's talk. So I loved Yejin your example of visual question answering data sets, right? Where, you know, the professor doesn't teach and the professor just gives you a whole bunch of multiple choice questions and gives you the right answer, right? And then can we learn structured representations from that kind of a professor? And I sure couldn't, and I don't think many, I don't think humans can. And so you said we need to evaluate machines more generatively. We need to teach concepts more directly. And so what do you think the most promising pathways are there to doing that is self supervise learning up to the task? Is it not? You know what yeah, what do you think about that? - Yeah, so I was going extremely fast over that slide and many others, but my current thinking is that self supervised learning is a really important and powerful substrate to have. But in answering your question, I would like to also throw a theory question to Sanjeev. I'm sort of going with all your vibes about you know, asking Sanjeev some like interesting theory question, because when you look at this self supervised language models, it's quite curious why GPT-3 can write really amazing quality life blogs. So, you know, there's this Berkeley kid who used the GPT-3 to produce a lot of life of blogs, life advice I meant, and I've done some of these experiments myself, and it's unbelievable how good the coherency is. There's like initial planning about what I'm going to tell you today. And, you know it maintains this longterm coherence. It's able to make some book recommendations relevant to what it's saying. I go ahead and check all these books they exist. Authors are correct. And then I went ahead and checked that it didn't plagiarize anything and it did not because I know it couldn't because the way that text is degenerated is through random sampling. It's not entire random sampling. It's either nuclear sampling or temperature sampling, but in any case, it's curious why it's so good at seemingly harder tasks and that it cannot do something so simple, like, you know, common sense queries. And then, you know, you're supposed to say something obviously true, but GPT-3 spits out things that are so untrue and confused. So in some sense, it's a very easy to break GPT-3 with much simpler task. And on that note, for example, we cannot still do paraphrasing or much easier, lower level tasks that we would assume that some who can write that nice blog post, it should be able to know about everything in language. But on the similar ground, it seems that machine translation is much harder, easier task compared to reading comprehension. And even the way that we currently quote unquote, solve the machine translation or reading comprehension is by converting everything into sequence to sequence models under which it almost looks a little bit like a self supervised learning. It's not, it's a supervised. But just the way that it's all about, you know, predicting the next to token correctly. The objective function that seems being recycled by all this powerful transformer based, sequence to sequence models, are again somehow formulated technical very much like a perceptual task. So when I was presenting is perception, system one and system two reasoning, even for such higher level cognitive task, like machine translation, when reading conversion, we move that into perceptual level stuff. And then that's how we do today. And it seems that this is really curious, whether there's any theoretical way to say something about why writing blog posts is so much easier than being able to learn trivial common sense knowledge that every human knows about. And then, I do a specular thoughts. That's not the order, but more practically speaking, I'm drawing inspiration from cognitive science in this regard to that. Josh talked about this core concepts, is that the right word, spoke as core condition concepts. Like the way the humans learn about concept to treat objects are different totally from actions, which are different from maybe forms and time is different and the location is different and the functions are different. These are all different fundamental concepts. Today we treat everything as just same vector. And it seems that that that's sort of the key challenge. And when I think about concepts, basically to the ability to abstract away, you know, what's the sort of stuff Josh is doing. For example and Aude was also presenting where there's some like abstraction that is happening, but I don't know how far we can push by just using only one method we might need to just broadly search, which may require really rethinking about the current paradigm of learning, whether supervised or not. I feel like they're both very narrow, although I love self supervised learning. - Okay, got some comments to answer those Sanjeev? - Okay yeah so we've been studying self supervised learning in my group for several years. I think Chris was very influential in helping me in the early days. Yeah so I think that the way I phrase out supervised learning is more generally that there's task A and task B and that you learn to do task A and it helps you with task B somehow. And the distribution may be very different and so on. So that in a sort of black box way will not happen very generally. I mean, unless the tasks and the distributions are fairly related. So it seems quite clear with a few toy examples that the inductive bias of the architecture is very important, what George was referring to as well. And I guess yeah, sorry I also referred to the architecture. And the architecture, I think includes the algorithm, the training algorithm. So that those are some of the basic things to say. And so then in terms of what we can do, I mean, saying something about what happens in training a hundred billion parameters is currently a very difficult challenge for deep learning theory so we have relatively toy models, but using those, you know, you can sort of start teasing apart these things. Like I indicated for language models, you know, like, is it, I mean, there's some benefit to just, you know, GPT-3 is a better language model and GPT-2. So you can see in some toy settings that Epsilon, you know, how far it is from optimum helps in the task B. But then to answer your other question, you know, why is it bad on some things. It's possibly true that learning the distribution, which anyway to begin with, you know, Chomsky's objection, all that. I mean, it's a shaky assumption that language has a distribution. And so if you're just phrasing your task as learning a distribution or predicting, then maybe there's only a limited number of things it implies, and it doesn't imply some of those other skills you want. Those are all great questions for theory. - Can I comment on that? That's along the lines of the way I think about it too. I mean, I think for many self supervised algorithms, you can think of it as like learning a distribution, right? Like people who liked graphical models or Bayesian methods should be very happy with these, right. I mean, GPT-3 is explicitly framed in that way. It's a lot like, you know, the way, like, you know, when I got into the field, the way we would approach unsupervised learning from a graphical models or Bayesean perspective is we define some latent variable model that is some latent structure. I'm going to try to diagram it it's latent structure with some prior. And there's some arrows coming down to the data and you know, classic probabilistic language models, whether it's hidden Markov models or probabilistic context for grammars cannot be framed in that way. And you could view what you're doing in, let's say self supervised learning in GPT or Burt. They're kind of variants on this. They're not exactly the same, obviously, they're not exactly what I'm saying, but GPT is close to this is like, you know, assuming maybe relatively little explicit structure of the latent variable space, maybe a lot of implicit structure which we don't fully understand, but you know, you're basically, you're training a generative model and by training many different, you know, predict this condition on that and this predict condition on that, like each word on any possible previous context or in Burt each held out word or small chunk of language, conditioned on everything around it. Right you know, like that's, you know, I think mathematically one should be able to show that that is similar to, maybe even in some cases exactly the going to be the same as maximizing likelihood in a traditional unsupervised learning sense, but it has an extra bonus, which is that it's not just learning the latent structure and the prior, but it's doing a kind of advertised inference as Noah Goodman and others have called it right? Like, cause it's learning both a latent space that, you know, signs high probability data and an encoding a recognition model that can estimate those latents from some other data. And maybe in some ways like there's some nice work I'm thinking of Richard Turner's group, I think has some work on this. Many people have looked at this where, you know, you might see -learning algorithms and other kinds of outcomes like this as basically like they're jointly learning a good prior, but also sort of an amortized posterior. And so you're learning a latent space that has multiple good properties. And then maybe what makes some architectures and algorithms better than others is not just do they capture the structure of the domain in some implicit way that can support good predictions, but do they, you know, are they running the inference network in the right direction? And that, you know, clearly I would say, we all say that that's why GPT or that would be related to why GPT is so good at generating long-range coherence because that is the direction in forward prediction. That's the setting it is trained to do that. So, you know, maybe in some sense, we shouldn't be surprised by that. It is just like with in earlier work with Gens envision, you know, it's surprising how much you can capture in these learn generative models and how much they can look like the real world, you know, even things you haven't seen before, but also just like what you were saying, Sanjeev. And I think what cognitive scientists would say, well, you know, in some ways going back to Chomsky, we shouldn't be that surprised that while there might be statistical structure to language or to images, right. That's different from that, i mean, it's maybe a consequence of the structure of the world and the cognitive systems we build, having those kinds of predictive statistical models are probably an important part of our real time processing and how we use our knowledge. But it isn't the same as the knowledge, right? It's not the thing that supports counterfactual reasoning, imagining things that are vastly different from anything we've ever seen before. Whereas the kind of common sense core knowledge that Sparky and others have studied in reference might be that right. And then some of the things that we build, when we build more, you know, at least my view is when we build more layered structured symbolic representations in natural language and others beyond that also again give us the ability to think about and talk about and learn about things that are very different from our experience and, you know-- - So maybe give Aude a chance. - Sorry I started going into the, my... Yeah yeah right, good. - Like I was just listening to just-- - Well, I'll just say one more sentence and then shut up and then it just closes off, which is just to say, imagine if we could do self supervised learning, but with a more structured Layton space basically that has the kind of common sense core of things that might be a way to do, to get the best of all of these worlds and maybe would lead to systems that, you know, that would have more probably more to talk about. And maybe that's what Comet is, even in some sense, starting to do, I'd like to understand it better there okay. - Okay. - So is there anything you would like to add, or should we go onto a different question? - You found question that was really listening, going into what was said I'm sorry. - Surya do you wanna... - Yeah sure let's stick on this topic of common sense. So it's easy to point out examples of common sense, right. And we all kind of have a common sense notion of what common sense is, but it's perhaps a little bit more difficult to come up with a precise objective function that one could optimize to teach a machine to exhibit common sense. And so the question is how might we as a field work towards that goal? I mean, Josh touched on this a little bit, Josh touched on this a little bit at the end of this talk in terms of, you know, program learning programs. And so can there even be an algorithmic optimization based approach to learning common sense or might evolution have found some more common sense approach to view us with common sense, whatever that might be and how might we exploit that, to teach machines common sense? I think Yejin and Josh probably had the most to say, but yeah, go ahead Yejin. - Yeah, I love this question. It's really interesting, almost a philosophical question, but practically speaking, it is true that it's really difficult to define common sense knowledge. And it's really surprising how much we actually know as common sense knowledge as well as how much it actually does not really align exactly. So you and I could have a different quote unquote common sense knowledge. It aligns well enough so that you know, we probably will not get into fistfight anytime soon. But it is similar to language though, in that, you know, what is the true definition of language is so that you can really define and incorporate everything that people ever say, well, today we kind of implicitly define it by just here's this bunch of texts that people spit out and let's optimize for the publicity during training or look like glue, whatever. So similarly, this is how I view, except that the challenge is that there's no natural corpora where people spit out common sense descriptions, which is why we had to start with this atomic knowledge graph as a symbolic, small subset of common sense that a lot of crowdworkers seem to share, but similar to language, you know, also when we write papers, reviewers always do not understand our perfectly written paper. There's nothing confusing about it, but they didn't understand. So that's how I also see common sense is like that, people might have a little bit different interpretation, but that's okay. And that shouldn't deter us from studying this as a scientific topic, just like how it's okay to study language, even though Revere to never aggressive with us. But about like, so for now, I didn't have any better way to do this other than directly optimizing for this atomic corpora for the time being. But what's wonderful about it when it's mixed in with self supervised learning on language itself is that it's learning significantly more than what I could ever imagine. So when giving these talks and then doing online demo often times I was given like adversarial examples and I would tell the audience that, oh, this is not going to work. And then I try anyways, just for your viewing pleasure. And then it does do better than what anybody would have expected. So this happened multiple times. I'm not saying that this is a perfect system, but it's very interesting how much it can generalize. And then I'm seeing that the stronger language model is the faster the generalization actually happens. So it's now I'm starting to speak things that's not in public space yet. And that's a very exciting, but I don't have, you know, ideally I really wish that we didn't even need atomic or anything and some magic arising from neural network to figure everything out. But I do wonder whether it's even reasonable wish to have, if we only have current learning paradigm setup, either for optimizing likelihood on a lot of a raw text or are solving this QA problem, right. I think we somehow need to have an entirely different objective function which might be even harder to write than writing the objective function for common sense. So I don't really have an answer other than some speculative thoughts. - What's very good about common sense is that the concept is so elusive that we basically don't know how to give an operational definition, that a lot of people are going to use it and do things in a different way. That's very good. So it's going to be a lot of the word used. I don't know how to define it. It might be defined depending on the task and so on, but the way I think about it is it's too far. I don't know how to do it. So I need a smaller goal except that the smaller goal is enormous itself. It's the notion of analogy we always speak and see and think about analogy and analogy is a space of possibilities. You can have analogy in space in time in concept, in mathematics, in relating colors and so on. So analogy itself is a huge research rendezvous. And I find very interesting that we were talking about GPT-3. Well, okay it was based on an enormous amount of learning, but it can perform a very simple level of analogy with one of the few short learnings. So analogy is like, if you say ABC is to PQR, what's ABD is to PQ and you have to finish it up. And I think analogy is a fundamental things to model. Go ahead Sanjeev, I'm done. - Oh, well I can go next. Okay, so let me follow on with another quick question. - Can I say something about common sense before we leave that topic? - You know to mention you had a concrete approach in terms of program learning programs but-- - Maybe I-- - How would you functions on Common sense that's the question. - Well so I just wanted to say, I think I'm more of an optimist, I think about, I mean I hear what Aude's saying and people use the word common sense in all sorts of different ways, but I think there actually is a core meaning behind both the classic way it was used in earlier AI and the way the Yejin is using it and the way I was trying to use it. And I think we can, and I think what Aude is saying too, and I think we can work on that. It's something like, you know, common sense knowledge is like all the things that we all know that nobody ever talks about. So right it used to be common sense was like all that you need just to know in order to be able to read the newspaper. The newspaper never writes about it. It's just what the background knowledge you need to know to read the newspaper or maybe to read Wikipedia. And it's why it's hard to get from data because nobody writes about it. So you can't just mine it from lots of texts. The spell key or developmental approach, the infinite approach is it's like all the stuff, you know before you know how to say anything, right. But that early language builds on. And it's kind of in there, especially like in syntax and semantics of verbs and how phrases are put together. So like, you know, the word, like the word object or the word agent, those are common sense concepts, but they're not actually words that young children learn, (chuckles softly) but they are there in sort of the way nouns and verbs work. You know, for example, our common sense notions of space and time. Those are also things that verbs, syntax and semantics. For example, the work of Beth Levine here at Stanford is really the seminal work showing this. It's sort of in there in how we learn about how verbs work, and meaning form correspondences. So what that suggests again is it's sort of like, basically think of this common sense as multiple stages of knowledge starting with what babies have and then what you have when you first learn language. And then what that comes to enable, which are like basically building languages of thought. It's the stuff, Steve Pinker calls it, like the stuff of thought in his book "Of That Name," right? It's the stuff you talk about, but it's the stuff you need to know to form new thoughts and then to form new sentences and then to use those sentences to form yet more complex new thoughts. So there's not really a metric that you optimize other than basically trying to solve a bunch of problems of the right sort and then building whatever is the language of thought that supports flexible open solutions to that flexible open-ended classical problems, right. - Let me ask a question to Aude, which isn't exactly common sense, but sort of heads in the direction well, of some of the topics that came up of having prior models and of the world, right? So you emphasized these three second activity scenes as and? Or at least what I took away that you primarily motivated having these sort of three second snippets in terms of you know, humans responding in the natural visual environment that they wanted to get a quick snippet of things. But I mean, I was thinking in my head, well, so there's an obvious advantage to seeing some movement rather than just a static scene. You obviously get some enormous amount of extra value from seeing a little bit of movement in there. But, but beyond that it's a lot of the secret of the effectiveness of these three seconds scenes, not sort of their human hunter on the prowl or something but it really has much more to do with humans have these really rich models in their heads. And as soon as we've seen a second or so of video, well, we can predict backwards and we can predict forwards. And it's really, it's much more a story about our very good models of the world, both from priors and learn that's going on there. - Well, the account is it's just a number, in order if you want to train a model. What we were looking after is that the way of brain conceptualize the world is there's a certain dynamics to it. So if I want to pick up an object, it takes some times. And there's a lot of the activity that are meaningful to us that unfold within a few seconds. So then three is just an average. And the thing we observe is that if you train a model on shorter or longer video, or then if you train on the three, on shorter video, you will miss information, but on the sort of three seconds, there is enough information, in order to develop robust features. And also it's the kind of this average length for short term memory or things you do, you put in memory, you think about something else. So we have a big time, yet time module over here. And it's at several scales and it was something that just makes our model works much better. It's a good number. - It seems like a magic number though. I mean, in a good way, right? Like people have proposed two or three seconds as some as magic number all over that. That would be an interesting paper to write. And if anyone's written that paper, but I'd love to see, you know, what is it about like the length of a sentence, the length of an activity and the length of a professional football play, or even a basketball play, you know, there's something about like the brain (laughs) that makes that the length of human activity at the micro scale or at the, at the basic kind of single sentence level, right. - The other correlation of the movement of prey, (laughs softly) the time it takes to turn (laughs softly) and while you're chasing it. - Well, yeah, I mean, for mammals, at least maybe I know, I think they're faster, but you know, at the level of... it's easier for me to think of things like sports, whether it's baseball or basketball or football. 'Cause they, they kind of unfolded that timescale, you know, but that's our, I don't go hunting, but I dunno. - Well, the other thing is the human brain will have certain buffer will hold for a few seconds. And if you want to remember something after 20-- - Yeah the remembered presence. - After let's say 30, 40 seconds, it's sort of a structure. And those go with the silent result-- - Yeah, exactly there's something fundamental-- - It's like some kind of speech. - Right but yeah, it seems like there's something fundamental about the fact that we're talking about human activity. So it's like the timescale of human activity, planning and execution is going to then turn into the timescale of where perception is most meaningfully. And again, that speaks to the generative model perspective. - I guess I have a question for Yejin actually. So in the first panel discussion and the talks of the first session, the issue of an embodied cognition came up a lot. Know that in the brain, language areas like Broca's area in Wernicke's area, they did talk in a very strongly coupled way with other parts of the brain that, that both represented drive action the world. But you've gotten a lot of mileage out of discovering good causal representations and model restrictions just from language alone or language plus static scenes. Do you think that we need embodiment to sort of create powerfully eye systems or do you think there's a lot more we can do without resorting to embodied systems? - Yeah, awesome question ,yes and no. So I didn't talk about it very much about my other grounded research, but so I, that's how I studied actually thinking about common sense. I was thinking, oh I'm going to just extract knowledge from images. And then they realized it worked, computer vision is really hard. I thought that language is hard but contribution is not only hard. It's also very slow. So after processing so much data, all that I got was like hundreds of knowledge, then I'm impatient person. So I wanted to go very large scale, very fast. And it seemed that language is going to allow me to go very large scale, very fast, covering a lot of broad scope of things. But I certainly do not believe that that alone will be sufficient, especially for systems that might want to interact with the people through like robotic bodies or even computer vision, because you do need to somehow translate what you know about the world in language with like intuitive physics, the sort of stuff Joshua was showing all year in his presentation. And that's really huge missing link. And so that's why I think embodiment is important, but there's something really dissatisfying in the way that current language and vision research works, which tends to focus more on the sort of like a, conjunctive a subset of concepts between what's in the image and what's in the language as opposed to being able to really unify everything and then be able to learn everything in a more meaningful way. Another concern about so I'm currently playing with a three D environment as one of the work in progress, which is really frustrating how basically the world is defined by some engineers who build to the three D environment and I cannot get out of it. And I don't know how the robots degeneralization will easily happen between that world and in the world that I presented in the images like, you know, people might crash the car or drown in the ocean. And I certainly do not need to experience any of that in the embodied the environments in order to understand what it is like to crash in the car or drowning in the water. So yeah, a lot more research to be done, really, but I do like embodied research direction quite a bit. - Let me, let me ask one last question. So how far go a couple of minutes over time since we started the session a little bit late, but this will be the last question, but actually I'm going to start with Yejin again, but I think it's one that other people can also have interest in. So I liked where Yejin again with sort of talk about Daniel Kahneman's system one and system two. And I guess there's been a lot of talk about that. One of the most prominent recently was Yoshua Bengio. It is Europe's 2019 keynote talked about system one and system two. And I think a crude version of what you were meant to take away is deep learning has nailed system one. And so now we can work on conquering system two, whereas you pointed out that somewhat more nuanced version where there are three things, perception, intuition, and reasoning. And so, well maybe deep learning has nailed perception, but that then still leaves two things left. And, you know, I was wondering what kind of models and methods we really have that are good for intuition. Because you know, some of the other things that were mentioned such as probabilistic programming, that sounds a lot more like the reasoning level to me. And some of the other about sort of neuro symbolic architecture as the symbolic part is kind of on the seems like it's normally on the reasoning level and well what have we got for something like intuition? Well, Josh mentioned the metaphor of the game engine in your head, but I'm not quite sure that's really right for intuition. I mean, I was actually thinking of the example Josh gave of the Jenga game. I mean, I kind of think, you know, whatever the flaws of your average game physics engine, I think it's a hundred times better than my head is at working out the physics of what happens with Jenga blocks when they fall. But somehow I have enough vague intuition. I can play jenga okay with my kids. So what should we be doing about intuition? - Yeah, I, that was such a nice summary. And it's true that I was opposing what Ben just said. But so what I realize about intuition as I read the more into this non-serious coxae books is that, wow, there's so much going on in the intuitive level that humans are extremely powerful about. So, probably don't you always also have this experience where you have an idea about new algorithm, but only intuitively that you have a hunch that this could work. You don't know why you don't know why yet, but you just know and you believe it very strongly. And then you look for some solutions to it. And then you'll eventually you do system two reasoning in order to figure everything out. But there's something really powerful about intuitive reasoning that even goes beyond the common sense type intuitive inferences but today I was focusing more about the easier pieces, which has seemed just a simpler to target first, but other than admitting that I don't know the full answer to this, but this might be something really important to pay more attention to as an AI field, because this is really fascinating about human intelligence and this might be something neural networks are very good at for some reasons. - Have ideas on what ways forward the intuition level? - But I mean, we do need to-- - Can I raise a cock side perspective on this and then just to frame the discussion. So cognitive scientists don't study intuition and they don't for the most part, really work on system one system two either like those are system one and system two, is kind of a convenient summary of a whole bunch of different distinctions that is helpful for people outside of psychology to maybe understand, but it's not very helpful within psychology. Do you know what I mean? Like there's no part of the brain that's system one and part of the brain that's system two and there's many distinctions that are wrapped to that. And what individual people usually do is unpack those and study particular ones. And likewise intuition is not, it's not just one thing it's, you know, maybe for people who are in AI or neuroscience would help to make an analogy to learning. Like if you said, if someone asks you, how does learning work in the brain? Well, you know, there's so much that, you know, do you mean reinforcement learning? Do you mean classification learning? Do you mean learning at this timescale? Do you mean snapping of there's so many things going on, but there are some things we can say about what I think the common sense intuitive notion of intuition is, and the ways cognitive scientists approach this, which fits with just what you were saying. So I might just put it out there, right. Which is like often what we mean by intuition I think, are our unconscious computation, so we're not conscious of the computation or much of which includes the representations and the algorithms. But something pokes into consciousness and it often has a kind of a probabilistic character. So we can have a strong intuition or a vague intuition. I can have intuition that that's right or that's wrong, or that's, or that's good or that's bad. And I can't say why, right. So a lot of the things that we pack into the word intuition are of that sort. And I think actually if you look over different areas of cognitive science and computational modeling, people have had success at capturing different things, different aspects of that with different tools. But they all come down to some kind of approximate probabilistic inference, or at least the ones I know. And that includes that's at least a way to render certain kinds of analogy models. Cause like Aude was talking about sometimes there are analogies where it's like, I don't really know about this, but I think it's like that. So like high level similarity where you can say, I think it is likely that what was true, there might be true here or some of the Bayesian models that I used to work on earlier in my career. Or like some of the neuro symbolic things, like when I was talking about Kevin Ellis's work there or a lot of other work that people are doing in like neuro symbolic program SymSis where you're basically training a neural network to suggest not to like write code, but the way we use it, is it sort of guides the search. So it suggests to a search algorithm that searching through this space of expressions in some programming language, where are the more or less promising ways to go based on the data that I've seen and maybe what I've done before. So it sort of is a-- - Intuition is an imperfect word for this stuff in the-- - Right but like, let's just say like approximate amortized probabilistic inference basically. - I cannot go to Sanjeev and say Sanjeev, when you have an intuition as to how to go about solving a problem, do you think you are doing approximate probabilistic reasoning in your brain? I'm not quite sure that's how-- - It's interesting your intuition about how you do intuitive reasoning, but (laughing). - I know we are running out of time, but if I may add a little, so I love a lot of what Joshua says and I keep following his talk, but minor bits that I may not necessarily completely agree with here is that so I don't know, like I didn't read the too many coxae books in a year. There's limit to how many you can take a look, but it seemed that this contrast between conscious and unconscious inferences is a thing that people do talk about a lot. Even like maybe not for reasoning, but more for like, you know, emotion-based research, their focus just on emotion. There was a simple you also seemed to-- - You mentioned world moral judgment-- - So, you know, we could just think intuition equals unconscious first associative reasoning thing that it seems that there have been such research but... - It's not always associated, I think it's usually-- - It may or may not, so I agree with you that these things are very complicated in actual research and these things are very much modeled up and it's hard to separate that out so that I totally agree with you. - Things that are fast and unconscious, but that can have a lot of computational structure. - About the probabilistic nature of it. I don't know if I can marginalize any of what I actually feel and assign, Oh, this is definitely, an idea in which I can assign 0.81 versus 0.278. I just, I really don't know. - The probabilities are approximate too. I mean, you know, cognitive scientists usually-- - Maybe we just know how to. score things in a very inconsistent manner though. I suspect we are very inconsistent. - I wanted to ask one quick question. So intuition is being sort of equated with fast reasoning, but I think the more fascinating intuition for me and probably everybody on this zoom call got this at an early age, the subconscious thing, right? You're you're thinking about something you sleep over it a night or two, and suddenly it's clear in your head. There's something else going on there. - Yeah during sleep, you are dreaming about it. - No that's not just about you go about go to the gym, whatever I do-- - We're saying sometimes exercise can do it too. - Right like play with your kids, whatever, right? - We can definitely talk a lot more about this, but I'm afraid, I think we're out of time at this point, you really have to move on to the final session, I'm afraid. So I'd like to thank all our speakers. It was a wonderful discussion. I think we could have done another 40 minutes of it, but unfortunately-- - Thanks to you guys for-- - Do have a great day and truly we can say thank you and goodbye to those speakers. And then we're going straight into the final short session where we're being joined by Bill Newsome and Michael Frank. I'd now like to open our final session of the day and to welcome two guests. First I'd like to welcome Michael professor of human biology and director of the Symbolic Systems Program at Stanford. And then secondly, there's Bill Newsome, who is the director of the Stanford Wu Tsai Neuroscience Institute. Let me, first of all, start with you Mike symbolic Systems. That's a sort of strange Stanford thing. I know we're not unique. There is another university that has a Symbolic Systems Program, but it's not such a common thing. So maybe you'd like to tell us about the Symbolic Systems Program and you better bench in somewhere. How do you cope running a Symbolic Systems Program in the middle of the neural revolution, I guess? (Chuckles softly) - Thanks for having me. And I'm so glad that SymSys could be a co-sponsor to this really a fun day of talks. So like you said, we're an undergraduate program at Stanford. That's a fairly unique interdisciplinary mix. So we try to give students an education in computation, in formal methods, in philosophy and in empirical cognitive science. So the idea is a student comes out of our program, not only able to jump into building exciting new projects or participating in research, get hands on, but also able to understand the projects and the things they build in a broader theoretical and social context. So I'm, I'm hugely biased. I'm an alum of the SymSys program myself actually. But I think we've got evidence for our success in this amazing broader list of alums, which includes founders from Silicon Valley companies like LinkedIn and Instagram, and also a wonderful list of academic alums doing research everywhere from neuroscience to philosophy and law. - Send us more about how you think that comes together. cause I mean, obviously the challenge in any interdisciplinary program is the students learn less about any particular thing than they could have if they had majored in psychology or majored in philosophy. - So one way of describing the major is Stanford's take on cognitive science. But honestly I think that's not quite right. We call our introductory course minds and machines. And that's the really the guiding relationship that I think is at the core of SIM SIS. So I it's all about these different relationships that you can put minds and machines and how do you use machines to understand minds? That's neuroscience and cognitive science. Minds and machines interacting that's human computer interaction. And of course, building machines with minds or that approximate minds as an artificial intelligence. So I guess I think of SymSis as a jumping off point for the ways that that relationship between minds and machines can get construed and can lead to future research. Well, thanks Mike. And also to turn out to, to Bill, can you tell us a little bit about the Wu Tsai Neuroscience Institute and yourself and... - Yeah, happy to Surya. - And like Michael, I'm thankful for the opportunity to be involved with symposium today. It's been a really rich set of talks and in regard both HAI and symbolic systems as wonderful partners here on this campus to neuroscience. Neuroscience has been praised at some points during the day today. And it's been denigrated though. There's I believe that trash was a direct quote. So it's been fun to kind of sit on the sidelines. I have lots of reactions, but I think that we have a lot of mutual agenda here. And I really look forward to working with EJI and with some box systems going forward. The Wu Tsai Neurosciences Institute was conceived about eight or nine years ago by a couple of guys named Hennessy and Etchemendy. I think most Stanford people are familiar with those names. Our former president and provost, who thought that neuroscience is one of two scientific disciplines on campus that had the potential to create a campus wide community of scholarship ranging across multiple schools, multiple departments, multiple conceptual approaches. And I think they were very farsighted in that, in that respect. And so the re-assign neurosciences and students campus wide, it does not sit under any one school. It sits under the Dean of Research. And so I interact with deans and department chairs from all over the university. And that is because neuroscience is so inherently interdisciplinary. All of the subject material we have been talking about today is definitely highly related to neuroscience. And yet we could go talk with a non-overlapping group of people, which we will be doing tomorrow morning. Our symposium on the neurobiology of disease and it would be equally relevant to neuroscience. And we have contributions coming from applied physics that's Surya's home department from genetics, chemistry, statistics and engineering. It's important for people to understand that this isn't just icing on the cake for neuroscience. This is bread and butter. This is the future of brain science. The brain is too big a problem to be solved by any one discipline, any one set of experimental techniques or any one set of concepts. And we've all got to come together and work in tandem with each other and energizing each other and educating each other in order to solve this problem. And that's what the Institute is about. It's about doing that. And we do that by investing in people, you know, hiring faculty, creating interdisciplinary fellowships for postdocs, for graduate students. We do that by investing in research through our big ideas and neuroscience internal granting program, along with some seed grants and neuro translate grant program, getting translated into neuroscience ideas on the real world. And we also do that through a new undergraduate education program that we started a neuroscience interest group. So we're in there and we're doing this thing and it's a natural allied AGI and to SymSys both. - Thank you Bill. And those remarks, you've just said a really minimally, a very good introduction to the next question. I was hoping that people could address, but let me focus a bit more on linkages. That I think one of the feelings that we had in setting up HAI was that there was really a lot of connective tissue that needed to be developed to really get the synergies that we're hoping to see between research and neuroscience, cognitive science, psychology, and artificial intelligence. So let me maybe start with you, Mike. I mean, how do you see the prospects and the key things that we need for creating that type of connective tissue at Stanford? You know, what are we doing or what should we be doing perhaps, you know, you focus quite a bit on the undergrads interest in this space. What's the secret to connective tissue? - One little claim that I would make in response to Bill is that we're also kind of connected with HAI in our DNA since John Etchemendy was actually one of the symbolic systems founders as well, but you know, we're really working to expand that connection. So for example, we just rolled out a new concentration, which is a specialization track in symbolic systems for human centered artificial intelligence. And the idea behind that is to guide students towards the intersections of AI science policy and ethics. But I think really characterize the HAI vision. So that's kind of one intersection, but more generally I think, where this comes together is in undergraduate research for symbolic systems and talented undergraduates do an incredible amount to advance research. So one of the most fun things for me as a working scientist in the lab is to host undergrads in the lab over the summer where they put their full attention on the problems that we're interested in and really make big contributions. And so with support from HAI and from Wu Tsai this last summer, we actually had the biggest ever group of SymSys undergrads embedded in labs, getting full earnings replacement, to be able to really devote themselves to research. And that's precisely the kind of experience that motivates folks to go to grad school. And one last kind of stop along the way before grad school is honors research, where we're really seeing this kind of integration happen already. So our Firestone medal winner last year, Nanny Crawford won a university level prize for work investigating face recognition and CNNs. Really this kind of very HAI topic of trying to understand the linkages between neuroscientific representations of face features. And what's going on in the training data for networks. So I think we're really trying to build these links and getting students involved in research, getting them the tools they need to get kind of placed in labs early, so that they can go on to do honors research and eventually to grad school, is the track we want to encourage. - Great. thanks Mike. Bill I guess the same question for you, you're less of an undergraduate institute and so you're more of a research Institute. So I was wondering how do you see the intersection between AI, cognitive science and neuroscience going forward as a research field and through Wu Tsai. You know, how do you see these fields working together - Yeah, so we've given that quite some thought series, you know, 'cause you've been part of the process and we think that this interface is actually embodied. We embodied it in the design of our new building, the Stanford Neurosciences building here, which has a Theory Center inside it. And one of the principles here is that neuroscience going forward is going to go forward is going to go forward more rapidly. And with a deeper understanding and more incisive impact in the theorists, data analytics specialists are embedded cheek by jaw with the experiment in the labs. It's important for both groups to work together, designing experiments, analyzing data that theory center when it is fully staffed, we'll have six research groups. Surya and Dan Yamins will both be there, as well as Scott Linderman, a new hire in the statistics department. And I would say that researching in that center and then filtering out all of our experimental labs will involve theory. It's Surya being in classic theorist, who really wants to identify general principles of operation and ideally where you can write down a close form equations. But there are neural network people who are really impactful. Ben Hammons being a classic person, who's working hard with the deep, deep convolutional networks and there's high level data analytics that are so important to neuroscience because we like many other fields are getting these floods of large, very sophisticated data sets coming into our field due to advances in technology and the traditionally trained neuroscience like myself just as not prepared or competent to deal with these data sets And Robert Laughlin the physicist here at Stanford once warned me. You know, the biggest danger year field is that 10 years down the line, you're going to have disks and disks and disks full of data. And you're simply going to be 10 years older. (laughing loudly) The point being that understanding is really what we're after, right? And that's the goal of theory and computation and network modeling. And that's where we will have the closest interface with HAI going forward. And I would love to do, I mean, these interfaces have to be embodied in people. They have to be real people. My side of embodiment, by the way, in your previous discussion today and hiring joint faculty, I think it was something that we really need to do. So these faculty would build the bridges. The students are already there. The students love this area and they love the connection between neuroscience and artificial intelligence. Every faculty member we have hired is instantly glammed on to by a dozen students wanting to work in their labs. So the students are ahead of the institution at this point and we need to do some hires together to faculty level hires together to create the environment where this can really go forward. - Let me continue a little bit along that theme and just see if there's any other items that people mentioned. So yeah, we've said quite a bit about the great programs, departments and institutes at Stanford we already have, but I mean, there are always opportunities to build a more perfect union. And so Bill sort of mentioned the idea of how the students are already ahead, and we just need to have a faculty there for them to work with. But I mean, for both of you and I've maybe I'll start it again with Mike. I mean, are there things you see as to how we could really do a little bit more to take things to the new level in really building the synergies between these three areas. - What I think his is going to be a success here, but that hopefully we can replicate more. So one thing that is amazing that HAI has done as an Institute is launched the Hoffman Reed program. So this is through the generosity of Reed Hoffman who will claim as a SymSys alum here. So the, this kind of moonshot project is bringing together interdisciplinary teams and getting them to work together to try to accomplish really large scale projects. So Danielle leads, one of these teams and I, Chris, your colleague, Chris Potts from one linguistic leads another that I'm involved in where there's some SymSys students already starting to do some work, but you know, that that's kind of... that starts with the moonshot and that's an important thing, but I creating a broader ecosystem of opportunities that brings faculty together, even when that's not the most convenient or easy thing to do is really critical. So that kind of community building that brings the faculty together and maybe is also lead and leads the students as Bill was saying towards this intersection is really what we need. So there's a combination of faculty and also opportunities for interaction because the work of getting these conversations going is fun, but it's hard and it's not the first priority for a lot of folks, especially younger faculty. So as we've seen in some of the discussions today, you know, there are foundational issues that we disagree on. And so for each of the projects I've been involved, you really have to have people sitting down again and again, to hash out the vocabulary that allows them to have the interdisciplinary conversation. And sometimes you need people that are really trained in that conversation in order to be able to contribute. So creating those spaces is I think is the next frontier for these affiliated programs, - Yeah Bill, did you have any further thoughts you wanted to share on this particular issue of moving to the next level in terms of building connections - Yeah, I think really focusing for Wu Tsai on the faculty level interactions is really important. I think that though, you know, we have discussed Surya within the Institute. I don't know how much this has been discussed. We're going high. But building interdisciplinary postdoctoral program would be a really good thing to do. I mean, Surya himself came out of string theory and physics for his PhD and it was only me as a postdoc, but he got interested in biology and did a detour through yeast and biochemistry before winding up in neuroscience. But giving postdocs that extended kind of by disciplinary training and the freedom and time and money to explore a second area, I think is a critical thing for us to develop the leadership of tomorrow because tomorrow's leaders can't be somebody like me, who's a traditional neurophysiologist and I know how to do one thing well, but unfortunately, I've been open to some new ideas and having, having good collaborators, but the future really belongs to the people who span these boundaries intellectually and practically and can speak the language already. I mean, as Chris and Mike, I think Mike was saying, you know, having these conversations across disciplines, it's really hard just at the beginning, you're using the same words and you're meaning completely different things when you use those words. And just, just speaking a common language is so difficult. So the earlier we can start on that, the better. - Great, well, Mike and Bill, you're not only program leaders, you're both also very accomplished scientists. So before the time runs out, I wondered if you know, you have any thoughts or takeaways related to our theme and sessions today. So for Mike, for you earlier on, there was actually a cameo appearance of your C cam project. And I guess by background, you're actually more of a developmental psychologist, but as things are going like this willingly or unwillingly, so heading more in the direction of cognitive psychology, I think what are your thoughts? - Because my interest is always in development and growth and change and learning. I was really fascinated by this morning discussion about sample complexity, about how much data it takes to learn something versus how much data it should take. And of course, right, you know, if we want to try it out these examples, if you look at GPT-3, tremendous success by some, but not all measures, it takes five orders of magnitude more data than a three year old. And I think it would probably beat the three year old on a lot of tasks, but my seven year old, which has got only about, you know, she's got about double the data is probably gonna win on a lot of notes. And we heard a lot about interesting ways to improve transfer and efficiency today, but I guess my personal interest is in learning from other people. So yeah you know Chris, you mentioned scientists in the crib and Josh talked about the child as hacker, I guess, you know, maybe I'm primed by undergraduate education, but I think about a slightly different metaphor, which is the scientist is actually the undergraduate student working in the lab. They do discover things, and they are absolutely valuable discoveries that come out, but there's also a lot of coursework in reading, right. There are a lot of people around providing the scaffolding. And I think that that's an important insight that I'd love to see talked about more, you know, my colleague Joe Guan has really explored how many cases you can get kind of really rich information transferred by just a simple social situation that sets up kind of a very subtle inference in a child. And that's the kind of inference that deep learning systems don't tend to make a lot of AI systems are you know, you repeat the same thing over and over again, rather than inferring a tremendous amount from a single gaze or a single, a single utterance. And so that kind of social learning, I think, is something that I'd love to see kind of integrated here. - That's a great point. And that's also one that I vibe with as someone who works with human language, I tend to feel like in a lot of discussions, both artificial and human learning, there's a real overemphasis on a single brain or a single computer, rather than thinking about the way that humans work collaboratively and gain so much from their interactions with other people. And Bill, you mentioned you had some strong reactions to some of the neuroscience aspects of today. - Do you have any takeaways you want to share? - Oh, I, I have a lot of interaction. I mean, reactions, Surya, you know, many times they're going to have, it was fun. It was very provoking set of talks and thoughts so many, but I guess just the one that I would call out is I was really interested in sort of both the, what was similar and what was contrasting between the talks of Danny Ammons and Chelsea fan this morning. But what was similar about them is that they're both interested in agents and agents interacting with an environment, but Dan is doing it virtually simulating it at a computer. And Chelsea is actually doing it physically with robots, that actually moving the world and physically interact with things in the world. And I found myself in my gut leaning more towards, you know, the Chelsea's approach is probably to be in the end lead us to something more like animal intelligence than perhaps the digital simulations. And that's not to disrespect, Dan's work at all. I admire his work tremendously, but there's something about interacting with the physical world. And I think that's what builds these physics engines. You know, that Josh Tannenbaum was talking about at the end of the day, Chris, I even think it's relevant to the intuition point that you raised right at the end of the previous discussion. Because if you really look at babies, a lot of interesting things were said about babies today. One of the most astounding things is their urge to move and their urge to interact with the environment. I mean, right from the get go, like first using that hand and then struggling sit up and then wanting to stand up. And if you watch the baby, my mind goes back to JJ Gibson, in fact ordinances. It is when I touch that thing, what happens? Does it just stand there like a rock, does it fall over which way does it fall over? And they're constantly experimenting with their bodies. And I think they're building premotor structures, Chris, in their brains and in motor premotor cortex that are being connected with sensation to such a say that AI maybe has solved perception, but now needs to work on intuition and whatever the other one was. I think that the connections between perception and action are far too deep to be segregated that nicely. So I think that was maybe the thing today that I had the strongest reaction to. And you can tell I'm a biologist, cause I want to see an organism in front of me. Right I wanna see. I wanna see an actor. It's not just digital zeros and ones out there in the world. - Yeah, I think that point of connecting perception and action is just a really important point that... - I also agree. - I think we talk about at the start of the AI class, but you know, very little of the work that's being done outside of certain areas of robotics, like Chelsea's is actually doing it. - Yeah, you know, I think it's, it's super exciting. And also the way we design neuroscience experiments should deeply intertwined those. They shouldn't be separate fields in neuroscience, but anyways looks like this is all the time we have for today. I'd really like to thank you, Bill and Mike for joining us here and for helping HAI as a cosponsor of this important event. I'd also like to thank all our speakers for today for fantastic talks. To everybody out there in the world we're gonna to make this entire broadcast available as a recording, which will be live following this event on our website, or on our YouTube channel videos of each individual session will be available, you know, within the week, if you're on our high mailing list or if you registered for this event in advance, you'll receive a link as soon as that's available, as well as a survey. We'd love to hear from you about your thoughts about this event, about this field in general. Thank you for making the time to hear about this important research. Bill Newsome alluded to the fact that I was a string through us before. So I used to study the physical world, but I shifted to, to this field. And I think this intertwined quest, both understanding biological intelligence and creating artificial intelligence may actually be one of the most exciting intellectual adventures of this of the century. Whereas, you know, physics was the extremely exciting intellectual adventure where we made a lot of progress. I think this is to me right now, even more exciting. So have a great rest of the day, everyone. And thank you everybody for joining us. - Okay. 