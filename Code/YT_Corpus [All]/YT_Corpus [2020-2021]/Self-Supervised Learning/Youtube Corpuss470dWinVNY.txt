 A: You seem stressed out. What’s wrong? B: I want to classify medical sentences based on their content, but I know nothing about the field. A: I see. I have a friend who is a physician that could help out. B: Oh great! Can they label a thousand sentences for me? That way, I can understand what’s going on. A: My friend won’t have that much time and effort. What if you just pick 10 sentences that are the most crucial to learn? B: How do I decide what’s most important to learn? A: Well, you should be an active learner and figure out what you need to know. For example, you can pick the sentences that confuse you the most. B: I don’t know anything about medicine...so everything confuses me! A: That is certainly a good point. Have you heard about the new transformer models? B: You mean transformer robots? A: No, I mean the models that have revolutionized language understanding for artificial intelligence. They contain lots of general knowledge about the world from training on information accessible on the web. Plus, there are certain models for medical text. What if you use that to help you become an active learner? Labeling data is a fundamental bottleneck in machine learning, especially for NLP, due to annotation cost and time. The goal of active learning is to recognize the most relevant examples and then query their labels from an oracle. Then, people can use this small labeled dataset to train a competent model. A widely-used active learning algorithm is uncertainty sampling, where the learner queries the most uncertain example. Typically, uncertainty is measured by the entropy in the example’s label distribution. Active learning would be useful for today’s state-of-the-art models that can cost thousands of dollars in cloud computing and hundreds of pounds in carbon emissions to train. However, traditional active learning algorithms, like uncertainty sampling, falter on deep models. These strategies use model confidence scores, but neural networks are poorly calibrated. High confidence scores do not imply high correctness likelihood, so the sampled examples may not be the most uncertain ones. These limitations of modern NLP models illustrate a twofold effect: they show a greater need for active learning and make active learning more difficult to deploy. Ideally, active learning could be most useful during low-resource situations. In reality, it is impractical to use because the active learning strategy depends on warm-starting the model with information about the task. Thus, we need a cold-start approach, one that does not rely on classification loss or confidence scores. Dasgupta describes uncertainty and diversity as the “two faces of active learning”. Uncertainty sampling is like an efficient search through the hypothesis space as it finds the most difficult examples to label. The decision boundary is defined more clearly as we sample the points that are most confusing for the model to learn. On the other hand, diversity sampling exploits heterogeneity in the embedding space. If there are patterns in the feature representations, then we should find the examples that best represent the dataset. Prior work show that a successful AL strategy should integrate both aspects. We need information about the downstream task and an understanding of the underlying data structure. However, the exact implementation is an open research question. Nowadays, models like BERT dominate the leaderboard. The success of BERT lies in the self-supervised objective, which is masked language modeling. To pre-train BERT, input tokens are randomly masked, and the model needs to predict the token labels of the masked tokens. Since we are developing active learning for BERT, why not use the information accumulated during pre-training for active learning? A recent paper called “Language Models as Knowledge Bases?” explores how BERT can replace traditional knowledge bases. They find that BERT can store relational data through language model pre-training. This paper, and many others, motivate us to apply language modeling in active learning. ALPS Our paper proposes an active learning strategy called ALPS, which stands for Active Learning by Processing Surprisal. ALPS substitutes the supervised loss with the self-supervised objective to estimate uncertainty and sample examples. First, we iterate over the unlabeled dataset and compute a surprisal embedding for each example using pre-trained BERT. Second, we run kmeans clustering on the surprisal embeddings. Third, we search for the sentences that are closest to the cluster centers. Finally, we query labels for these newly found sentences and add them to our labeled dataset for model fine-tuning. Now, let’s break the algorithm down into more detail. To construct a surprisal embedding, we use the masked language modeling head of BERT. Suppose we are given this toy sentence as an input. We pass this sentence through BERT and its masked language modeling head to get a representation for each token. Then, we want to predict tokens based on these representations. As in BERT, we only perform predictions for 15% of the tokens in the sentence. In this case, we only predict the words: highest, mountain, and europe. We use cross-entropy loss to score our predictions. After we receive a vector of scores, we normalize this vector for clustering later on. In the end, we get a sparse vector of surprisal scores. The nonzero values correspond to the words, highest, mountain, and europe, because they were predicted when we computed cross-entropy loss. Notice that there is one difference between BERT pre-training and our construction of surprisal embeddings. We do not mask any tokens in the input. This maintains consistency between the active learning and fine-tuning stages. Now, let’s see how ALPS samples data. These are sentences from the PubMed training dataset, a sentence classification dataset for medical abstracts. Notice that the red sentence is harder to understand while the green one is short and easy to comprehend. Given these sentences, we compute surprisal embeddings like before. Remember, only 15% of the tokens are evaluated with masked language modeling. Notice that the red vector has larger surprisal scores than the green vector. This indicates that the green sentence surprises the model more because the token may not fit within its sentence context. Now, we plot these points and observe the kmeans clusters of the surprisal embeddings. Different sentences can surprise the model in various ways. By using kmeans to cluster the surprisal embeddings, we can separate sentences from the way that they surprise the model. Finally, we take the sentences closest to the kmeans centers as our active learning samples. Some sentences are easier for the model to learn, while others are more difficult. Through sampling with ALPS, the model can understand the task by learning less surprising examples, and also challenge itself by learning very surprising examples. We run active learning simulations on sentence classification datasets. For this video, we will analyze results of the PubMed dataset. For other experiments and further analysis, please refer to our paper. We use a BERT-based model called Scibert, which has been pre-trained on scientific publications. In our experiments, we simulate an active learner for 10 iterations and sample 100 sentences on each iteration. We compare against random sampling, which is known to be a strong baseline, entropy sampling, an uncertainty-based strategy, BADGE, a recent state-of-the-art active learning work that combines uncertainty and diversity Bert-km, which is kmeans clustering of the pre-trained bert representations for the dataset FT-bert-km, which is kmeans clustering of the fine-tuned bert representations for the dataset This is the result for random sampling over the 10 active learning iterations As expected, entropy does worse during cold-start Bert-km doesn’t do better than random. Ft-bert-km also doesn’t make a difference. With Badge, we see a significant improvement in accuracy Our method ALPS performs as well as badge The dotted line indicates the full training dataset. By the end of 10 iterations, ALPS and badge almost reach that mark. Last but not least, let’s see how quickly these sampling algorithms run for one iteration. While Badge may sample the examples needed by the model, the algorithm takes very long to run. For PubMed, Badge takes 70 minutes to run for 1 iteration while ALPS only takes 24 minutes. In our paper, we also analyze the algorithmic complexity of the strategies to show that ALPS is an efficient active learning strategy for language data. 