 all right can everybody hear me this is work for the recording there okay great well welcome to the first lecture of CS 294 158 deep unsupervised learning let me start by introducing the course team so I'm Peter reveal in the background not present here we have Peter chin and Jonathan Howe I think you might never see them but they'll contribute to the class from the background and help shape materials from there then arvin screen of us over here Alex Lee and Wilson young also last year the problem with space in the room was resolved when we released homework 1 well try to do the same thing again so we promised a great homework 1 coming out next week that will resolve all of this all right communication a couple of logistics first we have a website it's maybe not the easiest to remember URL but if you google Berkeley deep unsupervised learning it'll probably give you the website from last year which also links them to the one for this year I said it might be the easiest way to find it we will communicate with you announcements through Piazza please sign up today for the class Piazza and we'll link the web page there too for questions please also use Piazza whenever possible if for some reason that doesn't seem the right way to reach us you can reach us at the staff mailing list and if that's still not the right way you can always individually email us but ideally that wouldn't be necessary because in any one of us should be able to help out if you email all of us at the staff list you'll get faster and better answers all right office hours are all starting next week so no office hours this week if you have questions this week it's just today after class is the moment to caches in person and other questions can be done online my office hours will be Thursdays from 5:00 to 6:00 next door here but starting next week not tomorrow Alex office hours will be Mondays and Tuesdays Wilson's office hours will be Wednesdays and Fridays so every day of the week the result is our to come and find us if you want to little tip for homework that ta office hours are going to be more informative so if you have detailed questions about things you're coding up for homework I highly recommend to go to Alex and Wilson's office hours you're welcome in mind but they're going to be able to help you better for anything else any of the office hours should work out fine any questions so far okay admission to the course so right now about 50 of you are actually in the course and about 50 of you are wait-listed or maybe just here without being wait-listed if you're on the waitlist or hoping to get into the course the course website has a link to a survey which will also post on Piazza fill out that survey and by the end of the weekend you'll hear back from us okay and then you'll know for next week maybe you are not admitted into the class because we don't see the right fit maybe you are admitted and maybe or something in between not sure that will exist but keep track of any responses you get to that service so you'll know next week whether you're in or not syllabus for the course we have a lot to cover because deep on surprise learning is a pretty big field and also a fast-moving field today is just intro so today's lecture shall be pretty short and pretty atypical of other lectures lupron go for about I don't know an hour maybe a little less than an hour to give some logistics and motivation for why this course is going to be at least interesting to us hopefully to you from that onwards the lectures will be very different we'll have for example next week aught aggressive models will go in great detail how they work and then Ashley will release a homework that topic that will be due two weeks later and this process will repeat we'll look at flow models both of these are essentially ways to learn probability distributions that model data you have collected and we'll look at latent variable models another way to model parabola distributions will look at implicit models such as ganz that will actually require almost two lectures and we'll also have some time then to do final project discussions so we'll present some ideas maybe that you might want to think about for your final projects you might want to throw out some ideas for feedback from the rest of the class and your ideas and so forth then we'll switch to self supervised learning where we don't necessarily generate the data like in a probabilistic model but there is different ways of learning representations from data that we'll see there that'll also take two lectures then we'll actually do a bit of a recap of everything we've seen so far because sometimes we'll have seen many many methods that are kind of doing the same thing like they're learning a model of your data and what are the strengths and the weaknesses of all these methods and how come we actually need to cover so many of them why isn't there just one lecture on the one that is the one you should always use well it turns out that's not clear yet many different strengths and weaknesses and we'll go over those then Spring Break week so no lecture then semi-supervised learning and Unsworth distribution alignment then we'll look at compression which is a very important application of unsupervised learning a little bit of hints towards that today also then we'll look at language models so we'll have a debt again a lot of the ideas we talk about will apply to language to images to video to other data but especially in language has been so much progress in the last year they want to dedicate a lecture just to generative models and salsa bars learning for language then there's a midterm that might be a surprise if you looked at last year's website it's a new thing I'm excited about it I hope you're excited too so here's them well we'll say more about the midterm later just be excited for now then we'll look at representation learning in RL which is the last lecture and then there is our our our week that's I think it's something reading resting and recuperation week or something I think most people study and do projects and then final project presentations will be during finals week just in the same slot same room as we have lecturing so also be Wednesday 5:00 to 8:00 here final project presentations and final practice reports will be due that week - all right so homework will have four homeworks in the class one on our aggressive models when a flow models when a latent variable models one on implicit models / gans the first one will come out next week you'll have two weeks at that moment the next one will come out and this process will repeat eight weeks in a row at which point will be through all four homeworks homework policy you can discuss your assignments with other students in fact offer you learn a lot by discussing them so we encourage you to discuss but you must cut up and write up your own solutions so everything you code up must be you coming from your head not from just somebody else dictating it to you or looking at their code you can discuss but not copy anything needs to be your own thinking going into it late assignments of course there's always things that can come up you get a budget of a total of seven late days you can use them anytime you want but seven total max for for any single homework so homeworks will be due on Tuesday nights so somehow you run out of time couldn't get it done you would have worst case till Saturday night if you used full for late days on that homework to get it done midterm a little more about that that will be during lecture slot so the time should already be good for you topics everything covered including the lecture before the format is a new format we are kind of tried out in CS 287 last semester and worked pretty well there hopefully work well here too the idea here is that as we go through many concepts we'll make a document probably about 20 pages and each of those pages will have a question and answer to that question so it might be something like derive the racial lower bound for a man from VA ease and then maybe some follow up question and that would be one page and then you'd be expected to be able to drive that and so you'll actually know the questions ahead of time you'll know the answer is ahead of time that we expect you to to write so it shouldn't be a stressful event but it should give the opportunity to actually study these materials and if somebody I don't know you're walking to somebody at a conference and you know they ask you Oh any chance you can explain V to me you can actually do it you actually have remembered what it is so that's the idea here there's going to be about 20 concepts that we think you should just know and be able to put on a whiteboard anytime for anybody else and explain and that's what we're gonna put into those roughly 20 pages and again there's going to be no other questions no surprises you'll know exactly what's coming final project scope ideally you explore and push the boundaries in unsupervised learning for example maybe some proposal or evaluation of new algorithms architectures investigation of an application of unsupervised learning maybe some benchmarking of unsupervised learning some play application the compression may be studying synergies between a service learning and other types of learning and so forth those are just examples not limited to those directions but just to give you an idea ideally cover some interesting new ground not that by the end of the class you'd have a conference paper but that could be the start at least you have uncovered some interesting ideas where you see signs of life that maybe this could be worked into a paper in the future if you want some ideas for topics staff input we're very excited to talk with you about that of course you come up with your own ideas we encourage at least to try to come up with your own ideas I would say one of the most difficult things in research is coming up with your own ideas of what to work on so you should at least try and work that muscle to learn to come up with your own ideas but we're also very happy to bring some ideas together pass out ideas that if you're not sure what to work on well the main reason she was so excited by teaching the class is to see more work in unsupervised learning Berkeley for example last year when my favorite projects come out of the class became nervous paper and it was three come by Oh students who since you start working on representation learning for proteins set up a benchmark for that and became a really nice newspaper and this kind of got seated in the class that's where the collaboration started and we all work together on I mean the instruction team and students work together on getting that into a paper and that's really I think the most fun part of the class see what you come up with as new things new ideas and hopefully you see a lot of unsurprised work at Berkeley and so when when you are working on things feel free to come by whenever throughout the semester the office hours or ask questions via Piazza and so forth now cheer about next step you say okay this is the next step I'm thinking I should take it's gonna be a lot of work do you think this is promising or can we brainstorm a bit about what the right next step is or maybe you have some results that you're not really sure what the next step should be just come talk to us this is really fun for us so don't hesitate to take advantage of our time to discuss timeline March second year project proposals will be due it's just a one page description goals for the milestone and you'll submit it through a Google Doc that Google Doc will allow us to give comments and within the week after that we will have given comments we left together iterated on your proposal and by the ninth of seven days later there should be a project proposal in that doc that we agreed upon with you is interesting the right scope for the class and so forth then there'll be a milestone in mid-april just three pages giving an update on progress that you've made so far and this all an opportunity to get yet more feedback from us he'll again put this in a Google Doc and we can just comment the way at it and make suggestions hopefully that can help you make more progress from now onwards and then another four weeks later roughly project presentations and then reports will be due well presentations on Wednesday reports to you on that Friday I forgot to put on this slide but you can team up for projects actually would recommend you team up I think often more interesting ideas come from not working alone working with somebody else so you can have a team of two or three for your final projects any questions so far great grading logistics homework sixty percent 15 percent on each homework 10% midterm 30 percent final project do we need to attend class well there's no hard requirement to attend class there's nothing in the grayness says you need to attend class but we would very highly recommend you attend class why it's a great opportunity to get to know other students at Berkeley working in defense of ours learning people are learning is still a field AB hmm okay it's still a field where it's not nearly as much work happening in the field as could be happening and that means that you're not gonna Hatcher aliy run into other people on the street that are working in deep on surprise learning but if you come to this class you are gonna run into people who work on deep unsupervised learning and I think that's going to help you for a very long time much possibly for many of you even more so than exactly what you learn in the class so it also helped with the community building around defensive as learning and also because the class is kind of late in the day we'll be serving pizza every lecture except for today as a sorter lecture every lecture middle of lecture we'll have pizza we'll have a kind of 15-minute break wherever I can kind of get to know each other chat and that way build more community and also make sure your fad morning second offering of the course once a core is only offer for the second time there's going to be some rough edges bear with us and please give us feedback especially if let's say you start the homework early I hope some people are here start homework early and then something doesn't seem like as we explained it let us know as soon as possible it is possible some things are not that clear that's it in this class we'll give you very little starter code so it's less likely our starter code will have any mistakes ok that was all for logistics let's pause here and see if anybody has questions about logistics before we start on unsupervised learning yes it's a class webcast yes classes webcast and usually should go online the next day I think it'll just be a private link just for the class for the time being but everybody is registering the class should be able to access the webcast other question good question yeah it might be that in some university systems it still gives a different location but Ashley's gonna be here all semester yep any hard prerequisites for the class you you should have done some implementation in deep learning before it doesn't have to be the unsurprised learning but at least deep supervised learning which is a simplest kind of deep learning problem the first people would mostly learn about so for example Berkeley's 182 the materials covered in there or the claws on Drakkar path originated at Stanford that's also available online those kind of materials will be very helpful if you work through them there's also actually enterings deep learning AI if you if you work through all parts of that course and do actually do the assignments and carefully think through it you should be ready if you've done none of that then you probably want to do it soon to ramp up yes by the end of the weekend or at least you'll get notified whether you're admitted not admitted possibly or whether you're kind of borderline case that might take slightly longer but you'll get something from assuming you're on you filled out the survey you'll get a response from us by the end of the weekend yes it's hit again absolutely lectures ending early today not sure how early yet but earlier than usual normally it ends at 8:00 and so today it's not going to end that it it's gonna be well before 8:00 actually one other thing logistical thing that might be relevant especially to people who maybe are gonna ramp up in the next week one new thing we're gonna do this year is actually we're gonna release our coded solutions to the homework so after the four late days have expired so nobody can submit anymore we will release our solutions and because we think that's a really good way to learn you've worked in it hard and then you can see well what do the instructor solutions look like and compare notes you might have better solutions and please let us know if you got something better that we can use in the future but at least it gives you a reference of what it could have been and you can compare notes now we will not require you to use any particular framework you want to use tensorflow pot torch it's all fine but we're going to release our solutions in pi torch so if you want to compare your code with our code most directly you probably gonna want to work in part torch also because that will give you the most direct way to compare yes that's a good question I don't have any media to answer for that let me think about that in principle I don't see an issue is that the the advantage of only us putting them out there is that we can take him away before we offer the class again and it's harder to reach out to everyone when we offer the class again to take him away but then if you'll change it so quickly so maybe about next time we offer it enough things have changed then doesn't not so sure yeah maybe hold off for now and we'll think about it some more any other questions are you able to see things from over there okay great so what is deep Enterprise learning well it's about capturing rich patterns in raw typically sensory data with deep networks in a label-free way label-free is in contrast to supervised learning and super is letting you go from an input let's say image to a label cat or dog and as far as learning you would just be getting the image not the cat or dog label we're kind of split into two cateresa of unsuppressed learning generative models are models where you are able to recreate the data distribution you are essentially able to generate data that is logged it in and often even assess the probability of a new data point in Salesforce learning you don't really worry about modeling a distribution or necessarily sampling the data you are just trying to learn a representation inside your neural network that understands something about the data that might be useful for maybe some other task in the future so often it's elsewhere as learning tasks and more puzzle like things and you'll see things like ok somebody feeds an image into a neural net and could have rotated 90 degrees one way and that agrees the other way 180 or left it untouched so four possible configurations and if you feed that in can the neural net predict which of the four things happen to the image sounds very simple turns out one of the most sophisticated or successful self supervised learning techniques so that's what it is why would we care about it let's look at some of the pioneers of the field what they have to say geoff hinton this is from 2014 but this has not changed since the brain is about 10 to the 14 synapses that's a hundred trillion synapses and we only live for about ten to the nine seconds so we have a lot more parameters than data that we take in in our life so this motivates the idea that we must do a lot of unsupervised learning since the perceptual input including proprioception is the only place we can get 10 to the 5 dimensions of constraint per second the assumption here is that I mean it'd be crazy to have a brain that's bigger than where you can fill up throughout your lifetime so we should be somehow using it and so that means we need to thicken this much per second to make that happen and no but none of us are getting labeled data at that rate so it's got to be something else one of the other pioneers of the field Jana Kuhn says need tremendous amount of information to build machines that have common sense and generalize ok that's reasonable and there might be many ways to collect that information he's just saying he's not saying yet we need unsurprising he's just saying we need a lot of information to go into those machines but now if we think about how we're gonna get that information it's a famous picture of a cake and this is a cake where y'all mix an analogy between the cake and the three main areas of machine learning so the on likes to say the pure reinforcement learning learning is just a cherry on the cake and what is referred to here is that in some times volume or mass of what we're talking about in this case just to cherry refers to the amount of data you're getting information you're getting in in reinforcement learning in reinforced learning is just rewards and they're often far apart so it's not a very high through signal you're getting supervised learning is the icing on the cake it's you can give it more signal more data collected that way often then reinforcement learning but it's still in the icing on the cake unsupervised learning is where the real action is it's the whole foundation of the cake and much bigger volume much bigger mass because most of the data that's available out there is unsupervised data and so if we really want to get enough information the build machines have common sense and generalize his conclusion is the only way to get there is to make unsupervised learning really really good and of course the other pieces are necessary you can't complete the AR puzzle without the other pieces but the foundations going to be lots and lots of unsupervised learning and in fact he presents this cake very publicly for the first time in the 2016 nerves keynote he gave since then the cake Scout in the name is called the luck cake and here anybody at the congress talk about luck cake it's this story they're talking about then other people get excited about as far as learning according to some motion is not an official name but they think like what would ideal intelligence be like well they would say ideal and tells us all about compression it's about finding patterns in the data and like if you large amount of data compression and finding patterns is kind of the same thing because compression is okay I see it as data I can describe it in a more compact way because I understand the pattern that's underneath the data finding all the patterns is then what you're trying to do and there is some thing called coma gaurav complexity which essentially says what is the shortest piece of code that can generate your data and if you can figure out what's the shortest piece of code I can generate your data now you've effectively compressed your data for example if it was a sequence of once a thousand once if you can just write a piece of code that says for I you know through one three thousand I put a one that's much short and actually writing all the ones so that piece of code is more effective and hence as a better in addition it was the sequence of random bits that might be much harder to do and you might need a pretty long computer program in fact one that stores the entire bit sequence to be able to reproduce it some of induction is in some ways a kind of maybe an advanced version of a thing a bit of komagawa of complexity which looks at all hypotheses essentially all computer programs one way to think with all computer programs that could generate a data and has a prior over computer programs and the ones that are still consistent with the data allow you to have a posterior over computer programs that could have generated data and sooner you have a distribution of possible explanations of your data this tends to be not very tractable to to compute but at a high level it makes a lot of sense and can be the starting point for other ideas then there's actually extensions of this for optimal decision making agents for RL which is called a ixi and can quit flavors along the lines of if you do model-based RL in principle you're bethere over models could be achieved with sallman of induction and you can work from there all right aside from these threat achill interests why might you care about unsupervised learning well has many powerful applications for example you can generate new data and we'll see some examples data similar to data already existed in the world but now it's new never existed before you can do conditional synthesis where you maybe generate some new speech based on text erode so now just speech but conditional on things that you care about you can do compression we that will see that the better your unsurprised model the better your generative model for your data the better you can compress your data one of the main reasons in AI people care about at least originally cared about and still now a lot care about unsupervised learning and self-arrest learning is then if you do a lot of us advise learning first the hope is that you can then later train that same neural network with a small amount of data maybe RL maybe supervised learning and from a small number of labeled examples do well on a provides dusk whereas if you only had to rely on supervised data you might need a very large set of annotated data so this effectively transfer learning but it's transferred where initially to unsupervised learning and then later you do super hustling it can also be intertwined at times so it's actually some of these things have already had production level impact for example this google's bird is a very recent paper so about a little less than two years old and it's already now underneath the google search engine and it's proud that the biggest improvement to the google search engine in a very long time by just better understanding documents what what essentially is the content of a document because that's what you search for rather than a specific string also on sparse learning as we go through you'll see provides many flexible building blocks that you might reuse in other efforts so it's not just what we cover is relevant for unsurprised learning a lot of what you do in neural nets is thinking about architectures and architecture ideas that play a role in unsupervised learning might also start playing a role in other domains like supervised learning or reinforcement learning so let's look at some examples some pretty early examples this is from 2006 these are class conditional images generated here amnesty images with something called deep belief nuts deep believing that's have gone a little bit out of fashion um not a lot of people work on them these days but they were the first model people got to work to get these kinds of results then more recently free auto encoder can do unconditional digit generation they're not perfect but this is also 2013 you'll see things are gonna improve as we step through these slides then 2014 was when genera virus cell networks were invented by in Goodfellow and collaborators and it was I would say the first time most people working in the field would say halt down from among all these previous images it was even just ditches but it still didn't feel that realistic and here all of a sudden that were faces that we really had a sign of realism maybe the image on the right to Lola's but it was this notion that this might actually allow us to generate realistic images if we can somehow find a way to improve upon this then people start improving up on this and very quickly this was just a year later this is now all bedrooms so images of bedrooms generated by the neural network these are not training images are automatically generated images by the neural network then here at faces generated again this is 2015 just one year before again was invented one year later this is the quality we're getting then 2017 we see how similar ideas can be used to do super resolution so turn a low resolution image shown where's the well the lowers is now not shown here but in goes a low resolution image then you have traditional interpolation schemes and then you have the gam second from the right and all the way on the right the high resolution you're hoping that you would end up with or something close to that you can generate images that do effectively image translation where here horse is translated into zebra done by a Leo Jeffress group here at Berkeley and then say one of the the big coming-of-age things for ganzar kind of really truly coming-of-age was this 2018 result a little over a year ago oh why not play so these are all automatically generated images by the neural network high resolution and the reason sequential it you me just coming one after the other are pretty similar is because there's a latent code going into the network which get turn into an image and the latent code is slowly varying as the latent code slowly varies we see that the image also slowly varies and every step along there almost every step along the way show something quite realistic as the output of the neural network there's even musically where you see here what makes the recent sequence of image so interesting is that effectively had discovered the notion of changing the viewpoint on objects all unsupervised because you could see it was getting different viewpoints on the same object by changing the latent code so are these just interpolations in this video do you remember you want to grab the mic for a moment yeah so the video you feed in the class and then you feel you're feeding a latent variable Z some 128 dimensional vector and the different written variable you change you get different images right so that's how the gentle model works and the deep neural net is transforming that latent code into the actual image and like Peter said it was never fed in information to condition on the pose or like the actual texture if you just feed in say a dog or a car it's not given any information about what specific car so what specific breed of dog it is or something but it's still able to understand like different texture patterns different geometric aspects like poses and you know if it's generating humans it can generate different aspects of the hair or other parts of the face and so on so it is understanding concepts that are not present in the classes because it's seeing a lot of data then from there people went on to just another year later or also 2018 generated extremely realistic faces so these are faces dad are none of people in the real world but they look like they could be faces of people in the real world of course for some of these people can still figure it out at times but is becoming harder and harder and you could imagine that for sure ten years from now I'm probably much much sooner it's gonna be impossible to distinguish a automatically generated face from a picture of a real person in the world then audio has seen a similar revolution so if you look at audio generation here it's going to be going from text turnitin so sequence of characters or words turned out automatically after a bunch of training into speech and so traditionally is dumb parametrically you would essentially look at words or parts of words and you have essentially every word you might essentially look at the dictionary and it tells you the pronunciation of that wording you just used out in sequence tap together maybe with some smoothing between the words and so you might get some let me see if I can get it to go through speaker the blue lagoon is a 1980 American romance and adventure film directed by Randal Kleiser so again it's a 1980 American romance an adventure film directed by Randal Kleiser so there's a very kind of monotonic way of saying what was being said here compare that with wavenet 1980 American romance an adventure film directed by Randal Kleiser the Blue Lagoon is a 1980 American romance an adventure film directed by Randal Kleiser and so witness was kind of the first big breakthrough were listening to sound it actually sounded very realistic it was hard to distinguish the human had read that I mean of course it was your friend you know my mean maybe not their voice but human voice versus computer-generated voice became very hard to distinguish one thing I just want to add to the wavenet point is that these systems how to work when you're using your Google Maps or something right like when it's dictating your directions like so the the generations you saw like maybe you didn't see too much difference but those minor differences make a huge impact when you're annoys surroundings or something so that's why it's so important to get natural speech generation working then here's another example this is from just a few months ago showing it's now possible to generate pretty realistic looking video so not just single images anymore but videos then are a couple seconds long that look like real videos obviously they're on loop here but there are a couple seconds long and they look like regional video segments hi so just like how we saw in mid generation and audio generations work really well one of the probably even bigger success in generative models has been on language so this was the famous char by Andre curve party back in 2015 which was just an lsdm and you could see that it's generating some kind of coherent sentences like like yeah it's it's getting the notion of language the grammar some meaning and like what words going together and so forth and the nice thing is since you earlier people used to work with bigram models and Engram models or n is some reasonable value that you can deal with as far as your building frequency tables goes but the nice thing about Chara and is that the neural net doesn't care about what data you feed in as long as it's some discrete sequence so it doesn't just have to be English where you clearly understand what tricks you need to do to make statistical Engram models work it can also work on a bunch of latex if you throw in a later document and just learn a char and and over it it's actually beginning to generate some mostly syntactically correct latex code that you can actually render and see for instance andrey tried it on algebraic proofs by creating a data set himself of like some archive papers or something so so a caveat is that it wasn't able to compile i think they did do some final changes to make it compile but it was more or less compatible so that's already amazing because this was back in 2015 and it was able to understand anything english shakespeare novels Ladak documents I think Andre also try it on UNIX code and was generating syntactically correct code it was so exciting that even linus torvalds actually wrote some wrote some google+ post about it so that was that was pretty pretty big at the time Chara and then but obviously nobody uses Alice team to Train language models anymore and there is a new powerful architecture called a transformer and openly I basically took it to a whole new level with their GPD too so for instance this is a human prompted by prompted sister initial sequence of tokens or words that the model takes in and it's asked to complete the rest so in here it's like a train carriage containing control nuclear materials was stolen in Cincinnati today its whereabouts or no and now the GPD to is going to complete it because of using its train language model so here's one completion the incident occurred on the downtown train lane which runs from Covington and ash Astron stations in an email to Ohio knows outlets the US Department of Energy said it's working with the Federal Railroad Administration to find the teeth so you can actually see it's understanding that steel is associated with the thief and like and nuclear's associated with energy and Cincinnati is associated oh hi oh and also that so the federal administration has to come in because it's important and you know and and because it's stolen it understands that you need to find someone who's stolen air so it's understanding so many fundamental things yeah so the question is how much cherry-picking goes into showing these examples that's a really good question I like the I can't quantify exactly how much it was for this example but I think opening I when they did the release they tried around twenty five two hundred different completions and pick the most interesting ones but they were also somewhat harness about it where you can see various different completions - and in general there are websites like we shown out there you can actually see different completions yourself by running it multiple times but that's a great question it's not like humans where it's not like you you when you go talk to someone and you have to wait for a hundred times to see what what responses they will give and then pick whatever you want to hear right so the language model is obviously not a human level yet but it's all it's amazing that at least one of the responses is so coherent understands fundamental aspects of the real world like energy and stealing and you know federal departments and so forth and here's another completion oh it's a continuation okay the theft of this nuclear material will have significant negative consequence on public and environmental health our workforce and the economy of our nation said Tom picks the US energy secretary in a statement our top priority is to secure the theft and ensure doesn't happen again so it clearly understands that nuclear is like something really important like it has significant consequences right and the stolen material was taken from University of Cincinnati's Research Triangle Park nuclear research site according to a news release from Department officials the Nuclear Regulatory Commission did not immediately release any information according to the release the US Department of Energy's Office of nuclear material safety and security is leading the team's investigation so it also knows how to complete it with a story right like it's it's coming up with an entire story basically and that's what mostly it had a big controversial release at the time because openly I felt that people could use this kind of firm model you can even call it a technology because news news writers do need to understand what a transformer or what are not progressing water that's right for them as long as they have a website and they just feed in some prompts completing it it becomes a software actually there and then they can actually write fake news saying they can do some fake prompt and then write up a coherent story there there was the the other side of the arguments that humans can also write faking us pretty well like that's really the biggest problem in internet now so so but but the point is you need you do need to take significant amount of effort into generating a convincing faked story right and and and and it's possible that neural nets can do that way better because they just seen a lot more data and they don't have to understand what like how do fool people and so forth so this is really the state of language models now it's it's probably even better because it's very likely the models are scaled even further now but this was vey this is you can see the significant difference from Chara an end to this right from a few sentence from one sentence or a couple of sentences to an actual news story and finally yes so again it's stressing on how safety is so important as far as nuclear goes and environment so here's another thing which I really like this is also pretty interesting to see because the model doesn't actually need to be grounded in what reality is it can actually figure out a story even if you ask give it a prompt as if there's this alternate reality right so so recycling is good for the world and then you say no you could not be more wrong so now you want the model to actually argue for recycling being bad for the world and it's actually able to do it pretty well it's bad for the environment it's bad for a health economy I'm not kidding it's it's not good it's destructive it's a major contributor to global warming and contributes to obesity and diseases like cancer and increases the cost of a product and turn the price of everything that's made of the product and it's not good for the nation pay tremendous price for the privilege of having the world's most advanced and efficient recycling system it's a colossal waste of time energy money and resources and of course there is no information gained in any of these arguments but it's still able to put in a cohesive paragraph that might appeal to the layman right and that was open as plaintiffs more like hey this is already crazy and then so so so here's the website doctor transformer that we can actually go check out yeah do you guys have any suggestions for some prompts sorry what okay let's try let's play okay maybe we should try the other one okay so it's not that good we probably need to try again okay so this got the sense like the sentiment I guess the respect was Swift is greatly exaggerated she does want to be she doesn't want to be what I think she is it's very cool sure anyway so let's try again so he thinks it's happening this year keep saying that also actually so we really we're now a little over a month of it so you can actually see it's also getting things like wired I triply spectrum robot you know it's like a huge dictionary lookup it's sort of nine massive patterns yeah yeah so that's a good question so question is what text data it was trained on because it's clearly really good it's trained on so what open-eyed did is they scraped a lot of reddit links and anything that had a karma forward three or something like that they just took it and if it had a link to some webpage they also took the content of the web page and then they created this massive data a text called web text but some people have tried to recreate an open source and it's actually available so so that that's what they trained on but I think that was still order 40 gigabytes which is not much in Internet is a sheet we can you can say it's roughly terabytes of text right so it's not much yeah anyway okay that's enough fun with language models so let's look at compression which Peter talked about obviously we're not going to get into the details of how these models work that's the next lecture but compression basically means you're trying to you're trying to compress data in a lossless way such that you store away less information right and then the decoder side recreates that information so generative models especially auto regressive density models can actually learn probabilistic models of your raw data a very very high dimensional data and you can report log likelihood scores and the lower the log likelihood score the better a model is at compressing the patterns present in the raw data and there's a Seifert and data set which is a which is a 50,000 images 32 by 32 images data set which was created by alex crusades Keylong back and people usually benchmark on that and on that particular benchmark you can see that the bits per dimension which is so there are like 32 times 32 times 3 as far as the data dimension goes and each of these numbers is an 8 bit number right so because a pixel is 0 to 255 so you need 8 bits to store if you had no information about the data at all if you had no understanding of the patterns in the data at all you would allocate 8 bits for every pixel and so you would store 8 bits per dimension where there are like 32 times 32 times 3 dimensions but if you had if you did some more work and actually looked at all the images and saw that there's a lot of common patterns across pixels at every spatial location and across images then you don't need to allocate 8 bits you can allocate way lower and that's what compression gets you and that's what I'm supervised nothing gets you and if you look at the state-of-the-art models it all started with pixel CNN which reduce it to three bits per dimension four all the way from eight bits and now you can see that the models are gone way better it's like 2.8 bits per dimension so even these like Oh what is this is this point do and like a lot of effort but but it's actually really hard because you only have fifty thousand images to train on and the death set is different from what you trained on so c4 is like really really hard so you're trying to create models that are really generalizable and n week eight is similar similar to c4 it's a small language compression benchmark and you can simply see that self attention and transformers have had a huge impact there and getting bits per dimension of 0.99 emission at 64 by 64 is this a down sample version of the original emission idea said so you can argue that if it works for C far should also work on a lot more images because there's a lot more patterns to mind from and that's turned out it turns out to be true and similarly right right house pixel seen and started off with three point five seven now it's all the way down to three point four four so that's the compression story as far as Auto regressive density modeling goes and we learn a lot about how these models work what are the details needed to make it work and Wilson and Alex have prepared an excellent homework for the pixel CNN story so you you learn a lot about how actually the models can be how you need to implement them what are the details you need to take care of and so forth and one one point again which I wanted make is the same models the same class of techniques the code will be more or less the same except for the data loader and the output loss output distribution the same thing works on audio on images on text different image datasets so that's amazing because if you were to compared to compression which is not distribution of air like JPEG it the set of hacks needed to get whatever four bits per dimension in JPEG there's a lot of image specific hacks there's a lot of video specific hacks like for instance JPEG works by converting images into a bunch of contiguous blocks and then taking the Fourier transforms a discrete cosine transform and so forth and for videos that the motion vector on videos but these methods the generative model approach it does not take into account all this inductive biases at all and that means that in future if you were to ever think of compressing more complicated data sets like point clouds or or Babli relevant for augmented reality or virtual reality then it may be easier to just take the best deep learning pipeline rather than creating a new hockey JPEG of sorts for that particular modality right so that's why progress and compression is really really important and that said we don't want to make you think that generative models are compression technologies like if you want to actually care about real-world impact image compression or video compression it's better to focus on the lossy version rather than the lossless version Peter will we're all like or in future we cover all these what is the difference between lossless and lossy house--it lossy lossless and so forth but imagine lossless being not losing anything about the data that you stream whereas lossy is like you're losing some information like high-frequency aspects and these techniques work on really large resolution images like JPEG like JPEG JPEG 2000 and and all that but even in that space deep neural nets are having a huge impact there is a startup called wave 1 which is applying learn ed compression techniques for lossy compression pipelines both in high resolution images as well as videos and they're using a lot of ideas like taking the most learning some up learned optical flows and all that and and you should check it out if you're interested in compression si as far as technology goes and but but it's not it's not for unimaginable that maybe we'll have a great generative model that works even in the lossy scenario and without to adapt the current pipelines for that so that's also an interesting direction for future work and so there was the other aspect that Peter stressed on the initial motivations for the class which is downstream tasks and so far you've seen Lange language modeling that the kind of work that could generate a lot of text but what if you wanted to take that language model you train and apply it for something else and or openly right before the GPT they used to Train language models with by turn and so by Dallas TMS which is a separate kind of Ellis tiems and one of the hidden dimension in the lsdm was actually able to discover sentiment the aspect of sentiment without actually having any label data for text to corresponding sentiment so if you look at it green it's more positive and red is more negative and it's and this was just like the neuron that is running through this text and just tracking this one hidden dimension that's responsible for that sentiment and you can see how it's currently able to capture that the like you know the aspects like beautifully developed and all that is like really positive whereas movies is dreadful it's like really negative so that that's another aspect that you yeah so the question is how do how does it understand this they do not then the nice aspect is they do not use any kind of signal they just train the language model and that language model would have captured some features in its lsdm hidden states now you throw a new data at it and you just track the hidden state it's actually uncovered this sentiment feature because for that means that to generate long-term schoo has a language it's important for you to understand sentiment and therefore it captured it and now you can use it for a downstream pass yeah yeah that's why they call it the sentiment neuron so I think char RN and from karpati also had some other kind of visualizations so the more like apart from sentiment sentiment analysis probably really like not the most hard a hard NLP task a lot harder tasks like like benchmarking language understanding like given a sentence and another sentence you're trying to predict if one entails the other or they contradict each other or they're neutral with respect to each other or or you just given a sentence you're trying to predict if it makes sense or not and and so there are a lot of these benchmarks created by language language folks and none of the top ten has anything other than a transformer apart from the human baselines and if you look at the human baseline the average do scores eighty seven point one and massive transformers either bird a bird kind of models they're all all the way up to ninety this doesn't mean that oh that's it like we saw languages we were already better than humans that's not the case the benchmarks were created for when they were created the current best models that were using LSD aims and attention they were not as good as a human baseline so I grew human based on 87 just Elliston baselines were like very low like 60 or some things like even lower than that now with all the pre-training unsupervised retraining and transformers it's gone all the way above the human level but that so they are constantly going to create new benchmarks like they created superglue and like we keep creating more benchmarks and and that way we also work on research that progress is more on language understanding but the story is pre-training which is birth we will talk about it later but the way it works is it just takes in a text it hides some portions of it looks at what's remaining and tries to predict the hidden portions and trains like this sonic map all Wikipedia a lot of internet data and then it's learning great for feature space and then you take it take to take that particular transformer and you find unit on any of these downstream tasks and it's able to beat all the existing baselines now what is the case in computer vision so before that I want to mention that that was a particular bet made by between Gitano and Alyosha at Berkley here obviously it's not relevant because the Turner's timeline for that is September 2015 so it's like we're already like way past the date but the bet was that there is this model called our CNN which is an pipeline for object detection in computer vision and there is a benchmark called Pascal where the amount of label data available is very little so the only way in which you can get object detection to work there is to use a supervised pre-trained checkpoint for the rest net backbone that you use and Jitender purpose to made a bet with Alyosha that that you cannot have any backbone without supervised learning that works as for the supervised learning and for a long time this bet was true like even last year when we thought the class it was true but it's not it's no longer true like the best supervised learning performance if you use even really really deep model like a restaurant 152 you can get this metric called mean average precision up to seventy four point seven on the Pascal benchmark but now all the models that were trained with self supervision where we're never getting all the way up till seventy four point seven they were all stuck with like even if they tried training all possible self-supporting objectives in one model it was only able to get up to 70 point five there's a significant gap and the latest methods like momentum contrast of moko from Facebook and CPC which is from deep line both these methods are able to beat the supervised baseline of seventy four point seven but moko gets only four point nine slightly more and simply we do get seven six point six though moco's using a fifteen layer backbone so obviously it'll get better with the deeper backbone this was like this also means now that self supervised free training can not just happen language were also envision and by help I don't I don't mean that it'll just make your model slightly better can make significant improvements especially when you don't have a lot of label data as in Pascal so just like how bird change language I think CPC or contrast the pre-training that's on the it's it's it's on the path to changing like vision benchmarks as well and just like how people made strides in harder language benchmarks like like like squad which is question-answering and blue which is language understanding envision I think it's possible that an instant segmentation detection pose estimation and so forth backbones from South supervised learning would most likely be better than supervised learning and that's that's already going to have a huge impact so as a summary as a conclusion that gelato Bell is like if you ignore the timeline it's it's been solved very recently so yeah so in summary unsupervised learning it's a rapidly advancing feel thanks to compute thanks to very very good deep learning engineering I can't stress that even further it's like well the most important aspect in all these successes is a lot of a lot of focus on low-level details and and in class we hope that you take that back with you because that's the most important aspect in any of this research like Bert or CPC or generative modeling a lot of the work goes into how you write the architecture how you write the data loaders what kind of augmentations you use what is the learning rate square use like everything so we want you to understand those details by working on the homeworks and try to understand like how even small changes can make huge differences and there's also a lot of people working on it now it's no longer a niche field and there's a lot of good available datasets like image net Wikipedia and so and all that for different modalities and it's not just an academic topic anymore it used to be very academic Hinton was working on big belief nets but or even when sentiment you understand like not a lot of people were working on unsupervised learning but now it's already like a production little impact like it's already in all the technologies we're using every day and vision will also have a huge impact like how it had in language and so it's the right time to sort of focus on how these models to various class of models work so this most important thing is what is true now me not be true even a year from now so when we like when we taught the class last year salsa boys training was really bad for vision I could say the linear classification scores or the down stream performance they were not as good as supervised and the most people were like oh no it's never going to be supervised and but now it's actually being supervised not just in the low data regime but also in the hydrator regime and that means that even if you argue that you if you have a start-up and you just want to hire people to annotate more data for you it's possible that the final peak performance you get will be from pre training and then doing supervised learning rather than just doing supervised learning and all these are really working very well and the best it's all it's the best time to learn these ways class of models like language modeling in my generation pre training for different modalities it's the best time to learn like these models really well and you can make impactful contributions in like ways way so you can try to improve these models which is really hard because these models already working so well but you could but but but but you could also think of taking what what the aspects the key aspects that make these models work and put it in some other domain where they haven't seen the successes for example you could you could try to make an huge impact in RL where a pre training doesn't work yet or you could have it you know like Peter pointed out one of the best projects left last class was last last offering was using it for protein folding benchmarks or or general benchmarks about protein understanding and like these are domains where even deep learning still new and so you can instead of directly going for the supervised learning solution you could think of just going for the unsupervised learning because it's not likely to work better in the long and finally I think fundamentally autoregressive density modeling flows ways or applying all kinds of unsupervised learning or cell supervised learning for our L that's a huge room for improvement like like for instance the same kind of statements we were able to make about language modeling or language and vision pre-training we cannot make for these these topics because there is no huge paper that just says oh this is done it's over or like something like that so that means that this we must go for research to be done there and you can and that's me like this class can be useful for you to do that so we hope that you can take what you want to take from this if you're interested in your own applications you can study the general class of techniques and then figure out how to apply them or if you're just interested in making progress on the general techniques you could try to like pick one of them and try to make improvements on that and the projects are like a great great great place to do that in the in the class and hopefully the homeworks also teach you how to do that very well thank you any more questions for us in person come find us in the next few minutes up front here and otherwise see you next week 