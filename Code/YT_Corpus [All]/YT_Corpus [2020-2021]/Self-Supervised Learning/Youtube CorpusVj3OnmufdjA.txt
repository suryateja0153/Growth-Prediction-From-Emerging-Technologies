 Unknown: Yeah, hi, everyone, I hope you had a good week and work through all the NumPy material, because this week we will be using it, we will be talking about data processing and machine learning with scikit. Learn, I think it's really useful to do the coding stuff like this one, in this lecture upfront, because later in this class, we will be using it extensively. Because I really think it's useful to learn about concepts of machine learning, but then also try them out in practice and use them and explore them using code examples. Yeah, and with that, there's not that much to say for this lecture, except you maybe saw that I uploaded homework one, the first real big homework, which will be testing you on stuff that we covered in earlier lectures, like supervised learning concepts, and so forth. But it will also test you on the code examples and NumPy and ask you to tinker around with the K nearest neighbor algorithm to get a better feel for how NumPy and psychic learn work. And with that, um, yeah, there's not much to say, watch the videos, do the homework, do the self assessment quiz afterwards, and stuff like that. And while you're doing that, please also don't forget to have some fun once in a while, cause Yeah, my favorite season of the year just started fall, I really like fall in Wisconsin, because it's finally getting colder, and the leaves begin to change. It's really beautiful outside and stuff like that. And I was also super excited. Last weekend, I already went to a pumpkin patch, you can see I got some pumpkins that need to be carved. So with that, let me get started with the lecture. So I can then get back to my little pumpkins and get them carved for Halloween. So yeah, let's get started. Yeah, we already arrived in part three of the computational foundations lectures. Just to tell you what this lecture will be about. Here's a list of the topics we're going to cover. So first of all, we are going to talk about reading in a data set from a tabular text file, such as a comma separated value file, or CSV file, which is the most common file format for traditional machine learning artists. Then we will talk about basic data handling like getting the data into shape for a while machine learning algorithms and training procedures. And then we will be talking about psychic learn machine learning with scikit learn. But before we do, so, I wanted to briefly recap on Python classes on object oriented programming. I gave you some exercises to prepare yourself for Python, or just understanding Python better. However, I think it's just useful to briefly recap what object oriented programming is and what Python classes are. Because the second learning API for machine learning is heavily inspired, or is heavily relying on object oriented programming. So in order to understand how scikit learn works, it's kind of necessary to understand object oriented programming. Then, we will talk about preparing training data using the psychic learn transformer API. And we will also talk about defining psychic learn pipelines, which help us chaining different operations such as, you know, preparing the data set like data sets, scaling, normalization, and then also dimensionality reduction, classifier itself and so forth. So we can define a very efficient training pipeline that connects many aspects of the machine learning workflow together to make things more convenient, which is one of the big strengths of Cyclone. Also, I should mention this lecture, I decided to make slides again. So in the last two lectures, we talked about things using Jupiter lab. And I like Jupiter lab a lot, but I think it's sometimes easier to explain things if I can use my pen here or my pencil. So with this pencil here, I can just better annotate certain code examples. So what I did for these slides was I made screenshots from the Jupiter lab, or Jupiter notebook, and I will annotate them in this lecture. However, the whole code notebook is also available on GitHub. I just uploaded it to GitHub. And I also added some explanations there. So you can consider this this document as an optional course notes or lecture notes document for this lecture. Yeah, just a brief recap, where are we in this course right now. So we had an introduction to machine learning covered cane and were already showed you a little bit on how scikit learn works. Then we set up a Python learned about NumPy and scientific computing. And now we are going to we are going to cover data processing machine learning with scikit learn and then in the next lecture, we will get back to core machine learning concepts like decision trees and sambal methods and model evolution after what So while this is the last part in the computational foundation lecture series, Doesn't mean that we won't be using code in the future. However, you can consider this as like the foundation that you can use to play around with concepts we introduce later on. And you will also be implementing certain things as part of the homework. So in that way, the computation foundations are really like the tools that we will need to explore the concepts we cover later in this class. Here is again, the overview of the typical machine learning workflow that we covered in lecture one, I don't want to go over the details again, because that's what we did in lecture one. However, I wanted to highlight the main aspects of this pipeline. So there's the pre processing part, where we read in a data set, split into training and test data and so forth. And then we get to the learning part where we use this data set the training data set to fit a model using a machine learning algorithm. And then later, we evaluate the model using the test data set, and then we can use it in the real world for making predictions. So in this note, in this lecture here, we will be talking about the pre processing steps. And we will also get to the learning steps. Some of it will also include the evaluation. However, we will talk much more about that in future lectures. So here, we will briefly talk about evaluation. And when we discuss pipelines, and later on, we will go back into more detail about model evolution. So the main focus here is on pre processing and learning. And for that, let's start at the very beginning reading in a data set from a tabular text file, such as a CSV file. So for this lecture, I'm using the iris data set quite heavily. Again, I know not everyone is very excited about working with Iris flowers, because the data set is a little bit old and not super exciting. However, I think there's an advantage of using a simple data set. And it's that it gets out of the way, we don't have to spend much time thinking about the data set and can think about the methods here, because this lecture really is more about understanding the computational tools rather than getting distracted by discussions of the data set. So just briefly recap what the data set looks like. So we have 115 flowers. And we have three classes, Iris setosa, Iris, versicolor, and virginica. There are 50 flowers of 50 flowers from each class in this dataset. So we have 50 flowers from each and the features, we have the measurements, so the length and width of the CEPA, flowers and petal flower. So we have sepal, length sepal with petal length, and petal width. So we have 150 times four dimensional data set here, in plus, of course the plus the labels, and labels. Personally, before I start working with the data set in Python before reading it in, I usually always use my terminal to explore the data set, or at least take a look at how the first lines have a data set look like and also the last line that can be useful, just to get an overview of the data set before we use any sophisticated tool. So we know which tools we have to use, for example, which options we have to choose. And this is super fast. And you also note that in we can do that directly in Jupiter lab. So we can use this exclamation point parameter to execute terminal commands. So this is a terminal or shell command. And this head on command, or it's actually a little program in Unix and Linux. For terminal, what it does, it's returning the first 10 lines of a data set of a file. so here we can get an idea of how the data set looks like. So if we take a brief look at it, we can see there is one row that specifies the column names and so column names are known. It's not always the case that we have the names of the columns included in the data set. Some data sets don't include the first row with the column names. We can also see we have an ID column, which just enumerates the different training examples here. And the last column is this label column in a string format. And then we can see there are four columns with your floats or decimal numbers, real numbers. And they're all comma separated, it's also important to know. So there's something we can learn by just doing this quick head command here to just get an idea of how the data is that look like looks like usually also in practice, um, use tail, which gives me the last 10 lines of the data set. Then my favorite library for reading in the data set is pandas. pandas is short for Pennell, data's. So I, I've never seen that written down somewhere. But I heard or listened to a podcast by with, with Kenny, he was it was an interview where the podcaster interviewed Wes McKinney, who was the original developer of pandas. And he mentioned that he named a tool after this acronym, panel data is okay, so pandas is a data frame library. So it allows us to work with data frames, can think of a data frame as a more sophisticated version of NumPy array. But I bet that most of you have worked with R. So you can really think of this data frame like our data frame. But you can also have mixed column types and so forth. So now let's use the data frame library pandas to read in our data set into a data frame. So on this one function, when we import pandas that is called read CSV, and the read CSV is a very powerful function that can read in a CSV file super fast, it's evolved over many years, and people made it really fast. It's one of the, I would say best ways of reading in a data set. So here, because our file has a very nice form, so it has, this is the form where we don't really have to pay attention to anything like function arguments, we can just call the function with a filename, and it will read it into a data frame. So this will be our data frame object. And the data frame object also has a head method, which is kind of similar to the head method of our terminal command here are four terminal, however, it's only giving us the first five lines, there's a way you can get the first 10 lines, but it doesn't really matter, we really just care about the first few lines. So we can see that the data set is read incorrectly, if you omit this part, if you just call the F, it will show you the whole data file, which is also okay. However, sometimes the data set is very large. And we don't have to see the whole file. Here, it's more about seeing whether everything looks okay, if it was correctly passing the file. Otherwise, if it's not, okay, we'll get some weird output here. So we can see here, everything went correctly, because we have all the column names read incorrectly. And then we can also see, it gave in an additional index column, we could have specified here, an argument index and a score, call. And we could have used the first index or column index for that, and then it would have used this ID column for these IDs, however, doesn't really matter here. We don't really care about the ID here. Um, yeah, and that I mean, that looks great. So everything was written correctly. And you can also see, you can have mixed types. So here, for example, these are integers. I mean, you can call some functions to check the types. However, we can already see that there's no decimal point. So this is an integer. This is a float type, can see the decimal point. And here, this would be an object or string object type. And yeah, there's also similar to NumPy, a dot shape attribute. So if you use the dot shape attribute, you can see the dimensions of the data frame. So in this case, it's 150 times six dimensional data frame, like 150 rows and six columns. Yeah, like I mentioned, there are many options for the read CSV function. You don't have to memorize any of that. However, if you have problems with reading in a data set, it's worth exploring those. For example, there's the Sep here, which stands for separator. In our case, we are working with comma separated files. So we put a comma here, whoops, a comma here, but it could also be for example, slash t if we work with tip separated files, and so forth. So it really depends on how our data file is formatted. Then we have, for example, in parameter for the header, so it's automatically set to infer so it kind of guesses whether we have a column header or not. If that doesn't work, for example, if we have a file that doesn't have this role here, so if this is not included, and we have problems with reading in the data set, we can try, for example, setting this to none. And then should usually always work the index column I mentioned, we can set it to zero in this case, because we have action index column, we can also just set it to none. So it will create an index. starting at zero, it doesn't really matter in most applications, if we aware what the index is, but in this notebook, for example, in this lecture, we are not going to work with index i think. So there's also use call, which means use columns, specifying which columns to use. I will show you an example where on how to use that in the next slide. And there are other ones like I'm skipping rows, how many rows to skip, and the number of rows to read in total, and many, many other options. Yes, so what do we do if we have a very large data set that is exceeding our computer memory. So here's an example code that I wrote, like, five years ago or something when I was working on very large molecular property files. So I had a data file that had like 100 million lines of molecule data. And it was like molecular properties, like the weight of the molecule, the number of hydrogen acceptors, and donors and things like that. And I had a very small computer memory, I think I had a MacBook Air with like, four gigabytes of RAM, which was not really much. So in that way, I had like, challenges on working with a data file, especially if I wanted to work on my laptop. So there's a way for example, you can use the arguments and pandas to work with large data sets. later on. Next slide, I will show you some other ways. But this was one way of doing it. And also note that notice there is some iterator function that is also useful for that, I think. So. But the way I was doing it here was I was defining a chunk size. And the chunk size was how many lines of the data frame or the CSV file I wanted to read in one iteration. And then also to make the file smaller, or the data frame smaller, I only read in those columns that I cared about. So that was, for example, the molecule ID. So the name of the molecule, the partial charge of the molecule, whether it's currently purchasable or not. So drugs, no means basically, whether you could buy it or not. How many hydrogen bonds except us and how many hydrogen bond donors. So I think Originally, the file had like 10, or more columns, but I don't. So I think that the file headlines, at least 10 or 15 columns, but I didn't need all of the columns. So I only specified five I cared about. And then I was also getting the total number of lines in the CSV file, how many rows it had. So it was a little bit too, too large to open it, let's say in Excel and count or let's say, flip to the last row. So there's a way you can do that in Python, you can just iterate over the lines like using, like opening a file, and then using read lines or just iterating over the files, line by line. However, that took also a long time if you have 100 million rows. So what I did is I was just using this terminal command, which is b c minus L. So sorry, WC means l. w means. So WC means word count. And minus l means lion. So this will just give you the number of lines. And then of course, the filename, which was, let's say, my, oops, my large dot CSV file. I could have executed that in pandas directly using this exclamation mark thing. However, the problem with that would be it would be printing the number of lines, however, I wanted them assigned to a variable. So I was using this sub process module, which is spawning off a Python sub process in, let's say, the terminal using check output. So that is executing a terminal command, which I put here. And then I was just splitting the output, getting the first menu, which is basically the number of lines. So here I was just using this terminal command to get the number of lines very efficiently. And then I was iteratively. Reading the CSV file, I was actually reading this into an SQL light table but doesn't really matter here. On let's say, I was just iteratively, processing the CSV and in each iteration, I was doing something With a data frame in the data with the data and data frame, so in each iteration, I only read 100,000 lines, all of the 100 million lines. How did I do that? So I had this range iterator here, I set it to start at zero. And then going up to n lines, where in lines would be something around 100 million. And I had an increment size of step size of chunk size, which was 100,000. So if I would execute that, that would mean, for example, the first one would give me a zero, so I equals zero. And then in the next iteration would be i equals 999,999. And it should be i equals 199,999, and so forth. And then in each iteration, I was specifying, of course, the filename I cared about. I didn't care about the header. And then I had the number of rows. So here, this is how many rows in total, it should read. This was 100,000. And how many rows it should skip. So that means in the first iteration, it would read the first 100,000 rows, and the second iteration, it would skip the first 100,000 rows. Because in the second iteration, we would start here, and then it would read the next 100,000, rows, and so forth. And I could do that and to process the whole data file, even though the data frame didn't fit in my memory. So I'm showing you that here, because it's just a nice illustration of how we can use these pandas function arguments for the read CSV function. However, there are also more efficient ways of using a pandas like deep pandas like API for large data sets that don't necessarily fit into computer memory or are very slow to process. So one of them is the modern project, which is following the second pandas API very closely. So it's kind of like a drop in replacement of pandas. So you can just import modern pandas SPD. Instead of importing pandas directly, of course, you have to install modern first. But if you do that, the API will be the same as the pandas API. And last time I checked, they cover 90% of the pandas API. And they basically implement functions to make pandas run faster on multiple processes. And I think also a lot of quad processing, which means it does not all have the data into memory, at the same time, and so forth. So this would be one option. Another nice option is dask, which is a very established project in the in the Python, scientific computing community. And it also really has a lot of options. For example, if we use a cluster of multiple computers can set the number of workers the number of threads per worker, how much memory to use, and stuff like that. So three out so cool tool. If you work on a cluster, let's say chpc cluster or something like that at all University, then you can use dask or modern, both have a very similar API, because they follow try to follow the pandas API. Yeah, this was all I wanted to say at this point about reading in data. So it was mainly about the pandas read CSV function when we have a data set in a CSV file format. However, in practice, you may encounter other file formats. But I don't want to discuss all the possible file formats right now. There would be a very long list of file formats and possible ways files are formatted. So but I hope that this was just a very basic introduction that gets you pretty far because nowadays, for example, if you look at kaggle, and stuff like that, most data files are in a CSV format. In the next lecture, we will then discuss how we can process the data set further, once we have it in a panda's data frame. 