 all right welcome everyone to lecture 21 of CS 287 today in the second half we'll look at how to make your computer simulate physics and the first half we're going to wrap up some ideas around model-based all that it we didn't get get to cover yet last time a couple of logistical things first though you have an exam coming up then you should have received exam slots you also have on Piazza all the material to study question and answer format so she'll be very clear when you get the questions what these questions are asking for because you have a handout that tells you what the answer should be to those questions and yeah well we'll run some random thing ahead of time to randomly extract some subset of those 20 questions and then you'll have a handout to complete let's see homework 5 is still being finalized hopefully going out in the next couple of days and whenever it goes out you'll have two weeks to complete it and soon we'll share some sign ups for final project presentations any logistical questions all right to technical accountant then so last time we covered the foundations of model-based RL and so the kind of main kind of algorithm outlined was collect data under the current policy learning dynamics model from past data all past data and improve the policy by using a dynamics model and repeat and we saw that historically there are some challenges with this because often the dynamics model you have learned doesn't match with the real world all that closely and when you optimize your policy you might take advantage of essentially quirks of the simulator that you learned and think you're gonna do really well because in simulation that looks really good but in reality it might not work so well one way on is just you should learn a better model go around this loop for longer but if the space is big this might actually be hard to achieve the solutions we saw them seem to work better are ensemble based methods where essentially you have an ensemble keeping track of a posterior over possible dynamics models and then you can try to learn a policy that either directly works across all members of the ensemble or is an adaptive policy that whichever member it's dropped into it knows how to adapt quickly so also it can probably adapt quickly to the real world so we saw that actually doing adaptive approach can give us learning curves that are very impressive where we see model-based or I'll up much more quickly that model free methods which we expect but not only that also achieve the same level of performance as model free methods and then we even watched a video of a real robot and ten minutes of interaction with the world learning to stack a Lego block the model based with meta pause optimization approach shown on the left and then PPO model free shown on the right it's been about 10 minutes the model-based approaches able to learn to do this which is really really fast but when I say in about 10 minutes it's more Sullivan that if think are we done with all this actually it's not really happening in real time when you're on the model base they're all often in the background to do a lot of calculations when I say ten minutes of real-world interactions sure ten minutes of real-world interaction but it'll do half a minute of real-world interaction fall by ten minutes of compute fall about another half minute of real-world interaction then repeat and so question you might ask is can we get this close to real time not to mention if you really expand with actual robots off we need to turn hyper parameters and so if your experimental cycle is really slow it's hard to get these hybrid parameters tuned to get this to work well then another limitation in current model base real methods that they often limited to short horizon problems because your simulator will have a hard time simulating precisely your learned similar well the hard time seeing precisely over long horizons and so it's hard to learn about what to do over long horizon in your simulator if your simulator can simulate that and then the other things that a lot of is from States so that two things went to touch upon now are resolving that first bullet point and the third one do some extent and the second one I guess is maybe maybe by the time the lecture comes around next year we'll be able to say more about that but for now it's it's fairly unsolved so how about this real time thing so the typical way you would run model-based roll you have an environment you collect data then you say okay time to learn a model from all the data I put in my data buffer then time to improve my policy by running it in my simulator that was learned from that data and now I have the best I can possibly do presumably based on the data I've collected so far let me put that in the real world and hopefully it does better than the previous policy and repeat so in some sense it's this is meaningful because you say I'm gonna if I'm gonna collect data I'm going to do all the work that I could possibly do with that data to learn the model then we know all the work and possibly do to improve the policy and then I'm gonna deploy that policy and some using all past experiences to have the best paulsen deployed to collect data from again and repeat so it makes sense to do it this way but it also means that often between your data collection and then all the learning happening in the next data collection there's just a long time span and question is can we avoid this and one way to do it is of course is to say well instead of doing this in a synchronous fashion where there is three steps that get executed one after the other how about doing it asynchronously there's always data collection happening and similarly there is model learning workers and pulse improvement workers and they continually all pass the results to each other so as you get a new data point it gets passed to the data buffer which gets pulled from by the model learning worker which right away might use the latest data it doesn't wait till you finished a bunch of episodes just pulls it right away same for your pulse improvement worker it will continually update the simulator it's learning against with the latest model parameters that come from the model learning worker and then data collection will happen always with a new policy because whenever you're collecting data you're going to draw the latest policy parameters and execute on that so some questions of course that come up then is will this have the same level of performance because we're not essentially consulting everything we could have gotten out of our data before we do data collection again might have some effective policy regularization and the reason that might be affecting regularization is because you're now always learning in a different model the model is always changing so maybe has some effect that it kind of regularizes your policy which could make it more robust creative effects on data exploration again why might it have expect there well if you always change the policy then the way you collect data is in a more diverse a more exploratory way than if you have this policy you think is the best policy you then collect a ton of data with that policy and repeat and how robust it is to hyper parameters the interesting thing is that other than what I described there are pretty much no hyper parameters so this works this would be nice because you're just continually feeding data through have to worry about how long do buy a train the model hardware long how long that arena policy how long do I collect data is just all happening all at the same time no tuning of how long it should take and then you can ask well how robust is that all that whole thing to data collection frequency because well now we have some kind of implicit dependence on the speed at which your robot collects data can speed up with your robots collect data determines really if the speed at which new data goes through this loop and that can depend a lot from system to system how fast you can collect data may have many of that collect data in parallel and so forth so how does it perform this is a very recent paper presented at quarrel two weeks ago we're comparing asynchronous model ensemble T RPO mul ensemble PPO and MB MPO which is the meta-learning version and we'll compare that with baselines the baselines are the synchronous versions of the same algorithms as well as the model free methods T RPO and PPO and we'll look at average a term versus time which is this is actually learned fast moving the robot in real time is it faster and we can look at average return versus sample complexity number of time steps it takes there's a data collection well here's against wall clock time the solid lines are the asynchronous versions and the dashed lines are the synchronous version so we see that if we look at wall clock time clearly being asynchronous is much more effective than being synchronous and well the model free methods they take a lot longer if this is here run on the joke environments were essentially from a joke environments you can think of okay yes we can run this faster than real time that's one of the beauties of doing research with them you can also run them as if it were real time say if it is for what we're in the real world how long would it take to collect one rollout because there's a real-time system behind Motoko that you could access so it's on with what it would be in real time on these majokoi environments then how about sample complexity complexity it's actually fairly comparable between it two approaches so somehow this notion that you collect your data not necessarily from something that has consolidated everything you've seen in the past but is quite different it's synchronously collected it's just about our sample efficiently in fact it a little bit more sample efficient in some scenarios especially real-world scenarios and we'll see an analysis of that soon so let's think about the policy regularization and the exploration during data collection how can we investigate this well rather than running a synchronous versus synchronous we can run synchronous and to make one modification that is do update the policy frequently under which we well also learn rotations update the model frequently and then expression there because just updating the policy frequently so policy learning the synchronous approach is shown on the left it all passes one after the other partially asynchronous will pull in the latest model at all times to keep learning the policy against whatever is the latest model and so what you expect to happen is that now this is a policy that is not just learning against an ensemble but is learned against an ensemble that is continually changing and so you might get additional robustness out of your policy additional regularization during learning meaning you're avoiding even more so overfitting to any specific model you might have learned because it's only there very briefly and that's indeed what happens if you look at the interleaved versus in order learning we see that interleaved learning actually learns a lot more quickly likely due to not overfitting to any specific model by seeing it for too long so it's an additional benefit of the asynchronous approach aside from running more real-time it also actually seems to learn policies in a cleaner way we can investigate the same thing for the data collection what if we send out whatever is the current latest policy rather than waiting to be converged in the current simulator and then sent out the policy as a conversion one for the current simulator what could happen well different things could happen it could be the case that well these policies we sent out are not that great because they're not as optimized and we actually do worse or it could be that because we send different policies at all times that the data we collect is actually more informative and that more informative data because it's more exploratory allows us to overall learn more quickly turns out the latter is the effect that dominates and we see indeed you can learn faster if you continually update the policy under which you collect your data then you see how sensitive is this to the data collection frequency we because it's this arbitrary time scale on the whole process is how long does it take to get data in the real world compared to everything else so we experiment with the sampling speed being up by a factor 2x slowing it down by a factor 2x and we said it indeed I mean it does have an influence on different environments that certain data collection speeds are slightly better than others but it is pretty close to each other you can ask the question can it work for real robotic toss can we now essentially run a model based RL demo where we just show up and five or ten minutes later it's actually doing the job when you wash the learning all in action in fact it's possible so for reaching let's see for reaching the table to learn in about 300 time steps which is quite fast then here we see it in action so there is synchronous on the Left asynchronous on the right so after 60 seconds the asynchronous is already it's time to reach the target pretty well whereas synchronous well synchronous is collect at far less data has done far less learning innocent area so in about two minutes it gets the job done how about getting the shape into the matching opening same thing here synchronous versus asynchronous listen asynchronous again in just a couple minutes wall clock time is able to get the Block in the meshing over in two minutes it's already getting it in whereas synchronous still has a lot of learning to do at that point in time how about another task getting Lego blocks stacked same thing here also by the way that the curves on the left are a comparison of synchronous versus asynchronous modem requires about 150 times steps to get there the other one about 20 so factors 7 different its wall clock time so here I got an asynchronous on the right learning to get the block and the matching open I believe it takes about five minutes in this case of real-time where a synchronous as we know from the plot on previous slide will take about seven times longer and so now we are technically have very fast real time model based RL for some pretty complex tasks so quick summary of what's possible now you can do essentially real time learning for robotic manipulation tasks these are in state space by the way just to be clear sample efficiency is better actually running asynchronously thanks to probably better policy regularization better exploration when collecting data very effective on real robots all right so what's the missing piece here that we haven't covered so much yet is the vision based yes question is could there be still some overfitting happening and all we're fitting of course can happen in many ways but a specific question is the kind of model bias effect can your policy over fit your simulator because maybe your simulator doesn't change that quickly and that's that's actually one of the things to pay attention to if you if you run this if your models update very quickly you can often even not bother having an ensemble because well it's always a new model ever every time and it's quite significantly different every time and then it's effectively acting as a sequence of models which act as an ensemble however if your models change only slowly because the way your data collection rate is and the way your model updates as a consequence then you still want to have the ensemble so there's a question there in practice you still need or do you not need anymore they'll samel and it depends exactly on what you're asking about which is how fast to the underlying models that you learn update based on the data you collect the faster they update the less you're gonna need an ensemble and the slower they update the more is still gonna want an ensemble but they kind of serve a little bit the same purpose so it really depends on the situation what's needed so vision based model based role what's different about it well now you need to somehow learn to simulate the world as you perceive it not in state space we need to simulate it in pixel space because you just get pixels in and then you see pick us up the next time and you're supposed to learn to predict what happens and then build a simulator that can do that one of maybe the kind of most widely seen works is the world models work by David haw and collaborators and especially they looked at is this kind of this doom environment there's a simple simulated environment where you move around the stage and then you also do some some shooting in this case essentially the they proposed a very I would say natural approach they said well I don't want to go from current frame to next frame directly because then I need to learn directly on pixels everything that's not the most efficient way to learn there is an underlying state to all of this why not try to compress what I see into an underlying state and then back out to the visual observation what's a method to get there while modern method you could possibly use is a variational auto encoder because that's what they do they take it and then some sensory information turn into it compressed it and then reconstruct if you set it up that way then maybe in the latency space you can learn a dynamics model there which might be more effective to learn a model to be able to learn to learn a policy that relies on Z rather than relying on pixel values and now your politics in something lower dimensional then let's say many many pixels and might be able to learn more quickly because you're taking lower dimensional information often you can find a signal more quickly if you have very high dimensional information coming in so what is it look like so then you need to simulate in this latent space then beyond that you want to of course have a policy and of course you're going to condition the actions your policy takes and so if you set this up you can say okay I'm going to train a v8 you I'm going to learn an art and on top of that action additional Arden that simulates in a latent space and once I have the I can run reinforcement learning in this new simulator which is really a Z space simulator where I can learn to act they tested this on a car racing environment so the input to the neural net is the overhead view that is shown here or one snapshot is shown here and it used to steer the car to stay on the road and go fast who hears familiar with Yamla Coons cake you know quick raise our hands some of you so yall MacEwan has this cake the well Iyanla cake or cake and for a luck cake essentially the stories as follows when you have a cake like the one shown there and you make the analogy with machine learning especially deep learning well in deep learning reinforcement learning doesn't give you much signal there is just through sparse rewards and so if you think about the cake as the whole problem you're dealing with well the reinforcement is just a cherry on the cake because not much signal there it's important you can live without it but it's not giving you a lot of signal then supervised learning where people still need to annotate and so it provides signal by annotation but it's very laborious so it's not gonna be a lot of single still going to medium amount a signal that's the icing on the cake again you wanted you need it but it's not where the main meet this of the cake the main meet is the whole foundation of the cake and that's unsupervised learning or salsa bars learning if you want to kind of use his terminology and so the whole idea that he puts forward is down when you want to learn something you should really be mostly focused on self-service or unsupervised learning because that's where you get the most data if you can extract signal from that massive amount of data that's very easy to get then from there hopefully a little bit of supervised learning a little bit of reinforcement learning and then finally I have something that works and this experiment here actually illustrates is pretty well so if you look at the number of parameters in the neural network shown here so the neural network consists of a very solid encoder RNN and a policy so really three neural nets well look the VA II has about four million parameters the RNN which is really supervised learning for the dynamics model has four and a thousand parameters and the controller neural network as has 800 parameters so we indeed see this kind of scaling effect yon loves to talk about in this experiment where the controller can actually be quite small and also doesn't get as much signal so you know it can only learn so much compared to the unsurprised part and the supervised part so world models is something is kind of I would say maybe one of the simplest things you could try and if you do it right it actually can work surprisingly well there are some aspire more sophisticated things you can do whose the action commercial video prediction using deep networks in at our games Mali based RL for Atari and learning latent dynamics for planning from pixel solving called planet these are some papers that you might want to check out if you want to learn more about this where there's some very good results on learning models from pixels to then learn to act in that environment now here's a very different idea imagine you know you're gonna do control so your nurse is gonna train a VA II then hope that the embeddings good you say I know I'm gonna do control and I know how to do control I know that it's really convenient to do control with lqr then in principle you could decide to say I'm gonna set up my space such that in the Leighton space I'm gonna have a linear dynamics model or potential time varying linear in this case linear and we have a linear dynamics model and if I can do that if that can learn a latent space where the dynamics model is linear and of course here that means the loss has to be on next frame predicts you have to say current frame and bad predict the next embedded state project back out to the next image you're gonna see where you force the next in state prediction to be a linear prediction if you can learn that way then you have forces system into having a linear underlying representation and you've made your control problem a lot easier because you know how to solve those problems really well so embed the control does that and it shows successfully learning controllers for essentially pendulum card pole and three link arm where we also know that in principle linear control should work quite well in the underlying space but then here it's not learned from directly having access to underlying space the system just as access to images of the type you're seeing there and from that forces into space where a linear system is a good way to describe the dynamics and then control just falls out of it so it's a very nice kind of coming together of understanding optimal control and understanding representation learning that allows you to get a really nice result here then actually one of our tiers Laura Smith with a few collaborators worked on extending this to linear time varying systems and was able to then succeed at this on more complicated systems like actual real robots so this robot stacking Lego blocks was learned from pixels now we're essentially would learn an embedding space in which the dynamics is linear or a near linear and then it would run law detail here but it run iterative lqr under the hood to find a controller and learn extremely efficiently to do Lego block stacking just from pixels as input another work in this direction is that deep special autumn encoders the idea here was that if we know so in most representation learning you might go to some M from image to some latent space that still looks like an image but if your latent space still looks like an image and has in some sense pixels that are not RGB but just embedding vector pixels it's not as clear how you turn that into control signal you might still have to do to work or how you learn a dynamics model in that space so a Chelsea here proposed Chelsea Finn proposes to say that after you do a bunch of convolutional layers introduces spatial softmax which for each feature map figures out the softmax of in which location this thing is maximally active but in a soft way maximally active then feeds out the coordinates in the image so now get out real numbers there are coordinates hopefully of objects in your scene or maybe parts of objects in your scene and then you can run your control or learn the control in that space which would correspond to state space hopefully it's not a lot of work we're essentially the embedding was learned to be such to have certain properties that you think are gonna be true for robotic systems are forced to be true during learning so you go from image to embedding space but what do you say well so phi is doing the abetting of the observation to hopefully the real state space what is l variation that's something L says the embeddings of different images should be as far apart as possible so it is trying to spread things out and everything else will try to bring things together so it is trying to spread things out then the slowness has done if you embed two consecutive frames they should be embedded close to each other because they're related to happen right after each other inertia if if something is moving out of some sense a certain speed they should keep moving at that speed so the velocity at time T and T plus one should be similar or you could say acceleration the acceleration should be small then conservation of velocity so the speed at the next time an energy should be similar to speed at the previous times not always true because forces could be applied but this is a way to incentivize it whenever possible to do that and then controllability which is essentially you want you want your system to be embedded in a space where the I mean there's a strong relationship between the actions you you and what the next state is going to be so they also have some good success in this relatively you know simple simulated environment but it's hard because you've taken pixels rather than raw state then another angle people looked at is well you can turn every an autoencoder but for it to output meaningful state maybe you need to regularize it some extra so what they showed is that if you it's called a beta view but the idea here is actually fairly simple the idea is that you try to make your latent variables more independent than it normally would be so regular view is kind of has a regularization I'm coming from a Gaussian you tried to make it maximally independent each of the variables then maybe the representation you get is easier to learn on top of because variables have been disentangled compared to all of them being closely related to each other at any time this showed by doing that you can actually learn things that can transfer more quickly when you really change what the environment looks like so they would train regular DQ n versus Q learning with this VI e that I talked about with maximal disentanglement or not maximal but optimizing for disentanglement and show that it actually transfers a lot better we need to change the environment they'll work a lot better for Darla than for a traditional DQ one what else is there and again we're kind of just going through a bunch of ideas here at a high level just to give you a notion of what what's happening out there causal info again when you train again it's about generating realistic images but if you just all what you do is to make realistic images you ignore the dynamics of the environment cause what in fog and tries to do the same thing but account for dynamics in the environment making sure we're trying to make sure and when you know it works well that's what it does it essentially tries to make sure that if you have two states - embedding states he interpolate between them that the images you see rendered along that interpolation are not just realistic images but are actually a sequence of images than is a reasonable sequence to try to achieve in the real world so we can do the anything say I have a current image I have a goal image I'm gonna embed both of them do a linear interpolation in an embedding space project back out to a sequence of images and I'm going to try to do control to follow that sequence of images as targets for my robot to follow planet we've talked about earlier presentation a few extra things compared to things I mentioned so far when it's multi step prediction such is something very useful at times where you say hey I know that my dynamics model can easily become inaccurate over long horizons but what if I just directly optimize for that I just put it in my loss I say I'm not just trying to bring next dead given current state action trying to break state five steps from now given current state and the five intervening actions but it reca optimizing for that you can often get somewhat longer horizon reliable simulation and then they also they they do planning so direct planning as opposed to let's say running a policy grand method in the simulator why might this help why what might be the trade-offs let's think about that you've learned your model you can either run an NPC in your model or you can run policy gradients or something like that in your model and then use a policy that comes from it well if you think as the model of having all the information you've collected from the environment condensed in your model then running planning slashing PC should do better because you're not somehow balan egging yourself through learning a policy is saying this is the model I'm just gonna find the best sequence of action against the model take the first one and repeat so often it'll work a lot better to run the NPC against the model what are some downsides often it's slower because you got to run an optimization every time you take a step you need to run your optimization a second downside could be done times about having a parametrized policy you can generalize in ways that you know your actions that you find with NPC might over fit to some quirks in the model more easily then here's another line of work that's been pretty exciting but happen over the last couple years a lot out of Sergei Levin slab actually showing that is becoming more and more possible to do video prediction for videos that are pretty high resolution once you can do video prediction action conditional video prediction well then you can run npc often using cross-entropy method or something like that to find a sequence of actions execute the first one and repeat to try to achieve goals shown in a goal image now one thing you might think about is say well well well I mean I'm simulating this entire scene maybe that's you know okay here because the scene is not too complicated and kind of everything that's semi complicated in the scene is something you care about but what about if people do things like dropping a glass bottle and it shatters into pieces do you really want to learn to simulate that and how precise you're going to be able to do it are you really gonna get all those details right it's gonna be very hard in fact I think it's probably impossible I mean the dependence on initial condition and so forth will be quite delicate and you're not gonna get the exact match with reality so but maybe you don't need to I mean when you drop a bottle and it breaks you don't care about all the details you care that it broke and that's all you needed to know for next time to decide not to drop it if you don't want it to break so you could say well we should not simulate in pixel space we should actually simulate in a latent space okay so let's make a latent space we'd take our image put in late in space and then predict the next latent variable we don't care about reconstructing the image we just want the next latent variable so okay let's make it really good at predicting the next latent variable what's the solution gonna be it's gonna make the latent variables always the same identical let's say always zero because well that's the easiest way to make sure you can always break the next latent variable so then I said oh I guess we need to reconstruct after all maybe you could but then we're back to where we started where you're now trying to reconstruct again is there something else we can do this work poke at our wall and collaborators show that maybe the way to think about this is to say hey what we care about is action so maybe we don't need to reconstruct the image I mean we might still do it as a regularizer it might not be the main thing we need to be able to do we need our latent space just to be such that given latent State at time T latent state at time T plus one can we predict the action that was taken to go from T to T plus one so we're gonna embed but optimize the embedding search that we're capable of predicting the action I was taking between time steps and so that allows us to ignore everything in the embedding that we cannot effect noisy stuff in the background we can now not have any influence on doesn't have to go in this embedding because it doesn't help us predict the action that was taken from T to T plus one this is a very interesting way to approach the problem there anything about what do we care about its action we learn embeddings that allow us to predict action from T to T plus one and so forth that actually work quite well then you could say well action maybe something else would care about rewards maybe we also want to predict rewards to happen so again let's not worry about predicting the actual frames let's just predict rewards and maybe this one is just for worse predict Ron for depending on essentially given a way of making your billiards shot how much reward are you going to get that's what it's trying to predict without necessarily worrying about all the details of everything in the scene generalization of that is successor features if your reward has many many features in it there's many many contributors to your reward try to independently predict the future of all of those features rather than just simply the sum of them in that reward once you do that you can say well sounds good you can actually then do this action conditional and that's what Greg Kahn and collaborators did here such an action conditional prediction of future features that relate to reward and so you learn a latent state that's focused on the things that are gonna matter for your decision-making this actually worked really well for learning to navigate so this robot learned to navigate in quarry hole by just driving around on its own and a night of practicing driving around on its own it learned really well to predict the features that matter for a word related to collision sometimes also related to doors and things like that and then from there it could run either queueing on that or NPC against tab to find actions that avoid collisions fully vision based so there's another thing you might want to think about it's just some reference I'm not going to step in if you can think about what is the ideal state representation can we come up with a notion of given some sensory observation what is the proper way of turn that into state and there's a bunch of work trying to do that trying to declare objectives and properties of what it means to be a correct underlying state representation there's a whole paper claiming that you know they have a separation principle or control and age of deep learning remember separation principle what was that it was saying that for linear dynamical systems we had this notion that if we don't have access to the state we just have access to observations which are again linear plus Gaussian noise in the state the optimal policy will run a common filter and use the mean output of the common filter to do control as if that mean is the true state so that's a separation period several is on your controller assume you have access to full state and your filter which tries to find the posterior over States that's what the separation principle refers to and this is trying to generalize this to deep learning scenarios where of course you wouldn't have a linear system a very high-dimensional would say pixel input and what can you do there all right that's what I had for model Bester all any questions about this before we switch gears okay let's take a couple minutes break and then let's switch to physics simulation alright let's restart so we're gonna take a look at physics simulation obviously you can do multiple semester courses on just how to do physics simulation so the fact that we're gonna try to do is in 35 minutes means we're not going to be experts at at least not worldly and leading experts at the end of the 35 minutes but hopefully you can get a gist of what are some of the concepts that go into this and be aware of pitfalls that might be present in certain scenarios and so just generally get a sense for what you might want to look for maybe if your physics emulation doesn't work right or you want to get started on building one what are the keywords to look for and how does it relate to the results you're getting so a very fast lining to a physics simulation 35 minutes we will look at Newton's laws for rigid body motion Lagrangian formulation then we'll go from continuous time to discrete time which we have to do to run things on a computer and then we'll look at contact and collisions which of course complicate things a lot now as I said we're not going to cover anywhere near what you need to know to truly be an expert but there are resources out there for you to become an expert if you want to so Featherstone's book is probably reference them almost everybody will refer you to if you ask them okay how where to learn more about physics simulation on a computer how do you do it that that book is a starting point pretty thick book but that's good because it means it's very comprehensive then two of the most popular physics simulators majokoi and Bullitt have quite a bit of documentation available on how they work so it's not just a black box they actually have written up papers sometimes even like almost books like Joe clays my online book describing how it works so the information is out there and then the one at the bottom kind of somebody who's game gaming software engineer who wrote up kind of a mini tutorial on some of the key things that matter for physics engines so where to start we want to simulate physics and what assimilated at the mechanical level is what we're going to pay attention to here so well what is it it's just F equals MA right I mean if that that's that's what we need to do we didn't make sure that at all times the accelerations of our objects are you know the force they experience divided by mass and now if your object is a not just a point mass but a rigidbody you don't want to describe in your simulator the velocities and and positions of every single point on that object you want to represent that object as just a position velocity orientation and angular rate and so then the way it gets put together is the equation at the bottom here there's still F equals MA where this applies to the center of mass of the rigid object so the center of mass will follow that principle but you also want to know the orientation of your object they need to also track the differential equation at the bottom where the torques are equal to the inertial moments matrix which can be a motifs tends to be a positive definite symmetric matrix inertial moments matrix times the time derivative set a dot on top and the notation we'll be using here tends to mean time derivative so the time derivative the angular rate times inertial matrix but then there's this other thing there which essentially comes down to the fact that the way inertia works in it's when you spin something around a given axis it's actually for most axes if it's an asymmetric object it will not continue to spin around that one axis it'll start tumbling around and change the axis around which it spins and so that's in sometimes what that second term accounts for now each object is to have at least three axes the three I mean eigenvector axes of that inertial matrix if you spin around any one of those three axes that I'll keep spinning around it but the other axes it often will not because the inertia around different axes is not the same so that's what that is that's why it looks more complicated and just a Tau equals I times Omega dot all right so we have but this is a known thing you can compute so I tends to just be you need to essentially do a compute an integral that looks at the mass description of your object where the mass is relative to the center of mass and once you do the right integral calculations it's very tractable you will have your I matrix and if you have the mass measured then you can actually have all the things you need for this you have inm then you can simulate you just need know what forces you need to apply what torques you need to apply or are being applied and then simulate now things can get complicated pretty quickly because you think about the forces well imagine you have robot with a link like here arm and just like the link now there's a lot of forces happening in the joint and the details of those forces are pretty complicated but at the same time we have a very simple abstraction and at the end of my upper arm link and the beginning of my forearm link are going to be joined together at a point and that point is going to be the joint between them at all times assuming though like bad deformation so the very similar is that this point should stay put but new Newton's laws don't really tell you how to do that I mean you could get into the details of all the forces in that joint but that's very complicated to simulate given we know that there's this invariant can we do something that maybe it's a bit more tractable than simulating all the forces in that joint well whenever there is multi body systems with constraints and internal forces there's a Lagrangian formulation for dynamics that actually makes it much easier to write down the differential equations that govern the motion of this multi body system how do Lagrangian dynamics work well it's it's a very clean recipe it says we need to define some generalized coordinates are I so I have maybe R 1 where R 0 could be one joint angle and then R 1 could be the other rotation axes that half a year and then this one would have every degree of freedom would have a coordinate associate with it then we need to total kinetic energy so we need to somehow be able to write out for our multi body system as a function of the generalized coordinates and more importantly as a function of the time derivatives because kinetic energy tends to relate to how fast you move the time derivatives of the generalized coordinates what's our total kinetic energy what's our total potential energy and what our generalized forces okay so generalized force essentially comes down to tying forces into the degree of freedom that they're acting upon the Lagrangian then is t minus u and we have that D essentially we have this equation here tying the generalized forces into the time derivative and the coordinate lower credit derivative of our lagrangian so this is the equation we can use that will be valid for any system and so it's a it's a recipe you come up with the quantities in that bullet point list and then out comes all the quantities you need to write down this set of equations if there's more than one degree of freedom set of equations that govern the dynamics of your system once we've done that we end up with a set of differential equations that we can use to simulate our system so let's let's actually do this by example we'll do a very simple example to kind of well showcase up this it's very feasible of course is some scenarios are gonna have to do more work so let's look at this Lagrangian thing in a very simple case so imagine we just have a point mass so we want to simulate a point mass we all know what we need to do is just F equals MA but we want to verify that this new idea actually matches up with the intuition already have for point mass to build confidence in this new idea and also see it in the context of something we already understand so not everything is new so point mass what do we need generalize coordinates what will be our generalized coordinates well is going to be X Y Z then kinetic energy was the kinetic energy of a point mass it's one-half mass times velocity squared that's kinetic energy for a point mass so T equals one-half mass times velocity square but expressing generalized coordinates so X dot squared plus y dot squared plus Z dot squared how about potential energy well what's the potential energy of a mass it depends on how high it is above Earth essentially like the higher your up the more potential energy which you can turn the kinetic energy by dropping so potential energy U equals mass times G which is the well essentially nine point eight one meters per second squared times Z which is your height where you measure height from it doesn't matter too much in potential energy will the weight potential energy will show up is that the absolute offset doesn't really matter like if you said plus a constant here that does not depend on the generalized coordinates in any way the equations of motions we're going to get are going to be the same anyway so let's just call it Z and not worry too much about exactly what's the zero point but a natural zero point could be the surface so what are the equations now so this thing over here for each generalized force what are the forces here we can apply a force along fourth generalized force it need to be aligned with generalized coordinates so we have a force along X along Y along Z and so we see that the equations of motion are for each generalized force this one f of X is equal to D DT during the respective time of what the Lagrangian but then derivative with respect to Q dot I so that's in this case X dot minus D L D X what is this well L is t minus u so L is one-half M X dot squared plus y dot squared plus Z dot squared minus M G Z so DL DX dot if you look at this will be DL DX dot will be equal to M times X dot then we have during respect to T in front of it this will make it so d the T in front of it will make it their time fear of this thing will make this double dot that's what means to have doubled out two time derivatives so this first thing is M times X double dot then minus D LD X L here well there's no X in it so that's going to be zero and this we have for our first out of three equations that f of X equals M times X double dot how about FY well the roles of X and Y are really the same I mean C is the direction which the potential energy works x and y are both horizontal directions so we can either grind through it again I can just see well the roles of X and Y are the same so the equation here will be F y equals M times y double dot how about FC what is that going to be equal have to be equal to or what's that going to be causing well F Z it's gonna be equal to the first term DDT DL be Z dot that first term here will be similar to the X term which will be M times Z dot but then the second term the l DZ there is actually a Z over here and I'll be M times G so we have M times Z double dot minus M times G so if we look at this another way to look at this to say we have found here that X double dot equals F force in X Direction FX over m y double dot equals force in the Y Direction over m and Z double dot equals external force not caused by petitioner G that's what these generalized forces are technology is separately account for external force of Z divided by M plus G so that's exactly we expect the point mass will be c7 susceptible to external forces and that will cause accelerations and then of course its gravity which is another external force with an external force we put into our potential energy and so it's captured there so that's one example for this example you might say well that's a lot of work to arrive to a conclusion we long knew ahead of time but hopefully builds our confidence in that this actually does the right thing and in a simple scenario we can see how to grind through this and I'd say that's the beauty here all we have to do is grind through this we did not have to come up with very clever tricks no just grant through this method and out comes a set of differential equations describing the equations of motion of point mass in this case so put this on the slide also how about something like this a two link arm well we already generalize coordinates they are going to be the angle theta1 and theta2 and then we'll have some shorthand notation for sines and cosines and so forth because otherwise it's not going to fit on the slide but the generalized coordinates are just theta 1 and theta 2 those are Q 1 and Q 2 now what do we need to do we need to find expressions for kinetic and potential energy as a function of those coordinates or and by the way what this thing I didn't clarify this so this thing over here to understand this generalized force thing it's really it's really saying what is the work done so Q is really the work done along a generalized coordinate so for each force that you have have J external force it'll be aligned with some generalized credit and as that force is exerted as a generalized coordinate changes some work will be done the traditional work is for supplies time displacements the force times displacement there's a classical physics thing for work but it also be torque times change of angle so those are the most common ones with torque times change of angle or force times linear displacement and that's really here the generalized version of that and you look for every external force or torque you look at how much is applied to that specific generalized coordinate and then that's what you put in there so what do we need we need those generalized forces we also need the kinetic and potential energy so what are they well we can look at that thing over there there is essentially two links there is a first link in a second link we can look at where the masses are so the links you are soon to have no mass just a very simple model but assume that the mass all lives in those blue balls okay just a specific dynamical system for easy to work with so really care about is the velocities of those masses and the altitude the Z coordinates of those masses that's going to affect the kinetic and potential energy and so that's what being computed here you can think of the first thing as the x coordinate the first one X yeah first one X and then the second one is the vertical coordinate Z and we compute us for x1 and x2 we can also write out the velocities as a function of the generalized coordinates so the location what this thing is the location of m2 is this expression over here it's x1 plus this and X once over here and then the velocity of x2 is this thing over here and once you have the velocities of our masses as a function generalized coordinates we can write out kinetic energy if you have the position so you can write out the potential energy u then this point we can write out the equations of motion where utilized for us here is that the torques and we have these expressions here which again might be hard to come up with otherwise if you just do the attorney and please say what force is a link applied to the mass and we force equal counter force a lot of work you have to do to get here whereas now we can just write out these quantities that are just following a recipe to get them and you get your two equations of motion why - because there is two degrees of freedom here is some other equation of motion and I just put down these are kinematic equations often you'll use kinematic equation so there's no forces here just kind of a slice which have gears for a moment but often you'll say something like okay I have velocities that depend on in this case my steering angle and my forward velocity speed and that can write out some kinematic the path of the car will follow as a function of my steering and forward velocity let's go to another Lagrangian one cart Pole again you work through all of this help me you end up with an equation shown at the top and in fact they'll pretty much always look like this you'll have something times Q double dot so second your expected time of each of the degrees of freedom generalized coordinates then something that depends on Q and Q dot and then some thing that depends on gravity and then something that depends on the external inputs a Krabat so cart pole you have only one control is the force horizontal force in Acrobat you also have only one control input which is the torque at middle joint there you don't get to control the torque where it's attached to the world only in the elbow you get to apply a torque it's supposed to still be able to control this you go through the same exercise end up with equations of motion so what do we have at this one we have a way to go from potential energy kinetic energy equations to the equations of motion now kind of what we ignore it a little bit and often e to add in as forces is friction and drag because when there's contact between objects it's not the kinetic energy not the potential energy that contributes this is going to be those external forces you need to understand what external forces are present so two objects one sitting on top of the other if you try to push one the one on top away from the one at the bottom it's gonna be a friction force working against you something called static friction and dynamic friction static friction is the friction when it's not moving yet and so when it's not moving and often the resistance is higher than when it already started moving and so there's this friction coefficient which is saying that the friction force working against you is friction coefficient times the normal force so the heavier something is the more normal force there is to support it and the more friction force can be generated you can never have more friction force than the friction coefficient times the normal force now the friction coefficient will trop once you start moving see this funny effect it push very hard nothing happens which harder harder is still nothing happens and then all of a sudden things start moving and then actually the friction will drop when it starts moving and so you'll accelerate it even more it's not a force that you often want to account for a drag forces so when something is going through a fluid could be gas fluid or liquid fluid because it's pushing the molecules of the fluid out of the way to move through the fluid it's experiencing some kind of force pushing back against it which will slow it down it's why if you drop let's say an object from very high it will not accelerate at 1 G all the way through because there's these drag forces slowing it down ok how big is that gonna be here's kind of a I mean you've got to do this specific and probably want to collect data on whatever system you're working with but the general notion is that drag forces will be some coefficient which depends on the well sometimes the material as well as the fluid type of fluid that you're in times the surface area that you're trying to push through that fluid times the density of the fluid the denser the fluid the more drag force you're going to experience just like when you're running water it's a lot more tiring than when you run in air and then times velocity squared so that's interesting and then the faster you go the more drag force you experience but it's not linear it's quadratic so there's also why I mean if you look at it fish is it let's say you you drive I don't know drive it tesla it'll tell you how many miles you have left left to drive right under your current charge the faster you drive the less mileage you have with your car I mean from a certain I mean and once you had a reasonable speed the faster you go the less mileage you have left because the amount of drag force you encounter squirts quadratically with the speed you're going on but the amount of time saved of getting somewhere only scales linearly with the speed you're going out and so overall you have less mileage left by going faster and so I'll often tell you you know if you want to get to the supercharger with the charge you have left you should not go above many miles an hour anymore or you have too much drag effectively and won't make it it does it does those calculations so these are forces that we do know in Lagrangian formulation would be external forces that we might want to add in to account for things that happen in the real world now when you specify a robot robots are so common that there's some kind of standard way of specifying them so instead of necessarily following the whole Lagrangian approach you can actually just specify the sequence of links your robot exists off you just say first link is this long and the end point of that Linkens up relative to the start point of the link over there then you say how heavy the link is and what the moment of inertia is of that link if you specify the sequence in this standard coordinate system you effectively have a full description of your robots physical properties at least for this kind of simulation and then most simulators will take this in maybe in the form of a you RDF file which describes the properties of your robot and then build the differential equations on its own from that now what else we need to do we need to go from continuous time to discrete time because everything we've seen is differential equations so we've seen how to turn a physical system into a differential equation describing how it behaves but computers don't really do continuous simulation they do things in discrete time so what now imagine we have a system and we're going to look at in a generalized way so we're not going to worry about specifics of what we're simulating but we have something Y and a time derivative Y is some function often of just Y depending on the state that you're in there is things that happened to you but it also be a function of time because maybe some forces get applied over time not just as a function of Y so if we have F we've seen how to find F F is the whole Lagrangian thing that we discover we know how to find f but now let's say we know F and we know y at x 0 equals y 0 how do we now know why at some later time T well sometimes we can do it in closed form of course some differential equations of closed form solutions most famous ones will be things like y dot equals a times y and y 0 equals y 0 then y equals y 0 times X a times T right so that that is probably you know best known like closed form solution for a differential equation but very often the equations we end up with are not in that beautiful form where just Y dot equals a Y we have something much more complex so what do we do maybe you can find a closed form solution probably not for complicated systems so what can we do there's something called forward Euler and it says to compute y at discrete time step n plus 1 if I know Y at discrete time step y N and to go from n to n plus 1 an amount of the delta T is often denoted by H in the literature here I'm going to say well we weren't yn we know the time derivatives so let me just say it's going to be plus h times F the time n and yn so that's just saying we know the derivative let's assume that derivative does not change but of course the derivative actually does change because the derivative depends on Y and Y is the thing that's changing let's assume it doesn't change let's assume that for a small time interval whatever the derivative is now is a good enough approximation and let's assume it stays constant for this much time and then we can simulate forward now here's something this is one way to do it another way to do is say backward Euler the idea here is you say yn plus 1 equals YN plus h F T n plus 1 yn plus 1 kind of sometimes might seem crazy but it's the same thing really you're saying I'm gonna pick one point where I look at the derivative and use it as Samoas Council of the entire interval you have instead of thinking at the beginning I'm picking at the end of the interval now when you have this since you're solving for yn plus 1 and it appears on both sides now you have to solve some system of equations to find yn plus 1 so the calculations going in to this might be conversational more expensive because here you just get the derivative you know it from this evaluation you just compute the next one here nonlinear system of equations to solve you might say dad you know the reason people would do that they must have a very good reason otherwise they're just doing extra work with no payoff from it and there's actually very good reasons so let's take a look at why people often don't like the or cannot get away with the first formulation so in the first formulation forward euler often also called so forward or explicit it's explicit because you can compute the derivative and just use it and here it's implicit because you don't have the derivative available you can't just use it it's implicit in this equation imagine our simple example again y dot equals and I will say negative and let's assume a is a positive number negative 8 times y what's a solution Y and a t equals y at 0 X negative 80 what does it look like well you start somewhere here which is your wide 0 this is T and you essentially go like this ok now let's apply forward Euler and it's an approximation you can see what it's going I'm going to show you that the approximation can sometimes be pretty bad and we need to be careful so for Doyler says y at n plus 1 equals y n minus a why n times H so that's equal to 1 minus a times H times yn now H is how long we simulate and a is something that essentially says how fast you being driven to zero in the real system let's look at different conditions slightly imagine 1 minus a times H is a number this thing is going to be positive this is positive so this is gonna be a number below 1 the question is how how far below 1 is it going to be let's say 1 minus a H lies between 0 & 1 then what happens well depending on how close it is to 0 we might just jump very quickly to something very close to zero even more quickly something like that might happen why more quickly than the original well because I mean the derivative becomes less steep later so when you're proximate it with where you're now you're going to be steeper than the original huh so maybe we go a little more quickly to zero that might be ok but what if this thing here is actually smaller than 0 1 minus aah smaller than 0 and actually let's make it can be I mean how about it is smaller than negative one then what happens is we'll have this thing flipped to the other side that's something that's absolute value larger than Y zero someone that maybe over here then will again have it multiplied with a negative number that's absolute value of bigger than one will become bigger in absolute value but now on the positive side and this thing actually completely goes out of control unstable even though the real system is a very simple stable system driving itself to zero and interestingly the the bigger a is the more negative you might end up being here so the bigger a the stiffer the system is what it's called the easier it drives itself to zero the quicker you'll get in these oscillations what's the fix well we all know that in principle this should work was the fix you need to make your H really small all right so your H you can let this happen right you need to stay above zero seem to make sure H is very small and so the bigger your a the smaller H needs to be and so for large a your H needs to be tiny if your agent needs to be tiny the amount of time it takes to get along this axis for tiny H will be a ridiculous amount of competition before they finally get there so the tricky thing here is that with forward Euler you can get these instabilities for stiff systems that are steep derivatives forcing you if you want to stick with forward Euler to use very small time steps which might make it extremely time-consuming how about backwards Euler backward Euler says yn plus 1 equals y n minus a yn plus 1 times H this case actually we don't have to solve a nonlinear system of equations here we can do this in closed form and we have yn plus 1 equals 1 over 1 plus a times H times yn this thing here this number here is always between 0 and 1 so this thing is guaranteed to always drive your Y to 0 independent of your step size now a large step size will make it less precise a large step size will think you're right away there and sighs will more precisely track the path but no matter what step size you pick for age you don't have these instabilities and so that's a big advantage of the backward now keep in mind sometimes it's not that easy I did it in closed form here sometimes to solve this thing you have a nonlinear system of equations to deal with making it somewhat hard to solve now in practice so let me show you this looks like we covered forward Euler backward Euler you can actually do something implement and semi implicit which is in between where for state use one time for velocity use the other time this kind of interesting is actually it has some interesting properties where it tends to manage have conservation of energy and so that's a good proud to have because if your system conserves energy in the discrete time approximation it might lead to more realistic simulation and so interesting little intermediate here then this thing called room could have methods which I don't want to step into the details of them but I want you to be aware of because these are often the the ones that the method of choice and it essentially looks at 4 YN plus 1 equals YN plus an average of derivatives at four different places where does evil you ate the derivatives do such a procedure for is it says first of all I'm doing at the current point then I'm going to simulate forward from the current point there's a simulation forward half time half time to go forward so yn plus k1 over to evaluate there and we evaluate their advantage used k2 as my ders and how they use D if I have to use the third way I get at the end of the interval I get my k3 and then there's that the variant that involve k4 so it's essentially a recipe that doesn't require solving nonlinear equations like the backward Euler and a lot of that uses a mix of derivatives at different times of course approximately achieved they're not the exact ones at that time because it's a forward progression to get those numbers but that tends to be one of the most popular methods and it has good properties in terms of your age doesn't have to be as small as I needs to be for Euler methods to get the same level of precision of course you do a little bit more work per step then in one minute I want to cover this so we can switch topics next time contacts and collision koujun checking that sense to have a broad face in a narrow face what does it mean broad face is about there's so many objects which one should I even carefully collision check you split up your space into regions and you see which one live in the same region but the ones that live in the same region of your space you might design some bounding boxes axis aligned makes it very easy to check if there might be overlap instead of checking overlap between objects check overlap between the bounding boxes of the objects which makes it easy to do that's broad face it won't give you a true answer I'll let you know if it's possible these objects might be in collision because the bounding boxes don't overlap then definitely the objects won't overlap so the way of getting a very quick check which objects might overlap and so to split with space often people use something called quadtrees split in volumes and see who is in what volumes some things could be in two volumes if they're next to each other those volumes because not everything will be nicely split off by a volume and then you can just see okay who's in the same volume now let's check if they're bounding boxes overlap or let's do a sweep along an axis and see if there's overlap on the projection of these objects then once you did your broad phase collision checking yet I put a few candidates that might be in collision the underlying thing that's easy to check is convex to come backs so if two objects are coming back so you can check very quickly whether they are colliding or not so what are you doing practice you often take your originally non comics object turn it into a union of comics objects and you collision checking for each of the members of that Union against the other objects convex parts logan's for this are called gjk andp and those are actually fairly sophisticated in complicated algorithms the beauty is that they're available I mean implementations are available but they're pretty calm get it to do very efficient checking of collisions or not having collision between two convex objects then once you have contacts what do you do well if there is collision meaning there's overlap it means you ran your simulator for too long because you know that before that collision before the overlap happened they would have collided and had repelling forces to each other so the simplest thing you could do is you going to say oh well I need to reduce my time step I step for too long let me step shorter serious collision and do a kind of bisection search they all know when it comes into collision and then you can from there simulate the collision forces and continue another thing you can do you can actually solve your equations that do essentially conservation of momentum and energy where you do it then not instantaneously because the time scale might be tricky to deal with you do it with impulses to essentially say if I have an interval that happened over that interval the integral of forces applied to an object should be equal to the mass times change in velocity and so that is essentially saying I'm not gonna do instantaneous things I'm going to do it over site longer intervals so I don't have to worry about like all the details of the time stepping in collisions of course you still need to be careful you don't want to the crazy thing is that if you don't if you make it time steps too large two hours will have passed right to each other your clean check will not even know they were on the other side of each other before so you need to be careful about that but this thing is a way to not not have to go to the lowest lowest level of you know checking out what time every collision is happening you have a bunch of bodies that are colliding coming back apart and this thing will give you integrated over time the effect that you'd have on the set of objects if you do this right you can get pretty impressive things so my Joker is one that you've been using I mean here's a video from the Madoka homepage but this is all physics simulation right is a bunch of objects through the equations of motion for them there's a grease of freedom everything we talked about it's all in there and then as you run this to find those generalized forces that are part of you emotionally I shouldn't need to understand when collisions happen when they don't happen and simulate the forces that or the impact the impulse that comes from them and so we choke we can simulate a very wide range of robots and objects similarly for bullet which is I mean I guess there are rivals in some sense a lot of people like to use both of them they're both very highly used these days bullet also spends a lot of time with a builder of bullet and interestingly both of these are essentially not exactly but almost one-person project like Madoka is essentially a Matata of University of Washington that's what he does and so it's a one-person project building one of one of the two most widely used simulators in anything robotics and reinforced one against the fourth research sending four bullet is Erin Cummins from he's now at Google but used to do it on his own before and essentially it's I would say so how do I send all him that's not a fair thing cuz people do contribute but it's again largely just one person managing to build this entire thing it's pretty amazing cuz most software these days is not a one person effort and definitely not complicated software like this but in both cases it actually has been single dry singular driving forces making it happen and everybody using it keep in mind none of these simulators are perfectly accurate why they're not perfectly accurate one now do you understand how they work you realize it's hard to make it perfectly accurate because you do a discrete time stepping you have to collision check the forces that come from that and there's complicated geometries and so forth never going to be perfect that's one and two you never have perfect measurements of the world so the initial state is not right and the properties of your robot are not exactly right you put in an ER so you put in measurements you put in friction coefficients all of those are not precise and so we'll see next Tuesday Josh Tobin from opening our formerly Berkeley PhD student then opening our research scientist who pioneered a lot of the work on domain randomization will give us a lecture on how you can get around this given that your similarities are never going to be perfect but hopefully someone representative of real-world how you can randomize your simulator in a way that you end up with learning something that can actually be effective in the real world so that's next Tuesday no lecture on Thursday just the exam on Thursday and yeah next lecture on Tuesday Thanks 