 hello and welcome to the UCL ex deepmind lecture series my name is Alex graves I'm a research scientist at deep mind and I'm going to be talking to you today about attention and memory in deep learning so you may have heard people talk about attention in neural networks and it's it's really is emerged over the last few years as a really exciting new component in the deep deep learning toolkit is one of the in my opinion is one of the last new things that's been added to our toolbox and so in this lecture we're going to explain how attention works in deep learning and we're also going to talk about the linked concept of memory and so you can think of memory in some sense as attention through time and so we're going to talk about a range of attention mechanisms those that are implicitly present in any deep network as well as more explicitly defined attention and then we'll talk about external memory and and what happens when you have attention to to that and how that provides you with selective recall and then we'll talk about transformers and variable computation time so I think the first thing to say about attention is that it is not something it is it's not only something that's useful for learning it plays a vital part in human cognition so the ability to focus on one thing and ignore others is really vital and so we can see this in our everyday lives we're constantly bombarded with sensory information coming from from all directions and we need to be able to pick out certain elements of that signal in order to be able to concentrate on them so a classical example of this is known as the cocktail party problem when people are attending a noisy party and listening to lots of other people talking at once we're still easy able to easily pick out one particular speaker and kind of let the others fade into the background and this is what allows us to hear what they're saying but there's also kind of a form of introspective or internal attention that allows us to append to one thought at a time to remember one event rather than all events and I think the crucial thing that I want you to the crucial idea that I want you to take away from this is that attention is all about ignoring things is it's not about putting more information into a neural network it's actually about removing some of the information so that it's possible to focus on specific parts now I know you've all heard about neural networks and how they work and it might seem at first glance that there's nothing about a neural network that is particularly related to this notion of attention so we have this you know this is this big nonlinear function approximated that takes vectors in and gives vectors out and so in this kind of paradigm attic example you have an image coming in being processed and then a classification decision coming out is is it a leopard or a Jaguar or a cheetah in this image and this doesn't appear to have much to do with attention at first glance the whole image is presented to the network and then a single decision is made but what you can actually find if you if you look inside neural networks and analyze what they're actually doing with the data is that they already learn a form of implicit attention meaning that they respond more strongly to some parts of the data than others and this is really crucial so if you want to distinguish you know a leopard for example from a tiger or something like that part of what you need to focus on are the spots in the leopards fur and you need to do that we need to focus on these parts while ignoring perhaps a relevant detail in the background and to a first approximation we can we can study this use of implicit attention by looking at the network Jacobian so the Jacobian is basically the sensitivity of the network outputs with respect to the inputs so mathematically it's really just a matrix of partial derivatives where each element j IJ is the partial derivative of some output unit I with respect to some in you know J and you can compute this thing with ordinary back prop so basically the back calculation that's used for gradient descent can be repurposed to analyze the sensitivity of the network all you do is you instead of passing through the errors with respect to some loss function you set the error is equal to the output activations themselves and then you perform back problem and by doing this we get a feel for what the network what pieces of information the network is really focusing on what it's using in order to solve a particular pass so by way of illustration here's a neural network that's it's known as the dueling network this is from oh and architecture presented in 2015 it was used for reinforcement learning now it's it's a network that was applied to playing Atari games and so the input is a video sequence and the output in this case the network has has a two two headed outputs one has attempts to predict the value of the state as it's kind of normal for reinforcement learning of a deep reinforcement learning the other head attempts to predict the action advantage so which is basically the the differential between the value of given a particular action and the expected value overall or to put it in simpler terms it tries to guess whether performing a particular action will make its value higher or lower and we look at the video here this image on the on the Left represents the the Jacobian with respect to the value prediction and what's being shown here so we're seeing the the input video itself this is a racing game where the goal is to try and overtake as many cars as possible without crashing and overlaid on that this red heatmap that we see flaring up this is the Jacobian so the places that are appearing in red are the places that the network is sensitive to so if we concentrate on the left side of this video we can see some things that are of the network is really interested in so one of them is it tends to focus on the the horizon the car is appearing you know just appearing on the screen and of course these are very important as a predictor of how much score the network is likely to obtain in the near future because it's by overtaking these cars that it gets points it's also continually focused on the car itself and obviously that's important because it needs to know its own state in order to believe this value and interestingly it has another area of continual focus which is the score of the bottom so because it's the score that it's attempting to predict the score is the value of these games I it kind of makes sense that knowing what the current score is is very important that's what gives it an indicator of how fast the value is is accumulating if we look at the image on the right which is also a Jacobian plot but this time it's the it's the Jacobian of this action advantage so the degree to which any one particular action is better or worse than the expectation over other actions we see a very different picture first of all we see that there's less sensitivity over all the Jacobian these red areas of sensitivity are a lot less prevalent and when we do show up when they do show up we tend to show up in different places they're not looking so much at the horizon they're not looking at the score very much they tend to flare up just in front of the car that's driving and the reason for that is that the information it needs to decide whether it's better to go right or left is really the information about the cars that are very close to it so that's the point it's only really when it comes close to another car that it has this critical decision about whether it should go right or left and so what I'm trying to get across with this video is that even for the same data you get a very different sensitivity pattern depending on which task you're trying to perform and so this implicit attention mechanism is allowing it to process the same data in two very different ways it's seeing essentially even though it's being presented with the same data it's effectively seeing different things and seeing these different things is what allows it to perform different tasks so once again this the whole point about attention and the whole reason it's so important is that it allows you to ignore some parts of the data focus on others and this same concept also applies to recurrent neural networks I think you've covered recurrent neural networks in an earlier lecture and the idea here is that you've got a lecture that basically takes an input sequence to take sequences inputs and produces sequences as outputs and what really makes recurrent neural networks interest things that they have these feedback connections that give them some kind of memory of previous inputs and what we really want to know and as I said at the start of the lecture memory can be thought of as a tension through times what we really want to know about recurrent neural networks is how are they using the memory to solve the paths and once again we can appeal to the Jacobian to try to measure this use of memory this use of past information or surrounding context and in this case I tend to refer to it as a sequential Jacobian because what you're really doing now instead of getting it to a two-dimensional matrix of partial derivatives you're really looking at a three dimensional matrix where the third dimension is to time and what you care about mostly is how sensitive is the network how sensitive are the decisions made by the network at one particular time to those inputs over other times in other words what what part of the sequence does it have to birds that have to recall in order to solve the task okay so to make that a little bit more concrete we've got the sequential Jacobian is a set of derivatives of one network outputs of one output at one particular point in time with respect to all the inputs over time so there's a time series there's a sequence of these 2d Jacobian matrices and what it can what you can use this sequential Jacobian to analyze is how the network responds to inputs that in the sequence that are related in the sense that they are needed together in order to solve a particular aspect of the past but are not necessarily together or contiguous or close to one another in the input sequence they may be widely separated and so in the example of God here this was from a network that I worked on some years ago that was trained to do online handwriting recognition so online handwriting recognition means that someone is in this case writing on a whiteboard with a pen that has an infrared tracker that keeps track of the location of the pen and is therefore able to record a trajectory of pen positions and it also records a special end of stroke markers for when the pen is lifted off the whiteboard and so this text at the bottom shows that the the words that the person wrote were once having and then the the sort of this next this next graph up on the bottom shows how the information was actually presented to the network so the network actually saw was a series of these coordinates x and y-coordinates for these end of stroke spikes and then above that excuse me above that what we have is the sequential Jacobian and now what I've really looked at here what I'm really interested here is the the magnitude of the sequential Jacobus all these matrices over time and what I'm really interested in is is essentially the magnitude of the matrix the magnitude of the response of the network so of that particular of one particular network output with respect to the inputs that the time and so the the network out there chosen is the point so I should say the task here is for the network to transcribe this these online pen positions and to kind of to recognize what it was that the person wrote and see these there's this output sequence here where it's emitting label decisions onz e in the space character and it misses out the V in this case it wasn't entirely classified or transcribed this image correctly but the point that we are looking at is the point where it decides the output letter I in having and what's really interesting if we look at the sequential Jacobean we can see that there is a peak of sensitivity around here which roughly corresponds to the point in the input sequence where the stroke the main body of the letter I was actually written so it makes sense that there's a peak of sensitivity here however we can see that the sensitivity also extends further on in the sequence it doesn't extend so far back in the sequence only very slightly so that sensitivities mostly to the end and I believe the reason for this is that this suffix ing the ink at the end of having is a very common one and so being able to identify that whole suffix helps you to disambiguate the the letter I helps to tell you for example that it's not an owl in there and what's really interesting is this peak is very sharp peak right at the end what that corresponds to is the point when the writer lifted the pen off the page off the whiteboard and went back to both the ISO they wrote this entire word having as one continuous stroke and their cursive handwriting and then they lifted the pen off the page and put a little dot there and of course that dog is crucial to recognizing an eye right that's the thing that really distinguishes an eye from an owl so again it makes sense that the network is particularly sensitive to that point but it's nice to see that by analyzing this sequential Jacobian you can you can really get it to a quantifiable sense of the degree to which is using particular pieces of information and once again and what the stress was really critical here is that means it's ignoring other pieces of information it's focus those parts of the sequence that are relevant and ignoring those that are irrelevant and you know we can see that this is really quite quite powerful it's able to bridge things that are related in the input sequence but may actually be quite far apart another example here comes from machine translation now a major challenge in machine translation is that words may appear in a completely different order in a different language and so we have a simple example here where we have this infinitive to reach at the start of an English sentence that's being translated into German but in German the corresponding verb appears at the end of the sentence and so in order to correctly translate this the network needs to be able to reorder the information and from this paper in 2016 what it showed was just with a very deep network without any any kind of specific mechanism for rearrangement or for attention the network was able to use its implicit attention to perform this this rearrangement and so what we're seeing in the heat map on the right here is again this idea of sensitivity is a sensitivity map of the outputs at particular points in the target sequence so in the German sequence with respect to the inputs in the English sequence and you can see mostly there's a kind of diagonal line because in this particular case most of the sequence most of the words have a more or less direct sort of one-to-one translation but there's this part at the end of the sequence for the final two words in German are particularly sensitive to the words at the start in English so this word reach is is there's a peak of sensitivity from the end of the sequence and of course this is this is once again showing that the network is able to use this implicit attention that it gets in some sense for free just by being a very deep network by being a very you know rich function approximator is able to use that to focus in on a particular part of the sequence and to ignore the rest of the sequence well you know implicit tension is great but there are still reasons to believe that having an explicit attention mechanism might be a good thing so what I mean by the explicit attention mechanism is one where you actually decide to only present some of the data to the network and you know completely remove other parts of the data and one reason this might be preferred of course is computational efficiency so you no longer have to process all of the data you don't have to feed it to the network at all so you can save some compute there's a notion of scalability so for example if you've got a fixed size what I call a glimpse or like a Fourier ssin where you you take in a fixed size part of an image then you can scale to any size image so that the resolution of the input doesn't doesn't have to sort of alter the architecture of the network there's this notion of sequential processing of static data which i think is an interesting topic so again people look at kind of visual example if we have a for real gaze moving around a static image that what we get is a sequence of sensory input and of course this is how images are presented to the human eye were always actually even if the data is static we're always actually receiving it as a sequence and there's reason to believe that doing this gives can can improve the robustness of systems so for example there was a recent paper that showed that networks with sequences of glimpse or fovea attention mechanisms for static data were more robust to adversarial examples than ordinary convolutional networks that looked at the entire image of on go um last but not least there's a big advantage here in terms of interpretability so because explicit attention requires you know making a hard decision and choosing some part of the data to look at you can analyze a little bit more clearly what it is the network is actually using so we know the physical tension we've got we've looked at the Jacobian as a guide to what the network is looking at but it really is only a guide it's not really it's not a necessarily an entirely reliable signal as to what the network is is using and what its was with the explicit attention mechanisms as we'll see you get a much clearer indication of the parts of the data the network is actually focusing on okay so the basic framework for what I'm going to call neural attention models is that you have a neural network as usual that is producing an output vector as always but it's also producing an extra output vector that is used to parameterize an attention model so it gives some set of parameters that are fed into this attention model I which will describe in in a minute and and that model then operates on some data whether that's an image that you're looking at or audio or text or whatever it is and gives you what I'm gonna call a glimpse vector and this is non-standard terminology I'm just using it because I think it helps to kind of unify these these different models that glute inspectors then passed the network as input at the next time step and so there's this kind of loop going on where the network makes a decision about what it wants to tend to and that then influences the data it actually receives it the next step and what that means is that even if the network itself is feed-forward the complete system is recurrent it contains a loop okay so the you know this the the way this the way this model usually works is that we define a probability distribution over glimpses G of the data X given some set of attention outputs so I said this attention vector a and that's used to parameterize something like the probability of gluts G given a so the simplest case here is we just split the image into piles in this image on the right here you can see there's nine possible tiles and age is the sines probabilities through a set of discrete glimpses as in to a set of two each of these tiles that's using one of these tiles so it's just a kind of good old-fashioned softmax function here where the softmax outputs have the probabilities of picking each tile and so we can see that having done that if we have a network that is using this distribution what it's going to do is you know output some distribution over these nine tiles and then at each point in time it's going to receive one of the tiles as input so rather than sieving receiving the whole input at once it's going to keep on looking at one tile at a time now one issue with this of course is that it's a hard decision and what I mean by a hard decision is it's no longer we no longer have a complete gradient with respect to what the network is done basically what we've got is a stochastic policy in reinforcement learning the terms that were sampling from in order to get big glimpses and we can train this with something like reinforced so kind of given it you know the simple kind of standard mathematics here for how you get a gradient with respect to some stochastic sort of discrete sample using reinforce and and this is a sort of general trick here we can use these sorts of what I'm gonna call RL methods by which I really just mean methods that are designed for getting a training signal through a discreet policy and we can sort of fall back on these for supervised tasks like image classification anytime there's a non differentiable module in what we can't do is just ordinary end-to-end backprop and this is a assists a significant difference between using kind of hard attention as I described it so far our versus using this implicit attention that's always present in neural networks so generally we want to be something a little bit more complex than just a soft max over tiles one example that I've kind of already alluded to is this notion of a phobia model where you have a kind of multi resolution input that looks at the image takes part of the image at high resolution so in this case the center this square in the center here is kind of recorded at high resolution as basically it's mapped to one to one this next square out is also presented to the network but at a lower resolution so you can see the actual is taking something that's maybe has twice as many pixels as the one in the middle and some sampling it down to something with the same number of pixels and then the third square out looks at the entire image here that gives this very kind of squash down low resolution version of it to the network and the idea is that you're mimicking the the effect of the human eye where it has high kind of it has high resolution in the center of your gaze and much lower resolution in the periphery with the idea being that the information of the periphery is sufficient to alert you to something that you should attend to more closely you should look at directly in order to get a higher resolution view of it and we can see an example of this apply it to image classification this is from the 2014 paper where then the network was given the cluttered and this data where these endless these familiar endless handwritten digits are basically dropped in an image that has some visual clutter and the idea here is that in order to classify the now the image the network has to discover the the digit within the clutter again once again it's about being able to ignore distractors being able to ignore the noise and the green path here shows the the movement of the this foveal model through the image over this kind of six-point trajectory that is given well it classifies the image and we can see for example in this on the the example in the top row it starts out here in the bottom corner where there isn't much information and then rapidly moves towards the digit in the image and then kind of scans around the the pictures to the right we can see the information that's actually presented to the network basically you know it starts off with something where there's very little information of image but there's a blur over here that suggests there might be something useful and then it moves over to there and by moving around the image it can build up a picture of you know everything that's in the digit that it needs to classify we have a similar example here from the letter 8 where it kind of moves around the periphery of the digit in order to classify it so similar and so one you might ask you know why would you bother doing that when you can you can feed the whole image into the network directly and so one issue I mentioned earlier is this idea of scalability and one way in which a sequential glimpse distribution is more scalable is that you can use it for example to represent multiple objects and Suz explored another paper in 2014 where there were so for example in the street view house numbers dataset there are multiple numbers from people's street addresses present in each image and you want to kind of scan through all of those numbers in order to recognize them in order rather than just looking at the image in a single goal although it can also we provide applied the more conventional image classification shown here once again in order to classify the image network will move its attention around the really important parts of the image and this gives you an indication it allows you to see what it is in the image that is necessary in order to make the classification so so far we've looked at both implicit and explicit attention but the the explicit attention we've looked at has involved making hard decisions about what to look at and what to ignore and this leads to the need to train the network using our l-like mechanisms it makes it impossible to train the whole thing and then the back prop so what we're going to look at in this section is what's sometimes known as soft or differentiable attention which makes gives you explicit attention but makes end-to-end training possible so whereas in the previous examples we had these fixed size attention windows that we were kind of explicitly moving around the image now we're going to look at something that operates a little bit differently and you know it's important to realize that you know if we're if we're thinking about a robot or something where you have to actually direct a camera in order to direct your attention then in some sense you have to use hard attention because you have to make a decision about whether to look left or right but for the kinds of systems were we're mostly focusing on in this lecture that isn't really the case we've got all the data and we just need to make a decision about what to focus on and what not to focus on and so we don't actually need to make a hard decision about attention we want to focus more on some regions and less on others in much the same way that I showed that we already implicitly do with a neural network but we can take this one step further than implicit attention by defining one of these soft attention these differentiable attention mechanisms that we can train and to end and they're actually pretty simple is a very basic template so if we think back to the glutes distribution I talked about before where we have the parameters of the network defining some distribution over glimpses and what we did then was take a sample from that distribution and it was because we were picking these samples that we do to think in terms of training the network with reinforcement learning techniques so what we can do instead is something like a mean field approach we take an expectation over all possible glimpses instead of a sample so it's just this weighted sum where we we take all the glimpse vectors and multiply them by the probability of that glimpse vector given the attention parameters and sum the whole thing up and because it's a weighted sum and not a sample this whole thing is straightforwardly differentiable with respect to the attention parameters a as long as the glooms distribution itself is differentiable which it usually is so now we no longer have you know reinforce or some reinforcement learning algorithm we really just have ordinary and in actual fact because we're doing this weighted sum we don't really technically need a probability distribution at all all we need is a set of weights here so we have a set of weights and we're multiplying them by a attention we're multiplying them by some set of values which are these glimpses and and the weighted sum of these two things gives us the attention readout now there's I've got a little asterisk here on the slide where I'm saying yes we don't actually need a proper probability distribution here but it's usually a nice thing to have so just if we if we make sure the weights are all between 0 & 1 or they some the one that everything tends to stay nicely normalized and sometimes it seems to be a good thing as far as changing the network curves but anyway if we look at this weighted sum this attention readout V which is just now if we think stop thinking probabilistic times and just think of some of our eye times from some weights I and some vectors I this should look familiar to you it's really just an ordinary summation a sigma network a sigma unit from an ordinary neural network and in fact we're where these weights WI look like network weights so we've gone from you know glimpse probabilities defined by the network to to something that looks more like network weights and actually we can think of attention in general as defining something like they do dependent dynamic weights or fast weights is this sometimes not in there fast because they change dynamically in response to the data so they can change in the middle of processing a sequence whereas ordinary weights change slowly they change gradually over time with gradient descent and so to look at these two sort of diagrams I've got here on the Left we have the situation with an ordinary called net where this would be sort of a 1 1 dimensional convolutional Network where you have a set of weights that are given in different colors here that are used to define a kernel that is mapping into this this input that the arrows are pointing into but the point is those weights are gonna stay fixed they're fixed the same kernel is gonna be scanned the same image and those weights are over the same sequence in this case it's one-dimensional and those weights are only gradually changing over time and in addition of course because it's a convolution there's a fixed size to the kernel so we've decided in advance how many how many inputs that are going to be that are fed into this kernel with the tension we have something more like the situation on the right so we have the set of weights that first of all extends can in principle extend over the whole sequence and secondly critically those weights are data dependent they're their function because they're emitted you know they're they're determined by the attention parameters that are emitted by the network which is itself a function of the inputs received by the network so these weights are responding to the input received so they're giving us this ability to kind of define a network on the fly and this is this is what makes attention so powerful okay so my first experience of attention with with neural networks of soft attention with neural networks was a system I developed some years ago now I think seven years ago to do handwriting synthesis with recurrent neural networks so handwriting synthesis unlike the handwriting recognition networks I mentioned earlier here the task is to take some piece of text like this the word handwriting on the left and to transform that into something that looks like cursive handwriting and basically the way this works is the network outputs it takes in a text sequence and outputs a sequence a trajectory of pen positions and these positions define the movement of or define the actual writing of the letters so you can think of this as a kind of sequence the sequence problem with the the challenging thing about it is that the alignment between the text and the writing is unknown and so I was studying this problem with recurrent neural networks and I found that if I just fed the entire text sequence in as input and then attempted to produce the output it didn't work at all what I needed was something that was able to attend to a particular part of the input sequence when making particular decisions about the output sequence so for example I wanted something that would look at the letter H in the input sequence and use that as the conditioning signal for when it was drawing a letter H and move on to letter a and so forth so once again I needed something that was able to pick out certain elements of the input sequence and ignore others and and this was achieved with soft attention so basically the solution was that before the network made each predicted each point and the the handwriting trajectory it decided where to look in the text sequence using a soft attention mechanism and so the mechanism here which is is a little bit different from the normal attention mechanisms that you see in neural networks that we'll talk about later the mechanism here was the network explicitly decided how far along the slide a Gaussian window it had over the text sequence so there was a kind of I thought of it as a soft reading network and so the weights the the parameters mitad by the network determined a set of gaussians this is a shown here Gaussian functions whose either shown here by these couple of curves and those functions had a particular a particular Center which determined where they were focused on the input sequence and on and also it was also able to parameterize the width of the Gaussian so kind of determined how many of the letters in the input sequence it was looking at and I should say the the sequence of input vectors here what I've shown is a series of one hop vectors which is how they presented the network but what these actually correspond to is letters you could think of this as an H here and an A here and so forth and then what the network is deciding is where to put these gaussians which implicitly means once we perform this this summation at the top here that gives us the attention weights what part of the Technic sequence should we look at in order to generate the the outputs distribution and so doing this the network was able to produce remarkably realistic looking handwriting these are all generated samples and you can see that it also generates as well as being able to legibly right particular text sequences it writes in different styles and the reason it does this of course is that is trained on a database of networks of people who sorry I take the base of handwriting from people writing in different styles and so it kind of learns that in order to generate realistic sequences it has to pick a particular style and and stick with it so I'm claiming on the slide that real people right this badly maybe that's not quite strictly true but you know you can see at least the the here is a system where attention was allowing the the network to pick out the salient information and using that to generate something quite realistic and so as I said one advantage of this use of attention is that it gives you this this interpretability it allows you to look into the network and say what were you attending to when you made a particular decision and so this heat map here what it shows is while the network was writing the letters shown the room along the bottom so if the the right thing here is that the handwriting here is the horizontal axis the vertical axis is the text itself and you can see where this heat map shows is what part of the text was the network really focusing on when it was producing a particular when it was predicting a particular part of the Penan trajectory and you can see that there's this roughly diagonal line because of course you know there is here a one really a one-to-one correspondence between the text and the letters that it writes but this line isn't perfectly straight so the point is that some well some letters might take you know have 25 or 30 points in them or even more others letters might have much fewer and so this is the whole issue of the alignment being unknown that attention was able to solve in this instance and so this is an example an early example of what's now kind of photo is location in based attention so the attention is really about just where how far along the input sequence you should look and so it's important what's kind of interesting here is to see what happens if you take that attention mechanism of way and just allow the network to generate handwriting on and this was very similar to the result I obtained when I first tried to you to treat this task as a more conventional sequence the sequence learning problem where the entire sequence was fed to the network at once and what happens is it generates things that kind of look like words that kind of look like letters but don't make much sense and of course it's August the reason for this is that the the conditioning signal isn't reaching the network because it doesn't have this attention mechanism but allows it to pick out which letter it should write at a particular time okay so that was a sort of an early example of a neural network with soft attention but the form of attention that's really kind of taken over the one that you'll see everywhere in neural networks now as what I think of as associative or content-based attention so instead of choosing where to look according to the position within a sequence of some piece of information what you can do instead is attend to the content that you want to look at and so the way this looks is that works is that the network the attention parameter emitted by the network is a key vector and that key vector is then compared to all the elements in the input data using some similarity function so typically you have something like a cosine similarity or something that involves of taking a dot product between the key and all the elements in the date in the data and then typically this is M normalized with something like a softmax function and that gives you the attention weights so you know implicitly what you're doing is you're you're outputting some some key you're looking through everything in the data to see which parts of the data most closely match to that key and you're getting back a vector an attention vector that focuses more strongly in the places that that are more that are closer that correspond more closely to the key and this is a really natural way to search you can actually define you can do you can essentially do everything you need to do computationally just by using content-based lookup and what's really interesting about it is that it especially with this sort of cosine similarity measure it gives you this multi-dimensional feature based lookup so you can put a set of features corresponding to particular elements of this key vector and find something that matches along those features and ignore other parts of the vectors so just by sending other parts of the vector to zero you'll get something that matches on particular features and doesn't worry about others so it has it gives this this multi-dimensional very natural way of searching so for example you might want to say well show me an earlier frame of video in which something red appeared and you can do that by specifying the kind of red element in your the representation of your key vector and then the associative attention mechanism will pick out the red things so typically what's done now is is given these weights you can then perform this this expectation that I mentioned earlier where you sum up over the data you compute this kind of this weighted sum and you get an intention readout what you can also do and this has become I think increasingly popular with attention based networks is you can split the data into key value pairs and use the keys to define the attention weights and the values to define the readout so there's now a separation between what you use to look up the data and what you're actually going to get back when you read it out and this has been used I mean as I said this is now really a fundamental building block of deep learning and it was first applied in this paper from 2014 for neural machine translation and once again so similar to the heat map I showed you in previous slide for implicit attention we have something here that shows what the network is attending to when it translates in this case from I believe it's translating from English to French or it might be from French to English and what's kind of interesting here you can see first of all if we compare this to the earlier heat map I showed for implicit attention it's clear that the decisions are much sharper so you get a much stronger sense here of exactly what the network is attending to and what it's ignoring um secondly in this case there's a more or less one-to-one correspondence between the English words and the French words apart from this phrase European Economic Area that is reversed in French and you can see this reversal here in the image by this kind of line that goes sort of against the diagonal of the rest of the sequence and so this is this is a very as we'll see this is a very powerful general way of allowing the network in a differentiable antenna trainable way allowing the network to pick out particular elements of the input data here's an example of a similar network in use here the task is this is the the task is to determine what this this this removed symbol is in the data so if we look at the example on the Left we have so B I should say the proper names have been replaced by numbered entities here which is quite a standard thing to do in language processing tasks because proper names are very difficult to deal with otherwise and we have this this task where entity 1 one-line identifies the C singer as X and what the network has to do is to fill in X and you can see from this heat map here which words it's attending to when it attempts to fill in this X and you can see it's mostly particularly focused on this entity 23 which was presumably the decision it made and which is indeed correct it says he was identified Thursday as Special Warfare operator and the d23 in general it's focusing on the entities throughout because it can kind of tell that those are the ones that it needs to look at and read answer these questions similarly X dedicated their fall fashion show - mums you can see it's very focused on this particular entity here is helping it make its decision and and what's really crucial here is that there's a lot of text than this piece there's a lot of text that it's ignoring but it's using this content based attention mechanism to pick out specific elements and this can be taken is this you know attention mechanism this combination typically of a recurrent neural network with attention can be used much more broadly it's been applied to speech recognition for example and here we see a plot not dissimilar to the one I showed you for handwriting synthesis where we have an alignment being discovered between the audio ratings and there is a spectrogram and the text sequence that the network is is outputting the characters that it's using to transcribe this data so for example that was this long pause at the start when nothing happens Network mostly ignores that it knows that when it has to start emitting for example the s T you start with the sentence it's very focused on these sounds at the beginning corresponding to those two those noises in the speech signal so basically this attention mechanism is a very general gives a very general purpose technique for focusing in on particular parts of the data and this is all done with well mostly all done with content-based attention okay so another form of so there are a huge number of possible attention mechanisms and we're only going to mention a few of them in this talk and one idea I want you to leave you with is that there's a very general framework here having to find this attention templates that gives you this weighted sum there's lots of different operators you could use to get those those attention weights and one very interesting idea from a network known as draw for 2015 the idea was to determine an explicitly visual kind of soft attention so this is kind of similar to the phobia models we looked at earlier only instead of an explicit and of hard decision about where to move this this phobia around the image rather there was a set of Gaussian filters that were applied to the image and what these did is they have a similar effect of being able to focus in on particular parts of the image anymore other parts but it's all differentiable end to end because there's a filter that is being applied everywhere that gives you these attention weights and what does this filter look like well if you look at these three images on the right we show that four different settings of the parameters for the Gaussian filters the filter variants essentially the you know the width of the filter the center the stride as we've shown here with which the filter is applied throughout image also this last parameter for intensity by varying these we get different views of the same letter v so this one here is quite focused in on this central part of the image this one here is looking more at the image as a whole and it's doing so with quite low variants so it's getting quite a sharp picture of this image this one on the bottom here is getting a more kind of blurred like less distinct view of the entire image and so we can see a video of drol Network in action what we're seeing here the movement of these green boxes shows the attention of the network I'm just going to play that again it's rather quick the attention of the network as it looks at an endless digit and you can see that it starts off kind of look attending to the whole image and then very quickly zooms in on the digit and moves the box around the digit in order to read it and it does a similar thing when it starts to do generate theta it's it uses so this red box shows attention is as its generating with agent once again it starts off kind of generating this kind of blurred out view of the whole image and then focuses down on a specific area and kind of it does something that looks a lot like it's actually drawing the image it's actually using the attention mechanism to trace out the strokes of the digit and so again what's nice about this so we have something that's kind of transforming excuse me transforming a kind of static task into a sequential one where there's this sequence of Association decisions being sre this sequence of glimpses or views of the data and what's nice about that is that we get this generalization so we can now generate because it generates these images kind of one part at a time it can be extended to something that generates multiple multiple digits for example within the same image and this is a sort of a general an illustration of this general property of I think scalability that are referred to for engine mechanisms so far what we've talked about is attention applied to the input data being fed to the network but as I mentioned at the start of the lecture there's another kind of attention which I think of as introspective or kind of inward attention where we as people use our kind of user kind of cognitive attention to pick out certain thoughts or memories and in this section I'm going to discuss how this kind of attention can be introduced to two neural networks so as I've said in the previous slides what we were looking at was attention to external data so deciding where in a text sequence to look which part of an image to look at and so forth but if we sort of apply this attention mechanism to the network's internal state or memory then we have this notion of introspective attention and as I've said a way I like to think about this is that memory is a tension through times a way of picking out a particular event that may have happened at some point in time and ignoring others once again just want to come back to this idea that attention is all about ignoring things it's all about what you don't look at and so there's an important difference here between internal information and external information which is that we can actually modify the internal information so we can do selective writing as well as reading allowing the network to use attention to iteratively modify its internal state and an architecture that I and and colleagues deepmind developed in 2014 did exactly this we called it a neural Turing machine because we what we wanted was something that resembled the the action of a Turing machine is the ability to read from and write to a tape using a neural network by a an attention set of attention mechanisms I'm going to talk about this architecture in some detail because it shows it gives you a sort of nice insight into the the variety of things that can be achieved with attention mechanisms and it shows how it really shows this link between attention and memory so the controller in this case is a neural network it can be recurrent or it can be feed-forward once again even if it's feed-forward the combined system is is recurrent because there's this loop through the attention mechanisms and then we have we referred to the attention modules that are parameterized by the network as heads and so this was in keeping with the with the Turing machine in analogy the tape analogy this is something that I think has been has been picked up in general people often talk about attention heads and you know these are these these heads our attention mechanisms or soft attention mechanisms in the same kind of template that we've discussed before and their purpose is to select portions of the memory the memory is just this real value matrix it's just a big grid of numbers that the network has access to and the key thing is different is that as well as being able to select portions of the memory to read from these these heads can also selectively write to the memory so yeah once again this is all about selective attention we have to we don't want to modify the whole memory in one go maybe you know I should I should stress here the key part of the kind of designing decision underlying the neural Turing machine was the separate out computation from memory in the same way as it's done in a normal digital computer we didn't want so for a normal recurrent neural network for example in order to give this is the more memory you have to make the hidden state larger which increases the amount of computation done by the network as well as giving it more memories a computation and memory are kind of inherently bound up in an ordinary network and we wanted to separate them out we would have potentially quite a small controller that could have access to a very large memory matrix in the same way that a small array of a processor in a digital computer can have access to you know a large amount of RAM or disk or other forms of memory and and so it's key you know if you look at in that from that perspective it's key that it's not processing the entire memory at once if this thing's going to be large it needs to selectively focus on parts of it to read and write and so we do this basically using a similar the same template as I mentioned before for soft attention the controller the neural networks outputs parameters that basically parameterize this what we're calling a distribution or awaiting over the rows in the memory matrix but is it this waiting is really just the same attention weights that we discussed before and we had two main attention mechanism so I've mentioned in in the previous section that my my first experience of soft attention in neural networks was around location-based attention as was a ID for this handwriting synthesis Network which was in fact the inspiration for the neural Turing machine so having realized that the handwriting synthesis Network could selectively read from an input sequence I started to think of what would happen if we could write to that sequence as well and wouldn't it then start to resemble a neural Turing machine um but as well as the location-based content that was considered in the handwriting synthesis Network this also incorporates content based information attention which as I've said is the kind of the the preeminent form of attention as used in neural networks so dressing by content looks a lot like it does with other content based networks is the key vector emitted by the controller that is compared to the content of each memory location so each row in memory and treat that as a vector and then we compare the key to that vector using the similarity measure which wasn't the cosine similarity which we then normalize with the softness we also introduced a extra parameter which isn't usually there for content-based attention which we called sharpness and this was used to sort of selectively narrow the focus of attention so that it could really focus down on individual rows in the memory but we also included this notion of addressing by location and the way this worked was that the network looked at the previous waiting and put a shift kernel which was just a sort of a soft max of numbers between plus and minus N and we then essentially convolved that with with the waiting with the the previous waited waiting from the previous time step to produce a shifted waiting so basically the mass your very simple as shown below and what this thing is it just essentially shifted attention through the the memory matrix shifted it bounds if you started here and I'll put a shift colonel focused around maybe five steps or so then you'd end up with intention and attention distribution that would look like this and so this combination of addressing mechanisms the idea behind this was to allow the controller to have different modes of interacting with the memory and we thought about these modes as corresponding to data structures and accessors as I used in sort of conventional programming languages so as long as the content is being used on its own the memory is kind of being accessed the way it would be in something like a dictionary or an associative map well not strictly like a dictionary because we didn't have key value attention for this network although you could define it um rather it would be like an assault more like an associative array through a combination of content and location what we could do is use the content-based key to locate something like an array of contiguous vectors in memory and then use the location to shift into that array the shift index a certain distance into the array and when the network only used the location based attention is essentially iterated it acted like an iterator just moved all in from the last focus so it could essentially read a sequence of inputs in order so basically reading we as we've said this network uses attention to both read from and write to the memory reading is very much the standard attention sort of soft attention template we get a set of weights we get a set of rows in the memory matrix to which those you know that are that are to which the network is attending and we compute this this weighted sum so we take each row in the metrics and multiply it by the weight which gives the sharpness of attention that agreed to which the network is attending to that particular row so this is just very much this is exactly like the the soft attention that I described soft attention template I described before only it's being applied to this memory matrix rather than being applied to some external piece of data the part that was kind of novel and unusual was the the right head the writing attention mechanism used by neural Turing machines and so in this case we kind of inspired by the way long short-term memory LS TM has sort of forgets and and input gates that are able to to modify the contents of memory the contents of its own internal state we defined a combined operation of an arrays vector E which behaves sort of analogous Li equivalently to the forget gate in long short-term memory and an ADD vector which behaves like the input gate and essentially what happened is the once the the right head had determined which rows in the matrix it was attending to these the contents of those rows were then selectively erased according to E and I should say here so E is basically a set of numbers between 0 & 1 so basically if if some part of the erase vector goes to 1 that means whatever was in the memory matrix at that point is now is now white set to 0 and if E is set to 0 then the memory matrix is left as it is so once again there's this kind of smooth or differentiable analogue of what is essentially a discrete behavior the decision of whether or not to erase and adding is more straightforward it just says well take whatever's in memory and adds whatever's in this add vector a multiplied by the the the write weights so basically if the right weight is high and you're strongly attending to a particular area in a particular row in the matrix then you are going to essentially add whatever is in this add vector to that to that row and if the white right vector the important thing here is if if this WI for all the rows in the matrix for which this WI is very low nothing happens nothing changes so if you're not tending to that part of the memory you're not modifying it either so how does this work in practice so what we what we really looked at was well can can can this neural cheering machine learn some kind of primitive algorithms in the sense that we think of algorithms is applied on up on a normal computer and you know in particular does having this separation between processing and memory enable it to learn something something more algorithmic than we could do for example with a recurrent neural network and we found that it was indeed able to learn it's a very simple algorithm so the simplest thing we looked at was a copy task so basically a series of random binary vectors were fed to the network list that's shown here we started the sequence and then the network just has to copy all of those and output them to through the the output vectors now all it has to do is just exactly copy what was in here to what's over here so it's an entirely trivial algorithm that would be you know very uninteresting it's not it's not interesting in its own right as an algorithm but what's surprising about it is that it's difficult for an ordinary neural network to do this so neural networks generally are very good at pattern recognition they're not very good at exactly sort of remembering storing and recalling things now it's exactly what we hope to add by including this access to this memory matrix and so the algorithm that it uses well a kind of pseudocode version for it here is given on the right but we can also analyze it by looking at the use of attention and the way it depends the particular places in the memory during during this task and so these two heat maps that are shown here at the bottom are again heat maps showing the the degree to which the network is attending to that particular part of the memory so when it's black it's being ignored when it's whitelist focusing and you can see that there's a very sharp focus here which is what we want because it's it's it's basically implementing something that is really fundamentally a discrete algorithm and so what it does in order to complete this copy pass is it picks a location in memory given here and then starts the Wrights whatever input vector comes in essentially just copies that to a raw memory and then uses the location based iterator this location based attention you just move on one step to the next grow memory and then it copies the next the next equipment so forth until it's finished copying them all and then when it has the output it uses its low content based lookup to locate the very start of the of the sequence and then just iterates through until it's copied out everything remaining and so you know just once again what was really interesting here was to be able to get a kind of an algorithmic structure like this going through a neural network you know it was completely parametrized by neural network and was completely learned and to end um and there was nothing sort of built into the network to adapt it towards this sort of this sort of algorithmic behavior and so the real issue is and actually a normal recurrent neural network analysis the end model Alice TM network for example can perform this task you feed it in a sequence of inputs and ask you to reproduce them as outputs just as a sequence the sequence learning problem but what you find is it will work reasonably well up to a certain distance but it won't generalize beyond that so if you train it the copy of sequences up to length 10 and then ask it to generalize the sequences up to length you know 100 you'll find it doesn't work very well as we'll see whereas with the neural Turing machine we found that it did work quite well so in this these heat maps here we're showing the targets and the output so basically this is the the copy sequence given to the network if it's doing everything right the each block at the top exactly matches each block at the bottom and you can see that it's not perfect like there's some mistakes creeping in as the sequences get longer so this is for sequences you know short sequences like 10 20 40 so on but you can still see that most of the sequence is still kind of retained like most of the targets are still being matched by the outputs in the network and that's because it's just basically performing this algorithm and using that to generalize the longer sequences so this is an example of where attention and being able to selectively pick out certain parts of information ignore others gives you a stronger form of generalization and so that that this this form of this kind of generalization that we see with no Turing machines does not happen with you know a normal LSD end model for example essentially it learns the copy up to ten and then after ten it just goes completely awry it starts to it sucks the output random mush and this really shows that it hasn't learned an algorithm it's rather kind of hard-coded itself has learned internally to store these ten things in some particular place in its memory and it doesn't know what to do when it goes beyond that so in other words because it lacks this attention mechanism between the network and the memory it's not able to to kind of separate out computation from memory which is what's necessary to have this kind of generalization and so this can be extended we look at other tasks well one very simple one was to learn something akin to a for loop so if the network is given a random sequence once it's then also given an indicator telling it how many times it should reproduce that output sequence and then it just has to output the whole sequence end times copy end times and so basically what it does is just uses the same algorithm as before except now it has to keep track of how many times its output the whole sequence so it just keeps on jumping to the start of the array with content based you know to a particular row in memory using content based location then iterates one step at a time gets to the end and jumps back and meanwhile it has this sort of internal variable that keeps track of the number of steps that it's done so far another example of you know what it could do with memory was this Engram inference task so here the is that a sequence is generated using some random set of Engram transition transition probabilities so basically saying given some binary sequence given the last three or four inputs there's a set of probabilities telling you you know whether the next input will be a0 or a1 and those probabilities are sort of randomly determined and then you generate a sequence from them and what you can do as the sequence goes on you can infer what the probabilities are and there's there's a you know a sort of a Bayesian algorithm for doing this optimally but what we're sort of interesting is well how well does a neural network manage to do this how well does it sort of it's kind of like a meta learning problem where it has to look at the first part of the sequence work out what the probabilities are and then start making predictions in accordance with those probabilities and what we found was that yes once again Ellis team can kind of do this but it doesn't do it in a very kind of it makes quite a lot of mistakes I've indicated a couple of them with red arrows here oh no sorry excuse me those red arrows I think actually indicate mistakes made but the neural Turing machine but in general the the neural Turing machine was able to sort of perform this task much better and the reason it was able to do that is that it used its memory use specific memory locations to store variables that count of the occurrences of particular Engram so if it had seen 0 0 1 for example it would use that to define a key to look up a place in memory might be this place here and then the next time it's all 0 0 1 it would be able to increment that which is basically a way of saying okay if I've learned that 0 0 1 is a common transition that means that the probability of one followed by of to 0 is followed by a 1 must be really eyes and basically it learns to count these occurrences which is exactly what the optimal Bayesian algorithm does and it uses it is able to do this by being able to pick out selective specific areas in its memory and use those as counters ok so here's a little video kind of showing the system action so this is being performed this repeat and times tasks or the start here where the network is going quickly we see what happens during training then we have the system everything slows down here we have you know what happens with a chained version the network the input data comes in while the input data was coming in we saw this this blue arrow here which showed the input data being written to the network memory one step at a time so it's stored this input sequence in memory and then the writing task begins once the writing task begins we see this red arrow which represents the the right waits the attention the attention parameters used for writing and we can see that these are you know very tightly focused on one particular row in the network the one that it's admitting as output at any one point in time and it's then iterating through this array one step at a time and what you can also see as this video goes on is that the the sorry the the size of the circle size and colors of the circles here represent the magnitude of the variables within this memory matrix I believe they're sort of the hot colors are positive and the cold colors are negative as I remember but as what's happening as the network is looping through this whoops is running through this loop several times just from that video again and we can see during training that you know at first these attention these these these read and write weights are not sharply focused they're blurred out so this sharp focus kind of comes later on once the network is finished writing the whole the whole sequence it then sort of you see this these variables in the background become larger and that's because it's using those to keep count of how many times it's being through the how many times it's repeated this copy operation and then at the end the changes this this this final this row at the bottom which is an indicator the network that is that the task is complete so it's using this memory to kind of perform an algorithm here and so quickly I'm just going to mention we following the neural Turing machine we introduced the kind of extended version of a successor architecture which we call differentiable neural computers and we introduced a bunch of new attention mechanisms to provide memory access and I'm not going to go through that in detail but just to say one of the rather than looking at algorithms with this sort of updated version of the architecture what we were really interested in was looking at graphs because so while recurrent neural networks are designed for sequences in particular many types of data were naturally expressed as a graph of you know nodes and and links between nodes and because of this this ability to store information in memory and the store and recall with kind of something that came into random access it's possible for the network to store quite a large and complex graph in memory and then perform operations on it and so what we did during training of the system is we looked at random randomly connected graphs and then when we tested it we looked at specific examples of graphs so one of them was the graph representing the zone 1 of London Underground and we were able to ask questions like well can you can you find the shortest path between war Gate and Piccadilly Circus or so or can you can you perform a traversal when you start an Oxford Circus and follow the central line and the circle line and so forth and it was able to do this because they could store the graph in memory and then selectively kind of recall elements of the graph similarly we asked it some questions about a family tree where it had to determine complex relationships like maternal great-uncle and order to do that it had to keep track of all the links in the graph so for the for the remainder of the lecture we're going to look at some further topics in attention and deep learning so one type of deep network that's got a lot of attention recently is known as transformer networks and what's really interesting about transformers as they're often known from from the point of view of this lecture is that they are they really take attention to the logical extreme they basically get rid of everything else the all of the other components that could have been present in similar deep networks such as the recurrent state present in recurrent neural networks convolutions external memory like we discussed in the previous section and they just use attention to repeatedly transform a data sequence so it's a the idea that the paper that introduced transformers was called attention is all you need and that's really the the fundamental idea behind them is that this this attention mechanism is so powerful it can essentially it can essentially replace everything else in a deep network and so the attention the form of attention used by transformers is is a little it's it's mathematically the same as the attention we've looked at for before but the way it's implemented in the network is a little bit different so instead of there being a controller Network as there was in the neural Turing machine for example that emits some set of attention parameters that are treated as a query rather what you have is that every vector in the sequence emits its own query and compares itself with every other and sometimes I think of this as a sort of emergent or anarchist attention where the attention is not being dictated by some central control mechanism but is rather arising directly from the data and so in practice you know what this means is you have quite a similar calculation to quite quite a similar attention mechanism to the content-based attention we've discussed previously where there's a cosine similarity calculated between a set of vectors but the point is that there's a separate key being emitted for every vector in the sequence that's compared with every other and like as with NTM endian see there are multiple attend heads I use in fact every every point in the input sequence gives not just one [Music] one set of not just one attention key to be compared with the rest of the sequence but several so I'm not gonna go very much into the details of you know how transformers work the although the attention mechanism is straightforward the actual architecture itself is fairly complex and actually I recommend this blog post the annotated transformer for those of you who want to understand it in more detail but if we look at the kinds of the kinds of operations that emerge from the system is very intriguing so as I've said there's this the the idea is that a series of attention mechanisms are defined so transformers I should say are particularly successful for natural language processing and I think the reason for that and the reason that recurrent neural networks for attention were also first applied to languages that in language this idea of being able to attend to things that are widely separated in the input is particularly important so there's one word at the start of the paragraph might be very important when it comes to understanding something much later on in the paragraph there may be if you're trying to extract for example the the sentiment of a particular piece of text there may be several words that are very well that are very spaced out that are required in order to make sense of it so this is sort of a it's a natural it's a it's a natural fit for attention based models so in this particular example here from the paper we see that when the so the network has has processed has created keys and and vectors for each element in the input sequence and then and this creates a you know a sequence that the sequence of embeddings equal to length of the original sequence and then this process is repeated at the next level up so the network basically now now finds another set of key vector pairs at every point along the sequence and those key vector pairs are then compared with the original embeddings to create these attention masks and so we see for this word making here this is being this is being while this word is being processed I forget what the exact task was here while this well this word making was being processed it was attending to a bunch of different words laws 2009 the word making itself but also this this phrase more difficult here at the end of the sequences all of these things are are tied up with the semantics of how this word making is used in the sentence and generally what you find is you get these these difference as I said there are there are multiple attention vectors being defined for each point in the sequence and what you get are different patterns of attention emerging so for example in this example here and this is showing the kind of the all-to-all attention so how the embedding corresponding to each word in the sentence at one level is attending to all of the embeddings at another level and we see that this one is doing something quite complex it seems to be looking for phrases or what the word what is kind of attending to this is what the law the word law is attending to the law and it's and so forth so there's some kind of some you know complicated integrating of information going on whereas here another one of the sort of attention masks for the same network we see that it's doing something much simpler which is it's just attending to nearby words and so the overall effect of having all of these access to all these attention mechanisms is that the network can learn a really rich set of transforms for the data but what they realize is the transformer networks that just by repeating this process many times they could get in a very very powerful model in particular of language data and so from the original paper they already showed that you know the the the transformer was achieving state-of-the-art results with machine translation since then has kind of gone from strength to strength you now provide state of the Arts for language modeling has also been used for for other data types besides language it's been used for speech recognition it's been used for two-dimensional data such as images but from this this blog post here this was this was posted by by open AI in 2019 we can see just how powerful a transformer based language model can be so in language modeling essentially just means predicting iteratively predicting the next word and a piece of text or the next sub word symbol and so in this case once the the language model is trained it can be given a human prompt and then you can generate from it just by asking it to predict what word things will come next and then feeding that word in and repeating this whole transformer based network that is able to attend to all of the previous context in the data and what's really interesting about this text relative to texts that have been generated by language models in the past is that it manages to stay on topic is that manages to keep the context intact throughout a relatively long speech a relatively long piece of text so having started off talking about a herd of unicorns living in an unexplored valley in the Andes it continues to talk about unicorns it continues to it keeps the setting constant in the Andes Mountains it you know vents a biologist from the University of La Paz and once it's made these inventions for example once it's named the biologist it keeps that name in fact I haven't called it Perez once it knows to keep on calling on Paris throughout and the reason it can do that is that it has this really powerful use of context that comes from having this this ability to attend to everything in the sequence so far so what what attention is really doing here is allowing it to span very long very long divides very long separations in the data this is something that even you know before attention was introduced even the most powerful recurrent neural networks such as LST M they struggled to do because they had to store everything in the internal state of a network which was constantly being overwritten by new data so between the first time Perez is introduced in the last time there might have been you know several hundred updates of the network and this information about Paris would attenuate during these updates but attention allows you to kind of remove this attenuation it allows you to bridge these very long gaps and that's really the secret to to its power particularly for language moment and so one interesting extension to so they should say there have been many many extensions to transformer networks and they kind of you know go on from strength to strength particularly with language modeling and one one extension that I'd like to look at in this lecture which I find very interesting is known as universal transformers so the idea here is that the the basically the weights of the transformer are tied across each transform so the transforms you if we look at this model here if we have the input sequence over time along the x-axis then this the effect of the transform is to generate a set of of self attention masks at each point belong the sequence and then all of the the embeddings associated with these points are then transformed again the next level up and so on now in an ordinary transformer these parameters that happen the the parameters of the attention the the self attention operations are different at each point going up at each at each transformer each level going up on the y-axis and this means that the the functional form of the transform is being enacted is different at each step so tying these weights going up through the stack makes it act like a recurrent neural network in deaths so what you have is something like a recursive transform and what's interesting about that is that you then start to have something that can behave a little bit more algorithmically something that is not only very good with language but is good at learning the sorts of functions the sort of sort of algorithmic behaviors that I talked about in the neural Turing machine section and so this could be seen from from some of the results for universal transformers where it was applied for example to the baby tasks which are a set of kind of toy linguistic tasks generated using a grammar and so a related topic to this idea so there's this idea here of this having a recursive transform and oh and the other thing I should say is that because the way it's divided it means you can you can enact this transform a variable number of times in just the same way that you can run an RNN through a variable length sequence so now we have something where the amount of time it spends transforming each part of the data and can become variable it can become data dependent and so this relates to work that I did in 2016 which I called adaptive computation time the idea of adaptive computation time was to change so with an ordinary recurrent neural network there's a one-to-one kind of correspondence between input steps and output steps every time an input comes in the network and it's an output and the problem with this in some sense is that ties it ties computation time to what we could call data time there's one pick of computation for every step in the data and now you can you can alleviate this by stacking the layers on top of each other so now you have multiple takes a computation for each each point in the input sequence but the idea of adaptive computation time was that maybe we could allow the network to learn how long it needed to think in order to make each decision we call this the amount of time it's spent pondering each decisions the idea is some network some input comes in at times the x1 the network receives a hidden state it's hidden state from the previous time step as usually the recurrent neural network and it then thinks for a variable number of steps before making a decision and and this variable number of steps is determined by a halting probabilities these numbers of might reading up like 8 the idea is that when that probability passes a threshold of 1 the network when the sum of these probabilities passes the threshold of 1 the network is ready to emit an output and move on to the next time step and so what's the sort of the relevance of this mechanism to the rest of this lecture which has been about more explicit attention mechanisms is that in some sense the amount of time a person or even a neural network spends thinking about a particular decision that it makes it's strongly related to the degree to which it attendance to it so there have been cognitive experiments on with people where you can you can by measuring the amount of time it takes them to answer a particular question you can sort of measure the amount of attention that they're needing to give to that question and so if we look at this concretely if we look at what happens with this adaptive computation time if we apply this to a recurrent neural network this is an ordinary lsdm network that is being applied to language modeling in this case next step character prediction and this what this graph is showing is the the y-axis shows the number of steps that the network stop to think for now this number of steps is actually not it's not integer because there's there's this question where it can it can it can slightly overrun a complete step but that's not really important what's important here is that there's a variable amount of computation going on for each of these predictions that it has to make and you can immediately see a pattern so for example the amount of ponder time goes up when there's a space between words and the reason for this is that it's at the start of words that we need to spend the longest thinking because it's it's basically it's easier once you've gone most of the way through a word it's easy to predict the ending once you've seen P OPL is pretty easy to click the P is gonna come next once you see a space after that e then it becomes harder now you have to think well and the many people what word could come next so this takes a little bit more fall and then it tends to drop down again and it spikes up even further when it comes to a kind of larger divider like a full stop like a comma so there's this very close if we think about if you think back to the the plots I showed you at the very start of this lecture were to do with the visitor tension where we saw that a deep network or recurrent neural network will concentrate in some sense or will respond more strongly to certain parts of the sequence we kind of see that same pattern emerge again when we give it a variable amount of time to think about what's going on in sequence and there's some interesting consequences here so for example one is that the network is only because this is this is a question of how long it needs to take in order to make a particular prediction the network is only interested in predictable data so for example if you see these IV tags this is from Wikipedia data which contains you know XML tags as well we can see that the network is law there's no spike in instead of thinking time when it comes to these ID numbers this is kind of interesting because these ID numbers are hard to predict so it isn't simply that the network thinks longer whenever you'd find something that's harder to Hey it thinks longer when it sort of believes that there's a benefit to thinking longer when thinking longer is likely to make it better able to make a prediction and the reason it would be better able to make it a prediction is that it allows it to spend more time allows it to spend more time processing the context on which that prediction is based and so this kind of goes back again to the idea we talked about in transformers of having these repeated steps of both contextual processing as being the thing that builds up the information the network needs to make a prediction and so there's a nice combination of this this idea of adaptive computation time with these universal transformer models and so in this case here we have a task from the baby data set where there's a series of sentences so these these sentences as entered along the x-axis are kind of like the input from the network then or these are the contexts that the network needs to know about and then it gets asked the question the question here was where was the Apple before the bathroom and if you go through all of these sentences now I think I've cropped this grass it doesn't have all of them but you can see that things are happening with the Apple John dropped the Apple John grabbed the Apple John went to the office so I think the Apple at this point is probably in the office John journey to the bathroom well maybe now it's gone to the bathroom in between those two things where some pieces of information that weren't relevant sandwich the milk for example John traveled to the office were back in the office again so there's a little puzzle here that the network has to work out as to where the Apple has ended up and of course some parts of this sequence are important for that puzzle and some parts are John discarded the Apple there well of course that's very important basically all of the ones that mentioned where John is are important and generally those are the ones that the network spends longer thinking about so we're kind of via this adaptive computation time and bhayya this transformer model where you know and every second time each point along the sequences pending for all of the others but we build up a similar picture in some sense to the one we had at the start of the lecture where we can see that the network has learned to focus more on some parts of the sequence analysis so once again this is what attention is all about is about ignoring things and being selective and so to conclude I think the main point I would like to get across in this lecture is that selective attention appears to be as useful for deep learning as it is for people as we saw at the start of the lecture implicit attention is always in present in some degree in neural networks just because they learn to become more sensitive for certain parts of the data than to others but we can also add explicit attention mechanisms on top of that and it seems to be very beneficial to do so these mechanisms can be stochastic so-called hard attention that we can train in reinforcement learning or they can be differentiable so-called soft attention which could be trained with ordinary backprop in Indian learning and we can use attention to attend to memory or to some internal state of the network as well as to data so many types of attention mechanism have been defined and I should say that even the ones are covered in this lecture only or only cover a small fraction of what's being considered in the field and many more could be defined and I think and what's become you know very clear over the last few years is that you can really get excellent results state-of-the-art results in sequence learning by just using attention by using transformers that essentially get rid of all of the other mechanisms that deep networks have for attending to long-range context and that is the end of this lecture on attention and memory and deep learning thank you very much for your attention you 