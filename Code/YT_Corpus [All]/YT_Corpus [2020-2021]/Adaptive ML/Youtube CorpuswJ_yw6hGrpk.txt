 great well welcome to our talk which is called deep learning at scale with spark and determined I am Neal Conway and I'm joined by my colleague David Hershey so determined is a deep learning training platform and the goal of the product is to enable team the teams of deep learning engineers to train better deep learning models in less time to collaborate more effectively and to seamlessly share GPU resources so in this talk we cover key main topics first we talk about what what determined is and why you might need a training platform and second we explore how you would integrate determined into the rest of your Michigan workflow in particular if you're using spark for other and the spark uses done for other parts of your machine learning workflow how determined can can be integrated with those other components so let's take a step back and present maybe a very typical slide that you might see in a lot of deep learning talks which is you know what are the steps involved in in in doing deep learning at scale so you typically might start with your your raw training data you know labeled labeled examples that are appropriate for the problem that you want to solve and then you have some kind of transformation to to take that raw data and transform it into a more appropriate format for doing machine learning you might store that transform data in a data Lake then comes the actual model training development process where as an ml engineer you explore different model architectures different data representations different optimization techniques to try to get a model that works well on the problem you're trying to solve and then once you have an effective model you deploy it to your to where you want to do inference so you might be doing real time serving or batch inference or serving a mobile device so if we think about those is kind of the basic steps of doing deep learning today how do those how do those aspects of that workflow map to the modern software ecosystem the advanced slide great so once you have your raw data you want to transform a two to a data like spark is a great tool for doing a high-performance ETL at scale and then you know once you've transformed your data and you need to know where to store it something like Delta Lake is a is a very good choice for a mutable version storage of deep learning training sets if we switch over to the serving side when we think about how we wanted you how we want to do real-time inference there are several Rita or real-time model serving there's several good machine learning serving packages Seldon or pencil serving are our two examples and then if we want to do batch inference spark again is a great choice for doing batch inference at scale and that really you know kind of returns us to the middle part of this diagram which is what you know what is our model training a development environment what what does that look like what are the requirements for doing that well so I'll make the case in this talk that determined is is a good implication of that middle middle part of the the deep learning workflow so as I mentioned in this talk kind of two main parts we'll go into a bit more about what a deep learning training platform ought to do and some of the features of the platform we built to determine and some of the benefits that deep learning engineers can see but using it and then the second part we'll walk through a demo of how you integrate determined into the spark ecosystem so in particular how you use it with Delta Lake and spark on the input side and how you use it to do patran friends with spark on the output side so if we think about you know the problems that we're trying to solve with a deep wound a training platform I think an actual question is aren't those challenges of how I train deep learning models aren't those really solved effectively by tools like tends to throw in pie charts and I think the tools that had some pie charts are great great tools and they're very effective at what they're trying to do but ultimately those tools are designed to enable a single deep learning engineer to train a single model on a single GPU or a handful of GPUs but as your team size as your cluster size as your data set size all increase we find that teams that are in that situation frequently rendered challenges that are really you know outside the scope of a tool like of tools like tensor flow or prior torch so for example you know as your team grows as your cluster size grows you might move from having 1/8 GPM machine that a single researcher use it uses too many GP machines a whole cluster how do you share that customer effectively among a team as you get larger models and more GPUs using managed GPS train one model becomes an increasingly important challenge how do you do distributed training that's something that's not well solved within those existing packages it's similarly how do you manage your models how do you manage your metrics with training data and especially as you go from just training a small number of models in a research setting to training many different models and pulling those models production metrics management and model management becomes a much more important concern some early tools that you know tasks that you might do manually like a parameter search and small skill suddenly become increasingly painful if you're training many different models and you secretely important recently important to automate those tasks with integrated support for higher parameter search in an efficient way so in our view there's you know really a cause to have a training platform as as a kind of piece that sits between your application frameworks tools like tensor flow applied torsion terrace and the hardware that you're doing training so that might either be an on-premise GPU cluster or a cloud environment with cloud hosted GPUs on you know cloud providers like yet abused GCP or Azure and again the goal of that training platform is to work with your existing data so determined supports data in a wide variety of formats and to enable bottles trained within the platform to be exported to the environment where you want to use them to do inference but really what it's doing internally is allowing that the deep learning engineers on that team to work together more effectively to train models more quickly to collaborate and to spend their time really doing deep learning and kind of training better models and spend less time on DevOps or on writing boilerplate code for a comment tasks like fault tolerance or distributor training so you know just a little bit more detail into the kind of functional areas within the architecture of the system you can think of determined as essentially a kind of integrated system that combines job scheduling and GPU management distributed training hyper parameter tuning and experiment tracking along with visualization all into kind of an integrated package and the idea is to enable to design each of those tools in a way where they work together smoothly so that as a deepening engineer you can get access to the best debride functionality in these areas and really get to spend more of your time on the problems that you care about deep learning in less time I'm kind of pulling different packages together to form your own ml platform so if that's roughly what the architecture of the system looks like what benefits do you see as a user so what are talked about kind of three key areas where we think the determinant is able to enable you deepen team is to be more productive than they otherwise would be so when it comes to high parameter optimization that's something that is kind of build deeply into the platform so when we initially built determined we started immediately by thinking about when you want to do high parameter search what's the right system support for that what's the straight GPU scheduler support for that what should fault tolerance and metrics management look like if - researcher is gonna be one of my co-founders are determined is me telecard who's a professor at Carnegie Mellon and his research group have invented a number of methods for very efficient high perimeter search algorithms and you know hyper bandits probably the the the best-known of those and those methods are dramatically integrated into the product and what we found is that using really intelligent search methods and then designing system around them leads to very efficient hyper parameter search so in our experience we and find high-quality models up to a hundred times faster than standard methods like random search and after 10 times faster than research methods based on Bayesian optimization second key capability is distributed training so in command you configure distributed training or configure the number of GPUs you want to use their training it multiple just by changing the configuration center you just tell the system I want to train this model with one 16 or 64 GPUs and determine takes care of scheduling that task on the cluster orchestrating their super training operation actually doing the gradient updates and and and so on ensuring that this whole process is fault tolerant as a user you don't need to think about any of those manual operations so I think that has two benefits you know first distributor training becomes a tool you can use much more easily rather than setting up a tool like Avadh by hand every time you want to use it or if ignorant how to use it in a multi-user setting this is kind of a built-in capability you can use whenever it's helpful and second we have a bunch of optimizations on top of stock or Avadh and that lead to significant performance improvements Laskey capability are basically you know a collection of tools that we provide for deep learning engineers that allow them to focus on doing deep learning rather than managing infrastructure or writing boilerplate code so some of those things are you know every workload within our system is automatically fault tolerant so as you're running these workloads that you know might take days or weeks to run we take care of checkpointing models and eight periodically if any failures occur we restore the the state of the model and resume training from the most recent check point we built an experiment tracking visualization we have a job scheduler there fair shares the cluster so that multiple users can very easily share a GPU cluster and make progress at the same time and we really simplify GP and management both on-premise and in the cloud where we can provision GPU instances for you automatically so that was kind of a quick overview of what determine is what a deep learning training flat problem is and some of the key capabilities now I'll turn turn things over to my colleague David who will give you a demo of how to use determine with the spark ecosystem thanks Neil I'm excited to go through this demo it's really showing how determined can fit into an end-to-end ecosystem for machine learning and what I mean by that is being able to build sort of reproducible pipelines where everything from what data you used what model you used and how you train that model how you did inference that's all easily reproducible trackable and really seamless and so what we're gonna be doing today is some amount of ETL using spark tool and image data into Delta Lake that's going to be really useful up flow useful for us to be able to version our data in determine and so we'll be able to see exactly what version of the data set we use to train our model determine we're going to focus a lot on determined and how we can use it to scale our experiments beyond as Neil said sort of the single GPU single researcher scheme to working with a team on a whole GPU cluster and the really cool things you can do when you have a cluster for deep learning and finally we're going to show how determined can sort of manage the artifacts of training to create what we like I'm going to refer to as a versioned model and you can then use that version model to export it and do inference in this case in batch using this Marko system again the example we're going to be using for this is a object detection model in particular the data set we'll be using is the Vox 2012 data set if you're familiar and so object detection is basically trying to draw a bounding boxes around objects in an image this is very much a deep learning problem at least right now all of the best methods are deep learning and so we're really going to run into a lot of the difficulties that Neil described when he was talking about teams doing deep learning for this demo I've landed all of these images into a delta table basically we had a Miss raw images in s3 and I use spark to land them into a Delta table where it's images file names and annotations which is basically all of the bounding boxes and some other information about that image and first before we dive into determined I want to talk about what this would look like without determined if you just wanted to build an object detection model with sort of the standard tools out there and so this is my version of what that looks like and it's worth diving into because it's still pretty cool this is a deep learning model I'm training it using PI torch I built a faster our CNN model with a res met backbone in PI torch training it in this notebook the models here did some cool stuff to pre-process the data load the data right rode a fancy data connector that can connect to a delta table load that data creates a PI torch data set that we can use to do all of our pre-processing and that kind of thing training the model here saving checkpoints doing some you know cursory amount of vlogging to keep track of things like I mean average precision and to see our progressing through training and what I want to really get at is this works fine especially for one person I'm running on a fancy TV 100 GPU so this is running as fast as you can on one GPU saving the checkpoint if I train this model I can you know copy that file some way or paste it somewhere else use it for spark inference say something like that and it's all possible but the core of what determined does is makes it easy to take that and scale it to do larger scale experiments where you can iterate more quickly and also work way more effectively with a team on this kind of thing and so for the sake of this demo the situation I want you to imagine is that my co-worker actually already built an object detection model on this data set they they built the model they trained it they used it even to do some sort of inference in spark so that we could score some data for whatever case our company has and that got done and then a month or two or five went by and my manager came to tell me that I needed to take that model and try to find some way to make improvements they said if we can improve it by just a few percent we'll get you know it'll be worth a lot of money to the company so what can you do and so that's the story we're gonna tell it and we're gonna start by looking at how my coworker trained to the model in the first place and you know the first thing I want to imagine you'd imagine is if they train the model this way without determined in the notebook you know the extent of their logging is saved in this notebook the check point hopefully got saved somewhere but maybe I don't know where I don't really know what hyper parameters they used hopefully or these ones but if they were trying out a bunch of stuff I don't have any way to track that or associate that with the model they did you know to understand their code or to run their code I need to figure out what it's doing I need to figure out what script they ran I need to figure out what environment they constructed to run it it's really really a nightmare and this comes up a lot at a lot of companies to try to run this kind of code without the right tools now luckily at my company they my coworker that our tutoring this model use determine tutoring the model so what that means is they wrote all that same model code but instead of you know coding up some experiment they basically told described to determine an experiment to run with this config file that I'm showing you right here so it's worth diving into this config file to see what they were doing the first thing we'll see is they were loading data and this is cool they were using a connector to Delta Lake to load a version of data for both training and validation so they loaded their version zero of the data they're doing a really cool hyper parameter search the one that you'll describe to you in our overview that's searching over simple in this case but learning rate and momentum two really popular hyper parameters and so they're going to be doing a search over those to find a good configuration they're doing this with the adaptive search algorithm that sermon has this is based on the popular hyper band algorithm it's actually one of an algorithm that one of our co-founders invented at determined and this hyper band algorithm is a state-of-the-art hyper parameter tuning algorithm it uses early stopping principles early stopping which means it tries a whole bunch of hyper parameter configurations all at the same time but the ones that aren't performing well it kills off very early so that the more promising hyper parameter configurations have more time to Train we'll dive into this experiment to see how that actually looks but leave it to be said now that it's a really sophisticated type of parameter search so just because our co-worker told us like sent us this file they wouldn't even need to but just from this file we can see exactly what they did they trained the model on some version of the data they did a sweet high parameter search and let's flick over to determined to figure out exactly what that ended up looking like so this is that experiment in determined and what you'll see is the first thing you'll notice is the performance of the experiment this is the evaluation metric they tracked tracked over time throughout that experiment so mean average precision is this metric it's very commonly used to evaluate the accuracy of object detection models and you'll notice they got ma P of about 0.5 to on which to just take an aside I want to point out for science I actually ran this with the recommended type of parameters for faster are CNN the recommended values of learning rate and momentum that the like authors of the paper suggested and I actually only got point four five mean average precision and so just right out of the bat by using determined our co-worker was able to build a model that was way better than they would have gotten if they just sort of used the stock option so hyper parameter search already got us a long way to a really high performing model when my coworker trained this model say a month ago now coming back to myself and now this is a really helpful view for me to understand exactly what happened first off I can look and see exactly what configuration they used you'll notice this one-to-one mirror is the one that we were looking at in this notebook so no questions about how they train this model we know exactly the configuration they use here associated with this experiment page further we can actually download the code they use to train that model so I can get an exact copy when they ran this model of the code that was used and so there's no question marks of what version what code was used what configuration was used I should be able to take those things and recreate this experiment by the way if that's what I wanted to do so I can also click into any one of these so these are all the configurations that that hyper parameter search tried on you'll notice that many of them only ran for like three training steps the ones that we're doing very well but the more successful ones ran for the full duration of training if I click in I can see exactly what hyper parameters were used like the learning rate and momentum that resulted in this much better configuration I can click into these check points and see exactly where those checkpoints are being stored in case I need to grab that to go retrain a model or fine-tune a model or do inference on there's no questions about where on this file system I saved this check point if I go back here so now now is where I get to start sort of critically doing my job I was asked to say how can I improve this model so my first instinct is to hope that they provided me some some good logs that help me really get some more insight into how this model is performing and so what I'm going to do is open up a tensor board we in determine directly support tensor boards so if you do in the extra logs or any add any logs in your model we make it so it's easy to open up a tensor board and check this out and lucky me it looks like my coworker happens to have made their be per class validation metrics so I can actually look at how accurate the model is on each of the classes and hopefully use that to try to diagnose some of the issues with the model and flicking through this the one that really sticks out to me is this dog class you'll notice that the accuracy on the dog class is ranging somewhere but depending on the model between like point three five and point four five mean average precision and that's not great especially for a class like dog where I'd expect there to be you know their millions of pictures of dogs on the Internet I'd expect there to be lots of labeled examples of dogs I'd expect that we can build a model that did better than this level of accuracy on dog and so here's my aha something seems wrong with our ability to detect dogs let's try to figure out what's going on so flipping back to this notebook I can go in and visualize this data set and try to get a good sense of why this is happening and when you look at the number of examples of each class we had and that V zero of the data set you'll notice we only had like 50 images of dogs and that's compared to thousands of images of people and you know hundreds of images of cats there's our probable reason for why we weren't doing very well in the dog class so most of the time the best answer to not having enough data is to go get more data so I'll turn around go to my data team ask for more labeled images of dogs hopefully I have a good team that will help me out here and then say they come back and provide me with this new data set new version of the data set and look at that we've got 500 dogs now this is where the Delta Connection really becomes useful we're able to you know we got those new images I could run all of that same ETL code to land them into that Delta table but now I have a very clear second version of that data set and hopefully I can really easily use that and determine to kick off a new experiment but with all this sweet new data that I've got so that's what we're gonna do you know as a data scientist you know I'm trying to improve this model all like I want the minimum amount of work to plug in this new data train it really really quickly so I can iterate quickly and hopefully see improvement and you'll see very quickly how determined it makes that very easy first off to change the data version all we're gonna do is change the version in this config file you'll see it was version 0 before up here and we're just gonna bump it up to version 1 and just like that will be connected to a new data set that we can use to train with no questions about what data we're using it's very obvious we're using the new one we're going to you know I think this the other really important thing that the term is gonna let us do in terms of the other really cool thing that the term is going to do for us is let us do distributed training really easy what I mean by that is um you know if we're doing this up here method of running in a notebook it's really pretty slow it's gonna take hours days maybe depending on the problem and that's not like we want to iterate quickly turn this out so we can go work on the next model and not have you know be done with our co-workers model that we didn't really want to work on in the first place and so distribute training is one of the fastest ways to do that to accelerate your training and in determined all we need to do is specify that we want 12 slots per trial and what that means is we want to distribute our experiment across 12 GPUs we didn't have to go in and read out fit our co-workers code with distribute training code we didn't need to figure out how to do distributed training on some arbitrary cluster we didn't need to find three machines with 12 GPUs we simply told determined we want an experiment with 12 GPUs and the term it's going to handle all that current work of kicking off a job like distributed across 12 GPUs so to summarize we're going to use new version of the data we're going to fine-tune based on the output of the experiment that our co-worker did that high programmer tuning experiment and we're going to do it using 12 GPUs to do it really quickly so if we quick kick off that job sending it to determined and we flip back to determined if we switch the dashboard what we'll notice is that job got kicked off and we're off to the races well this initializes and starts training to speed things up for the sake of the demo I ran this experiment ahead of time just so we could check out what it looks like and so if we go here we can see that we train this model for what amounts to something like 5 a pox over the new data and first things first we immediately got like a 1 percent lift in mean average precision so 30 minutes of training later we're already seeing a percentage point accuracy gain which as her boss told us that could mean you know huge change it or like be very valuable for the company in the configuration we can very clearly see that this model was trained with the new version of the data so there's no question marks if my boss comes back and say says you know what happened how did its model get better it's easy to go in and say well we trained it on this new version of the data more importantly than that one percent bump in mean average precision if we go look at the dog class we can see that our accuracy on the dog class has actually jumped to something like 0.59 you remember before was something like point four or five so we got a huge huge bump like a 50% jump in performance on the dog class which is exactly what we would hope for collecting all that new data so now we did it like we accomplished our task we did it in like 30 minutes by simply changing this config file specifying a big distributed training job with new data and now we say we want to use it to actually do some kind of scoring do some kind of inference the thing that's actually going to UM generate business value for the company well before say our co-worker wrote this code to launch inference using spark and the cool thing about determined is it makes it really easy to just specify some experiment ID and get a check point out of that experiment instantiate a model and use it wherever you need to use it so they wrote this inference code that just takes a determined experiment as an argument takes the location of some new data that we want to test on and then launches a big spark inference job to quickly do inference and because Dermott determined tracks all of this important information what the artifacts the check points all of that by simply bumping this from 1 to 26 or in this case we launch this new experiment 31 if we simply bump this to experiment 31 we can launch spark inference using that new check point the one training on the new data seamlessly and quickly you spark to do inference over a huge data set this is so useful because not just you can change one number in a jupiter notebook but you can integrate it into a workflow tool like air flow or some sort of CI CD solution to make sure that you're delivering the latest and best versions of your models to do inference whatever you're doing it so this is you know to summarize this demo showing how you can go from version data and Delta Lake scale your experiments collaborate on on models and train new better models more quickly than would ever be possible without determined and then use determines ability to keep track of the artifacts of those experiments to do inference quickly using whatever your infrastructure is such a spark I want to remind you that all this code is available on github including the actual inference and ETL code but it does use spark so if you have any curiosity about how we use spark to load a checkpoint from determined and actually do inference I highly recommend going to check that out and otherwise I appreciate you checking out the demo and more important than anything I hope you're saying safe and sane and I hope determined can make your life a little bit easier if you're doing deep learning great thank you David so that concludes most of want to tell you in this talk last thing we wanted to share with you is that determining is open source so we've spent the last few years working with the cutting edge deep learning teams and really evolving the product based on really great feedback we've gotten from teams doing deep learning at scale a variety of industries in the last month or so we are really excited to have released the product as open source on an apache license and we would love to have people try it out give us feedback and you know explore whether it's a good fit for your team's and and your deep learning use cases so you can find us on github all the recommendations online we have a you know community snack channel that we would love to you would love to see folks join and interact with us so thank you very much for your attention and would love to hear questions and and any feedback you might have you 