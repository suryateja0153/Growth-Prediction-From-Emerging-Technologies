 Good morning everyone! Recently I made a video talking about StyleGAN 2. It is the latest state-of-the-art GAN for unconditional image generation. It produces very good looking faces and has improved many aspects compared to the original StyleGAN. This morning I'd like to start a three-part series explaining how StyleGAN 2 works under the hood and showing you how I implemented it in Tensorflow 2.0. This video assumes you have at least a basic knowledge of GANs and convolution. If you're not familiar, that's okay check out some of my other tutorials then come back to this one. Jumping right in the first improvement StyleGAN 2 made was in revisiting adaptive instance normalization. They replaced this with something called convolution modulation and demodulation which I will get to in a moment. Adaptive instance normalization or AdaIN if you don't have much time is a method of applying a style to the feature map of an image AdaIN takes a feature map, normalizes it, then rescales the feature map based on a style vector with scaling values for each feature map. In this paper they define this as first demodulating a feature map then modulating the feature map with a given style. The original StyleGAN would demodulate then modulate feature maps immediately. The authors revisited this they define a style block as any operations done between modulation and demodulation. This is where feature maps are changed using convolution and the modulation of them apply certain features more than others in the original StyleGAN all operations were done inside the style block. The authors thought that it wasn't necessarily a good thing to be adding inconsequential noise with a set scale to modulated feature maps which had a variable scale, thus they revised the architecture to move this inconsequential noise to be outside of the style block as you can see here. Now each style block modulates the input feature map uses convolution then demodulates the output feature map. What the authors then changed was genius in my opinion. They realized they could get the exact same effect by modulating and demodulating the convolution kernel weights without doing all the calculations of modulating and demodulating the input and output feature maps, respectively. Here I'll go into more detail about convolution modulation and demodulation. Here is an example of what a set of image feature maps might look like with three channels red, green and blue. Here's an example of what a convolution layer might look like going from one set of feature maps to the next. Each line represents some convolutional kernel. This kernel goes over the entire feature map but for simplicity they are simply represented here as lines from one feature to the next. To get the same effect as scaling the input feature maps we can scale each kernel based on their input channel. All kernels with the same input would have the same scale. This scaling factor is determined by the style vector, thus being affected by the latent input modulating differently for each image. Now to complete the style block the outputs must be demodulated, which means normalizing them to have a standard deviation of 1. Assuming the inputs have a standard deviation of 1 since they were also normalized at one point we can normalize the outputs by dividing the modulated weights by their l2 norm. Thus convolution demodulation divides each modulated kernel by the l2 norm of all the weights leading to each output. Now the entire style block has been baked into the convolution layer greatly speeding up processing; but other than speeding things up, did it help with anything? Actually, yes. The original style gain had these little droplet artifacts that you can see here. In each image and as part of the feature Maps AdaIN would need little pieces where the feature maps had very extreme values caused by the way and normalized the feature maps. With modulated convolution these droplet artifacts go away. This is a video from the author's showing this improvement Let's jump into the code for this modulated convolution. If you want to see the full code the github link is in the description. I implemented a custom Keras layer for modulated convolution. There's a pretty hefty amount of code but a lot of it is just configuring the layer itself, copied from the original convolution implementation. The only important part of this code is the call function. The first thing I did was change my configuration to channels first instead of channels last. This just made reshaping easier later in the function and I change back to channels last at the end of the function. Now we have two inputs; one with the feature maps and one with the weight scaling values. We want to reshape the weight scales so that they're compatible with our kernel. Then we want to add a minibatch layer to our kernel as the weights will be different for each instance in the batch. Now we move on to modulating and demodulating just like what we looked at. We multiply the kernel with the input weights. Then we get the l2 norm of the weights based on their output weights and divide by that value. What we do next is a little complicated but please bear with me. Since convolutional kernels can't have a batch size in Tensorflow 2.0 we need to fuse all instances of the kernel weights in a batch to all be in a single layer weight instance we also need to fuse all inputs so that they're all feature maps of a single instance to make up for the fusing of the kernels. This was a headache to do and would be a headache to explain so I'll just leave it at that. The important thing is is that you understand the concept of modulation and demodulation. So now, once that's done we can finally apply the convolution then we unfuse the output putting it back to being a batch and go back to the channels last configuration. And that's it we have our modulated convolution layer which we can now use in our model here we see the new code for a generator block. First we get the values for the style vector from the mapping network and scale the inconsequential noise. We then use our modulated convolution with the image features and style vector at our consequential noise and then apply a non-linearity function. So that's it for convolution modulation, if you enjoyed the video please leave a like and don't forget to subscribe to catch parts 2 & 3 which will go over the other improvements StyleGAN 2 made. Thanks for watching! 