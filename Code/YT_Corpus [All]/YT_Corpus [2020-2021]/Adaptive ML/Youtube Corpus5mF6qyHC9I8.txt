 [Music] you [Music] you hello students will to this review lecture to in this lecture I will briefly review the signal estimation by optimal linear filters we discussed optimal linear filters then adaptive implementation adaptive filters and Kalman filters will briefly discuss those Wiener filter agent a linear filter structure for the estimator and estimate the filter coefficients by applying the minimum mean square error principle so Wiener filter already is a linear filter structure this is xn is the signal BN is some additive white noise yn is the observed signal it is passed through this linear filter we get the estimated signal this estimated signal should be optimum under the minimum mean square error criterion with this linear filter structure given random observations Y n minus M plus 1 yn minus M plus 2 up to yn and then 2i n plus n so we have a block of signal from time instant n minus M plus 1 2 and plus and capital n so for this duration signal is available our minimization problem will be now minimize Y of xn minus this filter output that is summation a tie into yn minus I Y an input signal I going from minus n to M minus 1 whole Square this we have to minimize over HJ j going from minus n to M minus 1 and we derived a winner half equation corresponding to this if we differentiate with respect to e sub H I it's up AJ we get do in our how because given by Rho FD is equal to summation it's I into R Y Z minus I I going from minus n to M minus 1 we discussed the Empire winner filter a PI R PI R wiener filter here we have to minimize filter is accept n is equal to summation it's I into y n minus I I going from 0 to M minus 1 this is the M type a fire filter we considered and the filter parameters were obtained by applying the orthogonality principle what was that ye of signal minus the filtered output that is H I into y n minus I I going from 0 to M minus 1 so this is the error this error is orthogonal to data the Y of and minus Z that is equal to 0 so from this this is the orthogonality condition from this we will get that our X Y of Z is equal to summation H I into R Y of J minus I I doing from 0 to M minus 1 so our X sub J is equal to summation H I into our Y of J minus I I going from 0 to M minus 1 that is true for J is equal to 0 1 up to M minus 1 because these are our data point so that way this winner hop equations we derived for a fire filter and similarly minimum mean squared error also we can find out MMSE minimum minimum mean square you know that is given by your b square n and this is same as Y of X n minus summation it's I Y n minus I I going from 0 to n minus 1 this is the error into X of n so because remaining data are orthogonal to this error so that way we got it as our X of 0 minus summation each side into our Y up hi I'm doing from 0 to M minus 1 so this is the minimum mean square error we also considered it non-causal I our Wiener filter in that case impulse response sequence is infinite therefore our except n is equal to summation H I into y and minus I I go in from minus infinity to plus infinity so in this case the mean square error will be e of xn minus summation Delta into y and minus I I going from minus infinity to plus infinity whole Square and this mean square error we have to minimize with respect to each H again we apply the orthogonality principle what is orthogonal T principle error is orthogonal to data yup xn minus summation H I into y n minus I I going from minus infinity to plus infinity this is the error is orthogonal to Delta Y of n minus Z they going from minus infinity to plus infinity so this must be equal to 0 so this is the orthogonality condition and this orthogonality condition will give us that is our XY of the j yi of x and into yn- day that will be our x-ray object that will be equal to this summation ei into y of Y and minus I into y and minus J that will be our Y of J minus I so that we summation h AI into our Y of J minus I I going from minus infinity to infinity that will be equal to R XY of G this is the Wiener Hoff equation which is obtained by applying the orthogonal take principle and here we have two sequences which is known to be a two-sided sequence because impulse response is from minus infinity to plus infinity and autocorrelation function is also a two-sided sequence therefore we can perform convolution and then solve this problem in that from domain so non Cadell IR Wiener filter is therefore given by H a controlled with ry j is equal to r x YJ they are going from minus infinity to plus infinity then applying that transform we'll get this is that is sequence transfer function is a set here as i said is equal to s XY dead so from this we get that it transfer function of d non-code l ir Wiener filter is given by it said is equal to s XY is that / sy that cross power spectral density divided by the power spectral density of y and once we have a set we can apply inverse transform to point hm now how to find out the MSE again same orthogonal T principle we can apply that is e of MSE is equal to Europe en is xn minus summation H I into y n my i arguing from minus infinity through infinity into xn because remaining part of the error will be orthogonal to this part so because the remaining that apart will be their death part will be orthogonal to this error therefore only this expression will come and that is equal to our X sub 0 first term and remaining term will be summation H I into R XY I are you in from minus infinity to infinity that is our X of 0 minus summation H AI R X Y cross-correlation I are going from minus infinity to plus infinity then we discussed cos L I are in our filter in this case the estimator except n is given by except n is equal to summation H I into y n minus I I going from 0 to infinity because the filter is causal this filter sequence is defined from I is equal to 0 to infinity so that way it is causal and the mean square error is given by here we square n that is U of xn minus X hat n whole square and this is to be minimized with respect to all it is so we now hop equation again here is given by Europe error is orthogonal to data that is xn minus summation H I yn minus I I going from 0 to infinity into error is orthogonal to data Y up and minus J Z is equal to 0 1 up to infinity so that way we will get first on will be our XY j r XY Z and then remaining term will be summation H I I going from 0 to infinity into R Y Z minus I so if I take the difference between this and this will get the minus I so that was samus on HR into r YJ minus I I going from 0 to infinity is equal to R XY Z so this is the winner Hoff equation corresponding to cos L I are we not filtered so now you know that here is each day J is equal to 0 to infinity is a right-sided or causal sequence what are Y here today to say that sequence arise it two sided sequence sequence therefore direct convolution is not possible so therefore which cannot be solved directly in the transform domain and now we will apply the spectral factorization theorem to the power spectral density of Y that is as I said is equal to sigma v square into H that into HT z inverse so that way we will use this relationship to get it whitening filter so that we apply the whitening filter H on death is equal to 1 by AC death to generate the innovation process so we apply suppose this is my yn pass it through one by is see that that is the innovation filter or whitening filter to get the innovation sequence and now using this innovation sequence xn now can be estimated using BN and this was shown this and the corresponding filter is given by is 2 that is equal to 1 by Sigma half v square into the causal part of s XY Z divided by HC jayden version this is the Wiener filter to estimate xn from the white noise sequence and finally we got the gazelle I have Wiener filter as the Cascade of this filter one by AC jet and then into that is to Z so BN is passed through is 2 and this is P n BN is passed through this is to that that is the filter we have already obtained then we will get this estimator exit and that way it is cascade of two filters one is one by AC jet and of course this sigma v square is also there on y sigma v square into so so spectral factorization gives the whitening filter its own jet is equal to 1 by AC jet and this energy innovation sequence be ends of variance sigma v square and this innovation sequence is used or used to estimate accept n from bien so that way the transfer function of this filter is to death which is optimum is given by this expression 1 by sigma v square into the causal part of s XY that divided by AC dead inverse so this is the winner estimator of xn from the BN sequence now once we have this is to that part we can cascade it it one way I see that to get the the combined Wiener filter therefore T with a filter will be Cascade of this and this and it is given by 1 by sigma v square a jet into sxy jet divided by 1/3 inverse and then we have to take the causal part we considered one application that is linear prediction of signals the linear prediction problem was formulated as follows y hat n is equal to summation HR into yn minus I I going from 1 to M so this is the linear prediction or it is forward prediction problem again we apply the orthogonality condition that is y up here actual data is yn and we are predicting Y hat n that is orthogonal this is the error is orthogonal to date add the tie is y of n minus days because it is M they are going from 1 to M because we are using only past data up to n minus M so that way from n minus 1 to N minus M therefore this is the token LD relation we get here yn minus summation Y hat n into y and minus j is equal to 0 you know it is orthogonal to data and from that we directly get D we are happy to a son or normal equation and this is given by r YJ is equal to summation ei into our Y J minus I R going from 1 to M and this is there are M equations corresponding to M values of Z and in matrix notation this equation we write like this this is the autocorrelation matrix into coefficient vector is equal to the autocorrelation vector so this autocorrelation matrix we see that it has very attractive property it is symmetric and it is topless for example this sub diagonal will be same this time it will be same like that oh this is a symmetric topless matrix into a specter is equal to our Y vector and similarly the mean square prediction error also can be obtained by applying the orthogonality principle that is mm espy minimum mean square prediction error is given right yup that is error is yn minus summation H I into y and minus I I going from 1 to M into yn and the remaining part of error that that will include yn minus 1 yn minus 2 etcetera they are orthogonal to the seller therefore their contribution will become 0 and we get this on the minimum mean square prediction error is equal to e of Uyen minus summation si into yn minus I I going from 1 to M into y n so remaining term will contribute 0 so that way it will be equal to ry 0 minus summation each side into ry of I I going from 1 to M minus 1 so this is the expression for minimum mean square prediction error similarly we also discussed the backward prediction problem here given Y n yn minus 1 up to Y n minus m plus 1 we predict Y hat n minus M so Y hat n minus M is summation BMI this is the backward prediction coefficient into Y of n plus 1 minus I I going from 1 to M this is the backward prediction problem and here also we apply the orthogonality again error is orthogonal to data and that we applied there are M data y n yn minus 1 up to Y n minus 1 plus Capital m so that way M that are there and corresponding to that we will have this we now have equations and from this winner hop equation and linear prediction winner hop equation or forward prediction winner habituation we establish an important result that is backward prediction coefficient at instant I is same as forward prediction coefficient HM M plus 1 minus I so this is the relationship and also we established at backward and forward minimum in mean square prediction errors are equal so we use this result and also this symmetric table structure of this matrix to derive D famous Levin Sundarban algorithm that we have discussed in detail we also discussed adaptive filter the basic setup for the adaptive filter is like this this is yn is the input and this this filter structure generally didn't a fire filter structure and this is the absorption L and we get some output here that we call it at the and and there isn't reference signal in the case of adaptive filter this is DN the we want to match the output of this filter with this DN so that way the energy desert output which we know somehow and we want to match the output of the adaptive filter with this DN and whenever there is no matching that error will be there that error is paid back to update the adaptive algorithm which will compute the filter coefficients adaptively so this adaptive filter coefficients will be computed by this adaptive algorithm based on this error en and the input signal yn and then this updated filter will be used again to filter this Hawaiian and to be the estimate of the desired signal the adaptation of the filter coefficients is based on the error en between the filter output and a reference signal DN usually call the desired signal so the in DN is tricky it depends on this specific application for example louis's so how we can choose DN in the case of adaptive channel equalization and adaptive system identification we considered one basic adaptive filter what is known as lms adaptive filter and it's objecting rule was this a sub n plus 1 is equal to HN plus mu into n into yn we get this by applying this steepest descent algorithm to minimize this cost function is square and so minimize is square n with respect to it's AI n n is equal to 0 1 up to M minus 1 so this optimization problem when we apply this steepest descent algorithm we'll get this updating rule so this is the LMS updating rule so yn is the input and this is the afire filter and then D hat n is the output and this is compared to digital signal output this is compared with the desired signal and the odorous feedback to the LMS algorithm so LMS algorithm update the filter copy sent by this rule it computes the you know error is nothing but yn will be filtered at instant n with filter parameter up so I will get D hat n that is summation H I H I and into yn minus I I going from 0 to n minus 1 so we filter the signal oin with the filter coefficient at instant N and then after that we'll get the error error is equal to en is equal to DN minus D hat n so this error is used to update the filter coefficient H of n plus 1 that is obtained as that previous filter HN plus mu into en into yn and we discussed several modification of this algorithm that is an LMS algorithm leaky el lms algorithm that is leaky LMS algorithm then for a efficient implementation will consider block LMS al him and sign a lower LMS algorithm the LMS is a very simple adaptive filter but problem is how to suit this step length parameter mu so this depends on the eigen value of the autocorrelation matrix suppose we are filtering it W as a signal Y n then the eigenvalues of the corresponding autocorrelation function of yn will determine this filter step length parameter so that way we establish some relationship that is mu should be lying between 0 and 2 by lambda makes this type of relationship we establish and then also with this chosen mu the LMS adaptive filter will not convert to excel Wiener filter there will be always some excess error that accessory will also depend on the parameter mu and we also observed that the rate of convergence depends on on D eigen value eigen value spread if it is high convergence rate will be low and that was a serious problem in the case of lms lms filter and to overcome this we consider the RLS adaptive filter so in the case of RLS adaptive filter the error is sum square error we discussed the least square estimation principle same principle is applied here also and here the error to be minimized is sum square error weighted by Hector lambda R if R and minus K so that way the error to be minimizes epsilon n is equal to summation lambda to the power n minus K into square K K going from 0 to n so from the beginning and is equal to 0 to current n all the errors are considered and this is given by summation lambda to the power n minus K into DK minus y transpose K into h n whole square K going from 0 to M and this is important that we are filtering the all the passed signal values which respect to H n current estimate of the filter and that way we are determining the error and this error are weighted by the what is known as the forgetting factor lambda to the power n minus K and this lambda lies between 0 and 1 in the case of W as a signal we can take lambda is equal to 1 and this is used to take care of non stationarity of the data so for example since lambda is PacSun as n tends to infinity so lambda to the power n minus K for k is equal to 0 it will become lambda to the power n it will be close to 0 when n is large so therefore for large value of n it will give less and less weight to the previous errors that is the idea behind tackling the non stationarity of the data past errors are given less with present errors are given more weight and that way we derive the RLS adaptive filter we got a corresponding normal equation by filtering that is del epsilon n into del hñ that is equal to zero so from this we got a matrix normal form of normal equation matrix form of normal equation and which is sold by shown by the matrix inversion lemma we go get a recursive estimate or using the matrix inversion lemma so that we we discussed the RLS algorithm and this RLS algorithm estimates the signal estimate the signal recursively so it is a recursive least square method we discussed two form of Kalman filter that is scalar Kalman filter and Vector Kalman filter this scalar Kalman filter this use SD applies the model what is the model model is a r1 signal so xn is equal to a times X of n minus 1 plus W n so W n is a white noise and a the constant which is known and it can be considered as a function of n so that non stationarity can be tackled and observed data YN is equal to xn plus VN and we discussed that we derived it derived using the innovation representation that is get D innovation what was the innovation in Kalman filter will got it s and that is why and prediction error minus summation its I into y and minus I I going from 0 to n minus 1 so this is the prediction error and this prediction error we saw that this prediction error this prediction error is an orthogonal sequence that way we generated the innovation sequence and we also considered the vector Kalman filter it is a recursive state estimator and uses a state space model for D signal that is xn is equal to a n into xn minus 1 plus W n if we consider int which I am reading then this state space model is xn that is the signal vector is equal to a n into X of n minus 1 plus W and W any noise known as process noise and the observed data yn is equal to CN a matrix CN is a matrix multiplied by xn xn is the state vector plus VN VN is again another noise vector which is observation noise vector which is uncorrelated with WN and uncorrelated with xn so that where WN is assumed to be white noise vector does the covariance matrix QW is a diagonal matrix V and is also assumed to be a white noise vector with covariance matrix Q P further VN is independent of X and WN the Kalman filter structure is given by this block diagram so yn this is the input signal and this is the output signal and normally we denote it by accept and given and estimation of the signal at instant and and this is delayed by one unit here we will get except n minus 1 given n minus 1 and then if we multiplied by n matrix we will get the predicted signal so this is the except and given n minus 1 this is the predicted signal now to get the estimate we use this predicted signal in the observation equation there is a multiplication of CN and this state vector therefore this predicted state vector is multiplied by CN and we will get this predicted input vector this is the predicted input input so why aren't we are predicting here this prediction will result error that is the prediction error this is the innovation so this is the innovation output and this innovation output is scaled by D Kalman gain and then this corrected value this part is the correction term and this correction term will be added to the predicted value so this except and given n minus 1 plus this below that is K and n times y tilde n that will be added and we will get accepted given n so that way we see that here there is a prediction part and there is a correction part that collection part is obtained by passing the predicted signal through that output matrix multiplier and then we get the predicted input that predicted input is subtracted from the input and we will get the innovation and this innovation is used to forget that collection term this innovation is used to get deep correction term that we Kalman filter is a prediction character filter I will conclude my lecture here and this is the end of the course I hope that this course gave you a background on logistical signal processing there are advanced techniques of statistical signal processing which was not possible to cover in this course thank you [Music] [Applause] [Music] you [Music] you 