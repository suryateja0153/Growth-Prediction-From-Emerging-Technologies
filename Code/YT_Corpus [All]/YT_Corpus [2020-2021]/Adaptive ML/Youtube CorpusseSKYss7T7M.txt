 welcome to our session in this session we will present our work in building our thermal solutions in a distributed fashion using ray and the Railsback it is provided as part of the analytic student project and has been used by many of our users for the tongue serious and as applications such as data center management manufacturing pipeline maintenance stock price monitoring telecom network monitoring and soil this is agenda for our talk we were first to give some background of the individual time series analysis Allah Allah and so on and after that we were tapped into the details of the scaleable optimal solution we have built as well as the real-world use cases from our users first some background at Intel we have been focusing building very stuck technologies to bring AI to Big Data we have open such a big deal a distributed a different framework for Apache spark so with big deal our users can write new diplomatic asians just as standard the spark programs running on existing building clusters on top of those different frameworks like big deal pencil flow pipe arch and so on we also developed a open source project or culture analytic zoom which is supposed to be a higher level data ethics and a platform which is integrated tensorflow Python Charis and whereas Pat Frank ran in this area so that we can provide a end-to-end a unified the pipeline to our user so this page gives a very high-level overview of the analytic juice tank as mentioned before it is good on top of the low-level deplane frameworks like tensorflow pi touch and also open be know as well as distributivity and it takes frameworks like a sprint and the rain it can run on your laptop and a zoom without any coach change it can automatically scale to a larger cluster to process your production region so inside anything you there is three layers at the bottom layer and they are ready what we call a integrative and X and they are paper if he is a horizontal layer which allows you to apply those deep learning and the AI models choose the big data in a distributed fashion for instance a lot of our uses wronging distributed tensorflow acts back in an exhume so that they can apply a standard intensive role models to the bigger data clustering a destroyer venturing and everything is running inside the spot across a trusted processing in memory on top of this paper layer we also provided a automated and a water layer so that we can help you sir ultimate a lot of the manual tasks in building this entry and gem machining applications all time areas were such an example and we will talk more in the following slice on top of this workflow layer will also provide the essential beauty in models and M reasons for say recommendations time series analysis and as well so our users and Directory users models and a grizzly invented ipython and because the user can also use any standard the tensorflow a Python model and in an ending you and yours anything doing through transparent rescale source a deep learning and am are those - they're big data trust so next my co-speaker changing we're talking more details on the time series analysis is the ultimate solution built at the better degree of what use cases so now now I will talk about time series so what is time series basically a time series is a series of data that is observed sequentially in time examples of time series include stock prices sales volume secure aisle monitor metrics and KPIs in telecom networks there are several types of time series analysis time series forecasting is to use a history to predict the future and anomaly detection can detect data points which have abnormal temporal patterns and there are also times series classification in the clustering and so on one thing to note that it is very common to use forecasting as a priest step for online alumni detection and in that case we first to forecast the future value and when the actual value comes we compare the difference between the actual value and the predicted value if the two value differs too much the new value would be considered anomaly and there are also many applications for time series analysis the mountain forecasting can be used for inventory management and resource scheduling Telecom KP analysis can be used for network quality management in telecom operators since the readings are also time series and their analysis can be used in critical maintenance of high value equipment and analysis on service monitoring metrics can be used for intelligence IT operations for clouds well time series forecasting is one type of analysis that we've been dealing with for quite awhile the problem can be defined as given all history t observations y12 YT we need to predict the values of the next H steps that is YT plus 1 2 YT + H although in theory it is possible to use all the history T observations normally we only consider the previous K steps which we often call the look-back period as illustrated in the proper area in the right finger there are various approaches to solve this forecasting problem traditional methods such as both aggression exponential smoothing and ARIMA are widely used in real-world applications they're relatively simple but are not very good at capturing in complex temporal dynamics and relationships across a time series recently machine learning and deep learning methods are becoming more and more popular unlike traditional methods which are often informed by domain expertise new machine learning and deep learning methods are often data-driven and some deep learning methods which has been used for sequence modeling have been applied to time series such as our cm and GRU temporal revolutions and attention and there is also evidence that they can outperform traditional methods in several well-known competitions and real world applications although machine learning can can do well in forecasting trainee Amish good machine learning model is not an easy job especially for those who are not so experienced in this field and that's why old Amell is gaining popularity these days the purpose of what ml is to make it easier and more efficient to build machine learning and deep learning applications by automating various stages of the machine learning pipeline as shown in this figure stages such as h-her generation model selection optimization and evaluation can all be automated by automail essentially the task of an amount is to optimize a target metric within a given budget modern approaches perceive this as another optimization problem and uses far more complex methods to solve it and single grid and random search auto mode can be used before to search for hyper parameters as well as model structures in deep learning we have chosen ratio at the back end to increment our autumn of support in an ELISA project retail is a library on ray for scalable experiment execution and high parameter tuning array is a distributed framework for emergent AI applications open source by Rice lab real spark is a feature in Lau zu which designed for users to directly run rate programs on a spot cluster so with real spark we are able to run our own ml enabled time sears lands on star cluster without the need to care how to manage the two frameworks actually there's another session dedicated to real spark in this summit if you're interested you can check it out okay so far we have finished all the background part now let's come to the core part okay so this is a high-level overview of the time series support in the analytic diesel project and the time series solution part is highlighted in yellow at the bottom part there is a general automotive arch which provides abstractions of machine learning pipeline stages and automates processes such as future generation model selection and hyper Pemba tuning on top of that we provide algorithms and models specifically for time series including forecasting and anomaly detection and then our users can build their own applications on top of these supports for their different usage scenarios there are three major features in our solution as shown on the right side first is that we provide rich algorithms which includes state-of-art neural network models and hybrid models second we provide auto ml support which automates the training process and save the tuning efforts last we utilize was provided in analysis team rescale the automated training in the Big Data cluster okay now let's dive in a little bit into the software stack in the autumn Oh framework we can see that there are four major components include feature transformer and model our abstractions of the machine learning stages they can be considered as black boxes which takes in a hyper parameter configuration to construct the internal pipeline and control the execution process specifically feature transformers can do future generation risk aiding a selection and so on models can do a single-mother training as well as multiple model selection a search engine is responsible for - parameter combinations spawning the trials and the scheduling and keeping track of the trials in a cluster we have implemented rating search engine to enable search capabilities on top of ratio when the search finish the results of all trials are retrieved and the pipeline is constructed from the best Ahava parameters and the trainer modem the pipeline can be saved to fire and restored later for inference and incremental training now on top of this general ordinal framework we have implemented some feature transformer and models specifically for time series and class code time sequence predictor to enable the automated training flow and a class name the time sequence pipeline for the pipeline output some of the cross names may not seem so proper today due to some lags reasons and we're going to change it in the future releases and you may notice there are two blocks in gray in the Altima layer they are implant but not implemented yet this is earnest illustration of the trainee workflow around time so basically we use a search engine to drive the entire training process at the time of construction the search engine takes in different smaller and their model and it also takes in a search preset to configure how to search the hacker parameters at runtime the search engine generates hyperthermia configurations based on the preset span the trials and scheduled the trials on the rate cluster each trial runs a different combination of the hybrid parameters and when a trial ends it will writes his results to the HDFS and finally after all the trials finished the results are retrieved from HDFS and analyzed and then the pipeline is constructed from the best hyperchromic configurations from the results and the trainer model is rejected in the red arrow here here is a glimpse of the api's to train a pipeline you first create a time sequence predictor and then just call fit on it to launch the automated training process forfeit there is argument called recipe which you can use to configure how to do the habit parameter search actually we have implemented several handy recipes for your convenience but you can always write your own and there is also flag called distributed which indicate whether this is running on a single node or on a cluster the result of the fit is time sequence pipeline which you can save it to fire and load later and there's ap ice or for doing that kind of thing and the pipeline also has api's like evaluates predict and fit or evaluation prediction and incremental training on new data respectively and these interfaces are quite self-explanatory ok last i wanted to introduce a new project which we built on top of ours scalable automatic times your support in analogies ooh the name is pretty dull we have created specifically for telco applications in project so we not only expose the automail capabilities for time serializing the auto TS module we also expose to some building models without optimal and we also provide some reference solutions for typical telecom news cases such as net for traffic forecasting I will talk about the details of this reference use case in the next section now we come to the last part of the new sketch sharing and learning now let's first talk about the reference use case in project so that the network traffic KPI forecasting we know that actually many KPIs are connected from the telecom network such as download and upload speed channel utilizations network traffic and number of connected users payback forecasting has wide applications in telco a real application that we have worked with our customer is to use KPI forecasting for energy saving you know that some of the kpi's can indicate the real-time demand and our customer tries to for Catholic eight for each cell of the base stations so that they can know which cells are expecting low service demand in the next day we based on those KPI forecasts they can schedule shutdown for individual cells so that they can save a dramatic amount of energy with that much impact on the custom of our experience and besides energy saving people also use traffic KPI forecasting for adaptive Network splicing and that's why we choose KPI forecasting as a reference example in project award and in our reference case we use a public data set which contains real-world network traffic data maintained by the white project and we have used the aggregated track at the KPIs such as total bytes and average rates as example targets for forecasting and we use the data in the past week to forecast the traffic in the next two hours we have provided the to network to tune important samples to show how to use the low speed api's to solve this forecasting problem and one is to use the built-in forecasters and the other is to use OTS mojo which has the Ottoman supports this reference case is very close to reality and the some of our customers has built their own application from these reference need notebooks just with slight modifications now let's look at the case this actually is a project that we previously collaborated with SK Telecom this is another type of application for KPI forecasting in this case SK Telecom used the KPI forecast to identify potential anomaly in cells and base stations and they use them to generate alerts and other item in the 3d map the reason we make it example as such kind of anomaly is very common in telco although the case itself is not implemented using the time series illusion we talked about in this session we do have other customers who are using our solution to work on very similar anomaly detection application SK Telecom has how can last year's submit and you can know more details about this kind of application from that talk and besides in project so we have a similar forecast model as the one used in this case and it proved to work better than traditional methods in the real world custom use cases now let's look at a real case which uses our times your solution introduced in this talk this is a project that we collaborated with new soft which is IT service provider whose customers include some of the most famous German lecturers automobile manufacturing and in their intelligent AI op system they use forecasting and allowi detection on monitor metrics for DMS and services they use results to generate alerts help adaptive resource scheduling and for visualization in this case they use our solution with autumn elf to achieve the desired forecasting accuracy in a very short time so now here is the summary of the takeaway is from our early users first of all some highlights we found that it is a advantage that our models allow extra features beside the target values themselves for example we have generated extra time-based features such as peak or off-peak hours holiday weekend and our customers also have their own domain related features such features can significantly improve the forecast accuracy in real applications and with our provided models and automated training most of the users can get a satisfactory accuracy faster than their manual Tony however not all use cases can achieve a desirable result at very early stage and most of the time it is caused by poor data quality and missing value plays the major part in it and the waste to fill the missing value could have huge impact on the accuracy results another problem many customer faces is a scale of data and the problem is up to note we the time dimension but there are too many times yours to forecast for example one of our telco customers have hundreds of thousands of cells and each cell has several KPIs to forecast which asked up to millions of times years to forecast at the time and similar thing happens in a VM metrics forecasting in a call set ins where you can have millions of containers and each container may have up to hundreds of metrics to forecast which would have to add up to hundreds of millions of time-series to forecast so as this cut a sub scale it is clearly not efficient to build one model for each time series and we have been working on this problem and got some progress now the future work first as I just said I we have been working on solving the problem of extremely high dimensional time series forecasting and we also plan to improve the search algorithm at the meta-learning and on global components and we're adding more models and features for time series based on customer request and we are also considering the possibility of adding automatic data pre-processing which improves the data quality especially missing matter imputation ok so this last page is a big picture of intel's AI technology scope you can see that intel provides a wide variety of hardware software and integrated solutions for you to accelerate your data analytics and AI end-to-end if you're interested in any of these technologies you can follow the links below to find more details so now now so now I'm ready to take some questions if you have any thing that you want to know further you can ask me now you 