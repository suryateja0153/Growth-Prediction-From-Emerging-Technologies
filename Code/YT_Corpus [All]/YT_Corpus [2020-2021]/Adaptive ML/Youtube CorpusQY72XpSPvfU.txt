 Thank you very much for inviting me, and thank you for staying up to the last talk of the day. So I'm going to be talking about statistical learning theory approach to causal inference problems. Primarily I'm going to be focusing on a single paper, which is joint work with Dylan Foster who is a postdoc here at the IDSS, but I'm going to be giving a brief overview of other papers in the area. So the motivation of the work is that machine learning is infiltrating more and more decision making. So we see that in multitude of domains like in business, personalized medicine, or law. People are trying to use machine learning to answer a lot of decision questions, for instance, how would you offer personalized discounts or how would you offer personalized drug therapies or what bail should a judge set. The problem is that most of this decision making questions require understanding of answers to causal or counterfactual questions. So what would happen if we change the existing policy that we deploy in the data that we collect and did something else? Well, most of machine learning methods are typically targeting prediction questions. What would happen in the future if we continue with the same treatment policy as the treatment policy we used for the data that we collected? And so you might think that these two points seems to be implying that machine learning is not a good fit for causal inference, but what I guess we'll try to show in this talk is that many times we can still get a lot of benefit from using machine learning in decision making or causal questions. And for instance, many times we can estimate causal ML models as long as we use auxiliary models for debiasing the pure predictive models that we could have fitted if we deployed machine learning in a naive manner. So there is some hope of combining multiple machine learning models in an intelligent way so that you can get a causal looking model. So the goal would be to develop a general framework for such machine learning with nuisance models. These auxiliary models are typically called nuisance models. And two main points is that there's a lot of benefit from combining techniques from both econometrics or semiparametric statistics and machine learning. And one is like from the field of econometrics there is this very nice notion of Neyman orthogonality that is typical using a lot of semiparametric statistical theory. And we'll see in this talk that there is a nice way to combine this notion of Neyman orthogonality and generalization bounds that people typically try to proof in machine learning. And on the other side what we'll hopefully show is that we can phrase a lot of causal inference questions as generalization bound questions and apply a lot of the recent results in machine learning and statistical learning theory, and get guarantees for causal inference problems that are meaningful for people working in causal inference. So I'll try to highlight the main ideas of the framework through a series of three examples, and then I'll present the more theoretical formulation. So the first example. Let's try to look at the problem of suppose that you're trying to estimate how demand of a product change as a function of price. And suppose that you were a data scientist in a company and someone came to you with historical data of purchase and demand and price, and they asked you what is the function of demand as a function of price? And these are the data points maybe you had. And if you're a beginner data scientist and you know some of machine learning, what you might want to try to do is to fit a model that predicts demand from price, and then say that this model that you fitted maybe using machine learning is the model how demand changes with price. And so you use here some sort of like maybe even a linear model, you fit this linear curve through your data, and it actually has a very small mean square error or the residuals are very tiny. You do maybe some cross validation you also see that the out of sample loss is very small. And you say that this is the demand as a function of price. And then you go to the person that asked you to do the task and says we should increase the price to infinity because that seems to be maximizing our revenue. The problem is that you obviously made a big mistake here in that in trying to solve this actually you forgot that typically price changes in anticipation of demand. So there is a lot of other types of variables that would cause a lift or a change in price that would also simultaneously cause a change or a lift in demand. For instance, it could be that during Christmas price increases and also demand increases. But it's not that demand increases because of price, but it's just that they could simultaneously correlate through this other variable. So if you know better data scientist what you would try to do is you try to control all of these variables typically called confounders which simultaneously affected the treatment or the pricing decision in your historical data and also could simultaneously have a direct effect on the outcome. So you could add here the season indicator into your model, and maybe you would fit a better machine learning model that also includes decision indicator as an input. Now everything is much nicer because by including this you are going to split the data into the data that came from Christmas and the data came not from Christmas. And then locally in these data points, you're going to fit a downward sloping curve. And so the coefficient for the price is going to be this negative thing. And now you're going to get a more meaningful answer. The problem that you might now start to face is that if you want to be sure that the coefficient that you feed in front of the price is the causal coefficient is that you might need to control for hundreds or maybe thousands potential confounders. Especially if you're fitting a linear model maybe you want to fit for nonlinearities or maybe you create a non-linear features of a small set of variables, or if you're in a big company where you did a lot of data collection, maybe you have a lot of features for the products that you priced and so you want to control for all of those features. So then the question becomes a statistical question. You want to fit this very high dimensional model that control for a lot of confounders but all you care about is this coefficient in front of the price. And so typically you won't be able to estimate this quantity over here very well, but the question is can you still estimate this single parameter that you really care about in a fast parametric rate? So that's one question that you might want to ask. I might need to estimate this quantity over here, this function f of X of how the confounders affect the outcome, but it's not that I really care about this function over here. All I care about is this coefficient in front of the price. And we'll call the price also interchangeably treatment because it's a more general set. So the question is these are like the auxiliary models that you need to fit. These are like the nuisance models that you need to fit in order to get a causal estimate but you don't really care about. And so maybe if this was a low dimensional object so that you could estimate the root and rate, and then maybe you just around your standard machine learning algorithm, and you try to predict the outcome from both the treatment and the X's and then you might get reasonable answers, but typically this won't be estimable at the root and rate. And so the question is how can you devise a better algorithm that estimates this at the root and rate while controlling for a very high dimensional object. And so here's a solution that was recently proposed by a group of econometricians many of them here at the Econ Department. That's typically referred to with the name as double machine learning. So what double machine learning propose that you should do? It says let's try to predict the outcome from all of these things that you're trying to control from all of the confounders. So here you're trying to build a predictive model, and you can use whatever predictive modeling pipeline you like. You can use any machine learning algorithm that you want to solve this prediction problem, which is just a normal regression. Similarly you will try to predict the treatment, the price, from all of these things that you want a control for. Now the interesting part is that if you write the residual of the outcome, whatever you could not predict from the outcome, then this has a linear relationship with the residual in the treatment, which is whatever you could not predict from the treatment. And so in a final stage you can just run a single dimensional linear regression problem that regresses the sum predictable part of the outcome and the unpredictable part of the treatment. And so this is the symbol of square loss minimization here. And the nice theorem is that if you do these first two regressions on a separate sample, say you split the data, you run the first regressions on half of the data and you run this last regression on the other half, then as long as these first stage regressions satisfy a root mean square error rate that is quite slow of order into the minus quarter, then your final estimate in this regression will behave as if you had the normal single-dimensional regression problem. So it will-- It should be fast rate in that [INAUDIBLE]. It fast. Yeah, at least that. But this I'm saying is slow. It's a slow rate but it should be at least as fast as I [INAUDIBLE], yes. So then this would behave as if you were just running a single-dimensional linear regression. So that means that the estimation of these is going to be one over root 10. And also importantly, it's going to asymptotically have a normal distribution. And so you can also do hypothesis testing for your final estimate theta as if you had a single-dimensional parametric problem. One of the reasons that you can get away with slow rates here and still get roots and rates is this property of Neynam orthogonality that your final regression here satisfies. So if you take the derivative of this final loss, then this corresponds to what is known as a moment. And so your estimator here is an estimator that is based on finding a zero of a moment that satisfies this notion of Neyman orthogonality. And I'll describe later on as we generalize this notion what exactly it is. But this is the reason why you can get away with these rates. And then the other reason that you don't need to assume anything about your first stage models other than the mean square error rate is that you do this sample splitting. So sample splitting and Neyman orthogonality together give you this very nice theorem. You can also if you're really scared about sample splitting, maybe you have a few samples, there is also a better version of sample splitting where you fit these models on the first half of the data and predict on the other half, and fit on the other half and predict on the first half, and then you use all of your data in the final regression, which is called cross fitting. And this would still have the same properties as the sample splitting estimator So some remarks. In order to estimate this causal effect which is this single coefficient that we were interested in and the fast stage we had to estimate these nuisance functions, which were like regressional prediction problems, and so we could use machine learning for that. One drawback of this theorem is that it was targeting an average treatment effect. So this would say, on average if I change my price how would the demand change? Many times in most of these motivating scenarios I mentioned in the first slide, we might be interested in personalized decisions, which means that we need to estimate the heterogeneous effects. So how does the price affect demand conditional on a bunch of features that I observe of the customer I'm pricing or of the patient I'm treating. And so you don't just want to estimate an average treatment effect, theta, but maybe most probably heterogeneous treatment effect model or a function theta of x. So apart from using machine learning in order to estimate these nuisance functions that you don't really care about, you might want to also use machine learning and data adaptive estimation for also estimating this complicated heterogeneous effect functionality theta of x. So there are two reasons why you might want to use machine learning, I mean more like data adaptive estimation. It's because you might want to estimate more accurately your nuisance functions and be non-parametric, and also because you might want to estimate your heterogeneous treatment effect more accurately and also be non-parametric. So how would you alter the previous theorem in order to address this heterogeneity problem? You can do a very simple adaptation of the double ML algorithm and target heterogeneous treatment effects. And the simple adaptation is instead of just minimizing with respect to constant here just minimize with respect to some hypothesis base theta of the square loss where now instead of a constant, we have any function that takes as input these features that you observe. And this is like now is a statistical learning problem. We would just want to minimize this loss with respect to some hypothesis space. And we can also apply our favorite machine learning pipelines. We can do cross validation. We can apply our generalization bounds for whatever hypothesis space we have [INAUDIBLE]. So it's just like X is now W? So yes. So I'm saying we're now controlling for both X and W, but we might want to use just X for heterogeneity. I'm splitting now-- So you're splitting X into X and W and the one that you want to keep is X? Yeah, the one that I want to keep for heterogeneity is X. The rest I want still control for, but maybe I don't want to estimate heterogeneity with respect. And there were some papers that used this approach to estimate heterogeneous effects where they analyzed this algorithm for particular hypothesis spaces here. So for instance the paper of Nie Wager analyzed a case where theta is producing kernel Hilbert space. In a paper we wrote we analyzed the case where theta is a sparse linear function. So high X is high dimensional and this is a linear function. But we only assume that only a few of the X's are relevant. And in another paper we also analyzed the case where theta is represented by a random forest. Now for this complicated hypothesis space is proving asymptotic normality is typically hard. So for forest we could do something, but in general you wouldn't hope to prove some form of asymptotic normality, but you could at least hope to prove some sort of mean square error of your final causal model. And this is exactly what these papers were showing. They were showing that the mean square error of the estimate in this final regression, in this final loss minimization problem  gets a second order hit from the errors in this first stage model. So this is sort of like the robustness that was also in the parametric case where we were able to get root and rates for the final estimate as long as the first stage functions were into the one quarter. And here the same thing is happening here. You can show that the mean square error of theta is of the order of the Oracle mean square error that you would have gotten had you really known q and p plus a second order impact from the mean square errors of q and p that are coming from the first stage estimation errors. So you get this robustness to the nuisance quantities. So this was the first set of results that were showing such robustness of the final causal model to estimation errors in the nuisance models that you didn't care about. So is it just one thing? If W is nothing does this reduce to just regressing Y into X or not? Not exactly because you still run some form of non parliamentary regression in the first two stages. And maybe still the confounding part is a hard error function to learn than the heterogeneous effect part. So maybe you believe that the heterogeneous effect is a sparse linear function, but maybe the confounding effect is a denser function. So you're still going to get an improvement here. So this was also one motivating example in this paper. We're saying maybe everything is high dimensional linear, but maybe the heterogeneous effect is a simpler function to learn. It has fewer non-zero coefficients and so you get a benefit from doing this. So let me show you a second example that has a very similar flavor. And this is the case where we now have binary treatments. So instead of having these continuous treatments that we analyzed in the previous step, though you could apply also the previous algorithm to binary treatments. But there is another nicer algorithm for the case of binary treatments. So when you are just trying to decide whether to treat or not, say in a personalized health care type of application, you still want to estimate what is the effect of the treatment based on some observational data. But again, if you forget to realize that your treatment in the observational data might be biased, so maybe you are treating more people that look like X, and maybe those people that look like X also have better clinical outcomes, and so you would be wrongly attributing those better clinical outcomes to the drug as opposed to these features that define this subgroup X. So if you want to estimate an unbiased estimate of the treatment effects, say the average treatment effect, you need to control for all of these variables that describe this bias. So you need to control for all of the variables that might have entered into your treatment decision in the historical data. One way to do it would be a very model heavy approach where you just try to predict the outcome Y from both the treatment and all of these features that you want to control for. This would be a direct regression method. And then once you learn such a model that learns the conditional expectation of the outcome condition of the treatment and all of the things you want to control for, then an unbiased estimate of the average treatment effect is just the average of the difference between the treated and the untreated. So you can apply your model that you learned and apply it for t equals 1 and apply it for t equal 0 and then take the average of the whole population. This is a very model heavy approach because essentially what you're doing here is you're learning a model that learns, for instance, the main outcome from the treatment population and then apply that model that you learn on the untreated population because you're going to apply this function on both the treatment and the untreated. And so you're doing some form of domain adaptation and that might also perform poorly. Also what you're essentially doing is you're extrapolating from the treated population to the untreated population based on a model, whatever model you use here to fit this regression. Here the term t are the individual treatments or the average? Individual treatments, yes. So another way to deal with this problem is to use what is known as inverse propensity weighting. Most probably many of you know about it, which is to observe that if you divide the outcome that you observe for every patient or for every sample by the probability of that sample receiving the treatment that you observe, then this is an unbiased estimate of the outcome of the counterfactual outcome of that sample under the every possible treatment. So this would be zero if you didn't get the treatment. And this would be Y divided by the propensity if you did get treatment t. And so in expectation you're going to get that the expected value of Yi is 0 is equal to the true potential outcome and the expected Y of I1 is equal to the true potential outcome. So now that you have a number assessment of both potential outcomes which is like what would have happened to exactly you if we treated you or we didn't treat you, then you can just take for every person the difference of these two potential outcomes and take an average of this difference over the whole population. And this would be an unbiased estimate of the average treatment effect. How did you resolve the problem of not having a model to estimate Pt at this point? Yes, so here if you knew the propensity in the data then here you wouldn't need to fit any model. So if you knew what policy was being deployed in the hospital for instance, you would just use that. You wouldn't need to fit the model. But as you say many times you would, and this is why this I'm saying is not a very model heavy approach, but again if you don't know the observational policy then you'd need to fit a model for Pt. And so you will have, again, a problem that estimation errors in Pt are going to directly affect the accuracy of theta. And so if this was a non-parametric estimation or some high-dimensional estimation, you're going to get the same type of inaccuracy on your final estimate theta. And so this is also not a very good approach because of these direct hit from the estimation error on P on the final estimate theta. Also this is not a good approach because it also has high variance. Even if you knew the propensity, you're dividing by it. And so you're dividing a relatively large number by a relatively small number, so you get a high variance estimator. So one way to resolve the drawbacks of the two methods would be to combine them. And this is the doubly robust estimator that dates back to early work of Robins and Rotnitsky and Zhao where you basically fit both the propensity model and the direct regression model. So you try to predict the counterfactual outcome based on a regression model. But then you try to the bias that, you say OK, but whatever is not really well explained by the regression model, which is this residual, I'm going to do an inverse propensity correction to it. And you can see that this is also not-- so this has the W robust property, which means that if either H was a correct model or P was the correct model, then this Y is an unbiased estimator over the counterfactual outcome. So this is why it's doubly robust because we need only one of the two models to be correct. And you can see that here Y because suppose that H was the correct model then this difference over here in expectation is a mean zero difference. And so this whole quantity cancels out and the effect of P on the counterfactual outcome is mean zero. So P does not really matter in the counterfactual outcome. Similarly if P was the correct model, then this indicator here divided by this is going to be a minus one. So minus 8 plus 8 is going to cancel out. And all that you're going to get this back the inverse propensity model. But this is a better estimator because it has both W robust property. Also it's a more efficient estimator because now you're dividing just the residual by the propensity, so you're dividing a much smaller number by the propensity. And it provably has a better variance than the previous estimator. So now instead of taking the average of the inverse propensity counterfactual outcomes, you're going to take the average of these W robust counterfactual outcomes. The problem again is that here this was primarily targeting the average treatment effects. And so how do you estimate the heterogeneous effects. And here there is an easy adaptation of this algorithm for heterogeneous effects where you construct all these W robust counterfactual outcomes. And instead of just taking the average, you minimize a square loss trying to predict the difference of this counterfactual outcomes from a function of the features X that you want heterogeneity with respect to and you minimize the square loss of some hypothesis space. And then in these papers we could show that again the mean square error of your model theta over here gets a second order hit on the mean square errors of the estimates of P and H that you did in these first stages. So you again have these robustness of the final causal model to estimation errors of these nuisance models that you need to estimate but you don't really care about. And the intuition here is this W robust property that I mentioned before. And essentially W robustness has an intimate connection with this notion of Neyman orthogonality. They're not identical but they're for example, typically W robustness implies Neyman orthogonality. So again deep down the reason why you have these robust properties is this Neyman orthogonality property of this final loss function that you're using here. And finally let me give you an example from the literature and policy learning, which is a small tweak to the previous setup. So suppose that instead of trying to estimate the heterogeneous treatment effect now we just want to find optimal treatment policies. So now what you want to say is given the features X that I observe do I want to treat or not treat this patient? So this is not a model that tries to predict the actual effect, but it's just the model that tries to predict will the effect be positive or negative, and then say treat or not treat based on the sign of a policy. So if this is all that you want to do, many times you can bypass the whole modeling step and bypass estimating the actual heterogeneous effect and directly target this policy problem. And here's how you can do it using this W robust idea from the previous slides. So because we know that this W robust estimates are good estimates of each counterfactual outcome, we can now write a good estimate of what would be the value of any treatment policy. So now where theta is not the model of the heterogeneous effect. Theta is a policy that maps the features to an indicator whether I treat or don't treat. And the value for any such policy theta is take an average of your population of if you treat that sample and you collect the counterfactual outcome from the treatment. If you don't treat them you collect the counterfactual outcome from the known treatment. So this is now your value function, and this is what you're trying to optimize. Optimizing this essentially means that you're optimizing this linear function of your policy theta where the coefficient in front of every sample is the difference in counterfactual outcomes. Do you sample spilt here or? Yes, in all of this you always sample split for the nuisance spots or do cross fitting. And again there was this paper by Athey Wager that says that they regret of your final policy here, so the difference between that final policy and the best policy within the policy space, if you knew the true value function, is of the same order as the Oracle or regret plus a second order effect of the nuisance estimation errors of either regression model or the propensity model. So here the Y's are not random variables? They're expected values? So the Y's here are these exact quantities over here. They are random variables, and they depend both on the regression model and the propensity model and the real data Yi. But you're not trying to maximize the expectation? I'm trying to maximize the expected policy value. So you're taking an average of the policy value. But now I have a good proxy of what would happen to this particular patient if I treat or not treat which is this in expectation this is the correct thing. And so this expectation will be an unbiased estimator there. So the Oracle is the one that's minimizing the expected values? Yes. And so all these prior works have led us to formulate a general version of all of these results which we call orthogonal statistical learning. And so here's the general set up. You have a target model theta that is defined as the minimizer of some population loss function. But this loss function crucially apart from the model theta that you're trying to estimate also depends on other models that you don't really know about which you also need to estimate from the data. So these are the nuisance models, and this is the target model that you're interested in. What you care about is achieving good Oracle excess risk. So given n samples find the model theta hat such that the difference between in the population loss and when evaluated at the true nuisance functions, the difference of theta hat and the minimizer is at most rate Rn. So for instance, in the pricing example this is the implicit population loss that you were trying to minimize, the square loss that depends on the direct regression model and the propensity model, and theta is your heterogeneous effect. In the binary treatment case this was your population loss that again here it depends on the regression model and the propensity model. And this is your target model that you're interested in. And in the policy example this is the linear population loss that you were interested in where again it might depend on these nuisance functions. Now why do you care about Oracle excess risk? For example here for the case of square losses, good excess risk is going to imply good mean square error because of strong convexity properties of the square loss. And here Oracle excess risk is essentially equal to the regret of your policy. all of the things that you're interested in here can be abstracted away to this Oracle excess risk target quantity. Another reason why we want to formulate this as a generalization bound problem as excess risk problem is because we want to use a lot of recent advances in statistical learning theory and in statistical machine learning that prove better generalization bounds for a different hypothesis basis. So by phrasing it as a generalization bound problem we're closer to being able to use all of these machinery that is recently being developed. The only problem is that most of this machinery is targeting situations where you know what your population loss where you don't have this unknown nuisance function. So you're given some population loss that does not depend on any auxiliary quantities, and you can achieve some excess risk rate that in many times you can also prove minimax optimality. So most of ML theory addresses such questions. And as I said, this generalization bound can also imply several quantities that you might be interested in where you're doing causal inference or policy evaluation. So the question is can we reduce our problem which is the statistical learning problem with the nuisance component to a standard machine learning problem which is statistical learning without the nuisance component? The other side advantage of focusing on excess risk as opposed to other quantities that people are looking in causal inference and econometrics is that we typically by focusing on excess risk we do not require several stringent assumptions on identification of parameters of the model. For instance, if you're trying to estimate the model of heterogeneity that is sparse linear function, if you want to do parameter recovery, you might need to assume some form of restricted eigenvalue condition. If you just want some generalization bound you might not need to assume such a condition. And many times you can also prove results when you have missspecification when you focus on excess risk you can say, my true model of heterogeneity might not be in the model space that I'm optimizing over, but maybe I'll achieve good excess risk with respect to the best policy in the class. Correct me if I'm wrong. So along [INAUDIBLE] to sample splitting because they care about to inference the others [INAUDIBLE] the result for the individual effect, the heterogeneous individual effect, seems like it's really hard to get normal results, then maybe you don't need some of the [INAUDIBLE] We still need for the general theorem that we're going to show because we want to say that you can apply a generalization bond for a plugin estimate as if your data where ID. if you've already fitted it on the same data, then you don't have this IID in this condition, and so it's not clear what correlation among the data this nuisance estimate creates. And so you cannot really apply black box in a black box manner all of the statistical learning theory guarantees that typically assume IID data. But the data is still IID, it's just the having the evaluation depends on a certain function of the data. [INAUDIBLE] Yeah, and maybe you'll then need to do some sort of like-- you might need still need some sort of like [INAUDIBLE] property for your nuisance estimate. But here with sample spliting you will only need mean square error [INAUDIBLE] if you have your nuisance estimate. So even that buys you something. So again, so yeah, so this is exactly to this discussion, this is the meta-algorithm that we're going to analyze to reduce statistical learning with nuisance component to normal statistical learning. We're going to split our samples in half, and on the first half we're going to estimate the nuisance estimates based on some arbitrary estimation method. Subject to that estimation method that seeming some form of mean square error guarantee. And then we're going to plug in this first stage estimate into the loss function, and apply any other ML estimator or any other estimator that you want that achieves good excess risk for the plugin nuisance functions. So it manages to achieve an excess risk for this loss function of Rn hat of theta. So now once I plug in the g hat, this is a normal statistical learning problem. And you can apply any theorems that you want from statistical learning theory to these plugin statistical learning problem. But of course, this is not what you really want. You don't want an excess risk on the plugin nuisance function. You want an excess risk through nuisance function. So the main question then you want to ask is how do these two relate. And can you show that this thing is of the same order as this plus second order effect from these. To be able to do that we need to introduce this notion of Neyman orthogonality of the loss function. So to define the notion of Neyman orthogonality, we first need to define directional derivative because we're going to try to understand derivatives with respect to these functions of our loss function with respect to these functions. And the directional derivatives the pretty standard thing. If you have a direction mu in this function space, you're going to define the directional derivative as the standard derivative when you're moving in that direction and when you evaluate this derivative at t equal zero. So this is a directional derivative. And then a loss Ld Neyman orthogonal if this cross derivative quantity is equal to zero. So if the derivative with respect to your nuisance functions over the derivative with respect to the target function when evaluated at the true target model of the true nuisance model is equal to zero. The intuition of this condition is that small perturbations of the nuisance model around its true value do not change by much the grading information that is in your loss for the target model that you're interested in. So by slightly changing the nuisance functions, the first order condition roughly remains the same. And this is satisfied by all of the loss functions that we described in the first part of the talk. So for instance, for the square loss over here of the residual, if I take a derivative with respect to theta, this is sort of like a normal derivative with respect to the output of this function. So this is going to be just two times this quantity multiplied by this quantity. And then when I take a derivative with respect to a nuisance, say I take a derivative with respect to this, then all that will remain is this difference over here. But this difference is the residual, which under the true nuisance model mean zero. And so this quality is going to be equal to zero. So this residualization of the treatment was really what made this loss function be Neyman orthogonal. And so one of the main theorems that we proved was that if a loss is orthogonal and strongly convex or these types of square losses and satisfy several smoothness properties, then the Oracle excess risk that we care about is indeed upper bounded by this plugin excess risk plus a second order effect from the first stage estimation errors. Here we're going to have to the fourth rate. This is really a square because we assume that root mean squared error rate for the first stages, and these are typical square losses, so that's why you get to the fourth. But for the case of parametric functions theta, the rate that you should expect for the square loss would be one over root 10. And then the rate that you would need here is into the minus quarter. So if you assume that if you take the first two derivatives against G advantage can you put a six up there if you want? Yes, you can. But it's not clear how you would achieve the second order Neyman orthogonality. We had one paper where we'll basically proving the existence of such a second order Neyman orthogonal for this particular set up of a residual residual. That thing that we did there was more like a moment based estimation, and it didn't really correspond to the gradient of a loss, so it's unclear how you do heterogeneous effect. So yes, you can but the conditions of the existence of these second order Neyman orthogonal are more stringent. Many times you can't even have them. It's easy to achieve a first order orthogonality by just running a debiasing term typical. So I'm not really going to talk about all of these. I think I'm also running out of time. The other result that we have is for the case where you don't have a strong convexity. So the case, for example, of policy learning where our loss was linear so it was not strongly convex. Then if we make a slightly stronger assumption of universal Neyman orthogonality, which basically says that this cross derivative is zero not only at the true theta but every theta. Then we still get this second order dependence from the first stage estimators. But here because we expect some slow rates, one over root and rates, we get the square and not the fourth moment. So  with that, I think, given that I'm out of time, I would just like to stop here and just mentioned some takeaway points. So machine learning is useful in causal analysis for estimation of nuisance models and for estimation of heterogeneity. Neyman orthogonality is a concept that can improve machine learning theory for causal problems. So you can show robustness to nuisance errors. In many times you can even show asymptotic validity of the confidence intervals. And on the other hand, focusing on excess risk and causal problems we can get more robust theorems that do not make stringent assumptions on identification, and enable the use of recent advances in statistical learning theory for fitting causal models that belong to complex hypothesis spaces. That's it. Thank you very much. [APPLAUSE] 