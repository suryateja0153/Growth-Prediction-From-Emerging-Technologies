 you [Music] okay let's get started thanks for coming to today's Microsoft Research AI distinguished lecture it's an honor today to have Michael Kern's with us Michael is Professor and the National Center chair in the computer and information science department at the University of Pennsylvania you also have as appointments in the Economics Department and at the Wharton School of Business he's the founding director of pens Warren Center for networking data Sciences he's as you all know published widely in machine learning algorithm a game theory and quantitative finance he's beyond his scholarship he's worked in the finance and technology industries he's consulted for government agencies and companies on legal and regulatory matters involving algorithms data and machine learning he co-authored an introduction to computational learning theory and beyond of that I was as I mentioned we had a great session last night at seattle town hall with aaron roth his co-author on the ethical algorithm and as i mentioned last night to the audience i have ability like the stick memory of first meeting michael in at you AI 1986 in philadelphia he was right over here at the side of the room and I looked over and he was making a giving a presentation I was very impressed with his crispness and I thought to him because we had the old-timers the founders of the UI conference from 1985 I was the first one we were all there and who is this new person who is staying all these interesting things and I followed his work ever since then he and Aaron Roth wrote this I think I think fabulous book I had to read it because I moderated the session at Seattle Town Hall last night and I was really happy to have forced myself to read every page of it and really really enjoyed it it's a really fabulous contribution to the discussion on the role of algorithms and the rising interest in their influence inadvertent and even malicious in the in the world and the fact that the book goes from some of the topics we all know and study fairness and bias and and intelligibility and explanation and privacy issues that many topics we cover at Microsoft Research and on the ether committee efforts microsoft also dives into science which I thought was a my favorite chapter on you know sort of the how the inferences we make in our scientific endeavors are a great a great bulk of of these inferences and results are likely to be non reproducible and false with that Michael okay thanks Eric for the kind introduction in for your terrific madhuri moderating last night it was a great event and it's great to be here actually somehow in my now long career i've never managed to make it to MSR redmond so this is kind of a bucket list visit for me i'm happy to be here rich i spend time in cambridge i've spent a lot of time in the new york lab i visited the new england lab but i've never actually been to the mothership so yeah yeah yeah yeah I was so yeah so my close friend and colleague at Penn Aaron Roth and I decided a couple years ago to write a general audience book and I'm gonna do something a little unusual with this talk because I get the feeling that on the one hand it's a diverse audience but on the other hand there are probably some card-carrying AI ml researchers in the room and so what I want to do is spend the first half of my time kind of giving the description of the book that we give to rather general audiences at the risk of boring the more technical people in the room but I'm kind of hoping that even if I'm shredding over territory that you know well scientifically or at least familiar with in theme you might find it interesting just the way that we try to phrase it for a more general audience and get something out of it for that reason and for those of you that haven't seen these topics I'm hoping it'll just be interesting out right but then what I want to do is spend time at the end doing a more deep on recent algorithmic work that we and others have been doing on more refined notions of fairness in machine learning and related areas so our book is called the ethical algorithm and this would the subtitle the science of socially aware algorithm design it just came out officially a couple of weeks ago although it's been shipping from Amazon for a little bit longer than that I mean so what is this book about so you know quickly on the heels of the rush of excitement around the Great's empirical successes of AI and machine learning in the earlier part of this decade quickly came like the buzzkill of reports about all kinds of what we might call anti-social behavior by algorithms often but not always algorithms that really we might more accurately refer to as models because they are the output of an algorithm that took data that it trained a model on and then the thing that's actually deployed and making the decisions is let's say a neural network for example and you know it's an interesting time to work in the so-called fate area fairness accountability transparency and ethics because literally you know I'm giving variants of this talk you know all the time these days and pretty much every single time I'm able to include a fresh mainstream media news report on antisocial behavior by algorithms I'll mention this week's favorites a little bit later in the talk but you know this is this has been kind of in the public consciousness enough for long enough now that there are some good general audience books that precede us on these topics you've probably heard or read some of these like weapons of mass destruction algorithms of oppression these are really about kind of bias and unfairness in algorithmic decision-making and books like data and Goliath which are you know kind of examining privacy and security issues and making the argument that we have essentially arrived at something akin to a surveillance state and you know Aaron and I I think many people in this room doing technical work in these areas we like these books but and feel like they do an excellent job of identifying real problems and making them kind of visceral to ordinary readers and so we admire these books and consider them inspirations but we were disappointed in these books when they came to the parts about you know what to do about these problems what the solutions might be and all these all these books have sections like that usually a little bit towards the end and I perhaps unfairly paraphrase but I think they're they're kind of about the form like well you know we need better laws we need better regulations and policies we need watchdog groups you know kind of we really need to keep an eye on this stuff and Aaron and I agree firmly with all of that but the part that we we're disappointed with is the fact that as researchers in the field know you know if if there are problems with algorithm algorithmic misbehavior you could also contemplate making the algorithms better in the first place you know fixing the algorithms in some sense so that they don't in you know entail violations of privacy or racial or gender discrimination and the like and you know to be fair some of these books I think were written when that research agenda was still rather immature and we don't know as much as we do now but our goal in writing our book was to kind of tell the world that there there is a science you know you can make scientific a lot of these questions or a lot of these problems of unfairness privacy violations interpretability etc and design alguns that are you know demonstrably better in the first place and so our book is kind of a survey about why you would want to do that how things go wrong in the first place what the potential algorithmic solutions are and also what this what the costs of those algorithmic solutions will be because there will be costs to them as well as as we'll discuss an early in an early draft that we was sent around for you know for comments and input from various people by our publisher we got a very interesting critique back in which the the critic basically said you know your title kind of is is a conundrum or doesn't make sense you know why would you talk about ethical algorithms because you wouldn't talk about ethical for instance so the point that this reviewer was making is that look you know a hammer is a tool it is a human artifact designed for a specific purpose and even though of course that tool can be misused perhaps in unethical ways I could hit you on the hand with a hammer nobody you know nobody would sort of think to ascribe that moral lapse or ethical failing to the hammer itself they would blame me for hitting you on the hand with it and so this this critics argument was you know algorithms are the same way you shouldn't talk about algorithms being ethical or not and you shouldn't talk about like the moral failings of algorithms you should talk about the moral failings of the designers of algorithms and we we thought carefully about this comment and and we disagree we think that algorithms even though they are also tools and artifacts designed for particular purposes there's a number of things that give algorithms kind of a different ethical or moral stature than their designers okay and you know we go into more detail on this in the book but the basic argument that we make is look especially in the ear of machine learning it can be hard to assign blame to the designer of an algorithm due to the multiple steps involved in the machine learning pipeline and the sort of levels of indirection involved so you know that pipeline usually starts with data gathered from somewhere and you may not know exactly what that how that data was gathered or exactly what every attribute means and furthermore that data might be a hybrid of multiple data sources whose provenance is sort of uncertainty so already from the beginning the input to your algorithm might be of uncertain origins and you might not know for instance if it's police data whether the arresting officers might have been racially biased in the first place and so the data that you see from the beginning has that encoded into it we then usually use this data to define some sort of objective function or landscape and primarily that objective function will be will primarily be concerned with predictive accuracy for example and as well say this is one of the problems of course we take that data in that objective function and use some complicated optimization procedure like stochastic gradient descent on a multi-layer neural network and we get some rather complicated looking model and the point is is that after all these steps are done you know not only can it be hard you know if the algorithm causes harm to some specific individual not only can it be difficult to sort of say that the algorithm designer was specifically responsible for that particular misdeed unlike when I hit you on the hand with a hammer we may not even know that these harms are being caused until we decide to look at them so if I hit you on the hand with a hammer both of us know what has happened and it's very clear that a harm has been caused to you in algorithmic decision-making often the harms are subtle diffuse and until somebody bothers to look and document them we don't even know that they're happening okay so the basic premise of our book and the research field that we are trying to represent to a broader audience is that we need to in addition to legal regulatory policy and other solutions we need to think about embedding social values into our algorithms and so I literally mean changing the code of our algorithms in order to a court incorporate social norms that we care about now one of the interesting things maybe the most interesting thing about this exercise is that you know of course if you're going to explain something to an algorithm if you're going to encode a social norm into an algorithm you have to be exceedingly precise in your definition of that that social norm right you need to be able to write the code that implements it and so even though we will be the first to acknowledge that there are centuries or maybe eons of scholars and thinkers of all variety who've thought about you know fairness for way longer than we have and privacy for way longer than we have and you know other topics as well you know legal scholars philosophers economists etc that have thought about these issues have never had to think so precisely about them that they could be put into a computer program and some the very act of forcing yourself to think that precisely will expose flaws in your intuitions about topics like fairness and privacy that you wouldn't have uncovered if you were thinking in a looser more macroscopic way okay so I'm listing here several different social norms that are ones that there's a lot of discussion these days about you know can we encode these so can we define give satisfying definitions of these different notions and then encode them in our algorithms and you know that you notice that they're in in kind of increasing greyscale here as we go from top to bottom and roughly that greyscale represents in our view how much is known scientifically about these topics at this moment so we feel that for instance the study of data privacy and algorithms for enforcing data privacy is the most mature of these topics and and furthermore isn't the most mature but has kind of a central definition that we argue is the right definition which is differential privacy which I'll talk about in a minute the study of fairness is off to a fast start it's much much more recent but as we'll see it's it's still more embryonic and we already know that it's going to be Messier there won't be a definition you know there won't be a differential privacy of fairness in the way that all in the way that I'll describe we already know that we are gonna have to maintain multiple notions of fairness and this is just the reacts I until Acree ality similarly we have things like accountability and interpretability which we talk less about in the book not because we think they're less important but there's less to say scientifically of the type of science that we're interested in here and then one might of course already put morality here and I promise you there's a final bullet there which is the singularity but you can't see it because it's entirely in white okay so what I want to do just in the in the first part of this talk is spend a little bit of time talking about privacy and fairness which together with the introduction takes up about the first half of the book and again these are the areas where I think we're on the firmest ground and give you a sample of what's known there and and why this process of thinking hard about munitions can be quite revealing even in and of itself then I'll take one slide to kind of tell you about the second half of the book which takes an interesting left turn but a related left turn and then what I want to do in the last part of the talk maybe last third maybe a little bit more if I'm lucky is just tell you more about some scientific some actual research going on in this area that we and others have been involved in in trying to move fairness definitions from these very aggregate coarse group definitions to ones that provide stronger guarantees to individuals in a population rather than just groups okay so that's the game plan and so let me just start off with privacy okay by the way you know I won't call it out specifically as I go through this talk but the influence of MSR is all over in this talk and all over in the book you know so differential privacy was founded by Cynthia Doric and her co-author z-- when she was at Microsoft a lot of the fairness research that we implicitly discuss in the book and that influences our own research is done by MSR people which has a fantastic faith group but but so let me start off by you know talking about you know why thinking hard about definitions is a valuable thing even before you getting or get around to putting it into algorithms so this quote at the top of the slide here is actually due to cynthia dork anonymized data isn't and the way to parse that is that you know either you haven't anonymized the data and so it's not anonymized data or you've anonymized it so much that it's not really data anymore okay and so it's not useful for anything and so prior to differential privacy or you know maybe even to this day um the prevailing formal definitions of privacy including all those in my view unfortunately that are in force in industry and and practice today are all based on notions of anonymization or you know things like removing personally identifiable information PII and so the basic idea is you try to get some sense of privacy or some assurances of privacy by taking a data set and redacting certain columns entirely or corseting others in the hope that sort of you know by doing this you can't re identify any particular people in the data set okay so in this toy example at the top there is a database of medical records from a hospital maybe it's the hospital of the University of Pennsylvania and in the efforts to anonymize you know it's been decided that names should be redacted entirely ages rather than being given precisely should be grouped by decades genders left alone zip code last two digits are redacted etc etc so some columns we've removed entirely others we've coarsened okay and one notion that's popular in kind of the scientific literature on anonymity would say something like okay you should you know you should do these two operations redaction and coarsening until your database has the property that prefer any particular individual or line in that course and dataset there are many many records that match it so whereas before this redaction coarsening maybe you know your medical record was unique in the sense that you know nobody else's attributes matched exactly yours the goal here is to make lots of people match and so then there's you know allegedly confusion about who's who okay so for example at the top you know this is a two anonymous data set in the sense that for any particular entry in this table there's at least one other matching entry and so let's suppose for the sake of argument that you have a neighbor and so because she's your neighbor you know stuff about her that you couldn't like to get out of this redacted database you know that her name is Rebecca and you know that she's a 57 year old woman okay so from this other source of data that you know about your neighbor you can go look at this database and say like okay there's two possible records that match what I know about Rebecca and the ones highlighted in red and so now you have some uncertainty right the idea is like now you don't sure which one belongs to Rebecca I note that already you know that she's either HIV or has colitis and she might consider your knowing the or of those a violation of her privacy already and you might try to wish this away by saying okay I know in a real medical database there'd be ten thousand records and I might do enough of this that a hundred records match meant Rebecca and so now I'm really confused about what her diagnosis is the real problem of course is you know triangulation right or or you know linkage analysis is sometimes called so in this second database which is maybe the database of Jefferson Hospital in Philadelphia you might also know that Rebecca had been a patient there maybe you gave her a ride there one morning and so in this database you know this has been sufficiently anonymized that three records match Rebecca and now of course from the join of these two databases you uniquely know from the top and the bottom that she's HIV okay and you know you can try to wish this away in various senses but maybe the most damning thing that I could say about this entire category of definition is a statement in plain English which is the problem with definitions like this is that they pretend like the data set in front of us is the only data set that exists in the world or that will ever exist in the world in perpetuity and they don't anticipate other information I might know like about my neighbor Rebecca because she's my neighbor or other publicly available information that all of us have offered about ourselves thinking that it's innocuous like our activity on Facebook or our social media posts okay and so we argue in the book that these types of definitions are fundamentally flawed and one of the one of the reasons that they're fundamentally flawed is that they're kind of ad hoc they don't they don't start off by saying this is what privacy should mean and now can we design algorithms that meet that definition of privacy they sort of say like oh look you know if you do this then it sure seems like there's some guarantee of anonymity here and I would I would offer a very crude metaphor which is from Krypton rafi so those of you that know the history of cryptography in my view the history of cryptography up until like the development of public key cryptography in the 70s more or less had the form like oh well if we scramble up the letters this way it sure looks random and it looks random until it doesn't look so random to somebody until somebody cracks the code and then it's all over and the advent of public key cryptography in the 70s basically put the study of encryption and security more generally on kind of firm algorithmic footing right it basically gave a definition that said this is the criterion that should be met and then the question is can you design algorithms that meet that so continuing with this metaphor we think that the you know if if things like anonymization were are like you know pre public key cryptography data privacy we think that sort of the corresponding landmark in privacy to public key cryptography is differential privacy but before I talk about differential privacy let me let me say like okay so I've pointed out one flawed type of definition for privacy let's step back for a second and say well what would be what would be the strongest definition of privacy that we could possibly ask for okay and again you can make this mathematical if you want to and people have but in plain English one perhaps strongest definition of privacy I could give is I could say look it should be the case that any analysis that involves your private data whether it's you know the release of a data set that's in a course and in some way or the result of a computation that involved your data in that computation or the release of statistics about a database in which your data was present whatever it is no harm should come to you as a result of that statistic analysis or computation period full stop okay so certainly that is a very strong definition of privacy like nothing bad can ever happen to you from an analysis that involved your data okay let me argue that this is asking too much this is asking too much because if we insist on the we basically won't be able to do anything with data including many things that we'd very much like to be able to do so suppose it is 1950 and I picked that year very particularly and suppose you're a smoker and if it's 1950 you're almost certainly a smoker because in 1950 pretty much everybody as a smoker there are no known there's no health stigma associated with it there's no social stigma associated with it it's actually seen as fashionable and glamorous okay and suppose you were asked to participate in a medical study that that asked for your medical record and your smoking habits which was actually conducted in a famous study in England in the 1950s or after the period I'm proposing that you were a smoker and you go ahead and you say sure go ahead and use my medical record and then as it did this record this this medical study establishes a firm link between smoking and lung cancer okay so the fact you were smoker was public knowledge you didn't hide it everybody that knows you knew that you were a smoker now your medical record was used in the study and now the world knows that smoking and lung cancer are linked and now everybody's posterior belief about your having cancer goes up as a result of this study and in particular your insurer might raise your premiums because of this Bayesian update so to speak okay so we could really say that actual harm came to you your data was used in this analysis this analysis established this link between smoking and lung cancer so harm has come to you as a result of this study so therefore under the definition of privacy that I proposed you wouldn't be able to do this study okay so clearly this is asking too much in the sense that we won't be able to do scientific research that's data driven if we insist on this notion of privacy and this brings us to the definition of differential privacy which is a similar kind of definition but weakened in an important way that sort of makes it practical while still providing very strong guarantees and so the observation here is that in this story about the smoking and lung cancer you know even though your medical record was used in that study it's not like your medical record was the key day piece of data that established the correlation between smoking and lung cancer the correlation between smoking and lung cancer is what we might call a fact about the world that can be discovered from any sufficiently large data set it wasn't that your data sets your data's inclusion was crucial it was just that there is enough data okay so so the definition of differential privacy sounds similar but is weakened in an important way it basically says that no harm should come to you as a result of an analysis involving your data that wasn't going to happen to you already if you hadn't included your data okay so you do a thought experiment in which you imagine an algorithm that's let's say getting many people's medical records as input and you you basically say privacy is preserved if for any particular individual in the input database if we compare the result of the computation including all of the data and the result of the computation in which we have n minus 1 records would just your record removed if that if the output is almost exactly the same then we have differential privacy okay and if one looks at the definition of differential privacy you'll see that interestingly it requires randomization because the notion of the output being nearly the same as a distributional notion so on any given input the algorithm induces a distribution over possible outputs okay and what can't change much when we include or don't include your data is that distribution in a technical sense that's quite standard okay and to demystify this a little bit the way differential privacy works the way you achieve differential privacy when you can achieve it is by adding randomization to computations so if for example I want to publish in a differentially private way the average salary of all Microsoft employees what I do is I do the tradition non-private computation which I just take everybody's salaries I averaged them together and I output two numerical precision what the average salary is I don't do that I do the precise computation and then I add let's say Gaussian noise with zero mean to that number that I get okay and it turns out by adding the right amount of noise you get two desirable properties nobody you you you obey differential privacy for each individual salary in the input data set but yet the output will still be quite useful so to a first a first approximation it's fair to say in this specific example if there are n employees of Microsoft and I add sort of order noise whose variance is on the order of one over square root of n I will achieve this definition okay now differential privacy I think was first defined in about 2005 and it's you know it's theory underwent a great deal of development in the ten years subsequently when I first saw the definition of differential privacy I'm a relatively recent you know researcher in the topic when my first when I first saw it I said like well okay that's a great definition of privacy that's you know kind of the strongest definition you could provide while still hoping to do useful things with data but I was worried that it's too strong did like you know interest the computations that we want to do and machine learning for instance wouldn't be able to be made differentially private subsequent work has like you know happily shown me to have been wrong so for example pretty much every method tool or computation you know in statistics and machine learning whether it's classical hypothesis testing methods or back propagation for neural networks none of them are differentially private all of them have modifications that are differentially private and still give answers that are quite close to the non private deterministic computations so we know how to do machine learning in a differentially private way and differential privacy has started to gradually creep into practice and products so the so for instance Apple was one of the first companies to field differential privacy in the iOS reporting of app usage statistics to the mothership in Cupertino the big moonshot test for differential privacy is coming next year when the US census has decided that every single report or statistics it releases based on census data will be done under the constraint of differential privacy and we talked about this a little bit in the book but there's you know even though the theory is well in hand we know how to do all this stuff in principle there's a lot of engineering considerations that have to be made including about the trade-off between how much noise you add which you know of course adding more noise gives you more privacy there's a knob here and but you'll have less accurate answers there collecting detailed data and storing it how they look but the but how they share it will be done under the constraint of difference and they have to buy by law collect that you know they have this this is why they can't randomly sample they actually need to literally canvass every single residence in the United States or try to yeah so my question is related to your comment on the pragmatics of differential privacy and experience so the issue is that when you think about a very vanilla database system or a data system which serves a vast a question like a relational database system it gives answers it seems to me that what you are saying is that the notion of differential privacy has to be very tightly coupled with the task you're trying to do so I'm if that's the case it creates the problem for the system developers because yes I want to play my relational database system can support differential privacy what is this supposed to really mean because that's where is after I get a result required you become way if people do it is they do a selection and then you know at some point there's a trade-off that you referred to within usability yeah so if I understand your question correctly I disagree with your characterization so one subset of techniques in differential privacy is to take a database and to release a synthetic version of that data set database so the odd you know so it's not like differential privacy's only good if I have a database and I want to run backpropagation on it if I have a database and I and the object I want to share with people is a database so that they can do sequel queries the database has actual records in it that you can look at you know it's rows and columns it's a spreadsheet one technique in differential privacy is to start with an original database and to produce a version of it okay there will be a database and will basically preserve the statistics of the original database so there'll be some set of queries that you might have wanted to run on that original database and what will and what will promise what differential privacy will promise you can't you can't do this by the way in all cases but when you can do this the synthetic database will preserve up to some error all of the statistics in some class of statistics on the that of the original database okay you're you're going down a great line here if you can pre specify that class you can get away with adding less noise because you were able to commit to that before if if it's adaptive it's more complicated and by the way this ties in with the chapter that Eric was mentioning about the reproducibility crisis and the sciences where you essentially are doing adaptive data analysis you form a hypothesis you gather some data your hypothesis was wrong but you saw something else that's interesting so then you formulate a new hub and and the Quinns sort of a lot of the questions that you're asking or but like when can you do that when is that okay when can you do that without like you know creating entire fields of false science right like like food science for example okay okay so let me so that's a little bit about privacy and about why thinking hard about definitions can sort of expose flaws and your thinking and lead you to a better definition I want to sort of tell you you know where we are in that agenda with fairness yeah just a general all right is there any discussion about defining privacy in the terms of harm so actually if you dig into the definition of so differential privacy has there's multiple ways you can phrase the definition that are equivalent but let me let me tell you the original formulation which is exactly of the type that you're you're described so so the definition of epsilon differential privacy basically says the following you have some private data you're contemplating on whether to contribute it to some computation that will include lots of other people's data as well take whatever event it is whatever outcome now we're in the future it is that you're worried about that you think constitutes harm so this might be you know the world learning about your medical condition this might be your insurance premiums being raised it might mean getting robo calls on your cell phone whatever harm it is you're worried about you get to pick it okay the promise of differential privacy is that with respect to sort of the output distribution of a differentially private algorithm that the you know consider the probability that this harm was going to come to you if you didn't allow your data to be used and the probability that this harm will come to you if you allow your data to be used those two probabilities whatever you're worried about are within 1 plus or minus Epsilon at each other the seiner this is a universal quantifier anything that you're worried about including things that you can't even articulate or think about yourself are protected by this definition every single one of them okay let me go on and talk about fairness so we already know that the study of fairness is unfortunately going to be Messier more complicated than privacy one things that was interesting about writing this book is I'm like okay privacy is a social norm fairness is a social norm discussions around fairness are much more divisive in society than discussions about privacy at a high level you know people have different attitudes about privacy for themselves but in general people are like you know privacy is a good thing and we should all have it okay with fairness you need to talk about fair to whom for example and you know you immediately go down paths that are you know very divisive and evoke strong emotions from people but so it's not just kind of messy in that social way it's also we know it's gonna be messy technically so we don't agree on the definitions in fact as a result of some very public controversies over fairness and criminal sentencing software for example researchers you know a couple of different groups independently noted kind of what I might call like arrows impossibility theorem for fairness so these are papers that say okay look here are three here are three different axiomatic properties that you'd like any definition of fairness to me right and you know you look at these three properties and you say yes yes these are very weak properties any definition of fairness should certainly meet these three properties and I'd like any definition of fairness to actually be stronger than this okay and then of course the punchline of the paper is well guess what here is a proof that you cannot achieve those three properties simultaneously period okay so this means that not only will as with privacy there be trade-offs between you know the social norm privacy or unfairness and and predictive accuracy there will actually be trade-offs there may be trade-offs between different notions of privacy so it really might mean that if I want to build a predictive model that reduces racial discrimination I literally have to do that at the cost of increased gender discrimination like this maybe the reality and there's just no squirreling around it and you know stakeholders and society will have to make hard decisions about these kinds of things right people like us can articulate these trade-offs but you know real you know real stakeholders have to to make their choices about them but so I live I get a little bit ahead of myself let me just give a very brief tutorial for those who might have not seen these examples about why machine learning even in its standard principled form might result in discrimination of various kinds and here's just as I promised you know the two latest media articles about apparent unfairness in statistical modeling one from a paper from science from a couple of weeks ago that established that a widely used health assessment tool used in many many large US hospitals had systemic racial bias in it and then the other one that you might have heard about I don't think this one has been scientifically established even though New York has does Orion launched an investigation you might have seen these media stories about the fact that Apple's new credit card to several couples who tweeted about it offered to the husband a credit limit that was ten times that offered to the wife even though they filed joint taxes and the wife had a higher credit rating okay so doesn't look good for Apple okay but let me let me go back to basics here for a second and give you another kind of toy example along the lines of my medical databases you know suppose Aaron and I are asked to help the Penn admissions office build a predictive model for success in college based only on let's say high school GPA and SAT score so here each plus or minus here has an X value representing the GPA of some of some student and the Y value has their SAT score and let's suppose that these are students that were previously admitted to Penn this is historical data so we know for each one of these individuals whether they succeeded in college or not and for success in college pick any objectively you know quantifiable notion of success that we like maybe it's they graduated within five years with at least a 3.0 GPA they donated twenty million dollars back to Penn within thirty years of graduating you pick your favorite okay as long as we can measure it okay and so there's a couple of things I want you to notice about this cloud of points first of all if you looked carefully at it you'd seen that this population of app of past students slightly less than half of them succeeded there's a few more minuses than pluses here okay in general as you might expect the pluses are sort of towards the upper-right higher SAT scores in GPA and if I was asked to build a predictive model from this data I'm you know a simple threshold rule seems to do a pretty good job if I sort of predict success for everybody above that blue line and and not in success for everybody below I make some mistakes right there's some minuses above the line and some pluses below the line but it does a pretty good job okay okay now can suppose though there's also a second population sub population in our historical data the red population and I want you to notice a few things about the red population first of all they are a minority in the dictionary sense of that word right there are many fewer of them than the orange population notice that they're equally or even slightly better prepared for collegiate success half of them are plus half of them are - but notice also compared to the orange cloud that the SAT scores though all these red points just seem like they're shifted down compared to the orange points in particular their SAT scores are lower okay and you know one explanation for this might be that the orange demographic is wealthier and pays for SAT preparation courses and multiple retakes of the exam and reports only the highest and the minority red population doesn't have the financial resources for that so they just take the exam cold once and that's it okay now notice also that you know if I build a predictive model for the red minority population there's now a perfect classifier for separating + from - but if I combine the two populations and I just say look the overall goal is to maximize predictive accuracy minimize the number of false positives and false false negatives then basically I'm still going to pick the model that's determined by the orange majority because you can see if I try to shift that line down to include the red pluses I will pick up so many more false admits from the oranges that it won't be worth it okay so this is one of several ways that just the the you know usually sound principles of machine learning can result in a model that has systemic racial discrimination now you might notice like well look suppose and suppose we just noticed this thing about SAT scores it's not that the red population is less successful in college they just for whatever reason have lower SAT scores I might propose a hybrid model in which I say like look if they're an orange applicant use the upper line and if they're a red applicant use the lower line okay and not only would have I in is this a more fair model right now the false rejection rates between the two populations are much closer than they were before now I'm actually more accurate as well so this is what I would call like a Pareto improvement if my two criteria are fairness and accuracy I've actually made both of them better unfortunately many laws and regulations in the United States in for instance areas like lending explicitly forbid this type of model because this is a race-based model right I am looking at the race of the applicant orange or red first and then you know it's like it's like a decision tree if they're Orange do this if they're if they're red do that and so this is kind of points out how wrongheaded those rules and definitions are this idea that you know by excluding race as a variable from your model that you're going to protect a minority group this simple example shows you that not only might you not be helping that minority group or protecting them you might be actively enforcing damage on them by refusing to allow the model to look at this variable okay okay now notice though that if I wanted to fix this problem algorithmically there's kind of a straightforward path to doing so so in particular right for any particular you know line like this one I can write down two numbers I can down it's overall error rate on the data on the entire data set the two populations combined but another number I could write down is there's the sort of unfairness measure right so in this case I might say that that what constitutes harm is false rejection rejecting somebody that would have succeeded in college by our definition and this particular model here right has a false rejection rate on the orange population that's quite small it's basically the fraction of pluses below that blue orange pluses below that blue line but it's false rejection rate on the red population is a hundred percent every one of the red pluses is rejected by this model okay but so now I could change my objective I can basically say instead of just like minimize the overall error I could solve a constrained minimization problem minimize the error subject to the constraint that the false rejection rate between the two populations is zero or I could turn that knob and say the false positive rejection disparity has to be at most one percent or five percent or ten percent if I crank that knob all the way up to a hundred percent it's like I'm not asking for any fairness at all okay so this gives me a knob that lets me tune the trade-off between accuracy and and fairness or err and unfairness and when you do that you know you trace out what we would call like the efficient or Pareto frontier right you get you get a class of models corresponding to the different trade offs you can make between these two on quantities so on three real different data sets I won't go into the detail you know each red dot corresponds to like a different model the x-axis is the error of that model okay so lower is better the y-axis is the false positive false rejection disparity or the unfairness of that model so again lower is better we'd like to be at zero zero that's not happening on any of these curves we can choose to go for the lowest possible error ignoring fairness and we'll get bad unfairness we can choose to go for zero disparity or unfairness and get the worst error or we can get anything in between now there are lots of models that are not on this line right there above this line and to the right you don't ever want to consider those models if these are your two criteria because they're dominated right you could always move to a model on the efficient frontier and improve in one or possibly both of these measures but you know one of the points we make in the book is that whatever social norm you decide to encode in your algorithm whether it's privacy or fairness it's gonna come at a cost at a minimum it's gonna come at a cost to predictive accuracy and these curves are exactly quantifying what that trade-off looks like and then it's again up to stakeholders right if if this is sort of you know you know false false rejection disparity in admissions decisions and this is accuracy of our predictions admissions offices need to get used to looking at a plot like this and committing to a point on it and justifying that point yeah question your definition of fairness and the inclusion of race and criteria why just not conditioning on something like household income which predicts race but also identifies you know majority factors where they can't retake the test either right if you're from a low-income group but a majority group you also can't retake the test and presumably the same argument you gave applies to that group as well that they would be skewed there essentially because it's not a good measure overall probably to perform over multiple tests I'm not quite sure I followed what the Western is you proposed the argument that race should be included explicitly to to fix some of these issues but the underlying causal argument you gave was household income and access to resources I'm not proposing that including race fixes anything I'm proposing that insisting on excluding it might cause harm they're not the same thing right so you know another way of putting it as we put it in the book is that we think that good fairness definitions and unfortunately or fortunately there are multiple competing good definitions of fairness I mentioned but they should basically constrain the output of your predictions they shouldn't constrain the inputs right you're fooling yourself if you think oh by excluding race as a variable that on somehow not racially biased or that there aren't other proxies for race like zip code in my data set so we're saying like if your if your way of thinking is like oh I want to protect these different groups and so I'm gonna exclude what kind of we just think that that's false thinking and that you're fooling yourself and can cause active damage we don't say that like not doing those things guarantees like fairness we but we think that the right definitions of fairness basically say you know your model should not exhibit this empirical behavior that's a violation of fairness and then it's up to an algorithm designer to figure out whether you can achieve that criterion or not and at what cost it should be included as part of the metric but not as part of the optimization I I'm not I I don't have I don't have a prescription or recommendation for what inputs you use in building a model I think fairness should be something that you define on the behavior of the model and and you know if you can do you can achieve that definition excluding or excluding certain variables then fine but we don't think the definition of fairness should be like oh you know it is a model is racially fair as long as it doesn't mention race as a variable that that's you know this is I think kind of roughly akin to anonymization definitions and privacy yeah coming in I think that whether it's a proxy or not the goal here is you still want to have enough inputs that you can discriminate appropriately and I the word discriminate are used intentionally here because the whole point of any classifiers is to discriminate and put things in a right yeah right group I understand what you're saying it's a confusing use of the word in the context of like discrimination of the social variety but yeah and so I I mean it's the comment is if you don't use raise but you still want to find the appropriate signal coming in wherever it's from or the data so that you can organize and get the right result coming in there and I think that's yeah so in general like if we that's not a Laredo sort of computational and data resources set aside if I didn't care about fairness you know I want to add as many features to my model as possible just in case they might have some marginal predictive value especially if I'm building something complicated like a deep neural network but but of course when you do that you know you also introduce more opportunities for things to go wrong of the variety that I described on the last slide and this is why we and others argue that you need like a crisp definition about fairness that sort of is agnostic to what inputs the algorithm used but constrains its actual behavior yeah right so ago you rejected the use of race as a choice of model right I use different models what you've done race you reject as being not legal I mean I didn't reject it as maybe not legal the law reject constraints equally speaking what's the difference your students in what in in the constructs you're hearing things you're using the false positive you myrrh popular race as well as the constraints that's right and so some of what I'm suggesting here depending on the application area might violate current law so so like our goal in the stud scientific study of fairness is not to adhere to current laws that we think make no sense we think eventually those laws will need to be rewritten just go back if your argue we could use we should use race of the factor make decision why don't we just use race it has to be clear like in this toy example allowing this two-part model will not necessarily solve all racial discrimination I can give you a different data set in which even when I let you have a two-part model that uses race you still will have discrimination against one population or the other and you need to explicitly it to enforce a fairness constraint in your optimization okay so just because you know this example is constructed to sort of show you one thing that can break you know the thing that fixes that in this particular example is not a general solution for that problem unbelievable race because you know you are going to get to a situation I can define a sub population at home like a predicate with a predicate and say hey I want to be sort of be fair to them and if I start asking this you know random collection of predicates that define the sub population if I get to it this is exactly sort of some of the research that I would talk about but Eric advised me on how much time I actually okay so we'll come to exactly this question shortly okay so before I get to sort of the researchy part of the talk let me just quickly tell you what happens in the rest of the book and about midway through the book we take you know as I said one of a wide left term but we think an interesting and relevant wide left turn and let me kind of tell you what the the basis of that left turn is in all the examples I've given so far about violations of privacy or fairness and the like and in many other of the social norms that I had listed on that kind of grey scale list at the beginning like you know interpretability accountability and the like it really seems fair to a first approximation to think about individual citizens or consumers as the victims of algorithms so in particular you know maybe you get rejected for a loan by an algorithm you don't even know that it was an algorithm that rejected you you know you haven't you have no idea that it wasn't a human loan officer that looked carefully and you know thought about your application you know you might not even know this and similarly you might not know if your data was used to build such a model that made decisions about other people we might be completely unaware of it and as I said on the ethical hammer slide right we might not be aware that certain types of harms are being caused because nobody's sort of thought to look for them yet okay and there's a lot of different ones that you can imagine in other settings you know there's an algorithm at play but it's mediating the Preferences of a large group of users who might have strategic or self-interested considerations and it's not you know I might even put it at its meet the album's meeting the preferences of a large population that might be competing and it's not so even though the algorithm or the collective of the algorithm and its users might exhibit undesirable social behavior it's not so easy to entirely blame the algorithm okay and this is where we kind of get into the realm of game theory and and relate it to topics so let me give maybe what is the cleanest combination of something that can be precisely formulated mathematically and that people have everyday familiarity with which is you know commuting or driving on the roadways and the use of navigation apps like Waze and Google Maps okay so on the one hand what could be better than ways in Google Maps compared to when I grew up when you know if you wanted you know your goal is still the same it's not to minimize the length of the of your you know that the the miles that you drive its to get from point A to B the fastest and so traffic comes into consideration okay and you know back in the 60s the goal that goal was still the same I'd like to somehow figure out the fastest way of getting from my point A to my point B but the means I had were like you know fold up maps in the glove compartment which would literally just tell you the roadway is available and maybe like half hourly traffic reports on just the major roadways that might be stale by the time that you know you're on that roadway okay so what could be better than these apps right in response to what basically everybody else is doing right now all of the other you know all of the real-time traffic from everybody's GPS devices these you know algorithms or apps compute my you know fastest route from my point A to point B so in like game theory language I might even say that they're basically helping me compute my best response to what every other driver is doing right now okay so you can actually think about commuting or driving it's a game right it's an incredibly complicated game you can't write it down like prisoner's dilemma but it is a game right every driver on the road is a player every player has a payoff function or a cost function which is the time it takes them to drive from point A to point B these apps are basically computing your best response they're computing your optimal route giving what everyone else is doing and as such they are literally essentially encouraging us all to play the competitive or Nash equilibrium of this very very complicated game now you might say like well so what you know how can that be a bad thing if you've ever had any game theory you know that just because something is in equilibrium doesn't mean that everyone or even anyone is happy with that outcome and there are clean mathematical examples and some real-world evidence that show that you know at Nash equilibrium we might collectively be driving let's say as much of as much as you know a third more 33% more then we might under some other solution okay or you know which which in economics would be called the maximum social welfare solutions so you so you can still imagine algorithmic solutions here you could imagine a variant of ways that collects the same information from all it takes real time GPS information it has a map of traffic right now but instead of handing out to everybody their shortest driving time it does some more sophisticated computation that tries to minimize our collective driving time okay so we talked about this we talked about this not just in driving but in things like you know content curation ala Facebook's and news feed algorithm and many other settings as well and we kind of go back and forth between talking about algorithms that could help these situations in which you know the system collectively is exhibiting suboptimal you know behavior from a social standpoint but the interesting thing is that you know unlike fairness and privacy you have to worry about the incentives so if I proposed some alternative to Google Maps and ways that if if we all adopted it would lower our collective driving time I need to worry that individual people might say like well suckers that's great that you're all doing that but I'm gonna like open my way you know I'm gonna open Google Maps and I'm essentially gonna defect as in prisoner's dilemma and then you know this would cause us all to possibly slide back towards the competitive equilibrium and so we we talk in this kind of gaming chapter about the algorithmic issues involved in making the algorithms better behave socially but also kind of what the challenges are from an incentive or adoption standpoint and then this this other chapter that Erica mentioned we kind of you know follow the game theory chapter the algorithmic game theory chapter with a discussion of how these topics play out in data driven scientific research and how there is really in some sense a formal game going on there as well that's basically established by sort of the incentives of publishing and of publishers as well and at a high level this is getting back to the question you are making you know if you know you take a field like a machine learning where we all share these benchmark datasets and researchers want to publish in prestigious venues and we're all kind of munging around on the same data and looking at each other's papers so you're influencing and it making me adapt my hypotheses and the journals have the property that you can't publish negative results right if I you know if I go to the CFR data set and I try some fancy new algorithm and I say oh you know I tried this new method and it is not competitive with the state of the art I promise you that paper will not get published at ICML Learner ups and so you have this like you know you have this in this machine that is essentially giving biased reports or many an uncountable number of experiments are collectively being run there's a publication process that only about the positive ones right so in the same way that you know you always see this clickbait you know eat this particular type of berry and it'll improve your health and then six months later you know that's not reproduced or it's falsified we discuss with some seriousness the possibility that the machine learning community is driving itself in this direction hopefully not that far yet but we also talked about algorithmic solutions that interestingly include approaches involving differential privacy like by by kind of adding noise to datasets or by changing sort of the information that we like you know provides two competitors in these large ML competitions we might actually sort of be able to balance between having a lot of experiments and research being done without having these problems of false discovery and then we do have a last chapter on the things that we think are important but on which there's less kind of firm scientific things to say or even not even kind of agreed-upon definitions yet and these include interpretability accountability and yes we do even talk a little bit about the singularity at the end okay so let me take just for the aficionados in the audience let me talk just a little bit about kind of more current research at a technical level and and one of the things that our group and others have been thinking about quite hard lately is definitions of fairness that provides stronger guarantees to the individual so if you think about the types of fairness definitions i've alluded to so far they are all group fairness definitions so as a user of these definitions here's what you have to specify you first asked to specify what group or groups it is you're worried about being harmed if they you therefore wants to protect okay so you might be worried about let's say racial discrimination or gender discrimination and then you need to say well what constitutes a harm right and so you might decide if it's college admissions I'm worried about racial discrimination and false rejection is the harm right sort of somebody that would have succeeded that I rejected and I might not think of a false-positive as a harm that's somebody who wasn't gonna succeed but got in you know they got lucky but but the other type of error I'm gonna consider a harm okay and then what you basically do is you say like okay this is the group I want to protect this is what constitutes a harm and what I want to do is sort of equalize the rate of harm across you know the group I'm trying to project protect in the majority okay and the the the pros of these definitions include the fact that they're relatively easy to implement right I can kind of adapt my objective function in the way that I describe and say like don't just minimize the error full-stop minimize the error subject to this false rejection disparity limits so and they have fairly strong so they have fairly strong theory and practical implementations they provide no promises to individuals so in particular you know if in the toy example I gave you or a member of the red minority that was falsely rejected from college your consolation for the fact is the knowledge that I was falsely I'm going to falsely reject qualified members of the orange population at an equal rate so it's like I'm you know I've committed an injustice to you but you know you should feel better that I'm also you know committing that injustice to people different than you at the other extreme there is a literature on individual notions of fairness okay and these usually have kind of a metric based quality where you say similar individuals should be treated similarly so you know if I have two applicants that look very similar by some metric of similarity then they should either both be admitted or both be rejected but one shouldn't be admitted or and in general you know these provide stronger guarantees at the individual level but as you pointed out right if I go to the logical extreme of this right and then I could sort of say like well every particular person is a group under themselves and I want to like equalize the false rejection rate across all groups so this is sort of an a intractable definition because it doesn't allow me to make any kind of mistakes at all right as soon as if i falsely reject one person I've got to falsely reject everybody okay so so in general the only you know technical for the machine learning people in the audience you kind of to use these definitions you generally need you know what we call realizability you need it to be the case that the ground truth that you're trying to predict can actually be represented in your model class more or less perfectly which is essentially like a non-starter in the practice of machine learning okay so we've been spending a lot of time in the past several years trying to play around with definitions that somehow you know don't go all the way to the rabbit hole of trying to provide like literally specific individual fairness guarantees but are much more refined than these course group definitions and there's sort of a lot of technical stuff on this slide but let me let me let me skip ahead a little bit and give an example of the type of the definition that we have in mind here so let's let's suppose that let's say in college admissions are lending we want to make sure that like the false rejection rates aren't just approximately equal across racial groups but also across gender groups and also perhaps across income levels and age groups and disability status and the like okay so I might say like you know I'm gonna add a lot of constraints no I'm gonna say minimize the error subject to the constraint that you know across gender the rejection the false rejection rate is the same across ages it's the same cut it out okay so we know how to write that optimization problem down it's not that different from these group definitions it's just that now there's you know a handful of groups instead of one more attributes even if you do this you still might end up with a model that badly discriminates against disabled Hispanic women over age 55 making less than seventy five thousand dollars a year because I asked for fairness marginally in each one of those attributes and I didn't mention anything about combinations and I probably don't need to say it to this audience but one of the things we say early and often in the book is that especially what machine learning on complex models is involved you should never expect machine learning to give you something for free that you didn't explicitly ask for and you should never expect to avoid a behavior you don't want that you didn't tell it explicitly to avoid okay you just should assume those things are going to happen if you didn't say them okay so this leads us to questions like well suppose I have a bunch a collection of attributes and I'd like to not just promise fairness marginally but on combinations of them or functions of the meze have been more generally okay so so this is sort of interpolating between group fairness and individual therapist I'm not going to go all the way down to combinations of attributes that are so refined that they they act as a fingerprint for each one of us individually but I want to go closer to that than these group definitions and you can write this down as a constrained optimization problem right it's not a problem to to formulate it it really is minimize the error subject to the constraint that for sort of all combinations of these attributes you will have this approximate parity constraint with respect to whatever your notion of harm is okay where things get interesting is the algorithmic problem now because in the example I gave for instance if there are D of these attributes that I want to protect there will be sort of exponential in D many combinations of them and especially if D is large I don't want to actually get this fairness by explicitly enumerated all the possibilities so it turns out that there is a very nice algorithmic framework for solving these problems that when you can make it work gives you not just algorithms that are sort of efficient in the theoretical sense but they are practical in the empirical sense as well and I'm not going to go through these bullets but I'm going to kind of tell you what the approach is so the approach is to basically you know write down this constrained optimization problem and I'm gonna say some things here that some people in the audience will understand and others won't but you have this constrain optimization problem but the problem is that there is the the model space first of all might be very large so the the space of models that the learners choosing from might be like neural networks it's gonna be you know it's not gonna be something you want to like exhaustively enumerate so the learner has a very large space of models that they're searching over as in vanilla machine learning but now there's sort of an exponential or even infinite number of constraints on that model that come from the fairness considerations okay and so the first idea is to start with this constrained optimization problem and just sort of pass to its dual like sort of classic convex or linear programming duality and for those of you that are familiar with that theory this basically lets you go from a constrained optimization problem to a two player zero-sum game okay and it is to this two player zero-sum game has an appealing interpretation one player is the learner and the learners action space if you like in game theory terms they're pure strategy space or like all the different models in their class okay all neural networks for example the other player I'm going to call which is in food so the learner is the primal player and the dual player is what I'm going to call suggestively the regulator so the regulator basically is the way they're to enforce the constraints and so how does this game play out at a high level at a high level the learner you know in round one the learners it says okay I'm gonna minimize the error on the data set and they pick a neural network and then the regulator you know looks at that and says okay this neural network discriminates badly against disabled Hispanic women over age 55 making under seventy-five thousand dollars a year fix it and so this constraint gets added to like the Lagrangian of the learner who now has to essentially balance between accuracy and fairness to disabled Hispanic women over age 55 making less than seventy five thousand dollars a year and so this back and forth goes and of course the worry about this back and forth is that it will maybe never converge or will converge only after an infinite exponential or infinite number of steps it turns out that if you can if you can first of all figure out how to let the so called best response of each of these players if you can figure out how to reduce it to a standard non fare like a classification problem which you often can that's kind of that's kind of technical step one of this agenda and to can figure out how to implement at least one of the two players as what's known as a no regret learning algorithm which I'm not going to bother trying to describe but if you can accomplish those two steps then this back and forth between the learner and the regulator will actually converge rapidly and because part of the reduction involves you know an appeal to a standard learning algorithm this gives an obvious way to implement this game in real algorithms right you basically at a high level you've got you've used something like back propagation in neural networks as a subroutine it doesn't know about fairness at all but the players are kind of creating instances for training for that algorithm that in encode the fairness constraints and as long as one of the players you can implement in this no regret fashion this will provably converge so so so basically you you know it turns out that this but by the way this this algorithmic framework is very interesting and might be reminiscent to some of you of ganz by the way where you again you know set up a two player or a game between a learner and you know and a generator that's trying to fool the learner and you know for those of you that are familiar with that literature it wouldn't surprise me if sometime in the next couple of years using something like this algorithmic framework somebody actually kind of created a provably convergent version of Ganz where you actually would sort of say like okay you know here is the goal of the optimization here is a formal way of going from that to like LP duality and here's an efficient algorithm that provable because the way Gant's are usually done in practice now the setup is very same but the execution of it is very ad hoc you do sort of you know run two players against each other and it doesn't converge and you sort of pick the point in you know along the trajectory that looks the best but so I'm gonna skip the rest of this but but it turns out that this this framework is good for several different types of definitions of fairness in which one or both players might have a very large or exponential number of constraints and this is what lets you get closer to these notions of individual fairness than these group fairness notions and there's a lot of very interesting work going on by in this general area by by other groups as well and let me stop there and take any Q&A that there's time for the support from the beta the size of the data set you have about the minority group is going to finish as financially as well good good good good okay so I skipped over the technical slide but if you actually look at the constraints that we're enforcing it incorporates that right so what I mean by that is you know suppose for instance you have sort of unlimited conjunctions of these marginal attributes that you want to enforce fairness on and you want to get the combination so so basically what we what we really put a constraint on is the amount of unfairness that group can suffer times its mass in the distribution okay so what this means is that I'm because I'm looking at the product of those things at the if a group is a sizeable fraction of the population I've really got to make its unfairness equal to like the background rate but if you get down to sort of vanishingly small subgroups then I'm allowed to be more unfair to them and and they're sort of there's no way around this in the definition for the reason that you say right if I really sort of ask that arbitrarily rare groups have to be protected they might be so rare that even in a very large sample they're not represented in my dataset at all there's not even a single member of them and so you have to have just for statistical estimation convergence purposes you have to sort of only make this promise to the groups that have some minimum mass in the population or the distribution but then you can protect all of those and there might be an exponential or infinite number of those already it's a good question yeah I think you worked the back next my question is about differential privacy specifically how you convince companies to adopt the French or privacy based solutions in light of the following consideration if you take a company like Facebook right so they asked you you know to provide your age you know of a whole bunch integration of Excel and you could refuse to do that right when you can you can even fill those fields and look junk but what Facebook can then do is ask somebody who are your friends or just I mean some random people who might happen to know some information about you asking them to maybe guess or provider guess your age for instance okay and then they can do anything they like put this information and then say well we're not actually using information coming from this person or exact information about this person we're just using some guesses right and in this sense they are kind of be evacuated differentially can be vacuously differentially private and yet you know they still have a lot of information about you it can cause you harm but you did not provide so let me give a a high level answer I mean so you know all of these algorithmic solutions or maybe improvement as a more modest word that I mentioned in this talk and we discussed in the book you know adoption of them by companies has to you know macum Panisse want to say that they're adopting these definitions and figure out ways of gaming them you know the opportunities for that are rife I think you know so the companies either need to decide that they're really serious about this and they're not trying to fake it and kind of you know say that they're doing differential privacy but having other means of circumventing it or providing fairness in one place but you know having it be not consequential to their bottom line for instance you know I think of those as kind of social problems or maybe regulatory problems and I do think we're entering an era where regulatory forces on these companies are going to have much more teeth than they have in the past you might have been making a more technical point about differential privacy which I think we did I think I if I follow what you're saying an even better example of where differential privacy kind of doesn't promise you anything even though you might think that it does some of you might have heard about these recent uses of genealogical gene sequencing websites to solve cold case murders okay so in particular the Golden State Killer that was active in the 1980s I think was a serial murderer and rapist and was never discovered was they know they didn't know he was they had DNA from multiple sites of this individual but you know that he didn't they that this the DNA didn't match any criminal genetic database and and so this you know this was like 30 40 years ago but now have cropped these you know sites like 23andme where you can get your your your gene you know gene sequenced and you can upload them to these sites that let you find your distant relatives and other you know dark secrets about your past that you never wanted to know you've probably seen articles like this and so some a particularly like a retired detective basically said I'm I'm gonna unfreeze some of the Golden State killer's DNA and uploaded it to one of these data these services and they immediately found like several cousins of the Golden State killer and by and because these people are trying to find their relatives and are saying who they are and where they live suddenly they they knew you know here are cousins of the Golden State killers and they live in this area and this very quickly like within a matter of weeks led them to the Golden State killer who turned out to have been a former Sacramento police officer during the period that they were active okay so even if you protected these geological databases with differential privacy the Golden State killer didn't contribute their DNA to these databases yet these databases basically you know revealed him and the problem with DNA is that even though we might think of our DNA or genetic sequences as our private data the problem is is you share a lot of that with your relatives and if your relatives decide for instance to reveal that data it compromises you in a way the differential privacy and no notion of privacy can possibly protect if we're going to allow data analysis of any kind at all we have time for another one notions of grouping and individual fairness there's something that also looks at the two of them so in the case of group fairness I'm worried that you know we create algorithms and we make them broadly available and then we get into cases where for the individual could be that you know a disabled female um not only is she left out but no matter how many times she tries she never sees success because everybody is using the same algorithm and whereas if you had just even a random probability of success there's some probability that you succeed and that success adds more to it is there some sort of notion of an amortized individual fairness yeah in fact this yeah you know this slide average individual fairness I'm not sure if it's getting at the same idea you are but it's basically saying like well if there's sort of multiple decisions that are going to be made about you across your lifetime you know like what college you get into whether you get a loan what criminal sentence he received after that's enough I could ask for a different type of individual fairness that doesn't say that on each one of those decisions you are V you are treated fairly but sort of that you know your mistreatment across all of those decisions is sort of roughly the same as anyone elses and so this sort of same algorithmic framework can be used for that kind of fairness but I think there's another point that maybe you're getting at that this is not a solution to which is that in an era of algorithmic decision making you know so one of the things we've say in the book is like look discrimination in decision making is not new to society because of algorithms right you know there were racist loan officers and are racist loan officers for you know time immemorial but at least like before algorithmic decision making if you felt like you got rejected for racial reasons by one loan officer you could always go and try others and hope that there would be a different decision I think you're right that one thing that's special about algorithmic decision-making is that it happens at scale and so for instance I think I now believe that you know conditioned on your being rejected by one bank for a loan the chances that you'll be rejected by all of them are extremely high because they're all you know using the same credit scoring agencies they might you know and and even if they even if they developed their own algorithm right you know machine learning people like to think like oh you know their algorithms are unique they're really proprietary etc but you know it's like really the date if you have if you have informative features about what you're trying to predict and enough data then you know to a first approximation we're going to arrive at similar models even if we have different data sets and different like you know details of our methodology and I think that that aspect of scale and uniformity of decision is a problem and you know people it's sort of I haven't I don't think I've said anything here about fixes for that problem that you can imagine you know kind of obvious ones that include for instance the requirement of a certain amount of exploration regardless of what model you're currently deploying so when you talked about like the exponential number of yeah it's really exponential only in the number of attributes right because of course you have like the number constraints I you know for this there's sort of D attributes that I want to protect and I want to protect all combinations of them that would be exponential indeed no but you you need like but but but in order to sort of I mean to say it kind of very technically if I have an exponential number of constraints the amount of data that I need to you know make sure that my in-sample and out-of-sample fairness will be the same as only logarithmic in the number of constraints so it's basically linear and D again yeah which might take anymore about that top line thank you everybody [Applause] 