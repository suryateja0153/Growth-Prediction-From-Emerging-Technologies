 OK then maybe we do start because  the program is a bit tight anyway. On behalf of all of us, welcome to this tutorial! It is with great happiness that we're doing it again after several years. Give me a second I was just told that while sharing...  ok is this better? This is screen sharing if anybody is having issues with seeing what is being presented or knowing that something is being presented, please let us know. So I was saying it was a pleasure to be doing the tutorial again. We have done this a couple of times but it has been many years since the last installment and we are very  happy to be doing it again, and we hope that this will be interesting for all of you participating. Very few words about who we are: there's Stefan Weibelzahl who is currently at the Private University of Applied Sciences in Göttingen, there is myself Alex from Contexity, a company in Switzerland, and there's of course Judith from the Utrecht University in the Netherlands. A bit of what we know about the audience: obviously more people  replied to the questionnaire than are currently present, but as you can see here the makeup --at least of people that answered the questionnaires-- is predominantly people who have worked in personalized recommender systems and in intelligent user interfaces. And there's of course people from other domains like personalized health, personalized social web... sorry this is the next slide. This is the actual real-world domains in which people have applied the technologies: education, health, and cultural heritage are among there, e-commerce, etc. And we are going to try and pick up examples when explaining or presenting things that fit as many of these domains as possible. Of course you're always welcome to ask questions on how things might be translated into what you're doing yourselves if you like. A bit about the plan: the introduction part which is what I'm doing now will hopefully only last 15 minutes if I'm, if I don't take too much time. Then Stefan will be making an introduction into layered evaluation and what it is. We will have two coffee breaks and short ones, the first one will be at 5:00 after which we will have our first the hands-on session. Following that then Judith will be talking about evaluation methods in relation to layered evaluation and after a second short coffee break we will go on to the hand- second hands-on session and we'll wrap it up with a few things that one needs to keep in mind when evaluating adaptive systems and when applying layered evaluation. So to set the stage a bit what is evaluation? I will not spend too much time on it, I'm sure all of you have at least had some experience even if you haven't performed evaluations yourself. In the context of this tutorial we consider evaluation all types of user studies that inform the development or improvement of a system or demonstrate the impact of a technology and in different words we consider evaluation not only as a summative thing that happens at the end, when everything is ready and implemented and we just want to test if it really works, but as a formative activity as well, which one can do to inform the design of a system - and as you will see this is a subject that will come up several times especially in Judith's part that a lot of the methods are constrained by what- by at what stage of the development one wants to evaluate things in. Something that is important to address up front is why the evaluation of adaptation is different to normal evaluation of interactive systems and the main point here is that the premise is different. In traditional Human-Computer Interaction evaluation, the assumption is that all users experience the same system and this very assumption is actually what doesn't hold in adaptive systems. If every user experiences a personalized, a different version of the system, then how can one build an evaluation that actually measures what one really wants to measure? And there are things that have been tried in the past and they haven't really worked out very well. The most, the most common approach is to try a system with and without adaptivity and another approach that people have been- that people have tried is to treat adaptivity as a feature, as one dimension of a system and address that. It would be a bit- it would take too much time to go into detail on why these things don't work but a few things are worth mentioning. If you try any of these approaches, there are several different points where evaluation can get problematic. One is the selection of non adaptive control. So if I compare an adaptive version, what is the best non adaptive version that I should be comparing it to and why? The selection of equilibrium points, if there is- if there are processes which converge to the best places to the best values, then how do I choose that this is an equilibrium point at which I will compare this to something else. And very similar is the next point, if you have dynamic, the dynamics of adaptive behavior we have different optima - like you have a local optimum which works well for some users but there is a better place somewhere else so that we haven't found or that the evaluation is not designed to find. So another ... these are some some of the points and there are many more there's a lot of detail but I think it is enough to get an idea, and the evaluation goal itself is something that one really needs to consider before starting. In this particular case because in an evaluation very often we look for the best possible version of something and in an adaptive system we might actually be looking for several best possible versions that are best at achieving different goals and this in itself is again a very different approach to what we have had in the past. So what is the goal at the end of the tutorial? It is for the participants to really understand what are specific problems that are involved in the evaluation of adaptive systems and why we think that layered evaluation can help address at least some of these problems. And with- hopefully with the hands-on sessions, and what you hear today, you would be able to go back and plan a formative evaluation, a study of an adaptive system addressing either a specific layer or a specific set of criteria, or the system as a whole. So, this from myself and I think I will pass over to Stephan now who will talk about layers. Very good! We are ahead of time, unexpectedly. Let's see what we can do about that. Introduction of layers! So, I think this is actually probably the main concept that we want to get across here, tonight or this morning - wherever you are - today. We learned from this pre-questionnaire that actually many of you haven't even heard about the concept of so-called "Layered Evaluation". This is what we want to get across and something that you hopefully take home from today's session here. We will have lots of examples, but one example that will go through the following half an hour or so is a system called HTML Tutor. This is a system that I was working about a couple of years ago and the concept is really quite easy. This is an online tutorial system so you can learn HTML (what else) and so you can go in and just look at some content and also answer some questions. But, if you wanted to, then the system offered you to take a pretest. This can be at the very beginning or can be somewhere in between. Suddenly, this line here might pop up: "if you want to take a pretest go click here". And then, if you do that, you have to answer a couple of questions and go through that and these questions are especially selected, so that the system gets as much information from these answers as possible. You also get some feedback, whether you have a right or wrong and this is then used to adapt the system. This HTML Tutor is obviously adaptive and it adapts in several ways: one is, you can see here a concept called adaptive link annotation. This is like traffic lights. Green means: yes you have covered this, or we think or the system thinks you know this concept, you know this chapter, the section already. Yellow means: you're ready to start, you have met all the prerequisites, so if you want to, you can go to that section. And red means: you should not, or it would be better not to go to this section yet, because you are missing prerequisites. So this is one way how the system adapts. It's called the link annotation. And then there's a second way, a second type of adaptation here called adaptive curriculum sequencing. Throughout, there will always be these links here: "suggested pages". In the beginning, but also at the end of each page, there will be this link. And so this doesn't follow - does not necessarily follow the structure of the table of contents but will guide you through the course towards a concept or a page that would be the perfect place to start learning now. And so this is a couple of years ago, but at the time we were wondering: how can we actually evaluate this? How can we - the system was running we had thousands of users on this and and similar courses and we were wondering: what can we do? How can we find out, what's wrong? How can we improve it? So I'm asking for your suggestions: what would you do? What could we do to evaluate the system? Just raise your hand and then you can make a suggestion: what can we do to evaluate the system? [waiting for answers] Anybody? You can also type in the chat, by the way. [summary]  There is one in the chat saying: the mostly naive idea would have been a control version. Indeed. And this is what Alex mentioned earlier. And yes, we can do that and it's, I think, at first glance, this would be the obvious thing to do. Why not just having an adaptive version and an unadaptive version. We just switch off the adaptivity. We don't do curriculum sequencing and link annotation and then we check what happens. How do people differ in their behavior, in what they learn? And we did that. But well, there are some problems with that. Imagine, you do such a study. You compare adaptive and non-adaptive version and it turns out: they learn about the same. There's no obvious improvement then in learning. Then you wonder: well, great, but what's wrong? From such a comparison we don't really learn what to improve. You learn what the difference is in the outcome. But an adaptive system is really such a complex system with lots of dependencies, lots of assumptions in it. We need to actually dig deeper and try to find out what's going wrong. And this is where  Layered Evaluation comes in. The idea of this Layered Evaluation is to actually look at the different processes involved, look into the components of the system and to evaluate those separately. In order to learn what's going wrong and then in order to find out, how the system works overall and what to improve. This goes back to ideas back even almost 20 years by now and it took a while to fully understand what's happening and how to go about it. But I think, it's a rather well established concept by now. So this is what this section is about. This next half'n hour I'll try to introduce those layers. Those components that should be evaluated separately. And I'll mention criteria that would go with these layers. Different criteria at each layer to use. We also need to think about methods. What method can be used to collect the data. This will be done later on by Judith. For now we are just looking at the layers themselves and associated criteria. Here is an overview of these layers. An adaptive system is, I think even by definition, an interactive system and would have some kind of front-end. Front-end is not necessarily a graphical user interface, but some way of interacting with the system. Could be a display, could be speech-based or whatever interaction you might have with your adaptive system. And I'm not sure whether you can see this already know, but the idea now is that there is some information processing. Information processing steps, starting here from input data going through user modeling or modeling the world and adaptation which in the end changes or adapts the front-end again. So we are starting here with some input data. This could be all kinds of sensor data, could be observations by the system, you could observe what the user is doing, we could track the mouse or clicks, we could have observation from webcam, we could have context data from, whatever, your SmartWatch or  we might know about room temperature, whatever. Or we might have direct input from the user. The user answers some questions, completes a questionnaire or like in HTML Tutor there might be answers to quiz questions. We are collecting some data, we interpret these data. I tell you more about this later on. We try to give meaning to the data. From that we actually model the word. More than often than not we are modeling the user, but we might also model the context or the interaction in some way. From that we decide how to adapt to that current state. And we would also apply then the adaptation. So that in the end then the interface changes in some way and the user is exposed to a new version of this system. Now here in the middle this might not be as obvious. Well, this is probably the heart of the system. The user model itself but there might be other dynamic models for example a context model. We might log the interaction history, there could be other models depending on the type of system you are working on. Other dynamic models. On top of that, we usually also have some static models. So we might have an idea of what the current task is. How the domain works. We might have assumptions about what we need to achieve what we what we want to achieve at a certain point in time. Also, it's called adaptive theory here. We have, sometimes more explicit and sometimes it's more implicit, but we have an idea of how to adapt. I mean this is what the system is all about. The idea is that one site size fits all doesn't really work. So we need to adapt. We need to change the system in some way. And the adaptive theory captures this.  So we have an idea of how to adapt to a certain user, to user types or models.  So let's go through this step by step. I think I don't need to say much more but the models here in the middle (that's the same as in the middle here) of the graphic. So we have static and dynamic models. As well as then this adaptive theory. Feel free to ask questions if that doesn't really fit with your system, with the system you are currently working on. We might be able to explain how it matches. And you see these dots here. That means, we are open to extensions. Maybe the models, the example models mentioned here, don't really fit all types of systems. But these would be very typical models involved. So the first layer, we call this collection of input data, this is really about the very first information processing step, about the data collection. We need to be sure that the data collected is actually accurate, is correct, is also complete. So accuracy is the main criterion at this stage.  Often enough we just take input data for granted, but actually it's not! Youu really need to check that the data you're working with, you're working on, that is used then for modeling, that this data is correct, are correct. So for example I have done a bit of work on these waerables like SmartWatches and wristbands and stuff. And then very quickly it turns out the data is not as reliable as you might think. Maybe the user is not wearing the watch at a certain time. Or the heart rate: the SmartWatch might read out a heart rate, but often enough that's just very inaccurate. And if you take the data for granted then all the modeling efforts afterwards might go wrong. In case of this HTML Tutor we checked whether the items, the quiz items, are actually reliable. Reliable means, in the diagnostic way now, reliable means, would the user give the same answer later on again? How reliable is each and every answer actually. I know that many of you are working on recommender systems. Tink about a movie recommender. In recommender systems, like movie recommender systems, often we would use ratings from other movies. Are we actually sure that this data is consistent? Are we actually sure that the data is correct? For example, maybe even the data collection mechanism as such isn't really working. There might be some widgets that give you wrong, that collect incorrect answers. But it might also mean that the user is just too lazy  to provide answers and then you have very incomplete data. Or think about a location-based system. In that case often enough we have issues with say GPS or other location measures. They're not as accurate as we think. And then you might draw a wrong -, afterwards you might draw wrong conclusions because you rely on these coordinates. So message for this first layer: check your input data, be sure that it is correct. Or at least be aware of the gaps and know where issues are. So that you can consider these in further processing steps. And this is actually how it works then for all the following layers. We started here with the first layer, which is basically prerequisite for the following layers and so you want to make sure that the output of that layer is correct or at least is as good as we assume then for the next layer, and this will be true for the next, all the layers then later on: one layer provides the output / the input for the next layer. So we want to check that each layer is actually working. So let's go to the next layer: interpretation of collected data. We have checked that the data is accurate. But that doesn't really mean that we can interpret it the way that we wanted to interpret. Well, sometimes  it's just trivial. So if the user clicks on the next button that probably actually means that he or she wants to proceed to the next page.  But it is not as obvious in all cases. Sometimes we give - well or we put more interpretation to it. Here are again some examples. Sometimes we assume that a page that - like in HTML Tutor - a page that has been visited, we take this as as "known", because the user has been there. But it's not, we can't take that for granted. Even if test items, quiz items on the page, have been answered correctly, does it really mean that this page is known? Or take the movie recommender again: same thing, just because the user gives a positive rating does it really mean that he or she likes the movie? Or also in the other direction: if the user gives a bad rating that might be just because at this very point he or she is in a bad mood or, whatever, on that day he had a fight with his girlfriend or whatever put him in a bad mood. And so this is the reason for the bad rating. But two days later he might have given a much more positive rating. So we did this with HTML Tutor system as well. And we compared what was labelled has "visited" and "known" in the system. We compared this to real data, as in, we actually gave an external knowledge test, a post-test  like an exam, and we tried to find out whether those assumptions were correct. And in most, I'll show you some data later on, in most cases that actually worked well enough. Next layer, next stage, next processing step. So we have meaningful input data now. This is now where the real modeling happens, the user modeling. In most cases this is really then the AI, where the AI is involved. So we draw inferences about what we think are the user's goals, the user's characteristics, preferences, whatever. We try to model this. This can be a simple rule based approach, this could be  really very complex inferences, depending on the type of system, depending on the type of domain. And again, we want to find out does that model now, the inferences, do they really reflect the real state of the world. So we can check the validity of that system or of those assumptions. Are they correct? But as you can see here, there might be additional criteria that are not as obvious. So we can look, for example, at predictability. So is the user actually able to see what will happen or how the inferences were drawn. This can be kind of quite confusing. You all know these examples from Amazon, where suddenly weired product recommendations come up, that have nothing to do with your real preferences, they are just there because you bought this present for someone else or because some strange inferences were happening in the background. Another criterion that is also not as obvious but could be important in some systems is scrutability,  open user modeling. Finding out about these assumptions and maybe even being able to change those. For some systems that's easy. You can just list all the preferences, list all the assumptions. But in others, there's some complex model behind. Or even a neural network or something that can't be easily explained. And then scrutability may become a problem. We are also suggesting some secondary criteria here. Comprehensiveness for example. That may be not as important as, say, validity. But if you miss certain parts, miss out on certain parts completely, then this may be a problem. At the same time, you would want a very concise (or would be better to have a concise) user model. that is not too verbose, not too extensive. You want it to be precise. Also sensitivity is an interesting one. Think about the cold start problem.  How quickly would the system adapt to changing preferences, to changing goals? We actually did this for the HTML Tutor as well. For example, we taught people, we gave them lessons and then exposed them to the system. Or: we exposed the system to the users and checked. The users (the learners) had to learn something in between and we checked how quickly the system would find out and would adapt again to the new situation. So sensitivity in some domains can be very important so that you find out quickly what the situation is. As I said, what we did for the HTML Tutor is that we compared concepts (or here they are called chapters, sections of the system) where the user model said this is "known" or no it is "not known" yet. And we compared this to a real post-test exam that learners were taking separately. So we were pretty sure whether the concept was "known" or was "not known". And then we compared whether this actually is in line with what the system thinks what the state of the world would be.  As you can see here, for most sections that worked out well. There were some sections or some incongruent concepts. And then, once you know about those then you can dig deeper and try to find out what's going wrong. Is there a certain quiz item that doesn't work as well, that is not as exact as you want it to be.  For a recommender system, again, you could say: compare the inferred preferences to the real preferences. Also for recommender systems, I want to note that often enough we see the checking of current state of the world is done through RMSE or through precision and recall. As if this was the one and only criterion that you can use here for accuracy. But in fact, depending on the recommendation task, there may be other measures that may be important.  In this publication here, you will find some alternatives that could be used for example prediction at K. Not sure whether that gets across now. The recommendation tasks might be that you have to recommend X items. Five items, ten items, maybe only one. Then it would be important that the best items are actually among those as opposed to just finding out (with precision and recall) whether you cover all of them in some way. I think, so far, it is obvious enough. But the last two layers are a bit more tricky now. Because my experience is that people, first of all, are often enough, they are not aware of this difference and they cannot easily put them apart. But in fact, I think it is actually a very important distinction. We start with "decide upon adaptation". We found out what the state of the world is. We found out these are the user's goals, these are her preferences, this is what she likes, these are the items that she likes, this is the user model. But that does not necessarily mean that we know how to adapt to these. Sometimes this is called the so called "efferent step" in adaptive systems. So what can we, how can we actually adapt to that user model. Because it may not be obvious that it's actually necessary to intervene. Even if it is obvious, even if it is obvious that we want to intervene, it is not quite clear when to intervene. Say, for example, the movie recommender again. So we found out this would be the perfect movie to watch. But maybe the user is right now watching a movie. Should we actually intervene now and like, in the middle of the movie, come up with this splash screen and say: "look we found out this is the best movie!  Watch it now!" Probably not. Even if we know about the user we still need to decide when and how to adapt to the state. Here are some criteria that could be applied at this layer. So I was just talking about necessity. This is an obvious one. And probably also the most important one here. But even if, is it appropriate at all to intervene? At what time is it appropriate? Would the user accept such an intervention, a certain adaptation? Can we explain why we came up with this suggestion and would this lead to substantial acceptance? We might have an idea of all the recommended items, the recommended products, recommended movies at a certain point in time. But then, from those, you might want to select certain ones that are, say, more diverse, that help improve the breadth of the experience. In other cases we just keep it to those that are just the best,  most suitable items of all. That applies also of course to the HTML Tutor again. We know what chapter the learner should look at next. But does it really mean that we should recommend it? Should we put the lable "go to this page". Well, we could even force them to go to if we are really sure what the best section in our course is. We could actually force them to go there. But more likely we just want to recommend it in some way.  The same for the movie recommender. I actually used that example just before, I think. Should we actually push a recommendation? Should we actively recommend that movie? Or should we just wait for the next opportunity when the user asks us for a recommendation. Then we pull out our ideas and then we show what we would recommend, but only on request. This is, I think, really an important distinction here. I hope you can see this now. Modeling is really not the same as then adapting. Even if, in some systems things are a bit mixed up, these are different decisions. Now it gets even more weird! We have decided how to adapt or what to adapt. This is actually the more exact term: we have decided what to adapt, in what way to adapt. But even then there is usually different ways of HOW you can adapt. How you can implement this adaptation decision. This is also a distinction that may not be as obvious to everyone at first glance. I think that's actually an important one. This mainly refers to the user interface. Basically, the user experience or usability component or aspect of it. Say, in the example of the HTML Tutor. We have decided: yes we want to provide link annotation and we saw in our particular case we use this traffic light metaphor. We had green, yellow and red buttons/bullets that would in some way try to convene this idea of what is recommended and what is not recommended. But it's not obvious that red, green and yellow buttons/bullets are the best way to implement this idea of link annotation. Maybe we should use blue, green and purple. Probably not. But there could be different ideas. Maybe we have not only different colors but also different shapes. Maybe want to change the color of the link or the concept rather than just having bullets there. Maybe we want to really point this out very obviously. There could be different ways of how to implement the same adaptation decision and again this will have an effect on the total behavior, on the reaction of users.  If you messed that one up then the whole thing might go down. So even if we have decided what adaptation to take, we still need to work out what would be the best way to implement, to instantiate this particular adaptation decision.  We have criteria here like usability, of course. Do people understand what it means? Are they aware of it? Is their attention drawn to it? But at the same time, maybe it is obtrusive. Maybe we distract people by pointing out what we want them to see. Or they get confused. Is it acceptable from the users' point of view? Is it the right point in time? Do they have control over how it looks like? How the system shows the adaptation. As we had before, also at this stage predictability, as in, does the user understand what will happen or why certain adaptations happened.  This may be checked here. With the HTML Tutor, we were working on some examples of that. We tried to change the formulation of that. As I told you, originally it said: "continue at the next suggested page" and then you can make a more strong formulation. Something like: "we know that, so we think that you're aware of this and that and of certain prerequisites and thus we now highly recommend that you go to this next page". "Example and studies have shown that people who follow the recommendations will actually be quicker and learn the same". You can try to actually convince or have a very strong message there. Or it could be more subtle and you just recommend the suggested page. Same for the recommender systems, the movie recommender. All types of implementations are possible. You could show just one  recommendation, the best-fitting movie, initially, and let the user ask for more.  Or you show all of them, there is a long list and users can scroll through the list. You need to decide what would be, or you need to find out what the best way is. It's probably a good idea to run several versions and to compare them in some way. To be sure that we are not in this local optimum but we actually reach the global optimum of what we can do with the adaptation.  Stephan, may I interrupt you, because I think there is a nice question that needs to be answered also live. What exactly does breadth of experience refer to in this context?  Very similar to what we had before, so in particular for recommender systems, you could go for diversity. Rather than just always recommending the best fitting item, best fitting product, best fitting movie, you can try to actually broaden the experience and show items that may not fit as well but that would get you out of this personalization bubble. Because in the end that might then mean that once you have experience with, you have seen other items say movies from a genre that you haven't watched before, then you might like those and this opens up a complete new area. You get out of of the bubble in that way. This might be a criterion. Now on the interface level here, you might want to find ways of how to get this point across. "See, this this may not be the best suggestion, but we think this is an interesting suggestion. Try this out." This could be an idea. Actually I don't see, I'm not sure how I can see the chat. You're watching it, and you tell me if there's something else happening. We have gone now through these information processing steps and now we have actually adapted the interface. Beyond that, there are some criteria that apply to, well we labeled it as "that apply to all layers". Maybe not to each and every one but something that really cuts across the layers and does not really make sense to stick or to put it only at the level of one layer. One is privacy. Of course, we want to design for privacy. Even more so currently as as before. It's still probably something that is looked over in too many adaptive systems or user modeling systems. Let's try to achieve high privacy for users despite the adaptation and despite all this data collection that we do. In all the information processing steps we should/need to consider how to improve privacy or how to for example and throw away information that we don't want to keep, that we should not keep.  Transparency: while we had the scrutability mentioned in some of the layers, transparency is related to that, as in, can users see what's happening, can they, do they have expectations of what would happen? If I rate this movie high, what will the consequences be? Also related to transparency then even: controllability might be important. If things go wrong, can we stop it? Can we change the assumptions of the system about me? Things that are in the user model or in the context model, can we actively change that in order to achieve a certain state? Privacy, transparency and controlability probably need to be considered at all the stages. Once we are through and once we are sure enough that the layers work and that input and output of the different layers are of good quality. Now we can look at the system as a whole and then actually try to demonstrate the advantage overall, advantage of the system or the benefits of the systems overall. Often this is about effectiveness or efficiency. Sometimes it is "just" (if I may call it that way) "just" about the usability. It is easier to use (or more straightforward to use) if we adapt. And more often than not we have system specific criteria that are specific for this domain or specific for the task of the system. So we can't really list all of these here. For example, for the HTML Tutor, we did that. We had this overall study, as discussed in the very beginning of this lesson.  We actually got users to use the system. And some users were taking a pretest at least for some of the chapters. And then we compared  what they were doing. 140 learners. We could demonstrate that in the end (again this is an external separat test) in the end they had the same knowledge. Some were even better, but statistically the same level of knowledge afterwards. But those who took the pretest and were guided through the system, those were much quicker. They achieved the same level of knowledge in lesser time. Well, for a movie recommender then you might actually try to compare or to check: do people actually like those recommendations? How do they react to the recommendations? Rather than just looking at accuracy. Really try and maybe check the prediction but actually checking: do they like this type of recommendation? Would they follow the recommendation? Maybe not even just the recommendation, but do they like the movie that is recommended? And if they accept the recommendation (that might be actually the ultimate goal). So I have one minute left here to go through this last example. We do this really quickly. So you have seen this picture before. Now let's apply this to a health recommender sytem. I think this is something that Judith was working on. We might reuse that example later: personalized rewards for walking. Trying to encourage more activity among users. In that case the system is tracking the activity. We had a pedometer. We were able to measure the steps and also users had to answer some questions on their personality. We try to interpret this in the sense of what are their main personality characteristics, their scores on this personality questionnaire. But also, does it really reflect how much they walked in a day. Then, as I said, this is not obvious, the data is not always accurate. We need to find out, how accurate it is. Once we have that, then we can draw conclusions, as in, what is the user's performance. This week, have they met their goals? How does it interact with their personality? Based on that then we can reward so-called status rewards. We can praise them or warn them, whatever they have done in the week. But again, it's not as obvious. Let me point out this difference again. This is just about deciding what to reward or  what to tell them. And the next step applying adaptation means, how to do that. What would be the best way actually to show it or what formulations to use.  So taking this all together, this is the very last slide in this section. Layered evaluation breaks down this process, the adaptation process, into different constituents, different components. We recommend to evaluate those separately. It is a suggestion in the end. Not each and every layer may be as relevant for your particular system that you want to evaluate. But I think, it's a very good way of pointing out, where interpretation problems could arise and what problems may arise if you do a simple study on, say, even just adaptive, like the famous adaptive versus non adaptive version. What could be issues in interpreting the results? I'll cut this short for now. Give me one more minute. Later on, we will have this hands-on session. We will break into or create working groups. And we will give you two example systems. We have two examples, we brought two examples of systems. We want you to apply this idea of layering. Apply this to the system and discuss what criteria to use, what to evaluate at all and what criteria to use at each stage. Okay. Let's stop here. h For those who want to use the time of the break that is now starting, we have shared files in slack and in this window, which you might want to download and have a first look at. But let's give everyone a few minutes to breathe and we will be back at ten past. All right! Is everybody back? Not everyone yet. I think it's better, we go ahead now. I'll briefly introduce the idea of this, what we call hands-on session. As I said, the idea is that you learn how to (or get more experience in how to) apply these different layers and which criteria and which methods to use for these layers. Now, in the first part, this is actually about identifying the layers and deciding what needs to be evaluated. We give you examples of systems. You have seen in the chat these leaflets (or a leaflet with two systems in it). There are actually two of those. I'll skip forward here. There is an adaptive supermarket. As you might expect, this is an online supermarket. We watch what the user is buying and browsing, what he or she is searching for, what they are purchasing. And based on that, initially, we may just do some classification. But later on, when we have more knowledge about the user, then we might start with collaborative filtering and recommend items. This is the supermarket. There is a second system called Musify, a completely invented system. This is an adaptive music player and again we get feedback about what the users doing, how they like certain music. We recommend based on collaborative filtering and machine learning algorithms. We recommend what to do. These are the two systems. The idea is that we assign you into these breakout rooms, in groups of about four each. Would be good if one of you could open the template (one of the documents is called "template"), where you can fill in solutions or ideas that you are discussing. First of all you probably need to read the system descriptions in the leaflets and then actually discuss how the layers apply here. So this is the goal of the first session: trying to find out what actually needs to be evaluated? And in particular then, which criteria could be used? It is probably good if you don't even try, in the short time, if we don't even try to apply all the layers, but maybe focus on one of those and try to work this out in more detail. At this point we are not looking at how we can do that and which methods to use, but we need first, which criteria to use for a particular layer. This will take about, well, it's only 15 minutes.  Then, we come back and hear more about the methods. All right? I think you will be assigned automatically now into those groups and we will pop up in the groups and try to support you when discussing the system. Alex and Judith, did I forget any important information? I just saw the question: "is it possible to have a summary for each layer?" I think, what we will try to do, we will pop up in the break out rooms, that will be created (can you leave it just now?). We will prepare it very quickly, if you guys just don't mind taking a screenshot at the moment just to make it faster. And wewill find here something and send it also in the chat and on slack as well. For the process, once again, everyone will be assigned to the breakout rooms. Stephan, Judith and myself will pop up in the groups to try and help, especially if you have questions or need some input on how to proceed. See you all soon! Welcome back in the main session where my presentation will start as soon as all the other people have discovered how to get back to here as well. Are all groups back now Doris or? Everyone is back. Everyone is back, good. Let's get started. I'll share my screen. Okay so in this session we're going to talk about the evaluation methods, so how you can actually evaluate the individual layers once you've identified which ones we want to evaluate and on what you would like to evaluate them. And this will be a combination of some older methods but also methods which have been adapted specifically to adaptive systems. Different methods exist depending on when the evaluation is done, how it is done, and by whom it is done. So, partially this was already mentioned by the others: if you are doing an evaluation, in principle we would like you to do the evaluation at every single stage of development. So not just at the end, but also towards the beginning. In this particular presentation I'll distinguish three different development stages: at the beginning when you're trying to decide on the specification of the system, so you are still trying to figure out what the algorithm should be, for example. Then at the design stage, when we have either got the user interfaces or maybe the algorithms designed for your system and you want to know how good they are, how you can improve them. And the third phase, is implementation, when you already have a prototype implemented of your layer. To evaluate your layer, there are different ways you actually can do this. But for all of these you need to know what input the layer receives and what output it produces, so that's specific for layered evaluation. So there are two ways in which you can provide the input it receives. Either you give it to the person evaluating the layer, so I'm using a little present here to kind of indicate giving it to the participant, or you let the participant decide what the input is going to be, and I'll show you examples later for how this can be done. Now input could be input over a longer period of time because for example you might have tracked people's interests over a longer period of time, so the input could be a user model which has been built up over a long period, for you then to evaluate the Decide on Adaptation layer for example. Then we also need to know as an evaluator what output the layer produces and again you can either show this to the evaluator or let them produce it. But to show the output might require some additional effort, as not all the layers have graphical user interfaces. And also output might sometimes be hard to distinguish; if you are looking at the Apply Adaptation layer and the Decide on Adaptation layer, sometimes it is quite difficult to separate them properly because you might be looking at not just what it has decided to show you, you know, what recommendations it is making to you but also how it's actually showing it to you and again we will come back to this in the different methods. Now to evaluate the quality of a layer, you can either get people's opinions so analyze the strenghts and weaknesses of the layer or you could compare it against given criteria, heuristics, or a gold standard, or you could look at task performance. Who does the evaluation? Now the most realistic one is probably using the end users of the system, as they are after all the ones that are going to use the system. However, very often with adaptive systems and layered evaluation you might also have to use experts. You might need those because it might be very difficult for a user to properly evaluate an individual layer for example if the input or output is a Bayesian net a normal user might not be able to understand and you may need an expert. And also experts are needed because in some methods, when we for example use criteria, the experts might understand the criteria better. A third way of doing this is by using simulated users and that might be done because it requires less resources than real users but can of course not replace real users completely. We will come back to simulated users as a method later on. This then leads to a set of methods which can be distinguished between which phase they are used for, that is what the little things indicate, whether it is specification, or design, how the input is provided, given or provided by the participants, and how the output is provided and also who does the evaluation: normal people, experts for example, how it's done, and in which layer it's done. Now you don't need to understand this table yet. I hope that by the end of this presentation you'll be able to understand this kind of table. There is actually another set of these methods and that is used in the implementation phase. There are more methods out there. These are just done to give you an example of the kind of methods you can apply, but of course, whenever I say focus group I could have said interviews or diary studies or some of the others. But this gives you examples of how it is done. So to start with the methods for the specification phase. There are three methods I'm going to discuss, and the first two are pretty good when it's a task humans are good at. The latter one, the Data Mining one which is used a lot for example in recommender systems work, is only applicable if actually a gold standard is obtainable and we'll come back to that when I discuss the method. So, starting with focus groups. So if we are in the specification phase, we might want to understand what the layer's performance will be like. So, traditionally in focus groups you tend to talk to people about the problem and ask their opinions. For example, talk to them about the design of a user interface and asking their opinions about it. In this kind of layered evaluations, it's more about the layer's performance. But this means that we have to show them the input into the layer in a user understandable way. And as I said before that might mean that I might have to use experts in the focus groups if the input is a Bayesian network model. Or sometimes with some layers I can actually just use normal people. Now as adaptation takes time and is dynamic, input might have been achieved over a longer period of time as already said. So, just to give you some examples. Now suppose we are looking at the Modeling layer of an intelligent tutoring system and the learner has answered a certain percentage of questions correctly in the last two tests and maybe the learner is leaning forwards. The question then is to the focus group: What do you think the learners' emotional state is at this particular moment in time? What do you think? I think most people might start saying things like the learner seems to be engaged with the learning, the learner is doing okay, is probably feeling quite happy as they are doing fine. You know this kind of discussion could've been done in a focus group and this might then inspire the specification of how you are going to design your Modeling layer. To give you another example. This is an example from focus groups we have been running for package recommendation. So, in this particular case we're trying to recommend pairs of items to people, in this case a shirt and a pair of trousers for example, and we gave the focus group as input the ratings of individual people for t-shirts and bottoms and we asked them to what extent do you think this user's gonna like these particular combinations and why. And from the focus group you get all kinds of insights into what attributes people actually are using to come to these decisions. In addition to the ratings for the individual items they are also using things like long sleeves should not go together with short trousers and things like that. Another example is the Apply Adaptation layer of a recommender system. Now suppose as input the decision to emphasize the football news and de-emphasize the cricket news, which came from the previous layer. The question to the focus group could be how do you think this emphasizing, de-emphasizing should be done? Should we actually just remove cricket news, should we put it in a smaller font, should we put more stars next to the football news, and people can have a discussion about this in a focus group. In summary, focus groups are quite good because they can be done very early in the process and can discuss events which are happening over a long time span. The limitation, of course, is it is just subjective opinions; this is what people think might work well and people are not always right in what may work well for them. It demands on a good moderator and of course you can only cover a couple of topics because otherwise it takes too long. The second method which you can use in the specification phase and also in the design phase of the algorithm when you already have an algorithm design is called User as Wizard. In the User as Wizards method what is special is that it is actually the participants who are producing the output of the layer, so therefore in the out thing we have a question mark, the participants are producing the output of the layer. Participants could be either end users or could be experts. And what people do is, the participants are taking the role of the system so in this case the role of the layer.  So, they're given the input just like the layer would have been given and perform the task of the layer. Whilst they perform a layer's tasks we could use the same observational methods as in task based experiments, for example co-discovery, thinking-aloud, these kinds of things you could use or you could interview people afterwards of course, about why they were making the decisions, the way they were doing it. And then what you then can do as a second step is compare the performance of the participants with the design of the system's algorithms if you already have a design. This method is particularly useful for Modeling layers, Decide Adaptation layers, Apply Adaptation layers,  so the higher layers in your system. Though you might also be able to do it for some lower layers if you think very carefully about how to do it. So to give some example. This is an example of a Decide on Adaptation layer of a group recommender system. So in this case we have a user model given to us which tells us that Peter really likes item A, Mary likes item A, and Jane really dislikes it with a rating of one and the system is supposed to now figure out what items to show to this group of users, for example if they have time to watch five clips, which five should it be. And in this particular study we gave participants exactly this task, so we asked them what would you recommend to the group, the task of the Decide on Adaptation layer, if they had time to watch one thing, two things, three things, four things, five things, and why are you making this recommendation. And then we compared what people do with what different algorithms in the literature did regarding the combination of tastes of different people to make these group recommendations. Another example: a Modeling layer of a persuasive system. So suppose we have a set of arguments about, in this case, nuclear power. We know already how strong these arguments are, maybe we  verified that in another experiment. We could ask people suppose you know a person is neutral on nuclear power, what will his position be after hearing this particular argument. So, here we are trying to figure out how the user modeling should work. Some new data is coming in; how is this going to change the position of this particular person. Or the Decide on Adaptation layer of a persuasive system. In this particular study we gave people information about the context: like you're going to a local quiz night with your friends. You're trying to persuade another person to join you. this other person is friends with one of the people who is already going with you and they share a hobby with a third person who is going with you. How are you gonna persuade this person? What are you gonna say? So you basically have given them the user model and the context model, so you know about the friendships between people, you know about the shared interests, and you are asked to decide which persuasive strategy you are going to use. This one is a Decide Adaptation, Apply Adaptation layer of an intelligent tutoring system, so a combination of the two a little bit. We give the participants stories about ehm, we tell them basically about the person they are doing this for. In this case we have a student who is low in emotional stability as expressed by the story given to participants to read. We talk about the performance of this person: he just failed and we ask the participants what emotional support would you provide. So in this case the Decide on Adaptation is how do you adapt emotional support to in this case the personality and performance of the learner. And I say the combination of DA and AA because of course in this case you can also see the words you're going to use, so that's also expressing it already. Now as an alternative of the second step in the User as Wizard where you are comparing it with the algorithms which already exist, you can also do something which resembles a Turing test. So what you could do is you show participants output provided by experts, by both the participants of the User as Wizard results or by the system algorithms and you get the experts to judge the output without knowing  who has produced what and then you can see whether the output produced by your algorithms is just as good, rated just as highly by the experts, as the one which was produced by the participants. So, are your algorithms just as good as humans. And we did this for example with an automatically generated hierarchy so this is an intelligent user interface kind of topic, and we showed people hierarchies which were produced by system algorithms and hierarchies which were produced by humans and we asked them about how understandable these things were and all kind of other things. Or in a group recommender system, we basically had algorithms to decide what we should show to a set of people and based on the user model of the individuals which is shown in the matrix at the top. And in this particular case we showed people different sequences like you know you where we first shown item A and then item C and then item E for example and ask them how satisfied do you think everyone will be with this. So, this is to evaluate how well the Decide on Adapatation layer is doing, because those different sequences they were looking at came from different kinds of algorithms we could use, and also from different participants in our User as Wizard study. So, in summary this User as Wizard method is quite good because you can do it before the underlying layers have been implemented, as users are going to take the role of the layer. So you can also do it before the layer itself has been implemented and it might inspire how you actually design your layer. Of course, it means that the layer input has to be understandable to the participant because they take that input and produce the output so if they cannot understand the input then this is not a good method and of course it requires a task the users are actually good at. Now if humans are not good at the task, there's another method that you could potentially use which is called Data Mining. In this method, systems are basically, you know you're basically using machine learning and things and statistical approaches to look at your data and to figure out what inputs should result in which outputs. In order to do this and to figure out how to do this, we need a gold standard so you need data which actually has input and output examples so you can actually learn which inputs should result in which outputs. The gold standard, so the ideal outputs, either they might already exist, there might be existing data sets for example the Movie Lens dataset which has ratings for lots of movies you could potentially use that. Or you can run a special study; for example if you want to model how well people are learning you could potentially get learners to do a test and then you could use that test data as the gold standard of how well they understood things. The method could then try to learn how you could deduct this from what they were looking at. Or you could potentially do it indirectly. What do I mean by indirectly in this case? Well for example suppose we were deciding on the emotions of a learner by looking at their facial expressions then we could have experts of human emotions, psychologists, looking at videos of the learners and telling us what they think the emotions are and we can use that as the gold standard. And this is an example of one of these studies where they did something like this; they were trying to deduct emotions from dialog features and but they also measured learners' emotions using videos and had three judges but also paid other learners to judge then what they thought the emotions were. In summary, Data Mining is pretty good at finding patterns in data. The limitation is you need a gold standard and normally it's unsuitable for the higher layers just because you need a gold standard. So it's typically pretty good for the lower layers. So moving on to the design phase now. In the design phase, you can use focus groups again, you could use Cognitive Walkthroughs, Heuristic Evaluations or you could even use a user test if a Wizard-of-Oz technique is used. So starting with the focus groups. So in this particular case you show people the design and ask for opinions. Now in this case the design is the layer's performance so you actually have a discussion about, you know, given this input and this output how well do you think this layer is performing. You might need experts again and again the input could have been achieved over a long period of time just as we discussed for the specification phase. In this case because the output is also provided to the participants rather than produced by the participants, also the output needs to be understandable, obviously to humans. Again this is done mainly for the higher levels so in the User Modeling, Decide on Adaptation, Apply Adaptation as when we were discussing the specification phase. So for example suppose we are working on the modeling layer of an intelligent tutoring system; the input might be interpreted user actions, so in this case how many items they got right on the elements of a test and the output could be the user model of the mastery of, for example, if then statements, when statements, and so maybe I think they now have mastered if then statements really well, a nine out of ten, and I am highly confident as a layer on this aspect and so we could as a focus group discuss whether they agree with those user models which have now been produced. Whether interpreted user actions should indeed result in this kind of user model, you know, like do we agree that the learner is demotivated by now and that we can be not very confident about it but reasonably confident about it. Or if we do this with the Decide on Adaptation layer of a recommender system and we have as input say the interests of users and how confident we are in those interests and the output might be that the system decides to emphasize the football news and de-emphasize cricket and rugby, do we agree with this as a focus group, do you think that's the right decision to have taken at this particular moment in time? Or for the Apply Adaptation layer of a recommender system, we could show screenshots perhaps of how recommendations are explained and then discuss which way is preferable and how to improve. And these are the kinds of studies which have also been done in the past by people. So is this the right way where you kind of say how many people like you rated these movies? What does that tell you, how could you improve this kind of explanation and maybe people might say oh I don't like these bars maybe I might prefer pie charts better, or something like that. Focus Groups, again, early on in the design process,  events happening over a long period of time. The limitations are very similar to the ones we had before. The second method you could use in the design phase is  Cognitive Walkthrough. Now traditionally when Cognitive Walkthroughs are done for usability evaluations, what you do is you get experts, you get them to walk through typical tasks of end-users, and determine at any moment in time,  any step, whether they think a novice user would have difficulty performing them. Now when we look at an adaptive system, we have a problem, because this normal correct action sequence one uses in a Cognitive Walkthrough in this case of course has multiple instances because the system adapts and so for different people the correct actions to take might well be different things. Also you know normally you have user interface elements so this is basically useful only if you actually have a user interface for your layer. So typically you can do this perhaps for the later layers: Apply Adaptation combined with Decide on Adaptation, or the system as a whole. Sometimes you can also do it earlier. For example if I was working on a Modeling layer and there was a graphical user interface for modifying the user model, then I could apply Cognitive Walkthrough to see if the user would be able to change the model to a desired state. Now there are other ways you could do this. Now suppose you don't have a graphical interface for modifying it directly maybe if you had a graphical interface for the layers below, and a correct action sequence can still be made somehow for the algorithm of the Modeling layer then you might still be able to do it but it becomes a lot more complicated. So, typically it's better to do it if you have an interface for the layer itself and a task  a correct action sequence can be made for the task. So the nice thing is the strong task focus and it can be done early in the design process, but it's very heavy GUI related and only looks at learnability - the first time people do things and therefore doesn't really cover adaptation in itself but you can still use it for certain layers like the scrutability thing about the modeling I showed you. A method which is more often useful is a Heuristic Evaluation. So in the traditional Heuristic Evaluation you give people criteria, heuristics and they judge user interface elements against those. But actually there's nothing stopping you from doing this even if there are no user interface elements, as long as you have criteria against which you can compare the layer, you can use this one. So what you do is you take experts because it should be people who really understand the criteria, and you give them a layer input and show them the layer output so what the layer does with that and then they judge the layer's performance on a set of heuristics. You can use the general layer criteria as heuristics but you might need to make them a bit more specific per layer. You can also use Anthony Jameson's usability challenges which he wrote about at some point. In this presentation I'll be using the layer criteria and try to make them a little bit more specific. So for example, if you look at the criteria like transparency, comprehensibility, which you can apply to any of the layers, then for the Capture Input Data layer this becomes a question of what does the user actually understand what the system has captured? For the Modeling layer, it could be does the user understand what the system has modeled and why it has modelled that? So does it understand that, you know, if you look at it for a while, the system might say oh you spent five minutes reading this item you know while you were staring at the screen and that the Modeling layer might then decide you actually like cricket, does the user understand this? And also does the user understand which adaptation decisions it has taken and why, like it's no longer showing me the football news, this is because I kept looking at the cricket news. And how the adaptation has happened. Oh look the football news is now in a bigger font, yeah, something like that. And you can do this for all these different criteria. I won't go through all of them in detail. You can read it on the slides later. So for example, for predictability, can you predict, is the user asked to approve major changes? So, there is a couple of these kind of things you can do. For controllibility, can the user influence how the decisions are being made? Can the user undo a user modeling action, so tell the system, after all I'm not interested in the cricket, I was just looking at the screen because I was thinking about something else, or someone interrupted me, I was just drinking a cup of tea? Can I actually tell the system I'm not interested in the cricket? Can the.. Is the timing appropriate for the Decide on Adaptation, Apply Adaptation layers? Is personal data protected? You know, the ones you have interpreted about me, the things you've modeled about me, how do I know that my data will be, is my data safe? Can I actually decide what they think they can capture about me? So depending on the layer you have different questions you might want to look at. For something for breadth of experience, you know we've discussed this earlier in the tutorial, does it actually allow the system, does the system still allow users to access material the system thought is less suitable for them. And the same holds for unobrusiveness, aestetics, appropriateness, necessity, a whole load of  of these with sub-questions depending on the layer. So to give you an example of how such an heuristic evaluation might look like, suppose we want to evaluate the Apply Adaptation layer of an intelligent tutoring system, and you might recognize the thing on the left because that's the HTML tutor Stephan was talking about earlier. The question here is, he was using those traffic lights, the green, yellow red things, to indicate whether users were ready to learn a particular lesson or had already mastered the content of the lesson or should not go into that lesson yet because they weren't ready for it. So if we use those questions we could then ask things like are the symbols we are using clear to the user, are they speaking the user's language as Jakob Nielsen would have said in the old days of Heuristic Evaluation. Does the link annotation make the interface inconsistent? Is it visible to you what the system status is at any moment in time? Can the user get rid of the link annotation and can they change the way the annotations are done? For example, if you are colour blind you don't like red and green because you cannot see the difference,  could you actually make it do something else? So those are the kind of questions you could ask during an heuristic evaluation. In summary, this one is good because it's applicable more widely than just for graphical user interfaces. You can do it early in the design process just like the others. You need to decide on appropriate heuristics and this might also depend on the system you are developing, the domain of the system. And experts are not real users so while you're using experts sort of the apps economists understand the criteria, please keep in mind that that you might also still need to ask the real users because maybe the expert thinks everything is perfectly clear and the real user might not think the same thing. Finally let's move to the implementation phase. In the implementation phase we'll be looking at four different methods: User Testing, Play with Layer, Simulated Users, and Cross Validation. So for a user test, user test is what we used to do a lot of particularly if you were evaluating for example intelligent user interfaces, intelligent tutoring systems or persuasive systems, we like our user tests. And typically what you do is give evaluators well defined tasks to do and then you measure for example how quickly they can do the tasks, how many mistakes they make, and you can observe them. You might use observational methods such as thinking aloud, co-discovery, to kind of understand what people are thinking of whilst they're doing the task. We might interview them afterwards, there might be questionnaires afterwards. And that's still the same if we're doing adaptive systems. However. And in this case you could do things like how fast can they find a book they like, or how fast do they learn, which adaptations do  they like, which confused them. There are a couple of problems here. One problem is adaptation takes time, so it can take a long time to learn about the interests of a user if you are evaluating a recommender system or to figure out how well we understand all these different topics in a tutoring system. So, the solution often is that you do a longitudinal study so, you know, studies over a longer period of time. The other thing of course is if you do a layered evaluation, remember we are trying to evaluate just one layer and not a system as a whole necessarily. You have to make sure that the input to the layer is correct. Yeah, so if you want to evaluate the Decide on Adaptation layer in isolation you have to make sure the user model is correct which goes in. So how do we get this correct user model, how do we make sure that the user model actually is correct before we kind of start evaluating how well the Decide on Adaptation thing works. So what we  typically tend to do is that we focus on the higher layers, we focus on Decide on Adaptation Apply Adaptation for this and then we have the model either given to us by the user so that we know for sure it is correct, the user tells us what the user model is, or we provide the user model to the user and call it an indirect experiment. And this is a better, more easily explained by giving you an example, so that is what I am going to do. To start with, suppose I want to evaluate the transparency of a recommender model, user model. I need a system which works at least up to the modeling layer and the question then is do they understand how the modeling works. For example I could tell them as a task make the system believe you hate cricket and love football. Yes. If they can do that, they understand how the modeling works. If they can directly interact with the model I could also test the scrutability: how easily can they actually change the user model to reflect the model stated. In an indirect experiment, the user performs the task for somebody else rather than for themselves. And we can control the kind of person they do the tasks for, that's how we can give them the user model. So, for example, we did an experiment in which we were trying to investigate how  diverse recommendations should be in a book recommender. And what we did was we gave users the task to buy a book for somebody else, and because it was for somebody else we could tell them exactly what the other person was like, so we could give them a user model about which authors this other person likes, what genres they like, and all those kind of things and then we could see how diverse they made the recommendation set for this other person who had a personality as themselves, that's what we said, and we could measure of course the participants' personality. This means we don't have to wait to build up a correct user model. We don't run any risks that the user model might not be accurate because we just gave it to people. Of course there is a slight problem with this in that it might be less natural for users because they are doing an indirect one and it may make the results a little bit less reliable because you have not tested really on these people but you have done it indirectly. So for example I might ask a user to select a lesson to suit a learner and I can tell them a lot of stuff about that learner, yeah,  and that the system knows these kind of things. So for example I could tell them how well a learner has done on a lot of topics in the past, I can tell them how much effort the learner has put in on studying those topics in the past, I can tell them the personality of the learner, that the learner has low self-esteem for example, and this is actually an experiment we've done. And then I could ask the user to, you know, select a lesson that suits this learner, what lesson do you think this learner should now do? And then I could measure things like how quickly can the user decide this, or you know, is the user making the right decision as judged by teachers perhaps who have seen the lessons, does the user think this is a pleasant way of doing this, do they trust the system and I could ask them to do the co-discovery, so we have two users doing it at the same time, talking to each other, so I can figure out what they're thinking or just ask them why they are doing things in a particular way. Another variant which we often use is a Wizard-of-Oz variant which means that we can test a layer which is not implemented yet. When I spoke about methods to use for design I also said user test at that momement if you used Wizard-as-Oz. Now Wizard-of-Oz is an old-fashioned thing which we invented for example when we wanted to test speech systems in a time when they were not very good yet and so as we would tell the participants that they were speaking to a computer and they would be just speaking at the computer as if the computer had good speech recognition and in reality whatever this person said would just be typed in by someone else so the speech recognition did not have the work yet for people to test whether the dialogue system actually worked, and whether the interactions would work if the speech was recognized accurately. And this is the same here, we could potentially test a layer which has not been implemented yet if the experimenter could do the bit the layer normally does based on the input  by the user. So in summary, it can be quite natural for users to do tasks with systems and with layers because this is what they normally do, it can provide objective performance measures. The limitations of course, the layer has to be implemented or you need some Wizard-of-Oz setup and it requires a task the user understands so it's easier for the system as a whole and may be difficult for lower layers where you have tasks users just don't understand. Users are not perhaps very good at the user modeling bit, but they're very good at the decide on adaptation and those kind of things. Input has to be easy to do and this might mean that sometimes you have to build specific user interfaces to be able to input the input of the lower layers into the system. And so for example when we were doing an experiment for testing explanations in a recommender system we would build a specific thing which would allowe people to tell us which directors  they like, which genres of movies they like, all these kind of things, so we had a correct user model and we could just test the adaptation of the explanations themselves. Now regarding observational methods, I mentioned such as thinking aloud, co-discovery and just looking how people are struggling while doing the tasks. We've all been taught that if you do this in normal interaction studies not to help the user, let them struggle, we're supposed to see the causes of difficulties. A not to ask direct questions of the user, like what do you think this label means and things like that, or guide them, you know that's like, you shouldn't be doing that. However, if you're dealing with an adaptive system sometimes the user may just not even notice the adaptation and you're really interested to know what they think of the adaptation. So you might have to ask them and make a note that you asked them and take that into account at a later stage that maybe it is not visible enough. So for example there was a study once by someone who really wanted to know about scrutability and they build a scrutability tool so people could change the user model and they wanted to know what people thought about it and none of his participants  ever even saw this thing, so they didn't even know that his tool existed. So in that particular case it is okay to tell people look there is a tool here could you click on that for a moment to tell me what you think about it. Yeah see if you can use it. That's fine. Or you have to explicitly put adaptation related activities in the task to make sure that the adaptative bit is not overlooked in some way. Another method you could use is one I've called the Play with Layer method, and basically what that means is you give people the layer and just let them play with it. So instead of giving them very specific tasks, you let it up to the user to decide what they are going to do and then you could potentially judge the layer's behavior against the criteria or use a questionnaire, do interviews afterwards. This can be done for any layer and this might get you some objective measures, how often events occur such as adaptation and things like that as well. So for example I might get users to just look at the screen of a system and the system might be using an eye tracker and I want to see how quickly the system can pick up what I'm looking at. So when I'm looking at one side of the screen will it immediately know that I moved my eyes and figure out that I'm looking at something else. Now I might need an extra GUI element for that, I might need something which shows on the screen what the user is looking at or visualizes the output because then a user can later comment on how accurate it is. Look it didn't figure out that I am looking at this bit. I could also test the Interpret Data layer by afterwards telling the users how interested the system thinks they are in each movie, so suppose there are movies on the screen and they are looking at movies. The system now thinks you are interested in this movie and the user might say: no way I'm just looking at it because you know it has this picture on it which I found really interesting they look interesting than if so you could do those kind of things and in this case you might not need an extra GUI, we could just replay the interaction so that people actually see and can comment on why they were looking at it if it wasn't correct. Another example of a Decide on Adaptation layer of a group recommender. So this was a simulation we set up of a couple of people in a group, and they could walk in and out of a room, and based on the people in the room the music would change. Yeah, so people could just play with this. They could make Jane and Peter and John move in and out of the room. We know individual preferences for music of these people and they could see how the music was changing, what music was playing and then we could have discussions about what they thought about it and whether it makes sense that when Jane moved into the room this happened and how it would feel you feel to be Jane and walk into that room. And this one I already kind of mentioned some examples for the explanations, you could potentially have people just setup their own user model and then see what kind of explanations they get for recommendations and then get them to rate those explanations. So, that's another thing that we could actually do. I mean we did it as a controlled experiment with user tasks but you could also do it as a Play with Layer kind of thing as decribed here. The advantage of Play with Layer is this can be done before the underlying layers have been implemented but you have to have some implementation of the layer itself or you could also use Wizard-of-Oz here potentially. Now, the layer input has to be understandable and producible by participants, so they can't just put Bayesian user models in, so it wouldn't work for that, but they can tell you to what extent they like the cricket and to what extent they dislike the football for example and you can then see what happens with the adaptations and you might need to put a bit of effort into the graphical user interface and produce some extra bits. Simulated users. Quite often in adaptive systems you have very many different kinds of users, that's the whole point of having adaptation after all. And it might be very expensive or difficult to get lots and lots of different kinds of users. So sometimes what we do is we build models of users, which do the actions of the user, to see how our system will react to all these differences and then we test the layer on the simulated users and this can be done  particularly for the lower layers. Because simulated users, we cannot really see how they react to apply adaptation things very well, this is not modeled, but the earlier things you can actually do. So for example for my PhD a long long time ago I implemented all kinds of models of how people learn, which could predict whether people would get, what the chance would be that a learner got a certain item correct or wrong. And then I would use those models to figure out how different strategies for deciding which items to present to a user to learn, would result in how quickly they would end up learning them. Or in a group recommender I built models of the affective state of people's, how they would be feeling after a sequence of items and those models had all kinds of parameters in them and I tested all kind of different parameter values to see, you know, what people's emotions would be if the recommender used one strategy or the other and I can still draw some conclusions about which strategies were better than others. You know, which ones always would make people miserable and which ones tend to be okay if I use certain values of the parameters. The good thing about simulations is you can test a lot of things very quickly. The bad thing is of course that if I build the simulation and it's going to test my algorithms, then the same wrong assumptions which I put into the algorithms might also be there in the simulation. And of course also I am modeling static user behavior and the system is going to change so that's sometimes not that easy. Finally, we also have Cross Validation. So Cross Validation is typically done after doing Data Mining first to determine what the algorithm should be and then we use Cross Validation to see how good the output is you have to use from the Data Mining. You can do this if you have a gold standard, so you know what the real output should have been. You split the data into training data and test data. You do the data mining on the training data, to inform the design. You then use the test data to see how well this algorithm now works on the test data. And if you do this multiple times, you split it and you test, split it test, split it, make the model, and test, you can call it n-fold or k-fold Cross Validation. So, 10-fold Cross Validation is nothing else than doing it 10 times. yeah and it gives you the average results at the findings. This is typically only possible for the lower layers. For example, in this particular case someone is trying to decide on a utility function, something to decide on the best items for a particular user for which we have preference statements. But in order to do this, they needed to know ratings as well as the preference statements, to be able to test it. So you need to have the gold standard otherwise there's no point in doing it. So, this is a particularly good to evaluate the  designs from Data Mining. However, very often they only test accuracy and even if they do accuracy correctly so they also do precision at 10 not just precision, accuracy is not all in the world, yeah, there are other interesting things to know about recommender systems such as whether there is serendipity, breadth of experience and all these kind of things. And another problem is when you report accuracy, don't just say my accuracy has improved a tiny little bit, that's not interesting, also look at in which cases did it not improve and try to understand you know what kind of mistakes it is still making. So now you should understand this table; so here are the methods, when they can be applied, what goes in and out of the layers, who is doing it, how you're doing it, and which layers you can typically apply them for. Though as I said, you know, this is just guidelines. You could potentially apply certain methods also on other layers but you have to think about how to do it. So in summary, task based experiments is only one method for evaluating adaptive systems, just like cross validation is only one method depending on the tradition you come from. Think about all the other methods as well. And evaluate early on, not just at the end. The formative aspects are important, it is not just how good your layer is, but also what is actually causing the problems. And you need quite some skill to evaluate the layers separately; I've shown you some examples of how this can be done. Now what method is best also depends on the type of system and when the evaluation is happening. And you might need to adapt traditional methods to suit the requirements of adaptive systems. There is a schema in the paper which is linked to this tutorial which shows you which method is best in which circumstances. Now to introduce the hands-on session second part. In the previous part you decided which aspects of the system you wanted to evaluate and what the layers were. So you said okay these are the layers and for that layer these are the things I want to know and you probably focused on one layer and said okay for that layer I definitly want to know these kind of things. In this hands-on part, what you're going to do, take that and decide how you are gonna do it, which methods are you gonna apply. So, select an evaluation method and data collection instruments and link this to the output of the first section. So, you should go back into your groups now, the same that you were in before, and if you joined late you will be added to one of the existing groups. And then decide on the evaluation methods to use. And obviously think about how this can actually answer the questions you have and how it links to the criteria. Enjoy your work in the hands-on session Hello everyone group 1 is back I guess. We have everyone back except Alex's group.  Ah, no, there he is! So I think we're all back in the meantime. Let's try to ... ah, an interesting question came up when we were finishing in the group I was in, that people might want to share because of one person was taking notes and I thought that if they like they can put things directly in slack in the main channel, but of course if somebody would rather not do that, they can also share it directly with the people that were in their group. Let me ask quickly: is it possible? Do we have the lists, do we know who was in what group? Excellent! So we can share at least that, and then you can choose how you would like to share the things. Yeah are there any questions or any other points before we go into the last part of the tutorial? Alex, Vania here, maybe I think is helpful because the length of discussion we had about breadth of experience maybe it's helpful just very quickly to summarize what it was. What? The breadth of experience? Sure, this is the same topic that Stefan addressed before: adaptive systems or personalization systems can sometimes very successfully create a bubble around you, so that you only experience the specific slice of the world the system has realized that you'd like, and this can be great, in the case for example of Musify you can listen to music that you really like, but there may be even more types of music that you would love but you are never exposed to. The same is true in Centaur, maybe you use, I don't know, a particular shampoo because we have used it for the past 20 years and you're happy but there's an even better one you never know exists because nobody has told you about it. And this limitation, this bubble, it has become obvious in recent decades that they are actually bad for the users. They create an unrealistic piece of the world to show them and that the concept of making- making it the broader part of the world that the user experiences is very important, serendipity --like letting the user experience new things by chance-- is very important and it is  not just accuracy or the strict metrics that should be applied in the case of recommender systems. I apologize in advance to all the people from the recommender systems domain for whom this description was a bit too simplistic but I hope it was ok for getting a feeling of what is meant. OK so if it is OK I will just take you into the last part of the presentation and do that from here. Here we are. I will finish off with a few points about... I think this is the place... give me a second to find the right place.. here we are. We talked about layered valuation in the first part and the methods that we discussed in the second part were also linked to layer evaluation, so we would like to say a few works- a few words specifically about layered evaluation. The first one is that it is not prescriptive, it doesn't tell you exactly what you have to do, it gives you a way of thinking about the evaluation, a way of considering what are the main challenges that one might have and a possible way to address them. And the the main message there is: the adaptive functionality of the system is not a monolith, it is based on layer upon layer of things that happen behind the scenes and evaluation needs to take that into consideration. A thing, another thing to consider is that layered evaluation needs to be adjusted for particular systems. In several systems it is not possible to evaluate layers in isolation and this is perfectly fine. For each type of system we need to, we need to consider what makes sense and do that. So it is not about really separating every single layer, definitely not about applying every single criterion, but it is more a way of organizing your thoughts about evaluation. Something which is becoming more pertinent in recent years is that meta-adaptivity is not covered by layered evaluation. Meta-adaptivity refers to the capability of a system to adapt the way in which it adapts. So this is definitely something that we do not explicitly cover, this is a very complicated subject and this definitely needs to be considered separately. And as you may have already realized if you're seeing layered evaluation for the first time today, this framework and everything we presented is not a "small" thing so if one tries to really evaluate for all topics, all the layers, in all stages, then the evaluation is going to definitely be more challenging than developing the adaptive system itself in the first place! So it pays off to consider what are really the important things and concentrate on those rather- rather than blindly trying to apply everything, everywhere. We also want to mention that there are alternative approaches and that layered evaluation is not the only possibility one has. There are different ways of looking at adaptivity and there have been suggestions in the past about how to address specifically adaptivity with slightly simpler systems, and there are also approaches which are domain-specific like for example for evaluating user models or learner models or specifically for evaluating recommender systems, and we will share the slides of the tutorial and in the reading list you can find a lot of those approaches listed, and it would be our suggestion that you at least have a look so that you are familiar with what exists out there. The next thing we would like to share with you is some of the pitfalls or some of the things that might go wrong, or usually go wrong, and you should be aware of, when you starting an evaluation. It is human nature to not always learn from the mistakes of others: we typically prefer to make these mistakes ourselves again ourselves, but in the hope that some of the some of the messages here stick with you, we will try to at least enumerate the most important things! One of the take-home messages is what I mentioned before: the evaluation of adaptation needs to happen throughout development, in the piecewise manner, and the main message here is you need to be aware what layer or what constituent of the system you're evaluating, and target data. And the second message is that you need to make sure that you can collect data which can be interpreted in a clear manner and to run pilot tests before you run big tests that are not guaranteed to provide you what you need. So some of the pitfalls, and there's a paper by Stefan where all these things are discussed in great detail, and I would highly recommend to read that, but to go quickly through the list: Do not plan big evaluation studies that simply happen at the end of a project, because they may really go bad and at the end of a project it is already too late. A very similar thing is when no resources are left in a project in order to do the evaluation; the resources can be different types: temporal or human or monetary in some situations. A third point is about the control conditions and this is something we have already discussed. Comparing against the wrong thing. Then a fourth point is insufficient time for adaptation: a lot of systems need to take their time until they have the right model, until the adaptations "converged", something that is useful for the reader - sorry for the user, and you need to make sure in your tests that this is accounted for. And the fifth point is that there is very often too much variance in the data that gets collected, so it is not easy to then decide what you have to do about that. Criterions and their choices: I guess you have had already the experience in the group work and hands-on work today, that this can be less than trivial and choosing the wrong one doesn't help. Judith made a very good point about the visibility of adaptation effects and gave the excellent example of if you don't know that something is being recommended to you, then you don't experience any part of the recommender system. An 8th point .. (laugh) I'm sorry about the background noise, I'm in a place where I cannot control that, I hope it's not too bad for everybody... The eighth point is that the statistical analysis is problematic and the types of analysis that get applied have changed over the years. There's a large debate about what exactly is appropriate at the moment, and in modern statistics, and we encourage everybody to have a closer look at the ongoing discussion. Then we have a problem that was more- more pronounced in the past, but in recent years has become a lot better: that the evaluation results are reported either in a manner that is relatively incomplete, or, unfortunately very often in the past, more anecdotally. But like I said the things are better now. And the tenth and last point here is that other system aspects other than adaptivity may very well interfere with the evaluation studies, and make it difficult to tell if the results that you got really have to do with the adaptation or with things that were uncontrolled variables. So this was the end of the slides, like I said there is a reading list at the end, which you can go through and I think I'm speaking for Stefan and Judith also, that we would be very happy if people got in touch if they have questions or if they would like to discuss things in more detail. And I think I'll now be quiet and let Stefan and Judith also say a few closing words. I don't think- know whether you said so already but we also made some sample solutions for the hands-on sessions for one of the two systems at least. I guess, yes forgot to mention that we have that and we'll be happy to share that yes. Would you like us to do that now Judith, or...  I think we will share things on the slack Channel. I think that will probably be easiest.  OK, it's not linked to this meeting but it will be on the slack. Stephan? Yeah so take-home message: it's complicated! And, well you need to learn or you need to know a few things about evaluation and we're really just trying to share our experience here, so that you- you can learn from that and you won't commit the mistakes that we did in the past. And so that we, as a community, we produce better evaluation results to progress the community. That's not just about producing better systems but also producing better insight into what works what does not work, how to improve it, so that we have better insights in how to do better user modeling of all. Yeah and I guess one final thing, which I already said I think, is when you're a PhD student don't try to conquer the world yeah? there's so many things you could possibly evaluate on a system  and also so many things you could investigate on a system, don't try to build complete systems with bells and whistles which you can't then evaluate. You know it's fine to evaluate a part of a system and focus your research on part of something. And it's absolutely fine to write papers which say you know there's also these other bits which need to be done, but they are someone else's PhD. And it gives a lot of the PhD students when you see them in doctoral consortia- yeah and there's also a doctoral consortia session tomorrow night Europe time... You know, quite often people are trying to do too many things and doing something small good is preferable to doing a lot of things badly. Yeah same here. Are there maybe any questions or any comments from the people, anything you would like to share or ask? The chat has also not... doesn't have anything in it... OK, so to prevent awkward silences with nothing happening in them, I think we can thank everyone for being present, we hope it has been useful and you have heard interesting things. Please keep an eye on the slack channel and like I said before, and I'd like to repeat, we would be happy if anybody wants to get in touch and get additional information or simply say hi after the tutorial. Have a nice evening / afternoon / morning and hope to see you all soon in this or in a next event. Bye everybody! Bye bye! Thank you! Thank you very much! Great tutorial! Bye bye. Bye. Thank you! 