 hi my name is Patrick and together insomnia we work at Burge type 1 we will present today how did we manage to scale our simulations and analytics for the purpose of generating Hyperloop design so let's jump into the agenda of a representation today we will first tell you what are what are we doing at virgin heart 1 and what is our subgroup machine intelligence and analytics doing we will then show you an example of our simulation and what are the design challenges of building a data pipeline for running hundreds of these simulation in the cloud we're gonna show you which where the tools we use short demos of these tools followed by a demo of sample simulation run with our pipeline then will be quickly jump into one of the conclusions that we discovered using the pipeline that is related to demand modeling so we first have the one or a start up we were on 250 employees in downtown Los Angeles and we are building new transportation system that is based on a vacuum tube and small passenger or cargo vehicles that we call pods it offers you a direct emissions because we are using electromagnetic levitation and propulsion and because of that we can provide very short travel times but not only because of that we are also own demand so imagine ordering your Hyperloop ride using the web or phone app we've run hundreds of tests at scale and had a 500 meter test track in about the desert and we are constantly planning and building new tests and enhancement so please follow us to see how Hyperloop is becoming a reality and learn about the future of transportation we are building for you so with this new transportation system people are asking about questions about safety cost what is the real travel time and many many other and our group machine intelligence and analytics is doing operation operation research or Hyperloop we are providing analytics products for our engineers and business teams so that they can show governments partners and investors that Hyperloop is the solution they are looking for for example we can provide a loop with high-speed rail airplanes and other modes of transport we are answering all these questions using simulation and internal and external data for example we could answer the question what is the optimal vehicle capacity for a given geography and how many passengers can be realistically handled in a given scenario and again we answer all these questions using data so our sample data flow would be to gather the man were to on a travel for a given geography then fits route alignment in the land using our 3d alignment optimizer and your spatial data then we schedule the trips for the passengers using the demand and Hyperloop alignments and run the simulation after the simulation we then compute performance and cost metrics and usually we discover something could improve for example maybe we can sleep some more parameters so over time we run more and more simulation from simulations for more and more parameter sleeps and after a few weeks of running we have a bunch of answers to the questions asked so I mentioned running the simulation we call our simulation software syn loop and it's an agent-based high fidelity transportation system simulation that we develop in-house it can simulate the Hyperloop alignment pod which is the vehicle the physical behavior of the pod control systems interacting with the outside world as well as passengers and Hyperloop stations that people portals so let's take a short tour in our simulation software afterwards Tanya will tell you more about the pipeline [Music] hi I'm Sandia so we just now saw the Hyperloop simulation video now I walk you through the flow of the Hyperloop simulation so we first start by capturing the demand and ridership patterns of the existing route in consideration and then we do ML models to predict the future demand and with that demand data we send it into a trip scheduler which will then do the power allocation in departure schedule for this we also have a track optimizer which gets the possible optimized alignment for the route in consideration based on the geography cost we have additional suite parameters as well like say for example allowing weather ridership is coming whether we can write in parallel or a ride-sharing is allowed and those are all are fed into a simulation software which then means the simulation for us which we just saw those simulation results are then analyzed to give us some metrics transferring assistance so imagine doing this again and again for different set of parameters different set of alignments and files and we have to make sure that our analytics platform is able to handle this volume and is focused enough to run this again and again so while we need to be flexible and why do we need to be fast so we need to have accurate answers and we need to have them right now as soon as possible so we need to have it flexible and fast and we have data volume changes occurring based on different alignments and different use cases and also here when we are getting data from multiple engineering teams and different file formats and different processes we have multiple sources and also see we have to add new applications to our systems a pricing model prediction it has to be seamless now and being one of the kind technologies and we have to keep up with the latest of all the technology and tools we may need to migrate between two suits and platforms so we for all these reasons we need our platform to be flexible and fast so if this is not something new to set up such a robust and dynamic platform so a lot of big companies like Google and already shown us the way precisely they like you like adopt the cloud technologies they ensure there is micro service architecture for flexibility they use distributed computing to be fast enough they are also auto scalable to meet the large demands and they use the lambda of architecture - - in order to implement those architectural guidelines we have a little lot of challenges being a start-up so the first challenge is that we have this those constraints specifically with development maintenance we also have limited DevOps support in order to support such a platform and in addition being one of a kind technology we need to have security implementations in place and lastly we need to have traceability and governance in our platform so that the quality of our product is ensured because we are directly leading the passengers so this is our analytics platform next step we have a couple of tools primarily open-source tools which help us with our text n95 is our data flow manager which helps us will much in place the effects of the PMI picture mark a is a data format for columnar it's using it for huge demand and huge growing the amount of data sparked is at compute engine which is open source but we are using the raid objects platform so as to easels with the DevOps and the other limitations and and we use ml flow for our analysis so in all these in addition to all these we also use other like cloud-based technologies like docker Amazon's ec2 and s3 and Kuban accessible these all make a very flexible and nice stable platform so this is our overall architecture of our analytics platform so as you can see on the left hand side we have a couple of funds which come as input specifically say for the velocity profiles or the portal models and vehicle models etc the information and the relationship between these files are persisted on our database and we also have an additional configuration application which our users use for primarily picking and choosing the different files of simulation - one once they hit the run the 95 takes control it does some transformation cleanses the files and runs the simulations on our ec2 engines and paddle and once the simulations are completed the control from specs from 95 and is then given to spout for an analysis job once analysis jobs are complete the results are stored as experiments on ml flow and we use emag flow to analyze the different batch running results and compare them and see what questions we have answered and how much of it is like like givens gives us the information and we have a report so this is all overall flow so now my colleague Patrick he will explain about how we use spark and data breaks in our platform so let me tell you our Hyperloop data story about a year ago our data scientist started to grow from megabytes gigabytes and our processing time started to go from minutes to hours and our Python scripts especially panda scripts we're running out of memory and so we decided we need more enterprise and scalable approach to handing our data we try different solutions and obviously found that spark and his family at the facto standard solution and it's great with so many connectors and tools around so we decided we are going to have a bunch of spark workers and the drivers and Hadoop file system and a stream mounted in that file system this is already a big infrastructure so we already knew we don't have enough dev ops to manage that and this is why we are using data bricks but then we also realized that our previous code was coated in pandas and to use it with spark we would have to use either PI for occurs power and we and particularly me since I would have to do it weren't particularly happy about that since we would have to rewrite all thousands of lines of code in pandas and we rent experts in high spark or Scala and this is where a koalas package comes in exactly when we needed it more or less money one year ago around in our April data breaks open sourced koalas package that is doing exactly what we needed so it's translating our pandas API into PI spark so we can scale our pandas code very easily with with the familiar pandas idea so now let's talk about another part of our system which is called ml flow we use it to log track and analyze our simulation runs and results for those of you who don't know ml flow is an open-source tool that allows you to log machine learning models its parameters and metrics so for example here using this UI if you run a bunch of models and log them you can see the parameters and metrics for all of them and then select the best models then you can deploy it to production using ml flow projects and models on the top right and on the slide but in this presentation we are particularly interested in a node for tracking so let's talk about it more we found a demo called tracking service purpose very well also beyond machinery our simulation runs are also expecting a lot of input parameters that we can sweep and also have a lot of outputs that we score to create numerical metrics instead of developing our own solution to track query and analyze simulation runs we use ml tracking is a generic experiment logging and visualization tool we just treat every simulation as an experiment and log it as such and we found it super convenient and very cost effective we actually saved so much time but not having to develop a simulation tracking tool ourselves and it integrates very well of external tools for api's for example there is an API to query and export experiments to find us data frame that we use a lot and we found out ml flow serves this purpose very well for tracking our simulation and our simulation actually contain a lot of AI hydrants so instead of ml flow we actually call it AI flow now I will let my colleague Sandhya explain nightfly our data integration and pipelining tool so Apache 9 v r9 file for the data flow management and for any trader driven analytics you need to have a very good data flow so it is data agnostic on I Phi also supports transformations and data routing has 250-plus in both continents it also has a mini Phi and an eye-fi registry extensible weather and so for example now say if we wanted J merge to JSON files which have the same file pattern and perform transformations and put them in s3 so first you go and choose the gate file component you give the parameters of the file and the pattern you want to create and then you can use the merge file component much constant component to give the header and the footer information and how you want much the two files you can also in the connector between the two components you can see that you can do prioritizations and buffering mechanisms and in addition for each confident as you can see we can also have scheduling mechanisms like you can put the schedule in them and you can use the Joel transformation component for putting the transformations and add s3 attributes and also for use the profile component along with the AWS credentials as controller services now if I want to run I can pick and choose and run which over confident I want to so I am just run from the first component here and I can see that the files which operate that queued in the in the flow and I will be able to see the different flow content and the flow attributes which are related for that particular file once I confirm I can choose to run the remaining part of the flow as well and after the flow has completed I will be able to see the provenance of the particular whether the file has been put in SV or not and whether it was successful and what was the content of the file which which is good so this is the complete flow so Apache 95 is also good at real-time and in control say for example if you want to set up the concurrency control so for a particular component you want to set up whether the task needs to be confident and how fun for it it needs to be and the execution needs to be running on all nodes etc and the schedule needs to be arranged as well you can also set up whether the queue prioritization is like whether it has to be first in first out and buffer and etc in addition you can also add control services for external connectors like say Postgres or AWS and on top of all this knife I also gives the power to set up the data security using SSO or SSL visiting the most important feature of my fights is power of provenance it gives in volt problems and we need provenance and lineage for efficient traceability in our analytics platform so in this screenshot as you can see it's a screenshot of a put file components provenance even and these are the details of that problem ceiling and the right-hand side you can see a tree like icon when you click it you will be able to see the complete a lineage of that particular province event so each and every circle here is a components problems data and once you click any one of the circles you would be able to see the detailed description of what was that event what was the flow file its byte size and what with attributes of that particular flow file and in addition you'll also be able to see the content like whether like it's the input of output of that flow file or connection and you'll also be able to replay that problems even if you need to check if anything is going wrong this helps with good traceability force we just saw all the different components that are used as a part of our pipeline so we would like you would like to show you a small demo for company pipeline and we would first start by showing you the configurator application which in user uses to pick and choose the files then we will show how the control is transferred to night file and then we will also show how it is simulation of executed and pabulum and how we use ml flow to analyze the results so first we'll start off with the configuration application whether user pick and chooses the files as you can see this is our configurator window you can use the Add button to add a particular batch and pick at the alignment for that particular package and based on the executables available you can select each of executables which only you wanna run and then when you hit V dependency you'll be able to see the dependency between the executables and what are the files needed for it when you accept that you will be able to pick and choose the different files whichever was available for that particular file exits and as you can see you will be able to choose the different files you can also see in the tabs on the top of the tabs some numbers coming up those are the numbers of the different scenarios which are going to be executed as a part of this file choices which you are making so if you choose different set of files the combination needs more scenarios and as you can see you can also sweep different parameters and the more parameters you choose those also contribute to different combinations of scenarios which you wanted so when you hit run you will be able to see the total number of scenarios calculated and when you hit you again you will see that the successful batch has been created so once this batch has been created the control then transpose to 9 so now we will see how Wi-Fi picks up the control seamlessly from the back after the batch is created and how the flow progresses this is a sample flow which we have and once the ninth eye control is passed over here you would be able to see the data flowing through and you can see the provenance of the data which is coming in you'll be able to monitor and view the content of the data for example this is a new content and you will also be able to replay the content in order to see whether if you have any bugs or anything and as you can see here in this screen there are multiple some simulations running on ec2 all talk by nightfall so once it is kicked off you can also check what yielded to that particular execution by checking in the details of that particular flow file contained the attributes and the content and also the best part to see is the provenance lineage you can see that where the specific command came from and what all are the files which yield it for that particular command to execute and how the simulation is executed so this is the cloud flow basically so once the simulation is executed and our analysis metrics job is running on spawn all the results are saved on ml flow as experiments and Patrick will explain about it so thanks Sam yeah now I'll talk about ml flow our metrics listing and analytics platform now so as you can see here this is a no flow window and every row is a simulation run and for every simulation run we have some metrics and rameters and if we click on the row we can drill down into a single simulation run we can again for the simulation see parameters and metrics for that simulation but the cool part is for every simulation we compute a bunch of artifacts like for this simulation for example I created that the demand scatter plot and it's a very interactive here in the window you can explore you can can interact with the interactive cloth lip-lock as well as I created here a histogram of the trip times and so for example here you can explore that between different cities that the trip times are different for passengers but then what if you want to compare a bunch of experiments so to compare a a bunch of simulations we select all of this and click compare and again here we have a big table where each column is a simulation run and we see parameters and metrics but here we also have a way of exploring that data so if we select the sample parameter and sample metrics we can see which simulation is the best and we can also see a contour plot you like freely plot so you can see two parameters and one metrics and here we can see one of these two simulations may be perform to the path so this would be our further exploration so I hope you enjoyed our pipeline run for our simulation and now I would like to share one of the conclusions we drew using this pipeline as I told you before Hyperloop is on man and because it's on demand we found that we can prove provide increased efficiency and passenger convenience but there is one particular challenge when that with the own demand earnest of Hyperloop that I would like to discuss and analyze with you so the problem is we would like to predict areas of high or low demand so that we can redistribute our vehicles ahead of time from the area as low demands to the areas of hajima imagine network like in the picture on the bottom left if there's a high demand in the center we would like to redistribute the vehicles so that they are able to pick up passengers Wow right when they need to and for that purpose we feed the demand prediction model to our assimilation and we train these models using historical demand data we gathered for our analysis and which trained multiple models using in carousel SDM and GRU and actually we trained it using horrible inspark and we also run erima and profit models and and swept the input parameters to these models using mark UDF as distributed sweeps this is a rough example of one of our models and the blue line you can see shows a ground truth for the demand for a given that's origin destination care and the model prediction is shown by a green line and the prediction history is strong with the red line and this model is trying to predict hourly demand for three days in advance for a single origin destination pair so as a conclusion from that analysis the best model we fed into our simulations using using the pipeline was able to improve the number of required vehicles and cost by even up to 70% and and this is a really great finding we also found that sometimes simpler models like ARIMA or prophets sometimes outperform Carris and additionally to improve the models we correlated the weather data and the surrounding events data from predict taste you know with the transportation demand and we definitely found a correlation that provided a great improvement to our demand models as a general conclusion I hope you learned a lot about our Hyperloop project and I hope you can see we're doing really cool stuff and by the way we are still hiring so please check out our careers page our story of data breaks is an amazing partnership but also a very lucky coincidence with koalas and that's a great company we love using their tools we also showed you the design of our system that is running analyzing hundreds of data heavy experiments and how did we achieve it with valid with minimal development effort and we actually coated this platform with two to three people in six months and here I'd like to give a shout out to our front end engineer Justin he's great he really helped us figure all this out and it's actually the integration between the tools that took the most time and the tools that did the job for us we're knife wise Park ml flow and Parque if you want to learn more about Hyperloop is reinventing transportation feel free to reach us on twitter using this handle and i believe now it's the time for questions you 