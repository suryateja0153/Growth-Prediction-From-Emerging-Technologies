 now i would like to welcome today's presenter associate professor professor michael bernstein michael um the associate professor in computers of computer science in the stanford and the stanford school of engineering is also a fc microelectronics faculty scholar at stanford where he is a member of the human computer interaction group his research focuses on the design of social computing and crowdsourcing systems michael has received eight best paper awards at premier computing venues and he has been recognized with an nsf career award and an alfred peace loan fellowship his phd students have gone on both to industry working at adobe facebook and starting companies as well as faculty careers at carnegie mellon and uc berkeley michael hole holds a bachelor's degree in symbolic system from stanford as well as a master's degree and phd in computer science from mit welcome everyone today and welcome michael would you please go ahead and get started all right it's wonderful to have you all here uh thank you anita my name is michael bernstein as she mentioned and yeah i teach in and do research in stanford's computer science department which means that i'm interested in how we design computational systems that are a better fit for human needs goals uh and and essentially the broader context and a lot of what we do draws on the techniques developed by modern artificial intelligence algorithms to create new kinds of user experiences and that's really what our goal is going to be today we're going to be thinking about the kinds of opportunities that arise when you combine ai with human computer interaction or hci but also the the unique challenges that come and i want to start off by just mentioning that you know if we get this wrong there it can be really really tough i'm sure you've all seen examples public publicized on in the media uh demonstrating uh for example when people are applying ai techniques in ways that are uh racist biased are are creating all sorts of errors even today on the internet lots of people i think uh chiding a website that will tell you uh the gender of a name and putting in words like you know professor and finding out that it's 90 percent nurses is very female there's lots of challenges and issues that we have to consider as we as we think about these about these and i think it's important that we that we draw on broad stakeholders when we're doing this today we're going to focus on how ai is influencing sort of that last mile of user inter interaction uh so i'm going to go ahead and share my screen which means that my face is going gonna largely disappear but i have i promise i am talking to you live and uh i encourage you to put questions into the q a and i'm happy to have a conversation about this as as we wind up so again very thrilled you're here with us you're gonna get a brief moment of transition as we start the slides this talk is going to be about what i call interactive artificial intelligence so i think what we need to do here is acknowledge that that ai is not just a thing that happens in the back end that but that ultimately these are all done in the service of some human goal and sometimes they're there these ai systems are put directly in the hands of people and we need to think about how we co-design the ai and the interaction to be effective now give a few examples um here on the line on the right from from colleagues at uc berkeley showing that you know really ai hits the road when it starts to interact with people whether we're talking about um in these examples robots robots that are uh you can imagine a completely non-humanoid robot but at the end of the day the robot needs to interact with people so what these folks on on the left at mit's personal robotics group are doing are looking at ways in which robots are giving social cues through through uh what it's looking at for example that people can make sense of or on the right uh on dragon's group at uc berkeley is building grass uh grasping algorithms that people can understand they're legible they're that i can predict where that robot is going to actually grab because it turns out that the most direct path for a robot to grab say that bottle on the right on the right-hand side is not actually the path that people are going to find the most intuitive so instead the robot can take a less direct path but one that people make more sense and can predict what it's going to do now we already have ai that's interactive in everyday use um maybe you use siri or alexa or any of these uh these smart assistants or maybe you just use uh systems like google bing other search engines and those are all ais behind the hood they are ais that have been built with the interaction on top of them google very famously created just a single text box whereas previous search engines were much more complex and at the end of the day i think we have to acknowledge that automation here is not the answer that um actually a very good case in point as i'm speaking my alexa is actually talking to me because it thinks i'm trying to address it i just had to go unplug my alexa right now because uh it has not been designed effectively to take into account that i might be talking about alexa rather than to alexa so case in point automation full automation is not the answer i think a very uh simple case in point here is is uh self-driving cars we've i would love it to be the case that i could get into a car and that it would just get me where i need to go but a ton of studies have demonstrated that this is a really patently bad idea what you're seeing here is uh some some some moments from a study done by wendy joo who's now at cornell tech looking at handoffs between automation and live human control every car every automated car at some point is going to get into a position where it no longer can drive completely out in an automated way there's construction on the road it's confused it needs to hand control back to the human and in the bottom left you can see this person is actually staring at their phone while they're getting driven somewhere here in a simulator uh and then the car all of a sudden says oh guess what it's your turn now and the person freaks out um and and barely avoids careening off the road in fact there's another cars coming in as oncoming traffic they almost get into a uh into an accident they're also swearing um i've i've cut that part out and essentially it's not about is the automation good or is the person good but here it's that intersection where the automation had to hand back off to the person how do we design those those seams so that people can work more collaboratively and that the power can be handed back and forth more effectively and to make this point i'm going to start by introducing a term it's called intelligence augmentation and it actually has a long history intelligence augmentation i think effectively understood as a response notion that everyone's just um there are many folks including actually stanford has incoming faculty erin yolsen he was just uh arriving from mit uh studying which jobs in particular ai might actually eventually displace and they're largely very rote ones and in reality um there are there are some jobs that that will be replaced in that way but many are it's going to be the story's going to be much more complicated and it's there's going to be more much more of a pull and tug so this is a picture of douglas engelbart he's a touring award winner that's basically the nor the nobel prize of computer science uh douglas engelbart invented uh among many things hypertext the mouse um a two-handed uh one-handed key uh key chord uh corded key set which we use less the the initial word processor all sorts of stuff effectively you know the first working graphical user interface um but he had a vision around how all this was going to go one which he published at stanford research institute sri at the time about what he called augmenting human intellect that computers were not here to replace us but to actually make us smarter imagine a sort of cognitive prosthesis something that's like a strap-on cortex that makes our brains smarter and i think you can sort of see these two ideas as being a little bit in tension on the left artificial intelligence where you're trying to take human intelligence and just replace it with computation on but on the right and i think the the point i'm going to try to make here is much more focused on what i would call intelligence augmentation and what douglas engelbart calls intelligence augmentation where instead of just displacing what you're doing is actually using the computation and the intelligence to help us do think through things smarter better more effectively more collaboratively now any story about what ai and interaction are going to do together needs to start with a very brief uh collective moment of what ai can and cannot actually achieve and here i love this quote by etan adar at university of michigan where he says don't let your ui your user interface write a check that your ai can't cache what he means by this is that it's very common that people will build user interface elements that assume an a an ai that's smarter than ai can actually provide i think one of the most famous examples of this um if i'm not going to raise various forms of ptsd for for old-time computer users is clippy the microsoft office assistant i was trying to guess when you were say writing a letter and would pop in a little paper clip and say hey it looks like you're writing a letter but the ai they had tuned it down to the point where it was not really very smart and so the user interface was brill was you know assuming a brilliant ai that didn't actually exist so how do we know what might actually be feasible my colleague andrew ing in computer science is a machine learning researcher i think had this article in the harvard business review that i think puts a nice point on it um you know we can debate the particulars of it but i think in broad brush strokes this is this is not so far off that we have to think about two things first essentially when you're talking about ai almost all of its progress is through a particular type of function something that takes some input a and generates some simple response b and the a's and the b's can change but it's always a becomes b you give it an a that's a picture and the b response is a yes or no is there a face in it you give it an a that's a loan app and b is a prediction will they repay it again there's lots of issues with doing that and bias um that can come out of it a is uh might be an english phrase b might be a french phrase we're just translating everything sort of uh you feed something in it gives you something back and what he says that i think is is a good way of thinking about this is that if a tip if a typical person can do a mental task with less than one second of thought then we can probably automate it using ai either now or in the near future so what does that mean right it means that things like is that a person is that a face is that a dog is that um is this sentence in english um all of these kinds of tasks are things that you do almost subconsciously very quickly in a second or less and they vary obviously if not already can be done by ai other things like please design this software for me architect my home um is this is this person going to win the olympics in in 10 years these are all things that would take much more than one second of thought um and so this is likely to remain outside of the realm of ai for for the considerable future so again is my car inside the lane yes we can do that now that's a very fast task um but is is my harsher we can make judgments but maybe maybe uh we're doing okay but we're not it's not clear we're going to get it right yet so then we need to flip this around say okay if that's if that's what a i can and cannot do what can and then can't we do with with intelligence augmentation and here i'll give you a few examples of how um how things that are likely coming to market soon um are drawing on these these advances so you can imagine for example um a project in which a piece of hardware that you can install just sort of uh turn some turn something into the pipes outside of your the spigot outside of your house and it will start to disaggregate your water pressure your water usage and be able to start telling you how much water is being used by this toilet this bathtub this sink this dishwasher imagine just being able to screw one thing in and then getting all sorts of visualizations uh about what's using water right it's learning on it's learning on its own it's being trained what's a toilet what's a faucet and the way you can tell this is by momentary changes in the water pressure in your in your water line as some as say the faucet opens and once you have these you can start to think about how you design potentially behavior change interfaces that would visualize how much water you're spending who's spending it what time of day try to help you sort of uh be more sustainable another example are what are known as ability-based interfaces these are interfaces that are that can be customized to individuals say with different motor abilities so imagine that i were i was born with a motor disability this system can start to take a user interface like the one on the top measure various things about my motor abilities and then uh predict a version of that same interface that would be more effective given my my motor abilities and individuals who have these customized interfaces you can imagine this being like a web browser plug-in for example um are about 25 faster makes you know 73 fewer errors with these automatic adjustments so we can start to think about how ai can understand um how people who are born with various motor disabilities or or or seeing or visual disabilities might be able to perform or also as i age or as i'm as i'm distracted because i have a three-year-old at home and i'm working from home now because of code covid19 uh you know all these differences um in the style and fashion space um people are building technologies that allow me to describe in words what it is that i'm looking for i'm looking for an outfit for a beach wedding um it needs to sort of look effortless and breezy and build embeddings on top of this that can return uh items that that match the query even even though i may not have used exactly the right terms think of this as sort of a higher level search engine i don't just say you know breast dress dress with flowers i say uh you know a beach wedding with with that's breezy and it and it can can do this mapping here's another one um this will be a bit of a of a short video if you're a visual designer people are building ai models that can understand where people are looking so you want a heat map that says look at the red parts people pay attention to the red parts and as i move the design of my poster around let's say this text october um as i make it smaller or larger it you i can see immediately how much people are going to be paying attention to various parts of the poster so now imagine i'm a designer or a marketing person or something like this being able to work in a very tight loop where i can change everything that i'm doing i can change the design i can change the messaging and get an immediate prediction of what people are going to pay attention to because we have better models of human visual attention so if i change the color no one pays attention anymore so there are lots of opportunities but there's lots of challenges in this space as well and i think this is where we're going to start to dig into the court the core issues around designing interactive artificial intelligence which is uncertainty and error what do i mean by this so in essentially up to this point in computing and interaction we've lived in a world in which when i click on something or when i say something the computer does exactly what i ask it's known as direct manipulation that i say click on the save button and the machine saves click on the bold button and you know my text gets bold but people are trying to do stuff with ai that will inevitably introduce error and uncertainty and that's because i might be saying things at a higher level like make this look more professional or it might misunderstand me or it might mis-execute it's more intelligent but it's not deterministic it's not it's not for sure that it will do this so what do we do about this in some systems um when you have very high certainty that you just the systems will automatically take action so on the left you can see an example in gmail if i said um you know attached arm my tax documents but i didn't actually attach anything or various other versions of this gmail will actually pop up a window that says hey did you actually mean to attach files this is sort of like the are you trying to write a letter right now from microsoft word uh but it's so certain given your language that this is often viewed as a savior rather than an annoyance likewise when i tell siri to set an alarm in 25 minutes siri just goes it goes ahead and does it when it's high certainty and the probability of error is low it just go these these systems will go ahead but what if certainty is lower what do we do and here there are a number of different techniques that industry and research have developed to try and be a bit smarter about this one of them goes under the heading of adaptive interaction in adaptive interaction what you're doing is tweaking the interface to sort of bring things closer to you that you that the system thinks might be more useful so rather than trying to execute something automatically it's sort of more like um an assistant who's bringing you the stuff that you might need right at the time when you need it and you can notice for example that when you resize in this case microsoft word it's not that it just sort of cuts off the various items it will actually relay out some of them it'll collapse certain things it'll expand certain things because it has a notion of what you're likely to be clicking on and it and it will try to lay things out such that it keeps the tools that you need most available again you can think about being adaptive that as uh you'll notice that if i take a website and i turn it on my phone and i turn my phone from say vertical to horizontal it will relay it out ai's will sometimes do this kind of thing as well where i can automatically re relay out an interface that would say design for a mouse interaction to be to work well with touch interaction like on a phone or a tablet by trying to optimize a cost function on on how much time and effort it would take someone to to do this now so that you can be adaptive like this but another large set of options are are oriented around accelerators what are accelerators accelerators basically don't force you to do anything by default it might just do exactly you might be exactly as fast as you are now but it offers you an opportunity to sort of auto complete what you're doing sometimes literally when i type sanf into google this drop-down has lots of guesses about things that i might actually be typing and it's not san francisco that's the most likely it's actually san francisco weather so all so all i need to do is press the down down arrow and enter and it'll it'll speed me up or likewise um the when i'm driving driving directions like with google maps or apple maps um or various others um ways it will sometimes say hey there's a faster route now available and this is an interesting question right i can i can be accelerated by hitting accept or if i do nothing i stay where i am and there's always the important question here of whether this is opt-in or opt-out if i do nothing will it move me to the new route or if i do nothing we'll just keep me where where i am this is very context-dependent in general you want to my advice is to opt toward heavier user control which would mean by default let the do the last thing the user told you to do but allow them to revise their opinion another approach that many interactive systems use when trying to layer on top of of uncertain ais are suggestion this is when you have uh lower lower certainty these are often seen in the in the context of what are known as recommender systems recommender systems power everything from amazon to netflix to uh to stitch fix the basic idea here is that it's uh it's saying hey here's some stuff you might be interested in seeing so um you know netflix at some point tries to maybe he thinks that i'm that i'm into uh comedies and it'll show me parks and rec friends and cheers or stitch fix is trying to learn more about my style so that it's so that it sends me genes that um that fit my style if you want to learn more about this i would check out collaborative the term collaborative filtering there's a rich uh rich set of techniques around this but ultimately what these what these recommender systems tend to be tend to be used to do is to display a large number of personalized suggestions to the users so we don't know which one we might be wrong about any given one of them so we're not going to show you one we're actually a menu and and you're going to pick between them the hope is that at least one of the things there would be good enough even if some of them are are kind of off and then in general if you have low certainty you just do nothing this is in fact most interfaces today it does what's known as direct manipulation it just says the end user's in control we're not gonna we're not gonna screw anything up but we're not gonna do anything either we can make guesses but ultimately there's so much error that we might make that it's just not worth it it's just not worth it we're going to let leave you in control so this is overall known as a mixed initiative interaction framework and it's mixed initiative because sometimes the computer will take the lead and sometimes the the person will take the lead um if you're a little bit more um mathematically oriented i can sort of provide a visual a visualization of of the um of the notion here on on the x-axis along the bottom is the probability of of some goal being true given that the the system thinks it's true so on the on the bottom on the bottom right there is yep the machine is right on the bottom left is nope it's wrong it thought the goal that your goal was g but it definitely wasn't and on the y-axis is how much value this brings to you this is in sort of an economic sense like utility so is it really good for you or is it pretty bad for you um so if it's unlikely that the user has a given goal we're on the left half of this square and if it's likely that the user has a given goal then we're on the right half of of of that square and you can actually now build out several if you could build out a few utility measures so for example in the upper left that that little um half side half of sideways half t is is a not symbol logical operator not if you haven't seen that before um what it's saying is what's the utility to the user if i take no action and it was not their goal so let's say it means if i interactively do nothing just let the let the user do their thing um and and i was wrong about what the thing that they were going to do was and in the upper right likewise is the utility of taking action if it was their goal what what if they didn't want to change routes and i did it for them how good is that and likewise the other two would say what if i don't do anything but i was right what if i do do something and i was wrong that's the false positive right what happens if i if i if i say if you're writing a letter how much how bad is that and it turns out that you can actually reason over this space to figure out when you should do what for example here this green line is what you can think of as the utility of inaction they both say not a that means i do nothing right so if in the upper left that means if i was wrong about the goal this is how much utility i would be getting if i did nothing and on the bottom right if i did nothing but i was right about the goal that's how much benefit or or harm i would bring to the user likewise there's a utility of action and what you can start to do is say where's the where's the line highest and what you can sort of see is that the the line on the utility of inaction is higher it's in that upper left it's it's higher up um when the uh when when i'm low certainty on the left half of the diagram like when i don't when i'm not actually sure i should probably not do anything but as i become more sure then i should start to act and you can reason about this in a closed form fashion uh through through this this notion of mixed initiative interaction from eric horvitz now that's all what happens at the user interface side what happens about interaction when we're really dealing in the with the guts of these machine learning models what should we do what what what's going to happen so here we're going to dive into these machine learning models these ml models a little bit more directly because these ai systems are all built on the back of these modern neural networks and we have to understand what what we can do to try and make these models more powerful more effective more understandable uh and so on i'm gonna assume here at least at a basic level you know what machine learning is doing but if you don't i think you could probably figure out the assumptions that i'm making one of the core questions of this space is what is that black box learning you've trained some statistical machine learning model and what has it learned how do i know what it knows and what it doesn't know what kind of errors might it make these machine learning models are big black boxes they're powerful they're very but they're very high high level statistical functions uh that that that can basically they're universal approximators which means they could learn anything but what did it learn so it's opaque it's unintelligible which makes this thing very difficult to predict design and debug and it results in some very non-intuitive behavior this famous image by ian goodfellow is an example i built a computer vision underneath what it's looking at here in the bottom left is i feed it that image of a panda and it outputs panda with no an okay level of confidence about you know 57 confidence then um these authors demonstrated that i could add a very very small amount of noise um carefully chosen noise that uh that is a messed up version of a nematode or excuse me it's gonna think it's a nematode but be very low low confidence in that and produce the image on the on the right that image looks to humans basically exactly the same as the panda image on the left it does have this sort of high frequency noise layered into it that we but we humans can't really tell but this totally messes up the algorithm and now the algorithm thinks with very high confidence 99 confidence that this thing is a gibbon totally wrong so two images that look the same to people the algorithm is thinking are very different and so we need to be able to understand why did it think this way why did adding you know this seemingly random colored noise change its opinion the goal here is to produce what's known as intelligibility in these models a model is intelligible um as sort of as as is defined basically to the extent that a person can predict how a change to the inputs will change its outputs so for example uh let's examine whether my three-year-old is intelligible um and i was trying to feed him lunch if i uh earlier today if i give him a uh a peanut butter sandwich i can predict that he will be happy and eat it if i try to give him the lentil soup that i made i can predict that he will not eat it and will not be happy so i feel that that model is relatively intelligible because i can reason about it when i change the inputs to his to to his lunch i can predict the outputs from from his mouth so for example in the bottom left linear linear relationships we tend to view is it very intelligible right so um y equals mx plus b if um you know if i get more years of education then my then my lifetime income goes up that's a linear relationship it's just two variables um as i increase y you know as i increase x y either goes up or down convolutional neural networks of the sorts that are used in computer vision um far less intelligible there's you know these these sliding windows that are compressing down and then recombining um you know essentially one cannot really make a prediction of what happens if i change the little circle on the top of the robot's head to a square we don't know exactly what's going to happen um they they they make as we would say at the end of the day they would make errors so one way to to look at this from from an uh an intelligence augmentation point of view is to try and help visualize what the model sees these are images that might keep you up at night but what it is is on the far left you can sort of essentially accentuate the the nodes within the neural network that are activating with the with the notion of a labrador retriever and essentially get it to hallucinate what it's what it sees when it thinks of a labrador or receiver and then as you go over to the right a tiger cat and then you can uh build the entire linear space between those to look at what what is it paying attention to and as you can see on the far left when it's when it's recognizing labrador retrievers what it's really seeing are these big floppy ears and some brownish nose and so on and then on the right with the tiger cats you can see the small pointy ears the sort of texture to its fur the patterns on its fur um and in between so the i the question would be maybe we can't make the thing reason in the same way that a person can but maybe we can come around the other side and help a person see what the system is seeing now there are other approaches as well just simplify the model sometimes you can actually get you know 90 95 percent of the same of the performance out of the machine learning model with hot far higher intelligibility one of my favorite examples in this space is an algorithm known as lime um and what this thing does is try to point out that even if the the entire decision is complex maybe within a specific with the with the case of a single example it's much simpler to understand so let's say for example here you have these blue circles and these red crosses and the machine learning algorithm has learned to separate the the red crosses and from the blue circles uh through the sort of lighter colored irregular space that you see here so let's say you were trying to explain why is this red plus sign why is it in the red space why is it not a blue why do i not think it's a blue circle what would you do well lime made this observation that you could sample points nearby for example these big red crosses and big blue circles and weigh them in proportion to how close they are to the example you care about and then learn a very simple linear separator between those so you see that black dashed line around the red plus sign says you know what i can't explain the the full complexity of the space to you but for this particular example relative to other counterfactuals nearby here are the features that matter what's important to note here well let me give an example again my three-year-old when i say it's bedtime you can say why why is it bedtime i'm not tired i can give all sorts of different uh of of different reasons it might involve the fact that he needs sleep or he's going to become a mess it might be that you know i need to do some work tonight it could be many different things and they might all be true it's complex space but in the case of of this particular example the most salient reason might just be because it's after his bedtime and that's the simplest explanation to give if the time were to change to be earlier it would no longer be his bedtime so i've inserted a simpler explanation but it's important to point out that this is actually kind of technically a lie it's not a representation of the full model there are times where i would let him stay up past his bedtime for specific reasons but i'm not going to explain all that it's going to be a local explanation um percy liang and uh penguin co here at stanford also did some cool work on influence functions that are another way you can visualize this so in this case what you can do is say well here's this test image on the left and you can start to actually identify the training images the inputs you trained the algorithm on that were most influential in the decision so again here um with this with the sense of my three-year-old sending him to bed you can say why well i can say well let me tell you because when you stayed up late last time you were very cranky the next day so it pulls out the influential training data points and those are much more readily interpretable than a complex model now as dan weld uh and his colleague argue there's actually a dilemma here any model simplification like line is a lie it is not true that this is exactly how the model reasoned about it we're telling you a single thing when there's a much more complex set of factors in play here but it turns out that the non-simplifications are just unintelligible so we're somewhat in a in a sticky wicket here that we either have to lie or we have to give you the full details in a way that are going to be very hard to understand and i think it's really going to depend on your context what dan weld and argues is that you can draw on psycho psychology research to guide what makes for an effective explanation so you make explanations for example contrastive these are things that people tend to understand quite well well why did you recommend movie x the implicit critique there is why didn't you recommend movie y and you can explain that much more effectively necessary causes being better than sufficient ones using few conjuncts don't say it's because of a and b and c and d and so on now when we pull all this together we start to see a situation where these ai techniques are especially common as we kind of move off the desktop into voice speech computer vision based interaction because once you're off the desktop you often require intelligence i'm going to avoid saying the name of the spark speaker on the left in order to so that i don't uh say the wake word of the the one in my room uh but you can get the idea of what it is um fitbit apple watch all are all sensing my biking right now am i walking in my um soon the new version of ios will tell you whether you're washing your hands um the um nest thermostat detects when you should when it should be changing the temperature and whether you're home and so on this all requires artificial intelligence because we're not gonna sit there and and tap every button on everything and there's lots of examples where this has become successful so here's an example of a research project a number of years ago now at georgia tech demonstrating that you can figure out um what kinds of uh just like before when i was showing what's using the water in your in your in your home what's using the electricity in your home or here's an example out of carnegie mellon uh demonstrating that by just simply having a smart watch on it can it can start to pick up on the electromagnetic waves being sent out by various things you might touch and your computer can in real time detect what you're what you're touching so as i as i'm holding this iron sending electromagnetic signals body is as well smartwatch and that smart make sense of the signal run some machine learning on top of that and then uh detect what i'm doing so now imagine that my watch knows what i'm doing am i grasping my office door it should unlock the door when i'm grasping the door for example um and all sorts of other examples so am i turning on the am i turning on the stove am i get am i grabbing my my toothbrush am i trying to open a room am i using my first laptop or my second laptop imagine how our interfaces might be able to adapt themselves if they knew what i was doing these are the kinds of applications that become possible when we go off the desktop so i have just a few closing thoughts and then we have hopefully i left plenty of time for for questions and discussion and again as a reminder there's a q a area in your interface that you can feel free to toss questions into um and then you'll you'll soon be able to see my face again and we can uh have a discussion for the rest of the conversation here so a few reflections first ai is undoubtedly a very powerful tool but it brings along with it some massively unsolved user interaction problems and often and there are a few different sources of those one which i which warrants uh you know an entire lecture on its own is problems around bias justice and so on that these models can can inherit the one we talked about today also important is the fact that all of a sudden it takes some something that used to be a very certain interaction direct manipulation and injects uncertainty so it brings benefits in that it can do things that a direct manipulation interface never could but it also makes errors and it's not sure about what about its answers so we have to think about how to design so the second reflection here that i hope you're taking away is that if we're if we're smart and thoughtful about how we do the interaction design we can hide or manage that uncertainty more effectively that we can sort of build in a user experience that still feels good even though under the hood it might not be entirely certain we also now know that if you don't do that well as i'm sure you can now think of examples it can really cause someone to throw throw the thing on the floor and finally again quoting etana dar don't let your artificial intelligence write a check that your ui can't cache meaning don't build an ai model that we can't create an effective user interface around and don't let your user interface your ui write a check that your ai can't cache again meaning that you shouldn't be having building a user interface that assumes or promises things that an artificial intelligence algorithm can't actually do well enough yet uh it turns out um through prospect theory um a nobel prize winning theory from conomon tversky that people are very very averse to problems and errors people will much rather avoid a problem than than essentially get an equivalent benefit so if i have a choice between something that's slower but will do the thing for sure or faster but might make an error many people will choose the slow but sure route i think this is a major impediment in some ways or major reason why people are are avoiding intelligent interfaces unless they're very very accurate so i'll close here and i think we have a few minutes left for conversation questions and so on all right so i will stop sharing my screen and we will talk hopefully you can now see my face yes and hopefully everybody can hear me thank you michael this is very informative and an impactful presentation we've gathered a range of questions from the audience so let's go ahead and get started the first question is if you could talk more broadly how is ai being utilized to mitigate this particular question with supply chain disruptions during the pandemic for example in the food service industry transportation lodges logistics clean energy or any other industry what are you seeing seeing what are you hearing um out there right now sure um yeah well not very well for a very particular reason um and again hopefully you can see my face now um the here's the reason okay machine learning inherently makes the following assumption it has training data and the training data is assumed to be coming from the same distribution as the test data or the real world what that means is that essentially all of these algorithms assume that what they're going to need to do when they're put in the real world is exactly like the training data they were given so now imagine that your training data all comes from a pre-pandemic world you can make all sorts of assumptions and now your test environment your real world environment is a pandemic everything has changed and so many of these algorithms will in fact i predict making the wrong predictions because they don't know what to do in these sort of off-kilter scenarios um there's a really good example you may have heard um a few years ago there was a google's alphago algorithm was playing um go master the world champion and it did in fact win but there was a very telling moment where uh essentially the the player the human player did something that the the algorithm uh had not really seen before it was a confusing move and the system just totally collapsed and made some nonsensical moves so uh i would actually be very careful about uh thinking about how to deploy ais in a world that has changed because that's not what they're meant to be able to do i in in those cases i would actually be much more uh i would be much more likely to to make sure that there's additional human oversight so that it's not making um unusual or or poor decisions great um that leads me thank you that leads me into the next question and and this is actually a combination of a few different questions um sure how do we help ai to avoid bias towards things like race disabilities gender um where we know there's there there's an issue um how do we adjust for that yeah well i think we have to be careful that there might be you know um challenging assumptions bellying the question how do we adjust for that because it sort of assumes that um that it's that that's you know that if we could just get something that performed that didn't exhibit that bias that would solve the problem and it won't but let's let's take this in two parts so um if you're interested in the very uh cutting edge technical work in this space i would check out a conference um uh it's the i'll just list out the letters f a t star or asterisk uh stands for fairness accountability and trust and then everything else around that uh it's a conference where people really spend a lot of time thinking about this however as many have demonstrated um there's no single really effective definition of what what fair means or what what bias means so for example you could stratify on certain uh certain demographics and make sure that your that your algorithm is not performing better or worse on certain uh demographics than others uh there was a subject of a major po propublica article a number of years ago about how the algorithm is used to to predict recidivism um and used by judges i.e are you going to show up to your court date if i let you out on bail uh we're biased against african americans or black people and it's definitely true that we can start to to build um estimates of this but there's it's actually very the devil's in the details there and there's no really good way to capture everything about what we what we mean by fair um and so you end up having to pick one definition or another and then you're going to inevitably uh take basically be missing something um if you're looking for folks at stanford who have been thinking about this charade goel in the management science and engineering department has some really excellent uh fairly statistically heavy work looking at um how we can reason about these issues that said even a perfectly fair transparent and accountable algorithm might still be used to advance unjust causes right i can have a a perfectly fair algorithm that's yet still used by an authoritarian government to um you know identify people who are protesting and go through them in jail uh or or that is applied disproportionately toward people of color and i think that's where we need to consider we need to remember that computing doesn't just exist in some technical you know magical world where where ethics and injustice don't matter but but in fact that that ai and computing are very tied up with systems of power and oppression and we have to stop and think okay what is it that this system this ai et cetera is being used to do what's the goal who's going to be in charge of it and how do we how how comfortable are we with that answer what are the likely abuses of that power and what should be built in the system to predict to prevent it or should the system be built in at all you'll just be an input to a human uh who's been uh who you know maybe is elected or something like this great thank you um this is a there's another combination of a few different questions for an inspiring startup or a company that is never a larger company that is never implemented in any any ai um where you know the general questions where does one start um and then getting it you know taking that maybe you've done some ai implementation uh what are the new fields and which ai are being is being implemented sure well to take that in an inverse order um you know the first the first area the first sort of business areas that adopted ai were the areas that were fairly computationally heavy that sort of tech um tech field because essentially computer science majors were learning machine learning and they said oh i can work with this let's insert you know smart uh smart email uh prioritization into gmail et cetera and i think what you're starting to see or you've started to see over the last few years is the diffusion of this into you know supply chain management uh you know everything else uh fashion farming uh 538 trying to predict uh your political outcomes from polling and so on so you're seeing sort of a diffusion of this uh into um into other areas often it goes by different names in other uh in other business areas sometimes it goes it falls under the heading of you know a data science team but i i would say that there are very few domains now where that remain untouched by ai or at least where someone isn't trying it doesn't mean that they've necessarily succeeded yet but i'm i'm i would be stunned to find an area where someone hasn't tried to figure it out if you're yourself trying to transition um you know there are many people here on stanford's faculty who are actually um you study business transformation and technological transformation and so on um i'm thinking um my colleague melissa valentine in management science um and engineering who has through several of her studies pointed out that there's actually very complex power dynamics within organizations that have to be designed very carefully if you're trying to turn an existing business into an uh you know an ai uh smart business um because ai is going to change who has power over what um and i you know to be very concrete about that decisions that used to be made by people over here in the organization are now going to be made by by algorithms over here in the organization that are and those algorithms are run by you know other kinds of people and so what ends up happening sometimes is that it gets gummed up and if you're not careful about this these people who are losing power in the organization are going to resist the introduction of the ai because it's sort of reducing what they can say in the influence they have um uh angel christine in uh and here at stanford's communication department has studied how this happens in in newsrooms like if you're um like a newspaper in the us or she also studied in france um you know they have different reactions to these predictions of like what's going to get clicks or how many clicks something god etc and you know it's changing the values of of who's important and who gets to make these editorial decisions and so it can really go well and it can sort of make an organization sort of combust if you don't uh if you don't manage it carefully but if you can do that the other things that i've heard them mention and really i'm relying here on their expertise substantially is that well obviously you know you need to uh produce people who are very carefully in charge of the data right what is this data that's being trained you need a ton of data to make any of these things work who's who's collecting it who's managing it who's making sure it's available who's making sure that it's clean having clean data is surprisingly challenging um and then you have a group of people who are modeling building machine learning models on top of that data and who's telling you know who's working with them to decide what to model and how and when um what is good enough mean what's the error rate that is okay enough to launch a product um and so you know there are a number of programs uh through scpd and others uh where you can i think there's even um you know maybe uh anita can correct me i think there's like a digital transformations program here at scpd if you want to learn more about that but there's quite a bit to be said about how to manage an organizational transformation if you're a small startup i think in some way easier um you're luckier because you don't have all that baggage um but i think it it will cause things to look quite different than a traditional startup like a software startup great thank you and yes there is um the digital transformation program which this course is a part of um and you can learn more about that by selecting the learn more button um one other question and this might be our last one is uh i thought this was a kind of humorous but it's to the point how would you communicate to a company the users that your ai is going to give you answers that are not 100 correct there is that error and and how do you how do you send that message so people have tried a bunch of different things generally the most successful technique has been to hide it just to decide whether it's good enough and to own the fact that it might make mistakes um or to be in a domain where it doesn't matter if there are slight mistakes so um peter norvig uh at google i remember him pointing out that a lot of successes in modern ai are in domains where they're sort of like soft edges where it's okay uh there are multiple multiple okay solutions so if you look at these things where like gpt3 recently is producing all sorts of interesting um text and that looks human written and so on it's because there's not one single right answer things can be a little bit weird and off or there are multiple correct paths and it can usually find one of them so that's one approach that that places have have taken but ultimately so for example like with tesla's autopilot there may be multiple correct paths but there are clearly wrong ones that lead to like a crash um and it's and you can't just say oh i'm uncertain about this you have to decide at some point am i going to give the control back to to the driver and say i can't handle this and when would you do that so um you know i don't think that exposing confidence values many algorithms you might be aware of these of these sort and exposed confidence values you know i think i'm you know 92 percent sure this is the right answer those can be useful but i don't think that exposing those two end users get interpreted quite correctly the other thing to keep in mind is that people tend not to associate the errors with an ai that might be making errors they associate it with your product right again think back to clippy in microsoft word it wasn't that um there was a machine learning algorithm which there sort of was making errors they're like no microsoft word is dumb it gets attributed to the company or to the product so you know i would be very careful um you know launching things that have high error rates or again with my voice assistant in the room um i'm sort of surprised how much i put up with it sometimes you know they they misinterpret things all the times and become and it becomes very uh very frustrating even though it knows it's uncertain it still has to do something and and sometimes even just asking me oh did you mean blah is still seen as annoying so i think this is this again comes back to the don't don't make promises you can't keep you know if it can't do the right thing the vast majority of the time i would be very careful about designing user experience around it on the other hand if you have some system in which you know the machine is making predictions and then there's a human layer that's always vetting those predictions and then the only thing that gets seen by the outside world is the vetted version of it then maybe you could be more certain microsoft has a system called calendar.help which is a great example of this it's like an email address that you can it's sort of like a calendaring assistant you can say you know oh um maybe cortana or whatever it's called now um please find you know find a time when um anita and i can meet and it it's for simple requests it's an algorithm for more complex ones there's actually humans in the loop and you never know which one you're dealing with it just sort of is this opaque system that is some combination of humans and machines and i think that that as an intermediary solution can be quite helpful and over time you're generating data from the human interactions that that's the human supervision the the algorithm itself can get better and the in the people can be focused on continually on the harder and harder tasks so those are those are some of my thoughts there excellent thank you very much and that is all the time we have for today's webinar um michael and everyone in the audience thank you for joining us if you found this presentation to be helpful in any way we encourage you to share the recording with your colleagues and we will send out an on-demand version of this with your friends or family or whoever you think will find this information useful thank you for taking the time to join us today and have a great rest of your day you 