 [Alexander Rakhlin]: ... foundations of data science (MIFODS), Last year, he received his PhD in computer science from Cornell University advised by Karthik Sridharan. His research is in machine learning, and especially in online methods, online decision making, online learning, reinforcement learning, contextual, bandits, and he has received a number of awards already - COLT 2019 best paper award, 2018 best student paper award and another 2019 best student paper, all at COLT, so this is a an impressive achievement. He received the Facebook PhD fellowship and the NDS EG PhD fellowship. So, without further interruptions, let me give this stage to Dylan. [Dylan Foster]: Well, thanks a lot. Just to make sure, can everyone see my first slide right now? All right, cool, cool. All right, yeah. So thanks a lot for the introduction. So, today I'm going to be talking about a joint work with Sasha which is about, sort of, new algorithmic approaches to the contextual bandit problem. All right, so just to get into it - so the contextual bandit problem is a simple sort of sequential decision-making problem, a reinforcement learning problem, where we learn to interact with an unknown environment by, you know, making repeated decisions. So, let me just walk through with an example. So - news article recommendation, we're going to play for t rounds, big t rounds, and in each round we're first going to have a user arrive at our website with some sort of profile of attributes, like age or height. We're going select an article to display and then we're going to see a response, like whether or not the user clicked the article, and our goal will just be to maximize the user's response. To give another example - so if we want to do, say, like personalized medicine, at each round we're going to have a patient arrive with some medical history, we're going to choose a treatment to prescribe and then we're going to see whether the patient's health improved or worsened. And our goal is going to be to optimize the patient's outcomes over time. So, to make this a little bit more formal, the medical history in this case is what we'll call a context, x sub t. This is sort of like a feature vector. The treatment we prescribe is what we'll call an action, a sub t, and the outcome we see is what we'll call a reward, r sub t, and our goal would be to maximize the total reward over this whole process. So - and let me just say before i go forward in the talk, you know, depending on the application, it might make sense to assume all the data is like i.i.d or like stochastic here or it might make sense to assume, you know, we just see an arbitrary sequence. I'm going to be intentionally vague about that until later on in the talk, when we get into formal results. All right, so contextual bandits have been widely applied in practice, so especially to recommendation systems like news article recommendation, and more recently to personalized medicine problems, and in this talk what I'm going to give is a new general purpose algorithm for contextual bandits, so it's going to be faster and more memory efficient than previous algorithms and it's also going to be statistically optimal. So to - sort of, like position this result, I think it's helpful to think about - what are the challenges we face when we apply contextual bandits to the the applications that i was just going over? So, the first challenge we run into - there are two very basic issues in contextual bandits. So, the first is that we need to make decisions online or in real time - like, we have examples arriving one-by- one and we have to make a prediction for each example, and the second issue is that we only see the response for the actions we take. So, if we prescribe a treatment, we don't see what would have happened if we prescribed a different treatment. So the second issue introduces the classic bandit explore/exploit trade-off, right? We need to balance exploring new actions to get more information, and exploiting the information that we already have to choose actions we think are good. But what makes the contextual bandit problem especially challenging is that we need to figure out how to use the context effectively. So, this is what separates us from the classical just multi-armed bandit problem is we want to be able to exploit similarity between context to avoid having to relearn similar users, right? So these contexts are very like complex or high-dimensional. We may never see the same exact user twice, but we still want to be able to generalize across similar users. All right, so towards sort of explaining how we address this last issue, it's helpful to take a step back and look at how we address the multi-armed bandit problem, right. So the multi-armed bandit problem is a special case of contextual bandits where we have no context. So we just have k arms and each time we just pick an arm to pull, and see it pay off and we just want to select actions as well as the best fixed arm. So, this is a very well-studied problem. One algorithm you might have seen for this task before is the upper confidence bound strategy or UCB strategy and all this is going to do is at each time we'll form an estimate for the rewards just based on everything we've observed so far. So just by taking the average of the reward for each arm and then we're gonna max pick the action that has the highest estimated reward after we add a little bit of a confidence bonus that encourages us to explore actions that we haven't played too many times already. Right, and so this works really well if you have a classic multi-armed bandit problem, but what this is tacitly assuming is that there is one arm that's much better than all the other ones, and so in the context of, like, news article recommendation, this means we're tactically assuming that there's one article that everyone really, really likes to see, or for like medical treatment, we're assuming there's one treatment that works very well for everyone, and so a lot of, you know, that's clearly not going to be true in general. So you need a way to use the contextual information to better select news articles or treatments that are effective for specific users. All right, and the way we're going to do this is to generalize this idea of estimating the reward. So, what we're going to do is we're going to take as a given a class of, like, value functions or regression functions that are attempting to model the reward, and we're going to call this class script f. So each function little f in script f is just going to be trying to predict the reward of an action given a context. So if we have a function little f and we look at f of x comma a, this is trying to predict what is the expected reward of action a conditioned on me having seen action x. Okay, and there's a bunch of different choices we can use for a model class like this right? So, for context, let's say someone's age, height and location, we might want to use, like, a linear function which just takes a weighted combination of the different features, or we might want to get a little bit more sophisticated and use something like a regression tree, but what we're going to do in this talk is we're just going to treat this function class script f as a given, like this is an input to our algorithm or this is like a modeling choice, and our goal will just be to select actions as well as the best regression function in this class. Meaning, like, for each regression function, you know, this regression function has an idea of what it thinks the best action is for each context and we just want to do as well as, sort of, the policy that you get by following that action. All right, and so our main goal in this talk is going to be to develop general purpose algorithms for contextual bandits. So the point is, you know, depending on the application, these contacts may be very complex or rich or high-dimensional, and so to manage this there's a lot of different choices we might want to use for our value function class, right? So we might want to use linear models, neural nets, regression trees, kernels, and what we really want is algorithms that can take this choice of the value function class as a given, like as an input, and just work. So, we don't want to have to design a new algorithm every time we have a new function class that we want to try out for our problem, right? We want to separate the modeling of rewards from design of the algorithms, and so let's have a look at how, like, prior work stacks up against this. So, you know, research into contextual bandits has been very successful at designing efficient and practical algorithms for a lot of - kind of, like, very structured and specialized function classes of interest. So, like, linear function classes, or generalized linear function classes, or, like, certain non-parametric function classes like Lipschitz or Hölder classes, but the key feature of these results is, like, these are all kind of using refined properties of the function class script f to decide how to export or exploit. So these are all really using specialized geometric properties of the function class to, say, come up with things like confidence bounds that are tailored to linear models or generalized linear models. And so, if we want algorithms that can work with arbitrary functioning classes, like, even like very complicated things like neural nets, what we really need to do is separate the modeling of the function class f from the decision-making part of the algorithm, right? So we need to separate out - 'how do we learn and model with script f?' from 'how do we use our estimates for the rewards that we get from script f to select actions, to balance exploring and exploiting?" And the way we're going to model this in this talk, the way we're going to sort of formalize the idea of separating these two parts of the algorithm designed to separate blocks is via a primitive that we're going to call a regression oracle. So, the idea here is that if we want to try to use this regression function class to model the rewards, you know, basic primitive that we probably need is to be able to find the model in this class that best fits a given data set, right? Like, if you're hoping to learn the rewards, hopefully you can at least figure out what model is a good fit for your data set. So, you know, right - like, if i was wanted to do supervised learning with my function class, this is all I need to do, in fact. Right? Like, I just find the model that best fits the data set and return this, and we have lots of theory that says this has good generalization properties and so forth, but for contextual bandits, everything is done interactively, so it's not so simple. But what we're going to do is we're going to take this primitive, this regression oracle as a given. So, this regression oracle is just going to be an algorithm that takes as input a data set and finds the model that best fits the data, and gives this to me. This is - that's going to be our, sort of, algorithmic building block that we want to use as a starting point, and our question is just going to be - is this a useful primitive? So, can we use this to develop efficient and practical algorithms that really, provably work. But actually, in this talk, we're actually - we're going to be even a little more ambitious than this, and the idea is, you know, as i alluded to before, for applications, we really need to make decisions fast. So, we really want to be able to process examples quickly on the fly and make examples one-by-one. So what we're actually going to ask is that if we have, sort of, an online or streaming oracle - meaning, like, an oracle that can take in new examples one at a time and update its idea for, you know, what the model that best fits the data set is. We would like our contextual bandit algorithm to inherit this property, right? So if our - if we can fit a model in a streaming fashion, we want to be able to make predictions for contextual bandits in a streaming fashion as well. All right, so let me say a little bit more about how prior work sort of fits into this picture, and into this idea of regression oracles. You know, so there's a few kind of general purpose algorithms that we have for contextual bandits and these are sort of inspired by, you know, the simpler algorithms I was suggesting before, like these linear and generalized linear algorithms. So for example, there's a very general version of the the UCB algorithm that you can define for any function class that actually in a 2018 paper, we showed that you can technically implement this algorithm via reduction to like an oracle like the one i had on the last slide, and you can also do - say, like a bootstrapping approximation of this classic Thompson and strat-sampling strategies. So these are both, like, oracle efficient. You can put both of these efficiently if you have a general regression oracle, but these  are like - can be very suboptimal. So these can fail pathologically if you don't put strong assumptions on either the function class or the data-generating process. There's also another line of work which i should mention which aims to develop similar sort of oracle-based guarantees but works with sort of classification rather than regression, so these algorithms, they don't really model rewards, they just take as a given that you have a class of policies, and this is, you know, a little undesirable because, you know, even if i'm just working with linear models, like, finding the linear model that best fits a data set is much easier than finding the best linear classifier that fits the data set. So, you know, the former is like a simple linear algebra computation, the latter is like computationally hard in the worst case. And these are also unfortunately kind of memory hungry. So, like, even if you had an online oracle, these algorithms still have to keep, you know, the entire data set in memory. But I can summarize, sort of, all of these comparisons in one line by just saying that, in practice, everyone still just uses simple epsilon greedy exploration. So, like, if you look at say, the Microsoft personalizer which is this recommendation service that runs on conventional bandits, this still just does epsilon greedy, right? Epsilon greedy is just a simple strategy that says, you know, fit a model. Usually you play the action that it thinks is best, but you know some epsilon fraction of the time you just randomize. So this is, we know this is sub-optimal in theory, but everyone still uses it because it's very fast and it kind of works. But the reason I mentioned this is just because I think there is actually like a lot of room for practical impact here, in the sense that, like, the algorithms that we want to use for conventional bandits, these are often not very obvious, but if we can get, you know, ones that are faster and work better we, you know - there's a lot of room to improve what we're doing. Okay, so that brings us to the main results. Which is, we have an optimal reduction from contextual bandits to online regression. So in a little bit more detail - so we have a new algorithm which we call Square CB and so the Square CB is an algorithm that takes as input one of these online regression oracles, and if your online regression oracle has optimal performance, then our algorithm has optimal contextual bandit performance. So, it's the first efficient and optimal reduction from contextual bandits to online regression. Let me compare this to the previous stuff in a little bit more detail on a few fronts. So, point number one is, we use an easy-to-fit oracle which is regression rather than difficult-to-fit oracle like classification. Point number two is, we work with online or streaming oracles rather than offline oracles. We have no runtime or memory overhead per step, compared to - on top of what the basic oracle requires. So, in fact, up to a constant number of floating point operations. This is essentially the same number of floating point operations is like epsilon greedy, and lastly, we don't require any strong assumptions on the policy class. So, like, data-generating distribution beyond just that, our model is well-specified. So, you know, that there's a function in our function class, we're trying to use some model rewards that actually does a good job modeling the rewards This is sort of the minimal assumption you require if you want to try to model the rewards and use this to make decisions. Actually, before I move on I should say it looks like there's maybe a couple questions, but i can't really pull these open right now because everything's full screen, but if someone wants to tell me... [Alexander Rakhlin]: I'll let you know if there are questions. [Dylan Foster]: Okay, sounds good, sounds good. Thanks. All right, okay. So before i move on, I actually want to say just a little bit about how this fits into my broader research program. So, you know, I work on online or interactive machine learning problems - so contextual bandits, but also more work - more complex decision-making problems like reinforcement learning and control, and really, my main goal across these different models is to come up with a unified framework for online machine learning. Meaning, like, general algorithmic principles you can apply to sort of any function class, any class of models, policies, etc. you might want to work with and also just like a general theory of learnability, meaning what properties of the policies or the models you work with are necessary and sufficient to get sample efficient learning guarantees. So these are things that we have a great understanding of in the classical statistical learning model, but when we go beyond this to these rich interactive settings progress has usually been on a case-by-case basis in the past So what we really want is, like, general tools. And from a conceptual perspective, the new result we have in this work is exciting for this line of research, because it gives a new principle which is that we can separate estimation from decision making provably. So we can take out this part of modeling and learning the rewards and just plug this into our contextual bandit algorithm in a black box fashion and have it take care of the rest. Cool, all right, so in the remainder of the talk, i'm just going to actually explain the algorithm formally, say a little bit about how we prove that it works, and also go over some experimental results. So, are there any questions before I get into this? All right, if not, I'm just gonna dive in. Okay, so to say a little bit - instead of talking about how we actually, you know - the algorithm works, I need to actually, like, state the assumptions that we're making a little bit more precisely. So, as i mentioned before, we have this function class script f, and what we're going to be interested in is sort of the class of, like, induced policies that this function class gives us. So, for each function little f in this class, we can define - sort of, like, greedy r max policy pi sub f, which is just a policy that always takes the action that little f thinks is best, and this is sort of the class of policies that we want to compete with. So we're going to allow the context xt's, we're going to allow these to be chosen in an arbitrary and even possibly adaptive fashion, but we're going to assume that their words are stochastic, and what this means is that we're assuming there's a fixed conditional distribution over rewards so that at time t the word r sub t is drawn independently of this distribution - or independently from this distribution given the context x sub t. And the key assumption we're making is like a realizability assumption or well-specified model assumption which just says that like the Bayes reward function - so the conditional expectation function - actually belongs to our function class, and we're going to denote this by f star. I should say that our results actually don't require this holds exactly. They actually do gracefully degrade if your epsilon misspecified, but I'm not going to spend time on that in this talk. And lastly, let me say how we're actually measuring performance. So we're measuring performance in terms of regret and what this means is that what we're looking at is what is the, you know, sum of rewards attained by the optimal policy. Meaning the the policy induced by the Bayes regression function, and we're looking at, you know, how big is that compared to the sum of rewards attained by our algorithm. So we want to have low regret, this means that we're predicting almost as well as the best policy given the reward distribution. All right, so that's the statistical setup. Let me say a little bit more about this oracle setup more formally. So the the type of oracle we're working with, this online regression oracle. So this is actually sort of - like, a new contribution from this paper is to formalize this type of oracle and really what this is, is it's an oracle for online learning or regret minimization with the square loss. And this is going to be an algorithm that works in the following model. So the algorithm plays for t rounds, where in each round, it observes a future vector zt, which just consists of the context xt and the action a sub t. It's going to choose a prediction y hat t which we can think of that sort of like a prediction for the reward given this context action pair, and then it receives the true outcome which will denote yt which will be something like, you know, the actual reward for this context action pair and I want to emphasize this is not a bandit setting, here, this is a full information setting. So we see the right answer for the prediction we make in this model, right, and our goal is going to be to have low regret to the best model in our class in terms of square loss. So what this means is that if we look at the sum of squared errors between our predictions y hat t and the the actual outcomes y sub t, we want this to be small compared to the sum of the squared errors from y hat t for the best model in our function class f and this regret quantity here, we're denoting this reg sub sq for like square loss regret. All right and, you know, what I want to emphasize is that this is a very standard and well studied problem, right? So this is just online learning, this is like everyone at COLT's favorite thing to do. So we have, you know, algorithms for just about every concrete function class of interest you might be interested in, and we also have a good understanding of what the minimax rates are. All right, so this is going to lead me to the actual - our algorithmic results. So, we're going to assume that we have access to some online learning algorithm, which we're getting, like SqAlg, that just ensures that its regret is bounded by this quantity reg so sq for every sequence. And so, what we guarantee is that our algorithm, which we call Square CB, what it guarantees is that if our oracle has this property, then the contextual bandit regret after t rounds is bounded by some constant times square root kt times the regret of the square loss oracle. Where I should remind that k is the number of actions, so this is a finite action setting. And the overhead in runtime and memory to do this is essentially o of k per step, really like 2k. So that's the result, and what I want to do is sort of unpack this through some examples, like, probably the first, the what's - the question you have in mind right now is, like... okay, is this good? Is this bad? So, looking at some concrete examples for, you know, how big is Reg_{CB}, how big is Reg_{SQ}, this will hopefully clarify that a little bit. So, let's look at it as a concrete example, let's look at, like, linear function classes. So this is where, you know, f of x - f of xa is just the dot product of some vector theta in r to the d, and some fixed, like, known feature map phi of x, a in r to the d. All right. So one natural choice for the the online learning algorithm in this setting is just, like, you know regularized online least squares, like online ridge regression. So the regret of online ridge regression for square loss prediction is roughly order d, and that means if we plug this into the guarantee from our algorithm, what we get is reg is bounded by root kt times d. And so the dependence on t and d there is - that's optimal for finite actions. And i should say the runtime if you do this is essentially k times d squared per step. If you - this is, if you do sort of incentive, like, online update for least squares, right. But d squared by b2 is slow in some applications. So you know another natural choice we can do is just run online gradient descent on the square loss and online gradient descent has regret root t for the square loss, and so if we plug this into our reduction, what we get is, the regret is bounded by root k times t to the three-fourths. And this is actually optimal in high- dimension, so if you don't want - if you don't want the dimension to appear in your rounds, you have to pay this degradation and rate from root t to t to the three fourths, and the the run time for this is just o of k times d percent. So this is faster... yeah. For one more example, so if we have a finite class, it's possible to get log size of script f, as our square loss regret, and in this case the contextual bandit regret is just root kt times log size of f, and this is like the minimax rate for contextual bandits with finite function classes. And okay, of course, you know, there are many other function classes where you can go and look up what the best regret bound you can get is and plug that into this result. The broader picture here is that this algorithm is optimal, and it's sort of actually, like, universal in the sense that, like, for any function class there's always a way to instantiate your oracle so that you get the min-max rating. So what was happening in the previous slide is really that the best rates for prediction with the square loss, like, the optimal rates for prediction with the square loss are smaller than the optimal rates for contextual bandits so when you plug this smaller but optimal rate for square loss into the reduction, it turns into the optimal larger rate for contextual bandits, even though the reduction is paying this, like, root t factor, and let me also say that to prove this result, we had to characterize what the min-max rates for contextual bandits with general function classes actually are, and you can see the paper if you're interested in that. So what I want to do now is actually describe the algorithm and explain how it works. Okay, any questions before i move on? All right, so our algorithm is a generalization of this strategy that was proposed by Abe and Long in a paper from '99 and this is actually a kind of interesting historical tidbit. So this paper, this is actually the first paper that i'm aware of that has kind of the modern formulation of the contextual bandit problem, and it proves a simple algorithm for linear contextual bandits that gets the suboptimal rate in low dimension but the optimal rate in high-dimension but, you know, low dimensional contextual bandits has sort of been like a focal point of research in the last 10 to 20 years. People really like studying the low dimensional case So this result has maybe, like, been slightly forgotten over the years in favor of kind of, you know, more recent strategies based on, like, upper confidence bounds and this sort of thing. But it turns out that this algorithmic technique is actually, like, very powerful, and so really what our main sort of insight is, like, if you take this sort of probability selection scheme from this paper and combine this with the idea of an online oracle, it really gives a meta algorithm that works for any functioning class, and in particular actually if you just choose a better oracle than sort of the one that they were like tacitly using in this original paper, it also gives that the optional rate for, you know, low dimensional linear contextual bandits as well. All right, so let me let me just finally, without further ado, give the algorithm. So to say the algorithm i need a little bit more notation. So we're going to introduce this quantity called y hat t, which is a function of x and a. So y hat t of xa is just asking, given that we have our square loss circle at time t, if I've already fed it like a whole data set worth of context action reward pairs, what will it predict as the reward if i feed it a new context x and a new action a. So this is just, this is just saying we have our - we have our square loss circle like online least squares. This is just asking like online least squares, you know, what it thinks the reward for a new context action is, okay? It's very simple. So the algorithm will proceed as follows. So at each time t, we're going to get a context x sub t and then we're going to compute an action b sub t which is just the action that our algorithm thinks has, like, the largest reward on this context. Sorry, this should actually be x subscript t not just x here, okay? So bt is the action with the largest predicted award and now, for every action other than bt we're going to assign it a probability in the following fashion. So, we're going to set the probability fraction a to be one over k the number of actions plus gamma which is a learning rate parameter, times the predicted reward for bt, minus the predicted reward for this action a. And i should mention that this difference in rewards here, this is always, you know, non-negative because we chose b team to be the action of the largest predictor reward. So the point of this is just, you know, if two actions - if an action has, like, very similar reward to bt, then we're going to put a fair amount of probability mass on it we'll put something like one over k probability mass on it, but if the predicted reward for bt is much larger than the predicted reward for a given action a, we're going to down weight this. So, we're going to give it less probability. All right, and for bt we're just going to give it the remaining mass, and you can show that this strategy sort of ensures that bt will always have at least one over k mass on it, okay. And then finally, we're just going to sample the action a for the current round from this distribution and we're going to feed the resulting observation into our square loss oracle and then we'll proceed to the next round. So let me say a little bit about how this algorithm works, or how we prove that it works. So the first observation is as follows. So, if we just sort of apply standard martingale concentration, we can bound the contextual bandit regret by the, sort of, sum of conditional expectations of contextual bandit regrets. So, meaning sum over t of the expectation under this distribution pt that the algorithm produces of the gap between the Bayes reward, or like the conditional expected reward of the optimal policy and of the given action selected by the algorithm. And similarly we can show that, sort of, the prediction error between y hat t and the Bayes f star is bounded by the square loss regret of our oracle. So the key part of the analysis will be to show that for every round we can sort of relate these two summands. So, if gamma's the learning rate of the algorithm, then the conditional expected regret for round t is going to be bounded by gamma times the conditional expected square loss regret plus a factor of one over gamma. So if we sum this across all the rounds we'll get that the contextual bandit regret is bounded in by gamma times the square loss regret plus t over gamma. And if we set gamma to be like root kt over square less regret, so that just balances these two terms, that gives the result - but okay, but why - why does this actually happen? So why can we show that this key inequality holds? So to do this, it's gonna be, we're gonna adopt like a minimax perspective. So, we can sort of look at like a relaxed or worst case version of the inequality we want to hold up here. So this is going to take, for a given  like vector y hat in like r to the k, this is going to be a minimax problem that asks 'is there a probability distribution over actions such that if we take the worst case choice for like f star of x t and the best action for this f star What is the gap between sort of the the left-hand inequality up here and the right-hand inequality, and this is a pessimistic upper bound because this per round minimax problem is completely ignoring the fact that f star actually belongs to some structured function class. It's actually just saying like, you know, given everything that had happened until now, imagine i chose the worst possible choice for the base, but it turns out that this Abe and Long probability scheme that we're using certifies an upper bound on this per round manufacturer problem. So it certifies that the value of this game is at most 2k over gamma, and so that that's actually a pretty simple computation and you know, you can look at the paper if you're interested in details. Interestingly, in subsequent work, we've actually been able to figure out not just like an upper bound on this value, but like the exact strategy that achieves the value and this actually doesn't seem to be very useful because it gives a way of - it gives sort of a more tractable optimization formulation that you can use to extend this type of algorithm beyond infinite action, so like - or beyond finite actions to infinite actions. All right, so that's a high level view of how the algorithm works. What i want to explain now is, sort of, how well it works in practice. So, to evaluate the algorithm, we used this sort of large-scale contextual bandit evaluation setup from this empirical paper of Bietti, et al. from 2018, and so what we did here is we took about 500 different classification data sets from this OpenML data set repository. So this includes, sort of, a lot of the the standard classification data sets you might have, you know, used before in empirical ML like the the UCI data sets and like the LibSVM data sets, and for all these we just turn them into bandit data sets by simulating bandit feedback, meaning, like, even though we know what the optional action is we just, we hide this from the learner and we just only tell it the reward of the action it takes, right? And so we compare it with a bunch of, sort of, standard baseline algorithms, all of which are in this like vw or like Vowpal Wabbit repo or this is - you know, this, like, sort of standard interactive machine learning library. So this includes the greedy and epsilon greedy algorithms. It includes an algorithm called RegCB, which is from a 2018 paper by me and others, which is, sort of, before this was kind of like the best regression oracle-based algorithm we had and this was the algorithm that this Bietti, et al. paper kind of found performed best across these data sets in their old evaluation. And then, we also compared with bagging and bootstrapping-based exploration, and we compare against this well-known algorithm called online cover, which is one of these direct policy-based algorithms I mentioned before, and sort of the state-of-the-art from that domain. And so, to cut a long story short, we found our algorithm worked really well, but what worked even better is the following adaptive variant. So we go - this is an adaptive variant of the algorithm we call 'AdaCB,' and what this does is it just refines the strategy a little bit. So basically the idea is that each time, we can come up with a set called script a sub t of algorithms that are like statistically plausible. Meaning, like, we can kind of use confidence balance to rule out actions that are decisively bad and then we just apply the same strategy I already defined, but only to this set. So meaning we just restrict our effort to actions that we know we can't rule out statistically, and the point of doing this is it just avoids excessive exploration on algorithms that, you know, might - that are not - that we know are not good, okay. So let me give the results. So, still a bit more - we restrict ourselves only to sort of, like, challenging data sets. Being data sets where there's more than just two actions, and for all these algorithms, we optimized all their hyper parameters and we did this on a per data set base using validation loss. All right, and so these are the head-to-head results. So, if we look at this table, what the number in each cell of this table means is how many times, on how many data sets did the the algorithm in the row that the cell is in beat the algorithm in the column that the cell is in. So if we look at this bottom left-hand cell, this has number 64 in it. So that means that AdaCB beat out greedy in a sort of statistically significant sense on 64 data sets, okay. And, so we find if we look at this is so, you know, SquareCB, the basic strategy beats all of the previous algorithms except for this one for my previous paper RegCB, and this is because like a lot of the algo - the data sets in this collection are actually kind of like easy exploration data sets and RegCB is really good at actually zooming in on the good actions when you have data sets that are easy. So if we sort of augment SquareCB with the same capability which gives this this adaptive variant AdaCB, we're just beating it. We beat everything. So, we - we're winning over all the algorithms and just, sort of tautologically, we're the only algorithm beating all the other algorithms. And let me give a quick advertisement.  So, if you want to try this out, it's now implemented in VW, this is sort of like industry standard interactive or online machine learning repo. So if you go and, like, clone the latest version, you can try it out. It'll just work. All right, so let me summarize. So, we have a fast, practical and optimal reduction from contextual bandits to regression. This is the first algorithm with this property. There's some cool extensions. So, we have this infinite action extension I mentioned before, and there's also a very nice work by David Simchi-Levi and Yunzong Xu, also in IDSS, where they showed that actually the same probability selection strategy, as long as you choose the learning rate right, it actually also yields an optimal reduction to offline oracles as well if you're in the i.i.d setting. So that's really cool development, too, and it's a really nice analysis. I'm just going to conclude by mentioning the, sort of, broader program here again which is really to develop, like, unified theoretical tools for for challenging interactive learning problems like contextual bandits and reinforcement learning. So, to say a little bit more about this, so, you know, if we compare statistical supervised learning, we have a very good idea of, like, what are the algorithmic principles, right? So, for statistical learning we have ERM, which is just a principle that says for any model you know we just find the model that best fits the data, and this is the algorithm, and it has good statistical guarantees and so forth but it - and, so now for contextual bandits, we have a similar principle, right? We have, you know, a simple algorithmic principle that we can provide, that we can apply to any function class, and similarly when it comes to learnability and so forth, in statistical learning, we know that for a given function class like bounded VC dimension or bounded covering numbers are sort of necessary and sufficient for learning, and these characterize the right rates, and so something I didn't spend so much time on in this talk, but you can check out the paper for is, like, we showed something similar in this paper for contextual bandits. So we showed you can you characterize - what are the optimal rates for arbitrary function classes using, you know, the machinery covering numbers. And going forward, what I'm really excited about beyond just contextual bandits is, you know, extending these sorts of developments to reinforcement learning and so, we have a couple really exciting papers that will be on archive very soon which are sort of about giving similar, very, like, general purpose plug and play algorithms for different, like, challenging reinforcement learning problems with function approximation. So keep an eye out for this. All right, and with that I will conclude the talk. So let me know if you have any questions, and thanks a lot [Alexander Rakhlin]: Thanks Dylan. I guess we can do a virtual clap. Any questions, please unmute yourself and ask the question. Maybe I'll start, while people are thinking. Can you comment more on the infinite action methods? [Dylan Foster]: Yes. Yeah, so I guess more on like just what is the setup for infinite actions or, like, what the flavor of the results is? [Alexander Rakhlin]: What type of methods will you be using? [Dylan Foster]: Oh, sure, sure, sure. Yeah, let me actually just do the following and share my screen really quick. Okay, so the setup for infinite actions is basically going to be the following. So, we're going to assume that the f of xa's in our class actually have the following structures. So, we're going to assume these can be written as something like f of x inner product with a right, where, you know, f of x maps to r to the d. Okay? And so the style of guarantee in this case is going to be the following. So, right, so let me say one thing. So this is infinite now in the sense that what we're going to allow is that each round will allow a to b in some set a sub t, which is subset of r to the d and, you know, we're not placing any constraints on the size of a to the t here. It can be as large as we want it to, but it's still, like, subset of this vector space r to the d, okay, and the style of guarantee will be that the contextual bandit regret will be bounded by root d times t times the square loss regret, right? So basically, the dimension d will be replacing the number of actions in the the prior, you know, bound that I introduced in the talk. Okay, and this is clearly a generalization of the bound I had earlier in the talk as well, in the sense that, if I take the action set at to just be like, you know, the set of like standard basis vectors. You know, this recovers the previous setting, right, and the d turns back into k. [Alexander Rakhlin]: Other questions? [Amir Tohidi Kalorazi]: I had more - kind of a more general question? So you mentioned these applications in medical trials? [Dylan Foster]: Yes. [Amir Tohidi Kalorazi]: I was wondering in this exploit/explore trade-off, there might be sometimes some ethical concerns - like, if you know some treatment works very well then you might not want to explore new things on people. [Dylan Foster]: Yes, that's very fair. [Amir Tohidi Kalorazi]: How can you fit into your framework, or how people have - [Dylan Foster]: Good question. Yeah, so the basic algorithm here is, like, not addressing that at all, and yeah, that might be a concern if you want to apply it as is. So let me say the following, though. So, the fancier variant that I was talking about in experiments... So, this is actually kind of starting to do something along the lines of what you're suggesting, right? So, in this one, what this is doing is it's kind of using confidence bounds to basically rule out actions that we know are not good anymore, so that we're not excessively experimenting with, yeah basically, so we're not excessively experimenting over, like, people that we've kind of like already ruled out that like a particular treatment is good for them, and we can show that actually, like, this new very - so this is, like, in preparation, but we - this does have like provable guarantees. We can show that this kind of optimally adapts to certain, like, benign properties of the data distribution - but yeah, I think like coming up with so -like, more refined guarantees about safety and so forth for contextual bandits, especially in the general setting, is like, very interesting, and so there's definitely been some work on trying to do this and trying to get near greedy algorithms for, say, like linear contexual bandits, but this is not been addressed for the general case as far as I'm aware. So, yeah, it's a good question. [Amir Tohidi Kalorazi]: Thanks. [Dylan Foster]: Yep. [Alexander Rakhlin]: Other questions? Okay, then we'll let Dylan go, and we'll have a virtual clap. [Dylan Foster]: Thanks. [Alexander Rakhlin]: Or non-virtual, if you want to unmute yourself. Thank you, Dylan [Dylan Foster]: Cool. Thanks a lot. 