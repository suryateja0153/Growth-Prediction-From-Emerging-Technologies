 [Music] okay now I have to do two more times of learning one is learning the structure okay up until now we have been saying the domain designer gives us the structure and we use this structure and estimate the probabilities using data but the domain designer could say I don't know the structure or I only know approximately the structure why don't you use the data to figure out the structure also that is for structure learning there are lot of papers that were written in structure learning in mm but it never became very successful but I will give you the general idea and the general idea is because structure is a discrete subject let us do search over this structure space so how do you do search over the structure space so I give you a structure based on that structure you learn the parameters based on those parameters you see how well it fits the data and then you go back and change make some local changes in the structure right and what do you mean by making changes in the structure suppose I give you one structure how can you make changes in the structure what operations could I do delete or add an edge very good so suppose I give you this middle structure like B e goes to a goes to C then I can define some operations that will change the structure when operation could be add an edge like I add an edge from E to C one operation could be reverse an edge like I reverse the edge from E to a one operation could be delete an edge like I completely remove the edge from e to a for each set of structure I can learn the probability parameters by maximum likelihood on M ap or whatever then I can compute some score of what is the probability of generating the data and so on and now I can keep doing this using local search can somebody see a problem in this it's hard to see but if you can see that will be awesome can somebody see a problem in this approach we should you be able to be louder very good so we should it says I put keep on adding edges by continuously adding edges am i increasing my parameters or reducing my parameters increasing my parameters if I have more parameters then will my optimization usually lead to a better fit or worse fit see I have more degrees of freedom so it will typically lead to a better fit so what would what would I do I will add an edge my fit will increase then I add another edge my fit will increase further I keep adding edges eventually I will have a highly connected Bayesian network with a human less amounts of parameters and I have what is called over fit on the training data but it will not generalize very well so in order to fix this I use what is called the Occam's razor or the KISS principle do you know the KISS principle okay so the case principle is keep it simple what we are saying in the KISS principle or it's also called the Occam's razor in the language of machine learning Occam's razor states that a more complicated model must pay a heavy penalty for being complicated so the degree of fit should become so much better that you are allowed to add another edge and we accomplish this by adding a score which is for model complexity so maybe number of edges exponential of number of edges or something like that you add to your degree of fit so now they trade off they fight with each other if adding an edge increases the probability it reduces the score for model complexity but if it really increases the fit much more then it reduces then it's a you should add this edge otherwise you should not add this edge and how do you trade these off all of that you will learn in machine learning goals so this is one way of what is called a regularization of the model this is a technical term which says that I want the model to be less complex more regularized okay now the last algorithm I'm going over this a little fast I'm sure you can see this but you know these are important nuggets of insight and you know these are the first time we're introducing this concept we don't have to go into the depth ok so the last algorithm and one of my favorite ones in this class so let us say I don't give you a full data let's say I tell you that you know the data is 0 1 1 1 0 0 1 1 1 and then for the fourth data point I forgot to check the value of B now this would often happen in the real world now you are trying to learn a Bayesian network you send somebody into village you ask people ok go and check the vital statistics of every newborn in the village you check their height weight gender etcetera etcetera and you forgot to check their temperature or whatever the thermometer was broken that day you wrote down the temperature but there was the rain and the raindrop is removed the temperature all of these things happen in the real world in the real world data is highly noisy which we are not going to talk about but data is also often incomplete what do I do when I have to learn from this kind of data now this is what I describe as a chicken-and-egg problem why is this a chicken-and-egg problem because if we had the chicken you would have eggs and if we had the eggs we can get chickens but the problem is we neither have chicken or the egg in other words if we knew the missing value if I knew the value that is question mark can I do learning that's what we have been doing this whole class maximum-likelihood man I may be whatever your favorite let's say ml you can do learning however if I knew the parameters of this model if I knew probability of a probability of B given a probability of C given me if I knew the parameters of this model and then I was asked to compute the missing value it's called imputing the missing value if I was asked to impute the missing value I would have to ask the query tell me the probability of B given a is true C is false and the model will use its probability distribution to give me some inference probability so if I knew the model I can do inference to figure out the question mark if I knew the question mark I can use learning to figure out the model the problem is I know neither of the two now this would happen a lot in your life I am telling you I have seen this ferreted you will always have two sets of problems one problem determines the second the second problem determines the first you don't know the answer either of the two in all such situations do what is obvious what is obvious what is obvious assume some initial value let's say of the parameters then you use those parameters to compute the missing value use the missing value to compute the parameters use the parameters to compute the missing value you keep going back and forth mutual recursion and that's it what's your name oh I am Telling You often the obvious thing is the right thing to do and the hard thing to prove anything about but in the case of this Bayesian network setting this algorithm is called the eeehm algorithm and you can do many interesting theorems about it you can also do that it converges and and under certain conditions the the estimates are really good and the reason it's called the e/m algorithm is because and when we are doing estimations when we are computing the question mark that is called the expectation step because you are doing inference you are doing the expectation of B given a comma C and then then we are using its value to estimate probabilities then we are doing some maximization because you know Arg max of theta and so that's called the maximization step and this now the slides show you the example let's quickly run over the slides so let's say I initialize the values like this and then I in the east step I estimated what's the probability of the question mark equal to one I use Bayes rule and that gives me the probability 0 if that value is 0 so then I impute the missing value to 0 if I impute the missing value to 0 and then we estimated the probabilities I will get these probability estimates then I reuse these probabilities to rias tomatoe probability the question mark is equal to 1 if I get the same value I have converged and I stop now of course this example was written so that it can fit in a slide in practice you don't do it like this first of all you don't start with probabilities 0 we don't like zeroes right secondly when you do the e step you will not get a value between exactly 0 or 1 you will get a value close somewhere in the middle let's say you get a point 8 then you do sampling so either your sample and you say a point 8 means I will say this value was 1 all you say that point 8 means that I will make point 8 of a data point which has value 1 and point 2 it of a data point which has value 0 and this is what is called soft TM the first case is called hard here so pretty much everybody uses soft here in practice because that converges better and then you convergence condition is video probability estimates don't move by much this algorithm is a very important algorithm and has a lot of really nice properties as I said ok so this is called expectation maximization you guess the probabilities of no ones with missing values you use those to compute the probability distribution over the missing values then you update the probabilities and then you repeat until convergence and this is guaranteed to converge to a local optimum so this is the summary of what we learned today if I know the structure and every data point is observed we just need to do parameter estimation we can do ml or M AP or what not if I don't give you the structure but I give you full data then you can do structure learning for example you can do local search through the structure space if I give you known structure but missing values you can do expectation maximization if I give you unknown structure and missing values you will have to first estimate structure then estimate do a.m. then we estimate structure then do M that's called structural m and then they are more harder they're even harder problems that I am NOT going to talk about the last thing I want to say because we are now coming to the close of this whole mix series on Bayesian networks is that Bayesian network is one of the many models it is the first model that we learn it has a lot of intuitive properties it's a directed model but then there are also undirected models when it is hard to figure out who is going to be the parent we often use undirected models an example of an undirected model is a Markov network and in fact Bayesian network and Markoff network makes some similar conditional independence assumptions and in fact you can convert a Bayesian network into Markov network by just Mary marrying all the parents and that process is called model ization so if you start with the first Bayesian network and then you marry all the parents like for example parents of BR X and C you marry them and add an edge and remove the arrows that creates an equivalent Markov network and that's process is called mobilization there are also models which have both directed and undirected edges for example chain graphs I also want to point out that there is a full theory about the kinds of models people find in different applications for example we have learned the most difficult or most general form of Bayesian networks the directed generative models but specific structures where let's say I have one set of parents which transition between each other and a set of observations which depend only on the current parent is what is called a hidden Markov model very famous model so right from the 80s a lot of machine translation speech recognition a lot of NLP applications everything used to be done using hidden Markov models the very simplest form of Bayesian network is called the naive Bayes algorithm where there is one parent and lots of children and it's a very important algorithm that you will study in your machine learning course so after we have discussed what we have discussed you will find many many frustrations of these models and you will find them to have much simpler characteristics than what we have studied in our class and of course equivalently there are non generative models like conditional random fields CRFs there's a general CRF the linear chain CRF and logistic regression which is the most simple form of C F these are heavily used in a lot of machine learning applications and this is where we will stop talking about this and we'll start talking about something else from the next class so thank you [Music] 