 um i'm especially excited to to introduce this uh this session uh originally when we were planning to do this in woods hall we were planning for this to be a whole day uh uh gathering for for several of our uh star alumni uh and we have uh sort of shrunk the whole thing now uh because of the virtual format uh two to one hour so uh the the people the the elegant people that you see here on the screen are some of our uh stars from uh from cbmm who have participated in the summer course as uh as da's as lecturers as uh uh alumni uh and i asked all of them to talk about uh their career paths and uh their experience with um with cbm their experience with working at the interface of brain minds and machines and uh people have different uh backgrounds some people are uh doing uh amazing startups some people are starting uh their their own uh loves people in industry in academia and in different different paths so i think this is a great opportunity for for all the participants to get a bit of flavor for uh some of the next generation of scholars uh in in in the field so i'm not going to interview i apologize to all of you i don't want to take a lot of time so i don't i'm not going to introduce each one of you so you have to introduce yourselves i want to thank you all of you uh thank all of you again for for participating and for joining us and so without further ado then georges you're you're next thanks gabriel uh hi everyone i'm i'm super excited to be here this is like a pleasure and an honor to be among like colleagues and friends and be back in cbm um so i i talked briefly about like uh tell you a few things about me and then tell you a few things about where i work which is x the moonshot factory um which is the company alphabet and then hopefully we can have uh more interaction during the q a uh so like i appreciate i'm georgios i joined cbmm breeding mats at the beginning of the center so that was like a 2013 like middle of 2013 and i worked in what we called then the thrust five which is the theories for intelligence and i worked in tommy podge's lab um both with tommy but also with lauren who's leading the machine learning class in this course i guess and so i've i've been like part of both this course and cbm from the beginning so it's really a great pleasure to be here a few words about my background my phd was on computer vision i i i looked generally into image representations like both analytically mathematical representations but also like learning-based representations and then most of the stuff that i did in cbm was around like three axis i would say one was was was extending a theory for environment selectivity starting from um computations and theory about the vendor of visual stream and and the idea was like to check if some of these ideas um extend basically two computational models for audition and and and for speech for example so that was one the second one is like trying to save some of these ideas because i actually put them into machine learning models in terms of like either hard priors as like regularization constraints basically or as weak priors as sort of like can we have additional weak supervision that will actually have environs and selectivity emerge naturally from the brokers of the network so this is pretty much the stuff that i didn't cmm and in 2017 i joined x the mushroom factory as a research scientist so i'm going to briefly go through a deck of slides uh about what x is what x is doing and i just focus on a couple of public projects that actually involve a heavy amount of muscle x the moonshot factory is actually the the full title of the company is uh it's a common it used to be called google x uh it's a ten year old company under alphabet which is the the organization and a number of other companies and so the the mission of it's basically what we're trying to do at x is to create radical new technologies that will help solve some of the world's hardest problems and that's what we call it a motion factory basically because this is inspired by the initial moonshot and the the term factor goes to the fact that you know exit is itself is a moonshine we're trying to actually formalize the process of creating moonshots and taking much from from an idea up to full blown businesses and the way we define a moonshot at least attacks is basically the intersection of three areas like one is there should be a huge problem that if the war if if it's actually solved the world would change radically basically so this should affect like millions even billions of people two there should be a radical solution that that sounds impossible but it's sort of like it's easy to imagine sort of like science fiction but we think we can do it and three there should be a breakthrough technology that actually turns into the day think about like machine learning or ai for example that we can actually put to use for achieving those goals so the simplest example is basically way more so right like the huge problem is that like 1.4 million people dying traffic accidents right so the the radical solution is like how do we get safely from point a to point b and the breakthrough technology is like how do we use vision sensors navigation and mapping in order to sort of like understand the world around us and so the process inside x is pretty much we start from like a large set of ideas on the order of hundreds and these are explored by what is called like the rapid evaluation team which is they're trying to sort of like figure out if there's a problem a fundamental problem with ideas early on and like from these hundreds of ideas a few dozens basically will survive and will become what we call an early stage project and these early stage projects we will try to sort of like build the technology and prove that there's actually feasibility both technical but like on business side as well right and and so this project will typically last for months and then we go to the phase where like the you know the idea becomes like a full-blown x project which is think of it as a small startup that you get funding you get the resources and you can sort of like explore this space for a number of years um so in terms of technology that that as part of this moonshot x has been developed in the past i i cannot sort of like outline a few things but you can find more on our website basically things such as like underwater sensing perception uh lending robots self-driving cars obviously smart glasses this was google glass actually i should point here that even google brain as an organization is a moonshot that was actually incubated in xylex delivery drones energy kites salt-based anesthesias life sciences very cyber security sea water fuel internet balloons and a couple of things right um and so just to give you an idea of like what we consider basically graduates from x like which is the product of x here are a few companies either under alphabet right now or enter you know as an external organization an external startup that actually be started from this process that x is trying to be so very in 2015 way more obviously the the best known example uh win and loom which after like the the morrison graduates and a few of like clean energy projects like malta and makani and dandelion um so i'm not gonna spend time on things you like probably everybody knows about so so way more uh is basically a separate entity a separate company under after since 2016. this started as a self-driving car project this was actually one of the projects that kickstarted x um and it's been successfully running for like um seven years now uh then um uh wing basically is a moonshot on like uh disrupting the way we move goods and like uh actually uh accelerating the way we move goods but also the way the environment uh perceives our moving goods around so i i mean 50 of the world's carbon emissions is actually generated by the transport industry so we think that if we can accelerate that process and we can move like for example food or drugs fast from from from at one point to another point and this can actually be you know emotion and it's it's uh like i said a separate company right now uh loon is another one of these examples where we were thinking about connectivity and and uh loon is a network of stratospheric balloons that actually connect and provide different access to remote arrays of planet uh this is also actually a business right now which is uh retaining revenue to alphabet um verily uh was a moonshot still is emotion basically on the space of like health sciences and life sciences one of the first things that they did was developing smart contact lenses with tiny magnet chips that can passively monitor basically the glucose levels um and so some of the the the sort of like the focus areas that that x is is in general interesting and these are like this is sort of like public information you can find that on our blogs uh here are like the broad things that that we care about we care about clean energy we get about food supply um computational agriculture as a way to towards sustainability um environmental solutions waste reduce logistics in general robotics and a couple of projects in robotics connectivity mobility and health these are a few sort of like public made themes and announcements that were recently made that i think are like more closely related to to this audience i'll just give you a brief idea about what is happening this is the everyday robot project it was announced a few months back and the idea here is to train robots from imitation from simulation and using reinforcement learning basically to to learn to function and to act in the messy real world conditions so like how do we move how do we how do we do for robots what you know a computer is doing a silicon in silico with machine learning for us basically and so some of the techniques that the teams have been developing for for making this robust learn is actually developing simulation so how do you train a system a learning based system in the cloud applying reinforcement learning basically on especially for learning from human demonstration like if you you can show to a robot how to do a specific messy task how does the robot go about and do that and collaborative learning where you sort of like you have a fleet of robots and then the the learning is sort of like jointly shared by all of them um another area of focus is basically like i said computational agriculture there's a number of blog posts on that what you see here on the left is basically a plant baggie that is collecting images of soy plants uh on a fleet in illinois and on the right you actually see detected soybeans with green actually and and these types of like systems algorithms but also sensing capabilities are able to do it sort of like in a superhuman way things that like we humans can do with the naked eye on the skin and accelerate um you know the um basically food production another one of these projects that actually a good example of like where perception is is actually applied this is a simple title that was also recently announced by x which is looking into sustainability and protection of oceans and and how do we think about um you know providing food and maintaining uh life in the ocean so some of the things that the title is doing as like a an early stage project is basically improving visibility underwater and tracking uh the behavior of like uh a feeling of fish on an individual level across time this is what you see here on your right basically uh it's like a fizz then in norway where the team is actually tracking individual fish and the way that they sort of like feed across time um so i want to close with with basically like like my takeaway of like the way that x is thinking about um making moonshots is like this sort of like five principles one is like we're aiming for like 10 times magnitude of impact and not just like 10 percent relative improvement on things um we like to work with the hardest thing first so even if if there are like more interesting things or like like low hanging fruits we sort of like try to tackle the hardest questions first because this will dictate if the project will succeed or fail then we try to make contact with the real world as as early as possible so we have to know what the world needs then we fall in love with the problem not the technology so we can sort of like switch and pivot fast and then we embrace failure as lending so we try to fail fast and fail so i just want to start by while i'm sharing my screen echoing what um your ghost said that i'm really happy to be here i'm sad we can't all meet in person but it's um really a big pleasure to be back today i'm going to talk to you about my lab at johns hopkins which is the computational cognitive neuroscience lab and talk to you about our general research program which is to look at the neural basis of different aspects of social vision so just to give you a little bit of background um about myself to start is um i also have been was around at just the forming of cbmm so i was a phd student in tommy poggio's lab so i was there when jorgos was there and i studied invariant object in action recognition along with andrea who you'll hear from soon and so it was really an exciting time to be around and see the center forming and everybody coming together together and these big ideas about how to study intelligence in a whole new way um and then i was really lucky that i was able to stay around and do my post-doc with gabrielle and nancy kimwisher and so my postdoc project really sort of evolved directly out of the cbmm mission so i i think i wouldn't have landed where i did for my postdoc if it weren't for cbmm um and then in 2017 2018 i applied for academic jobs um and i got a job at hopkins and then i a year later i started it so i've been here for just about a year um many of which have been sheltering in place um which has you know presented its own challenges but i'm now an assistant professor in cognitive science at hopkins um and so to give you a little bit of an idea of what my lab does many of you have probably seen this image maybe even in this course because i think i stole the example from gabrielle but even if you haven't you can probably pretty quickly recognize that it's a funny picture and the way you tell it's funny is not by recognizing the people in the image or the objects or where they are you have to recognize even more rich information about the world so you have to know physical information like the fact that when obama is stepping on a scale it makes it read heavier but you also have to understand a lot of social information like who knows what like the fact that this guy doesn't know obama's foot is on the scale but all of these people do right so they're all in on the joke and that's why it's funny um and these aspects in particular we call social vision and that's that's what my lab studies now um so humans are really good at this you can very quickly extract very rich social information when you look at people and we rely on it all the time um and these uh tasks are becoming increasingly important for computers and ai however unlike some things like object recognition computers are still not very good at this problem and so overall my lab aims to understand how humans extract so much complex social information from visual input and if you think about it this is a really challenging problem so that image i started with was so complex that there were multiple think pieces written about it so here's one i think from the atlantic right so many many thousands of words written by humans just describing this one single image and we can compare all the rich inferences humans get from this image to what a state of the art computer vision system can get from it so um this is from the google vision api if you put this image in you get out the labels event white collar worker suit business person gentlemen formal wear etc um so if you think about it these are actually pretty impressive labels from an object recognition standpoint right so 10 years ago we couldn't have done this but i think it still highlights the huge gap between all the progress we've made in understanding object recognition and humans rich visual social visual abilities um and so the overall approach our lab takes is to apply the methods that have been successful in helping us understand visual object recognition both from a neuroscience perspective and from an ai perspective and use them to understand social vision so in particular we do high spatial neuroimaging mostly fmri high temporal resolution neuroimaging meg and eeg and computational modeling to try to understand the neural computations underlying social vision and just to give you a small flavor of one project we've been working on so a lot of the lab really is trying to focus on how we recognize other people's actions and social interactions using um large data-driven methods so we've collected um curated a large data set of everyday human actions here are some samples of people dancing people working together on a farm studying children playing people playing you know caring for their children and we want to understand like i said in sort of a bottom-up data-driven way how do how do people understand these images so um we're doing this with a combination of large-scale behavioral experiments um neuroimaging experiments and modeling experiments to understand what are the features that underlie our ability to so quickly recognize the actions and social interactions in these videos um so i'm happy to take questions um later but just to give you a little bit more information you can go to our lab website or email me um so hi uh also really happy to talk to everybody like everybody else wish it was in person but uh it's been nothing so my name is bill lotter um i was a phd student at harvard in the biophysics program also um was around from the the start of cbm and it was a great experience uh i was advised by gabrielle and then also david cox and worked on various things in my phd ranging from kind of computer vision projects of unsupervised learning to dealing with ecog data and just kind of generally the intersection of deep learning and computational neuroscience um and so near the end of my phd so i defended in 2017 um i co-founded a company called deep health where we are building machine learning models for medical imaging um and specifically we've focused since then on mammography which is um x-ray of the breast to detect breast cancer uh and so still been working on that i'm the cto there we actually recently got acquired by a company called radnet so rednet is uh they're essentially the biggest radiology services provider um in the us and uh so they're they're publicly traded and it's a really exciting opportunity for us in that we get access to a lot more data but also the ability to deploy our software uh across um you know the countries essentially um so so we're excited about that still some things we need to do in terms of getting fda approval and um never ending list of things things to do um and so i'm happy to talk about that more uh deep health in particular uh during questions but i was thinking of what um also be helpful for for people who are thinking about you know kind of number of things and what would be useful to know and i thought it could be useful to talk about some of the things i've noticed going from academia to startups and kind of the biggest changes and um you know some of the pros and cons uh so let's see we've done a few things um so i think the biggest change for me that i noticed uh going from academia to startups is really in our industry in general is really there's been a lot more of working with bigger teams and kind of everybody working together for towards one common goal i think in academia there's there's definitely you know uh gabrielle's lab and labs in general have great lab culture and everybody you know there's lab meetings and you're talking but people tend to have individual projects they're working on and you might collaborate um and certainly it depends on the different types of labs and scale but especially at a startup kind of everybody has one goal they're working on and that was one of the biggest things is kind of learning to deal with all that and dealing with people and managing and trying to set up processes to work together efficiently um sorry let me turn off some of my slack messages uh so yeah i think that was one of the the biggest things and the amount of time that's necessary to kind of set up that infrastructure is is kind of non-trivial and something that i didn't expect um another thing that's uh you might expect on from academia to start up is there's a lot more widespread responsibilities so um you know from the data side to research to hr to you know product and of course it depends on what you're actually doing but um that was also one thing that it was a big adjustment for me um and i had to get better at is kind of this task squishing so um a lot of times in your phd you might go in a day you know you're just working on one project all day or a week or so on and so there's less maybe distractions day to day versus startup it's kind of okay this pops up i need to work on this for a couple hours and then other things um and so i don't i don't think any that's necessarily groundbreaking but definitely something to keep in mind and know um and another thing particularly um around research is uh so we we still do a lot of research um continuing to refine our models try to come up with different models themselves and uh evaluate the models in clinical settings but it's a different type of research i would say than uh sometimes what you see in academia it's more kind of uh direct application based of you know we're trying to make a product we're trying to improve performance and i'm sure this also isn't news to a lot of people but there is somewhat of a dirty secret in ai and machine learning and that a lot of times if you do it right meaning you process the data right you do the right hyper parameter searches you um uh you know kind of set up your testing and training splits right in all the kind of the details of image processing and pre-processing you can get a long ways just from doing that and implementing somewhat models that are out there correctly um you can get high performance and that's somewhat uh orthogonal sometimes so what makes a sexy research paper so uh you know it's not always the same thing to come up with a new uh you know model or method it it doesn't always align with if you're trying to build a product and you you have a deadline and you care about performance um so again it's more of a kind of a different type of research in academia versus versus startups i would say and not that one is better than the other it's just different outcomes and it ultimately depends on what you're more interested in um and a few more things so one thing that i i've been surprised with is i i've actually found myself enjoying some of the nitty-gritty product details and regulatory things um that you know if you would have told me four years ago in my phd when i was doing these the cool research projects that i'm going to be spending time reading fda reference documents and trying to understand how we implemented qmss and all these types of things i would have said that sounds really boring and i don't want to do that but now that i am doing it i've actually enjoyed it and it's fun learning new things and kind of seeing the whole picture of how we can plug in the research to actually having a product and how all these things fit together and you realize how important it is and and so again it might be a personal thing but i i guess i would say that some things that may seem boring uh don't always turn out to be the case and and vice versa so some things that might seem exciting a lot of times you end up getting bogged down in nitty-gritty details anyway so um yeah those are the main things that came to mind as i was thinking of how to convey some of the differences and ultimately uh obviously a lot of you will be making career decisions and you know there's no one way that's right or the other and another thing i would emphasize is it's never set in stone um it does seem sometimes hard to go back into academia and kind of if you take a break but there are opportunities especially with deep learning and machine learning these days you can do cool research and industry your startups and still publish and vice versa so um i think knowing that there is that flexibility is also useful to know hello again everyone i'm gemma roch and of course i am very delight uh delighted to be here as well it almost feels like a reunion even if it's virtual but still very happy to be here and i would like to share with you my past experiences my career path and also my research interest interest also how they evolved especially after being at cbm so as i said i'll tell you a little bit about my career path so i'm currently a professor at the department of computer science at the ghetto university so my background is actually in computer science i did my ph in computer vision and in frankfurt i started last january in 2020 so the phd that i did in computer vision i did it at tth in zurich and after i finished i moved to to cambridge for my postdoc where i started there and i joined the cbmm uh so there i guess it's where i uh so being at cbmm let's say it helped me broaden my perspective from science and also how to ask bigger questions how to be in in projects with multidisciplinary teams especially with cognitive scientists which before i i didn't have so much the opportunity to to work with and so yes i have to say that uh it had a great impact in my career and it was a kind of a career change path in the research direction that i had before and actually while i was at cbmm i also helped organizing the summer school and i did some ta as well so i was at cbmm for three years and after that i moved to singapore to start my own group and then after a couple years i am now in frankfurt where i have a professorship position there so as i said while i was at cvm i started collaborating with cognitive scientists tackling questions not only on how can i get my model to perform better but also more related to how can i design more bio-inspired models or how can my models have also some cognitive abilities like humans uh and and which are the computational implications uh from the model site uh so in this sense uh i'm interested both uh so here you can see a little bit uh in the computational modeling and how can i use these models also to uh relate to the to the human brain and the links in between uh the two disciplines of course i'm always more linked to the computational side because my background is in computer science but i always work in very tight collaboration with neuroscientists and psychologists to answer the questions from the human perspective or the human intelligence perspective as well um so just to give a little bit of a sense of things that we are doing in my lab so today i will present a couple of works more one that is more only computer science and the other one in the interlink so uh i think both are kind of related to these brains minds and machines research topic that i think you all care about and so here in the slide you can see the uh the picture of one of my phd students who conducted most of the work together with other members in my lab and actually he was in the summer school last year uh he was lucky and he could go to woods hall and i think he had a very good time and enjoyed very much uh so here uh in in this part of the works for instance uh what uh what we are studying is uh how different models that are trained for different specific tasks and different tasks i mean object utilization or death prediction or other kinds of predictions that you could do in an image and in this case we only focus on images but it could be extended to other modalities so how are these models related between them and if we know these relations if we can find a way to select uh the best model to transfer form if we have a new task or a new domain that we want to to learn a model um and with this we are also working on extending these works towards multi-task models and incremental learning and then from another side we are also interested in using such models so for instance a set of um in our case we are working now in the annams but this is i guess now kind of a detail but it's important because these are state-of-the-art models but if we have dnn models that are trained on different tasks can we use these models to unveil and explore the the functionality of different brain regions if we have brain data for instance uh and just a little bit also of another project that is very related to this that i have in collaboration with otolith or oliva who might think you uh i saw yesterday or at least i saw it in the schedule and also with uh radika sihin who he was also at mit but now he's in berlin so we have this project in collaboration that's collagonauts project that's also going into this direction of explaining the human brain uh with algorithms or with computational models um so just to say that if you want to know more about what we are doing in my lab so you can go to the website or you can also send me an email i always like to collaborate with people i currently don't have openings but i might have in the future so uh keep tuned my name is andrea i am a senior research scientist at deepmind and i lead a small group of researchers that investigate learning in a multi-agent environments so what i would like to do in this five minutes is to spend a little bit on why i'm working on ai why i think it's why i decided to make a career out of ai uh give you if and then i'll give you a flavor of the type of research challenges that um we tackle in multi-agent uh learning and then i'll give you a specific example of the research recent project that i thought was very exciting so my fascination with intelligent machines began early but the decision to make a career out of it came while i was working as a model engineer as a control engineer at um at cern um so what i did there after graduation uh was that we took um there was a control board that controlled a specific part of the accelerator and this board every now and again failed and so it had to be replaced and when you replace this control board a senior engineer had to spend a few hours tuning the parameters of the new board because of changes in manufacturing details and stuff like that and so what we did was to take this board and make it smart what that meant basically is that you could just come in plug in a new board the new board would perform a safe data collection on on the system collect the data and then learn good parameters for itself the whole thing was relatively simple in hindsight but what stuck with me is that the new uh the new system took about two minutes uh and it required no human intervention and so we freed a senior engineer of you know hours of tedious work um and this was work that they absolutely hated and we're ultimately not very good at and so this learning system basically allowed a human and machine to really play to their respective strengths if that makes sense so i decided to uh try and make a career out of ai i joined tommy fargo's lab for my phd and i then moved to deepmind after graduating and so what i work on now is learning a multi-agent environments and i'd like to give you um a flavor of the challenges that we face in this setting which is going to be interesting without a slide but let's give it a go so imagine that you are an agent and you're trying to figure out how to play rock paper scissor and you have a relatively simple algorithm you start by playing a random action so randomly you choose from rock paper or scissor if things go well you start doing that action more if things go poorly you will do that action less often and let's imagine that in the world that you're allowed to train in uh people only play rock so you can imagine that if you play paper things go well you start playing paper more if you play scissor please go poorly so you'll start playing scissors less and eventually you'll play always you always play paper then you're done with your training you go out in the real world and in the real world there are other intelligent agents that also learn and so what these intelligent agents will do is they'll quickly figure out that you always play paper and they'll stop they'll start playing scissor all the time and your performance will absolutely tank and what i'd like you to take away is that there is nothing wrong with the environment it's just rock paper scissors is a silly game and there is nothing wrong with the learning algorithm what i described as reinforcement learning policy gradient the problem is that you as an adaptive agent are paired with other learning agents and so effectively you're chasing a moving target so this is kind of like the flavor of challenges that arise in in multi-agent artificial intelligence um and so i just wanted to highlight uh a recent piece of work that uh i think i uh recently finished with with my team on scaling up this type of stuff on very complex uh games so we tackled this uh board game called diplomacy some of you might have played it um so in this game there are seven players that control units on a map that looks like europe at the beginning of the last century and the objective is to capture territory effectively what is interesting about this game is that um in order to make any progress towards victory you have to form alliances with others you have to make friends and work together towards an intermediate goal however ultimately players have to stand alone to win and so the fact that our agents got very good at this game we beat the previous state-of-the-art by substantial margin this is an environment that has been studying since the 80s so it's it's kind of like a challenging thing to do we have evidence that our agents effectively understand the incentives faced by others and are able to forge alliances with other agents work towards intermediate common goals and then go about their business again and so this is really like was the core of this research and i just wanted to finish off by highlighting that multi-agent research is not only wide open we know very little about what to do in this kind of scenario where you have other learning agents but it's also extremely relevant for the real world you could imagine um on a phone today you could have a learning system that learns to save battery let's say and another learning system that learns to download youtube videos that you will watch and enjoy and it free downloads them on your phone so that you can watch them super fast you it's easy to realize that these systems have to come to an a to a head at some point so in this case they are competing for the control of the phone who is right saving battery or downloading the video so currently we have no idea how to solve these problems and that's why i'm so excited to work on agents that identify common intermediate goals and then are able to collaborate towards them effectively so this is the kind of stuff that i work on i'm very happy to answer any questions about this or anything else deep mind related in the q a thank you great thanks hanlon you ready yeah i'm here can you hear me yep okay great uh hi everyone it's great to be back if not virtually in woods hole for everybody my name is hanlon i was a phd student in gabrielle kreiman's lab and uh ta the summer school i think for the first two years uh i thought that it was around um and uh after graduating in 2015 i joined a deep learning startup called nirvana systems at that time i think deep learning was still not very hot as we actually had a lot of trouble raising venture capital funds thankfully we were able to um and we were a startup that was around building custom chips for deep learning and uh we're eventually acquired by intel where i sit now i now run an ai research lab with an intel so basically managing a research team that's spread across i think we have california arizona and in israel i think what's exciting is that we do both ml research but also ml applications and hardware design so maybe i'll spend the next couple minutes covering uh briefly two research ideas and machine learning that were that we're super excited about um and if i have time maybe some of the pros and cons of research in academia versus an inner corporation so the two that i wanted to talk about uh was so one is i think we're observing this this tension in the research community between uh very large industrial labs that are trying to build larger and larger models so you know the recent trend in building billions or even a goal to get to a trillion parameter model and then believing that you know magically some agi-like properties will emerge on one side but then on the other side i think there's some emerging work if you look at some of the work that's happening in the lottery ticket hypothesis or neural tangent kernels or in generalization or deep learning that's suggesting that this kind of brute force sgd with large amounts of data is not the right way to train these networks um so we're super excited about trying to find alternative training methods so for example can we analytically solve a subsection of the network and actually and do just a minor additional training on top or can we exploit we know that in linear networks they kind of learn the singular value of the covariance matrix the large values first before the tail of the distribution um can we exploit that to shortcut the training process um can we exploit this kind of these recent very intriguing findings from lottery ticket analysis to build more efficient training methods i think this class of research we're super excited about because we're kind of seeing people step back and say okay just throwing gobs and gobs of flops on training an extremely large model is not a sustainable path for for our community and maybe the second track that we're really excited about is around data data-driven optimization so uh we're seeing in in industry and and in the in the community a lot of traditional applications i kind of use heuristic-based approaches or bayesian optimization methods where data-driven optimization such as dprl can have tremendous impact so um this can cover everything from building uh learn compilers uh where instead of handwriting heuristics you can learn policies that govern automatic of operation you know mapping of operations to say the different memory caches of a chip two things like systems tuning so can we build an rl agent that runs on everyone's cpu laptop constantly tuning a lot of the hidden knobs of the cpu to kind of maximize your gaming or your word processing performance and i think the challenge we face that the in academia they are all research that we're observing and we participate in um those environments that people are testing on are missing a lot of the key capabilities that we think can really enable this future we're talking about scenarios with extremely large space states action spaces uh beyond what existing environments permit today uh we're operating often in environments where explainability becomes important um and the need to understand why these models are making these decisions and importantly i think multi-task generalization to kind of unseen environments is another type of key problem so our general attack i think is we have a lot of university collaborations to try to tackle some of these things and then turn them into actual products or algorithms that can be deployed on hardware or as tools to realize this future where we see rl agents not just running in simulation environments but controlling physical hardware and i think maybe lastly i think i think just what's intrinsically exciting for us is that one of the beauties of ml is that the same models so for doing something on rls for rl and graph neural networks um can apply to a whole host of different applications and that's what we're really interested in going after pros and cons of research so you know after i left my phd program um after i finished i thought i wouldn't have to deal with the rebuttals anymore uh apparently that turned out not to be the case uh which i think is is one kind of constant theme that's very challenging in machine learning research currently um and but i think what's on the exciting side is the applications um and instead of i think in grad school i had the privilege of spending uh four or five years working on one or two problems um here when we're searching for applications of our technology within a very large company uh as just a lot of creativity that we see um from domain experts that i never was able to interface with when i was in grad school um and i think it's that exploration like that's really exciting for me you know sitting down with people that design chips and say how do you do it what are the what are the pain points how can we solve your problems um and then translating that into hey what's missing in reinforcement learning today that can enable that uh for me at least in in um working for research and large companies um that's what i find super exciting so i'm very honored to be here um i'm ebay i'm ceo co-founder of at ic um i was a postdoc with professor josh tenenbaum and uh i work with chris baker who was previously a phd and postdoc with josh as well at that time we're working on collaborative and cognitive robots so we are interested in enable robots with the basic common sense of understanding humans so you probably very familiar with this video so people have played play this video a couple of times but that's what we're trying to to to do at that time so basically this young child uh without any uh supervision without any uh uh like large data that the child the first time be able to autonomously identify the intent of the adults and be able to find a way to help him which is amazing right um but look look at today's robots um um or today's ai so they are working on uh they we made tremendous help uh tremendous progress uh on various domains but most of the successes are with uh finite rules and uh limited domains um also those robots they are very powerful um but most of them are separate from human working space um so because uh they cannot understand humans and human cannot uh humans don't think they are safe so um when i did my research um at the cbmm with joshua i started to realize that there's a major gap between the promise of ai and robotics to what do we would like the robot to do um so i'm imagine a future that why not we cannot have billions of robots populated around us to help us do a lot of things um so um yeah and then that time so i i i told josh that uh so i have idea i wanted to start my own company and josh was surprised to say oh um oh that that's not what you told me so when you joined the lab but i i uh at that time i said i just find that it's actually more um can be more impactful um of what we're doing so that's how we start a company um so we started look uh working on autonomous driving um the reason that we work on that is um try to think about that trying to find a use case that uh ai have to navigate uh in an unpredictable world right rather than very well defined um environment so um today's um like computer vision machine learning robotics has advanced a lot but until today there's no company can really build a product that be able to drive reliably safely in the open environment so what is the challenge i'm just keep thinking about that question so the way that i'm thinking about that is if we imagine a case if we completely remove humans from the street just an autonomous vehicle only driving an empty street and the problem should be solved with today's technology right um so but we cannot do that uh that also tells us that the humans um pedestrians um other drivers are the actually the most uncritical part of the environment and also biggest challenge for town's driving um so that's why how we start the company so you can see those are the vehicle platforms that we have built today um just show you uh briefly some of our progress uh this is our suv navigate in a busy street um so we're trying to find a gap to change lane here we go this car didn't yield to us we keep trying and we did it right so the sophisticated part is not only like detection of cars um um and localize ourselves but more importantly be able to very sweetly recognize other people's intention and make a quick decision by considering the sophisticated multi-agent or games theoretical scenario we also apply the technology to the long-haul driving by enabling the semi-truck moving loads for customers between dallas and houston tens 10 hours of driving it's pretty intense right so during this 10 hours a lot of things can happen so a lot of people cutting in front of us merges um tunnel and the constructions um there's a lot um and we also enable our vehicles not only driving on the highway but enable them to navigate in the busy uh let's say warehouse environment so here is we set up a useful user interface for our customers just like the uber app right so and the customer to say okay i want to move this trailer and then truly have to navigate in a complex situation you can see this i stand in front of the truck and the truck can avoid me and navigate back and forth and park the trailer to the target spot that customers ask us for it's very exciting uh so working in the real world and exposed to all the uh challenges in the real world and uh that's just a question of doing so active hiring we als we have a lot of positions also in term positions uh our goal is to create a future where autonomous machines can strive alongside humans and seamlessly uh safely integrate into any environment so we are uh fortunately backed by mit engine which is mit venture capital fund initiated by mit president um we just stay at the um the central square uh right next to mit just between mit and harvard and last year we raised our serious ale um funded by funders fund so well we're very well funded uh we're growing very fast so if you're interested anything related to ai and toms driving feel free to reach out to me that's it thank you so the question is who has a favorite memory about the cbmm summer school um man i have a lot it's it's such an awesome experience academically which i'm really happy you all are getting to to see like just the combination of all of those speakers experts in cognitive science neuroscience ai it's so unique and awesome very intense but also very fun um but it's also i mean i'm sure as you've all heard it's in a beautiful place at a beautiful time of year so a lot of my fond memories are actually a beach related too it's hard to pick one unfortunately my favorite memories are also location related i mean academically as i said it was it was amazing as well um i really enjoyed seeing so the first year we did it it was only two weeks i believe and it was the first time the course was organized and you know we didn't have people like chris to help us streamline it let's let's put it that way i guess um and so but what i really enjoyed seeing was all the students did projects and they all came together in like two days at the end and i really enjoyed just seeing them all working on these really unique cutting edge problems in like 48 hours and coming up with these awesome solutions based on everything they'd they'd learned in the course so far so that's probably one of my favorite memories i would mention i can i can mention two basically one is like probably during the first year where we like everybody was living is in this big house basically so like this student and like tas it was like uh it was fun basically we had like afternoon discussions and drink afterwards and typically you would see like late at night i don't know just with a couple of students or like tommy with a couple of students like chatting about like the future of ai basically so that was pretty fun um yeah that was it yeah and i think also the sense of binding with all the people there because as leila said is quite intense and when i joined it was already three weeks so you are for three weeks with all these people together 24 hours learning uh the same topics that everybody is excited about and doing projects and also having fun so i think at the end later on you meet in conferences or you even meet occasionally and it has this sense of i don't know reunion as if we were best friends since forever so i think this is yeah this is really fun it feels sort of like summer camp for adult neuroscientists and also i would also mention the third year i was that was the third year or we actually had the icab shipped from italy yeah so we have the i-cap on on cbm and like you know students can put in their act they would sort of like develop projects around it and they could demo it so that was really fun basically i think part of it being an encapsulated course is giving myself the time to learn about all these other adjacent areas in neuroscience that i just kind of when i'm in my lab and kind of siloed i don't think about those things and cbm really opened my mind to i mean this whole area of child learning and how that may correlate with how ai and ml learned it was absolutely fascinating to learn that was a great experience for me all of your stories are super interesting one particularly for andrea how do you think symbolic manipulation could be built in via inductive biases also what's the correct level of synergy between hard-coded engineering quote-unquote nature versus online learning nurture um i'm going to take this one i guess uh yeah this is a it's a great question um i think how do you so let's do the both these are technical questions how do you think symbolic manipulation could be built in via inductive biases um so obviously i don't know that's why we need people like you to ask these questions and you know actually work on them and solve them there are a few answers out there so one i've directly worked on so these are these relation relational reasoning models like uh graph neural networks that are you know received a lot of attention and have recently scaled to like thousands of particles and so the idea there is that um the hard the broad strokes of the dynamics of what you're observing in the input is due to relations between discrete entities and so this is kind of like one of the ingredients for symbolic manipulations and these are really cool models where you can take the best of both worlds in terms of symbolic reasoning and uh the power of deep learning uh for you know the power of deep learning sounds weird but you know bring to bear all these modern techniques uh to learn from data um so i think that's an interesting place to start in terms of things that have not been done there i haven't seen something that uh taken input data uh tells you what the relations that exist or do not exist are so i think this could be really interesting for uh the stuff like layla is thinking about to make some connections um and so yeah i think that's the biggest open problem like given just just raw data how do you infer which relations exist and which do not exist and then the other piece of the question and i hope that people jump in after i'm done pontificating what is the correct level of synergy between hardcore and engineering and online learning i don't think there is a one-size-fit-all answer to this i'm sure you were expecting this it kind of depends on the problem i think biases and hard-coded structure in your model will bring your sample complexity down if you don't know what that word means just wait another hour and lorenz will give you an earful on it it just means how many examples you need to see to achieve a certain level of accuracy in the real world and so if you don't have access to a lot of examples encoding a lot of knowledge about the problem for example encoding which relations exist and which done exists in a graph network will buy you a lot in terms of accuracy however in a world where data is cheap and abundant it's i think nobody has a good answer on what the right level of synergy is yeah thank you for design these questions they're really cool hopefully somebody else can chime in or move on to the next what if the hardest problems cannot be solved by ai yorkos i think this might be targeted my short answer is probably that they're not going to be biased by ai period right so that's we know that but like it's it's it's similar to how we try to think about the brain basically so it's not that like ai would sort of like crack open a few of the big scientific questions or like what problems we have right now but it will help us understand how to get there basically so similar with the you know the strides we've been doing through ai to understanding natural language basically or understanding vision or understanding speech so so the more of this or sort of like different fields we bring into play the more we sort of we know if some of the approaches that we try to take like even mathematical approaches or scientific approaches actually make sense so from the from the point of view of like how do we tackle the hard problems in general i mean ai can be just like one technology that is actually useful and and efficient right now like we don't know what the next thing will be um but yeah but i mean there's no there's no sacred cows or anything that's nice i would like to provide a very interesting perspective um which is uh what can be solved by ai so we need to define what is ai right so and i feel the interesting perspective is ai is an evolving term um so 50 years ago like when the let's say dishwashers what uh washing machine invented they advertised it as an ai and the people are so worried that because so many jobs will be replaced by those dumb this this intelligent machines right but uh today we don't think so right so we think this just machines um i think 50 years later ai will have is an it's a new definition and uh people will think self-driving cars all this it's just on machines right so the car is supposed to drive by itself uh so why is why is uh why is something uh new also um and then it's as the ai is always having some more border uh or boundary um i think um it will it will keep evolving and solving more and more harder problems okay so the next one is what are from your perspective the most important skills for an ai specialist aside from python linear algebra and machine learning uh so i haven't answered this this that's okay with everyone so i think as ai and machine learning become more standard components of your of you know any cs or you know stem degree it's really going to be about the next thing that you know you know like a lot of people will be highly qualified in terms of you know python linear algebra statistical learning hopefully deep learning reinforcement learning this will be like standard stuff so it's what can you add what do you know on top of this um so i think especially for people who are approaching uh you know college or grad level education now i think that's really what you should think about is what what slice do you want to focus on if i may i think we come across a lot of folks that as andreas said you know very good at using tensorflow to build models and such but um one thing that we found really valuable people that can go a little bit underneath the stack a little bit because especially in research you often come up with i can't see how many times been bottlenecked by a new motif that we want to build but it runs too slow and we have to delve into the inners of the framework to figure out how to get it to run fast and i think that level of being able to iterate is much more valuable than kind of just kind of stacking or weaving existing components together so that's the first bit i think the second bit is that at least from my perspective ml is still relatively new in the sense that there are folks that are building the tools that are all can be very valuable uh compared to people that are using the tools um so things like contributing to the open source community and in pi torch or tensorflow or hugging phase um demonstrating those type of things i think are extremely valuable yeah maybe i would like to add something on top of this which i completely agree which is also have a bit more of a philosophy perspective thinking about what is an important question to best solve with the eye and not just going for whatever challenge is available and try to crack it and i think here also having the skills of or the ability to be able to work in a multi-disciplinary team also is kind of related to to this kind of skill that i set like thinking about what's next or what is important yeah uh one thing i'll also add is um you know in a lot of projects you're dealing with uh large amounts of data and so developing intuition of how to process data how to clean data um how to visualize and kind of understand of what to look at when you're looking at data it's somewhat of a amorphous task but i think as you gain more experience with that that's super helpful no matter what kind of problem you're trying to solve so these are basically excellent answers so like i agree with all of them i would just start on top of it that don't be scared of like you know going in deeply a bit into theory on some of those things because that would be like both adding up to your skill set like like andrea i was saying and i'm understanding you know your data and your problem better so like don't be afraid to go into like statistics more um information theory signal processing uh control um things that will be like part of your broader skill set even neuroscience basically um part of your broader skillset but will help you understand a lot why what you're building and how you're building it why it doesn't work when it doesn't 