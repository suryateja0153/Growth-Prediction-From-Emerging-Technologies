 good afternoon everyone our topic today is user behavior hashing for audience expansion and we are from Samsung or brief legendre today this is Praveen and I'm director of engineering at Samson research America and I am going to be joined by my colleague or co-presenter in nan who is our lead data scientist at Samsung Research America together we are going to cover our topic the user behavior hashing for audience expansion so at first I go and provide you a high-level overview of Samsung what we do next I follow up with I'll provide an architecture over your Samsung audience platform and I will introduce your look-alike modelling then in then would cover the rest of the topics with respect to the specific deep dive into how we accomplished look-alike modelling using hashing techniques and then we provide benchmarks and model performance and then we can go through some Q&A so we are Samsung we are global company we have more than 300 thousand diversified employees and we also have global revenues to the tune of 220 billion dollars and more importantly we are also operated at global scale and we have 35 R&D centers across the globe and our group specifically represents the global research group Samsung Research America and we are going to take a little bit deep dive of that so samsung research I mean these are some of the core areas of research for us they start with artificial intelligence this is where we focus more on hardware as well as software side of AI and then followed by data intelligence given beard Samsung we have huge amount of data sets we also focus on our 5g and 6g all the technological advancements related to mobile then we also focus on robotics and Tizen is our operating system for TVs and last but not least we also focus on next-generation display media so our specific example today is more deep dive into the next generation this and media because we talked about whole lot of data that we at Samsung we handle on day to day basis having a robust audience platform is absolutely necessary to handle these huge amounts of data so with our audience intelligence platform we focus on some of these core areas we work on recommendations we work on using modeling techniques and we also work on multi model techniques related to voice and vision and all of these are also powered using our AI experience so as part of our audience platform we work with both first body as well as third-party data sources so first body includes the data that we collect from our consumer electronics devices that include TVs mobile phones iwata devices etc and when it comes to a third party data it is obtained from TV networks ads and third-party device graphs that we have and so forth now what we do is we bring our first party data as well as third party data together into our data platform so we proceed by following the steps of ingesting the data first using our batch as well as real-time data processing then we store them because the amount of data is so huge be stored for several months of data here once you ingest the data we basically have our machine learning as well as deep learning platform as part of our ml and DL practices these are some of the algorithms or approaches or problems that we solve on day to day basis those are related to look-alike modeling problems related to recommendations in personalization and again some of the problems related to optimization as well as attribution especially those two are some of the problems related to the advertising space and last but not least some of the problems related to fraud detection and also use natural language processing in order to process speech voice and all of those now what is also supported by a a million DL platform is also a model management framework and also our experimentation framework will be run bunch of emitters on day to day basis but once you have this AI or ml platform we are going to have the data visualization as well as api's now when you look from the right side of the picture it is primarily the business applications so through our platform we support these core businesses so Samsung ads has been one of the tremendous in truth drivers so this is the platform that basically supports it alongside or Samsung marketing and we also have lot of recommendations as well as personalization that is enabled on our TVs for our consumers and many of those use cases related to multi modal and they work so those are the core business applications that are being supported as part of this platform now if you focus on the bottom of the screen there are five different stages first we start with audience discovery this is where we ingest all of our data from from all the devices the user interactions and whatnot the next phase is to basically do the high level segmentation understand exactly these audiences are coming from think about demographics location etc the it is followed by audience expansion today we are going to focus on this as part of the rest of the session once you expand the audience you would then be able to drill down into how you want to tune your audience to focus and target the some of those specific campaigns and last but not least how well you have targeted is measured by the audience measurement techniques especially attribution let us talk about look-alike modeling from Samsung context so as part of the local ID modelling we have two different goals that we want to cover today first goal is how we can improve our incremental reach and second goal is how we can improve our targeting then we are talking from the perspective of two different use cases the first use case being TV networks the second use case is for Samsung new TV purchases so let's actually dig be in TV networks what I mean here especially when it comes to name shows that air on different TV channels during fall premieres and so forth how can you really go and identify some of those new audiences that replicate some of the behaviors of your existing audiences so this is where you think of you know what I have context of certain type of audiences and how I can make use of the user context in Havana how I can expand it to potentially identify newer audiences now the same approach or methodology is similar for new TV purchase as well considering the existing TV universe understanding who is already an existing 8k or for calculating the owner understand from the perspective of what type of user behavior they exhibited and how could you really make use of it to find out who are your potential new TV or cheezer's so these are the two main goals that we can potentially solve by using our user by using our look-alike modeling techniques when it comes to our approach as part of Samsung we have ACR viewership data which we basically have from 50 plus million TVs in us and by applying user behavior hashing techniques we wouldn't be able to identify those TV viewers that are similar to existing audiences based on user behavior so let's look at let's look at our look-alike audience expansion example so on the graphic on the left so imagine the full circle is our entire TV universe here we are specifically talking about how can I find those audiences that are similar to my existing TV owners so as you notice the small circle on the left which is a so a is my seed audience now I want to find out those audience that are similar to that seed audience of a and this is where it is highlighted as the dotted circle now if we apply all of this to our hashing technique what we would be able to determine is you would be able to determine the circle B which is an expanded segment of a so using this you would be able to figure out given it a seed audience a how you would be able to figure out and identify the expanded set of B to actually go into more details and how the hashing technique itself world my co-presenter in n would be taking it through from here thank you for being that it is my great pleasure to talk about one of our previous works not related to audience expansion using look-alike technologies in this particular task we are trying to find a ranked group of similar users as you know Samsung has a large scale of a user base including Center Smart TVs and mobile devices and the the user interaction data scale is huge all this the data the user interaction data are related with something like contending a content consumption video on-demand consumption gameplay application usage your interaction with external devices because of this a large scale of data sets is that it's actually pretty important for us to derive efficient algorithm to to find the similar users although this discuss can be run offline we still want to limit our resources spending on this particular task there have been a lot of works in the industry to solve a similar problem in large scale to find the ten years neighbors nearest neighbor search this one typical approach is called a locality sensitive hashing LSH and there are also techniques related to finding similar users in recommender system however this type of approach sometimes they don't capture the users behavior change efficiently they also cannot capture the two contextual change one the user have this kind of interaction with the devices so to solve those problems we have to defy a very efficient hashing methods that can capture all the contextual information and preserve so at the same time preserve the similarity between users so that we can actually generating a bucket I as a user search space that user we can search find the emitter the most similar users in a timely manner so this is the high level workflow for these particular use case we mentioned first of all we collect data from a sense of first party and plus some third party data and then we do some pre-processing on these of behavior data then we can get it into the deep binary hashing model that were talking about after running through this de pasión model we will generate heated heterogeneous hash hash code for each users this hash code is efficiently it's efficient to be used in fast search for the particular look-alike use case we can use the seed segment of users and by bucket hiding the magnetizing this user hashing code we can implement a first search algorithm that can you know very efficiently to find similar users in so that we can expand the audience even in a line of your time a manner but the training process and the hash code generation process we will run it offline this slide shows the high level a training flow that we used in the hashing process the training is based on user pairs by utilizing external knowledge or predefined user similarity generates two users raw input and this encode will be input it will go through our network layers these that were layers will be explained in the next few slides after we learn to use representations from these network layers there's a - layer to specifically generate the heterogeneous hashing hash code for each users after the sine function we mostly sigmoid we use here then we can predict whether the users are similar or not for to solve this particular problem we try the different network architectures these are the workflow we talk about is not specifically to just one a deep network architecture we can actually try a different architecture once that you can design to fit into network for so today I'm gonna talk about two of them the first one we caught it's a time where we're attention CN n model in this in this model we have basically four layers the Inka layer is that is the data pre-processing way it's like a map the maps the sequential behavior data into a 3d structure that can be processed by convolution neural network because the behavior data is represented by a user interaction with items the first step we do in the pre-processing is to think that each item into a vector support in the second step we do is factorization eyes users history into a different time a unit for each session we actually accurate all atoms that the user had interacted with using the embedding of the items generate in the first step and the the you can look at the the the 3d image down there the H exercise access is actually the the short-term time unit we to find the W axis is the long term Mitra midterm to long term behavior of user time units the the D axis is representing the user the different Manning's for four different atoms in this embedding layer usually it is hadn't made it it means that it carries more actually consumption information than similarity information that we itself this actor would affect overall performance of this a TNC in the model especially in its ability to preserve the similarity information so to overcome this situation we introduced a embedding layer as part of the model this embedding layer applies a specific design evolution kernel so that in transfer transform the previous layers are put into a adaptive distributed representation the next layer will be the Tama where attention layer this this particular there is used to abstract time and wire tension features in this model this layer separate attention features into a short term midterm and long terms short term features or features abstraction that emphasizes users a small time scale in a small time scale maybe a day maybe week a long term features capture a longer term maybe amount of season of seasons kind of a time range that we try to capture users recent activities in long term preference at the same time the last layer will be the aggregation layer aggregation layer all features from the previous layers will be flattened that content identity together in this layer this particular slide explains the description of each layers are we mentioned previously inside the TAS in model we also introduced another deep neural network that is called a category attention model so because the users contact sumption behavior are mostly related with with general information of content or type of the information of applications this category potential model tried to catch different preferences on user forum user on different genres of content so to efficiently learn the crack user representation from sequential behavior so we want to build a hybrid attention network cut the category category aware attention network so this proposed model uses a attention in Canada from the user behavior history grew by the item category category information from the grouped list this attention Network and this act can discover in part items which are useful to represent users by their preferred preference the reason we choose the attention Mercer's of other networks such as lsdm rng are you it's actually because of the attentions secure performance our networks actually have also composed all four layers the first one is the sparse include layer that captured the user that they kind of need the users interaction interactive history with items and the group them by category the embedding layer that learns from these item embeddings and the attention layer that computes the weighted sum over atoms inviting quick category and finally we can buy all these years above this a metadata tension layer the output of this particular layer represent the users embedding as a result the final user embedding which contains users reference with a good reflection of the long-term historical behavior patterns due to the large scale of the date the nature of the data we process although this similarity preserving fashion can be calculated offline but we want to limit at the time to update to for each iteration so that we can update the users hash code more frequently to capture their recent behavior at the beginning we have to use the when we run the inference we use the Python UDF and clearly it is a very efficient methodology because it's dressed as a row at a time manner the good thing is spark introduce the pen as UDF from future issues back and when we are doing that this particular world we tried to utilize the pandas UDF group map functionalities so henna seriously actually the one of the biggest performed performer booster from spark so the they actually performs more than hundred percent times faster than the traditional Python UDF the utilizing the Apache narrow arrow I change data very efficiently between the Java Virtual Machine and this Python drivers so in the next slide yeah I'm going to show you this slide shows the kasnian that how we run the Tennessee DF in a group map group map matter so as as mentioned in the Keynesian model all this a user behavior data was reconstructed into a 3d animation and we treated them as images and feed this into the convolution neural networks we write this a group map function and then so that for each particular group we can utilizing we can apply this a particular pendency idea function this improved our performance speed the tens of twenty times faster one thing we know is actually the the for a group map data we have to look all the group into memory and so we have to carefully select how we control the memory so that it can efficiently run through all this large-scale data that we need to process without causing any exceptions and eventually these particular pendants UDF's included the speed significantly we also evaluated our two models versus a well-known high Sheen or similar user search approaches and we measure this performance by accuracy we when we did when we did this in a passive face we we went through three different data sets one is movie movie lines it is very popular in recommender system to me the other one is good grease data set this actually contains users book reading logs for more than ten years and the third data set we tried is behavior data from internal we compared this to approaches with LSH and numeracy F and the other approach called medias H is also a way to do user hygiene in all these are data sets our model is actually outperformed in different bit lengths of hash code so the reason we run different midlet of casualities to have a way that the longer of the Midland can replicate the users profile more processor nickel in conclusion in this particular work we designed a novel a novel deep ban rehashing architecture which can utilize in a different type of deep neural networks then this particular architecture helped us to generate the similarity preserving a user hash code and this hash code can be efficiently used in similar used search user search and our TAS a model captures the users preference with different time scales and try to capture the users recent activities and the long-term activities the other category of contention model utilizing categorical information and also the captured users time sequence behavior and the the most important thing here is the Tennessee DF help us to improve that the speed significantly so that we can update the users behavior hashing you know much more frequently manner so I want to say thank you to all this spark plus a I submit twenty twenty participants and the last but not least we also we are also Harry I'm sorry is Harry our team is also Harry there are many different positions related to did engineer the data signs if you're interested please feel free to contact Praveen or myself through our email or LinkedIn profile thank you you 