 Hi everyone. My name is Chelsea Finn. I'm an assistant professor at Stanford University. I'm sorry we couldn't be together in person today, but I hope that we'll have an opportunity to meet each other in person at some point. Today I'll be talking about Meta-Learning from few-shot adaptation to uncovering symmetries. The core motivation of what I'll be talking about today is the gap between our current paradigm and our current reality. At least in machine learning research, our current paradigm is to take a training data set, train a model on this data set, and then ultimately evaluate this model as of the static model. In contrast our current reality, is that the world is constantly changing, from stocks to changes in supply and demand over time to robots entering different parts of the world throughout the course of their lifetime. So, what I would like to talk about today is whether our algorithms can handle the changing world rather than just simply deploying a single static model. Now you may be wondering how does industry cope with such a changing world when we develop our algorithms to produce static models. Well if you ask Chip Huyen who works on machine learning in production. She would say that machine learning systems degrade quickly because of concept drift. And for online machine learning systems, you want to update them as fast as humanly possible. So clearly the way that our techniques are being used is not the way that we intend when we develop machine learning algorithms in research. And if we have a mechanism to bridge this gap to actually prepare models for this constant change in the data distribution, perhaps we'll be able to have a much broader impact. One approach to solving this problem is to use fine tuning, and fine-tuning works very well. From computer vision to natural language processing, we've seen large gains in accuracy for example when you pretrained our ImageNet and large improvements in error rate when you pretrained an language modeling and finetune it on other downstream data sets. More recently in robotics we've also found that the same can hold. If we pretrain our robotic grasping system with reinforcement learning and then finetune it on drastically different situations, including harsh lighting, transparent models, a checkerboard background and even a change in the robot's morphology, we see some substantial gains when we use fine tuning. The downside though is that finetuning also has its limits. It requires a nontrivial amount of labeled data in order to adapt to these new circumstances. So, what I would like to talk about today is how we can start to bridge this gap. Can we introduce assumptions that allow us to generalize further with less data, and can we make these assumptions as mild and broadly applicable as possible, yet also powerful in what they enable. A core aspect of what I will talk about is meta-learning. So, with this in mind, my talk will have three parts. First, I'll give a brief overview of meta-learning and what it is currently capable of. I'll also talk about whether we can use meta-learning to capture equivariances to allow us to generalize further. And finally, I'll talk about whether we can fine-tune to distribution shift without any labeled target data. But first let's talk about what meta-learning actually is. z And to do that it's hopeful to look at an example. Let's look at a few-shot image classification example. The goal here is given one example of five different classes, essentially a little tiny training data set, can we classify new examples as being among one of those five classes. Unfortunately, if you train from scratch, or even if you've fine-tuned on this dataset, your model would probably overfit heavily to such a small training dataset. So, what meta-learning algorithms aim to do is to essentially learn how to learn other tasks in order to prepare the model to perform this few shot classification task. So, for example what we can do is we can take image data from other image classes and structure it into mini-train datasets and test sets. With this data, with these training classes, we can train a model how to learn from small amounts of data. And then after learning how to learn or meta-learning across these tests, at meta-test time when given held out test classes, we give it the train data set from the held out test classes for this new test, it can effectively generalize to the new examples. Of course, this is an example specific to image classification, but you can also replace this example with problems like regression, language generation, skill learning and really any machine learning problem. Now this is how the problem setup looks like. How do meta-learning algorithms actually solve this problem? One approach is to parameterize the inner learning process using a neural network itself. So, you can take the training data set and pass that data into your own network model, and train the model to essentially learn from that data such that when it's given a new example, it can effectively predict the correct label and generalize to that test example. So, this is one approach. Another approach to bring a bit more structure into this inner learning process is to embed an optimization inside the learning process. So instead of parameterizing the learning process as a neural network, we can parameterize it as an optimization process with some trainable parameters. So for example we may have an initial neural network, we can run gradient descent on our tiny little training data set to get another neural network and train this entire process such that when we give it a new example, it can generalize effectively to this example and predict the correct label. Different algorithms correspond to optimizing different parts of this optimization process in optimizing them in different ways. The model-agnostic meta-learning algorithm optimizes for these set of initial parameters of this learning process such that one or few steps of gradient descent produce a model that generalizes effectively from that tiny training dataset. So that's how meta-learning works. What are these algorithms capable of? One thing we shared recently that you can do with these algorithms is allow them to enable robots to adopt to new objects that they haven't seen before. For example, you can show the robot something that you want to do like placing a peach into a bowl. It can adapt its weights to this task and to this object such that it gets a policy that can drop the peach into the bowl for different positions of the bowl. Beyond this one-shot imitation learning problem, meta-learning has also been successful at adopting to new molecules that it has never seen Rbefore with potential applications to low resource drug discovery. It can also be used to allow robots to adapt to new terrains and conditions that they experience over time. And finally, it has also recently been used to enable models to be adapted to new regions of the world for predicting the land covered segmentation of an image. So here are the wide range of applications of meta-learning that allow you to very quickly adopt a model to something that has changed about the world. Now what I like to talk about next is how we might go a bit further. So next I'll talk about capturing equivariances. And the motivation for this part is that there is actually second solution to handling distribution shift, which is to try to build-in structure into the model that captures the kinds of shifts that you'll see in test time. So, for example you can build-in translational equivariance into a neural network using convolutions and this will have a model to generalize more effectively to data that has been translated. This is great when we know the structure and how to build it in, but unfortunately, oftentimes we don't know that and this isn't a viable solution in those cases. So with this motivation, we want to see whether we can discover equivariant and invariant structure among a set of tasks using meta-learning then build-in that structure in such that we can generalize effectively when given datasets. So how do we go about doing this? One thing we can do to think about this problem is thinking about how equivariances are represented in neural network. And first let's look at an example, specifically the example of convolutions. So, here's an example of what a convolution layer looks like where you slide a filter across an input 'x' to produce an output 'y'. It turns out that you can represent the same convolution using a fully connected layer where the weights of the filter are repeated across the matrix. they are essentially shared in different parts of the matrix, and the rest of the matrix is zero. Now with this example, what we're going to try to do is reparametrize this weight matrix to separate how equivariances are learned from the underlying filtered parameters. So, keeping along the example of convolutions, what we can do is we can reparametrize this weight matrix W into first the underlying filter parameters a, b, and c, and how these filter parameters are shared and copied within their weight matrix. And if you take the matrix back to multiplication of these two things, we get a flattened version of the weight matrix on the left. We'll refer to these matrix and vectors as U and v, and essentially U can capture the symmetries, how parameters are shared and v can capture the underlying shared parameters. It turns out that we can prove that theoretically this U and v can directly represent a decoupled equivariant sharing pattern and filter parameters. And it can directly represent this not just for 1D convolutions for example but for all group-convolutions with finite group G. This covers things other than convolutions like permutation equivariance and some forms of rotation equivariance. Okay, so now that we have a mechanism to separate out the equivariance from the underlying parameters, now we have a way to actually meta-learn equivariances that allow us to translate those equivariances to new tasks. In particular, what we'll do is in the inner loop will only update the underlying parameters, keeping the equivariance fixed. And in the outer loop, we'll learn the equivariance U and the initial parameters. So essentially what we're doing here is training the model such that when you have these equivariances fixed and you update the underlying parameters, you'll get a model that generalize as well. And this will allow you in situations where you have symmetries shared by range of tasks to potentially uncover these symmetries and keep these symmetries fixed when you're learning new tasks. We refer to this method as meta-learning symmetries by reparameterization (MSR), in a sense that we're reparametrizing the weight matrices in the network and keeping part of that matrix fixed, and part of that matrix learned in the inner loop. Now experimentally, we want to understand whether we can recover convolutions from transitionally equivariant data. To study this question, we generated a number of synthetic tasks each task corresponded to a different translationally equivariant function and we had small data sets for each of the tasks. And from this synthetic data experiment, we found that we were able MSR with built on top of fully connected networks is able to perform just as well as MAML when you're using convolutional layers. It is also able to significantly outperform MAML with fully connected layers and locally connected layers, indicating that it’s able to recover something that is as capable as convolutional inductive bias. Further, we can actually show that with MSR we cover a weight matrix that looks like this that exactly corresponds to a fully connected representation of a convolution or we have exactly the a, b, and c parameters copied across the diagonal of the matrix. Further we wanted to ask whether we can recover something better than convolutions. To do this we looked at first data with partial translation symmetry. So, we took some locally connected functions with varying numbers of underlying filters either one filter which directly corresponds to convolutions or two and five filters which had only had partial translation symmetries. And we find that in these cases MSR is able to perform significantly better than MAML with convolutions because it is able to recover the correct symmetry underlying the data. Further, if you have data with translation, rotation, and reflection symmetry, MSR is also able to recover the underlying symmetries. In this case, rotation and reflection, when you build in translational symmetry. And performs significantly better than convolutions which isn't able to capture the rotation and reflection symmetries. Great. Now beyond the synthetic experiments we want to understand whether we can learn from augmented real data. To do this, we constructed a meta-training process that looks like this, where we wanted to try a bake data augmentation into the architecture or update rule of the network. So in this case, we took the train-set and the validation-set for each task. And we augmented the validation-set with different transformations of the data and essentially trained a model such that when it adapts with an unaugmented training data set it can generalize much further to augmented data. In this setting, we've found that MSR is able to significantly outperform MAML and other variance across the board, and also perform comparably to prototypical networks when tested on Omniglot and significantly better when tested on the more difficult mini-ImageNet problem setting. The take away from that part is that we're able to, by actually kind of recovering and encoding symmetries inside the neural network, we're able to generalize further potentially using this particular form of meta-learning. However, we still need label data in this case in order to adapt. So what I'd like to talk about next is if we can fine-tune an adapted distribution shift without any labeled target data. We'll specifically consider in this case a form of distribution shift called group shift. And what I mean by that is we'll have a categorical group variables z. This may correspond to the user, the location, the time of the day and can be easily derived from meta-data. And we'll assume our training data comes from this distribution and our test data comes from this distribution, where the distribution shift is captured by this underlying variable z. This can capture things like label shift, and most forms of covariate shift and it can also capture realistic problem settings like federated learning. Now how do we go about solving this problem. So previous approaches for tackling this form of distribution shift have built upon the idea of distributionally robust optimization. And the way that these algorithms work is they form an adversarial distribution q(z). And they optimize for the worst-case distribution over this variable. So, for example the worst case group. These approaches can able robust solutions, solutions that are robust to the distribution shift. And they're less pessimistic than adversarial robustness techniques because they're optimizing over a smaller set of perturbations to the data distribution. But they often sacrifice average or empirical group performance as a result of optimizing for the worst-case distribution. So, we want to be able to overcome this limitation by adapting for distribution shift rather than trying to be robust to distribution shift. In particular, we’ll assume that we have unlabeled data from the test sub-distribution. So, for example we'll have new unlabeled data from a new user, different type of day or a new place. In the case of federated handwriting recognition, we might have unlabeled data that looks like this. And then we want to be able to use this data to adapt our model and infer the labels of these data points. And the assumption here, is that the test inputs from one group, from one user, what type of day and so forth, are available in batch or in a streaming setting at a test time. We think this is a fairly realistic assumption since often times data is arriving from a single time of day or a single place at test time. We refer to this kind of general problem setting as adaptive risk minimization(ARM), in the sense that in a test time we're given the opportunity to adapt their model. What the training procedure looks like specifically is that we construct different sub-distributions of the training data using our groups and then we train for adaptation to each of these sub-distributions. And particularly this adaptation process like I mentioned will use unlabeled data, and we can experiment with a couple different meta-learning approaches to allow this fast adaption to unlabeled data. This includes the model-agnostic meta-learning approach using a learned loss function where we're running gradient descent on the unlabeled data using this learned loss. The second approach that we'll consider is meta-learning with the context variable where we have a neural network, takes its input to the unlabeled example, produces a context and uses that context when making predictions for the test example. In the simplest setting, this context variable can simply correspond to the batch from statistics where critically what we'll be doing is training the model such that when it is given unlabeled data in a group, it can use the batch from statistics to adapt to that part of the distribution. Now how does this work in practice. So, on the federated extended MNIST data set, we compared to a number of different approaches. We compared to empirical risk minimization or standard neural network training. We compared to distributional robustness like I mentioned before, and also a baseline that upweights the groups to the uniform distribution, rather than being skewed. And when we try to adapt to a new user using unlabeled data, we find that we are able to perform significantly better than these methods. We also compared to a federated learning method, and the results are here. We see a five percent improvement in average accuracy, average across the users, and we see a 10 percent improvement in the worst-case scenario, in scenario on the worst user essentially. And this evaluation is important because often times you don't want your service and your network to work great for some people and work terribly for others. We also evaluated on the CIFAR-C in the TinyImageNet-C dataset. In this case adapting to new image corruptions. And we trained using 56 corruptions, and tested using 22 disjoint or held out corruptions. And again, we find in this case that we see large improvements both in average accuracy and worst-case accuracy with an 8% improvement in worst-case TinyImageNet-C performance and a 21% improvement in worst case Cifar-C accuracy. So, to wrap up, if you take away nothing else from my talk it's that there is this gap between our current paradigm and our current reality. In machine learning research we often assume that we are going be evaluating static model, while in reality the world is constantly changing. I talked about three approaches for trying to bridge this gap starting with the general framework of meta-learning for enabling very quick adaptation to changes in the world. I then talked about preliminary evidence of meta-learning being able to capture equivariances in the data via reparametrized weight matrices. And I finally talked about how we can fine-tune an adapted distribution shift with only unlabeled data. With that I'd like to thank my students, especially my students and collaborators who deal a lot of the work that I presented today including Alan, Tom, Marvin, Henrick, Nikita, and Sergey. If you are interested in learning more, all of the lecture videos for my course on meta-learning are available online and I look forward to your questions. Thank you. 