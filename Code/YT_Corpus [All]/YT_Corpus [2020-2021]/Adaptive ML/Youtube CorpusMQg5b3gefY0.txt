 Hello, Demuxed, I'm Michelle Munson, coming to you from Berkeley, California. The title of my talk today is Low Latency Live From A Different Vantage Point. I'd like to start with a little bit of background. First of all, this is not just my work, but the work of many people in the Eluvio team. We're not video experts here, but we have learned a lot about video over the past two and a half years. We've spent our careers working in high performance networking, distributed systems and content over the internet. In 2018, we went deep into crypto and blockchain, AI/ML and especially video. And some of you may have heard my talk at Demuxed 2018 on the foundations of our content fabric. So why this talk now? Well, Low Latency Live is obviously a subject of the day, if not the subject of the day in the video engineering community. There have been many proposals, many great talks, and there's certainly very much technology. Dash Low Latency, LHLS, Low Latency-HLS etc etc, much more than this short 20 minute talk can possibly cover. But one thing is for sure, there is a lot of complexity in this technology, many complications in the various proposals, and there are tradeoffs throughout, which led me to ask the question, is it possible to solve this problem differently? I'd like to start by just defining some terms, which are probably very familiar to all of you, by low latency live, what I mean is chunked encoding of fragmented MP4 content, not full segments. We're targeting two to 10 second latency end-to-end, not ultra low latencies, we mean HTTP transport and broadcast to internet audiences, not real time peer to peer or point to multipoint protocols like webRTC, RIST etc. We mean the range of consumer streaming bandwidths from megabit to 25 megabit per second plus, especially if we're thinking AK, any source, MPEG-Transport Stream, RTMP etc. The baseline technology here is obviously chunked encoding and transfer using HTTP/1.1 POST and GET and the many variants thereof, Apple's low latency HLS being the most famous. And of course we mean common segments for DASH and HLS. So for background, I'd like to refer you to these many wonderful talks from people in the community, Steve at Mux, Will at Akamai, Marina at M2A, Ben at Wowza, Yuoshi at Twitch. So with that, maybe we will kick off by looking at an idealized, chunked encoding transmission system. This is obviously not real, but I think it's useful to think about the problem. So, the whole point of chunked encoding, at least as it relates to delivering video over the internet, is to literally release a pipeline, that is, as the video source comes from the encoder, that we could have one continuous pipeline as it goes through the live origin, as segments are created and it flows through the CDN to the edge and finally out to the player. And if this is in fact a perfect pipeline, we would achieve an ideal latency that's really equal to just the internet propagation time for a fragment, something like 100 to 250 milliseconds for the global internet. But in practice, things can be very different. First of all, there can be several sources of delays, for one, delay coming out of the encoder if it buffers the fragment, not significant, but something. And then we start to see some more significant sources of potential delays. Firstly, in transcoding, the fragments themselves are very small. And as files, a small file I/O can be slow. Especially if the transcoding is being done on object storage. Additionally, it may not be possible to transcode in a pipeline manner, that is, the transcoder may require operating on a whole segment, again, complicated by cloud storage. If we look downstream, we can see there can also be delays in distribution. That is when the CDN edge goes to get an update of playlist from the origin. That can be blocked in I/O. And similarly, the CDN edge to origin request for the first segment can also be blocked or slowed down. If we look at this at the best case, then we may be looking at something like four and a half seconds end to end latency. Quite good, but it could also drift out as much as 15 seconds, maybe even more, depending on how problematic the scenario. And perhaps worse than delay there are side effects. The side effect that is probably most well known is the fact that player APR is badly affected. The player ends up majoring its bandwidth at the video rate and this kind of fragmented pipeline system. Of course, low latency HLS proposes the alternative of making subsegments from the segment in order to allow for bandwidth estimation over the download time of the subsegments. This is better, but it's also a challenge due to the small size of these. And recently some of the improvements, such as the preload, has helped this bandwidth estimation. But then there is the further complication that these subsegments lead to heavy origin server load and potentially edge server load as well. The subsegment parts have to be advertised in the playlist. There are high frequency playlist updates as a result, and the subsegment serving is also high frequency. The origin server is not only heavily loaded, this is also complicated to implement and most of it requires HTTP 2.0 or Low Latency-HLS. As a result, this system is at best complex and can also be very difficult to achieve at scale. So how might we look at this differently? First of all, I'd like to propose that we could remove these inefficiencies in transcoding and distribution delay. If that were possible, then we could retarget what is our target latency of two to five seconds for good low latency and use that instead for good bandwidth estimation and also for simplicity. So what are the principles more formally in doing this? Well, obviously the ground principle is that we must remove this artificial delay from the contribution, transcoding and distribution stages with a fully pipelined approach and then building on programable content could actually synthesize the playlist, that is, using data and code just in time and using this ability to synthesize the playlist, actually pull back the advertized live edge just enough for good player bandwidth estimation without actually changing the player AVR. Today, that pullback might be one segment, tomorrow with new players and better AVR could be even less and specifically then the minimum pullback from the live edge is the sum of the input delay, the contribution, distribution and transcode delay in this pipeline just in time approach and the real time video delay. This means that our minimum latency from live edge is very simply that pull-back plus the player's delay in downloading a full segment or a fragment in terms of improved AVR. This allows us to achieve low latency without the side effects, we'd have good adaptive bandwidth estimation. That is, the player can download a full segment or whatever target amount its bandwidth estimation requires. We have reduced server load with fully pipelined throughput, one playlist update per segment and less I/O everywhere and simplicity. We have fewer or no changes to players, origins or protocols etc in order to achieve the target latencies. So now the hard question, how to actually do this in technology. First, I want to make a small digression into introducing or reintroducing content objects and just in time production of output from a decentralized network like the content fabric. First of all, a fabric object is really a referential structure that links together, points to binary parts that represent the metadata in the content, the video and the audio essence. Any associated content may be binary content for stills, as well as a contract that identifies the object and code, bit code that is part of its self rendering capability. This object and its parts, specifically the parts that make it up, actually live in a decentralized software overlay network. That is, they shard throughout the network at the data layer. And then at the time of consumption, that is, when a playlist needs to be rendered, when a segment needs to be rendered in streaming, for example, that triggers a process whereby the parts are found through a fast, centralized routing algorithm, which I talked about a lot in my talk two years ago, including code parts which are loaded and performed, transcoding, packaging etc. And optionally a contract interface is applied for access, control and security policy. So how is this useful now to us in live streaming? Well, when a live stream object is created, it actually commits its metadata parts and they distribute throughout the fabric to the nodes. At the time then of active streaming, that is, when the live ingress starts, the ingress node actually creates and publishes continuously live mezz parts to the nodes responsible for this part's partition. And when a client wants to get a playlist, the egress node synthesizes that playlist from the metadata parts. And when a client needs to get a segment, similarly egress node can pull mezz parts from the nodes that are responsible for its partition, using the routing algorithm and transcodes that raw part to segments, all being done just in time. And the key to this process is that all I/O is fully pipelined. Let's look at this in a little more detail in comparison to a traditional transcoding and content distribution system. The first thing you probably notice is the pink zone that's usually the problem with delay shrinks down to a very small time period. That is, the ingress node can be putting the mezz part to the target partition nodes in around 200 milliseconds or less. And then when a segment's requested, the egress node can be getting that mezz part and starting transcoding within 50 milliseconds. That cuts down this content distribution and transcoding stage to around 250 milliseconds worst case. Similarly, the egress node can synthesize the playlist from the metadata and do that on demand, do it systematically with the advertised live edge actually pulled back according to the expected delay, which in this case is the input delay for the encoder and the transmission to the ingress node plus this new just in time contribution, distribution and transcoding delay. And then finally, the time, most importantly, to generate one whole segment, that is, the real video time for a single segment. If you add those up in this system that works out to three and a half seconds, which means that the player is able to have available to it one full segment within three and a half seconds. We get low latency. The player gets to download the full segment for good AVR and we do this, of course, then without the side effects. So let's put this all together and see how it looks on a timeline. First of all, we've created a stream object, and its technical metadata has distributed throughout the fabric, and the fabric nodes are in a position to advertise a playlist with a systematic live edge pull back from that configuration. That live edge pull-back will be set equal to our expected latency. Now, when the player actually requests a playlist, we can see that the fabric is able to put the incoming mezz parts being generated by the ingress node and also get those mezz parts to the egress node and create one full segment in time such that when the player makes the request for the playlist advertising that segment, the segment is ready to go and can be served directly to the player at the time of request, allowing the player to download one full segment. It's in a position to download at 3.25 seconds and the player can begin playback at three and a half seconds. So how does this actually perform in the real world? We decided to test this from the perspective of segment arrival time and video quality, using a large fleet of open source NPB clients that we instrumented, running on about 290 GCP hosts and 24 regions around the GCP cloud. Those clients were all concurrently streaming a single live stream, which was a 15 megabit per second MPEG transport stream published to the Content Fabric. The stream was configured with a three second latency pull back and we were, of course, using two second fragmented MP4 segments within the fabric of our HLS. And we configured an AVR ladder with a top rate of 14 megabits down to 0.4 megabits per second. There was only one client per host because we wanted the bottleneck bandwidth at the host to be much greater than the top bit rate of 14 megabits per second. We ran the test long enough to have the clients request a total of 200,000 segments, and 100% of the segments requested by the players were in fact at the top bit rate. We also noticed in our monitoring clients that the stream performed in a visually flawless manner. We were streaming using the HLS JS player throughout. Most importantly, though, looking at the segment arrival time across all of the clients, we can see that the arrival time converged nicely to the expected two seconds for live streaming without buffering. We also took a look at the arrival time by client region, and you can see across each of the regions of GCP where the clients were running, the segment arrival time converged nicely to our target of two seconds. And then we decided to turn our attention to infrastructure, the infrastructure load. In the case of the high bandwidth test, that is, the 4,500 clients running all streaming at the top bit rate of 14 megabits per second or converging to that in their ABR, we can see that the ingress node, which had the most load, never exceeded even 15% load average. And the other nodes also kept an even more modest load average throughout. We then generated intentionally a high concurrency load using 7,500 up to 50,000 MPV clients also running on the GCP hosts in these various regions. They were all concurrently streaming the same live stream, same kind of test. And we noted there that the ingress node load average never exceeded even 50% and the load average on the other nodes was low and stable. Finally, we took a look at egress node bandwidth utilization and there we can see that the bandwidth spread evenly over the fabric nodes throughout both the high bandwidth load test and the high concurrency test. With this result, we can see that low latency live really is doable, it's quite doable without the challenging side effects, but it does require a different vantage point, one that considers the root causes of the latencies and also builds on fundamental innovation. I'd like to conclude with one of my favorite quotes from Albert Einstein. We cannot solve our problems with the same thinking we used when we created them. Thank you very much. 