 Welcome to this talk about key opportunities for reinforcement learning in modern video games. My name is Katja Hofmann, and together with my team in the Game Intelligence Group at Microsoft Research in Cambridge in the UK, we push the state of the art in machine learning and reinforcement learning in order to empower game developers, and to enable new player experiences in modern video games. In this talk, I am hugely excited to share details of Project Paidia, our research collaboration with Ninja Theory, a Microsoft Game Studio that is conveniently located just down the road from our research lab here in Cambridge. The goal of this research collaboration was to prove out how reinforcement learning agents can learn to genuinely collaborate in a game, so that they can eventually collaborate with human players in a team-based game. Ninja Theory's latest title, Bleeding Edge, turned out to be a perfect test bed for this type of goal. It is a [inaudible] for team-based game that requires close collaboration between characters of diverse abilities, and so it's the perfect platform for testing out whether our reinforcement learning agent can genuinely learn to coordinate. In order to give you a sense of what coordination in this game looks like, let's jump right into our first research demo. This is a demo of our trained reinforcement learning agents in Bleeding Edge. It's a [inaudible] for team-based game where each team consists of two assassins, Nidhoggr and Daemon, a tank, Buttercup, and a healer, here is ZeroCool. Teams score points by controlling capture points and by killing their opponents. To play effectively, team members have to collaborate. For example, the healer can provide crucial backup for teammates during battle. In this demo, we evaluate our trained reinforcement learning agents against a team of scripted agents. In this example, you see Buttercup in battle taking on several opponents. She can do this because ZeroCool is there to back her up and heal her or restore her health during this crucial battle. To evaluate the behaviors of our trained reinforcement learning agents, we also use this top-down view where we can see any strategic behaviors that has been learned. For example, in the video you see here, you'll see that the team is deciding to stick together to control point B rather than being drawn out in battle, and this proved out to be a very effective technique. In the remainder of this video, you'll hear from my colleagues at Ninja Theory and Microsoft Research to learn about how to actually use reinforcement learning to create game agents like the ones you just saw in the previous demo. First up is my colleague, Gavin Costello, Technical Director at Ninja Theory, and he'll talk about how our collaboration first started, and the things you need to know to integrate game agents like this into a game like Ninja Theory's Bleeding Edge.  Our collaboration started in 2018, and Ninja Theory was new to the Microsoft first-party Studios Group, and we're still exploring what that meant for us in terms of being able to collaborate within Studios Group. In the process, we found that the Microsoft Research Cambridge, which is hardly just across town from us, was going to be taking part in a AI hackathon that Microsoft was running internally that year. Now, anyone that follows Ninja Theory or our Dev Diaries knows that we tend to make quite ambitious games with very small teams, so it's really critical to us that we work smarter, we maximize the output of each individual within the team. For that reason, we were already quite interested in machine learning, and this seemed a great opportunity for us to go along and really increase our internal knowledge. We actually pitched an idea for the hackathon which was to try to get machine agents into Bleeding Edge. It's a three-day hackathon so it was just to start the process off. This happens quite nicely aligned with what Microsoft research wanted to do with collaborative AI. So we went out that year to give it a go. The hackathon was actually really fun, l had a real buzz to it. We got to meet all of the different teams and see the different things that they were trying to achieve. Then we did a bit of initial machines out which really didn't take very long, and then we separated into our groups to start working on our projects. One of the things that was quite surprising about it was just how easy it was to get the integration going. A case in point is we actually made two integrations during the first day of the hackathon, we [inaudible] too, it's just because initially we had a library conflict with Unreal, so we switched gears and made a bridge program that would sit in the middle and talk between Unreal Engine 4 and the OPC connection on the back end. So that enabled us to get up and going very quickly, and then while we were using that, we worked on a plugin, a separate plugin from Unreal so that it could link separately. We could use that to do the same thing but without the need for the bridge program in the middle. Once we have that integration working, we had that first exciting moment where you're explicitly you're typing messages, and you can see that they're talking to each other and things are being translated properly, and you're about to start actually training. Now, for the hackathon, we exclusively used [inaudible] , so it was all GPS style behaviors. But once we have the integration going and we can see that you could type thing and they toggle that move forward, we switched gears. We were like, "Okay, we've got what we need, we've got the actions being sent, let's go and do a jam where we all go and try and teach it to do different things just by changing the reward shaping function that you give it to learn from." The thing that says, "Did I do a good job?" You can say yes or no on the different scenarios and it will learn to do different things. We've got some really interesting results along with our first taste of how tricky AI could be. Not tricky in the like it's difficult since, but like tricksy, it will find solutions that you don't expect it to find. So an example of that was that we gave it a reward function for doing damage to its enemies, but it wouldn't differentiate between a single strike or a damage over time tick. Naturally, you'd get more ticks from a damage over time so that would be more favorable. At the same time, we rewarded it for staying close because we wanted to encourage it to close the gap with this enemy, which is a MLA GPS. But those two things combined, which sounds quite reasonable, that when it learned to do which says opponent on fire and then pin it to a wall, so that it was constantly in close proximity while receiving the maximum amount of reward for damage done for each of those ticks. So very quickly is tried to show us that it would learn things we didn't intend it to if we gave it a poorly specified reward function. But it was great fun and we did manage to train it as well as to do some very deliberate things like I was trained to avoid other players or we trained it to chase down other players. Of course, we trained it to be rewarded for doing damage, to be trained to aggressive behaviors as well. We trained it to move to a point in the world as well. At this point, we only had symbolic data being sent, no visual information. So just from the symbolic data we had it navigating to points, learning how to get from point A to point B within that environment without actually a clear view of the environment, so it was really cool to see that. As much as possible, you don't really want to be shaping the reward function like that for complex behaviors because this is how I would do it biases into the system. But for the hackathon, it was quicker and it meant that we could get some really interesting behaviors trained up really quickly, like we were training just on singular machines, each of us with our own reward function. Training on our own machines could actually get learned behaviors in quite short periods of time. Basic as they were, and not yet collaborative. These initial tests were really just to verify that the integration worked, verify that all the data and that was being sent and received, verify that we could train something. Then after the hackathon, we wanted to explore having multiple of these agents collaborating, not with a group policy that's controlling all of them, but individually, and having them still be able to work together as a group as human players would. In terms of the final integration that we're working with at the moment, we ended up with a really nice clean plugin, which is just an interface between the player controller within the game and the ROPY connection on the back-end. So we can drop this into any of our rather unreal projects, get up and running pretty quickly in theory. Most of the codes is reasonable exactly as this, which is really nice to have. The only things that you have to change are, the actions that you expect to receive and then act on, as well as the wallet state that you pull and feed to the connection so that it can support to, and what more action send you on the back-end. For Bleeding Edge, the actions that we chose to use, they're all buttons on the controller basically. So we wanted the AI to play the same way that a person would play. There's no shortcuts, they can't call commands, they are in the game or anything like that, it literally plays as a person plays. That's not say, the only way to do it, you could do it in other way. For example, the action could be moved to point action with a location in the world, if you were doing something like an RTS, and the brain is just deciding where things should move, is making top-level decisions rather than individual inputs. But for Bleeding Edge, what we wanted is for it to be able to play the game like a player would as if it's got a controller and it's pressing buttons, and it's making things happen, and to see whether we could do that in an environment where it could also learn to collaborate with his teammates.  Now, that you've learned about how our collaboration with Ninja Theory first got started, and got a little bit of a sense of how we integrate reinforcement learning agents with the game, it's time to look at a little bit more technical detail of how we actually use reinforcement learning to train these agents. Briefly, reinforcement learning is a type of machine learning, where agents learn through interacting with their environment. In this case, of course, those are the game agents and they interact with the game. At each point in time, a game agent would observe some information about the state of the game. For example, its current position and position of teammates. It then takes an action and as it interacts with the environment, learns about the consequences of those actions. Those consequences are typically expressed in terms of a reward signal which could be positive. In this case, for example, when dealing damage to an opponent, or negative, for example, when receiving damage. The key of reinforcement learning is that the agent learns about the entire sequence of actions that it took up unto a given point in time to learn about the long-term consequences of its behavior. Through trial and error, so trying some actions in a given situation, learning about the consequences of those actions, the agents learn how to act in the environment in order to maximize their reward. In this case, with the reward signal provided by the experimenter, it learns to improve the game. Now, I will hand over to my colleague Raluca, who did the vast majority of the training of the agents that you saw in the previous demo. She would tell you about the tips and tricks of how to make sure that reinforcement learning works as effectively as possible in a game like Bleeding Edge.  It was super-exciting to train these agents. I had so much fun just looking at how they progress day by day, and they learn new strategies and new tactics. So I'm very happy right now to share with you some of the things that we did in order to speed up our training of the agents. One of the first things that we did was to create this small 2D research environment. We did a small game in Python, and it mimics the exact mechanics of the game. It has these three capture points that activate randomly. You get two agents on each side, one is more like an assassin, one is more like a tank. They each have different health and speeds, so when you get to see these collisions, they are basically them doing damage to each other. This was a great test bed to try out different assumptions that we had. We could try different reinforcement learning algorithms that would be performing quite well at scale. That's what we wanted. We also were able to tweak the reward structure in a way that will be most beneficial, and would yield the most interesting strategies overall. Most importantly, because it was such a lightweight environment to work with, it was really good at helping us to find a good set of hyper-parameters for the algorithm. So if we were to try all these things in the game straight away, of course, this would have been much more time-consuming and of course, more intensive on our compute resources. Another thing that we tried, and it did speed up our training process quite a bit with using imitation data. The place where that helped us that most, as you can see here, was navigation of agents. This game is actually key to be able to get there, even a fraction of a second in front of a capture point before your opponent. This gives you a clear advantage, and you have to be very efficient in navigating the map. The map had these narrow tunnels, which proved tricky for the agents as they were learning. But with the human data added to the training, we were able to see progress much faster. So we basically recorded ourselves for 30 minutes playing against a set of scripted agents, and we fed the data into training. We didn't use only imitation learning, we combined that imitation data knowledge with normal reinforcement learning. Such that the agents had a chance to learn both from their own experiences in the game, and from the imitation data from humans that they were given. Another thing that gave us great flexibility during training was the ability to restore from a checkpoint. We had a training curriculum which enabled us to train for a set period of time with one reward, and then we would change that reward in order to get different behaviors, or to correct some things that we felt were not as expected. One really good example here is, for example, we decided to tweak the reward for the healer. So the healer would get a really high negative reward if any of their teammates would die. So after training with this reward, we got to see how the healer got really smart about prioritizing the teammates that he would actually heal first. Not only that, but it also learned how to stay very close to the combat area. As you can see in the video, it's right there, and it's switching targets depending on who needs health the most. These were some things that really helped us get to where we are right now.  In order to make reinforcement learning agents learn quickly in a modern video game like Bleeding Edge, for example, to learn effective behaviors overnight, we also need infrastructure that allows us to scale. In the next section, my colleague, Adrian is going to tell us about how we use Azure in order to achieve that.  When training agents, the more interactions of a game that an agent is able to sample, the better the final policy will be. The agent is able to try new things out in different scenarios to build a better model of the game and discover what works well. If training on a single game instance though, this could obviously take a long time for the agents to efficiently explore and learn how to play. So we needed to be up to scale out the games in order to increase the rate of which agents can gather data, and reduce the wall clock time for training. The obvious solution for this was to move the games to Azure. We can allocate the virtual machines we need on-demand and scale everything down when we're finished an experiment. However, traditional games aren't usually designed to be run as server applications, and during development may not be fully hardened or able to run for prolonged periods of time. So we have the additional requirements of making sure we can handle failures reliably and recover from them. We wanted to make sure that training could just keep running no matter what went wrong. We ran the games within a sandbox so that as soon as an error was detected, for example, a network error between game sessions or a crash, we could reboot only the affected game instances and mask the error from the training agent. The game instances would have no need for persistent data storage, so recovery can be handled quickly and training can continue as if nothing happened. By ensuring we could handle a wide range of failures, it had the added advantage of opening up a wider range of VisualVM options for us. We no longer had to use dedicated VMs to run the games and could opt to use cheaper and low priority VMs. These offer savings of up to 90 percent per hour compared to dedicated VMs. We could run experiments in order of magnitude larger for the same price we are paying originally. Then the move to Azure Machine Learning simplified things further for us. It greatly reduces the complexity of running large-scale experiments as Azure takes care of the infrastructure, there's no need to configure your own scale sets or manage deployment templates. We just needed to create a disk image containing the game and then in the experiment, configs say, "I'll have 50 of these, please." Azure ML will automatically allocate the hardware, deploy the disk image, and automatically spin everything down again when the training is complete, it will take care of running the training, combining the Windows game environments without training scripts. There was no need for any expert infrastructure knowledge. We could focus on the interesting task of creating our agent and the specifics of the game. All the infrastructure hassle became Azure's problem, we just let them take care of it. We still have the flexibility to deploy additional files as part of the training run. They get copied from your local PC to the Azure VMs running the games. So we could still tweak configs and provide extra data without having to recreate everything. There is nothing we needed to monitor other than the training graphs and results. Azure ML also made the iteration processes easy as it keeps a record of everything that contributes to a training run. We could make a change locally, click "Run", and it will create a new training entry that we could refer to in the future when we wanted to make comparisons across runs, there's no need to set up anything extra in order to keep track of experiments. For us, this made training an extension of our local workflow. We were able to use Azure ML as if it was a local tool we'd naturally run after editing files.  To learn more about Project Paidia and how it pushes the boundaries of reinforcement learning in gaming, you can visit our AI innovation website under the link shown below. On the website, you will learn about what it takes to train reinforcement learning on modern video games such as Bleeding Edge. You can even try our interactive experience and play a collaboration game with a trained reinforcement learning agent. As you can see in our demo here, the goal of the game is to escape from a locked room on a spaceship. You control one character here shown in white. You will need the support of the blue character, which is controlled by our trained reinforcement learning agent. Also on this page, you will find further reading and links to resources that help you get started with reinforcement learning on Azure today. In this video and our demos, we've shown you what's possible with reinforcement learning in modern video games today. At the same time, my team is pushing forward the state-of-the-art in reinforcement learning in order to develop agents that learn even faster to collaborate, for example, with human players. In this last segment of our talk, I want to give you a sense of the kind of research that we're pushing forward and why it might be relevant in games. If you would like to dig deeper and to learn more, you can follow the links shown in the video to a blog post and to the publications that I mention in here. I want to highlight three pieces of work that we recently published at the conference ICLR, which is the International Conference on Learning Representations; one of the major machine-learning conferences. The first piece of work I want to discuss was led by my colleague, Kamil Ciosek, and it's called Conservative Uncertainty Estimation by Fitting Prior Networks. In this work, we were able to show that a recently proposed technique called random network distillation can serve as a very effective technology to estimate uncertainty in high-dimensional settings. For example, settings where an agent might have visual observations or other complex high dimensional observations that are typically modeled using neural networks or deep learning approaches. The reason this type of technique is so important in the gaming setting is that it allows us to have an agent detect whether it is familiar with a game situation or not. This can be important in settings where, for example, we want to have an agent that combines learning from reinforcement and imitating human players. So you could imagine that if we have some demonstration data available from human players, an agent might imitate the human players' behaviors in situations that are well covered in that data so that the agent is familiar with based on the demonstration data that it has received. Whereas in other situations in the game, it may need to rely on what it's learned using reinforcement learning. So for example, in game situations that weren't covered in the demonstration data. So the key challenge here is to detect whether a game situation is similar to another one. Is it similar enough to the demonstration data so that the agent should follow the demonstrated player behavior? Or is this a different situation? Should it continue to explore and learn a better strategy than what might be available in the demonstration data. So this is where quantifying uncertainty is crucial because the higher the uncertainty or the more to do any given game situation, the more likely it is that the situation is different from what has been seen in the demonstration data and that the agent should be using its own trial and error learning. A second piece of work I want to highlight is called AMRL, or Aggregated Memory for Reinforcement Learning. This was work led by our former intern, Jacob Beck. Again, this is a paper that we recently published at the ICLR Conference. In this piece of work, we explored how to give agents that use deep learning or neural network architectures to give them more effective access to memory. So in principle, to learn long-term dependencies between something that the agent needs to remember and when it actually needs to take a decision. So for example, imagine that there's a situation where the agent needs to observe, for example, where a control point is in an environment. Then a long time later it needs to remember and just be able to navigate back to this environment. In terms of deep learning, that means that the agent has a very long trajectory of experience and it needs to backpropagate effectively in order to understand or model the relationship between what it has observed in the past and the correct action that it needs to take at this point. We, in this paper, proposed a new technique that combines a very commonly used technique of modeling memory, called LSTM, with the so-called aggregator module, which is able to maintain a running average over the kinds of experiences the agent has obtained. We can show that this modification results in much more efficient learning than what was previously possible. The third technique or the third paper that I want to highlight is called Variational Bayes-Adaptive Deep Reinforcement Learning. The reason I'm particularly excited about this direction is that it is a technique that allows meta-learning. So this means learning how to learn, or in this setting, how to very rapidly adapt to a new situation within the game. This was work done in collaboration with a team at the University of Oxford and led by my PhD student Luisa Zintgraf. In this work, we argue that a very effective type of behavior in a situation where there's uncertainty, for example, over the task that the agent needs to solve is to try to achieve Bayes-optimal behavior. Meaning that given the agent's current belief over the situation it is in, how do you act optimally? For example, this may include having to explore the environment in order to reduce uncertainty and get more information about the task that the agent has to solve. In this work, we propose a technique that turns out very effective, and it combines a predictive module that captures the uncertainty over the situation that the agent is in with more traditional model-free reinforcement learning technique that is conditioned on the current belief of the agent. In the paper we're able to show that this results in agents that very flexibly learn Bayes-optimal behavior in a wide range of settings. As for the other three pieces of work, I'm hugely excited to push this forward with my team and see how effectively we can apply these developed techniques to situations in modern video games and understand how they can create new types of game experiences for players or new tools for game developers. We hope you have enjoyed our deep dive into Project Paidia and the exciting opportunities for reinforcement learning in gaming. To learn more about this project, our latest research and on how to get started with reinforcement learning on Azure, visit our AI innovation website under the link shown below. Thank you for watching. 