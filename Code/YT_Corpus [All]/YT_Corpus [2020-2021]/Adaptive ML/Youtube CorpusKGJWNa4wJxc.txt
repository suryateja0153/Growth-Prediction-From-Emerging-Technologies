 well thanks everyone for listening and today I want to talk about tuning machine learning models focusing on hyper parameter tuning but it will be very relevant for those interested and speeding up performance where hyper parameter to uni and it's often very computationally expensive and also in terms of of course tuning the accuracy or you know metrics your own models this is also critical so just quickly about where I'm coming from I spent most of my time at data brakes as a software engineer working on ML and advanced analytics and analysis architect one records it slide about the company you know data bricks is proud to have been fundamental in building out these important open source projects actually spark Delta Lake and all flow cool so to get into it I want to spend two minutes quickly going over what hyper parameters are if you're not familiar with the material hopefully this will help if you are I think it will be nice in terms of setting in some perspectives well so I'm gonna be using later a demo looking at fashion amidst which has a bunch of images of clothing so let's take a look at this one you know it looks like a shoe my first model that I fit said this is a dress yeah probably not correct the next model I fit said this is a sneaker and that's actually pretty accurate and what's the difference here what model to actually had better hyper grander settings the learning rate model structure and so forth so stepping back what is a hyper parameter well the statistical is sort of definition or perspective which I'd like to give is assumptions you make about your model or data to make learning easier from a practical perspective this does inputs your mo library does not learn from data you know there are a bunch of parameters in a model which the library does learn from data but it also takes manual inputs and I'll call those hyper parameters because they're knobs which can be tweaked some of these I would call algorithmic these are worse or problem dependent configs which may affect say computational perform or speed but might not really be super elated to the statistics or modeling perspective well so how do we tune these well there's sort of three perspectives I'll get matching those from before if your statistician were application you know domain expert you might bring knowledge which lets you set some of these settings a priority looking at sort of that practical perspective you know there's an mo library it takes some inputs it gives an output model which can be tested and we can optimize it like a black box this is a pretty common way to do to Union and quite effective if you do it effectively and the final way is to basically ignore until needed and I think this is a common approach I take with some of the more algorithmic aspects you know if if a config really only affects the speed of learning then I may not bother tweaking it until needed cool so I'm not going to cover in this talk statistical best practices or overview different methods for to you name and that's because there were talks it sparkly eyes summit last year covering some of these I'll give references at the end in this talk I want to focus on data science sort of architecture and workflow best practices and tips around the big data and cloud computing space well so hyper parameter to me of course is difficult else what why they're giving a talk on it and I want outline a few reasons why the first is that these settings are often unintuitive you know if you've taken an intro to ml class the first hyper grandeur you probably touched on was regularization this sort of is intuitive you know of weighting overfitting limiting model complexity but if you take any given application it's really hard to say a priori how you should set regularization and you know of course let's not even get in your own that structure the next challenge is that this involves non convex optimization I here I took an example problem where on the x axis this is learning rate and on the y-axis is test accuracy and you can see it jumps all over the place well this is a bit contrived in that a lot of this stochasticity is from the optimization process itself some randomization there but it kind of drives home the point that this isn't a nice simple curve you can use traditional optimization methods on you really need some specialized techniques and the final element is curse of dimensionality since this is a non convex problem then as we increase the number of hyper parameters here plotted for an example problem on the x-axis with seven possible settings for each hyper parameter we can see that the fraction of coverage of the hyper parameter space for a given budget of say a hundred hyper brendor settings drops exponentially as we go to the right and so by the time we hit maybe three hyper parameters we can cover you know about a third of the space and after that we can hardly test any of the actual settings Fryeburg rameters and this leads of course to high computational cost if you try to push this coverage up so given these challenges I want to step back and see how we can address them I won't be able to give you know neat solutions for all of them but I do want to talk about useful architecture patterns and workflows and a sort of a bag of tips at the end so starting off with architectural patterns at a high level a lot of this boils down to single machine versus distributed training and I'll break it into three workflows which you know 99% of the customers I've seen in the field fall into our use cases fall into single machine training distributed training and training one model per group where a group might be a customer or a product or what have you so looking at the first one single machine training is often the simplest you know involving these or other popular libraries where you can fit your data and train a model on a scene machine now in this case you know if I'm doing on my laptop I'll just take say scikit-learn wrap it and tune in that tuning could either be a second learns tuning algorithms or another method and run it but if I want to scale it out via distributed computing it's also pretty simple where I can train one model per spark task like in this diagram where the driver isn't doing much but each of the workers is runyan spark tasks where each one is fitting one model and one possible set of I per parameters and I wrap the entire thing in a tuning workflow this allows pretty simple scaling out and is implemented a number of drools so hi propped which I'm going to demo later as a spark integration we built allowing the driver to sort of do this coordination across workers thermal team I'm from also built a job Lib integration with a spark backend which can power cycle learns native to Union algorithms and then finally this can be done manually via pandas UDF's and spark the next kind of paradigm is ROM distributed training and there if your data or model or combination thereof are too large to Train on one machine you know may need use spark ml or Avadh XG boost or something which can train a model using a full cluster if you do want to scale this out further then rather than training one model at a time I can train multiple ones than parallel and so that might look something like this square you know you wrap the training process which is orchestrated from the driver save I spark ml with two you name and the possible tools which come into play here are for example Apache spark ml which has its own tuning algorithms and those actually do support a parallelism parameter allowing you to fit multiple models in parallel to scale out further you can also note that from the driver's perspective like you can actually use any sort of black box to algorithm or libraries such as hyperope because it can make calls from the driver and never really need to know that these calls are using a full spark cluster the final kind of paradigm is training one model per group and here the interesting cases where there enough groups that we really get back to that single machine training case where we can scale out by distributing over groups and train each groups model per spark task so here's kind of a diagram of it and in this and recommend him using spark to distribute over groups within each spark task doing tuning for a model for that group you can do tuning jointly over groups if it makes sense though important tools to know about here are of course the Apache spark compan is UDF's were you - to do this aggregation and coordination and with any each worker you could use second learns native q9i prop or whatever cool so that touches on the main architectural patterns I've seen in the field and getting into workflows I'd really like to touch on common workflows and particular tips for each so in order to get started my first piece of advice is start small bias towards smaller models fewer iterations and so forth this is partly being lazy smallest cheaper and it may suffice but it also gives a baseline and some libraries such as say TF Kerris which I'll demo later support early stopping and algorithms which can take that baseline into account it's also important to think before you tune and really by this I mean a collection of kind of good practices so make sure you observe good data science hygiene you know separate your training validation and test sets also use early stopping or smart tuning wherever possible you know I often see people use say Karis where they are tuning the number of epochs but it's often better practice to fix the pervy pucks at a large setting and use early stopping to stop adaptively to be more efficient also pick your hyper parameters carefully well this is pretty vague but I'll give at least an example sort of a common mistake I've seen made within the Lib tree models they have two parameters hyper parameters which are important for controlling the depth or size of the tree max depth and min instances preneur these Serbs somewhat overlapping functions and I've seen them tuned jointly but it's often better to fix one and tune the other depending on what you're going for finally I'll mention picking ranges carefully this is hard to do a priori but I think speaks to the need for tracking carefully during initial tuning and improving that later on the next set of workflows I'll talk through our models versus pipelines so for this one important best practice is to set up the full pipeline before to you name and the key question tasks there is at what point does your pipeline compute the metric would you actually care about because that's really the metric that you should tune on related to that is you know whether you should wrap tuning and around a single model or around the entire pipeline and my advice there is to generally go bigger that's because you know future ization is really critical to getting good results and and they'll you know had a professor in computer science or an AI course back in early 2000s make the joke that you know if you had the best features possible then m/l would be easy for example you know if one of your future columns where the label you're trying to predict you'd be done and of course that's facetious but it does get to the good point that taking time to do feature ization in particular tune it is pretty critical so optimizing tuning for pipelines can come into play when you start wrapping tuning around the entire pipeline and my main advice there is to take care to cache intermediate results when needed the final set of workflows is really around evaluating and iterating the efficiently and there I'll say validation data and metrics are critical to take advantage of and just a priori record as many metrics as you can think of on both training and validation data because they're often useful later tuning hyper parameters independently versus jointly is a pretty interesting question where you know it since you do have this crypts of dimensionality it is tempting to say well let me do one hyper parameter fix it at a good value than the next than the next that's sort of more efficient but some of those hyper parameters depend on each other and so smarter hyper parameter search algorithms like an AI prompt which will demo later can sort of take advantage of that and be pretty efficient the final thing that's around tracking and reproducibility you know along the way of producing all of these models into you name you generate a lot of data code grams metrics etc and taking time to record these as well as using a good tool like ml flow to record them can save a lot of time and grief later one tip is to parametrize code to facilitate track in that way as you're tweaking what you're running you're not tweaking complex code you're tweaking simple and let's do that code well so I'd really like to demo this so I'll switch over to the data breaks notebook well so here I am and data bricks notebook keep in mind this code and the tools I'll be using or open source so good run and whatever venue um and in this I'm gonna demo scaling up a single machine ml workflow the goal being to show some of the best practices which we had talked through in this slides I'll be working with fashion M NIST classifying images of clothing using tensor flow terrace and the to name tool I'll use is high prompt this is open source provides black box hyper parameter tuning for any Python ml library has smart adaptive search algorithms and it also has the SPARC integration for distributing and scaling out which my team contributed a while back well so I'm gonna skip through some of this initial code for loading the data there are 60 km adjacent training 10k and test labels are integers zero through nine corresponding to different clothing items I just got a sense you know here the types of images we had seen from the slides earlier here's our sneaker not a dress and so I pulled a open source example from the tensorflow website and looked at it for what hyper branders I might tweak well modeled outfit had a few interesting ones batch size in a box jumped out at me course for epochs I'm going to use early stopping instead and that way I don't have to bother tuning it the optimizer Adam takes a learning rate which I'll tweak and the model structure for that I picked three example structures small medium and large in terms of size and complexity there are others which our future work so taking a look at tuning these I'm not going to go through the code in detail but I do want to highlight the beginning where I've taken care to parameterize this workflow this way as I tweak batch size learning rate and model structure I can just pass those on as parameters rather than changing my code itself I'm gonna skip through most of this code I will note that I'm taking care to log things with ml flow at the end and note that I can run this training code with some parameters this just an example run so now I'll skip down to tuning with high dropped so my search space here is going to be specified using the high prompt API if you're not familiar with it don't worry about it here I'm saying there are three hyper parameters I want to tweak and each is sort of given a range and a prior over that range where I want to search and here's a good example of taking care of choosing those ranges carefully here I'll call out I use log uniform distribution for the prior for learning rate device for smaller learning rates because I've seen success with those in the best and then I call hyper ops minimize function where I minimized the loss of this return by this training code over a search space using a smart search algorithm here's the spark backend same let's fit at most eight models testing possible hyper parameter settings and parallel at a time 64 possible models total and run it so I already ran this because on this toy cluster which had two workers no GPUs it took about an hour good example of why it's important to make sure you record beforehand the metrics you really care about it's also nice to take a look at those metrics of course and here I'm using the ml flow experiment datasource where I'm loading the runs for the experiment for this notebook adding a column duration and you know we can take a look at this here's a histogram of the duration of tuning models most Rin pretty quickly but a few took the better part of an hour and so you know might want to consider whether we want to spend that time well so I'm gonna open up the ml flow UI for this notebooks experiment and here we see I have this one run for Hyper opt with a bunch of child runs for each of the models we fit under the hood for different batch sizes learning rates and model structures different metrics and also of course the model artifact in the Mel flow you I gonna compare these I've already set this up and it's pretty interesting to kind of take a step back and look at the correlations between batch size learning rate and model structure with our loss you know if we look at some of the best losses we can see those came from this medium model structure small learning rates and so forth but if we increase this we can see pretty good losses actually came from that largest model structure but what's interesting is when we look at actually the loss or metric that we really care about the test accuracy there if we look at the best possible model it actually had kind of a middling loss and was from this largest model structure so release speaks to the need to kind of take time to record extra metrics and especially the ones you really care about and so that you can go back later and maybe explore this model structure further well so I'm gonna flip back to the slides cool so those demo to give you a bit of the taste of what we've been talking about in our connection workflows I'm gonna go quickly through these last slides because they're really a grab bag of tips and details and largely have quite a few references for looking at later on so as far as handling code getting code to workers is an interesting problem where it's generally simple use pandas UDF's or integrations but debugging problems can be tricky and there are my main recommendations are to look and worker logs and in python import libraries with enclosures passing configs and credentials is also can be tricky here I don't show it because Stata Brooks is handling the ml flow runs and credentials under the hood but this helpful resource has some info on some of these topics moving data is also an interesting problem where for single machine which ah ml broadcasting that data or loading from blob storage can both be good options depending on the data size caching data can also come into play and becomes even more important and distributed ml blob storage data prep is really important as data sizes start to grow and their Delta late at a storm and TF records or key tech to be aware of in these links to resources are are nice in terms of walkthroughs of these different options then finally for configuring clusters here are the main discussion points not not tips really by discussion points around single machine ml we're sharing machine resources and selecting machine types can be pretty important for sharing resources they're really talking about the question of if you have multiple smart tasks fitting different models on the same worker machine how are they sharing resources so thinking about that beforehand can be critical for distributed ml right sizing clusters and sharing clusters can be important topics and for the latter I'd really say take advantage of the cloud model and spin up resources as needed so that you don't need to share clusters and complicate things and again this resource has a bit more info well so to get started I'd recommend first taking a look at the different technologies which you've been working with tools to know about here are listed on the right for each of the technologies on the left I won't go through these but definitely take a look at them if any of these texts call out to you and finally the slides and notebook are in tiny whirls up there in the upper right and those slides will have the links to these many resources I've listed there are few more listed here and yeah I hope they're useful including those talks from last year which go into a bit more of the fundamentals of hyper Krim nerds yuning with that like to say thank you very much for listening and love to answer a few questions you 