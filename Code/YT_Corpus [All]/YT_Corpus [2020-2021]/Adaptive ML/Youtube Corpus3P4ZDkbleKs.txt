 okay can you see uh the jupiter page okay we can yes it's very small so you probably want to zoom in yeah it's very small don't worry don't worry it's gonna get bigger okay these are my actual slides ah perfect that's great go ahead okay wonderful all right okay so the the link on the agenda is just a link to this um jupiter uh page where there's a bunch of folders i just wanted to direct you to the fact that um my talk uh is is also a notebook and you can find in this nbs folder um i've left my binder open too long so you you can't actually see it uh because it's shut down but um the notebook is called talk dot ipined so if you want to follow along with the notebook uh that's what you can do uh okay so hi uh thanks uh lucas for kind of segueing into this talk uh i'm gonna talk about uh physics analysis as a differentiable program uh and this presentation yeah is itself a notebook uh so yeah if i run that style above i'm going to get four and so that'll help us have some interactive fun all right so uh what i'm talking about today um is you know what does it even mean for analysis to be differentiable you might uh sort of imagine what that could mean uh from the previous talk but then why even should we care about that um and moreover how can we differentiate you know physics in the first place um some of the libraries you might have seen already uh and then uh we can see in action um with uh neos which is an example of fermentation a differentiable analysis that involves machine learning oh my gosh everyone's favorite thing uh so okay i'll draw you in at the start with this uh workflow that i uh penned by hand um because this is jupiter notebook uh and you can't do normal slides so the uh from the bottom left um this is a typical analysis workflow maybe uh where we start off with some data um have some kind of cut flow uh then from there we'll construct some kind of you know observable or statistic of the data um and from that we build some statistical model um and uh using the likelihood from that statistical model we'll build up this you know test statistic profile liquid uh ratio uh then we'll do a hypotheses test uh and see where the new physics is uh we'll get you know like a p-value or limit as the result of this of this um analysis and so you can see that this analysis kind of has some uh free parameters right so those cuts are arbitrary unless we say otherwise and that observable might be a function that you can parameterize you know it could be something multivariate we do that a lot nowadays right and so uh how do we optimize those parameters um you know can we even uh well sure we can right uh and here's what we might do uh maybe if we have a set of cuts we'll do some kind of grid search uh and we'll try and you know uh maximize this uh azimuth significance um right uh nothing new there um and then if we have a neural network uh we'll just do we'll just do gradient descent we might like a really good discriminator between signal and background because you know it makes sense kind of a sensible objective right um but i guess the question i wanted to raise here is you know why do these methods and goals of optimization differ if they are two things in the same pipeline right surely they should have the same objective uh and they don't have to differ necessarily so here's a example um thank you to alex held for this notebook uh where um it's a cut being optimized uh with respect to the asthma of significance but using gradient descent um just like a neural network and so to make this work in practice we don't actually we can't use like this you know a harsh cut because it's it's non-differentiable uh and so we actually have to spread the you know the uh the profile of the cut out a little bit so it's not like a zero weights and one weights it's like you know a continuously varying weight between zero one and one uh applied to all events uh and there you see it training in getting a pretty sensible cut value uh and you can also there's a paper there link that uses the azimuth significance as a loss function and so really these uh you know these uh these things are closer than we think and so you know from this you might be able to imagine some kind of jointly optimized analysis pipeline uh with respect to a single objective uh trained using gradient descent because you know that's a well-studied method of optimization and we have all this automatic differentiation magic uh and so to do that you know that kind of that kind of raises this question right i mean like what should that objective be i mean asthma of significance is good and it's you know it's a very tractable and useful objective uh but maybe maybe we could do better um and there's some really nice insights into this if we uh have a look at uh look through this um survey of deep learning and how it's applied to lhc physics a great review by uh dan weitz and dan guest and kyle kramer and so there's a passage here that says that tools are often optimized for performance on a particular task there's several steps removed from the ultimate physical goal of searching for a new particle or testing a new theory and then you know that might make you think that like binary cross entropy for example could be an example of one of those things where you know because it's it's not a p-value right uh and then another thing i had to think about is that you know sensitivity to these physics objectives account needs to account for systematic uncertainties so we you know you see things like the sculpting of backgrounds uh and you know the setting for the nominal um uh nuisance parameters might not be the best when you profile over you know a a whole range of nuisance parameter settings and so you know just to summarize that you know so we want something that's a goal directly related to our physics objective and we we want to take into account this you know full profiling uh in order to be robust to all these systematic changes um but don't we already have this right it's at the top left of this plot it's you know we get we get our limits so p value out of the analysis and that's what we want to improve so you know if we can construct a full analysis uh where we can back propagate through the whole thing we can optimize you know the median significance um you know actual significance uh or with respect to that rather so we can optimize our you know these three parameters uh by taking the derivative of the limit and uh with respect to the cut and updating the parameters by gradient descent you know just like a big neural network if you want to think about it that way um okay so that's kind of cool so this is what we mean by differentiable analysis so an analysis with some free parameters that we can optimize and to end uh with gradient descent and so there's the sort of update rule right uh you know we just move in the opposite direction to the gradient uh modulated by some learning right uh and so this is you know this is an example of uh differentiable programming which is you know another buzzword con by uh yan likum you know which is sort of meant to be the superset of machine learning and that you know we're not just dealing with one neural network here we have this whole you know uh pipeline that's differentiable um and so one thing to think about that we kind of saw in lucas's tutorial is that uh we kind of have the uh the chain rule um to consider here um because the uh whatever we have to you know where we whatever we take the derivative of um has to back propagate you know through the entire analysis chain and so there are a bunch of different derivatives that we actually have to think about in terms of can we actually evaluate them uh and there's no guarantee that that's the case um and you know so this slide can then be amended to it's not just what should the objective be but can we also evaluate its gradient and so is every step of the analysis differentiable and so here's an example right it's an example of something in hep that's not immediately differentiable so when we think about histograms uh you know the the heights of the bin uh don't necessarily vary smoothly if we change the data a little bit right because the you know the bins will just jump up and down in height uh by one uh if the events in the bin or not in the bin um but you know this is the problem right because updating the you know like the data distribution you know or at least like a representation you know downstream of the data um updating the shape of that is exactly what we do if we updated like cut values for example so we need this change to be well behaved um and here's just a a bit of code that i'll run to show a an example where we shift um a gaussian distribution to the right you know we just vary the mu uh and then we see what happens to the corresponding histogram where we just we draw 100 points from that distribution and see what the history how the histogram changes and you can see that the bin heights are kind of just jumping up and down you know uh really nearly however you like uh and but not not in a nice sort of smooth way and so this is something we uh want to amend here you know this is the kind of derivative we care about um and so one way to remedy that could be this thing called a kernel density estimation and that formula might look a little bit ambiguous all it really is it's a mean of some kernel function okay centered on each data point uh and then smoothed with this parameter h um and to give you a bit more intuition what we actually do in practice is use a standard normal distribution for k and then that formula just reduces to something much more intuitive you know it's the mean of a bunch of gaussians centered on each data point uh with some global width that we uh that we evaluate ourselves we decide what that is um and so there's this one-size-fits-all kind of modulation and you can kind of think of it uh think of it a little bit like like the bin width right it's a sort of non-parametric hyperparameter in a sense uh and so just to give you a little bit of intuition here's the kernel density estimate um on a set of data which you can sort of see at the bottom um and this is what we have this is what happens when we vary the width of all those little gaussians uh and you know it can kind of get crazy if you make it really big and then right at the start it's kind of like horrible and jaggedy and so this is kind of interesting because you know the choice of bandwidth you know uh really uh changes the kd quite a lot it's very sensitive to that more so than the choice of the kernel function actually which is quite quite interesting um so say we have that and then we want to set of uh counts from that you know like in our pass on statistics kind of way well we can actually we can get that pretty easily right because a kde um since it's a bunch of gaussians it's got a tractable pdf and so we can just evaluate the area under the curve for each uh bind interval uh and this is what that looks like uh and so uh here is a histogram of fixed spinning uh on some data and here's the corresponding kde uh and and then there's the area under that kde um for each um uh point uh or each um bin sorry uh and that's a kind of nice kind of approximate histogram and you can see that for really small bandwidth it looks exactly like a normal histogram and there's a bit of intuition there because that mean that means that the bandwidth um for each gaussian is really really small and so it's basically like having a direct delta function at each data point which is basically counting right counting uh like a histogram so it kind of makes sense and so this is an idea of like a soft histogram and so does it resolve our issue let's run this code and find out um if you're running this in the binder um you need ffmpeg to actually like display the movies inline so there's like a little commented up thing that lets you save it and view it offline and so you can see actually that these bins are moving pretty smoothly as we vary the kd uh which is really nice right that's exactly kind of what we want maybe uh we can do it a little bit faster um oh wait no the um the variables in the previous slide i'll just leave it um but okay nice i you know here's one example of a smooth histogram um or soft histogram and here's another one um be familiar to anyone who's worked with a neural networks right uh this softmax uh function where we um sort of just uh you know exponentiate uh the data points and then normalize them um which is you know gives you values between zero and the sum to uh the sum to one uh which is uh really nice because that's kind of like spreading out that um that one um event into you know many different bins so it's not like the event is in one bin and it's not in any of the others it's kind of like it's weighted in all the bins and that's sort of the way to think about these differentiable histograms uh and it's kind of a natural choice right if you have a neural network because you can just slap this as your last layer all right you can just soft max the output uh and then you know if you do your neural network on an event-wise basis you can then get like a contribution to a set of bins uh on a per event basis just like a histogram so this is kind of nice um okay so here's the grams fine we can we can do those uh another step would be the likelihood function that was in our analysis workflow and thanks to you know awesome people at pihf this has mostly covered uh almost uh there are some you know things we need to iron out in terms of everything in pie chef being differentiable but we can do pretty we can get pretty far with it uh and yeah so just a big thanks for the team because this steps kind of already done for us uh if we're using his factories uh likelihood that is um but then we get um we hit a bit of a roadblock when we think about the test statistic because this profile likelihood ratio which you all know and love has these uh you know things with hats on them and you know if we have a hat we normally have a fit right that's kind of how it works uh and so um when we're doing these fits um to do this kind of automatic differentiation uh we have to you know unroll every single iteration um of the fit uh you know at run time trace all that execution and then back propagate you know like all these millions of different iterations which is you know computationally not really very nice um okay that's sort of the first bullet point here and then another thing to think about is you know the actual number of iterations itself doesn't vary smoothly uh when you change the data a little bit so that's kind of like another way in which this and uh which this is not differentiable how do we kind of remedy this uh well uh we can use this thing called fixed point differentiation which is kind of a nice thing we stumbled upon uh so if we think about like a minimize or maximize routine uh it has a fixed point uh when evaluated at the minimum that is to say when we initialize a minimized routine it will return some converged value x star if we would to evaluate that minimize again at x star we'd still get x star back and that's what we mean by a fixed point uh and we can do a bit of um we can we can kind of exploit this a little bit because uh if a fixed point exists we only really need to look at these uh converged iterations to actually um you know get the gradient information because that's what we care about right we only care about how this this optima or these optimum sort of vary as we as we're sort of changing um parameters upstream uh and so we don't have to you know think about all these you know things at the start uh where you know it's initialized at some value and it kind of jumps around a bit and it you know that's gonna you know that's not gonna affect our final answer uh and so this is kind of a nice way to get around it um and there's a little bit a little bit more uh information on this if you uh follow that a link and just just to kind of put this back in context uh you know what grading was i talking about there right and so the shape of the likelihood um and also of course you know its maximum point um is implicitly a function of the yields used to construct the hist factory model and what i mean by that is that you know if we were to change those yields of course the shape of the likelihood would change but you know we don't explicitly write the likelihood as a function of those variables and so um you know this is important because uh when we differentiate the profile like the hood we're going to end up differentiating you know the best fit values with respect to these yields at some point so um we want to know how the you know how that maximum varies as we vary the model construction because you know that's essentially the whole um everything from observable construction to the profile likelihood that's you know that's that entire derivative in the big grand scheme of things right so uh and then we just times it by how those yields uh vary with respect to whatever parameter we wanted uh and so you know to try and internalize that a little bit um you know we can think about that moving gaussian example where we changed some you know parameter uh mu uh which changed the shape of the likelihood or you know the gaussian just shifted it to the right a little bit um and then the histogram changed a bit but you know we wouldn't write the histogram as a function of that parameter right it doesn't really make even though it makes sense it's not how we would write it it's not an explicit function and so it's the same thing that's that's just to try and uh get some intuition yeah because we can find the gradient of how you know these fit um best fit points would vary with respect to those yields but we know we don't write them explicitly as a function of those yields and that's just kind of what we mean by uh implicit um in in this context uh and so okay just to you know zoom out a little bit put all this together uh so using you know soften histograms and cuts we can differentiate you know the the sort of start of the analysis you know all that data preparation uh and constructing the observable um and then thanks to pihf we kind of get the derivative of our likelihood function uh sort of for free which is really nice uh and then if we if we hijack you know uh the fitting using this whole fixed point differentiation idea uh we can extend that differentiability um all the way to you know the profile like the hood and you know like the p values that we get out of it which is really really nice uh and so we kind of have all the ingredients right in theory to like differentiate the whole analysis and so what happens when we actually try to optimize it uh and so this is where this whole neos things comes in right so you know initially it was named neural and optimized statistics but you know the whole point is that we don't actually kind of need this neural network to do this differentiable analysis uh so just change it to nice because it is also nice and you know click through to that github link to see you know the central repository and maybe leave a star you know if you're feeling like it um and so okay uh neos is implemented in jax just like uh lucas uh sort of uh gave you a really quick uh lightning tutorial on um i have an optional tutorial on the bottom there which i don't actually have to do now and so it's this just this idea you know you just write your normal analysis code in numpy which is you know pretty familiar and then you know those gradients kind of come for free in some sense which is you know really nice so this kind of pads out all the little steps in between you know that besides these you know big big picture blocks and we also make use of this facts library um that team's been really helpful um which you know uh sort of has this wrapper function that we wrap around this uh this fitting uh that lets us actually evaluate that you know the gradients in the way i described where we only look at the last you know like the converged iterations and so yeah thanks to both teams um all right now to talk about you know the neos workflow itself i got fed up with right thing you know all these slides are actually html right and so because it's a jupiter notebook and i can't do i can't format anything in a nice way so i got fed up and i drew this uh i drew this slide uh and so i'll just walk you through it very slowly uh um so the this is um this is the setup for this neos problem so we start out with uh two gaussian blobs we draw samples uh from both of them i call one signal one background and they're kind of close together uh so it's not like a trivial problem uh and then from there we create some kind of um systematic variation um you know like an up and down setting kind of like we might have uh in practice um and then we feed in uh x y coordinates uh for each of those four um four distributions um you know every single point we feed that into a neural network and that's what we're gonna you know think of as our observable here um and then so the output of that neural network becomes our observable and then we use one of you know our histogram methods uh you know it could be just a soft max at the end of the neural network or we could be using the kernel density estimate um if we have like a regression network instead uh and we'll get we will get some uh histograms out and then from there it's pretty smooth sailing right we just construct our model in the normal way um here with one nuisance parameter which is a hystosis if you want to use the the hysteractomy language we do our fit and then we get our you know cls evaluated at the nominal signal hypothesis and then we back propagate uh that's cls value let's update the weights and so what does this look like and here's a nice little animation to show what that looks like um so this is actually a slightly more simple example than i described uh in that slide because the um you can see there's actually only two orange bulbs and not three uh that's because um just for this simple case we're just taking the um we're using sort of we have like an up and down variation of the background and then we just take like the nominalism as the mean of each distribution uh and so uh we can see this is uh you know some optimization happening in real time uh and so the uh the background of that sort of a plot um on the far left that's the neural network decision function um you know it's the output um which is in this case it's two values because there are two bins and we're using a softmax and so as you can see it kind of not only does it learn to separate the signal and background it actually learns to wrap out a wrap around the uncertainty and it kind of does this trade-off between this not just signal to background ratio but also you know the background uncertainty you know really really nice behavior this is what we mean by like systematic aware learning um and we see the model on the far right uh doing that trade off and the cls value in the middle just you know going down down down down prairie park which is you know awesome uh if we do something slightly more sophisticated than that uh we use the kde and then we actually use that full hystosis that i was talking about um we get like a similar result it doesn't wrap around you know quite as nicely but you know uh this time we have three bins uh so a little bit more kind of um a little bit more complexity and expressivity there but you know the um kind of gets harder to enter you know just like um differentiating between two classes right uh so it's a little harder to interpret the output but you know uh even for this case we can see that a three bin histogram is optimized you know just as well uh which is you know super cool and that's that's pretty much you know the uh the meat of the talk and the kind of next that i just uh highlight um is you know to actually release a paper on this stuff although the software as it is right now is citable um then to you know to make phf fully differentiable as i was talking about before and then maybe even making you know awkward array uh differentiable because you know that you know it's used in like uh kefir and all these other um sort of uh you know iris type uh projects which would be you know awesome because then we could you know really differentiate like a real analysis um you know doing some more studies making some more physics tools differentiable uh and the kind of cool thing about all this stuff is that we kind of have an organization that's trying to target all this stuff now so um we have this thing called grad hep um which is an effort to just consolidate all these efforts into one place uh you know to build the differentiable um analysis blocks but then also you know also to apply them and create like you know common software packages and things like that and so there's like a new uh differentiable computing uh hep software foundation activity um you know which kind of encompasses all of this and i've got a link to that page there which you know lists a couple ways involved and you know literally everyone is welcome to join in if this is something that you think is interesting because it's a you know super early stage um and so uh just one last slide to give some thanks some extra thanks to the office of the inferno paper which is uh very similar to this work a big inspiration thanks to people in irish happ for some nice support and discussions thanks to lucas for being a great supervisor of course thanks for you know to the organizers and you guys for listening that's uh everything i've got to say nathan thanks a lot that was really interesting thanks um so we're a little bit over time but i think it would be a shame to cut so early and i think we could maybe take some questions so i will stop your share and if i can find the right place i can start my share here so yeah quite a lot of questions about the kernel density estimation so jim asks if you're using the kde to represent measured distributions does the choice of bandwidth relate to the choice of bin width in the histogram and is there methods for choosing an optimal one yeah that's that's a great question right um like how do you how do you choose the bandwidth it's it's a free parameter too uh and so yeah we're actually um we're doing a couple studies at the minute to sort of investigate this because um uh okay well strictly um there is some things in literature that you know give this quote unquote optimal bandwidth in terms of like minimizing the mean squared error um with the true distribution and you have to some sort of like there's an adaptive step where you do like a kernel density estimate of a kernel density estimate um to get that so that's a little bit uh complicated um but then um you know the the trade-off you kind of want to make um so you reference a bin width um and so uh in terms of like um how does how does it relate well i mean it relates in the same sort of way you know if you have like to if you have too large a bandwidth it's kind of like having putting everything in one bin in that you're kind of just like smearing out the you know the artifacts of your distribution um but i guess in terms of gradients that's kind of like another question um we'd actually want the bandwidth to be pretty close to the bin width because um if you can imagine you know like a kernel density estimate um sort of centered um like in the middle of a bit of a bin um or you know one of the individual data points that is centered in the middle of the bin um the kind of leakage into neighboring bins is kind of like optimal when the bandwidth is kind of on the order of the bin width or a little bit less uh and so in terms of like representing the gradients and the like a true histogram uh there's actually kind of like an interplay in practice between the bandwidth and the bin width uh just kind of interesting um yeah but you know that we're still doing studies on this and it's a you know it's a very good question um yeah sorry that was a bit rambly was that enough well it was a bit of a kind of watch this space i think you it clearly isn't yeah yeah a little bit lucas might also have something to say on that yeah no i mean i don't have anything special to tell you explain it very well okay thank you yeah and then there's a question from hans um how do you treat the soft histograms in lightly the destination which usually assumes possible statistics in the bin because the bin values of soft histograms are not poisson distributed right yeah so it's it's kind of like i mean yeah yeah so the um i guess um you know when we're using a histogram it's kind of like um doing some sort of uh density estimation right um and i guess yeah a histogram is a natural way to describe poisson statistics but i mean as you can see like the kernel density estimate um in the limit of quite small bandwidth it actually gets really close to this so i think i think they're you know you're right there is there is a discrepancy there but um i don't think it necessarily matters that much um it's something i'm actually still thinking about in terms of like how accurate the model is um you know because yeah like like you said like i said i guess we use that you know we treat it uh you know like a set of class on counts um so yeah okay it's a bit of an approximation um but i guess any anything that smooths out something discreet will always kind of be a bit of an approximation but um it still works in practice yeah another way to think about it it's a big approximation yeah yeah i mean another way to think about it is that for evaluation you can then always switch to the hard version again so this is basically to optimize the analysis let's say let's say your ventilation completely defined by your network and so you use that to optimize the neural network by additionally taking into account systematic uncertainties but then once you've optimized that you can still kind of resolve it back to the hard version and then just fill the histogram as you usually would right but it's as a tool to optimize the analysis yeah that's also a really good reason okay maybe let's uh just jump quickly to what lindsay was asking i think does the same kind of observation about poor statistical properties especially with sparsely populated data and a suggestion using penalized basis splines which might be an alternative interesting i'm gonna have to google that but um yeah no no thanks for the suggestion um and yeah yeah i guess you know the first question is kind of answered what lucas said like you don't actually you know in practice we don't actually have to use the soft histograms when we calculate our final you know result that we put in the paper and submit to conferences yeah but then you're you're uh throwing away all of the optimization that you're doing based on systematic errors and whatnot like that especially when it comes to the tails so it seems kind of at odds with itself i mean not completely if you go back to nathan's animation with the blobs um i don't know if nathan can pull it up quickly yeah i can pull it up if that's yeah yeah um but but yeah yeah let's say uh like the background which has a you know big systematic uncertainty as to where exactly in face space it's located and basically what this trains you is similar to what kind of adversarial training does it basically minimizes the uncertainty uh that is kind of associated with this uncertainty of you know that background and so you also like even if you switch back to hard versions you will still retain um kind of that thing like that robustness that you learn right you'll you're not kind of uh you know like the like the final class value will not be the cls value of the hard version but i mean some of that stuff you still kind of retain right so here on the left hand side you see these three blobs right and the decision boundary wraps around them in the same way that it's going to maybe have a serial training group very good okay there's clearly we could talk about this for a very long time but we're almost 10 minutes over time but uh thank you very much to nathan and lucas for giving the introduction to this topic and lots of comments in the zoom chat by the way about people really appreciating this talk and i'm very glad nathan that you also gave the link to the new hsf activity area so i think we have a forum for this moving forward for people who would like to uh to be involved yeah exactly these are really fascinating questions but yeah i think we will close the session here 