 We present a radically robust locomotion controller for quadrupedal robots. Our controller enables robots to traverse challenging environments that have been unapproachable by existing methods. The proposed controller is driven by a neural network policy that is trained in simulation, and deployed on different versions of the ANYmal robot. Without any real-world data or accurate terrain models, the control policy is robust in natural environments with highly irregular terrain profiles and unknown physical properties such as friction and compliance. To evaluate robustness and generalization, we tested under various conditions and with different robots over the time-frame of more than half a year. The proposed method does not use cameras, lidar, or contact sensor information but only relies on proprioceptive sensor signals to adapt to diverse terrain conditions. The controller can handle dynamic footholds, remains robust even when the robot's feet are impeded, and successfully handles significant disturbances to the robot's legs which are often present when walking through vegetation. In fact, our system was able to traverse  any terrain shown in this video without a single fall. For each robot, the same controller was used in all environments and conditions. Here is the structure of our controller. The controller is based on a temporal convolutional network policy that modulates foot trajectories. The policy acts on a sequence of proprioceptive measurements, which are provided by an inertial measurement unit and joint encoders. By analyzing 2 seconds of proprioceptive history, the policy can identify different events, like this collision with the step, and adapt to different terrains and disturbances. Our controller can adapt quickly  to changes in terrain properties and maintains stability. Since we apply random disturbances during training, our controller can robustly handle large model inaccuracies like unknown payloads. The control policy is trained in simulation to follow a command direction on rough terrain. In every training episode, a new environment is synthesized, and external disturbances are applied as the robot learns to remain stable and maintain the desired heading. Training is done in two steps. We first train a “Teacher policy" that has access to privileged information from simulation. With ground-truth knowledge of the environment, the teacher learns adaptive locomotion skills through reinforcement learning. The trained teacher policy is then distilled into a TCN student policy which is used for deployment on the physical machine, where privileged information is not available. Training is conducted on three types of parameterized terrains: hills, steps, and stairs. Throughout the training process, the terrain parameters are adapted depending on the performance of the policy, such that they remain challenging but traversable. This is achieved through the use of a particle filter. As the policy learns and locomotion becomes more robust, more difficult terrain profiles are introduced. Even though training is conducted on only a small set of rigid terrains in simulation, the learned control policy is robust to dynamic footholds and deforming surfaces. The robustness of our controller has been validated on outdoor terrains with characteristics that have never been encountered during training. The controller successfully handles dramatic variation in surface materials and terrain profiles. The reliability of our controller was further validated via real-world deployment in the DARPA Subterranean Challenge urban circuit. Our controller was used for locomotion on the two legged robots of team CERBERUS. They successfully completed four missions of 60 minutes without a single failure of our locomotion controller. Our work opens new frontiers for legged robotics and expedites the deployment of legged machines in natural environments. 