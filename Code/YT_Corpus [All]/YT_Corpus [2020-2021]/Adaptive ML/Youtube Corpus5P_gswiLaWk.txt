 Welcome to today's lecture. So, what I would like to talk about today is robust machine learning training with conditional gradients and this is going to be the lecture part, now, and later there are going to be exercise sessions, where several of the concepts that we are going to discuss here, today, will be discussed in more detail.  All right, before I start, just a very quick plug. So, we do have various positions available here in Berlin both at TU Berlin as well as the Zuse Institute in Berlin and if you're interested in either postdoc or PhD position, please feel free to drop me a line or send me a message. So, what this talk is going to be about today is the question whether we can train neural networks or other machine learning type of systems in such a way that they are more robust to noise and adversarial attacks. So, I won't have time too much to talk about both the corresponding noise models and and adversarial attacks models that are typically considered. But the way of how you want to think about this is very simple. So, you train a machine learning system and you train it on, say, clean inputs and for whatever reason your inputs are not noisy or corrupted in a relatively benign way in the case of noise or in a more adversarial way in the context of adversarial attacks and by how much does the performance of your classifier or your machine learning system degrade and whether we can find a way of training such systems such that the level of degradation is a bit more controlled. And that's what's going what this talk is going to be about today. The outline, roughly, will be as follows. So I will start with a very simple example just to recall a couple of notions from machine learning and in particular the concepts that we are going to use. Then, I will talk a bit more about more general considerations in machine learning. This is really just to set up the stage a bit for the later exercises, also. I will then talk about stochastic gradient descent, which is the standard way of training basically all learning type of systems. And then, I talk about stochastic conditional gradient descent, which is a method that allows for constrained convex minimization or non-convex minimization depending on the setup. And that's the one that we are going to use later to robustify in a somewhat meaningful way our learning systems against noise and adversarial attacks. Just for reference: there's many hyperlinks here in the slides. Basically, all the references provided in brackets are hyperlinks. So feel free to follow them and also, please check out references contained therein because the references that i'm supplying here or providing here are certainly not exhaustive. And on a related note the statements that I make here today in many cases are somewhat simplified for the sake of exposition. So, be sure if there's any questions to either ask later in the exercise session or to consult the original sources for the exact definitions. All right, let's start with our very simple example here so what we care for is the concept or the context of supervised machine learning and empirical risk minimization So, what we want to consider here and this is really just to set up the stage is a very simple linear regression problem. So, what's linear regression linear aggression typically is I give you a set of points and I give you certain response variables and I want you to find a linear model. So, that's typically a hyperplane as you can see in the graphics, here. So, it's this red line that passes through the blue points that best explains the blue points. So, that's linear regression. Slightly more formal: So, you have a set of points X here where the x consists of k data points x_1 to x_k and these data points are a subset of, say, the R^n and then we have our response vector which is basically a vector y_1 to y_k that lives in the R^k. And so, our task is now to find the linear function phi in R^n such that x_i times phi is roughly y_i for all our data points i from 1 to k. So this bracket notion here, basically, denotes the set from 1 to k. And, we can also reformulate this in matrix form which is probably the version that you are more familiar with where we say that X * phi should be approximately y, here. So, that's our task. So, this task is typically formulated as an optimization problem and so is the search for the best theta which is, also, typically cast in the form of an optimization problem which roughly reads something like minimize phi, we sum over all our data points and then, we take the absolute difference between x_i*phi - y_i and we square the difference and that's basically the same as minimizing, here, (X * phi - y)^2 and in the two norm. And here, as a distance measure, we just took the the absolute values here. But of course, there's many other distance measures that one could potentially consider here. So that's just a very simple linear regression problem and that's basically what supervised machine learning is about. I give you a set of points that I know that typically refer to as features and then I want to predict something and I have a training data set where I know what this expected answers are. These are my labels, in this case, the y's here and then I want to find a model that basically translates my access to my y's through the model where we plug in the x's. All right. So, more generally, we are interested, as I mentioned earlier, in empirical risk minimization. So what's empirical risk minimization? these are basically all these learning problems that are of of a type that we have just seen So typically they are phrased as follows. We want to minimize, say, over some parameters theta here and that is typically defined as we have done, here. So that's the expected loss. So this is one over the number of data points times the sum Of (x,y) in D. D is, here, our data set of l ( f ( x, theta), y). The f (x) theta is our model. So that could be in in the case of linear regression, that would have been just the thetas itself and then we would have multiplied the vectors with that and the l is the loss function that measures how far the response or the output of the model given input x is away from y. And that's the so-called loss function and this type of problem is called the erm problem and the empirical refers you to the fact that, typically, the D is just a finite set of points, of a distribution, of data points. And closely related to this erm problem is the so-called grm problem or general risk minimization problem and the only difference here is that we don't take a finite expectation as we have done up here, so, on average, but we take an expectation over the whole distribution of data that we might or might not have direct access to. So this problem is more of a limit or a guidepost type of problem, here. And otherwise, it has exactly the same structure. So we minimize the expected loss of our model f parameterized by parameters theta on the data point (x,y) where (x,y) are drawn from our distribution D hat, here. And yes, so, what do these two problems have to do with each other? Well, under somewhat reasonable assumptions and if D is chosen large enough, then a solution to the erm problem, so the upper one, our approximate problem is a good approximation to a solution to the grm problem. So more specifically, what we mean by that is if you take a parameterization theta here and you plug it in into L hat, which is the general loss so to say, so that's the expectation on the right, here, then that loss is upper bounded by the loss on the empirical problem. So where we have only used a finite subset of the distribution plus something here. That is basically the square root of parameters that depend on the model complexity. So that's basically how many parameters how many parameterizations we could potentially have. So that's log phi here plus logarithm one over delta. That's our probabilistic guarantee here. So we have a we have that this holds with probability one minus delta, so the higher the probability of being correct the more the larger this number gets here, divided by the number of data points and then we take the square root of the whole term. And so, that means the more data you get the better the bound gets, obviously. It means the higher the probabilistic guarantee should be, then, the bound gets looser. And similarly, if you have more models available, like more parameterizations available, then this also gets worse. And typically in applications or in actual contexts for actual models, this one is actually pretty much loose and yeah, so what this bound tells you at the end of the day is that, in principle, if you have enough data and so on, so forth, then solving the erm problem is good enough to get a good solution to the gm problem. But as I mentioned this bond is typically very loose. All right, so, now, let's maybe look at a few examples so that you see that there's really a pretty rich class of problems contained in this context or in this erm formulation. The first one you have already seen in the beginning. That's just linear regression. So, here, the loss function is just like the output of our model (z_i - y_i)^2 and the model, here, is simply multiplying x_i with the phi so that's a linear model and here, basic linear regression with the 2-norm as a norm. And yeah, another example would be, for example, classification or logistic regressions where we have, say, several classes C. Then, the loss is basically defined via the cross entropy, here. I will not go too much into details here, but the idea is basically that you now have a loss function that is geared towards classification and yeah, as I said, it basically measures the cross-entropy and, on the other hand, for the function itself, our model itself at this point is, for example, either still a linear model or we could also take a neural network as a model or many other models. So, typically, we differentiate between picking the loss function and the actual model here. All right. What else do we have? Yeah, for example support vector machines. So here, the idea is that you want to find, you want to also solve a classification problem, typically, and you want to find a hyperplane that best separates the two classes. So here we have the labels x_i being, say, zero or one, oh, sorry, y_i being either zero or one depending on this you pick one of the two terms and the model again is a linear model in this case here. And, last but not least, the all-abundant neural networks: here, the loss function can be one out of many loss functions. Those that we have seen up here or things such as hinge loss and many, many other loss functions and the function f here is typically a neural network that is parameterized with weights theta and upon input x_i computes our output z_i here then this part goes into the loss function. And yeah, so, there's basically an infinite number of possibilities and combinations possible, and it really depends on the context and the type of learning problem that you care for. All right. Yeah, so now, good, now that we have this erm problem, we need to ask how to to solve this erm problem. And so, this is typically done with optimization and a very simple idea that you might come up with is: hey, you know, let's suppose that my function L here is differentiable. Then, I can compute the gradient and I just do basic gradient descent. So I update my theta t+1. That's basically my old theta t minus some learning rate or step size times the gradient of L at theta t here and yeah, so, if you want to have a bit of background information here on gradient descent and general convex optimization, there's a blog post available that I wrote some time back that provides a bit of background information here. So this type of update is potentially one approach that we could try in order to optimize the loss function here. The problem with this is that if you think back that this function L was defined as a sum of our data points, then we can have many, many data points at the computation of this gradient can be actually quite expensive and that's a very well known and understood problem and so the solution here is the following. Let's just re-examine our loss function. So we have our function L here that was defined as this sum that you can see here and if you take the gradient and then recall that the gradient is a linear function, so the gradient of this whole thing is nothing else but the average of the gradients computed at the respective data points. So what this means is that, if I want to compute the gradient for the full function, it suffices to compute the gradient for each of the separate loss functions for each of the data points and then just take the empirical average over those. Why do we care? Well, we still don't want to compute this actually because it's too expensive. But the reason why we care is that, if we now sample a data point (x,y) from our data distribution D uniformly at random and we take the expectation of the sampling process and the computation of this gradient then specifically for that data point, then that expectation is actually equal to the gradient of the actual loss function of the whole problem. And, that's the basic idea that we want to exploit now. And yeah, in a very natural way this leads to what is called stochastic gradient descent. Here, we update in a very similar way as in the context of gradient descent. The only difference here is now that Rather than taking the exact gradient of our loss function. We now take the gradient computed at the loss function for a given data point (x,y) and this data point (x,y) is sampled uniformly from our data distribution here. And yeah, so this stochastic green descent algorithm is one of the most used algorithms for machine learning training together with its many different variants and just so that you get a bit of an idea what this means. So typical variants include things such as batch versions. So up here, we have sampled a single gradient. That's sometimes or often not such a good idea because the gradient could be quite noisy and you have a lot of variance. So what you do is you don't sample just a single gradient, but you sample the so-called mini batch, say, 250 data points, you compute the respective gradients and you average them together. This helps with reducing the variance of the estimator, but it also helps in general with stabilizing the whole optimization process. The second very common and important thing is so-called learning rate schedules or step size control. And the reason is that in its standard form, as it is given up here, the method for a fixed step size does not converge to the optimal solution but only to an optimal solution, only to a random variable that is actually oscillating around the optimal solution due to the variance of the gradient estimator that you cannot get rid of. And so the idea of these learning rate schedules is that you reduce the learning rate over time and by doing so, you implicitly also reduce the variance and that allows you to converge to the optimal solution provided that the problem has an optimal solution, it's convex and so on and so forth for now. In particular, this also means, of course, that this is, if you think about it for a few minutes, it's not completely trivial, because if you reduce your learning rate too fast, you might not converge fast enough to the optimal solution, so it might take a very long time and if you do not reduce the learning rate fast enough, you might be oscillating around the optimal solution for a long time. So it's a bit tricky to do that in practice, but there's a couple of guidelines or heuristics that people have been using beforehand and there's also algorithms that automatically adapt to their respective required learning rates and things like this, which brings me to these so-called adaptive variants and momentum variants. So, typical candidates here are RMS prop, Adagrad, Adadelta and Adam and these are all variants of solvers that try to overcome this dependence on the learning rate and either have learning rates that depend on the structure of the feature and they dynamically adapt to this or they use some other variant of learning rate control to basically reduce a bit of the dependence on the learning rate schedule, here. And, last but not least, a very important technique in this context is also variance reduction techniques. So, here, the idea is that you still sample gradients exactly as we have done up here in in equation SGD, but once in a while and that is a well controlled once in a while, we compute an exact gradient and then we use that gradient to build a better estimator of the gradient which has reduced variance. So that's called check pointing or variance reduction and that leads to algorithms such as SVRG, and these algorithm have algorithms have the property that they automatically reduce variance over time and they converge also, for example, with the fixed learning rate. However, which is not completely understood I believe, in the context of machine learning, very often, variance reduction does not lead to as well-trained models in terms of generalization as more traditional methods. All right, so that's just a bit of an overview of the different variants of stochastic gradient descent. Just to see what this means in actuality: So here, I took two graphs from the blog of Sebastian Ruder. Feel free to check out the blog. There's also the actual animations that I could not embed here in the slides. And, what you can see here is you see different solvers on the left hand side. For example, you see SGD. That's the very basic vanilla one that we have seen and many other variants like momentum variants adaptive variants and so on and so forth and you see that their convergence behavior can be very different due to the way of how they handle learning rate and how they also deal with local optimal solutions. Another problem that these methods very often have is that they are susceptible to being trapped in saddle points. And that's what you see on on the right hand side. So SGD and many other variants they get stuck in a saddle point and just go back and forth versus these adaptive variants such as Adadelta, Adagrad and also RMS prop are somewhat or much better depending on the application context in terms of escaping from saddle points. All right, so that's basically just what I wanted to mention in terms of stochastic gradient descent. So, what's the whole deal here with the robustness question that i've mentioned in the beginning. So let's go back to our erm problem. So the erm problem was basically of the form: minimize theta one over the number of data points times the sum over the data points and then we have these loss functions evaluated at the output of the model at x as well as y, here. If you trade these models for example with stochastic gradient descent, what can happen is that this theta here can actually get quite large. So, in actuality in terms of numbers. And since the neural network or the model, your prediction model is nothing else but a mathematical function at the end of the day and let's suppose it's smooth for a second, then what this means is the larger the numbers, basically, the more sensitive the model is to, for example, input noise and perturbations. And that's typically because the Lipschitz constants that are involved in measuring gradients here might be large as well or increase. So, and just so that you see that this is actually an issue, let's look at the following. So what we have here on the left is the performance of a neural network that we trained on the so-called MNIST digits recognition data set and what you see is this is a very simple model, nothing special. We trained it for, let's say, a hundred epochs and you get something like close to 80 percent performance. Just straight out of the box. And now, what we do is we take the same data and we evaluate the model at the different stages on noisy versions of the data and what means noisy here, we basically add a bit of noise, in this case actually pretty aggressive noise to the input and then what you see is that the performance for moderate noise, let's say, here, our standard deviation of our noise model was 0.3, already significantly drops from close to 80 to about 60 percent, and if you further increase the noise, then you see that the performance of the same model, basically, drops to under 40 close to 30 percent and what you can also see if you pay very close attention, you see that the performance goes down as the epochs proceed and that's because the model is keen of the clean data and it fits the clean data very well and the better it fits the clean data the more problems it has to deal with noise and that's due to the increased size of the Lipschitzness of the model among many other things. And yeah, so the question is can we do something about this and that's really  what we will see, now, how to do that. So a partial solution, there's many solutions to that out there, but a partial solution for example is to constrain the erm training and what I mean by this is that you don't pick the thetas from arbitrary sets, anymore, but you have to pick them from say a compact convex set. So, let's say you can take a two norm ball around the initialization and the numbers are not allowed to get too large and yeah, so that could provide a partial solution to the problem. And in fact, that's exactly what was postulated in many different contexts, beforehand, and the rationelle, really, is very simple: You want to find a better conditioned local minima theta that basically gives you the same performance as the one that you got from the SGD procedure, so stochastic gradient and any of its variants, but hopefully more stable because you control the numbers of this vector theta, here. This brings us to the Frank-Wolfe Algorithm which is a very important algorithm in convex optimization and sometimes also called conditional gradients. And what this algorithm does is it minimizes a smooth convex or non-convex function of a compact convex domain where the access to that domain is limited to a linear optimization oracle. What this means is, basically, you can optimize a linear objective function over the feasible region, here. So, let's see how this algorithm actually works. So, let's say our function here is f(x) being the two norm of (x - x*) squared.  x* is our optimal solution and let's suppose we are at some iterate x_t that could be for example the initial iterate and that's where we start and then what we do is, we compute the gradient of f at x_t, and then, that direction that is depicted here is the direction of the negative gradient and then we call the linear optimization oracle that allows us to optimize that objective function over our feasible region and that would lead to this point v_t here and the way of how you should think about this is you compute some kind of an approximation with the linear optimization oracle for the negative of the gradient here and then what you do is you form this line segment between x_t and v_t, and then, you compute a point on that line segment that is either given by a fixed step size rule or that minimizes the function over that line segment, and that's your new iterate. And then you basically continue the process and yeah, so you compute, you continue that process until you get close enough to the optimal solution and there's of course the guarantees for this algorithm: How fast it converges and so on and so forth. So why is this good? Well, first of all, this algorithm allows us to minimize f over the convex hull of, say, the vertices or any complex convex set that I provide as input and we sequentially pick up extreme points and form convex combinations that describe my current iterates. For many applications, it's very useful that if I do t iterations of the algorithm, I have picked up at most t plus one extreme points here, where the plus one comes from the fact that I have an initial point that i'm starting from. The implementation is very simple. What i'm giving you up here is the full algorithm. The only thing that you need is the two functions that compute the gradient here and that optimize the linear objective functions and they're typically provided in form of some oracle. So if you have these two functions, the implementation is very simple and then, as you can also see, this algorithm is pretty robust. There's very few parameters to estimate. There's just the gamma_t and even the gamma_t if you don't like you can replace by a function agnostic step size rule so that the algorithm is basically completely parameter free. So, yeah, so the key question of course is what do we do in the context of neural network training or machine learning training with this algorithm. Well, as before we would not want to compute the gradients for the full loss function because it's way too expensive, but rather we would sample a data point x y and then compute that gradient and what this really means in mathematical terms is that we choose an unbiased estimator here, nabla tilde of f at x_t. So x_t are the iterates here, not the data points, and with the property that if you take the expectation then this is really equal to the expectation of that gradient at x_t. Of course, these are conditional expectations, but I would not rather overload the slides with too much notation here. Yeah, and then the algorithm really looks the same. So what you see here is the normal Frank-Wolfe Algorithm. So, what you do is you replace the gradient by an estimated gradient which has the property that the expectation of that estimation is equal to the original gradient and then you let the algorithm take its normal way and yeah, so, as before, same thing holds here, there's many variants there. You can do the same thing. You can do mini batches to reduce variance. You also need some type of learning rate schedules to ensure convergence, exactly the same thing as before, and there's also variance reduction, variants of the Frank-Wolfe or stochastic Frank-Wolfe algorithm out there. They also use some form of a reference point although the guarantees that you get are in spirit a bit different from the ones that you would get for the stochastic gradient case. Of course, the key question now is like, given that we have this different algorithm here, does it work or is it useful or does it help? And, let's consider the same setup as before, the same MNIST type of example, and we compare two algorithms here, like we take the original stochastic gradient descent algorithm and our stochastic Frank-Wolfe Algorithm and solvers. And yeah, if you run them on the clean data, they, basically, on the clean data, they basically perform almost identically and however, now, if you go back to our noise experience experiments, then you see that, while the stochastic gradient descent trained model drops basically from close to eighty percent to below 60 percent, in the case of Frank-Wolfe, it only drops from 77 or something like this to a little bit below 70 percent. So it's about 10 percent better than the stochastic gradient trained model for the intermediate or intermediate noise level set up and in the large noise setup, of course, the performance of both algorithm further decreases but you see that the gap between the The Frank-Wolfe Algorithm and the stochastic grand descent algorithm is even wider and the model that you obtain with the Frank-Wolfe algorithm are significantly more robust. All right. Yeah, so that's basically all that I have to say. You will see many, many more details and experiments in the upcoming exercises and with this: thank you very much for your attention! 