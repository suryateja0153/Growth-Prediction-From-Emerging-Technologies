 ... ... good morning, and welcome to TensorFlow Dev Summit Good morning, and welcome to TensorFlow Dev Summit! Good morning, welcome to TensorFlow. Good morning, this is a fine morning, and welcome to TensorFlow. Good morning, hello, this is a nice morning. Welcome to the first-ever online-only TensorFlow Dev Summit. Good morning, welcome to TensorFlow. Dev morning, welcome to Summit. Welcome to TensorFlow Dev Summit. Good morning, and welcome to TensorFlow Dev Summit Good morning, everyone, and welcome to TensorFloDev Summit Good morning, welcome to TensorFlow Dev Summit Good morning, welcome to TensorFlow Dev Summit Hello, this is San Francisco Summit 2020. Hello, this is TensorFlow Dev Summit 2020 and welcome. Hello and welcome to TensorFlow Dev Summit 2020. We're having the entire summit online for the first time ever. Good morning, welcome to TensorFlow Dev Summit. Hello, welcome to TensorFlow Dev Summit. This is the first time we've done TensorFlow Dev Summit online. Good morning, and welcome to TensorFlow Dev Summit. The summit will be captioned. Good morning, and welcome to TensorFlow Dev Summit. Please stand by for live captions. Good morning, this is the TensorFlow Dev Summit. Welcome to the TensorFlow Dev Summit. Good morning, and welcome to TensorFlow Dev Summit. Stand by for realtime captioning of the event. welcome to TensorFlow Dev Summit. Stand by for realtime captioning. Good morning, welcome to TensorFlow Dev Summit 2020. Good morning, welcome to TensorFlow Dev Summit 2020. Good morning, welcome to TensorFlow Dev Summit 2020. Good morning, welcome to TensorFlow Dev Summit 2020. Good morning, welcome t Good morning, this is the TensorFlow Dev Summit 2020. Please stand by for realtime captioning. Is are Good morning, this is the TensorFlow Dev Summit. Welcome! Good morning and welcome to the TensorFlow Dev Summit. This is 2020, and we are here at the TensorFlow Dev Summit Welcome! Good morning and welcome to the TensorFlow Dev Summit. Good morning and welcome to the TensorFlow Dev Summit We're here for the #TFDevSummit Welcome! Good morning and welcome to the TensorFlow Dev Summit. Stand by for realtime captioning. Welcome, good morning and welcome to the TensorFlow Dev Summit. Good morning and welcome to TensorFlow Dev Summit. Good morning, and welcome to TensorFlow Dev Summit Good morning, welcome to TensorFlow Dev Summit 2020. We're here this morning to welcome you all to TensorFlow Dev Summit 2020. Good morning, welcome to TensorFlow Dev Summit. Good morning, this is your captioner, we' working to make your experience more enjoyable. Good morning and welcome to TensorFlow Dev Summit. Good morning, this is TensorFlow Dev Summit. Good morning, and welcome to TensorFlow Dev Summit Good morning, and welcome to TensorFlow Dev Summit. Good morning, welcome to TensorFlow Dev Summit. . Hi, everyone. Welcome to the 2020 TensorFlow Dev Summit livestream. I'm Megan Kacholia, VP of Enginring for TensorFlow. Thanks fortuning in to our fourth developer summit and our first-ever virtual event. We're wishing all of you good health, safety and wellbeing, while we can't meet in person we're hoping the Dev Summit is more accessible than ever to all of you. We have a lot of great talks along with some exciting announcements, so let's get started. When we first open sourced TensorFlow our goal was to give everyone a platform to solve real-world problems. I had to share an example of a real-world. Erwin is a self-proclaimed AI enthusiast but was discouraged because he didn't have a computer science background. But then he discovered TensorFlow.js which allowed him to build this machine learning application that could classify bone images.. An Erwin not alone. TensorFlow has been downloaded millions of times with new stories like Erwin's popping up every day. And it's a testament to your hard making TensorFlow what it is today. So on behalf of the team, wayn't to say a big thank you to everyone in our community. Tang a look back, 2019 was an incredible year for TensorFlow. We certainly accomplished a lot together. We kicked off the year with our Dev Summit, launched several new libraries and online educational courses, hosted our first Google Summer of Code, went to 11 different cities for the TensorFlow roadshow, and hosted the first TensorFlow World last fall. 2019 was also a very special year for TensorFlow, because we launched Version 2.0. It was an important milestone for the platform, because we looked at TensorFlow end to end and asked ourselves, how can we make it easy to use? Some of the changes were Simply plying the API. The community really took the changes to heart and we've been amazed by what the community has built. Here are some great examples of winners of our 2.0 Dev Post Challenge. Like Disaster Watch, a crisis watching platform. Or DeepPavlov, and NLP library for dialogue systems. And like always, you told us what you liked about the latest version, but more importantly, what you wanted to see improved. Your feedback has bee loud and clear. You told us that building models is easier, but that performance can be improved. You also are excited about the changes, b migrating your 1.x system to 2.0 is hard. We heard you, and that's why we're excited to share the latest version, TensorFlow 2.2. We're building off the momentum from 2.0 last year. You've told us speed and performance is important. That's why we've established a new baseline so we can match your performance in a more structured way. For people who have had trouble migrating to 2, we're making the rest of the ecosystem compatible so that your favorite libraries work with 2.x, finally, we're committed to 2.x core library, so we won't be making any major changes. But the latest version is only part of what we want to talk about today. Today we want to spend the time talking about the TensorFlow ecosystem. You've told us that a big reason that you love TensorFlow is the ecosystem. It's made up of libraries to help you accomplish your end to end goals. There's a will too for everyone. If you're a researcher, the ecosystem gives you control and flexibility for experimentation. For applied engineers or data scientists, you get tools that help your models have real-world impact. Finally, there are libraries in the ecosystem that can help create better AI experiences for your users, no matter where they are. All of this is underscored by what all of you, the community, bring to the ecosystem, and our common goal of building AI responsibly. We'll touch upon all of these areas today. Let's srt first with talking about the TensorFlow ecosystem for research. TensorFlow is being used to push the statef the art of machine learning in many different subfields. For example, natural language processing is an area where we've seen TensorFlow really help push the limits and model architecture. The T5 model on the left uses the latest in transfer learning to convert every language problem into a text-to-text format. The model has over 11 billion parameters and was trained off the colossal clean corpus mindset. Meanwhile, Meena has over 2.6 billion parameters and is flexible enough to respond to conversational context. Both of these models were build using TensorFlow. And these are just a couple examples of what TensorFlow is being used for in research. There are hundreds of papers and posters that represented last year that used TensorFlow. We're really impressed with the research produced with TensorFlow every day at Google and outside of it. We're humbled that you trust TensorFlow with your experiments. So thank you. But we're always looking for ways to make your experience better. I want toighlight a few features in the ecosystem that will help you in your experiments. First, we've gotten a lot of positive feedbackrom researchers on TensorBoard.dev. A tool we launched last year that lets you upload and share your experiments' results by URL. At Neurips, we were excited to see papers. Second, we're excited to introduce a new performance profiler toolset in TensorBoard that provides consistent monitoring of model performance. We're hoping researchers will love the toolset, because it gives you a clear view of how your model is performing, including in-depth debugging guidance. You'll get to hear more about TensorBoard.dev and the profiler from Gal and other talks later today. Researchers told us that TensorFlow 2.0.x make it easier to make changes. It support numpy. TensorFlow.data. And TensorFlow datets are ready right out of the box. Many of the datasets you'll find were added by our Google Summer of Code students so I want to thank all of them for contributing. This is a great example of how the tf ecosystem is powered-by-the community. Finally, I want to round out the TensorFlow ecosystem for research by higighting some of the add-ons and extensions that researchers love. Libraries like tf probability and tf agents work with experimental version. And things like Jvm. But TensorFlow has never been about just pushing a deep model. This is one of TensorFlow's core strengths. It has helped AI scale to billions of users. We've seen incredible ML applications being built with TensFlow. We're really humbled by all the companies, big and small, who trust TensorFlow with their ML workloads. Going from an idea to having your AI create real world impact can be hard, but our users rely on TensorFlow to help them accomplish this. That's because the TensorFlow ecosystem is built to fit your needs. It makes having to go from training to deployment less of a hassle, because you have the libraries and resources all in one platform. There's no switching cost involved. I want to highlight a few new things that will help you get to production faster. First, you've told us that you love working with Keras in TensorFlow because it's easyo build and train custom models so we're committed to keeping tf.keras the default high-level API. But if you're not looking to build models from scratch, TensorFlow Hub hosts all the ready to use pretrained models in the ecosystem. There are more than 1,000 models available in TFHub, all ready to be used. When you're ready to put your model into production, you can build production-ready ML pipelines in TensorFlow Extended to make sure your ML engineering just works. From data validation to ML metadata tracking and today I'm very excited to announce that using TensorFlow in production is getting even easier with an exciting launch. Google Cloud AI platform pipelines. We've panered with Google Cloud to make it easier to build end-to-end pipelines hosted by Google Cloud. Cloud AI pipelines are available today in your Google console. And if you're running TensorFlow in Google Cloud, it have enterprise that we announced last year gives you the long-term support and the enterprise scale that you need. Finally, you can deploy your custom pipelines on custom hardware. In the latest version, TensorFlow is now optimized using Keras. Now helps you scale to petaflops of TPU compute. All of these libraries are within the TensorFlow ecosystem. Are 2.2 compatible and help you scale so your ML application can reach your users. But for AI to have that kind of impact it needs to be where your users are, which means getting your models on device. Now, we all know this requires working in some constraints, like low latency, working with poor network connectivity, all while trying to preserve privacy. You can do all of this by using tools within the TensorFlow ecosystem. Like TFLite. Here's an exple of how we've optimized performance for mobile net v1 from May last year to today. It's a big reduction in latency and something you get right out of the box with TFLite. We're also adding Android Studio integration, so you can deploy models easy, just simply drag and drop into Android Studio and automatically generate the Java classes for the TFLite model with just a few clicks. When network connectivity is a problem and you need these power-intensive models to work while still offline, you can convert them to run better on device using TFLite. In the latest version, we rebuilt the TFLite converter from the ground up to provide support for more models, more intuitive error messages when conversions fail and support for control flow operations. The browser has become an exciting place for interactive ML and TensorFlow.js is allowing JavaScript and ML operators to build exciting operations. Ththere are new models like FaceMesh and MobileBERT. HuggingFace's NPM package allows you to do question answering directly in Node.js. The next few years we'll see an explosion of platforms and devices for machine learning and the industry needs a way to keep up. MLIR is a solution to this rapidly changing landscape. It's backed by 95% of the world's hardware acceleratorrer manufacturers anit's helping to move the industry forward. We see how important infrastructure like MLIR is to the future of ML, which is why we're investing in the future of TensorFlow's own infrastructure. The new TensorFlow runtime is something you won't be exposed to as a developer or researcher, but it will be working under the covers to give up the best performance possible across a wide variety of domain-specific hardware. We're planning on integrating the new runtime later thi year, but you'll hear more from Mingsheng later today. So to recap, whether you're pushing the state of research, applying ML to real-world problems or looking to deploy AI wherever your users are, there's a tool for you in the TensorFlow ecosystem. Now I'd like to invite Manasi. Hi, everyone, my name is Manasi Joshi and I'm an Engineering Director on the TensorFlow team. TensorFlow ecosystem is composed of a number of useful libraries and tools that are useful for a diverse set of use cases, whether they're coming from researchers or practitioners, alike. However, the field of ML andI is raising the question whether we are building systems in the most inclusive, and secure way. I' here to tell you how TensorFlow ecosystem empowers its users to build systems responsibly, and moreover, what type of tools and resources are available to our users, to accomplish those goals. Before we deep dive into the details of what TensorFlow has to offer its users, let's take a step back and define what we mean by responsible AI. As we know, thatachine learning has tremendous power for solving lots of challenging real-world problems. However, we have to do this responsibly now, to us in TensorFlow, the way we define responsible AI is based on a five-pillar strategy. No. 1: General recommended practices for AI. This is all about reliability. All the way from making sure that your model is not over-fitting to your training data, it is more generalized than that. Making sure that you're aware of limitations of your training data when it comes to different feature distributions skews, for example, and making sure that the model outputs are robust. And making sure you're not using a single metric across all of your models to determine its quality, because different metrics matter to different contexts, how your model is used for promotion, demotion, filtering, ranking so on and so forth, right? The second inciple: Fairness. Fairness is an evolving thing in AI. For us we want it to create or not reinforce unwanted bias. It can be context-sensitive and it is a social challenge. Third, interrability, all about understanding the mechanics of behind-models' predicon. Ensuring that you understand what features, which features were important, which features were not. Fourth,rivacy. For us, this is all about taking into account sensitivity of your training data and features. And fifth, is security. In the context of ML, security really means thayou understand vulnerabilities in your system and the threat models that are associated. Now, for a typical user of TensorFlow, this is how the overall developer workflow looks like. You start by defining a specific goal and objective for why you want to build a system. Then you go about gathering relevant data for training your model. As we understand, data is gold for machine learning model training and so you have to prepare the data well, you have to transform it. You have to cleanse it setimes. And then, once you're data is ready, you go about training your model. Once the model is built, it's converged, you then go about deploying the model in production systems that want to make use of ML. Deployment phase is not a one-time task. You have to continuously keep iterating in an ML workflow and improving the quality of the model. Now, along with this developer workflow, there are many, many different moments at which you as a modeler need to be asking all of these questions. Questions like, who's the audience for my ML model? Who are the stakeholders and what are the individual objectives for the stakeholders? Going on to the data side of it, is my data really representing real-world biases or distributions skews? And do I understand those limitations? Am I allowed to use certain features in a privacy-preserving way, or are they just simply not available due to constraints? Then on to the training side of it, do I understand implications of the data on model outputs or not? Am I doing deployments very blindly, or am I being a little bit mindful about deployingnly reliable and inclusive models. And finally, when we talk about iterative workflow, do I understandomplex feedback loops that could be present in my modeling workflow? Now, along all of these questions, I'm happy to tell you that TensorFlow ecosystem has few set of tools which could be helpful to answer some of them. I'm not going to go through everything here, but toust give you a few examples. Starting with fairness indicators. It's a very effective way by which you can evaluate your model's performance across many different subgroups in a confidence interval-powered way such that you can evaluate simple but effective fairness metrics for your We have a what-if tool that gives you the notion of interpreting the model's output based on the features and changing those features to see the changes in the model's output if it has very compelling textual as well as visual information. And finally, TensorFlow Federated. It's a library that helps you train your models with data that's available on device. Cat and Miguel have a talk later today that dives deep into fairness and privacy pillars of responsible AI strategy. Be sure not to miss it. We are excited to work on this important part of TensorFlow ecosystem with all of you, the TensorFlow community. And now, to talk more about the community, I would like to turn it over to Kamal. Thank you. Thank you, Manasi. Hi, everyone, my name is Kamal and I'm the product director for TensorFlow. I'm going to talk about the most important part of what we're building, and that's the community. And I want to begin by thanking all of you. Your feedback, your contributions, what you build, this is what makes all of this possible. We have an incredible global community. We love hearing from you, we really appreciate to everyone that came out to roadshowf TensorFlow World last year. And going into 2020 I want to take some time to highlight more opportunities to get involved in the community and new resources to help you all succeed. Let's start with ways you can get involved locally. One great way to connect is to join a TensorFlow user group. Thes grass root communities started organically and we now have 73 of them globally. We launched the first two in Latin America after the roadshow in Sao Paolo, and now we expanded in Europe. The Korea group has 46,000 members and China has user groups in 16 cities, I'm sure this map can have a lot more dots, so if you want to start a user group, please reach out and we'll help you get started. Another way to get involved are the special interest groups, or SIGs. SIGs exist to helpou build areas of TensorFlow that you care the most about. We will have 12 SIGs, with SIGs graphics being our latest edition starting at the end of the month. Most SIGs are led by members of the open source community, such as our fantastic Google Developer Experts. We love our DGEs, we now have 148 of them. I want to give a special shout-out to Haesun, pictured above. Again, GDEs are amazing, so please let us know if you're interested in becoming one. OK, so TensorFlow user groups, SIGs, and GDEs are great ways to get involved, but we all love a little competition and we all love Kaggle. As Kaggle now supports 2.x we've had over a thousandeams unrolled in our last competition. I want to give a special shout-out to our 2.0 prize winner, deep thought. And speaking of competition, we saw great projects in our last dev push challenge. Including psychopathology assistant. And everyone dance faster and everybody dance now video generation library using TensorFlow 2.0.0. Thank you to everyone who participated and today we have a new challenge. Manasi spoke earlier about how TensorFlow can help empower all users to build AI systems responsibly, so we want to challenge you to build something great with something 2.2 and as many that has the AI principles at heart. We can't wait to see what you build. Another area that we're investing in a lot is education. Starting with supporting our younger community members. For the first time we participated in Google Code-in and it was a success. We were very impressed by the students and we want to thank all the awesome mentors who made this possible. We hope someday to see the students at our Summer of Code program. I love Summer of Code. It's an awesome opportunity for students to work with TensorFlow engineers. We saw amazing project fact, one of the students worked on data visualization for SWIFT, which is still being used tod by our team. So I'm happy to announce we're doing it again this summer and we're excited to see what new projects studentsill work on. Programs like this are key to the growth of the developer community, so please visit the Summer of Code website to learn more and apply. We also want to help produce great educational content, starting with our machine learning crash course, a great resource for beginners. So today we launched and updated version of the course. Our engineering team completely revamped to the programming exercises using Keras 2.0 and making them much simpler in the process. Go check it out at this link. And we want to had provide resources at every step of learning. At University level, we want to empower them to sport design and engineering courses. This year we have a commitment to fund schools from underrepresented communities in AI, historical black and Latinx colleges and universities. We partnered with 200,000 people have enrolled in our courses. The data and deployment course is a great specialization course that covers TensorFlow.js, TFLite, TensorFlow dataset and more advanced scenarios, this is a great option for people who are looking to improve their AI skis and theres' more. Imperial College in London started with a course in Coursera. This course was created in part by the TensorFlow funding that I mentioned earlier and we're super-happy to see this. How do you show your expertise to the world? This is why I'm excited to announce the launch of the TensorFlowertificate program. An assessment assessmentthe TensorFlow created by the TensorFlow team. Basic ML programming, text classification, computer vision. U you'll be able to display your certificate badge on LinkedIn, GitHub etc. We're excited to offer a limited number of stipends for covering the certification costs. You can find out more at TensorFlow.org/certificate. So a lot of things to do. And I want to thank you again for making the TensorFlow community so awesome. As you've seen, the TensorFlow ecosystem is having incredible impact in the world today. And what it really comes down to is how AI is helping make people's lives better. That's really what inspires us as a team to build all these amazing tools. So I'd like to end by sharing one final story. [video] ... . ...: That's just amazing, and incredibly inspiring. en I see something like this, it makes me very proud to be building TensorFlow. So go build amazing things! And we'll be there to help. And with that, I'll pass it on to Paige to kick off our day. Thank you. Hi, my name is Paige Bley and I'm here today to deliver the talk of Karmel Allison and Yash Katariya who unfortunately couldn't be here today. So imagine that I'm Karmel. My son is in kind garden and he's just starting to read. Of course, doing what I do, I can't help be amazed at how similar his learning process is to a neural net's. The rule set for spelling and grammar is so complex in English that we rarely present a set of instructions thate can follow. Instead, he starts almost randomly with letters and sounds. And then he gets feedback. Some things he memorizes, some are lucky guesses and somehow over months of learning he started to enforce a consistent memorable model of written words. So here you can see, I was eating breakfast with my cousins and my sister. I've been particularly fascinated by my son's learning here, because it happens to align with what many of us have started to call an MLP revolution. We have enough data and tools that we've started to rapidly push the cutting edge of natural learning and related tasks. I've heard that with text today we're at a Cambrian explosion like when ResNet was started toeing published, we're at this faster than expected progression of language models. So we're going to take advantage of all of this research and tooling and we're going to teach a simple neural net to generate the next word of a phrase, and we'll use this to tell a robot children's story. So the first thing we need is data, and we're going to take advantage of the children's book test corpus released by Facebook. This is a set of creative commons chirp's creative commons children's books. From this model we just need the raw text. As an aside, these books are out of copyright which means they're often old and that means the corpus is full of literature that's problematic by today's standas. If you go and actually use this dataset, please do consider what cultural norms you might be teaching your machine learning models. So first we're going to load the data. We can load it into our Python interpreter and start playing with it. Here we use the TextLineDataset to load the data and we can simply print out a few lines to see what we have. Tell me what you notice here. It looks like we have some cleaning work to do. So using our dataset much like we would use a NumPy array. Here I'm dropping those pesky book titles and filtering out punctuation, and I can check out a few lines to make sure the data looks as expected. I've decided I want to train a windowed model to predict a new word, given some words to start. So I need to take these dataset rows and make them all equal length. Once again, without leaving TensorFlow datasets, I can split, flatten, and regroup set of 11 words. But I want a that last word in each row to be a separate label. So I just define a simple row-wise function or pop out my rows and voila, we have rows and labels. But there's still a problem with this data. Why can't I just feed this into a dense network. Well, it's words, had machine learning models don't understand language. We need only numbers, not lines, just numbers. So we're going to need to transform our sentences into numeric representations. And the way we do this is with preprocessing layers, which are highly exciting and a new edition to TensorFlow 2.0. So preprocessing layers were recently reviewed as part of the Keras specification. These follow the same APIs as normal layers, and can be called and composed just like layers. They play nicely with TensorFlow datasets, as well, so you can parallelize preprocessing transformations and importantly, just like normal Keras layers, these become part of your model and therefore get serialized in the save model and become part of inference to prediction which is critical to minimizing training serving skew. This is the set of preprocessing layers that is already complete. They're experimental in 2.2 as we ensure that the APIs match how you use them. So please check them out for all of your preprocessing needs. The next step is to build a language model that can learn a representation of all of these words so that we can generate text on the fly. One of the classic models used for text translation and generation is a sequence-to-sequence model. A sequence-to-sequence model typically has two parts. An encoder and a decoder that borrows state from the encoder in order to correctly predict the target outputs. Now, sequence-to-sequence models have some complicated moving parts. It's not a simple feed-forward mechanism and there are a lot of parameters to keep track of. But luckily, we don't have to work alone here. One such utility is the seq2seq package. You'll see me use these throughout this example. Because the architecture of the seq2seq is fairly complex and requires special state-passing we're going to subclass the Keras model and build our network explicitly. We start here with the text vectorization model we've already discussed. After we've vtorized the inputs we're going to pass them through the encoder blocks, first in embedding and then in LSTM. And you'll note that we're just configuring the layers and we're not passing any data tough them yet. In Keras, each layer has two stages, the construction and the calling of the layer which will come later when we pass our data through. The decoder is somewhat more complicated but here we can leverage the TensorFlow add-on and decoder and set it up to and connect it to the projection layer. Plus two token for predicting calls and out of vocabulary words. The final set of layers we will set up are a pair of attention layers. There is a large and growing field of attention research in ML, but given the time constraints of this talk I'm only gng to give you a very hand-wavy explanation. It will allow the model to give more weight to certain time steps when predicting the final word. Here we use a simple dense attention layer. You'll see how we connect this between our encoder and decoder in a minute. So we ha a lot of state, which means we can't use the standard Keras pick callOne of the things I'm most excited about in TensorFlow 2.0 is that we have refactored and cleaned up the Keras training loop and made it much more modular. So instead of overriding the entire loop or throw it out all together, I'm going to call it by model.fit with each step. So we overright train except in our encoder/decoder model. We just need to define the forward pass for that one pass and separate the example from the label. You'll note that we call our own vectorization layer here. Next is still inside our single training step we're going to record our forward pass. Anything that needs to be back-propped through should go through here. Our encoder embedding, LSTM, and so on all belong under the tape. And here we pass our input. We also set up our attention layers to track the intermediate state coming out of the encoding layers. And next we decode, which is to say we try to predict our targets using the staterom our encoder, the decoder will go over the many epochs it runs for, and in concert, the encoder and decoder will learn to predict text based on the outputs and now that we've run all our layers, we can compute the loss and collect the outputs of this step so that they can be optimized. The Keras model takes care of collecting variables and setting the optimizers, so we just choose how we want to pass things through here. As the final step we want to return and collect the metrics we set in our model. The next step is to pick an optimizer loss in accuracy model. We select them from a bunchf independently parameterizable options that are built into tf Keras, and then we train. It might take a. We can monitor progress as we train and our goal is to reach some degree of convergence of the reported accuracy and with this model this happens somewhere around 45 epochs at about 70% accuracy and 90% accuracy is it pretty good, but we might ask ourselves, can we do better? And the parameters I chose were thoroughly arbitrary. Maybe the models should be bigger or smaller. Typically we spend some time ning these model parameters to ensure we have the best results for a given model architecture. We call this process hyperparameter tuning and notably easy hyperparameter tuning is one of the most requested features for Keras, and we have a package for that! KerasTuner, it allows you to create hypermodels that especially code tuneable parameters, state the model for others to use. So let's take it for a quick spin in order to tune som of our model's parameters. For example, let's say we wanted to tune the number of rnn units in our model. We can import the Pip install and then define a function that takes a hyperparameter object and uses it to compare a model. Instead of passing a fixed in we can use the magic selector object. There are a number of different selectors you can choose here, including floating point numbers and enums, We can use the tuner to build and fit our model intelligently across the space. Everything works within the Keras ecosystem and the tuner will be able to call .fit just like we did to get the correct training behavior. The tuner will run through the different parameters you've configured and it even works with Colab. After a few tuning sessions you see that the best is 1024 units we improve our accuracy and now we have an even better model that gets above 90% accacy. So now that we have a trained model, of course our goal was not just to create it, but to actually generate text. So how do we take the model we've built and use it to write a sentence? The first step is to use our model to predict one word given the length of the input which is exactly what we trained the model to do. We can overwrite predict step. And our predict step we run the inputs through the same encoder and decoder we saw with training, but now with fixed weights. We can also throw in many so custom logic here and we allowed the model to predict from the top in choices d of the predicted word. We can try this out and see that we've produced the next predicted word correctly. But of course we don't want single words, we want a whole sentence, so we can go further and define a custom predict that just takes a single string and generates one word at a time to continuously append to that string and generate a much longer string and lo and behold we can generate vaguely meaningful statements. It's not perfect and it's epidural unpunctuated, but it can be a lot of fun. And unlike Karmel's 6-year-old, it doesn't lie and it happens to like lling funny stories and doesn't get tired. You can see that there is a little bit of punctuation that's vaguely human understandable, even though the model is quite simple and the data is relatively small and constrained. And indeed, the model we just built is the very first baby step of text processing and it is heavily restricted by having it fit onto slides, but these same tools and techniques r are used to build at scale. So if you're interested in just learning from sending emails, check out some of the tools that Google engineers have released, you'll also hear more about TFHub in the next Check out TensorFlow add ons, subclass Keras models, tune your hyperparameters with KerasTuner and don't don't forget to check out the entire ecosystem of tools built ontop of TensorFlow. So thank you very much to the author of the presentation, Karmel and Yash, to the illustrator of the presentation, thertist responsible for all of these drawings, and to all of you who are listening online. Very excited to talk about all of these tf.keras and TF Text improvements. Thank you. Hi, I'm San d Sandeep gup too.m presenting this work on behalf of my colleague Gus in our Europe-based team. You first need a suitable model. And now every day powerful any models are published in reseah papers or in blogposts. So let's say you want to read about one of these and you wanto say how great it is and you want to try it out. Sometimes the pretrained model is right there. Sometimes it is stored on some other storage. Sometimes you may need to download and run a script to get access to the model. So as you do this, you have many questions. How do I use this model? It is it safe? How was it trained? What was the data Am I using the correct version? This is where TensorFlow Hub comes in to help you. To TFHub.dev is your place to find this information. Code snippets and much, much more. So TensorFlow Hub's rich reposito of models covers a wide range of machine learning tasks for all of your common ML needs. For example, in image-related tasks, we have models for image classification, object . tection, image detection For section, we have state of the art models, like BERT and Albert, and many more embeddings that can support a wide range of natural languaging understanding tasks. We also have video related models which can help with video action recognition, such as gestures and also video generation and now we have recently added audio models for things like pitch detection. So we've invested a lot of ener in making models in TensorFlow Hub be easily reusable for composing new models for transfer learning for your problem. With one line of code, models can be pulled into your TensorFlow 2.0 code for retraining with your own data. Now, this works whether you are using the high-level .keras API or the low-level APIs. This can also be used through TensorFlow Extended. Recently we've added support for models that are ready to deploy on all platforms where we use TensorFlow. These have been prepared for a wide range of environments which run across the TensorFlow's ecosystem. For example, you can use TensorFlow.js models for web and node-based environments. You can use mobile for mobile devices. In TensorFlow Hub, you can these devices combine TFLite models with a fast and efficient accelerators. You can learn more about this platform at Coral.ai, so today we have more than 1,000 models available. With documentation and code snippets, and some of them there are also simple demos that you can try interactively. These models can be easily fou by searching or exploring the TensorFlow Hub repository. Now, many TensorFlow models also have an interactive Colab notebook which lets you play with the models with code examples right from your browser and this is all hosted on Google's infrastructure so you can get started with nothing to install. So now that we've seen what TensorFlow Hub is, let's take a look at a couple of examples and see how it can help you solve your problems. So the first example is about style transfer. So let's take a look at how so let's say you have an image of this yellow Laador shown here and you would like to imagine it in the style of your favorite painter. So you can find a style transfer model and then you import the TensorFlow Hub module. And now, you have the model is ready for calling inference on your image and you get back the Stylized image as shown here. You can get more details of this example at the URL shown on the bottom. The second example we'll take a look at has to do with text classification. So let's look at how you can use a model with a layer from TensorFlow Hub that you c use and train your own model. So imagine you want to participate in a Kaggle competition that's related to text classification. Now, for text problems, you usually start with a text embedding which is a way of converting ratext into a more useful structured numeric representation that a neural network model can take in. Now, training your own embeddings can take a lot of time and data. The good news is that TFHub has many embeddings in multiple languages that are ready for your use. So you can pull any of these pretrained embeddings with one line of code. Here we are employing one of these as a Keras model layer as you see there. And now this model layer can be incorpored in the rest of your TensorFlow 2.0 model training code by adding additional layers and then calling the model training and the compile functions. So you can see how easy it is to build a powerful custom trained model on top of a pretrained embedding directly from TFHub. Another thing I wted to highlight is that TFHub also brings these models to life in a very interactive way. So some of our customers have created custom components that highlight the amazing work of these models that you can try outirectly in your browser without having to download or install anything. Before I close, I want to show you some of the recent improvements and additions on TFHub.dev. We have greatly improved the search and discovery feature on TensorFlow Hub to make it easier to find the model that you need. You can filter by model type or formats or deployment targets. For example if, you need it to run on mobile or a browser, you can find them quickly and easily. Also, we have done a lot of work to support the variety of TensorFlow deployment formats, so for TFLite, we now support additional metadata along with the TFLite model file. It stores useful information which makes managing these models in your mobile applications much, much easier. For TensorFlow.js we are excited to announce two new models today for face-tracking and hand-tracking which are built by our media pipe team. These models enable some really cool interactive communications and in future we will be adding more text models for web use cases. Lastly, TensorFlow Hub is powered by the TensorFlow community. When we first launched the platform. For example, very recently, we completed a TensorFlow 2.0 Kaggle question-answering competition and we're happy-to-a announce that Kaggle has published all of the winners on TensorFlow Hub. Now with over 1,000 models with an increasing number of organizations you can find models that cover a much larger range. So to everyone who has helped contribute models to TFHub, a big thank you. If you are interested in publishing your models, we are accepting submissions. We are in the early stages of building out our third-party module collection and our focus is still on adding high-quality models with strong documentation and we are very interested in helping people share reusable pieces of models. So if you would like to share your models, please visit the link here or find it on our website. So that's it abou about about it. F hubk you. Hi, everyone, my name is Gal and I Product Manager on the TensorFlow team, I'm here to talk to you about llaborative learning with TensorBoard.dev. Machine learning is collaborative. And all of these there's something you're expressing about what you've done potentially through visualizations. For example, it is common to share results in papers through various charts, about you it is also common to include this type of information when asking for trbleshooting help. In this case, someone posted an issue on GitHub asking why they're not ashe having the expected results. The problem is, that pictures don't convey all the information. For example, you might want to show what other metrics are being tracked, how many trials were run or let the reader explore the structure in more depth. Even when the code is released, it can require more setup before the code can be reviewed. So how can we make this easier? Well, we already have TensorBoard. It is commonly used by researchers and engineers to understand their experiments and results. It lets them track metrics, visualize their model, view their embeddings, and a lot more. And we're consistently adding new capabilities to it. Last year we launched the H params dashboard. Given that many people are already using TensorBoard and some are even sharing screenshots of it, we launched TensorBoard.dev to make all of this easier. With it be.dev TensorBoard.dev, you can share your results for free. Others can view with no setup or installation. We stted with a scalars dashboard, but more dashboards will be added soon. We're only at the beginning of this journey and we want to continue evolving this to help drive state of the art research. I want to share a few awesome examples with you today. The TensorFlow/models repository on GitHub provides. Many of them now provide a link to TensorBoard.dev. This allows you to quickly view the training dynamics and gives you a point of reference if you're training the model yourselfnd trying to understand if something is going wrong. In this example, Shawn is using Twitter to share some work on large-scale experimentation. TensorBoard.dev is used to tell that story more effectively. This team of researchers at Google recently published a table on optimizer grafting to help better understand optimizer performance. Along with the paper, they've also uploaded results on TensorBoard.dev to help illustrate their technique. This would be difficult to convey purely in a paper. Another product from Google research interests big transfer. By transferring, using a simple heuristic, th achieve state of the art results across multiple vision tasks. They uploaded the results at TensorBoard.dev to show how it compares with the baseline across multiple datasets. This pa discusses binarized neural networks. that as part of theool is reproducibility challenge, another group of reseahers published a report on this paper. They included links to TensorBoard.dev, well, and include other useful information to help others with debugging. So all of this sounds great. But how do you get started? You continue using TensorBoard in the same way that you use it today. In this example, the TensorBoard Keras callback is shown, but there are other APIs that can be used. If you're not familiar with TensorBoard, check out this link for some great Colab tutorials. Just last week we added optionally adding a name. This can include licks to your paper, or --to ... Finally, you can share the experiment by copying the URL or using the share button in the top right. As I mentioned, we'll be adding more TensorBoard's dashboards over time, as well as enabling more collaboration capabilities, you can learn more at TensorBoard.dev and follow a simple example to get started. Thank you for your time, and next up is Julia to tell you about Kaggle with TensorFlow 2.0 Hi, I'm Julia Elliot and I lead Kaggle's competitions team. I'm speaking today about using TensorFlow 2.0 on Kaggle. This is based on work led by f Phil Culliton. So what's Kaggle? It's a competition. Over 4 million scientists. It's an incredibly exciting place. My job at Kaggle is to run machine learning competitions, they run the gamu Right now we have 17 running. Including Deepfake detection in videos, image classification, an abstraction and reasoning challenge. By Francois Chollet. An NLP challenge and a bot-building simulation competition. Basically if you're interested in data or machine learning, we'll likely have a competition that will excite you. TensorFlow is really heavily used in our competitions. It's actually currently being used in all 17 of them. But in the past few months we've run two TensorFlow-specific competitions, a question-answering and LP competition that was mentioned earlier. Built around the launch of TensorFlow 2.0 and a new competition for image classification that introduces TPUs to our platform. We have tens of thousands of people using TensorFlow 2.0 right now to solve problems. Three of the top solutions actually use TensorFlow and TPUs. If you'd like to check out TensorFlow 2.0.1, build your models fast, and with next to no specialized code. No provisioning the right kind of VM, no setup, datasets that are ready to go and a weekly amount of TPU-GPU time allotted to users for no cost. We'll look at some code right now that will build a model on Kaggle for a competition to classify flowers on CPU, GPU, and TPU, with no changes. So we start off simple: Import TensorFlow and the Kaggle datasets library. The rest of the code here does a little bit of magic. It function out what kind of calculator is attached to your VM and automatically parallelizes your model. The rest of the code is entirely accelerator agnostic. So normally you'd need to provision this TPU for usage. But on our platform, it's not necessary. Just select the TPU in the drop-down and go Now, we'll load up our competition dataset. It's provided in a sharded TF record format for fast loading. We set some parameters for our training and validation sets, shuffling our train set, batching them for optimal performance, setting our training set to repeat so it eachloop around for each epoch. This code does some real work. Here we load up a VGG16 for some transfer learning. Compile our model, and start training and validating. We're using a standard distribution strategy that was made possible by that previous code that automated the strategy based on the accelerator used. Note the width strategy. Basically saying this is the process that parallelized. Now our model will be built and trained on the accelerator. Making some flower predictions is real easy after this. load up our test set, run model.predict and write our predictions out in a format that Kaggle can read. With a few tweaks and with a few more epochs. You can see that some are right, they have the little OK above them and some are wrong. If you're interested, you're more than welcome to drop by and beat this model. It's not hard and it's super-fun. If you're interested in seeing TensTensorFlow 2.1 Come forward: In the next few mohs we'll be launching more competitions. This was a huge group undertaking and a special thanks to Martin Gorner. Thank you so much. Next up is Truman, to tell you about Tensolow profilers. Hello, everyone, I'm a software engineer at Google working on TensorFlow performance. Today I'm very excited to introduce to you our brand new TensorFlow 2.0 performance profiler. We all like speed and we want our models to run faster. TensorFlow 2.0 profiler can help you improve your model performance like a ofessional player. In this talk, we will first talk about what's new in TF2 profiler and then we'll show you a case study. I'm a performance engineer and this is how I used to start my day. In the morning I ran a model and capture trace of it I wld gather the profile in a spreadsheet to analyze the bottlenecks and optimize the model. We offer have gigabytes of traces and the process to go through them manually is boringnd time-consuming. Then after that, we will run the model again to check for performance. If the performance is quite good, hurray, we have done our job. Go and grab coffee. Otherwise, we'll go back to step one, recapture a profile, gather the results and find out the reason, fix it and try again. Repeat this iteration by n times until the performance is good. This is a typical day of a performance engineer. Can we make it more productive? The most repeatable work here is to gather the trace information and analyze the result. we always want to work smarter. At Google, we find out a way to build and to automatically perform our traces, analyze them and provide automated performance guidance. It does intensive trace analysis from Google internal experts tunes the performance and automate it for non-professional users. Here is what I am most excited about. We are releasing this set of tools today. The same set of 2 in TF2 profile has been used extensively inside of Google and we are making it available to the public. Let me introduce you tthe toolset. Today, we're launch eight tools. This enables consistent mrics and analysis across different platforms. The first tool is called overview page. This tool provides an overview of the performance of the workload running on the device. The second tool is input pipeline analyzer. It is very powerful tool to analyze a TensorFlow input pipeline. TensorFlow writes data in a pipeline manner and an inefficient pipeline can slow down your application. This tool presents an in-depth analysis based on various performance date collected. At the high level, this tool tells you whether your tool is improved bound. If that is the case, the tool can also walk you through the process and determine which is the bottleneck. Th third tool is called TensorFlow scats. Presents statistics in charts and tables. The last tool, the first tool releed today is called trace viewer. Trace viewer displays detail events timeline for in-depth performance debugging. We also provide four tools that are TPU or GPU-specific. They are all available today on TensorBoard. Please check it out. Now, let's look a case study. Let's assume that we are running an unoptimized resNet 50 model on a B100 TPU. In this talk, we will focus on Keras callback. To check out other ways of profiling, including sampling and programmatically profiling, refer to TensorFlow docs for more details. Using Keras, TensorBoard callback, we simply need to add an additional line specifying profiling range. The argument profile badge equals to 150 to 160 here. In the case we are profile from batch 150 to 160. Here is a performance overview. Let's look at the overview page. It contains three sections: Performance summary, stepime graph and the recommendation for the next step. Let's zoom into each of them. First let's look at the performance summary. It shows the average step time and breaks it down into compilation, input, output and communition time. The next is a step-time graph. We can see the step-time is broken down into compilation time, output, input, kernel and compute, as well. In this example, there's a lot of redness in this chart, and indicates it is severely input-bound. The next is what I feel most excited about. This is the recommendation provided by overall analysis: It says your program is highly input-bound because 81.4% of the total step sampled is waiting for input. Therefore you should first focus on reducing the input. It also provides a recommendation of which you should use next. In addition, this tool also suggests a related useful resources to check out to improve the pipeline. This related article is -- let's follow this recommendation and check out the input pipeline analyzer tool. See, these days the host analysis broken down provided by the tool automatically detects the most time extended on the data preprocessing. What should we do next?  Our tool actually tells you what can be done next to reduce the data preprocessing. This is what is recommended by our tool. you may increase the number of parallel calls in the dataset map or process the data offline. If you follow the link on the dataset map, you will see how to do that. According to the guide, we change sequential map to use a parallel cause. We should also not forget to try the most convenient autune option. After this optimization, let's capture a new profile Now, you can see the redness is all gone in the step-time graph and the model is no loer input-bound. Checking the performance summary again. Now you get 5X speedup. Overview page now recommends differently. It says, your program is not input-bound, because only 0.1% of th total step time sampled is waiting for input. Therefore, you should instead focus on reducing other time. Here's another thing we can do. If you look at the other recommendations, the model is all using 32 bits. If you replace all of them by 16 bits, you can get 10X speedup. This release is just the beginning, ande have more features upcoming. We are working on Keras-specific analysis and the multi-worker GPU analysis. Stay tuned! We're also wo welme your feedbacks and ask you to contribute your ideas. TensorFlow 2.0 profiler is the tool you need for investigating TF2 performance. It works on n GPU and TPU. There are also two more related talks on performance tuning this afternoon. They're super-exciting and don't miss them. Finally, I want to thank everyone who worked on this project. We are all super-amazing teammates. Thank you. And nex up, guests will come to answer a few questions. Hi, I'm Robert, I'm on the TensorFlow team here at Google. We've had some great talks, and we've received some great questions. So let's see if we can get some answers. OK. First up, this one's for Paige. OK. Do you have any ideas on the advances of TensorFlow probability and what is the plan on developing the module? I would love to see methods in TensorFlow. That is a phenomenal question. And we're really excited about all of the latest advances coming through with TensorFlow Probability. Something you'll want to check out if you haven't already is basian methods for hackers. We're also expanding out the surface area of TensorFlow Probabilities API, adding some new features. They now have a jacks, experimental. But work's in lock step with TensorFlow. There are also more advances coming soon. So that is definitely still continuing work. And we're excited to see what you built. So if you do create a really cool example, be sure to share it with us. OK, great, the next question is for Sandeep, some newhanges made specific for TensorFlow JS as part of TensorFlow 2.0 that machine learning developers will find helpful. Thank you. It's near and dear to my heart. There are tons of announcements and improvements. Specifically, for 2.0, new WOULT any conversion R necessary. We have a bunch of new models later today. Text models, for and we have a l of performance improvements and new backends. So I don't want to scoop my colleague's talk too much, but be sure to check it out later in the afternoon and learn more about this. Stay tuned. The next question is for Paige. In the NLP discussion, I like to know if there are any advancements on how to address language regionalization? So there are a couple, but I highly encourage you to look at documentation and also, the TF Text NLP resources we've made available. And for most , for most of that, you can find it on TensorFlow.org. But in terms of NLP, we're excited about the latest features being added to TensorFlow 2.0, and yeah. Great. OK. One more. If we have time. This is for Sandeep. Can I tune a pre-trained model, which is trained by transfer learning by using Ker. As tuner. It makes the job of hyperpareter tuning super easy, which is something that all developers deal with. And you can absolutely use it for ansfer learning and tuning pretrained model. Anytime you have a the model, it should help you for the parameter tuning. Look at examples for transfer learning and then if you create an end-to-end example, we would love to see that. Tweet it a us and maybe contribute to some of the developer documentation. Absolutely. There are several examples as Sandeep mentioned on the website. I also wanted, just in case, because I forgot for the last question for language regiona regionalization, check out TF Hub, as well, to the point of your talk. A whole bunch of embedings for languages outside of English. And we're seeing more and more getting added every day. Well, thanks, that's all we have right now. But thanks for the great questions. And stay tuned, we're going to be taking a 20-minute break. And then, we'll be back with more great talks and great questions. Thanks very much. Thank you. Thanks. ... ...  There's a whole lot more to come. The sessions are going to continue to on the main stage in a bit. But in the meantime, we're going to try to aner. You can keep Tweeting us your questions right now with the hashtag or ask them right here on the livestream chat. Jason, I and some of the other team members, we're going to be active in the chat throughout the day to answer as many of those as we can. Why don't w start and try answering some of the questions right now? Sounds like a plan to me. First up, whave how do we extend Keras APIs? Model subclassing and generally improve the ease of use? That's a great question. If you're new to TensorFlow, it does everything you need to do in the training models. But once you get advanced into TensorFlow, sometimes you're trusting code written for you by someone else. You want to override that. Customize it like custom training loops and that kind of thing. That's a relatively advanced thing you want to do but relatively important, particularly in research. So we've realized that the training and the information for for this is kind of scattered all over the place. There's lots of files. I think the original question he mentioned, there are over a hundred fi I haven't counted them. But I'm sure there are quite a few. So I'm actually right now hard at work at trying to put this together and produce a training course for advanced nsorFlow and advanced Keras, such as subclassing some of the classes that are there, creating custom training loops and all of that kind ofo you really. I like to see it as driving stick instead of driving automatic. We're working on that. Hoping to get it published in the next few months. Watch out, and hopefully, you will have something you can enjoy.  Next question. This is one for you Jason. IO628089 asked, what about web developers? And particularly web developers new to AI and machine learning and they want to get started with this. Do they have the technology to help them? Indeed, we do, that's why I'm here. I have TensorFlow.js, which is our JavaScript implementation of TensorFlow, and that means you can basically run it anywhere that JavaScript runs. That might be client-side in the browser, JS on the service side, and Py and React native, as well. That's new to me, I didn't know that. Wow, cool. And then, of course, if you are really prone to using Python, you can, actually, still use that. And we have a command line converter that allows you to convert the Python safe models into a TensorFlow.js format, which can be used in all of the environments. I love how you say prone to using. Which is one of the really nice things, like with TensorFlow.js, you can train it in the browser. Yes. Not just for the data scientists that do Python, you don't have to learn something new. You can -- You can stick to what you're used to. There are caveats of going client side or server side. More models that work very well on the client side and other features like privacy and such. And if you want to lock train larger models, it can do all of that hard, heavy lifting for you. And one of the things I liked when I started playing wh it, if I'm a JavaScript developer who hasn't learned Python, but I have teammates, you know, who have been building models in Python for a long time, there's a converter. Yes. It converts the model into a JSON object. And I can load my interpreter. And another fun fact, if you're usiing Node JS, you can' use them on the frontend. If you're sticking to server side in no JS, you can run it without conversion now. And they run slightly faster because of the compiler. Very cool. I'm learning a lot today. Thanks, a lot, Jason. And a shameless self-plug, but a lot of the stuff around TensorFlow.js, I decided to put together a course to teach. And I'm one prone to Python. So I did it from that perspective, like, for people who not necessarily just JavaScript developers but people who are Pythonevelopers who are used to building ML, just to see how easy it is to do in the browser. And we've got cool projects to play with. And it's all on. Another question for you. And this comes in, are there any TensorFlow.js transfer learning examples? That's something I looked into when I joined the team a little while ago. Before I answer that, though, I want to talk about a little bit about object detection and what that is versus image detection. It's essentially the ability to recognize one or multiple objects in a given image. And also, find their locations known as bounding boxes. Image detection, lets you know if something is in an image, but not where it is.  So now, how do we do it in TensorFlow.js? I think the easiest is to use Cloud auto-ML. It compares to TensorFlow.js formats. And you can use that anywhere as we discussed before. You can check out the documentation to get started online. But essentially, all you need to do is have a folder full of images, like CAT images and a CSV file for each image showing where the c is in the image. And that's using the training data to retrain the model to work with your data.  You download that and use it as you need to. That's pretty cool. So instead of, like, you building a transfer learn model yourself, you're using an online existing model. Exactly. Yeah. Doing this in TensorFlow.js might be a little tricky unless you have access to the full model, the original model, if you will. So if you don't have that, you can leverage Cloud Auto, instead. Check it out. Definitely. Sounds good to me. Cool. Should we go to the next question? Next -up, Conrad, does TensorFlow measure running on Mac OS. I'm taking it they mean iOS. There are a number of ways to u TensorFlow Lite, there's a GPU delegate that allows some access, on iOS, it's more common to have access to the GPU. With the GPU delegate and TF Lite, you can access which gives you the ability to run inference using the GPU to have much faster inference on the device itself. Super useful, faster inference means less battery life, more responsive at your application and stuff like that. It's cool. It's a bit complex to go over all of that here in this video. But I would say check out the TensorFlow.org/lite site. And you'll see all of the details. There's a whole bunch of stuff and sample apps showing u can use the GPU delegate so you can have a little bit of fun using you GPU to do faster on mobile. Also things like the neural network. Very cool. And the next question coming in, they asked what's the best way that someone like high school students could engage with TensorFlow and could learn theasics around ML and start doing real things? And then, from that, be able to move on from the different components and submodules for their work. It's a good way for a high schooler to get started. I guess when you're starting out, you want to try something a litt more graphical to get started. To just learn how things need, amounts of training data, what biases might come into this kind of situation. I recommend checking out a website called teachable machine, made by Google and allows you to point the web cam at various objects or use a microphone, whatever you want to use. And train on that data. And in about one minute, you can have a machine lrning model that can classify speech or objects and you can pose it, which is pretty cool. So very quickly, within the browser being able to put something together, so they can just see how machine learning models can work. And try it live after the models train. You can repoint to everything you're training on and see the classic predicts right there in the browser in realtime, super low latency. And if you like it, if it's actuallies useful to you, you can download the model, it's a JSON file, essentially. You can reuse on any website you wanted to play that on, essentially. Nice, what a great way to get started. Very cool. I wish they had that back in my day. Like last year. . Next up, we have what are TensorFlow records? And why are they needed for input? That's a great question. You have to kind of double click a level above that and think about data. Data is really the life blood and training any kind of machine learning application. But data comes in all shapes and sizes. Very true. There might be a zip file over here with images, might be CSV files. If you are inclined to be a JSON person, you might be JSON files, those kind of things. And it's without having a lot of skills and being able to uerstand this, it becomes a huge learning curve for people to say, OK, how am I going to use it? How do I unzip? H do I use JSON? And I've seen a lot of people building models, might have this much code for building a model, but this much is the model architecture and this much is downloading the data. Figuring it out. Putting it into formats, like I'm Python inclined, putting it in the non-py format to do training. And JS putting it into JSON. And it's a whole amount of calories I have to burn before I can get started. Very true. So the idea be and something on the TensorFlow Data services and data sets is try to make that as easy as possible. We've taken a whole bunch of different data sets and put them into an API so that with one or two lines of code, you have everything that you need to start training. Very cool. Now instead of going from this to this, you're going from this to this. If that visualization works OK. And so, things like that. And en, the core of that is the TF Record.  So you need to have that one kind of base css from which you can do everything. And then, when you're doing that, there's all of these different optimizations fortraining. Like, if you're doing distributed training and you want to manage pipelines, I like to think about it as, say, for example, CPU and GPU and doing the training on GPU, but do preprocessing on the CPU. The CPU is grabbing the data and handing it to the GPU, and while it's working, the CPU also has to be doing something. And to get the two to work in parallel, it can be diffilt. There's pipeline in technology and in TensorFlow, and that is built to use TFRecord to be able to manage all of that. Great. It seems like one small thing, you think why on earth would I use this if I have CSV or databases. But once you start using it in that way, you'll see it has great benefits for your training. Awesome. Well, that's the end of the questions. That was a whole lot of fun. Thanks. It was, thank you for inviting me. Stay tuned, the rest of the sessions will be coming up pretty soon. And for those of you at home, we also run in the web browser, of course. That means, we'll be sending those out after the show. Stay tuned for details on that. That's one of the nice things about JavaScript. Indeed, runs everywhere. And on the livestream at home, keep the questions come, #askTensorFlow, we'll be here to answer as many as we can. Thank you and we'll s you around. ... apps ... ... ... ... Hello, my name is Alex and I work on TensorFlow. I'm here today to tell you all a little bit about how you can use TensorFlow to do deep learning research more effectively. We're going to do today is we're going to take a little tour of a few TensorFlow features that show you how controllable, flexible, and composable TensorFlow is. We'll take a quick look at those features, some old and some new, and not by far all the features that are used for research, but these features let you accelerate your research using TensorFlow in ways you may not be aware of. If you've used TensorFlow before and I'm sure you have at this point, you know that a lot of our libraries use tf.variables for model state. These parameters are updated when you train your model, and part of the whole point of training models is so that we find out what value those parameters had in the first place. And if you're making your own layers library, you can control absolutely everything about how that state is represented. But you can also crack open the blackbox and control how state is represented, even inside the libraries that we give you. So for example say we want to reparameterrize the KerasLayer. The way to do this is to use tf.variable creator scope. It is a tool we have that lets you take control of the sta creation process in TensorFlow. It's a context manager and all variables go for a function you specify. And this function can choose to do nothing, it can del git or modify how variables c get created. So it's the same tool that we use to build TensorFlow that we make available to you so that you can extend it and here if I wanted to do this reparameterrization of the Kerr layers, it's actually very simple. First I define what type. Here I'm using this FactorizedVariable type. Which is tf.module. You can have variables as members and we can track them automatically for you and all sorts of nice things and I can tell TensorFlow how do I use objects of this type as a part of tensor computations and what we do here is we use, do a matrix multiplication of the left component and the right component and then now that I know how to use this object, I can create it. And this is all I need to make my own little variable_creator_scope. If I'm not creating a matrix, just delegate to whatever TensorFlow would have done normally and if I am creating a matrix, I'm going to create this FactorizedVariable that has a left half and a right half and finally I just get to use it. I can check it and I find that it is indeed using my facorized representation. This gives you power. You can do dependency injection to change how they behave. Probably if you're going to do this at scale you might want to implement more layers so you have full control, but it's also more valuable for you to be able to extend the ones that we have. So use A big part of why we usecope. libraries to do research at all as opposed to just writing plain Python code is that deep learning is really dependent on very fast computation and one thing that makes it very easy to use is our. The way we're doing this is using tf.function with the experimental compile = true function. What this means is when you mark a function as a function you want to compile, we will compile it or we'll raise an error. So you can trust the code you write inside the block is going to run as quickly as if you had handwritten your own fused TensorFlow kernel for CPUs and then all the machinery yourself, but you get to write high-level, fast Python TensorFlow code. One example where you might find yourself writing your own little custom kernel is if you want to do research on activation functions which is something that people want to do. But this is a terrible one but they tend to look a little like this. But in general, they do apply lots of little elementwise operations to each element of your vector. And these things, if you try to run them in the normal TensorFlow interpreter, they're going to be rather slow because they're going to do a new memory allocation and a copy of things around for every single one of these operations. While if you're going to make a fused single kernel, you just do a single one for each initiation. What we see here is if I take this function and I wrap it and I benchmark running a compiled version on this tiny benchmark I can already see a 25% speedup and it's even better than this. Because we see speedups of this sort of magnitud or larger, even on fairly large models, including BERT, so because in large models, we can fuse more computation into the linear operations, linear reductions and thingsike this and this can get you compounding wins. So try using experimental_compile = true for automatic compilation. You should be able to replace what you normally have to do with fused kernels. So you know what type of researchy code a lot of people rely on that hasots of very small elementwise operations and that would greatly benefit and I thi it's optimizers. A nice thing about doing your optimizer research on top of TensorFlow is that Keras makes it very easy to be able to implement your own optimizer. You can overwrite three methods. You can define your future operation. You can create any accumulator variables, like your momentum or higher order powers of gradients or anything else you need, and you can define how to apply this optimizer update to a single variable. Once we've defined those three things, you have everything TensorFlow needs customble to run your cuss optimizer. What I'm going to show here is an example of a very simple optimizer, again, not a particularly good one. This is a weird variation that has some momentum and some higher order powers, but it doesn't train h very well. However, it has the same sort of that you'd have on a real optimizer. I can get it to run just as fast as a hand-fused kernel and the benchmarks around here, it was over 2X speedup, so this can really give you -- this can really matter when you're doing a lot of research that looks like this. Something else, so Keras optimizers and compilation let you experiment really fast with fairly intricate things. The next thing I want to talk about is vectorization. It's again super-important for performance, I'm sure you've heard at this point that Moore's law is over and we're no longer going to get a free lunch. The way we're making our machine learning models faster is doing more and more things in parallel. And this is great because we get to unlock the potential of GPUs and TPUs. But it's also scary, because we have to write these batched operations which can be fairly complicated. In TensorFlow we've been developing recently automatic vectorization for you where you can write the element-wise code that you want to write and get the performance that you want. So the working example I'm going to use here is jacobians. You know it computes a gradient of a scaler, not a gradient of a vecr value, a matrix value function. You can just call tape.gradient many, many times. And we are I want to compute a jacobian, and I do this by writing this double for loop where where every row, for every column, I compute the gradient with respect so the roving column output and then stack the result together to get my higher order jacobian tensor. This is fine. This has always worked and however, you can replace these explicit loops with tf.vectorizemap and one you get a small r readability win. But also get a very big performance win. Of course you don't want to have to write this all the time, which is why, really for jacobians, we integrated it directly in the GradientTape and you can call tape.jacobian and if you do this, it's more than 10 times faster than if y write it yourself. But really why I opened this blackbox and showed you the previous slide is so you can know how to implement something that's not a jacobian but it's like a jacobian itself. And how you can use the vectorization capabilities together to make you more productive. And soemember to use automatic vectorization so you can write short code that actually runs really fast and let us add the batch dimensions ourselves. And here is another interesting performance point, because TenrFlow, we have always had the big rectangular array or hyperarray, the tenser as the core data structure. And tensors are great. In a world where we live in today where we need to leverage as much parallelism as we can to make our operations run fast, tensors tend to be very operative by default. And as long as youan stay within this tensor box, you are happy, you get peak performance and everything is great. However, aseep learning becomes more and more successful and as we want to do research on more and more different types of data, we start to want to work with things that don't really look like these big rectangular arrays. Structure that is ragged and has a different shape. And in TensorFlow we've been recently working really hard at adding native support to ragged data. So here's an example. Pretend it's ten years ago and you have a bunch of sentences, ty all have different lengths and you want to turn them into embedding so you can feed them into a neural n wo netwo You're going to use the index to look up a row in an embedding table and finally you want to average the embeddings of each sentence to get an embedding -- the embeddings of all the words in the sentence to get an embedding in the sentence and then you can use the rest of your mel. And even though we're working with ragged data here, if you think about the underlying operations that we're doing here, most of them don't actually have to care about this raggedness. So, we can make this run very efficiently by decomposing this representation into two things. A tensor that ccatenates against a ragged dimension, and a separate tensor that tells you how to find the individual ragged elements in there And also you have this representation, it's very easy and efficient to do all the computations that we wanted to do to solve the tasks from the previous slide. You have always been able to do this manually in TensorFl. We've always had the features and capabilities for you to do this. Now, with TF RaggedTensor, we're taking over the management of this from you and give you an object that looks like a RaggedTensor, can be manipulated like a RaggedTensor and can be -- Here is my data, same one from the previous slide. It's just a Python list and I can take this Python list and turn it into a RaggedTensor and the right thing is going to happen. TensorFlow is going to automatically contact the native ragged dimension and keep this array of indices under the hood. Then I can define the vocabulary table and do the lookup. And here I'm showing you how to do the lookup or any operation on a RaggedTensor where that operation actually hasn't been rewritten to support raggedness. You can always use ragged.map_flat_values. Also many of the TensorFlow core operations have been adapted to work with RaggedTensors, so in this case if you want to do a tf.gather to find out the rows for each word, you can just apply the tf.gather on the right tensor and the right thing will happen. It's very easy to use the standard tf.reduce_mean and the nice thing is at this point we have no ragged dimension and we just have a dense tensor that has the original shape you expect it to have. And I think this is really important, because now it's much easier, much more intuitive and affordable for you to work with data that doesn't necessarily look like the big rectangular data that TensorFlow is optimized for and yet it lets you get most of the performance that you'd get with the big rectangular data. It's a win-win situation and I'm really looking forward to see what interesting applications you all ar going to work on that use and exploit this notion of raggedness. So please, play with tf.ragged, try it out. It's very exciting. So next up we're he goi to go over a particularly interesting example of research done with TensorFlow and a Ph.D. at Stanford university is going to come and tell us all about convex optimization layers in TensorFlow. Thank you.  * Hi, my name is Akshay, I'm a Ph.D. candidate at Stanford and today I'm going to talk about some recent research that makes it possible to embed convex optimization problems into TensorFlow. This is an optimization problem, and in an optimization problem, the goal is it to find a value for a variable x that minimizes a cost function while also satisfying some constraints. So here the variable might represent a decision or it might represent weights in a machine learning model or it could be the design of a physical device and one thing to notice is an optimization problem is a declarative object. So basically all you have to do is articulate the objective function of X and then you specify the constraints that your variable has to satisfy. And also notice that the optimization problem here is parameterized by a vector theta. It's going to be important later. Unfortunately, most optimization problems are computationally intractable, but convex optimization problems are the subset that we can solve easily. Convex optimization has a lot of properties that make it a useful tool. So for one, convex optimization problems can be solved exactly and very quickly, up to thousands or even millions of times a second. Convex optimization is also easy to use in practice, thanks to software libraries like CPY. So first you construct a variable, then you construct the objective function and a list of constraints. Then the objective and the constraints are used to construct a solve object. And I guess the only reason that any of this matters is that convection optimization has tons of applications, it's got many more applications than people once thought, actually and I'll give you a few examples. It's used onboard self-driving cars. It's also used to control actuators in spacecrafts and generate landing trajectories for rockets Convex optimization has even been successfully used to design airplanes and other physical structures. Until now, it was very difficult, if not impossible to use convection optimization problems in TensorFlow pipelines. As a result, the parameters data in the optimization problem were chosen and tuned by hand, so this means that the structure of the problem was often painstakingly crafted by a human. This process of cosing parameters in a problem is carried out in basically every field and in different fields it goes by different names, so in machine learning, we know it as hyperparameter tuning. In finance it's called back-testing, but if machine learning has taught us anything it's that we should pref gradient tuning of parameters over manual tuning whenever possible. Last year my lab mates and I figured out how to differentiate through convection optimization problems, this makes it possible for the first time to tune the parameters in a conviction optimization problem using gradient descent. Once we figured out the math, we wrote a software library that makes the math actionable. Our library turns a CVXPY problem into a differentiable convex optimization layer that you can use in TensorFlow. So on this slide, the layer is represented by the function X star of theta, and that's the arg min And just like any other layer in TensorFlow, a CVXPY layer has a forward pass and a backward pass. In the forward pass of the layer, the parameters are taken as input, then the forward pass soft the CVXPY problem and outputs a solution. Backward pass computes the gradient of the solution with respect to the parameters. And our software lets you learn the theta. This automates what has traditionally been a very manual process. So let's walk through a short code snippet that shows how to use aVXPY layer in TensorFlow. In the first three lines we import three packages, CVXPY to construct the problem, TensorFlow, and then we import CVXPY layers. Next, we construct a simple optimization problem, in line 4, we construct a variable x, in line 5, we declare the parameters in the optimization problem, which is are A and B. They respond to the vector of theta that we saw in the previous slides. In line 8 we construct a problem. In line 9, we construct a CVXPY layer. An we can now use this layer in TensorFlow. In this last highlighted block in lines 10 and 11, we construct the variables. And line 13 is where the magic starts to happen. We solve a convex optimization problem and notice that the CVXPY layer is a callable object. The details of the solution algorithm are completely abstraed away from you the user. A lot of stuff happens behind the scenes, but we abstract that all away. In line 15 we compute the gradient solution. And that's it. So in just 15 lines of code, we constructed a convex optimization layer, solved a convex optimization problem and then we differentiated through it. And by showing one of example of the things that you can do can CVXPY layers. In this example we're gog to learn how to control a car. We want to control the car by choosing its acceleration and it's steering level at each step. So today in the real world, you might choose the controls using something l PID or you might used a painstakingly hand tuned controller. This allows us to naturally incorporate constraints on the acceleration and the steering angle in the policy itself. In order to learn the policy we randomly sample a bunch of different paths. The objective is to minimize some cost function that measures how well we're tracking the path and the speed. So this plot shows the performance of a control policy on a randomly sampled path but before any training happened. The solid black curve is the path that we're trying to track and the dotted curve is the path that the car actually took. So clearly the untrained policy doesn't do too well. So in order to try to improve the performance, we train the policy for 100 iterations in simulation and each iteration we solve a bunch of convex optimization problems and then we differentiate using gradient descent. The trained policy does a lot better. That's the plot on the right here. The car on the left is traveling more slowly and only made it to the top of the loop. So after 100 iterations of training, the training policy tras the path much more accurately. So if any of that sounded interesting to you, I would encourage you to check out the links on this slide. Our software package, CVXPY yers is online at GitHub and you can also download it using PIP. You can check out our NeurIPS paper. And check out our control policies. The code for all of these examples is online. That's it. Thank you guys for listening. Next up is Rohan to tell you abo about it. F.data. So let's start with a high-level view of a ML training job. Typically your ML training step will have two phases to T the first is data preprocessing, where you're going to look at the input files and do all kinds of transformations on them to make them ready for the next phase which is model computation. While you're doing data preprocessing with which happens in the CPU, you might be doing things such as images you're cropping them, for videos, you may be sampling them, and whatnot, so if your trainin speed is slow, you could have a bottleneck in either one of these two places and I hope that the talk on profiling would have given you an indication on how to figure out which one of the two phases you're getting slow at. And I'm here to talk to you about the first kind of preprocessing bottleneck, which is data preprocessing. So let's try to look into what this bottleneck really is. So the -- in the last few years, we'veone a fantastic job making accelerators, which do the ML operations really fast. So the more time it takes us to do the metrics operation and all the algebra operations is a lot smaller, but the host and the CPUs that feed the data to these accelerators have not been able to keep up with them, so there's a bottleneck. We thought we could help this by making the models more complex, but more importantly, where you deploy these models tends to be something like a mobile device or something like that, which tends to restrict the amount of complexity you can introduce into your model. So that hasn't really panned out. The second approach people take is that they try to do larger back sizes. But larger back sizes require larger preprocessing to assemble the batch, so then that puts further remember on them. pressure on them. So that's where this is an increasing problem within Alphabet and even externally. Tf.data is TensorFlow's data preprocessing framework. It's fast, flexible, and easy to use and you can learn more about it at our guide. For background for the rest of the talk, I think I'm going to go through a typical data pipeline and that will help us in the later stages. So suppose you have some data in some TF record files which are your training data. So you can now start off with a TF record dataset with that data, and after that, you start doing your preprocessing. This is typically the bulk of the logic. So if it's images, you're doing cropping, maybe flipping, or all sorts of things there. After that, you shuffle the data, so that you don't train to the order in which you see the examples and the input. And that helps you with your training accuracy. And after that, we will batch it, so that the model, the accelerator can now take use of vectorized computations. Finally, you want to do some software pipelining so that you ensure that while the model is off working on one batch of data, you can t then -- the preprocessing side can produce the next batch so that everything works very efficiently. Finally, you can then feed this dataset to a Keras model so that you can now start doing your training: So given that sort of basic pipeline, and suppose you have a bottleneck, the first thing I'd recommend you to do is go through our single host performance guide a try to utilize every trick and transformation that is available in tf.data to be able to extract the maximum possible performance so that you're using all the cores and whatever. There's excellent information at the guide that we have here, and even there was a great talk at the ML summit which you can take a look at to learn more about this. So that's the first thing I'd recommend you do. But suppose you've done that and you've tried all the recommendations that w have here, but you're still bottlenecked on that data preprocessing part, and don't worry, you're not alone. This is very common. We've increasingly seen this with a lot of internal customers, and so now I'm going to -- I'm very pleased to present a couple of solutions that we've been working on, on the team to help you solve that problem. So the first idea is that why don't we just reuse the computation? So suppose you're playing around with different model architectures you're input preprocessing part kind of remains the same and if it's expensive and time-consuming, why don't we just do it once, save it and every subsequent time we just read from it and do that quickly? So we noticed a bunch of internal customers, teams within Alphabet that were trying to do this on their own outside of it F data and we decided to bring it into tf.data to make it fast and easy to use and this is what we call snapshot. The idea is what I explained to you. You materialize the output of your data preprocessing once and then you can use it many, many times. This is incredible useful for playing around with different model architectures and if you'll settle down on an architecture, doing hyperparameter tuning. So you can get the speed up using snapshot. The next I'm going to go through a pipeline that we talked about before and see how you can add snapshot to it to make it faster. pipeline that we had. And so notice that there's this preprocessing step which is extensive, so now with snapshot, you just add a snapshot transformation right after that. With a directory path. And with this, everything that is before the snapshot will now be returned to disk the first time it's run and then every subseent time we would just read from it and we would go through the rest of the steps as usual. One thing I'd like to point out is that we place the snapshot at a particular location before the shuffle, because if it's after the shuffle, everything gets frozen. So all theandomization that you get out of shuffle you lose, because every subsequent time you'll justing readinghe same exact order again and again. So that's why we introduce it at that stage in the pipeline. So snapshot, we've developed it internally. There are internal users and teams that are using it, and deriving benefit out of it, and now we're bringing it to the open source world. We published and RFC which has more information about it and some of the technical details, and this will be available in TensorFlow 2.3 but I believe it will be available shortly. So the second idea is that now not all computation is reusable, so because supse you had some randomized crops in there, and if you wrote that to disk and read them back, you'd again lose that randomization, so Snapshot is not valuable in that scenario. So the second idea is to distribute the computation. So the initial setup is you have one CPU which is driving a number of those accelerators, but now you can offload to maybe a cluster and you can utilize e ability and the computational power that you have for all these different workers to be able to feed the host so that you're not bottlenecked on the input, preprocessing anymore, and things move fast. This is tf.data service. It's a feature that allows you t scale your workload horizontally, so if you're seeing a slowness in your input preprocessing, you can start adding workers and it will just scale up. It's got a master worker architecture where the master drives the work for the different workers, and it gives you fault tolerance. So if one of the workers fails, you're still good and you can still make progress. So let's see how you can use the tf.data service for your -- for the example that we have. So here instead of having sort of an expensive preprocessing, let's say you have some randomized preprocessing, so now this is not snapshottable, because if you snapshot, you lose the randomization. So we provide you a binary which allows you to run the data service on the cluster, setup that you like, whether it's Kubernetes, Cloud, something like that and once you have that running, you can add a distribute transformation to your tf.data pipeline and provide the master address. Anything before the distribute transformation would now get run on the cluster that you have set up, and everything after will run on the host. And so this allows you to sort of scale up. Again, note that because we are not doing any kind of freezing of the data, we can now put the transformation as late as possible in there. So notice that I've put it off the shuffle transformation. The service likes snapshot has been developed with internal users. They have been using it and it's been a game-changer and now again we're bringing it to you, and so we published and RFC, which was well received and this will be available in 2.3 for you to play around with. so to summarize, what did I talk about today? So as with various strengths with hardware and software, we've ended up with a scenario where a lot of ML jobs are getting bottlenecked on input preprocessing and I've told about two solutions that the tf.data team has been working on to help you solve this bottleneck. First is Snapshot, which allows you to reuse your preprocessing so you don't have to do it multiple times and the second is the tf.data service which allows you to distribute this computation to a cluster so that you get the scaleuphat you need. I hope you play around with these and give us feedback. And thank you for your time. Next up is Zongwei who's going to talk about t Good morning and good afternoon. Researchers and TensorFlow developers, my name is Zongwei, Zhou, let me start the talk by sharing a piece of experience that you might already have encountered before. Suppose that you have an awesome prototype machine learning model that you worked so hard to make it run efficiently on single hos with multiple GPUs, now it's time to really get it running end to end with some more resources. Then you started four of your beefy machines, each with multiple modern GPUs, with fancy gigabytes network, with all these resources you deploy your models and hope to see it run blazingly fast. But wait? Why am I only getting 1.5 faster than single machines? Yes, I know this is frustrating. if you have experienced similar frustrations, this talk is for you. You're going learn how to scale out your TensorFlow 2.0 Keras model to multi-machine multiple GPUs, and you're going to dramatically improve your training through-put. I use one case study BERT squad for today's talk. Of course it has to be this revolution NLP model. Your training model is going to give -- be given some reading materials and questions and your model needs to answer the questions and beerified for accuracy. The model we use today are in official TensorFlow garden, where you can download this link and try it out following. And here is a quick demonstration. Starting from the first line, this is the training through-put using TensorFlow 2.1 out of the box. The model is now running 2X faster. Aren't you excited and want to see these optimizations? Let me provide some background information and introduce thousand the model is trained on multiple hosts, multiple GPU. And here we are leveraging the multi-worker MirroredStrategy. In the figure here, we have two GIP R. GPU devices on two different hopes and we are using a simple deep learning model with one variable each. So each GPU received a subset of the training data and compute the forward pass using the local copy of model variables. And then it runs through the backward pass to calculate the gradients of each layer. After the gradients are calculated all the devices now start communicating between themselves. After gradient aggregation, each device will get back the same set of the aggregated gradients and use that to update their local variables. We call it synchronous because every layer has to receive their variables before they can provide to the next pass of the forward step. So with so many faces in the deep learning training, how do we actually identify where the bottleneck is? Is it in forward path, backward path, variable aggregation or variable updates? Yes of course we're going to use TensorFlow profiler which Qiumin just introduced to you today. Here is the trace you will get from the trace viewer. In this view if you see there's a pink bar which means this is the No. 5 training step in your profile and this is the whole operations with this training step. In the first five rows are events that is related to GPU. Specifically in the first row you will see GPU computation which includes forward pass, backward pass, and variablepdates. And you can also see a big blue bar NCCL reduced kernel. So by far, you may already spot the problem, right? This is exactly the power of the TensorBoard profiler which makes it so visualized and obvious. So the problem here we are facing is that the gradient aggregation time actually dominated the entire step time. All you see are these big blue bars of NCCL. Here comes the optimization. From the profiler you can get the information of the total time used in reduced NCCL or reduced. And you know the model. Then from this information you can calculate your NCCL or reduce through-put and you also know you use how many machines and each machine how many GPUs are the. You use these two informations and following the NCCL and media tuning guideline, you can calculate the ideal NCCL through-puts and in this case we found that the real NCCL or reduced through-put is much smaller than any ideal through-put and we know tt the first optimization is to actually tune your NCCL to fully utilize the cross-host network to improve the performance. Second optimization. So you notice the blue bar here stays at 32 which means the gradient aggregation is in the 32 format and we know the model can be trained with mixed precision so that the gradient can actually be in lower precision. So can we actually aggregate a gradient in float 16? The third one, if you notice the NCCL all reduce is changed data across the network and it doesn't even use the GPU you serve a lot. So you see an empty space outlayer above the blue bar, so can we actually push the NCCL reduce forward a little bit so that it overlaps with the GPU computation happening in backward path? This way we can reduce the GPU idle time and improve the training time. So here are the three ideas. They look good and I'm going to show you how to implement this using TensorFlow 2.2. So TensorFlow 2.2 is shipped with the latest Nvidia NCCL libraries. And we've done a lot of research to identify a set of recommended parameters to help you reach the peak through-put. Users can append those parameters when running their models such as the NCCL parameters here appended before the model_main.py, You might need to run the experiment to find the optimum parameters. And this is suboptimal and we are working with NVIDIA to see if we can autotune the parameters. So in TensorFlow future release we may not need to set any of these parameters. For thousand, we see a dramatic improvement in through putt through Bert squad squad. If you rememr, we are looking to. The prerequisite is that the model is already trained with Keras mixed precision API, and we have an online tutorial following this link to tell you about the techcal details and uses of this API. Generally speaking, mixed precision would use two types of float time representation. Float 32 and float 16. Float 32 can represent larger range of numbers with high precision than float 16. But float 16 has its own advantages. Computing float 16 in modern GPU can be up to 4X faster than float 32. But mixed precision API tries to give you the best of both worlds. With this code, user can get the maximum credibility to change every aspect of the training, and here is the training loop code in TensorFlow 2.0.1. Withics iin The gradients are converted back to FP32 and apply the gradient updates to the FP32 model variables. The applied gradients would aggregate the FP32 gradient for you. With TensorFlow 2.2, now you can change the custom training loop code a bit to enable gradient aggregation in float 16. First we modify the optimizer API that now takes an argument call or reduce some gradients. When you set this argument to false, it essentially tells the optimizer API not to do the gradient optimization for you. Instead, the user can easily call gradient aggregation to do by yourself. And the best thing is that the user can apply any c customized gradient operations. Including what we want to do for gradient aggregation in float 16, so you simply just need to cast a gradient to F p16 before the allreduce, do the gradient allreduce and pass it back to F p16. As you can see, you may find a lot of gradient casts here. Which is OK, because the TensorFlow will actually remove the redundant casts under the hood. So you be only live with one gradient cast from F p16 to FP32. Which anyway, you have to do. Also, I want to make one more point. For advanced users, they can use the custom training loop to get maximum flexibility, but for average users, they just want to use it out of the box. So we are working on supporting this in future releases so you can get or reduce FP16 out of the box. So with FP16 allreduce, we can see that the BERT squad training through-put is increased by almost 32%. But wait, we still got one more optimization. If you still remember, we want to reduce the GPU idle time by overlapping grade went aggregation by the backward propagation. So in this model en TensorFlow calculated the gradient of the layer B, we can immediately send out the gradient of layer B for aggregation and at the same time on the GPU, you can still calculate the gradient of the layer A, so as you can see right now, the network computation of allreduce, the gradients of layer B is now run in parallel with the gradient calculation in layer A, and which we can use the GPU resources network both efficiently. To enable this overlap, let's make some more changes to the same custom training loop code in previous optimizations. So in TensorFlow 2.2. We introduce these collective hints by inputting batch per pack arguments to the allreduce API, it tells the TensorFlow to break your model gradients into multiple paths and the TensorFlow runtime will actually send out a pack once this is upgradable in the backward computation, so it's as easy as it looks. But the next question would be what is the optimum bytes her pack? In TensorFlow 2.2 users need to do something to identify the optimum parameter. User can get these benchmarks running on their multi-hosts wit their networking environment and they need to change the data or reduce data size and typically what users will see along with increase of the allreduce data says, so does the NCCL through-put will decrease. But the NCCL through-put will start to plateau, which means it's reached the limits of your underlying network and here the data rap data range of thea size is. If you set the pack size to be smaller, it cannot fully utilize the network so you are wasting the network bandwidth. If you set the gradient pack larger, then it means that TensorFlow needs to wait for a longer time. It means that you have less overlaps. So we know that the optimum pack size, actually the allreduce pack size has reached the through-put plateau. But this still requires some work from the user, so in future release we're also working on autotune features. So by far, we have seen these three optimizations and it improves the training by 2.5X. And these optimizations are very useful in public cloud. We're also working with more improvement. We're going to support Keras compile fit, and autotune in NCCL and so stay tuned for more improvements. Next is Tim to actually tell you more about Colab. Hi, my name is Tim, and I'm the Product Manager for Colab. For those of you who don't already know, Colab allows you to write and execute arbitrary Python code via the browser. It's integrated with your Google account, I with means if you're signed into Chrome or Gmail, y're already signed into Colab and it's integrated with Google Drive. Colab started in 2012 as an internal tool. In 2014 we started using it for our machine learning crash course, this is where actually a lot of Googlers first learned TensorFlow. In 201 we launched it publicly. We wanted the whole world to have access to the same great tool that Googlers were using to get their work done. Colab is easy to use. When you connect to a virtual machine, your Python environment is already set up for you. The latest version of CUDA is already installed for you. It just works. So how do we do this? Well, on the back end we maintain a will p pool of prewarmed VMs. Libraries already installed. There are resource limits in Colab and those are there to ensure sustainability and reduce abuse. Without any further ado, here are my top ten Colab tricks for TensorFlow users. No. 10: Always specify your TensorFlow version in Colab. In Colab when you import TensorFlow, by default today it imports a version of TensorFlow 1. But soon it will by default import a version o TensorFlow 2.0. If you have old Colab notebooks that use TensorFlow 1 and you want those to keep working well after we switch the default to 2.0, make sure to specify TensorFlow 1. That will future-proof those notebooks.he bottom line, always specify your TensorFlow version in Colab. No. 9: Use TensorBoard. Colab output cells can render ash arbitrary HTML. It works great and I highly recommend it. No. 8: TFLite, no problems. Even though TFLite is for on-device learning, you can still train your models in Colab. No. 7. Use TPUs, it's free in Colab and very easy to do. You just go to change runtime type and select TPUs from the drop-down. If you want to feel the raw power of TPUs under your fingertips, give them a run for free in Colab. No. 6: Use local run times if you want. If youave your own work station and your own GPUs perhaps, and you want to run your workload on your own hardware that's fine, you can still actually use the Colab UI with the Colab runtime and that's easy to do, as well. No. 5 ... the Colab scratchpad notebook. If you have files cald untitled 15 and untitled 15, this might be for you. This is not automatically saved to your Drive account and is really great for doing your scratch work. 4: Copy your data into your Colab VM. For example, first copy all your data into the Colab VM. This can result in speedups, even if you're only going to use the data once. 3: Mind your memory. If you've ever run out of memory in Colab, you may have noticed that perhaps later you were assigned a special high-RAM runtime. Well, that may seem like a great thing, but it also means you're more likely to run into the resource limits of Colab later. So the bt thing is not to run out of memory at all and mind your memory when you're doing your work. No. 2: Close your tabs when you're done with Colab. This will help you disconnect from the underlying virtual machine earlier. Again, conserving resources means it makes it less likely that you'll run out of resources in Colab. And only use GPUs when they're actually needed for your work. When you're doing work that doesn't require a GPU, just use a default that uses only CPU. Now, we do hear from users all the time things like, we want faster GPUs, we want more reliable access to P100s, we want longer run times, we want more lenient idle time-out periods and longer max VM lifetimes or higher memory with more reliable access to higher run times. Well, we do hear you and that's why we recently ab proas. It Colab Pro. We launched it in February and it's been a big success. It's only available in the US for now but we're working as fast as we can to bring it to other countries. What's next? The free version of Colab is not going away. It will continue to could he Colab Pro. Faux to your development? That's where I want to hear from you. Let us know what you want us to build next in Colab, because it's user feedback from people like you that drives our roadmap going forward. So please let us know what you want next in Colab. That's it from Colab. Next up will be Chris Mattmann from NASA. Hey, everybody, this is Chris Mattmann and I'm not able to attend physically so I'm giving my talk remotely. My talk is. I'm the deputy CTO at the NASA Jet Propulsion Laboratory. What's JPL? It's a federally funded research and development center. It's NASA's only FF RDC. They call themethese the national labs. It's nestled there in the beautiful mountains of flint ridge, we have about 6,000 employees, about a 2.6 billion-dollar business base. We have a pretty large facility, about 167 acres. At JPL, I am the lead for the innovation experience center, I'm the deputy chief technology innovation officer. Our recipe for is to find the most difficult space, the space that looks the ugliest, and make it our own. Tak it, gut t have the actual engineers and data scientists put it back together in the way that they want. Sit-stand desks. Basically follow the sun, sun shades. IoT that frost and unfrost the glass for privacy and so forth. So that's our team. We're really excited to be do do doing machine learning with TensorFlow. In the Rover in that clean room, we measure particulates in the air to determine if that we're addingny sort of bio contamination, because we don't want to do that if we send it to another planet, if we discover life, we want to do that. So we have internet of things sensors that are measuring particulates, increasing our ability to do that, increasing the density of the measurements that we have, and we're doing redictions using TensorFlow and machine learning to determine the next measurements, the next contaminations if we had them and intervening if necessary. In the bottom right you see our detection counter. To basically count people's heads as they go in and out of our tents at events like our IT Expo and so forth so we can tell people when the right time to actually attend these events so that they're not overcrowded. We're also looking beyond that. Today our Mars Rovers e currently running on the raid hardened 750 type of processer. That's about the amount of power we had on the iPhone 1. Tomorrow we will have the ability to use things like snapdragon from Qualcomm, so a deep-learning chip so we could do actual recording onboard and we could do really cool thingsike make the Rovers intelligent, do things like drive-by science which you see there highlighted in the right as one of our three ongoing tasks and initiatives. Can we make Rover smarter? Absolutely we can take models like terrain modifierers. We call it SPOC, there's a theme here, Star Trek. Ripples, smooth, smooth with rocks, to figure out where the Rover should drive and where it shouldn't. We test this in aRoya Seco, another is the Google show and tell model for that, which is a combination of to basically do labeling, figure out the labels for a particular image for the Rover and then learn a sentence description for it so that scientists can review them and so that the Rover, they it's on Mars, itan send back millions of images. In terms of our terrain classifier, just some examples of that. SPOC looks at the geometric features and so forth and it's actually pable of recognizing terrain types. This is important for our service operations but also to potentially plan where we should do featured Mars missions and landings. Beyond that, we're putting TensorFlow models and taking them and porting them to TFLite and moving them to exotic hardware some of which isn't physically here and we only have emulatorfor. We've been trying to look at various TensorFlow models like deep lap mobile net, v2, and then figure out how do we port them into a TFLite-like model. One of our key computations here is that tests were conducted on smaller imagery and actually mobile n v2 performed the fastest. So we've got ongoing resech and working on porting these models into these TFLite environments. If our rovers are smarter and you look on the right, we don't want to miss that gre monster program. Where the Rover doesn't have enough power to be able to in light time and bandwidth itisses something that we actually really wanted to see, like our little buddy there in green. And one of the challenges with that is that the Rover has an 8-minute light time from round trip from Earth to Mars. It's got to do a lot of science and things on board. It's got to recognize things even without our direction. So having the ability to have the Rover be smart, do drive-by science on board and just send back again those textual captions and descriptions of images is really key, because then it can get beyond being able to only send 200 images a day and could actually send millions ofaptions. In particular, I've been collecting all the information and I've been capturing it, and I've been writing a second version of the machine learning with TensorFlow book. It's currently in the Manning early access program, or MEAP. Please check out the link right there and I would like you to ask me any questions. There's an online developer forum for it. Please let me know. I'm not there physically. But you can find me online on Twitter @Mattmann and thank you for giving me the opportunity to present today. First what is the relationship between TF module, TF Keras layers.layer? And that sounds like an Alex question. Hi. Bottom up, TF is just a building block for all sorts of weird things in TensorFlow. For example, you saw, if you saw my talkyou saw me using a TF Module to replace a variable. Pretty much anything that can have TF.functions as methods or variables or members or submembers, it makes sense to represent as a .module. We can track at runtime to find the variables that your module potentially references. We can save the functions and all sorts of other things.  It has no structure on top of this. There's no call method, there's no graph tracking, nothing about shapes. Everything about it is yours to decide. TF Keras.layer builds upon the TF.module, but it adds Keras specific policy. If you want your ler to interact with the rest. They have a particular method call that is very, very special that Keras will do all sorts O of things around, graph building mode, with Keras' training loops, Keras layers can track losses, metrics. They have, you can set them as frozen. They have a lot more functionality built in. That's expected of everything else that's going to interop. They have initialization, et cetera. But there are supposed to be building blocks. They're not supposed to be the entire end-to-end thing. And Keras model inherits from Keras layer adds the necesry stuff to make it an end-to-end thing. Layers with a compile method, fit method, and train batch method, you can have optimizers and it's a big, high-level unit. You can subclass model if you want to implement your own model with custom code. Are you can also build one using the constructer or one of the subclasses. Excellent. That's awesome. Thank you so much for that answer. Our second question. The profiler looks great but req requires CA 10.2. Thank you for the question. So our profiler technically CUDA 10.1, but only for single GPU. So if your model runs on a single GPU, that's fine, you can use CUDA 10.1. If your model runs on multiple GPU, CUDA 10.2 provides some good improvement and will recommend you to go to CUDA 10.2. Excellent. And the next question is about multi-worker strategy. So this, does this approach with multiworker work only with locally connected workers or with globally distributed workers over public net? Sure, I can answer that. Yeah, it works. But my concern would be that it might be a little slow because we rely on the network. And so, you will not have the bottleneck on the computation itself. But you might have, like, having a network-level bottleneck. If it's globally distributed, maybe the interconnect speed is not that fast. But sure, we definitely support it. And it'll work. Excellent. Thanks for the question, Phillip. And our next one. Does automatic vectorization handle nonrectangular loops? Is there any way to get away from padding and packing sequences? This is from James McKeon. So, we currently do not handle loops over data. The inputs of your TF vectorized map is expected to be regular. It's interesting and something we're working on. How to handle data. We have two alternative options. You can pad up to the maximum length and then use regular operations and somehow ignore the information that's in the padded area. Or you can pack acro the regular dimension and then you have to rewrite your operations to apply to the pack data. There are different sets of times wn padding is better than packing. And I think we still need to resolve what we're going to do about this before we move on to implementing it. Excellent. A lot of great work being done with intensers, be sure to submit questions. Or RCCs. Yes, awesome, as well. And our final question from Paul Lopez, are there any GitHub code examples for fairness with con TensorFlow. Or TFCIO. Yes, there are examples. I believe we have two on GitHub today. So if you go and look at the TFCO repo under Google Research, you should be able to find those. We also just recently released a , I believe it was a couple of weeks ago specifically around TFCO and the interesting applications with the tool.  So if you have any interest in learning more, plee go and check out those resources. And if you create your own example, we are always excited to see it, make sure to Tweet it out. And we will take a look and potentially consider adding it to the official model's repo. Or the official examples repo, rather.  So I believe, that's it for today. Thank you so much for listening to our second Q&A session. We have a ton of great talks planned for the rest of the day. And hopefully, you've enjoyed what you've seen so far. Make sure to send additional questions with t the #askTensorFlow and be sure to Tweet your favorite impressions from the day with TFdevsummit at the hashtag. Once again, I'm Paige Bailey, these are awesome speakers. And keep building with TensorFlow.   Neural networks have emerged as an effective and promising approach to many mission-learning tasks. Including computer vision, language understanding or classification in general. In this video series, we are going to introduce a new learning framework to you called neural structure learning, which enables neural nets to learn with structure signals for improving model quality and robustness. I'm going to be your guide. You do not need to know a lot to get started. And we'll be coding with Python language. Don't worry, it's simple to understand and you'll be up and running in no time. Consider you are creating a netwo to classify an image into a cat or dog. The ima is fed into the neural net activating neurons layer by layer. To be classified as a cat or a dog. Seems pretty straightforward, isn't it? What if I tell you there are other similar images related to this input image. That is, there is actually a structure, for example, a graph. And as you can see, all the images are English bulldogs. O so is it possible that we can make a neural net learn better with the whole structure in addition to just using one image? And the answer is yes. Neural structure learning jointly optimizes simple features and the structure signals existed among samples. In order to learn a better neural net. The first input is the features of a training sample. For example, the pixels of an image. And the second input is the structure. For example, the graph representing the silarity among samples. Both the features and the structure will be fed into a neural net for training. You may now have a question. We know in a neural net, the input features are used to activate a nuance layer by layer for making a classification. How do we use the structure to help a neural net learn? The structure is used to regularize the training. Don't worry if you're not familiar with this concept. We're going to provide more details for you about this whole training process. Are you ready? First, each training sample is augmented to include its neighbor information fm a given structure. Specifically, the neighbor information here refers to the features of a neighbor. So we get a new training batch where both the original training samples and their neighbors are included.  Next, both the training sample and its neighbors are fed into the neural net. After the training sample is fed into the neural net, its features activate different neurons layer by layer forming an embedding representation for the sample. If you're not familiar with the concept of embedding layers, just think of it as a new representation formed by the second to last layer of a neural net. The neighbor is processed in the same way. So we will have an embedding representation for the neighbor, as well. Then, the fference between the sample's embedding and the neighbors embedding is calculated and added as a regularization term.  What is the intuition here? By adding this regularization term, the neural network learns to keep the similarity between a sample and its neighbor. In other words, the neural net learns to maintain the local structure of a sample and its neighborhood. By leveraging the signals, neural nets can learn withess label data and also be more robust. We also provide several hands-on tutorial to guide step by step how to use the framework. In the next video of these series, we will take what you have learned and apply that to a language understanding problem classifying the topic of a document. You will find a tutorial for that in the description below. As well as more information on getting started with neural structure learning.  Hi, welcome to episode two of neural structured learning. In the previous episode, you learned about what neural structure learning is and how this new learning paradigm can be used to improve model accuracy and robustness. In this episode, we'll discuss how neural structured learning can be used to train neural networks with naturalraphs.  It's a set of data points that have an inherent relationship with each other. The nature of this relationship can vary based on the context. Social networks and the worldwide web are classic examples that we interact with on a daily basis. Beyond these examples, they also occur in data that is commonly used for many machine learning tasks.  For instance, if you're trying to capture user behavior based on the interactions with data, it might make sense to model the data as a core occurrence graph.  Alternatively, if you're working with articles or documents that maintain citations to other documents or articles, then we can model the data set as a citation graph. Finally, for national language applications, we can define a text graph where nodes represent entities and relationships between pairs of entities.  Now that we understand what they are, now let's look at how we can train them for a neural network. This is a problem that frequently occurs in a multitude of context.  As an example, machine learning practitioners might be interested in capitalizing machine learng papers based on a specific topic, such as computer vision or natural language processing or even reinforcement learning. And often, we have a lot of the documents are papers to classify, but very few have labels. How can we use neural structured learning to accurately classify them?  The key idea is to use citation information whose existence is what makes the data set a natural graph. What this means is that if one paper cites another paper, then both papers likely share the same label. Using such relational information from the citation graph, leverage is both labeled as well as unlabeled examples.  This can help compensate for the insufficiency of labels in the training data. You might be wondering, well, all this sounds great, but what does it take to actually build a neural structured learning model for this task? Let's look at a concrete example. Since we're dealing with natural graphs here, we expect a graph to already exist in the input.  The first step is to augment the training data to include graph neighbors. This involves combining the input citation graph and the features of the documents to produce an augmented training data set.  The pack neighbor's API neural structured data learning handles this. And it allows you to specify the number of neighbors use for augmentation. In this example, we use up to three neighbors. The next step is to define a base model. In this example, we have used Keras for illustration, but neural structured learning also supports the estimators. The base model can be any kind of Keras model, whether it's a sequential model, functional based model. It can also have an arbitrary architecture.  Once we have a base model, we define a graph regularization configuration object, which allows you to specify various hyperparameters. In this example, we use three neighbors for graph regularization.  Once this configuration object is created, you can draft the base model. This will create a new graph Keras model for straining loss includes graph regularization term. And you can then compile, train and evaluate the graph Keras model just as you would with any other Keras model. As you can see, creating a graph Keras model is really simple. It requires just a few extra lines of code.  A Colab-based tutorial that demonstrates classification model also exists on the website. You can find it in the description below. Feel free to check it out and experiment with it.  In summary, we looked at how we can use natural graphs for document classification using neural structured learning. The same technique can also be applied to other machine learning tasks. In the next video, we'll see how we can apply the technique when the input data does not form a natural graph. That's it for this video. There's more information in the description below. And before we get to the next video, don't forget to hit the subscribe button. Thank you.  Hi, welcome to Episode 3 of Neural-structured learning n. The previous episode, we learned about natural graphs and how they can be used in neural-structured learning. Why natural graphs are common, many machine learning tasks where the input data does not form a natural graph.  For instance, if you recall the document classification task from the previous episode, we used citations to form a natural graph. In the absence of citations, we wouldn't have had graph. Similarly, if you're doing simple image classification or text classification where the input dataontains just raw images or text, then we may not have a natural graph in either case.  In this episode, we'll discuss how we can apply neural structured learning. What do we do if we don't have a natural graph to begin with? The title of the video might have given this away. But the main idea is to build or synthesize a graph from the input data.  Building a graph can bee in many ways. But in this video, we'll use the notion of similarity between instances to build a graph. In order to define a similarity metric, we need to raw instances, whether they are documents, texts or images to corresponding embedings or dense representaons. We can do this using pre-trained embedding models such as those on TensorFlow hub.  Once we can draw instances to the embedings, we can use the similarity to compare how similar pairs of embedings are. If the similarities code is greater than a threshold, then we add a corresponding edge in the resulting graph.  Repeating this process to cover the entire data set builds a graph. And once we have a graph, using neural structured learning is straightforward. As we saw in the previous episode. Let's illustrate this workflow for the task of sentiment classification, using the IMDB data set. This data set contains movie reviews and the task is to classify them as good or bad.  Let's see what the code looks like to build a neural structured learning model for this task. Here, again, we use Keras for ill illustration. But also supports estimators. The first step is to load the IMDB data set. We use a version of it that is part of Keras.  Once that is done, we want to convert the raw text in the movie reviews to embedings. We use swivel embedings in this example. But any other embedding model may also be used instead. Once we have created the embedings, we can build a graph using the embedings. Provides an API called build graph to do so. Notice that it accepts the similarity threshold as one of the arguments. This allows you to control the threshold below ich drops from a resulting graph. In this example, we use a threshold of .8.  Once we have the grh, we define the features of interest for our model and combine these features with the graph using the pack neighbor's API in neural structured learning. In this example, we use a maximum of three neighbors to augment our training data. Now that we have the augmented training data, the next step is to create a graph regularized model. This part is similar to what we did in the previous episode.  First, we define a base model, which can be any type of Keras model, whether a sequential model, functional model or a subclass model. It can also have an arbitrary architecture. Then, we define a graph regularization configuration object, which allows you to specify various hyperparameters.  In this example, we use three neighbors for graph regularization. Once this configuration object is created, you can drop the base model with the wrapper class. This will create a new graph Keras model whose straining loss.  What's left is just compiling, training and evaluating the gra regularized model. This example is also available as a Colab-based tutorial on our website. You can find that linked in the description below. In summary, we looked at how to build a graph regularized model when the input data does not form a natural graph. This technique can be applied to all types of input data such as text, images and videos.  Now, graph building is not the only approach to handle input data that does not form a natural graph. In fact, in the next video, you will learn about another aspect of neural structured learning called adversarial learning, which can be very useful to improve a model's robustness to adversarial attacks. That's it for this video, there's more information in the description bew.  And before you get to the next video, don't forget to hit the subscribe button. Thank you.  Welcome to the fourth episode of Neural Structured learning series. In this video, we're going to talk about learning with implicit structure signals constructed from adversarial learning. I'm going to be your guide. You do not need to know a lot about adversarial learning to get started. And we'll learn the concept along the way. Let's first quickly refresh the concept of the neural structured learning. This is the example we mentioned in the first episode. Building a neural net to classify an image into a cat or a dog. In reality, there are usually other similar images related to that input image, formin a structure that represents the similarity among all of these images.  And a neural structured framework optimizes the sample features and the structure signals existing among the samples to learn a better neural net.  You may want to ask, what if there's no explicit structure we can use to train a neural net? One approach is to construct the structure dynamically by creating adversarial neighbors. You may have another question, what is an adversarial neighbor? It is a modified version of the original sample. Which targets the neural network to make the output classification. And the next straight line question is how to generate such neighbors. We craft a small amount of carefully designed usually based on the reverse gradient direction and apply that probation to the original sample.  Let's look at an example. Say, the sample is a pen dot image. The constructed adversarial neighbor also looks like a pen dot image.  Usually, human eyes cannot tell the difference between the two. However, the neural net is confused by the adversarial neighbor and classifies it incorrectly. This is because the small confuses the model. Even that we human cannot detect it.  After the adversarial neighbor is generated, we add an edge to connect the sample with its adversarial neighbor to dynamically construct the structure. Then, the structure can be used in the neural structure learning framework.  Let's ask ourselves why do we want to have a structure connecting the sample with the adversarial nehbor? The neural net learns to maintain a structure by keeping the similarity between a sample and its neighbor. Essentially, this is telling the neural net the sample and the adversarial neighbor are actuall pretty similar. So please keep their similarity and don't be confused by it. In the neural structure learning framework, there are TensorFlow library and functions that you can use to generate adversarial neigors. We also provide Keras APIs that you can use to enable easy to use end-to-end training with adversarial learning.  If you're interested in the details of this library and APIs, please visit our website. Let's use a task in computer vision to see how the adversarial learning works. Say, we want to train a neural net to recognize these written digits. In the next slide, we're going to write Python code to design this neural net and train it with the adversarial learning. Are you ready?  In this code example, we are going to train a neural net to recognize the handwritten digits using adversarial learning. First, we load the NS data set that contains the handwritten digits and the corresponding labels. The features of each image are pixels with the values ranging from zero to 255. So here, we normalize these features so that they will stay in the range from zero to one.  Next, we build a neural net by using Keras APIs. You can use Keras sequential APIs, functional APIs, or model via subclassing. The framework supports allhree types of Keras APIs. So feel free to use your favorite one.  Here, we invoke the Keras APIs from the neural structure framework to enable adversarial learning. There are several hyperparameters we need to configure. For example, we need to specify the multiplier, applied on the adversarial regularization. For each hyperparameter, we also provide a default values that empirically we know work very well. Then we invoke adversarial regularization to wrap around the neural net we just constructed.  After that, the rest of the workflow follows the standard Keras workflow. Compile, fit, and eval. That's it. With the APIs from the neural structure learning framework, we are able to enable adversarial learning within three lines of codes.  Let's take a look at the comparisons between the neural net's trend with and without adversarial learning. The true label of this image is a six. Both the baseline model and the model this adversarial learning correctly recognized this as a six. The next image is a nine. Again, both models correctly recognize this image as a nine.  Let's see the third image. This image is actually an adversarial image. The baseline model is confused and recognizes is it incorrectly as a five. Whereas the model with adversarial learning successfully recognized it as a six.  Let's look at one more image. This image is, again, an adversarial image. The baseline misclassifies it as an eight. Whereas the model with adversarial learning correctly classifies it as a three. So, yes, adversarial learning, indeed, makes a neural net more robust against this small but misleading. To summarize, in this video, we introduce how to construct the structure by generating adversarial neighbors.  We also guided you through a code example using the API from the neural structured framing network. Here's more information in the video description below. Along with the link to a Colab tutorial covering the example we discussed.  Don't forget to subscribe to this channel. Thank you. You've probably heard a lot about AI and machine learning over the last few months and maybe you've been inspired by videos showing what's possible with AI and machine learning. But what is it really? Once go beyond the hype and get down to writing code, what does AI look like? That's what we're going to go through in this video series. We'll teach you what it's like to write code for machine learning and how it provides different, new and exciting scenarios that will help you write appcations that P behave like a human being giving you artificial intelligence. I'm Laurence, and I'mo be your guide. You don't need a lot to get started and we'll be using the Python language. Don't worry if you've never used it, it's simple to understand and you'll be up and running in no time.  Let's start with a very simple example. Consider you're creating a game of rock, paper, scissors. When you play this with a human, it's basic. Now, let's take a look at the most basic part of a game that the human brain is good at. And that's recognizing what it's actually looking at. Consider these images. Most people KP look at them and instantly recognize which ones are rock, which ones are paper, and which ones are scissors. But how would you program a computer to recognize them?  Think about the diversity of hand types, skin color and people who do scissors like me, with the thumbs sticking out and people who do it with their thumb in. If you've ever written any type of code, you'll realize this is a really, really difficult task and might take you thousands or tens of thousands of lines of code and that's just to play rock, paper or scissors.  What if there was a different way to teach a computer to recognize what it sees? What if you could have a computer learn in the same way that the human does? That's the core of machine learning and the path to artificial intelligence. Traditional programming looks like this. You have data. For example, a feed from the web cam. And you have rules that act on this data. These rules are expressed in a programming language and are the bulk of any code that you write.  Ultramaltly, these rules will act on the data and give you an answer. Maybe it sees a rock, maybe it sees a paper and maybe scissors. What if you turn this diagram around? And instead of you as the programmer figuring out the rules, you give it answers with the data and have the computer figure out what the rules are? Now, I can have lots of pictures of rocks and tell a computer this is what a rock looks like and this is what paper looks like and this is what scissors looks like.  And I can have a computer figure out the patterns that match them to each other. Then, my computer will have recognized rock, paper and scissors. That's the core of building something that uses machine learning. You get a set of data that has patterns inherent in it and a computer learn what those patterns are. Before we learn a neural network, let's use a much simpler example.  Take a look at these numbers. Does a relationship between the X and Y values, can you see it? It's actually Y equals 2x minus one. If you saw it, how did you get that? Maybe you notice that the Y value increases by two, while the X value only increases by one. So it was Y equals 2x plus a minus something. And X was zero, Y was minus one, so you figured Y equals 2X minus one was a good guess and took a look at the other numbers and realized it worked. That's the principle that all machine learning works on.  Let's take a look. This is the entire code you can use to create a machine learned model that figures out what matches these numbers to each other. Don't worry if some of it doesn't look very familiar right now, you'll be able to pick that up in no time.  This first line defines the model itself. A model is a trained neural network and here we have the simplest possible neural network which in this case is a single layer indicated by the Keras.layers.dense code. That layer has a single neuron in it indicated by unit equals one.  We also feed a single value into the neural network, which is the X value, and we'll have the neural network predict what the Y would be for that X. So that's why we just say that the input shape is one value.  When you compile the model, there are two functions. The loss and the optimizer. These e the key to machine learning. How machine learning works, the model will make a guess about the relationship between the numbers.  For example, it might guess that Y equals 5X plus 5. And it'll calculate how good or how bad that guess is using the loss function. And then, it'll use the optimizer function to generate another guess. The logic is that the combination of these two functions will slowly get us closer and closer to the correct formula.  And in this case, it will go through that 500 times, making a guess, calculating how accurate that guess is and then, using the optimizer to enhance that guess and so on.  The data itself is set up as an array of Xs and Ys and our process of matching them to each other is in the fit method of the model. We literally say fit the Xs to the Ys and try this 500 times. When it's done, we'll have a trained model. Now, you can try to predict the Y value for a given X. What do you think would happen if you tried this code? Predict the Y when X equals 10. You might think the answer is 19, right? But it isn't. It's actually something like 18.998. Close to 19, but not there. Why do you think that would be?  Well, the computer was trained to match only six pairs of numbers. Looks like a straight line relationship between them for those six. It may not be for values outside of those six. There's a very high probability it's a straight line, but we can't be certain.  And this probability is built into the prediction, so it's telling us a value very close to 19 instead of exactly 19. Try the code out using the link in the description below this video to see it for yourself. This is something you'll see a lot more of in machine learning.  In the next video in this series, we'll take what you've learned and apply that to a more interesting problem, computer vision and seeing how you can teach a computer to see things using exactly the same methodology as you used here. We'll see you in that video. Don't forget to hit that subscribe button. Thank you.  Hi, everyone, and welcome to episode 2 of TensorFlow Zero to Hero. In the last episode, you learned about machine learning and how it works. You sawimple example of matching numbers to each other and how usingython code, how a computer could learn what the relationship between the numbers was. In this episode, you're going to take it a little further how you can teach a computer to see and recognize different objects.  Look at these pictures, how many shoes dough do you see? You might say two. But how do you know they are shoes? Imagine if somebody had never seen shoes before? How would you tell them that despite the great difference between the high heel and sports shoe, they're still both shoes.  Maybe, they would think if it's red, it's a shoe because all they've seen are these two and they're both red. But of course, it's not that simple. How do you know these two are shoes? Because in your life, you've seen lots of shoes and you've learned to understand what makes a shoe a shoe.  So it follows logically that if we show a computer lots of shoes, it will be able to recognize what a shoe is. And that's where the data set called Fashionamnist is used. So the 7,000 examples of each category, including shoes. Hopefully, seeing 7,000 shoes is enough for a computer to learn what a shoe looks like.  The images are only 28 by 28 pixels. So they're pretty small. And the less data used, the faster it is for a computer to process it. That being said, they still lead to recognizable items of clothing. In this case, you can still see thait's a shoe.  In the next few minutes, I'll show you the code that will show you how to train a computer to recognize items of clothing based on this training data. The type of code you write is almost identical to what you did in the last video. That's part of the power of TensorFw that allows you to design neural networks for a variety of tasks with a consistent programming API.  We'll start by loading the data. The fashion amnist data set is easy to load with code like this. The training images set of 60,000 images, like the ankle boot. The other 10,000 are a test set that we can use to check to see how well our neural network performs. We' see them later.  The label is a number indicating the class of that type of clothing. So in this case, the number nine indicates an ankle boot. Why do you think it would be a number and not just the text ankle boot?  There's two main reasons. First, computers deal better with numbers. Butps more importantly, there's the issue of bias. If we label it as ankle boot, we're already showing a biased towards the English language. By using a number, you can point to a text description in any language as shown here.  Can you guess all of the languages we used here? When looking at a neural network design, it's always good to explore the input values and the output values first.  Here, we can see that our neural network is a little more complex than the one in the first episode. Our first layer has the input of shape 28 by 28, which if you remember was the size of our image. Our last layer is 10, which if you remember is the number of different items of clothing represented in our data set. So our neural network will kind of act like a filter, which takes in a 28 by 28 set of pixels and outsets one of ten values.  What about this number? 128, what does that do? Well, think of it like this. We're going to have 128 functions, each of which with parameters inside of it. What we want is that the pixels of the shoe get fed into them one by one, the combination of the functions will output the correct value in this case nine. The computer will need to input the parameters to get the result. And the items of clothing in the data set. The logic is, once it has done this, then it should be able to recognize items of clothing.  So if you remember from the last video, there's the optimizer function and the loss function. The neural network will be initialized with random values. How good or how bad the results were, and then WH the optimizer, it will generate the new parameters for the functions to see if it can do better. You've probably also wondered aboutthese. And they're called activation functions. And rectified linear unit. What it really does is as simple as returning a value if it's greater than zero. Ifhat function had zero or less as output, it gets filtered out.  The output layer has ten probability that we're looking at that specific item of clothing.  So in this case, it has a high probability that it's item nine, which is our ankle boot. Instead of searching through to find the largest, what soft max does, ss it to one and the rest is zero. We have to find the one. Training is very simple. We fit the training images to the training labels. This time, we'll try it for just five. Remember earlier, we had 10,000 images and labels we didn't train with? These are images that the model hasn't previously seen so we can use them to tes how well our model performs.  We can do that test by passing them to the evaluate method like this. And then, finally, we can get predictions back for new images by calling model.predict, like this.  And that's all it takes to teach a computer how to see and recognize images. You can try this out for yourself in the notebook I've linked in the description below. Having gone through this, you've probably seen one drawback. And that's the fact that the images are always 28 by 28 grayscale with the item of clothing centered. What if it's just a normal photograph and we want to recognize its contents and you don't have the luxury of it being the only thing in the picture as well as being centered? That's why the process of spotting features becomes useful and the tool of convolutional neural networks is your friend. You'll learn about that in the next video. So don't forget to hit that subscribe button. And I'll see you there.  Hi, and welcome to episode 3 of zero to hero with TensorFlow. In the previous episode, you saw basic computer vision using a deep neural network that matched the pixels of an image to the label. An image like this was matched to a numeric label like this. But there was a limitation to that.he image you were looking at had the to have the subject centered in it. And it had to be the only thing in the image. So the code you wrote would work for that shoe, but what about these? It wouldn't be able to identify all of them because it's not trained to do so.  For that, we have to use someing called a convolutional neural network, which works a little differently than what you've just seen. The idea behind a convolutional neural network is you filter the images before training the deep neural network.  After filtering the images, features within the images could then come to the forefront and you would then spot the features to identify somethin A filter is simply a set of multipliers. So, for example, in this case, if you're looking at a particular pixelhat has the value 192 and the filter is the values in the red box, then you multiply 192 by 4.5 and each of the neighbors by the respective filter value.  So its neighbor above and to the left is zero, so you multiply that by minus one. The upper neighbor is 64, multiply that by zero and so on.  Some of the results and you get the new value for the pixel. Now, this might seem a little odd, but check out the results for some filters, like this one. That when multiplied over the contents of the image, it removes almost everything except the vertical lines. And this one is set to horizontal lines.  This can then be combined with something called pooling, which groups of the pixels and the image and filters tm down to a subset. For example, max pulling two by two will group them in sets of two by two pixels and pick thelargest. The image will be reduced to a quarter of the original size, but the features can still be matained.  So the previous image after being filtered and then max pooled, could look like this. T quarter the size of the one on the left. But the vertical line features were maintained. And indeed they were enhanced.  So where do the filters come from? That's the magic of a convolutional neural network. They're actually learned. They are just parameters like those in the neurons of a neural network that we saw in the last video. So as our image is fed into the convolutional layer, a number of randomized images will pass over the language. The next are passed into the layer and matching is performed by the neural network.  And over time, the filters that give us the image outputs that give the best matches will be learned and the process is called feature extraction. Here's an example of how a convolutional filter layer can help a computer visualize items. You can see across the top row here that you actually have a shoe. But it has been filtered down to the soul and the silhouette of a shoe by filters that learned what a shoe looked like.  You'll run this code for yourself in a few minutes. Now, let's take a look at the code to build a convolutional neural network like this. So this code is very similar to what you used earlier where you have a flattened input fed into a dense layer that in turn is fed into the final dense layer that is our output. The only difference here is I haven't specified the input shape. That's because I'll put a convolutional layer on top This layer takes the input so we specify the input shape. And we're telling it to generate 64 filters with this parameter. That is, it'll generate 64 filters and multiply each of them across the image.  And then, each it will figure out which signals gave the best signals to help match to the labels and much the same way it learns which parameters work best in the dense layer.  The max pooling to compress the image and enhance the features looks like this. And we can stack convolutional layers on top of each other to really break down the image and try to learn from very abstract features like this.  With thi methodology, your network starts to learn based on the features of the image instead of just the raw patterns of pixels. Two sleeves, it's a shirt. Two short sleeves, it's a T-shirt. Laces, it's a true, that type of thing.  We're still looking at the images of fashion at the moment, but the principles will extend into more complex images and you'll see that in the next video. But before going there, check out theotebook to see convolutions yourself, I've made a link to it in the description below. And before we get to that next video, don't forget to hit that subscribe button. Thank you.  Hi, everybody, welcome to the fourth and final video in this series of zero to hero with TensorFlow. I'm Laurence. And we're going to look at the very first problem we spoke about. And then, we'll see how we can build a machine learned model. Remember this back in episode one where we showed a scenario of rock, paper and scissors and discussed how difficult it might be to create an application that recognizes hands of different shapes, sizes, ethnicities, decorations and more.  We discussed how difficult it had be to write code to detect and classify these even for something as simple as a rock, paper or scissors. But since then, you've looked into machine learning and you've seen how to build neural networks first to detect patterns in raw pixels to classify them and then to detect features using convolutions to have a neural network trained to spot the particular features that make up an item, like the souls of a shoe. Let's put all of that together. And in this video, we'll see how to create a neural network that is traed on data of rock, paper and scissors to detect and spot them.  We'll start with the data. There's a data set here that has several hundred images of rock, paper and scissors poses. We'll s data. So first of all, we have to download the zip files containing the data. The code to do that is here.  One file has the training set, the other has a testing and validation set. In Python, you can unzip a file with the zip file library. And when we unzip them to attempt directory like this. This creates folders with subfolders of each of our categories. When using an ige data generator, it'll automatically label the images based on the name of the parent directory so we don't need to create label for the images. It's a really nice shortcut.  So I'll achieve that with this code. This creates an image dataenerator that generates images for the training from the directory that they were downloaded to.  We can then set up something called a training generator, which as its name suggests creates training data from that. We can do exactly the same for the test set with this code. Later, when you see the model.fit, you'll see we passed these in as the training and validation parameters.  Now, let's look at our neural network definition. This is very like what you saw on the last video, just with more layers. One reason is that the images are more complicated than the greyscale clothing you saw previously and the other is they're bigger. The input is 50 by 150. Our images are bigger than they were before. Under output is a layer of three neurons. Why would that be? Because there are three classes. Rock, paper and scissors. Between these, the code is very similar to what you saw previously. Just more of it. So we have four layers of convolutions, each with max pooling. Before feeding into a dense layer, the dropout is a little trick to improve the efficiency of a neural network by throwing away some of the neurons. We'll compile the neural network before with this code and then we can fit the data with the model.fit call.  Note that we don't have labels, that's because we're using the generator. The parent directories of both the training and the validation data sets. When you run this, you'll probably get accuracy of about 100% of the training data quite quickly. With the validation data getting to about 87% accuracy. This is something called overfitting, which happens when the model gets really good atg what it has seen before but it's not so great at generalizing.  Think about it this y. So, for example, if for all of your life the only shoes you'd ever seen were hiking boots, you probably wouldn't recognize high heels as shoes. You would be overfitting yourself. There are a number of methods to avoid this. And one of them is called image augmentation. And I've put the code for this into the notebook for you to try for yourself to see how it hopes to avoid overtting.  Once your model is trained, you can call model.predict to see how well it spots rock, paper or scissors. This code will take an image, reformat it to 150 by 150, which the model is trained for, and it will return a prediction. And here's a few examples that I ran so you can see that it's actually predicting quite well. But the best thing to do is to try it for yourself. I've put a link to the notebook in the description below. And you can use this code to train a neural network to recognize rock, paper and scissors images. That's it for this short series of videos. I hope it was useful for you to see the new programming paradigm that is machine learning. And through these examples, how to get yourself on the path of artificial intelli analyst engineer.  Don't forget to subscribe for more great content. Thank you. test test test test Test test tes This is one of the most important moments for everyone involved in manned space flight. This is why we are at Airbus are working hard and keep on innovating to make sure everyone on that space station is ...: ... S Goodfternoon, everybody. I'm Jacques, I'll be filling in for Tatiana today, presenting on MLIR accelerating TensorFlow with compilers. Now, I don't think I need to tell anybody in this room that machine learning i everywhere. There's a wide range of deployments happening in the industry today. Training happening on the Cloud and to the edge. We also have models getting larger and larger, and computational requirements for training these models ever increasing. We see a near exponential growth in the complexity and size and requirements nor training these models. If you combined the growth and deployment strategies, as well as models, velocity is a must. We need a faster, more scalable way to build infra, and to keep up with these models and deployment scenarios. So we need to build these ML systems faster. We want to unify efforts for extensibility and reusability. So we want toe able to standardize representation of some basic concepts. There's operations and types, what defines an operation, how you define an operation or a type. We want o create a common infrastructure of reusable building blocks and also support customizability and extensibility. We want a system that's able to scale and adapt for all the future needs. We designed MLIR which stand for multilevel intermediate representation. For TensorFlow and beyond as a part of the LVM project. So what is MLIR and why do we believe it's a compile infrastructure for machine learning? For one, MLIR is state of the art compiler technology. There's nothing like it. MLIR is modular and extensible. You can build different solutions using MLIR, building blocks that suit your solution. Importantly, MLIR is not opinionated MLIR does not try to force you into a box. It allows you to create a solution for your problem space. MLIR is also f fully customizable. We want to make it easy for all of these different deployment scenarios to work. Finally, MLIR is part of the LVM project, it's under neutral governance. Many compilers all around the world already. And the industry agrees. MLIR is strongly supported by our partners. Somef our partners include the largest hardware partners in the world, consisting of 95% of datacenter hardwe. 4 billion mobile phones and countless IoT devices. Academia and industry are all working together to solve this problem of compiling machine learning models. So MLIR what do we want to use it for in TensorFlow? We want to use it to build a better TensorFlow. We want to build a better user experience, as well as better pluggable hardware support. Now, if you're a user, we want to make it easier for you to debug and model. We want to make it if you have an error message in your optimized model, we want to be able to track it back to your original location and MLIR's location tracking enables this. And of course we want faster performance. So being able to get good performance on your hardware is essential. And for our hardware partners, it's an awesome time. We want to make it easier to integrate with TensorFlow. Because while accelerators it's a great time, it's only really interesting if it's usable for our users, and of course for researchers, we want to provide a standard infrastructure for research. So running an end to end workflow on production models, we want to make it easy for researchers to try new approaches, see a new fix and if it works well, of course contribute it. So let's take a little bit closer look at MLIR, the progressive loading and the infrastructure. Now, you've seen this before about the TensorFlow architecture and if we zoom in a little bit. We can expand the components. But let's just focus on the part where MLIR will be used. So a lot of these I mentioned before, the graph representation. But particularly for in the compation, so for optimization and conversion passes between different completed frameworks, as well as actually for writing IoT kernel or exploiting these. MLIR will be involved in all of these different parts. So as the previous slide showed, we can and will be using MLIR for many parts TensorFlow. If you think of MLIR as a common graph representation and legalization framework. It's a common state of optimizations and conversion passes, as well as a full code generation pipeline. But importantly as I mentioned, MLIR is modular, so you can tailor it for your use case, you can use it for what you need to solve your problems. So for example, you can reconfigure MLIR for graph writing and that's for example how we use it for the new TFLite converter. Just using the parts we actually need to get the final product we want. So let's talk a little bit about progressive lowering. MLIR allows you to represent multiple different levels of operations all in the same MLIR. So from a TensorFlow operation to XLA, HLO, to LLVM. You can lower progressively from one form to another and all of these can coexist together. For example you can have a function that -- this ability to mix and match these different levels of abstractions and dialects gives great power in actually modeling the problems to suit what your hardware specialization needs. But what about XLA? Sow we're using what we learned from XLA to build MLIR. It's a great tool for models with stable tensor shapes. For example, the tf.function API in TF2.2 enables great performance and we're working on ensuring there's full interoperability between XLA and MLIR. We're working very hard to make all components interact very well. So if you want to import from a graph, etc., all of these are possle. So you can mix and match your workflows. Importantly, MLIR allows for integration at any level of the stack. So you can start with the lower to HLOs, or go further and lower it to MLIR and codegen. MLIR allows you to hook into any part of this configuration, and in particular, MLIR does not require that you only use one, so if for your problem you need a combination of these ops, that's possible. So this makes it very easy to incremental enable MLIR in conjunction with your existing tools. Now, let's look at MLIR in action. So we're going to look at the new TFLite converter, as well as the new features provided by MLIR there. The new TF to TFLite converter launch upped just in February of this year. Very excited about this. Doing all the optimizations and legalizatio and then finally exporting to TFLite FlatBuffer. All of these with better error messages, so we were able to find out what went wrong, to support for TensorFlow conol flow. you can finally deploy many so of these models with deplo flow on the edge. Now, looking ahead, beyond the converter, you'll see MLIR in action in a lot of different places in TensorFlow. In particular, I mentioned ML as being the graph representation optimization framework in TensorFlow so we'll be unifying the different graph infrastructures that we have, as well as all the converters using MLIR, and the part that's very important for us is the partner integration and supporting for new hardware, as I mentioned new hardware is coming up every day, we want to make is very easy for folks to interact with TensorFlow. So please get involved with the team if you want to get involved with this. MLIR is also also integrating very tightly with optimization in codegen with the new TensorFlow Runtime. So th there is several ways to get involved. We have open design meetings where everybody can sign in and ask questions, there's talk from the team to other teams, we have a MLIR special interest group and of course we have code on GitHub, in the MLIR repo, as well as the TeorFlow repo. So feel free to add bugs and request features and get involved. With that, thank you, next up is Mingsheng to tell you all about the new runtime. All right, who here wants a shiny new runtime? I'm Mingsheng Hong, tech lead, manager of the TensorFlow Runtime team. Today I'm excited to share with you our new project, code named TFRT. As you probably guessed, it stands for none other than TensorFlow runtime. Some people might tell you that in the world of TensorFlow, runtime is what keeps tensors flowing. I think that if runtime does its job, you should never have to think about it, but since we are here talking about the runtime, let's first take a look at where runtime fits into the TensorFlow stack. Here's a diagram on the training workflow. Runtime can be driven by eager API. It can also execute graph programs produced by a graph compiler. Runtime is a low-level component that orchestrates all model execution by he relevant rel kernels. We're building TFRT to replace the existing runtime and let's talk about why. We talked to many of you and heard your pain points and requests. First, many of you are pushing the envelope in the performance and scalabili of model processing across both eager, and graph execution. Second, you're making continuous innovations through the addition of ops and kernels and devices to TensorFlow, and we need to make such extension work more streamlined and productive. And once you're done with the model research and tuning, you'll want to deploy TensorFlow everywhere across a diverse set of hardware platforms. For those great reasons, we're building a new runtime to help you. A new runtime that provides the performance, extensibility, and unification that you all are looking for. So, how does TFRT fit into the workflow of an ML model? Here we see the TF training stack again. Through TensorFlow APIs, your program can either eagerly dispatch ops to the runtime. As you can see from the blue arrows on the left side of the diagram. Or as the red arrows on the right side show, in the case of graph execution, your program first generates a computational graph which gets lowered to the optimized target-specific program, and then dispatched to the runtime. The optimization and lowering work uses the MLIR compiler framework which Jacques jt spoke about in in his MLIR talk. Finally, in both execution paths, TFRT, will call into a set of kernels to complete a model execution, as the purple arrow shows. Again, the term kernel here refers device-specific operations like a GPU-based matrix multiplication. TFRT orchestrates the efficient kernel exetion over a heterogeneous set of hardware. Now, let's dive a little more in the technical design and look at how we realize the vision of building a performant, extensible, and unified runtime. First, to achieve high performance, we built a lock-free graph executer. We have also made the eager op dispatch stack very, very shin. And the eager API calls will call into the relevant kernels with minimal runtime overhead. Second, to talk about extensibility, let's first cover some background. Host runtime is the component that drives host CPU and it also attached locally attached devices through the device run times. TFRT keeps device run times separate from the host run times. So you don't have to extend the rest of the runtime. The TFRT design also focuses on building common abstractions, such as shape functions and kernels and this way we get consistent behavior and also avoid duplicated engineering efforts. Now, if you feel a bit lost in the last slide, don't worry about it. Let's step back and let's look at how these key design decisions will benefit the core TensorFlow use cases. For those of you who care about training, you will see improved performance, as well as error reporting, and that should make it easier to debug with your models. If you deploy TensorFlow models in production, you'll be glad to see some improved performance, and some reduced CPU usage and I will show you in a benchmarking study shortly. TFRT will also support deployments across diverse hardware platforms. In the next couple slides I'll show you some initial results on sorting support. TFRT isntegrated into TensorFlow Serving to perform a flexible, high performance serving system forroduction environments. If you follow the orange arrow, it shows a pretrained model that's loaded into TFRT through TensorFlow saved model API. Now, the blue arrows show that the serving clients can send requests to the model and get prediction results back. We expect this TFRT integration to be largely transparent to the end users. So TFRT works for serving. How does it perform? In this benchmarking study, we used an MLperf benchmark model, and measured the performance of inference and compare it to the current stack. We chose to use FP16 and the batch size of 1 to perform the study of the one-time related of overhead. Let's now look at the numbers. Can I have some drumrolls, please? (Drumroll) Thank you! Of the I should first note that the current runtime is already highly optimized for graph execution, and serving needs. Through multiple runs, it had a respectable average inference time of 3.3 milliseconds. In comparison, TFRT had an average inference time of 2.4 milliseconds. Bam! There you have it. This is a handsome improvement of 28%, and there are more optimizations on the way. Our internal testing also showed that TFRT is scoring favorably over alternatives of TensorFlow on this model. We are very, very excited about this. The performance improvements are due to the more efficient use of multi-threaded CPUs, the asynchronous runtime design and a eral focus of low-level. This helps validate our initial work and prepares us for the ongoing push to make TFRT production-ready. I know. You're excited, too, right? And you are probably wondering, when can I have it? We will ship TFRT this year, and this is going to be an exciting journey. In additioto the main enhancementments to the we plan to integrate TFRT with the TensorFlow stack. Eventually, it will become the default runtime. We also plan to open source this project in the near future. We would like to get you all more involved. We will keep you updatedf our progress so the developers at TensorFlow.org mailing list, so please make sure to learn it. And also if you would like to learn more, please join our deep dive tech talks at the March 19th MLIR open design meeting. The meeting ispen to all members of the community. Thank you all and look forward to following up. Ne, you'll hear about TensorFlow Extended. Is Hi, everyone, I'm Tris war Kentin and I'm a product manager on TFX. I'm a tech lead manager from TFX open source. At Google, putting machine learning models into production is one of the most important things our engineers and researchers do, but to achieve this global reach in production readiness, reliable production platform is critical to Google's success and that's the goal of TensorFlow Extended, to create a stable platform for production environment at Google, and a stable platform for you to build production-ready systems, too. So how does that work? Our fill os if I is to take modern software and combine it with what we've learned about machine learning at Goog. In coding, you might build something that one person can create end to end. You might have untested code, undocumented code, and code that's hard to reuse. In software engineering, we have solutions for all of those problems, test-driven development, and much more. So how is that different in machine learning development? Well, a lot of the problems from coding still apply to ML. We also have a variety of new problems. We have null problem statements. We might need some continuous optimization, we might need to understand when changes in data will result in different shapes of our models. We've been doing this at Google for a long time. In 2007, we launched Sybil, which was our production-scalae platform for production ML here at Google and since 2016 we opened TFX and last year we open sourced it to make it easier for you to use in your platforms. What does it look like in practice? Well, the entirety of it FX as an end to end platform runs from best practices all the way through to end to end. You don't even have to use a single line of Google-developed code in order to get some of the best of it FX, all the way through to end to end pipelines that allow you to produce scalable, production-scale ML. This is what a pipeline might look like. On the left side of the screen, you'll see data intake. Then it runs through the pipeline doing things like data validation, schema generation and much more, in order to make sure that you're doing things in a repeatable, testable, consistent w and producing ML production results. So it's hard to believe that we've only been one year in open source for our end to end pipeline offering, but we have a lot of interesting things that we've done in 2019. Including building the foundations of metadata, building basic 2.0 support for things like estimators, as well as launches of fairness indicators in TFMA, but we're definitely not done. In 2020 we have a wide variety of interesting developments coming. Including native Keras on TFX, as well as TensorFlow Lite, trainer rewrite, and some warm. But we have something really exciting that we're announcing today. Which is end to end AI pipelines. We're really excited about these because they combine a lot of the best of Google AI platform with TFX to create Cloud AI pipelines. Please check our blog for more information. You should be able to find it if you just Google cloud AI platform pipelines. And now could we please cut to the demo? So I'll be giving an explanation about this demo. This is Cloud AI plaorm pipelines page, as you can see you can see all the exist Cloud pipeline clusters in this page. We've already created one and this can be found from the left of the Google Cloud console. If you don't have any pipelines created yet, you can use the new instance button to create a new one. This gives you a great experience about creating clusters. You can use the config button to create Cloud AI pipelines on Google Cloud. This gives you pipelines on Kubernetes. You can choose a class to run it from, choose the name space where you want to create your cluster inside and the name of the cluster. Once you're done you can simply click deploy and you're done. In this page, you can see a list of demo pipelines that you can play with. Youan see tutorials about creating pipelines and doing various techniques and you can use the pipelines tag from the left to view all your existing pipelines here. Since this class is already created. We are going to use newly launched TFX templates to create a pipelinen this cluster. I'm pretty much using this has a Python shell to run some simple pipeline commands. First you set up environments and make sure Making sure you havealled. environment and path properly set up. And the TFX version is up to date. Now you're making sure you have a Google Cloud project perfect config. And the cluster endpoint. Simply copy that from the URL into the notebook shell. Now we also making sure we create the Google container image repo, so that we can upload our containers to. Once that is done, we config pipeline name and the directory. Now we can use the pipeline creation to create a new so I'm going to show the content created by the creation. This includes a classical Teachers Colleges pipeline from Teachers Colleges and we have all the components necessary to do the machine learning. Some configurations related to Google Cloud. As well as about TFX itself. Once that is done, we enter the templates to entry, making sure that all the templates are filed from there. You can even run generated tests on the features to make sure the configuration runs right. Once that's done you can use configs.py command. This will create the temple image. Then create a pipeline using this container image on the pipelines page. As we see, the pipeline compiles, and the creation is successful. And we go back to the pipeline page, click refresh. Boom, we have our new pipeline. TFX components here readily available. We can create the test to run on this one. And click on the run. You are going to see each of the components when they run, they will gradually show up on the web page. The first component should be csve example gen. Yes, that's it. When you click on it you can see artifacts, input, output, volumes, component, manifest and you can even run the logs. We call that statistics gen, schema gen, and pipeline at the same time. So now all the data preparation is finished, the pipeline enters even the training stage which is presenting a TensorFlow model. If we click on the container component, you can even ease back those logs. Now, once trainer is complete, we do some model valuation and evaluation, using TFX components. And once all the model evaluation is done, we use pusher to push the generated model onto external system. So you have a model ready to use in production. You also use the tabs on the left to navigate on existing experiments, artifacts and executions. We are going to take a look at the artifacts generated from this pipeline, using the artifacts tag. So here you can see you have a pipeline, if you click on the model output artifacts from trainer, that represents a TensorFlow model. This is the artifacts ML metadata. We can see it's a model artifact produced by Trainer, and this explains what components produced in this model from what import artifacts and how this is used in other contains and what output is generated from the downstream components. OK, this is all of the demo, now I'm going to talk about another important development in TFX, which is supporting native Keras. For those of you who are not very familiar with TensorFlow 2.0, let me tell you a little bit of the history. TensorFlow was we released in Q3, 20 19. So this is a timeline of how TFX open source has been working on supporting all of this. We released the first version in the last Dev Summit and which only supports estimator training code. 2.0 was launched and we started working on supporting the Keras API. Previous slide, please? Back to Q4, 2019, we released the basic Tens TensorFlow 2.0 support. End to end, with a limited Keras support with the Keras estimator and now in the latest release TFX, I'm happy to anows we're releasing experimental support of native Keras training end to end. So what does that mean? Let's take a deeper look. Data ingestion analysis everything pretty much remains the same because it is model agnostic. For feature transform, we addedayers so that we can transform features in Keras model. For training, we creat a new generic trainer executer, which can be used to run any TensorFlow training code which exports a saved model. This also covers using the training Keras model API. For model analysis and anddation, we combined new finally when it gets to model serving validation, we will release a new component called infravalidator, this can component can be use to verify inference requests to making sure any exported TensorFlow model can be used correctly in production, including anything treated with native Keras. Now let's take a look at one of our partners, concur labs. They have all the develops in cop occur using ML effectively in their solutions. To do this, they need a modern pipeline tha They find that TFX with native Keras trainer allows them to do more things. One of the successful stories hee was an efficient BERT deployment. With a TF and Keras, they increaseimple models by using transform for the data preprocessing. Using state of the art models from TensorFlow Hub and create simple model deployments with TensorFlow Serving. The applications here covers some of the sentimental analysis and some of the question and answer type problems and they also created TFX pipelines to build preprocessing steps. We just published a blogpost on this, so feel free to check out the TFX blogpost. Another successful story is TFX pipelines for TFX models. They can create TFX pipelines that produce two models, one in TensorFlow and one in TFLite version. One of the most important things for the future is great partners like all of you on the livestream. We hope you join us to help make TFX work for your use case. Some of the areas we need your help include portability, on-empathy and multi-Cloud, Spark, Flink, HDFS and data and model governance. We have an exciting guest from Airbus, Marcelle, he could not be he physically today, but he recorded a video to talk about one of the interesting ways to use about TFX, which is in TFX in sce. For more, here is Marcel. This is one of the most important moments for everyone involved in manned space flight. This is why we at Airbus are working hard, and keep on innovating, to ensure everyone on that space station is safe and can return back to earth. To friends and family. Hello, everyone, my name is Marcelle Rummens, and I have the honor to tell you how Airbus uses anomaly detection to make sure everyone's safety on the International Space Station. You might ask us, Airbus, space, how does that fit? Well, Airbus has many different products, like commercial aircraft, helicopters, and we are also involved in manned space flight. For example, the Columbus module. It was built and designed by Airbus and finally launched in 2008. It is used for experiments in space, for example, in the field of biology, chemistry, Material Designs, or medicine. As y can imagine, such ad mo mod produces a lot f data. To give you an idea we have recorded between 15 and 20,000 parameters per second for the last ten years. And every second we receive another 17,000 parameters. So we are talking about trillions and trillions of data points. But what does this data actual actually represent? Well, it could be any kind of sensor data, and we want to detect anomalies in it to prevent accidents. Let me give you an example. If you have a power surge in your electrical system, this could cause a fire. If you have a drop in temperature or pressure, this could mean that you have a hole in your module that you need to fix. So this is very important that we detect these anomalies and fix them before something can happen. Because just imagine for a second, it is you, up there, 400 kilometers above Earth and you only have this few ihes of metal and plastic to protect you from space. It is cold and dark place without any oxygen. If something would have happened to this little prottion layer of yours, you would be in a life-threatening situation. This is why we already have countless autonomous systems on board the spacecraft to prevent these kinds of accidents. But with more and more automation, we can handle more complex data streams, increasing our precision of said predictions. But it is not about safety alone. It is also about plannability and predictive maintenance. Because the sooner we know that a certain part needs replacement, the sooner we can schedule and plan a supply mission, decreasing the cost of said launches. How does this work? Well, right now this is a manual process. Our automation works parallel with the engineers. Let me run you through it. We have our database on-premis, storing all these trillions and trillions of data points, and then we use a Spark cluster to extract the data and remove secret parts from it, because some things we are not allowed to upload. Then we use TFX on kube Flow to train the model. Then we use tf.estimator to train a model which tries to represent the normal state of a subsystem, so a state without any anomalies. Once we've done enough hyperparameter tuning, we're happy with the model, we deploy it using TF Serving, now, here comes the interesting part. We have the ISS streaming data to ground stations on Earth, which stream the data tour nifty cluster running on our datacenter. Here we remove a secret part of the data again and then stream it into the Cloud. In the Cloud we have a custom Python application running on Kubernetes which does the actual anomaly detection. The model will try to predict the current state of a subsystem based on what it has seen in the past and then using this prediction and the reality coming from the space station, we can calculate a so-called representation error. If this error is above a certain threshold, we can use this as an indicator for an anomaly. Now, if we have an anomaly, we would create a report and then compare it against a database of previously happened anomalies, because if something like this happened in the past, we can reuse the information we have on it. And then the final step is to hand it over to an engineer who will fix the problem and this is very, very important to us, because we are talking about human life on that space station, so we want a human to make the final decision in this process. But this is TF Dev Summit, so I want to have at least one slide about our model architecture. We're using LSTM-based auto encoder with dropout and then we replad the inner layers, so the layers between encoder and decoder, with LSTMs instead of dense layers, because our tests have shown that sequences better represent the kind of information we have, producing less false positives. What kind of impact did this project have? Well, we have been able to reduce our cost by about 44%. Some of this is projected because as I said earlier we're running in parallel right now. But the cost benefit mainly comes from the fact that our engineers can dedicate more and more time on more important, less repetitive tasks. Tasks for which you really need that Now we are talking about minutes, maybe hours. Another benefit is that now we have a central storage of all anomalies that ever happened plus how they have been fixed. This is not just good for our customer, because we have better document. But also great for us, because for example, it simplifies the onboarding process for new colleagues. Next steps: Looking into the future: We want to extend the solution to more subsystems, and more products, like the latest edition to the Columbus module and scheduled to be launched later this week. But overall these technologies are very important for future space missions, because as plans to go to moon and Mars become more and more concrete, we need new ways and new chnologies to tackle problems like latency, and the limited amount of computational hardware on board the spacecraft. Coming back to TFX, we want to integrate more components. For example, the model validator, because now we have more labeled data that allows us to actually do this automatic model validation, and finally the migration to TF2, which is very important because of course we want to keep up and use the latest version of TensorFlow. If you have any questions or want to learn more about the challenges we face during our project, have a look at the Google blog, we will publish a blogpost in the coming weeks that goes into more deil than I could go into ten minutes. Before I close, I want to thank everyone on the ISS analytics team for their incredible work and support, as well as anyone else who helped to prepare this talk. If you have any questions, feel free to find me on the internet. Write me so questions, I'm happy to answer them and also I will be available in the Q & A session of this livestream. Thank you very much, and  Thank you for the great video. And if you want to learn more, please check out our website, which has all of the blog posts, user guide, tutorials and API docs. Feel free to check out the GitHub repo with the source code and engage with us on the Google group. Now let's welcome to talk about TensorFlow Enterprise. Hello, everyone, I'm a software engineer at TensorFlow Enterprise, a part of Google Cloud. Now that we have seen the great story of TensorFlow and the cool uses even in space, now I'm going to talk about the enterprise application with TensorFlow Enterprise.  What does it mean to be TensorFlow Enterprise? What is so different? So difficult? Well, after talking to many customers, a couple of key challenges when it comes to enrprise grade ML. First, it is scale and performance. Often times the size of data is beyond. We need to think about this program different. Second is manageability. It is better not to have to worry about any nitty-gritty details of infrastructure complexity, including managing software environment and managing machines, whatnot.  Instead, it is desirable to only have to concentrate on the machine learning applications so it makes the most benefit to your business. Third, is the support. If your application is mission critical, the issues and the commitment to support application is essential. To continue operating your applications.  TensorFlow Enterprise brings a solution to those challenges. Let's take a look at the performance. In a nutshell, we compile a special build of TensorFlow, optimize for Google Cloud. It is based on the TensorFlow but also contains specialized optimization specifically for Google and Machine Learning interface. Let's take a look at what it looks like in practice.  This model is large training data that may be terabytes of data. As you see, it is no different from any typical except to the path of the training file. The deferred optimized specifically made for Google Cloud is making this even with training data and perf performant. This is another example that analysis from table which is the data warehou which may maintain hundreds of millions of business data warehouse. This example is a little bit, but API that all of you are familiar with. You can still train in your familiar ways, but you can readany millions of roles in parallel in efficient way and your training can continue with a performance. This is a little comparison of the u put when data is read. With or without optimization that TensorFlow interface brought in. The beta performance actually translates into the beta utilization of processor such as CPU and GPU. I/O is no longer the bottleneck of the training. Your training finishes faster and your training load time is shorter your custom training is cheaper because in a complete cost is you use the compute resources. Now, you have some ideas about what kind of we were able to make to TensorFlow, specifically for Google Cloud. Let's see how you actually see the benefit of it. We do this through managed services. Virtual images and content images it's managed on top of standard divisions. Most important, it has TensorFlow Enterprise built together with device drivers and version combinations and whatnot as well as configurations of other services in Google Cloud. Because this is just a normal virtual machine image of images, you can deploy it in many different ways in Cloud regardless of where you deploy it or how you deploy it, the TensorFlow Enterprise is there, you can take the befit of the other performance. You only have to take the TensorFlow image and the devices, such as CPU and opt myize optimize CPU. And TensorFlow Enterprise built preinstalled and preconfigured and it's ready to use so you can start, immediately start writing your code.  If you prefer notebook environment, lab is hosted and already started the VM, actually. All you have to do, you only have to point your browser to the VM and open up lab and open up a notebook so you can start writing your TensorFlow code, taking in the benefit of TensorFlow Enterprise.  Once you have the satisfactory models, now is the time to train your model at the full scale. It may not fit into the one machine and you m want to take the advantage of the training facilities of the TensorFlow first. So that it can support the large scale of the data in the model.  For this AI training is a managed service that takes care of distribute training clusters and complexities on behalf of you. More importantly, it drives the same TensorFlow Enterprise container image which is exactly the same environment you have used to build your action mod so you can be confident that your model just trains with full scale of data under the managed training service.  You simply need to overlay your code on top of the TF Enterprise content image and issue one command to distribute training cluster.  This example is grabbing workers with larger machines per each workers with attached to each worker, to train a large data set for your applications. This example brings up distributed training cluster with all TensorFlow Enterprise optimization included.  Now that you can train your model in a full enterprise scale, it is time to make it end-to-end pipeline taking advantage of TensorFlow pipelines and TensorFlow Extended. The pipelines is hosted on the community engine. It can also drive the exactly the same Enterprise container age so the optimization is still there, and you can be confident that your application to pipeline just ran because it is the same environment. The Enterprise support becomes essential, any risk of interruption, operation, and to continue operating your application in a business-critical manner.  Other ways to mitigate this risk is to provide long-term support with open source TensorFlow, we typically offer one year of maintenance window. For TensorFlow Enterprise, we provide the three years of support, that includes critical back-fixes and security patches and additionally and optionally, you know, we may back load future editions of TensorFlow as it demands.  As of today, we have version TensorFlow 1.15 as long time supported versions. If your business is pushing the boundary of AI and your business is sitting at the cutting edge of AI where application interface are critical to your business model and also your business heavily relying on being able to continue innovating on this space, we actually want to work with you through the regular service program.  We engineers and creators of TensorFlow and Google Cloud are willing to work with your engineers and your data scientis to mitigate any future bugs and issues that we may not have seen yet to support your cutting-edge applications to together advance the applications as well as TensorFlow and TensorFlow Enterprise as a whole.  Please check out the website and see for details of this white glove service program. Looking ahead, we are really excited to keep working tightly together between TensorFlow teams and Google Cloud teams. Being the creators and experts of both products, we continue to make the optimizations and implement TensorFlow for Google Cloud that includes beta monitoring and developing capabilities to your TensorFlow code that runs in Cloud as well as integration into, integration of this capability into your Google Cloud tooling for the beta productivity of your application.  We also are looking at smooth integration between TensorFlow popular high-level APIs such as Keras, and managed training services as well as even more managed services, such as TensorFlow Dev for the purposes of coherence and joyful experiences. Please stay tuned. This concludes my talk about TensorFlow Enterprise. For more information and full details, check out the website. Thank you very much. And next up is Tim to talk about TensorFlow Lite for mobile. Hello, everyone, I'm presenting today with TJ, a TensorFlow engineer who will be speaking in a bit. I'm super excited to be here today to talk about all of the improvements we've made over the last few months for TensorFlow Lite. So first of all, what is TensorFlow Lite? Hopefully, many of you know this by now, but we love to reemphasize it and provide context for users who are in need of TensorFlow Lite. It's our production-ready framework for ML models on mobile devices and systems. Other platforms used in edge computing. Now, t's talk about the need for TensorFlow Lite and why we built an on-device ML solution. We're in the midst of a huge demand for doing ML on the edge. It's driven the need for low latency that work in situations with poor network connectivity and enable privacy-preserving features. All of these are the reasons we built TF Lite back in 2 2017. As the world's ML framework, we've made a ton of improvements across the board. Recently, we've increased the ops we support, delivered numerous improvements, developed tools with optimized models, increase language support for our APIs and there'll be more on that in a bit.  And we're supporting more platforms like GPUs and DSPs. Now, you're probably wondering how many devices are we on now? Boom. TensorFlow Lite is now on more than 4 billion devices around the world across many different apps. Many of Google's own largest apps are using it as are a large number of apps from external companies.  This is a sampling of some of the apps that use TensorFlow Lite. Google Photos, YouTube and the assistant. Alongith third party apps like Uber, Hike, and many more. What is it being used for you ask. TF Lite is for use cases around image, text and speech. But we are seeing a lot of new and emerging use cases around audio content generation.  For the rest of the talk, we're going to focus on some of the latest updates and highlights. First up, let's talk about how we're helping developers get started quickly At TF World, we announced the e. support library. And today, we're announcing a series of extensions to that.  First, we are adding more APIs, such as our image API and introducing new, simplistic language APIs all enabling developers to simplify their development. We're also adding Androi Studio integration. That will enable simple drag and drop into Android Studio. And then, automatically generate Java classes for the TF Lite model with a few clicks. This is pored by the new cogen capability. Makes it easy for TensorFlow Lite developers to use a TF Lite model around the imports and outports and saves you a heap of time.  Here's a small example to show you what I mean. With the additions to the support library, you can load a model, set an input on it and run the model. Then, easily get access to the results in classifications. The cogen tool reads the metadata and generates the Java wrapper with the model-specific API and code snippet for you. This makes it easy to consume and develop wh TF Lite models without any ML expertise. And this is a sneak peek of how this will look i Android Studio. Here, you can see the model metadata from the drag and drop to TF Lite model. This will then cgen later for many different types of TensorFlow Lite models. How cool is that?  We're then committed to making Mobile ML super, super easy. Check out these in the next couple of weeks on the Android Studio Canary channel.  In addition to cogen, this is made possible through the new extended model metadata. Authors can provide a metadata spec making it easier for users of the model to understand how it works and then, how to use it in production. Let's take a look at an example. The metadata descriptor provides additional information about what the model does, the expected format of the inputs and the meaning of the model outputs. All of this is encoded our buff and our new tools generate the right metadata for your model.  We have mad our pretrained model repository much richer and added many more models. All of this is available via TensorFlow hub. We've got new mobile-friendly including Mobile Burt, LP plugapplications. These hosted on our TensorFlowhub. So check out TFhub for all of the details.  Now, let's talk about transfer learning. Having a repository of ready to use models is great for getting started and trying them out. But developers regularly want to customize these models with their own data. That's why we're releasing a set of APIs which makes it easy to customize using transfer learning. Calli this TF Lite model maker. It's just four lines of code. You start by specifying your data set, tn choose which model spec you'd like to use, and boom, it just works. You can see some stats of how the model performs and lastly, export it to a TF Lite model.  We've got text and image supported and new use like object detection and QA are coming soon. Now, we've got an exciting development in graph delegation. There are multiple ways to delegate your model into the TF Lite, through the NN API. Recently, we've added increased GPU performance and DSP delegation through Hexagon. And we've increased the number of supported ops through the Android and API.  But you knew that. What's new? Well, we have a big announcement that I'm incredibly excited to share today. I'm excited to announce the great news as of today, we're launching a core ML delegate to accelerate floating points in the latest iPhones and iPads using TensorFlow Lite. The delegate will run on iO S11 and later. But to get benefits of using it directly, you want to use this on devices with the Apple Neural engine. It's dedicated hardware for machine learning computations on Apple's processes, and available on devices with the A12SOC or later. The iPhone 10S. With the neural engine acceleration, you can get 4 to 14 time speedup compared to CPU execution. So that's our update on delegates.  We've heard from developers about the need for more and better tutorials and examples. So they're releasing several full example apps that show not only how to use the model but the end-to-end code that a developer would need to write to  multiple platforms, Android, iOS, Raspberry Pi. And now, I'm going to hand over to TJ who is going to run over more exciting improvements to TF Lite and dive in more. Awesome, thanks, Tim. Hi, everyone, I'm TJ, and I'm an engineer on the TensorFlow Lite team. So let's talk about performance.  A key goal of TensorFlow Lite is to make your models run as fast as possible on CPUs, GPUs, DSPs or other accelerators. And we've made serious investment on all of these fronts. Recently, we've seen significant CPU improvements added open CL support for faster GPU acceleration and have full support for all Android Q and API ops features. Our previously announced targeting low and mid-end devices will be available for use in the coming weeks.  We've also made impvements in our benchmarking tooling to better assist model and app developers in identifying optimal deployment configurations.  We've even got a few n CPU performance improvements since we last updated you at TensorFlowWorld. More on that in a bit. To highlight these improvements, let's take a look at our performance about a year ago a Google I/O. For this example, we're using Mobile Net V1. And compare that with our performance to date. This is a huge reduction in latency and can be expected across a wide range of models and devices from low-end to high-end. Just pull the latest version of TensorFlow Lite into your app and you'll benefit from these improvements without any additional changes.  Digging a bit more into the numbers, floating point CPU execution is the default path providing a great baseline. Enabling quantization now made easier with our post training quantization tooling provides nearly 3X faster inference.  Enabling GPU execution provides an even more dramatic speedup about 7X faster than our baseline. And for absolute peak performance, we have the pixel 4's neural core accessible via the NN API TensorFlow delegate. This kind of accelerator available in more and more of the latest phones unlocks capabilities and use cases previously unseen on mobile devices.  And we haven't stopped there. Here's a quick preview of some additional CPU optimizations coming soon.  In TensorFlow Lite 2.3, we're packing in even more performance improvements. Our mo model optimization tool kit has allowed you to produce smaller quantitized models for some time. We've optimized the performance of these models for our dynamic range quantization str strategy. So check out our model optimization video coming up later to learn more about quantizing your flows. On the floating point side, we have a new integration with the XNN pack library, available through the library but still running on CPU. If you're a you can build there source, but we're shipping them in 2.3 coming soon. Last thing on the performance si, profiling. TensorFlow Lite now integrates with Profeto the new standard profiler in Android 10. You can look at overall TF Lite inference as well as op-level. Also supports profiling of allocation for trackingemory issues.  So that's performance. Now let's talk model conversion. Seamless and more robust model conversion has been a major focus for the team. So here's an update on our completely new TensorFlow Lite conversion pipeline.  This new converter was built from the ground up to provide more intuitive error messages when conversion fails, support control flow operations, and it's why we're able to deploy new NLP models like Deep Speech, or Mask, CNN and more. The new model will be available more generally soon as the default option.  We want to make it easy for any app developer to use TensorFlow Lite. To that end, we've released a number of new, first-class language bindings including Swift, Objective C, C Sharp for Unity and more. We've seen the creation of additional language bindings in Rust, GO and Dart. This is in addition to our Java and Python bindings.  Our model optimization toolkit remains the one-stop shop for compressing and optimizing your models and now easier than ever to use with post training quantization. Check out the optimization talk coming up later for more details.  Finally, I want to talk a bit about our efforts in enabling ML, not just for billions of phones but also for the hundreds of billions of embedded devices and microcontrollers used in production globally.  TensorFlow Lite for microcontrollers is that effort and uses the same model format, converter pipeline and kernel library as TensorFlow Lite.  So wha are these microcontrollers? These are small, low-power, all in one computers that power everyday devices all around us, like microwaves, smoke detectors, toys and sensors.  And with TensorFlow, it's now possible to use them for machine learning. You might not realize that embedded ML is already in use in devices you use every day. For example, OK, Google on many smartphones runs on a small DSP, which then wakes up the rest of the phone.  You can now use TensorFlow Lite to run models on these devices with the same tooling and frameworks. We're partnering with a number of industry leaders in this area. In particular, ARM, an industry leader in the embedded market has adopted TensorFlow as the official solutionor AI on ARM microcontrollers. And together, we've made optimizations that significantly improve performance on embedded ARM hardware.  Another recent update, we've launched the official Ardwino library. This makes it possible to do speech detection in about five minutes. You create your models use TensorFlow Lite and upload them to your board using the IDE.  If you're curious about trying this out, this library is available for download today. So that's a look at where we are today. Going forward, we're continuing to expand the set of supported models, make further improvements to performance, as well as some more advanced features like on-device training and personalization. So please check out our roadmap on TensorFlow.org and give us feedback. We'd love to hear from you.  OK. That's it. Thanks, everyone, for listening. Next up is Nick to tell you about Project Jacquard. OK, we just heard from the TensorFl Lite team how it's getting easier to place machine learning directly on devices. And I'm sure this got many of u thinking, what's possible here?  Now, I'm going to tell you how about how we're using Jacquard to do this. Embed machine learning directly into everyday objects. Before I jump into the details, let melell you a little bit about the Jacquard platform. So it is a machine learning computing platform that extends everyday objects wh extraordinary powers.  At the core of the Jacquard platform is the tag. This is a tiny embedded computer that can be seamlessly integrated into everyday objects, like your favorite jacket, backpack or pair of shoes. The tag features that allows us to run ML models on the tag with only sparse gesture or motion predictors being emitted when detected.  What's interesting is tha the tag has a modular design where the ML models can either run directly on the tag as a stand alone unit or via additional low-powered compute modules that can be attached along with other sensors, custom LEDs or motors.  A great example of this is the Levi's jacket I'm wearing. If we can switch over to the overhead camera. I can take the Jacquard tag and add it to a specifically designed sensor module which is integrated into the jacket. Let me check that, again. This talks to a processor that's running on the jacket itself which is thought to some integrated sensor lines in the jacket.  The M0 processor not only reads data from the sensor lines, but it also allows us to run ML directly on the tag. This allows us to do gestures, for example on the jacket to control music. So, for example, I can do a double tap gesture. And this can start to play some music.  Or, I can use a cover gesture to silence it. Users can also use Swipe-in and swipe-out to do whatever they'd like on the app. What's important is that all of the gesture recognition is running on the M0 processor. This means, we can run these models at super low power sending only the events to the user's phone via the Jacquard app. I'm sure many of you wondering how we're getting the ML models to be deployed, in this case, in a jacket. And by the way, this is a real product you can go to the Levi store and buy today.  So as most of you know, there are three big on- device ML challenges that need to be addressed to enable platfms like Jacquard. So the first is, how can we train high-quality ML models that can fit memory-constrained devices? Second, let's assume that we've solved problem one and have a TensorFlow model that's small enough to fit within the memory constraints. How can we get it running on low-compute embedded devis for realtime inference? And third, even if we've solved problems one and two, it's not going to be a great user experience if they have to keep charging their backpacks every few hours. How can we ensure that it's ready to respond when multi-day experiences on a g single charge?  Specifically, for Jacquard, these challenges have mapped to models as small as 20 kilo bytes or running ML models on low-compute microprocessors like M0 plus, which is embedded here on the cuff of the jacket.  To show you how we've addressed the challenges for Jacquard, I'm going to walk you through a specific case study for one of the recently products that it actually launched yesterday. First, I'll describe the product at a high level, and then, we can review how we've trained and deployed ML models that in this case fit in your shoe.  So the latest Jacquard-related model is called gamer. It's built in collaboration between Google, Adidas and the EA Sports Mobile team. With Gamer, you can insert the same tag inserted in your jacket into an Adidas insole and go out in the world and play soccer. You can see here where the tag inserts at the back.  The ML models in the tag will be able to detect your kicks, your motion, your sprints, how far you've run, your top speed. We can even estimate the speed of the ball as you kick it.  Then, after you play, the stream of predicted soccer events will be synced with your virtual team in the EAFifa mobile game where you'll be completing weekly challenges. This is all powered by the ML algorithms that r directly in your shoe as you play.  So Gamer is a great example of running ML inferences on the device really pays off. As players will typically leave their phone in the locker room and go out and play for up to 90 minutes with just the tag in their shoes. Here, you need the ML models run directly on the device and be smart enough to know when to turn off when the user's clearly not playing soccer to help save power.  So this figure gives you an idea of just how interesting a machine learning problem this is. Unlike, say, normal running, we would expect to see a nice, smooth periodic signal over time, soccer motions are a lot more dynamic.  For example, in just eight seconds of data here, you can see that the player moves from a stationary position on the left, starts to run, breaks into a sprint, kicks the ball and then slows down, again, to a jog all within an eight-second window.  For a gamer, we needed our ML models to be responsive enough to capture the complex motions and work across a diverse range of players. Furthermore, this all needs to fit within the constraints of the Jacquard tag. For a gamer, we have the following on-device memory constraints. We have around 80 kilobytes of ROM, that needs to be used for the re required ops, the graphs, and deploying everything together to be plugged into the Jacquard OS. Also around 16 kilobytes of RAM which is used to buffer the data and buffers for the ML inference in realtime. So how do we train models that can detect kicks, a player's speed and distance and even estimate the ball speed within these constraints?  Well, the first step is, we don't. Well, at least initially. We train much larger models in the cloud to see just how far we can push the model's performance. This is using TFX, which is one of the systems we were showing earlier today. This helps inform the design of the problem space and guide what additional data needs to be collected to boost the model's quality.  After we start to achieve good model performance without the constraints on the cloud, we then use these learnings to design much smaller models that start to approach the constraints of the firmware. This is also when we start to think about not just how the models can fit within the low-compute and low-memory constraints, but how they can run at low power to support multi-day use cases.  For a gamer, this led us to design architecture that nsists of not one but four neural networks that work coherently. This design is based on the insight that even during an active soccer match, the player only kicks the ball during a small fraction of game play. We, therefore, use smaller models tuned for high recall to first predict if a potential kick or act of motion is detected. If not, there's no need to trigger the larger, more precise models in the pipeline.  So how do we actually get our multiple neural networks to actually run on the tag? To do this, we have built a custom-C model exporter. For this, the model exporter is using a Python tool that calls a number of C ops from a lookup. This then generates custom C code with a lightweight ops slide that can be shared across multiple graphs and the actual .h and .c code you get for each model. This allows us to have zero dependency overheads for the models and make every byte count.  Here, for example, you can see an example of one of these C ops that would be called by the library. Our rank 3 transpose operation which supports multiple I/O types, such as 32-bit flows. You can see how we're taking our neural networks and getting them to run on the Jacquard tag live. I hope you're inspired by projects like Jacquard and makes you think of things you can do with TF Lite microto build your own ML applications.  Next up is Na who is going to be talking about TensorFlow js. Hi, my name is Na Li. And I'm one of the engineers for TensorFlow js team. TensorFlow js is an open source AI platform for developing, training and using ML models in browsers or anywhere JavaScript can run. Today, I'm going to show you how TensorFlow js powers some amazing web applications and usage beyond the web. TensorFlow js was started in 2017 and launched in 2018 along with the model repo and no js support. This shows the major document milestones of TensorFlow js.  In 2019, we launched 1.0, we added four more models carving more use cases of ML, including body segmentation, toxicity detection and sound recognition. We launched two exciting features Auto ML and direct support.e'll show you how ML is made easy with these new features. Also, in 2019, we started to see adoption by large users, such as Uber and AirBNB. Also increasing adoption by app developers of many platforms, such as TikTok. We kept expanding to include some of the most popular models.  Today, we're going to showcase them to you. We also kept expanding our platform functionalities, we launched a fourth backend around Crhristma. In terms of user adoption, we collaborated with Glitch and Codepan, two of the largest online JS playground to further push JS developers to learn and use machine learning in the applications.  In 2020, we will add more models and we will keep investing in the TFjs platform to promote more research cases. This includes modernizinghe library, adding TF support and integrate within the TF ecos ecosystem. We'll release 2.0 later this year. So please, stay tuned.  ML is made easy with TensorFlow js. We provide multiple starting points for your needs. You can directly use pretrained models, you can retrain existing model in the browser, or you can build your model from scratch with a layer API.  We try to make running existing models really easy for everyone. We provide a collection of models, directly used with inputs widely accessible in web applications such as Image, Video, Audio and Text. If you already have a trained model somewhere, we provide a converter for you to convert a model to TFJS format.  And we launched the converter wizard, an interactive online tool to help you figure out the correct parameters. If you have a safe model and your model will run MLJS, you can directly run it without the conversion. So no missing ops headache anymore.  So now, let's take a look at some of the new applications and models we and our users built this year. First, we introduced the Face Match model. This is able to recognize 600 facial points. Its size is under 3 megabytes. We've seen good performance on high-end phones and desk tops. Performs inference at 35RPS on iPhone 11. Next, we introduced the Hand Post model. This model has a weight size of 12 megabytes and also shows good performance on model phones and desktops. It's worth mentioning that applications that will be enabled by Face Match and Hand Post are only possible running on-device. As you can see he, the experience needs to be in realtime. And TFJS enables this. Next, let's look at a language model. The model is 100 megabytes. It's pretty large for the web. The inference speed is 100 milliseconds. We made a Chrome extension for smart Q&A. Users can ask any question on the web page and the most relevant answers will be highlighted. We demonstrate it's possible to run large model in browsers. This model can also be used in conversational interfaces such as check bot. We'll have open source models using the models as building blocks, you can build a lot of applications for use cases such as accessibility, VR experiences, such as virtual, toxicity detection and conversational agents, et cetera.  Next, we show some applications that our users built. We're seeing a vibrant developer community buildg many interesting applications and APIs on top of TensorFlow js. There has been 1.4 million NPM downloads and more than 6,000 dependent GitHub reposts so far. Besides individual efforts, companies also use TensorFlow js in their products. An augmented reality technology company based in Canada. They have used TensorFlow js to build virtual applications for several beauty brands. Using TensorFlow js, they were able to provide a smooth UI at 25 on films. They can mitigate users' privacy concerns by restricting user data to stay on-device. They were also able to deploy their products across multiple platforms, including browsers and many program platform.  It won't be possible to support ML models to run on devices of all kinds of capabilities without the various backends TensorFlow js provides. I will talk about the characteristic of each backend and their performance. We currently have four backends, which includes the CPU backend that is always available for all devices, web backend which is the fastest backend by running models on GPU. A node backend that runs on laptops, servers and IoT devices. We add a fourth one that allows C++ to run in browser. We also invest heavily in future technology, web GPU backend will be coming soon. We expect to have even better performance than Web JL. We compared the performance of three backends browsers on two models. Here, you can see how fast it is.  On both models, it is ten times faster than plain js across all devices. Especially on small models like this face detector model on the left -- on the right. Which is 100 kilobytes. The performance is comparable to and sometimes even faster than Web JL. This is because the backend has a fixed overhead, costs .12 milliseconds. We've seen a trend of ultra light models being made for running on Edge devices. It provides a nice addition to the JL background with a really great performance. Especially it is more accessible in lower end devices.  It is available on about 90% of devices as a comparison Web JL is only available on53% devices. A medium to larger models, such as the one on the left, which is a mobile net model, 3.5 megabytes. Web JL is still the fastest. The inferen time can be as fast as 20 milliseconds, which is 50 FVM fps, whereas it is about 100 to 200 milliseconds.  But we're looking to further improven. With technology, it'll be two to three times faster. For node performance, people often think they need to use the Python-based TensorFlow because it's fast. Actually, as we'll show you here, our js background pro backend provides the same as the Python. We'll benchmark the mobile which is shown on the left in orange bars. Similar performance as the Python-based TensorFlow. About 20 lliseconds on CPU and about 8 milliseconds on GPU. On the right is a Q&A model. Developed by a Leoding research team specialized in NLP research. This graph shows the benchmark result in the local environment. The top bar is running the model on note backend, the bottom using the Python API. Underneath, both are calling TensorFlow C++ code. We can see the inference time is comparable and in some cases, Node is faster. With the four backends, we're able to support a variety of platforms, including all major browsers, servers, mobile devices, desktops and IoT devices.  It's also worth mentioning that our latest backend is available to run on all of the platforms, except we still to recommend to run Node backend for Nodejs. We also tried to retrain existing models really easy for everyone. TFJS supports transfer learning in the browser.  Now, we added auto-ML for vision. It's a Cloud service that allows you to train a custom model without writing any code. Never easier than before. Last, we'd like you to, we'd better. We launched a campaign for people to discover community creations and join the discussion of ML for the web and beyond. Please use #madewith tfjs when you most your TFjs relevant stories. Next up is Q&A.  And we're back. Hi, I'm Robert, I'm with the TensorFlow team, and I'm joined by Sandeep. And we haveome of your questions from the live feed. So let's get to that. First question from Morton, I hope I got your name right, will the new TFRT support distributed training? Thank you, Morton, for the question. Yes, the training is very important. We are pushing on the design and implementation in parallel to supporting training that we covered in today's talk. Distributed training has own set of challenges compared to just supporting serving. For example, we need to figure out how the hosts should talk to each other. And sometimes, they should go through the normal data central network. You might be familiar with GR as one possible. And sometimes, they go through accelerator networks, such as using GPU technology and Google also has TPU technology. So there are lots of interesting challenges that we're working through and we'd love to keep the community posted of our progress. Great. Thanks for that. Next question up, I don't have a name for it, but the question is, is TFX 2.0 compatible? And I can take that one. Yes. TFX 2.0 compatible. We just announced native Keras support for TFX. So you can have an entire TFX Keras pipeline that supports 2.0. There is one feature of Keras, Keras feature layers that we don't yet support. But other than that, yeah. It's 2.0 compatible. And next question. This is from Andre Mokolinko. Why is it impossible to use time-distributed wrapper over a Keras model? For example, if I have a one-frame model and want to extract features along the time access in video? Sandeep?  Thank you so much for the question. The short answer is, yes, this is indeed possible to do. What you want to do is use the Keras functional API. When you use the functional API, you can pretty much use any trained model as a layer in sort of other model code. So in this case, you would wrap it with the time-distributed class and then, you can, basicall use an image model as part of a video sequence.  If you go to Keras.io website and look at the functional API guide on the "getting started" section, you'll see a section that talks about how to use pre-trained models as callable layers. And that shows, specific specifical example. It'll give you everything you need to get started, hopefully. Thanks for that. That was a tough question. Next one is from Tagir Akmetchin. Is there any way to monitor on which device, CPU or GPU my tensors are stored? You want to take that one? Sure. Thank you for the question. Let me give you a short swer. And I think we'd love to learn more about the use cases, as well, so we can design something that is well thought  I think, overall, this seems like a very reasonable request. And for the runtime infrastructure perspective, we won't be able to track where the tensors or variables live. There's also the question of the API question. How we expose this? How to use this? That's beyond the scope of the runtime, but I'd love to continue the dialog and learn what are the use cases and how we should design the future property. Great. Thanks. All right. Next question from Subim. Any tutorial pages for TFX pipelines? I can take that one, too. Yes, there's a bunch of new tutorials, actually, available including a nice tutorial for Cloud AI Platform pipelines which includes TFX. But check out our tutorial pages on TensorFlow.org. And I think you'll find good stuff there. And that was our last question for today. So thanks very much. And next up, we have the next thing. We have a break. OK. Thanks very much.  AirBNB is a marketplace, we have 81,000 cities, which equates to hundreds ofillions of photos, making it possibly the largest collection of homes in the world today. When a guest decides to select a home, one of the biggest influences in their decision is a diverse set of images. But a lot of times hosts will take a lot of pictures of a single room and forget the pictures of the other rooms.  They also have the option to add captions. But in a lot of cases, they're totally off. We were faced with a challenge of identifying what's actually in these pictures and present them properly on the site. That's one area where Machine Learning excels. The real challenge was one of scale. We had upwards of half a billion images to go through. It was going to take months to go through. Using TensorFlow, we were able to speed up the process and deliver a reasonable model within days. We had the idea of making it very agnostic to different ML frameworks, and so we levered TensorFlow to train the model and BigHead helps with t model life cycle, the feature management and TensorFlow serving to help serve the model results. Before you're thinking about which tool to use, y first think about which model to use, right, and we did research on this and we find that resnet was one of the performing models. We used that as the basic model feature. We used the APIs and serving and distributed GPU computations. This ultimately led to a pipeline that we can deploy to go through hundreds of millions of images very quickly.  So the end goal is basically using these classifications of images to make sure their first initial set of photos that they see aren't just a picture of the garage and bathroom. But it cod be the living room that's gorgeous in the bedroom and the swimming pool. Future applications could be to detect different objects and homes. And if users decide to search on the website for specific amenity types, we can bubble that up to the surface. If you like how AirBNB operates today, it's everywhere in the company. Predictive booking. We're passing into the hundreds of models. It's something I expect to keep growing. And with a lot of the new frameworks coming out, we can make better experiences for our guests and better business decisions, as well. Extreme weather is . There's more extreme rainfall, heavy flooding, forest fires. Being able to predict these extreme events more accurately is going to be a big challenge we're facing right now. There's 100terabytes every day. It's a big data problem. We need things that are fast that can sift through all of the data rapidly and accurately. And Deep Learning is poised for problems in climate signs. A lot of users are using TensorFlow. It's one of the more popular frameworks. We use TensorFlow to iterate quickly over the different models, different layer parameters. For this particular planet project to create the Deep Learning, we started from segmentation models that have proven to be successful, for instance, the satellite imagery. And then, use TensorFlow to enhance the models until we found a set of models that can perform well enough for the specic task.  But from the volume of the data, the complexity of the data, the network requires 14 tera bytes. Do really tackle the problems requires the largest computational available on e pl planet. Two tennis courts in total size, this thing is state-of-the-art, a million times fasr than your common laptop. Imagine what you do at your workstation but now, image having 27,000 times that power. We can do that now. We were surprised as how good it scales. 2,000 nodes, 5,000. This is the first time anyone's run an AI application at this scale. Instead of having the climate scientists writing high-tuned code, they can express things in a natural way in Python in TensorFlow and get all of the high-performance code that most HPC people are used to within TensorFlow. We're entering the space where AI can contribute to the contradictions of these extreme weather events. When you combine traditional HPC with design, we can tackle things we never thought we could tackle. Understand diseases like Alzheimer's, cancer. That's incredible. We've shown with TensorFlow, you can get to massive scale and you can get awesome performance and accomplish your goals. Genetics, neuroscience, high-energy physics. That's exciting for me.  Some analysis is used in so many fields.uch as identification of music and identification of the type of animal based off the sounds they produce. Sound is critical to what we're doing because physiological sounds are important in medicine, but it hasn't changed for close to a hundred years. The duct is limited by the human ear. This model is very inaccurate and causes misdiagnosis. Our mission is to use machine learning and TensorFlow to revolutionize the diagnosis and treatment of respiratory diseases in low-resource areas.  It's a partial screening tube that helps doctors make decisions quickly. The core technology is trying to mimic the human auditory system. Once a patient walks in, the doctor collects the lg sounds with symptoms, risk factors and vitals of the patient, and then, the app combines all of that information and gives a doctor a probability of the patient having a specific respiratory disease.  Eric introduced TensorFlow to us because he felt that we can use TensorFlow to go through all of the stages of development of deployment to our model. Our model uses spectrograms. We take sound data from the scope and convert it into the visual problem that the computer. We have worked with a number of clinics and biologists and we're able to collect data from 621 patients and we use that data to bud our machine learning model. Once we have trained and evaluated our model, we deployed it on our app. TensorFlow Lite helps us to perform an inference on our mobile device without the need of a connection. So doctors can use the app offline without connecting to the Cloud. There are 216 healthcare facilities using the app and devices. These clinics are spread out and some are very rural clinics. It's really a nice project. Thank you. The device sounds and when combined with the physical exami examination. Misdiagnosis, which is one of the problems leading to deaths in Kenya is really creating a menace. It is helping me, shows me even the things I would have missed if I had to do the traditional way of doing it.  2.5 million people die each year because of pneumonia, COPD and tuberculosis. I believe we can use machine learning in the treatment of these diseases.  Looking at this book page by page and trying to decipher, read and transcribe whatever is there takes an enormous amount of time. It would require an army of polyographer. What I'm excited most about is it enabled us to solve problems up to 10 to 15 years ago that were unsolvable.  Before using any kind of machine learning model, we needed to collect data first. You had thousands of images of dogs and cats on the internet, but very little images of ancient manuscripts. We build our own custom web application for crowd sourcing, and we collect the data. I didn't know much about machine learning in general, but I found it very easy to create a TensorFlow environment. When we were trying to figure out which model worked best for us, Keras was the best solution. The production model runs on TensorFlow layers and inte interface. We experimented with binary classification, fully connected networks and, finally, we moved to convolutional neural network and multiclassification.  We can get 95% average accuracy.  This will have an enormous impact. In a short period of time, we will have a massive quantity of historical information available. I think solving problems is fun. . Hello, everybody, I'm a program manager for TensorFlow. Today, I'm very excited to bring you the greatest and the latest about our community. As you can see, our community's growing evt of the TensorFlow since we open sourced the technology in 2015. Since then, our project has received 142,000 stars. How impressive. And. I'm truly imessed by all other contributions and hard work. So thank you so much for all of your efforts. Now, let's have a more in-depth look of the different groups that form our community. Our vibrant community includes code and noncode contributors. Managers and students, but also we onboarded 148 machine learning Google developer experts part of the Google developer expert network. We also established 73 TensorFlow user groups worldwide. We formed 11 special interest groups and welcome 1,375 developers at the developers at TenrFlow mainland list. And last year, we welcomed 20 students at the Summer of Code.  Furthermore, our documentation has been translated to 12 languages and on GitHub, we have nearly 2,500 contributors and nearly 9,000 commits. How impressive, again. What a fantastic work. We've built an open source project we can be very proud of. I'm going to give you an overview of how to get involved in the TensorFlow community. Firstly, the TensorFlow user groups aim to upscale contributions t TensorFlow with a local focus. Some examples of the initiatives conducted by these groups include events, apps. The machine learnings are experts and practitioners from the community who support with outreach initiatives.  And finally, the special interest groups. The SIGs, contributing to TensorFlow to certain parts and more specific projects. Right now, we have 73 TensorFlow user groups distributed all across the world. And so far, these groups have reached more than 100,000 developers worldwide. In your country, if you don't have a TensorFlow user group yet, please feel free to reach out to us. We can help you with the resources and share best practices from other parts of the world. These events are a great way to engage with other developers and with the community.  We als strongly encourage students to engage in these initiatives and learn with more experienced developers. After the TensorFlow Road Show last October, we established the first TensorFlow user groups in L LATAM. We are extremely happy to welcome these developers to the TensorFlow community.  Also, in collaboration with our machine learning ecosystem teams, we enable contributions from the local communities. And I'm very excited to announce that this year, we are going to do a series of TensorFlow bootcamps in Latin America. Our goal is to really get closer to you, to the machine learning and the TensorFlow communities. So we look forward to staying connected with the communities in Colombia, Argentina, Chile and Mexico.  Documentation is also the backbone of an open source project. And so far,e have 268 tutorials and notebooks translated across 12 different languages. Additionally, we have over 40 translation pull requests that are still awaiting review. We really want to provide a good experience to our contributors.  And in order to achieve this goal, we'll continue to support translations of our documentation into several different languages.  Just last year, the Machine Learning GDs gave more than 100,000 talks and published 295 articles that reached out more than 500,000 developers. Our Machine Learning GDs, they play a critical role in advancing Machine Learning education worldwide. And if you'd like to be a part of the brilliant group, please, get in touch with us.  Just to highlight a few events where our Machine Learning GDs spoke at last year, we have a few pictures, as you can see here. And they referred to the African Buildup event, India Day and in Korea. For bigger projects in which we have to work as a team, we created the TensorFlow Special Interest Groups. This is a program that organizes contributors into focus streamed areas of work. These groups take ownership of specific areas to add and maintain new features to TensorFlow. It all started with the same build. And right now, we have 11 different SIGs.  The contributors are welcome to join the SIGs based on the parts of TensorFlow that they care the most. As I mentioned before, our ecosystem includes the SIG add-on s, build, networking, GVM and REST and Micro, community-led open source projected. Whereas Keras, Swift are Google led with an open source philosophy. Each individual SIG meets on a monthly basis. And if you want to be part of an SIG, you can head to GitHub for more information on how to join. You can also request a new SIG and we're here to help y and guide you through the process.  There are many ways you can contribute to an SIG, and these are just a few examples of specific projects that are looking for contributors in 20. A great way to get feedback on a large project is through the request for comments. It is the primary way we communicate our design rationale and receive feedback from the community. Anyone can create our comment on an RFC.  We have started Tweeting our RFCs, so please, keep an eye o at the TensorFlow Twitter account. We manage our RFC process on GitHub. And there, you can comment on an open RFC or propose a new RFC. If you want to propose a design, we strongly encourage you to socialize your idea, recruit the sponsor, write your RFC, respond to feedback and implement it.  And, if you are interested in learning more about TensorFlow, you may want to consider doing one of our fantastic online courses, either on Korcera or Udacity. Our university program includes new courses about TensorFlow at MIT and University of Hong Kong just to mention a few. And lastly, as announced this morning, we are launching the TensorFlow Certificate Program.  It is intended for individuals who want to advance the Deep Learning proficiency and want to demonstrate experience in practical business applications using TensorFlow. Head to TensorFlow.org-certificate for more information.  And then, lastly, please follow us on social media for the latest news and content about TensorFlow and the community. On Twitter, you can get the latest news about TensorFlow, head to the blog for more technical use cases and best practices. And then, our YouTube channel has more educational reurces that will help you through your journey. Get in touch and get involved with the community. Thank you so much.  Up next, we have Catherina and Miguel to talk about responsible AI. Hi, I'm Cat from the TensorFlow team, and I'm here to talk to you about responsible AI with TensorFlow. I'll focus on fairness for the first half of the presentation, and then, my colleague Miguel will end with privacy considerations. Today, I'm here to talk about three things. The first, an overview of ML Fairness. Why is it important? Why should we care? And how does it affect an ML system exactly?  Next, we'll walk through what I'll call throughout the presentation "a fairness workflow." Surprisingly, this isn't too different from what you're already familiar with. For example, debugging or model evaluation work flow. We'll see how fairness considerations can fit into each of the discrete steps.  Finally, introduce tools in the TensorFlow ecosystem such as faness indicators that can be used in the fairness workflow. It's a suite of tools that enables easy evaluation of commonly used fairness metric classifiers. Also integrates well with remediation libraries in order to mitigate bias found and is structured to help in your deployment decision with features such as Model Comparison.  We must acknowledge the humans are at the center of the technology design in addition to being impacted by it. And humans have always made product design decisions that are in line with the needs of everyone. Here's one example.  Quick Draw developed through the Google AI experiments program where people drew little pictures of shoes to train a model to recognize them. Most people drew shoes that look like the one on the top right. So as more people interacted with the game, the model stopped being able to recognize shoes like the shoe on the bottom. This is a social issue, first, which is then amplified by fundamental properties of ML, aggregation and using existing patterns to make decisions.  Minor repercussions in a faulty shoe classification product, perhaps, but another example that can have more serious consequences. Perspective API was released in 2017 to protect voices in online conversations by detecting and scoring toxic speech. After the initial release, users experimenting with the web interface found something interesting. The user tested two clearly nontoxic sentences that were essentially the same but with the identity term changed from straight to gay.  Only the sentence using gay perceived by the system as likely to be toxic with classification score of 0.86. This behavior not only constitutes a reputational harm, this can lead to the systematic silencing of voices from certain groups.  How did this happen? For most of you using TensorFlow, a typical machine learning workflow will look something like this. Human bias can enter into t system at any point in the ML pipeline from data collection and handling, to model training to deployment. In both of the cases mentioned above, bias primarily resulted from a lack of diverse training data.  In the first case, derse shoe forms, and in the second case, examples of comments containing "gay" that were not toxic. However, the causes and effects of bias are rarely isolated. It is important to evaluate for bias at each step.  You define the problem, the machine learning system will solve, you collect your data and prepare it, often times, checking, analyzing and validating it. You build your model and train it off the data you just prepared. And if you're applying ML to real world use case, you'll deploy it. And finally, iterate and improve your model as we'll see throughout the next few slides. P  The first question is, how can we do this? The answer, as I mentioned before, isn't that different from a general model-qualy workflow. The next few slides will highlight the touch points where it's important. Let's dive in.  Howo you define success in your model? Consider what your metrics in fairness specific metrics are actually measuri and how they relate to areas of product risk and failure. Similarly, the data sets you choose to revaluate on should be carefully selected and representative of target population of your model or product in order for the metrics to be meaningful.  Eve if your model is performing well at the stage, it's important to recognize that your work isn't done. Good, overall performance may obstruct poor performance on certain groups of data. Going back to an earlier example. Accuracy of classification f all shoes was high, but accuracy for women's shoes was unacceptably low. To address this, we'll go one level deeper.  By slicing your data and evaluating performance for each slice, you will be able to get a better sense of whether your model's performing equitably by using a diverse set of characteristics. What groups are most at risk? And how might the groups be represented in your data in terms of both identity attributes and proxy attributes? Now, you've evaluated your model. Are there slices that are performing significantly worse than overall or worse than other slices? How do we get intuition as to why the mistakes are happening? As we discussed, there are many possible sources of bias in a model from the underlying training data to the model and even in the evaluation mechanism itself.  Once the possible sources of bias have been identified, data and model repedouation methods can be applied to mitigate the bias.  Finally, we will make a deployment decision. How does this model compare to the current model? This is a highly iterative process. To iterate on models that aren't meeting the deployment threshold.  This may seem complicated, but there are a suite of tools in the TensorFlow ecosystem that make it easier to regularly evaluate and are remediate. Fairness indicators is a tool available TFX, TensorBoard that helps automate various steps of the workflow. This is an image of what the UI looks like as we as a code snippet detailing how it can be included in the configuration.  Fairness Indicators offers a suite of tools that allows a suite of commonly used metrics, such as false positive rate and false negative rate that come out of the box for the developers to use for model evaluation. In order to ensure responsible and informed use, the toolkit comes with 6 case studies that show how fairness indicators can be applied across use cases and problem domains and stages of the workflow.  By offering visuals by slice of data as well as confidence intervals, Fairness Indicators help you find outhich are underperforming with significance. Fairness Indicators works well with other tools in the ecosystem leveraging the unique capabilities creating an end-to-end experience. Data points can be easily loaded into the "what if" tool allowing to examine problematic data point points in detail. This data can be loaded into TensorFlow data validation to identify the effects on model performance.  This Dev Summit, we're expanding with remediation, easier deployments and more. We'll first focus on what we can do to improve once we've identified potential sources of bias in our model.  As we've alluded to previously, remediation come in two different flavors, databased or model-based. Involving collecting data, regenerating data, reweighting and rebalancing in order to make sure your dataset is more representative of the underlying distribution. However, it isn't always possible to get or to generate more data. And that's why we've investigated model-based approaches.  One of the approaches is adversarial training in which you penalize in which the extent in the model mitigating the notion that the ssitive attribute affects the outcome of the model.  Another methodology is demographic agnostic remediation in which the demographic attributes don't need to be specified in advance.  And finally, constraint-based optimization, we'll go into more detail in the next few slides in the case study we have released. Remediation, like evaluation, must be used with care. We aim to provide both the tools and the technical guidance to encouragteams to use this technology responsibly.  A large-scale dataset with more than 200,000 celebrity images with 40 binary attribute annotations, such as smiling, age, and headwear. I want to take a moment to recognize that binary attributes do not accurately reflect the full diversity of real attributes and is highly contingent on the annotations and annotators. In this case, we're using the dataset to detect a smile detection classifier and how it works for various age groups characterized as young and not yes, Your Honoring. TYoung. Bea with me for this example. We trained and unconstrained. You'll find out what unconstrained means in evaluated and visualized using fairness indicators. Not young has a significantly higher false positive rate. Well, what does this mean in practice? Imagine that you're at a birthday party and using this new smile detection camera that takes a photo wnever everyone in the photo frame is smiling. However you notice in every photo, your grandma isn't smiling, the camera falsely detected her smiles when they weren't actually there.  This doesn't seem like a good product experience, can we do something about this? TensorFlow Constrain Optimization is a technique released by the team here at Google. And here, we incorporate into our case study. TFX works by defining the subsets of interests. For example, here we look at the not young group represented by groups Tensor less than one. Next, we set the constraints on this group such that the false positive rate of this group is less than or equal to 5%. And then, we define the optimizer and train.  As you can see here, the constraint sequential model performs much better. We ensured that we picked a constraint where the overall rate is equalized for the unconstrained and constrained model such that we know that we're actually improving the mod as opposed to merely shifting the decision threshold. And this applies to accuracy, as well. Making sure that the accuracy in AUC has not gone down over time.  As you can see, the "not yog" has decreased, which is a huge improvement. You can see the false positive rate for young has gone up. And that shows, there are often tradeoffs in the decisions.  If you want to find out more about the case study, please see the demos that we will post online to the TF site. Finally, we want to figure out how to compare our models across different decision thresholds so we can help them in your deployment decision to make sure that you're launching the right model. Model comparison is a feature that we launched such that you can compare models side by side. In this example, which is the same example that we used before, we're comparing a CNN and SVM model for the smile detection exame. Model Comparison allows us to see that it outperforms SVM in a lower false positive rate across the different groups. And we can also do this across multiple thresholds, as well.  You can also see the tabular data and see that CNN outperforms SVM at the thresholds. Niche to remediation and model comparison, we launched Jupiter Notebook Support as well as metadata Colab with traces of root cause used in stored one artifacts, detecting which parts of the workflow might have contributed to the fairness disparity. We're releasing it here today so we can work with you to understand how it works withour needs and partner together to build a stronger suite of tools. Learn about fairness indicators here at TensorFlow.org landing page. And if bitly link is our landing page and not our TF.org landing page.  This is just the beginning, there are a lot of unanswered questions. For example, we didn't quite address, where do I get relevant features from if I want to slice my data by those features? And how do I get them in a privacy-preserving way? I'm going to pass it on to Miguel to discuss privacy tooling in TensorFlow in more detail. Thank you. Thank you, Cat. So today, I'm going to talk to you about Machine Learning and privacy. Before I start, let me ge you context. We are in the early days of Machine Learning and privacy. The field and intersection of Machine Learning and privacy has existed for a couple of years. And companies across the world are deploying models to be used by regular users.  Hundreds if not thousands of machine learning models are deployed into production every day. Yet, we have not ironed the privacy issues with these deployments. For this, we need you and we've got your back in TensorFlow. Let's walk through some of the privacy concerns and ways in which you can mitigate them.  First of all, as you all probably know, data is a key component of any machine learning model. Data is at the core of any aspect that's needed to train a machine learning model. However, I think one of the questions we should ask ourselves is, what are the con there are when we're building a machine learning system?  We can start by looking at the very basics. We're generally collecting data from an end device. Let's say, it's a cell phone. The first privacy question that comes up is who can see the information in the device? As a second ep, we need to send that information to the server. And there are two questions there.  While the data is transiting to the server, who has access to the network? And third, who can see the information on the server once it's been collected? Is it only reserved for Admins? Or can regular suites also access that data? And then, finally, when we deployed a model to the device, there's a question as to who can see the data that was used to train the mol. In a nutshell, if I were to summarize the concerns, I think I can summarize with those black boxes.  First, the first concern is how can we minimize data exposure? The second one is how can we make sure that we're only collecting what we actually ne. The third one is, how we make sure that the collection is only for the purposes tt we actually need.  Fourth, when we're Rhee leasing it to the world, are we releasing it only in aggregate? And are the models that we're releasing memorizing or not? One of the biggest motivations for privacy is some ongoing research that some of my colleagues have done here at Google.  A couple of years ago, they released this paper where they show how neural networks can have unintended memorization attacks.  So, for instance, let's imagine we're training a machine learning model to predict the next. We need text to train the machine learning model. But imagine that data or that core piece of text has, or potentially has sensitive information such as social security numbers, credit card numbers or others. What the paper describes is method in which we can prove how much is the, what's the propensity that the model will actually memorize some data?  I really recommend you to read it. And I think that one of the interesting aspects is that we're still in the very early days of this field. The research that I showed you is very good for neural networks, but there's the ongoing questions around classification models.  We're currently exploring more attacks against machine learning models that can be used by developers like you. And we hope to update you on that soon. So how can you get started? What are the steps you can tak to do privacy Machine Learning in a privacy preserving way?  Well, one of the techniques that we used is differential privacy. And I'll walk you through what that means. You can look at the image there and imagine that image is the collection of data that we've collected from a user.  Now, let's zoom into one specific corner, the blue square you see down the. So assume that we're training on the integral data I'm zoomingin. If we train it off privacy, we'll train with that piece of data. However, we can be clever the way that we train a model. And what we could do, for instance, is just let's flip its bit with a 25% probability.  One of the biggest concerns that people have when doing this approach is that it naturally introduces noise, and people have questions as to what's the performance of the resulti model?  Well, I think one of the interesting things from the image is even after filtering our 25% of the bits, the image is still there. And that's kind of the big idea around differential privacy, which is what powers TensorFlow privacy. As I said, TensorFlow Privacy is the notion of privacy that protects the presence or absence of a user in a dataset and allows us to train models within a privacy preserving way.  We released last year TensorFlow Privacy which you can check at our GitHub repository at GitHub.com/TensorFlow/privacy.  However, I want to talk to you also about some tradeoffs. Training with privacy might reduce the accuracy of the models and increase training time, sometimes, exponentially. Furthermore, and I think more worringly and tied to what Cat's talk. If a model is already biased, differential pricing might make thing even worse, as in more biased.  However, I do want to encourage you to try to differential privacy because it's one of the few ways we have to do privacy on ML. The second one is fed rerated learning. TensorFlow FET Federated is an approach where shared global model is trained across many participating clients that keep their training data locally. It allows you to train a model without ever collecting the real data. Therefore, reducing privacy concerns and you can also check it out at the GitHub repository.  This is kind of what I was thinking with our imagining about TensorFlow Federated Learning. The idea is that devices generate a lot of data all the time. Phones, IoT devices, et cetera. Traditional ML requires us to centralize all of that data in a server and then train the models.  One of the really cool aspects of Federated Learning is that each device runs locally only. And the outputs are aggregated to create improved models, type of setting or allowing the orchestrater noto see any private user data.  TensorFlow Federated as I was recap allows you to train models without collecting the real data. If you remember the first slide I showed, it really protects the data at the very edge.  In terms of next steps, we would really want you to reach out to TF-privacy. We would love to partner with you to build responsible AI cases with Privacy. As I said earlier, we're in this together. We are still learning. The research is ongoing. And we want to learn more from your use cases. I hope that from the paper I show you, you have the sense that keeping user data private is super important. But I think most importantly, ts is not trivial. The tradeoffs in Machine Learning and Privacy are real. And we need to work together to find what the right balance is.  And last, is Masoud to tell you more about TensorFlow Quantum. Good evening, everyone, I'm Masoud Mohseni. I'd like to thank for the presentation and making this livestream presentation happening today. Today, I'm going to announce the launch of TensorFlow Quantum, which is an open source framework for Quantum Machine Learning. I'd like to mention that in this talk, I do not assume any background in Quantum physics or quantum computing.  I would like to start with a famous quote from Richard -- on computational power of quantum systems. He said that if you want to make a simulation of nature, you better make it quantum mechanical. So, of course, we should know that machine learning is not about exactly simulating systems. But has the ability to predict system's behavior. I'd like to rephrase the quote. As natural is unclassical, if you want to make a model of nature or learn the model of nature, you better make it quantum mechanical.  So with that in mind, we developed TensorFlow Quantum, basically as a toolbox to model nature's artificial antum systems. What are the main objectives for TF Quantum? The first one is a software framework for hybrid quantum machine learning. And it allows researchers to perform training and testing of quantum for quantum data.  This can reduce the prototype. Also, it can help machine learning and quantum computing researcher to discover new algorithms for realtime devices or ultimately for quantum computers once they become available.  So for people who have no background in Quantum Physics, I'd like to make anntroduction to quantum mechanics, quantum computing and quantum machine learning in just one slide. So are you ready? So this is the setting that we want to discuss. This is known as experiment, and it's at the heart of the quantum mechanics. Let's consider that you have a ball machine. If you play tennis, you know you can set it up to send random balls. You have another wall or screen. You can detect where the balls are hitting. What is your expectation that balls can hit the screen. And you see a bump, corresponding to that. There's another one if they go through the bottom. And proper distribution is just overall to events. That's all you need to do. So what happens if the source for the particles is quantum mechanical. I assume if you have electron that sends electrons or if you have a laser to send photos, these are the particle of light, and you have setting obviously made smaller. Now, you can see an interference pattern emerging that is very bizarre. There's zo chance of getting a particle. This was very positive for our physicist more than a hundred years.  Until they realize, actually, there's a quick fix to this. You can do generalize the classical particle distribions to quantum, and for that, you just attribute an amplitude and then, that's a complex number. And so, for events, you have to add them up and then score them. And then, you score, there's a term appearing, the properties of getting through the top or bottom. There's interference there.  And this is related to the relative phase between this event. And you can manipulate this and by manipulation of that, you can do computation. That's the core idea of the quantum computing. And how do you do that? You place a stack of the walls at various different distances and the slits at various locations, and set it up to property distribution.  So you create a constructive interference pattern to generate distributions the desired outcome.  Now, that's basically is quantum computing. So congratulations, you've leaed how quantum computation works. Let's look at a specific example that's interesting. If you have random walls separating different places and random states on located in the walls, you get fancy property distribution meaning you have basically quantum system. And this is distribution of the system is exponentially classically. And this was at the whole of the experiment we announced last year. The setting already. That particular setting is not Scascaleable but that's technical detail. All I wt to say is you can basically set up a quantum machine learning on this. How do we do that? We parameterize this arrangement.  So we allocate or attribute parameters to location and then try to learn to generate desired output property distribution function. Or to create a set of solutions that are the correct solution to the computation problem. And that's at the heart of the quantum machine learning. So you just basically learn all of the concepts I want to introduce in this talk, except, you know, this setting is not a scaleable, so the rest of the talk is about scaleable implementation and how we can over TensorFlow. So to push analogy further, you have to think about generating or engineering the interference patterns or generating soundwaves. Like, you are composing or playing music. So there are certain strings and strings becomes cubits. You have quantum notes illustrated here. And in some of the musical instruments, you have electronics where you can generate electrical waves that generates this prorty distributions that correspond to the very nature of the particles.  So we did this at Google. Over the past five, six years, we have developed these technologies that you can fabricate the cubits of the scale. And this is a particular picture of arrangement. It could be a ring. And there's a pair of electrons tt is acquired in that cubit. That is counterclockwise, b it can be super position of both direction and goes in both ways. And this is like going through, the particle going through slits at the same time. And you can control it with the electrical gates. And this has to be low enough temperature for the super position to happen that's below a temperature of superconductivity.  So we use that device with a 53-cubit processor to implement a random, set of random quantum gates and showing it's very hard to sample them classically.  So let's back up a little bit and see what are these quantum bits? They can basically place and present them. Basically, if you consider this, if it's a classical bit, it's north pole or south pole. For quantum bit, it can be any point on this globe. So there are certain parts of classical bits thatlways have a value of zero or one, can't be copied. Measuring one bit doesn't aect other unmeasured bits. And none of them, actually, general works for quantum bits. The challenge is how we can manipulate quantum information and how we can do quantum computation. In order to do that, you can build by rotations around axises. And this can make anywhere in that globe, basically. That means that the first bit is one, the second one is flipped.  And quantum, understanding quantum as composing music is different strings correspond to cubits so they're placed in space and in time, the kind of things, operations you do through additional cubit rotation. At the end, you measure them through classical property distributions. And that's also means for each bit is one or zero, like classical bits.  The things that the outcomes are normalized so they sum to one. So what is the landscape compared to this kind of technology? So basically, right now if to quantify the computational power of this system, you have to consider two index, one is the number of cubits, and one is the quality of the cubits. To have maximal computational power, you have to have high-quality and a lot of them. In the shaded purple area, involving the dashline, basically things are not interesting because although you have a lot ofcubits, they can be simulated classically because of the state of the quantum cubits is localized. It does not have the power built into it. Below the dash line, we observe the but to have quantum computer millions of qubits to use to basically decode cryptosystem and factoring large numbers to prime factors. But thas kind of one of the extreme persons. In near term quantum, you can think about under tens of thousands of qubits the essential opportunities to do something interesting like quantum machine learning that cannot be classically done, especially when you want to analyze the quantum data. And this is where TensorFlow Quantum comes in. As a toolbox in this area that was not available and we are going to anticipate.  So how can we ensure this has quantum circuits? So here to see a particular analogy, you have to notic that when you print this kind of unitary operations or random rotations in a space time volum that you have, this is kind of parameterized rotation mimic kind of classical like deep neural networks.  So that's the reason for quantum neural networks that you have. But how do we create the quantum parameter circuits? Let's look at a concrete example. We use a toolbox introduced by the team for the simulation of quantum circuits.  So we import and next, write a parameterized quantum circuit for this particular processor on the right. Which is basically, here in the setting is acting as a quantum computing language. We will use sim point for symbolic parameters. Using the qubit command placing a qubit on the corner here. This command can take coordinates of qubits. We place it on the corner of 0-0 coordinate. And we can try to build a simple model. What would be the simple model? It's a parameterized quantum using one gate.  And then, just adjust rotation by an angle about some Y axis of the qubit. To simulate the circuit, we actually need to attribute values to the symbol data. This can be done by sic parameter. And we pass similar to object obtained that can obtain the result in output vector. So output is a vector that just spit it out.  Now, all we need to do to get the classical values from this wave function is to define a measurement. Here, we choose a standard operator which measures if the qubit is 0,1. The value is obtained by expectation from wave function command on the simulator output.  How can we do Machine Learning on this finite space times that we have available? So that was interaction to just set up a parameterized model. But you have to notice for quantum machine learning, this should upload the data, basically, we have to prepare it on the fly. This is the first part. You can see that first part is, like, basically generating the dataen O the fly and the second one, the parameter to try to land it using outcomes. possible sources of quantum the data? The output of a quantum, you can use quantum machine learning for verification, over outputs and discovering add-ons. In simulations of quantum systems. In particular, when you have quantum matter near a quantum critical points, the entanglement of the correlation diverges exponentially and becomes very hard to simulate them at that point.  So what are the other cases? Quantum communication networks, basically, in quantum contributors, quantum receivers, computational. When you have computational networks, the networks are noisy and you have to distill and improve it, improve the quality. And you can machine learn that. You can imp, high-performance measurement. Or also, cool devices, this is important for mitigating errors. The simplest possible case for quantum is for a single qubit. Blue and orange corrpond to two vectors pointing in two different directions, but the things could be noisy, so when you have a dataset, it could be any point near to that direction but not exactly there. So the question is, if someone gives you that data, can you do a valuable classification? And this is one of the examples I'm going to work to today.  So as a concrete algorithm is variational algorithms. Basically, here, we have two differencore processor, CPU and GPU that talk to each other. What happens, you collect some classical distribion of data and just pass it to CPU to come up with better suggestion for the parameters. And you do this through converge. And this is, basically, the iterative quantum classification. But why not just command quantum classical with hybrid models? Here, you use the best of both models to reach the, develop representation to capture some unknown data.  And also, you can improve genera generalization. How do we do that? We know that TensorFlow is one of the most used machine learning platform and also one of the wider used simulator for quantum circuit. It can be combined and what are the challenges for that?  Once you start looking into this problem, you realize there are certain technical hurdles. For example, quantum data cannot be imported. You have to generate it on the fly. And also, you need to have both data and the models layers in this quantum circuits and you have to, basically, have it ready to be at this highly computational graph that are dynamic.  So the other thing is you need to pass this to a quantum program to one time. You cannot just do measurements and just pass it every time there's a delay, there is a latency between QPU and CPU. Because QPU works at the few microseconds, including measurement. And the other issues with sending batches of jobs to realize many paralyzed quantum circuits. And considering all of the hurthosee have to hybrid quantification if you want to build hybrid models. And remain that we have to be able to generate quantum on the fly and train so many quantum circuiting power.  Also, there's an issue we need to be backend agnostic, but means we should be able to search from a simulator to a real device with few changes. Especially, so we can try a lot and compare their simulation against an actual hardware real output. So at the final principle, the minimalistic nature of aPROECH is approach is we want to build a bridge to TF. Not just to reinvent the wheel. So we don't want to force the developer to learn new toolbox that is different, so much different from TF.  So with that, we are arriving at the software archi architecture. It's very simple, we map everything to Tensors, and then, we use this kind of ops on TensorFlow framework to calculate classical property distributions for the output.  So, basically, here, you can see an example that circuit can become the Tensor, parameterize describe the quantum circuits, the Tensor or polyoperaters Tensors. And also, we can have from some note getting suggestion for the parameter's value to become the operations that you calculate the expectations. And then, you can feed it forward to other nodes in the computational layer.  So with that, we can look at the software stack as a whole. So at the user interface, there are two sets of data. Data could be classical quantum, so the TF Quantum support quantum data. But the interesting thing, quantum data, as I said is, like, circuit or operators. But you can map everything to TensorFlow and use standard TF layer, TF Keras high-level layers that you can manipulate them.  Below that, you can see the TFQ layers and differentiators. And below that is the TFQ ops, they can run a simulator or this highly performing simulator we launched earlier this week. Or you can run it on actual hardware, which is QPU, the classical part of the model can be run on QPU, CPU or GPU as a standard TF, basically, framework allows you to do.  So for particular concrete suggestion, to understand why this framework works is like looking at the pipeline for a hybrid discriminatory model. You can see the interaction between different parts, like quantum data preparation, evaluation of quantum models. And once you measure it, the data becomes classical, then you can feed it forward for post processing by classical neural network. And then, you can measure cross and cross function, feed it back to improve the parameter. You can improve the parameter of both quantum model and the classical neural network. And this is basically the nature of the hybrid model.  With at, we are now ready to do examples. This notion of many worlds comes from an interpretati of quantum mechanics known as many mode interpretation. And I have not necessarily identifying with that. But it is kind of an interesting way of looking at the simplest possible case for analysis of quantum data would be a binary classification for a single qubit. So how can we do that? Hour, as before, we place it on the grid. We also import non-Py TensorFlow packages. The first to do is setting up the labels and parameters for preparation of quantum data.  And then, then, simiplicity here, we classify two data points A and B classify to vectors. And then, we can convert this vectors to tensors. And with that, we can have, we already generated the quantum data. And now, we need to build our hybrid classifier. And in order to do that, we apply core TFQ functionality. And that enables us to have an easy integration with TensorFlow.  So inputs the hybrid model can be specified using standard cross input object. The cross-portional model begins with parameterized rotation of the Y axis and then, on hybrid model will land discriminate to the states to optimize the data parameter. To calculate a loss function, to, basically, land this, we need to convert the quantum states into classical information. We can do this by allowing measurement along the axis and embed the quantum model and measurement into a cross model. We use the TFQ PQC later. And this layer from the TF cross-layer, so it's compatible with Keras API.  For the last stage of the classifier, we attach two, basically, unit dense layer with soft activation. And then, the layers are ready to basically use as automation to predict the properties that the input data point is either from class A or class B. To train the model, we wrap classical and quantum layers in a Keras model. The classical layer and the parameters in the quantum models have both updated using Optimizer, the categorical cross referential. It's driven toward zero during the training.  Now, we can apply this classifier that we built for distribution of the quantum data. So we can give this model quantum data points, upon training data points to see if this model can generalize. The actual example was a bit artificial in the sense of fixed data point to the fixed direction.  Now, using the standard Keras model, we can predict for function and see that the noisy samples are categorized with enough accuracy. So, basically, this is has been very long journey, and we had a big team to be able to bring this up to you. And we would like to thank our colleague in our team and in particular, the quantum team we had contribution fm X and particularly the university, I want to mention a significant contribution from Michael Antonio over the past years. And we're excited to see what we can do with this and welcome your contributions.  So you can dig deepery going to the website TensorFlow.org/quantum, also, you can look at our archive paper, which provides detailed overview of the entire framework and also a lot of quantum applications that you can work the details in the Colab that are available in the TF Quantum website. Thanks a lot for your attention. Thank you, Masoud and thank you everyone for joining us. With that, this is the end of all of the presentations and demos from today. A big thank you to all of our speakers and staff and TensorFlow team and more importantly, all of you for joining us for this very first virtual livestream only TensorFlow Dev Summit. We are going to make all of our presentations and demos available online on YouTube channel. So also, don't forget to check out our blog. And with that, keep those tensors flowing. And I want to take a short break and, basically, show you some highlights from today. Thank you. 