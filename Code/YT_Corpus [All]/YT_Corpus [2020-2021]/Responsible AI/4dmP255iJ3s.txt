 for being here let's get right to it we're gonna talk about responsible AI with a couple of folks that we have with us right now we have Eric Boyd corporate vice president of Microsoft Azure AI let me get that right as your AI we have Rob Moss Eddie Mel sorry distribution ear at cosine data here at Microsoft and marilaine partner director ENS here at Microsoft let's start with you Eric what are some of the new things that people should be looking for an azure machine learning that they should be excited about yeah thanks for asking that Seth I mean we've done a lot of work over the last you know couple years really working with a lot of our customers and one of the things we're finding is that as AI is moving out of the lab and into mainstream production a lot more developers are needing help building AI responsibly and they're looking for trusted tools and open-source tool kits and things that will really help them so we've been investing in offering tool kits that are easy to use and offer native integration with Azure machine learning and they're just really seamless we focused really on three key priorities we really want to help customers understand protect and control their data and models and so in the understand front what we've done is we've offered capability is powered by interpret ml toolkit this is an open-source toolkit that we developed with Microsoft Research an integrated deeply into Azure machine learning and what it does it is helps developers understand why their models are predicting various things the next is the fair learn toolkit which also open source deeply integrated into Azure machine learning and helps developers understand how to make sure their models are making fair predictions and the third thing we announced is a differential privacy toolkit called white noise and it enables customers to build machine learning models using sensitive data while safeguarding the privacy of individuals and all these capabilities can be accessed through our machine learning and they're also available an open source on github now I love the the whole responsible AI you know pivot and I love the the understand protect and control let's dive into the understand so you mentioned a lot of products but why don't you dive into each one of the things that are in Azure machine learning what they do and why they're important at a service level then we're gonna get Rob to talk about how it works at the Church level yeah I mean what we're hearing from our customers is that they're really struggling to build responsible AI so we want to get these tools into the hands of developers as quickly as possible and we see a lot of need but we know also the area's not all that mature and so we're really innovating as we go and that's why we decided to put the core IP into open source to get the latest research in and iterate faster and really enable people to use it anywhere and so you know one of the first things is sort of the explained ability side I want to be able to tell a user an executive a regulator why is my model making the prediction that it's making and that's how interpret ml really helps you and then fair learn it really helps you you know why is my model predicting is it being fair and all the different types of decisions that it's making and and and so you know we can sort of talk through that a little bit more but that's kind of how it's set up and then the last is white noise using differential privacy it's really a complicated technology but really bringing an injecting noise into the data to make it so that you can bring data together and keep it nice and protected so do you have a demo for us to show us like how this stuff kind of works sure I mean maybe I can share with you the the fair learn toolkit and so here what I will you're seeing here on my screen is I'm in my Azure machine learning you know homepage here and I've got all the different work that I've done and I've run this experiment previously so I'll go to my experiments tab and load up my previously run experiment and here you can see that I've done a whole bunch of different runs on this particular experiment but I'll go into the particular run and this is a great thing about our machine learning is it gives you all the history the details and everything and so I can then you know go into the snapshot of what I've done and actually see you know the exact jupiter notebook so let me tell you what I was doing I was building a model where I was trying to predict whether or not I should offer a loan to someone based on historical data and lots of different features from that and this is an exact situation that we used with Arnstein young as we were prototyping and learning about the fair learned toolkit and you know since I'm a pretty good data scientist I want to make sure when I'm all not biased so I'm gonna go through and drop the sex and race categories to columns just to make sure that my model doesn't learn anything based on those but the challenge that you run into is when you go and then build and look at your model is if I later use I can use fair learn to go back and evaluate hey how did that actually perform against those different categories the things that I didn't want it to learn about and so by integrating fair learn you can see we've got the fairness directly into my run history here and so as I go and click on the fairness there what you can see is this dashboard this fair learn which is integrated directly into your Jupiter notebook and also directly integrated into a machine learning and I can say hey I want to look at the sex and see how that affected the accuracy of my different models and so what I have here is a graph where on one axis the y-axis I can see how much it's weighting sex as a factor but on the x-axis I can see overall how accurate it is so this first model that I trained the most accurate one you can see it's 84 percent accurate but if you look at it you can see that for men 25 percent of them basically get approved whereas it for women it's only 7 percent and so that's not the outcome that I'm looking for even though that's not something that the model was trained on so the next thing that I did is I did a grid search looking to vary some of the parameters in it and that's what I'm looking at here is all of the different models that I trained and so I can look for this one here and here I can see the accuracy is 82% which is very close to the 84% I have in the previous one but here as I look at the disparity in predictions male approvals at fifteen percent females at fourteen point four I'm much closer and much less much more fair in using that approach and so this toolkits really powerful for people to give them you know really all the controls that they need to look at their models and the work that they're doing and really make sure that they're making good and fair predictions and and that's really cool because data scientists are like if I just remove those columns then maybe I have a fairer model but those don't work drop it but still picked it up through other methods and so that's why you have to go back look and make sure you're doing it fairly so essentially the features leaked those features you know gender and and the other one our gender and race they leaked in through the other maybe by address or by something else and you can't just take things out and expect your things to be fair is that right that's right exactly well that was awesome Rob let's get into the research behind some of this stuff let's talk let's talk about what you do and then let's get into some of the some of the stuff that you've been researching sure thanks Seth so you know I lead the cosine data organization which is a data science and engineering organization building core operating systems products from Microsoft and that includes Windows the operating systems that run in our cloud and so you know research that we've done builds on some of the things that they're talked about they'll talk a little bit about interpretability for example you know the world is depending more and more on AI and machine learning systems for prediction and decision support it's true in many industries beyond tech including finance healthcare manufacturing agriculture and more those decisions and actions have higher stakes and it becomes important that we can explain the reasons behind the decisions to meet our commitments to fairness like Eric talked about also reliability safety accountability privacy and security one of the things I work on is Windows and in that case we're using machine learning to help assess the quality of the product and improve customer experiences so for example with Windows updates we train and use machine learning models to predict the impact of updates on system reliability and performance and that might determine how quickly we update a customer's given device we need to understand why behind these predictions to help engineers prioritize the work and determine best course of action to take this is where interpretability tools like interpret ml come in understanding the features that impact the model both globally and also for individual predictions and classifications important a training time to build trust and for debugging and also in production to understand how the models performing making decisions in the wild an interpretability can help with detecting fairness issues meeting regulatory compliance and providing transparency in these applications can you give us an example of something that interpretability would flag for you as a data scientist that would tell you to maybe go back and redo some of your model your model work or maybe even feature work sure well the key with interpret a package like interpret ml is it lets you bring together a variety of machine learning interpretability techniques both the ability to train with glass box models things that are transparent where you can see what's going on as well as to explain blackbox systems and so in this way you can get a look into which features are having an impact on a given model whether through feature importance explaining explorations or individual conditional expectations it integrates best-of-breed methods like lime and schapp as well as surrogate type models to mimic the black box and so forth so basically what you're doing is you're are you test like for example I understand like like glass box models because I can look at a decision tree and know what's happening basically but we're looking at models such as like neural networks how does something like interpret ml work around those kinds of black box models well we don't know what's actually happening to the features as they're going through the network yeah and this is where these methods like lime and schapp are used to basically to mimic the model testing different inputs and looking at the outputs and identifying which features are having the most influence over the outcome both globally and for an individual test case and that helps the data scientist or machine learning engineer to debug what's happening in there so let's talk about another thing and I've always look I'm a data scientist at heart as well and one of the things that I always struggle with is data because it's it's a very personal thing this data and we don't want data to leak out and we don't want people to learn stuff about people that they shouldn't for example doing testing on medical data is very you know sensitive talk about white noise and how that actually helps protect that kind of thing Rob sure so you know white noise project is set out to connect the theoretical work from the research community community with practical lessons that we've learned from real-world deployments and specifically applying differential privacy in a way that's more accessible for everyone so like let me offer an example one of the common things we do with data as data scientists is build dashboards and reports to help leaders keep track of things like roll up metrics count sums averages descriptive statistics you know typically we aggregate data into large partitions so any individuals data isn't revealed but there are various reification and restructuring reconstructions in the literature that can operate on aggregates using differential privacy it takes the guesswork out of the protection we add a small amount of statistical noise that has a negligible impact on overall accuracy of the aggregate but we get to protect the individual data from any any types of those kinds of attacks one of the questions I have about that is are we actually flipping actual data points and then are we releasing the new noised dataset or are we looking at flipping the data points and then revealing the aggregates it do you see what I'm asking well the differential privacy generally works in in two different ways there's something called local differential privacy where the statistical noise is introduced that the individual data point level in global differential privacy is done more at the aggregate level the white-noise project delivers the global differential privacy so you don't have to treat your your data at collection time you can treat it at query time and the platform contains really three key parts to enable that the core of white noise is an open source library of differential privacy algorithms and implementations that come from mature research as well as api's for assessing things like privacy laws the second part is an analysis graph for maintaining fine-grained control over the privacy budget because when you introduce noise you know the more queries you do across that data set the more risk to to attacking the budget is introduced and then the third component is the interface the systems interface with data and that allows you to do sequel like queries across common sources like Postgres sequel sequel server spark presto CSV files and so forth so that you mentioned a concept this notion of a budget which is kind of interesting because I mean if you introduce noise at a statistical level you can subtract that noise out given enough queries tell us what it means about this budget Rob yeah so when you set up the system you know you can you can basically determine what the budget is going to be and then you can monitor and maintain that budget and limit the number of queries that are happening over time to prevent you know the ability to reify the data well this has been really cool but now we're gonna move on to mural lane partner director ENS at Microsoft what is what is ENS actually mean why don't you describe what you do my friend here said ens stands for an ethics and society and my team cares about the effects of young people were a multidisciplinary group of product experts designers program managers engineers ethicist philosophers data scientists and we're vetted in engineering and we work with teams to guide our innovation towards ethical responsible and sustainable outcomes and yes there's a team like this in Microsoft it's kind of amazing that is actually really cool how how is responsible a I making everything our products our company better let's start there yeah well we think about ethics is innovation material people care about privacy they care about security they care that what you build works for everyone that it's fair so there is a way to innovate in this space by thinking through the customer demands of responsibility along with the impacts of what you're building because we all recognize that our AI models are often part of a larger product offering an ecosystem so there's a lot of ways that this innovation might manifest it could include innovations in data collection methodologies or leveraging synthetics for model use and model training this might also show up that as new controls or tools that you build to guard against misuse or unintended consequences of your tech it might also include more robust consent mechanisms or deeper thinking about different audiences like children and how they might use what you're building in a safe way and so when we think about better products we think of them as being more robust that they're not limited by influences such as race gender age or even changes to environmental conditions how is thinking about these things making products at Microsoft better or just Microsoft better in general well you know late last year our team announced a new speech service capability the ability to create branded voices using deep neural networks and while there's you know beneficial applications to synthetic speech we also recognized the potential for misuse and harm you can easily imagine the impact of politically-charged of messages or hate speech and attacks on individuals and institutions through fake audio that's ascribed to real known people but this also created a new it vector as well for malicious actors to spoof a voice with synthetic audio to gain access so this was definitely considered a thorny piece of technology and what we did was we embarked on an extensive exercise to examine the benefits and whether we could create and release this in accordance to our AI principles we did this by using some of our practices around bringing in diverse stakeholders such as voice actors so we can understand the impact to their jobs by creating meaningful consent and transparent consent mechanisms and importantly anticipating the harm and unintended consequences consequences these are some of the practices that we use as part of our development of AI technologies and you know to summarize we did release this technology but we limited availability through a gated service for a select set of custom trusted customers that's that's really amazing that word that you know involved with that and all and and what's the word I'm looking for that we we want to make sure that we're doing things responsibly our other customers seeing the benefits of this as well absolutely so you know customers large and small they're all looking for guidance on responsible AI and they're holding us accountable to build responsible technology for them and even with them so an example was an important factor for the BBC working with Microsoft and this was because of our mutual approach to responsible AI and you know the BBC is one of the most trusted news organizations in the world customer data privacy is huge for them it was a huge priority for them choosing a strong partner to help it build a voice assistant to deliver content to a diverse and underserved population so a big motivating goal was reaching a broader more global audience and many of those were preliterate so voice offers a unique delivery and interaction experience and allows people to access news and information and for the BBC you know this block broad and global reach means taking an inclusive approach to data collection and conversational design so it works for everyone you know seeing firsthand how Microsoft had developed these practices for anticipating and mitigating harms - how we bring in diverse stakeholders and how we think about technology was a key factor in the skill operation well this is awesome let's let's take a forward look now we'll start with you Eric what are we looking to do in the future with the services and products we offer for a I own a sure I mean there's so much more that we can do in this space you know particularly focusing on responsible their products right work that we've done around confidential machine learning we're working with Microsoft research to see are there ways that we can use special Hardware on clays protected regions within the chips to be able to do machine learning they're where the data gets you know it is encrypted everywhere except for this small place also we're looking at things like homomorphic encryption this is a technology where you can actually do machine learning on encrypted data it's very interesting of you know research areas extremely computationally expensive but it's a you know just provides complete encryption all the way through the machine learning process so a lot of things that we're looking at there we're also looking at innovation to really help the end to end machine learning process everything from you know how do you control your data and make sure that you know which models were built with which data manage you know governance capabilities and things like that things that you know sound a little bit dry and boring but as you're actually trying to use this stuff day in and day out is super important so a whole bunch of new things coming down you know further down the line we're looking at you know robustness and adversarial machine learning is an area we'd like to explore we see a lot more in reinforcement learning coming a whole bunch of new areas that we're looking into it's a really broad and fast-moving space this is cool Rob I want to talk to you what's the future of research in responsible AI and machine learning yeah well you know looking ahead we expect things like Eric talked about interpretability fairness privacy preserving technologies are going to be more important than ever as the stakes for these systems and individuals data and associated risk just continue to go up you know we're seeing increased adoption of differential privacy and I talked about that at Microsoft across the industry but there's more room for improving the ease of application and integration are tools I think security is another important frontier machine learning systems can be vulnerable to specific kinds of attacks things like data poisoning which is aimed at corrupting the predictive system by contaminating data and the training phase these require new kinds of threat detection and mitigation at Microsoft we've been updating our security processes and our documentation to include these emerging threats but that's just a start Microsoft's a platform company and along with our research team the hope is to build more these Aria responsible and I capabilities that we talked about into more of our software and data platforms so the customers who build solutions also benefit and can better protect their customers privacy that's really cool Emira we have about two minutes left I want to make sure that people get a sense for how important it is to start thinking about this now and what are some easy things organizations can do to implement AI responsibly this is a great question so we should start by taking the human centered approach and what this means is it's about understanding stakeholders and the value tensions among them and when I say stakeholders I mean people that will directly use your system those who will indirectly interact with it special populations like children who might be excluded from the design of your system and also thinking about you and project sponsors whose views and values are unconsciously encoded into what you build and and next you know we should exercise this muscle learning how to look around corners and anticipate potential harms and impact for example you know how might your system reinforce stereotypes or biases or and I might the system deny someone access to critical services and so as we start thinking about those the next exercise is learning how to mitigate those potential harms so you know as part of Bill releasing some mechanisms best practices things we've learned through the development of AI technology for bringing in diverse voices as well as performing harms modeling for your systems and when you do harms modeling you think about this in the same way you would think about security threat modeling so learning about you know anticipating harms thinking about harm in a really robust way and you know this is the future of innovation this is how we expect modern product development to occur well this is really cool we've got about 30 seconds left Eric where can people go to find out more about what we're doing on Azure with machine learning and AI I mean you can always go to Azure machine learning on the badger website on Azure comm and just learn all about the different offerings that we have there and so you know there's a whole bunch of things that we're adding and innovating and doing each and every day and so really look forward to working with everyone as they continue to work on this awesome well this has been amazing thank you so much for spending a little bit of time with us we're going to send you off to one of our digital breakouts c-sharp today and tomorrow with Matz torgussen remember to register for exclusive access to all of the concurrent breakouts happening right now back on the live desk after that stay with us 