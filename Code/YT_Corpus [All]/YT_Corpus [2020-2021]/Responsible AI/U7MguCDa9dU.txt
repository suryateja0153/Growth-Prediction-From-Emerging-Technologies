 hi everyone I'm Sara bird from Azure AI and today I'm gonna be telling you about some of our responsible AI capabilities on am sure when we think about developing AI responsibly there's many different activities that we need to think about and it's important that we have tools and technologies that help support us in each of these so in Azure we're developing a range of different tools to support us on this journey and so in this session I'm going to be talking about technologies that help protect people preserve privacy and enable you to do machine learning confidentially however we also have capabilities around gaining a deeper understanding of your model through techniques like interpretability and fairness and for adding new capabilities to our platforms as well as recommending practices that allow you to have complete control over your end-to-end machine learning process to make it reproducible repeatable auditable and a process that has the right human oversight built-in so a lot of these responsible AI capabilities are are new and they are actively being developed by the research community and in practice however at the same time we're already creating AI and we're already running into many of the different challenges in practice as we do that and so we felt that it was essential that we get tools and technologies into the hands of practitioners as soon as possible even if the the state-of-the-art is still evolving and so in order to do this we have been developing a lot of our capabilities as open-source libraries because this enables us to directly code develop with the research community it enables people to easily extend the capability make it work for for their platform and it allows us also to just iterate more rapidly and more transparently however and 10 machine learning is often best supported you know particularly in production with a platform that helps you track the process helps make it reproducible and so in order to make it easier for people to use these capabilities in that end-to-end process we integrate them into Azure machine learning so they can just be part of your machine learning lifecycle so if we want to dive in and talk about today's topic which is how do we really protect people and the data that it represents them one of the big questions that we need to think about is how do I protect privacy while using data and you might immediately think that you know you have great answers to this and of course the you know the simplest thing we can think about doing is making sure that we have excellent access control and that we don't have unnecessary access to the data set and of course the next step to beyond that is we can think about you know actually anonymizing values in the data so that for the people that can look at the data set right they they still can't see that private information about individuals however it turns out that these are important steps but they're not enough even when we use data being the output of that computation the the model that we've built or the statistics can actually still end up revealing private information about individuals so if we think of a machine learning an example of this I could have a case where I'm building a machine learning model that helps autocomplete email as sentences and in the model there could be some very in the day said there could be some very rare cases I have sentences like my social security number is and so when I typed that as a user it might be that then the model sees that it lines it up with that in a singular or small number of examples and it actually autocompletes my sentence with a social security number from the data set which could be a significant privacy violation and so there's cases where models for example can memorize individual data points and we definitely need to consider that but even if we think you know okay that that's you know a specific problem to machine learning there's also challenges when we look specifically at statistics and aggregate information overall and so let me jump over to a demo to show you what I need here so in this case I am going to be demoing in Azure data bricks but you could be using a Jupiter notebook or your Python environment of choice and what I'm gonna do here is I'm actually going to demonstrate how I can take a data set here and in this case my data set is for loan scenario so I'm going to be looking at people's income and trying to use that as a feature to decide whether or not to offer them alone and what I want to demonstrate is that we can actually reconstruct a lot of the the private information and the underlying data set just from the the aggregate information that we were using so in this case here I'm going to assume that I know the aggregate distribution of incomes for different individuals as represented by this chart here and then what I want to want to show is that I can actually take this information and if I combine it with a little bit of additional information about two individuals so in this case these values here then I can take an off-the-shelf Sat solver so here I'm using a z3 which you can just pip install and I'm going to use that sin solver to actually reconstruct a data set that's consistent with the published information that we know so if I run this Sat solver then what we're going to see is that we actually are able to reconstruct a data set that is consistent with all of the aggregate information as well as the individual information that we know and in this case we can actually combine that with the width or we can actually compare that against the real data set since we know the real information and see how well our attacker is doing here so if we look at the chart here what we can see is that for almost ten percent of people we were actually able to correct like exactly reconstruct their income and if we write in the range a little bit and say okay well let's look at for example within five thousand dollars then we're actually able to get that correct for more than 20% of people now in this case of course the attacker wouldn't know which twenty percent it has correct although in reality what you could do is run this attack many different times and start looking at sort of the distribution of possible values and so with more computational power it is still possible to get more information and so now the question is you know what exactly do we do about attacks like these and so this is where I think a really exciting technology is differential privacy differential privacy actually starts out as a mathematical definition says you know if if implemented correctly you can guarantee that a statistical guarantee that you won't be able to detect the contribution of any individual row in the data set in the output computation and so that enables us to exactly line up and say now you can't do this type of reconstruction attack and so that enables us to to guarantee that we can hide the contribution of the Indian individual and have a much stronger privacy guarantee and so the way that a differential privacy does this as I mentioned is it's originally a mathematical definition and since the the publishing of that idea the research community has developed many different algorithms that successfully implement this in different cases so that you can apply it and it works through through two mechanisms so I'm going to want to do some aggregate computation whether that statistics or build a machine learning model and the first thing I need to do is I want to add noise and this statistical noise it hides the contribution of the individual so you can't easily detect it and the the idea here is that in most cases particularly for this aggregate information we should be able to add a amount of noise that's significant enough to to get that privacy guarantee that you can't detect the individual but small enough that it's a small amount of noise on the overall aggregate and so that aggregate is still useful for our computation the second piece is that if you could do many queries or the right type of queries then you might still be able to to detect the individual information and so we need to calculate a how much information was revealed in the computation and then subtract that from an overall privacy loss budget and so the combination of these two capabilities enables us to have this much stronger privacy guarantee and so as I mentioned this is a you know very active area of research and there's many different algorithms that have been developed by the research community to implement this concept in practice depending on your particular setting and so we wanted to make this easy for people to use without it being a without it being you've been required to be an expert in differential privacy because I think it is such a promising capability but on the other hand it involves quite a range of algorithms to implement and so we partnered with researchers at Harvard to develop an open-source platform that enables you to easily put a differential privacy in your machine learning and data analytics applications and so we've developed this platform and it sits between the user and the query you want to do and your data set or your data store and so when you query through the system we will add the the correct amount of noise based on the query and your privacy budget and then subtract the information from the budget store and allow you to track the budget and so then we'll give you back that query your aggregate results but with that differentially private noise added so now you have the the privacy guarantee and so you can go forward and use that so let's go back to the demo to see what this looks like in practice so in this case I'm going to use our open source system and we're going to add the differential private noisy is to that that income distribution so here you can actually see the the epsilon where I'm giving it a budget to do that and so now we've actually added noise to our histogram and so let's let's check that privacy guarantee so in this case I'm gonna redo my same Sat solver attack but I'll actually see a really different result which is unsatisfied so this is great it means that we have successfully protected privacy at least against this type of attack although the great thing about differential privacy is that there's there is you know mathematical proofs behind it so we also know that we are protecting private if implemented correctly we're protecting privacy against a variety of other attacks besides this specific one that I'm demonstrating here and then it's not enough however to just use the to be able to protect privacy because we can also do that but just not using the data or just not having the data at all right and so the second thing we need to investigate is how we actually how well we're actually doing in terms of that noise and so let's run this and compare so here's the the comparison of the non private and private information and so you can see overall we're doing pretty well however it is a small data set so there's particular cases where you can definitely see the noise that's being added so that might be fine for my problem it might be that I can and tolerate a fair amount of noise or it could be that in this case I want to give the query more budget to use so that I can add less noise or it might be that I want to use a larger data set or different aggregate functions to enable me to have sit on a different point in the the privacy accuracy trade-off curve so we there's a lot of options here it's not a not a fixed amount of noise that has to be added so then if we want to look at you know one of the ways that we can use differential privacy in machine learning what I'm demonstrating here is with our open source system you actually can generate synthetic data so in this case we're creating a data set that using differential privacy that lines up so here's we're giving the budget and it lines up with the overall trends and patterns that we want to see in the data set at the aggregate level but actually you know hides the contribution of the individual as we discussed and so I can use the the system to generate my data set here and then I can take that and go and do machine learning however this isn't the only option I also instead could be I also and said could just use differential privacy directly in the machine learning optimizer so that as I'm pulling in the training data I'm calculating the the budget that I'm spending and feeding into that model so there are multiple options of how you might want to use differential privacy in machine learning but first this is a bit more straightforward where we can just go and add it on top of the on top of the aggregate results so with that I do want to mention that this this project is part of a larger initiative so this is really just the first system in that initiative where what we want to do is build the software in the open source and use this as a way to both allow more people to adopt it but also to advance the state of the art the more places where we can collaborate and really try it and and figure out why it doesn't work and then advance the research and iterate the farther we can take this technology and open source really enables us to do that the other thing is because these are difficult algorithms to implement it's great to have them in the open because we can have experts who can inspect them we can verify them we can develop you know tools that verify them and so it's a really great place for us to really create a community around this technology and so and the other thing I want to mention is that many people when they see this immediately think about the data they already have and why do I want to add noise that seems like that's just an absolutely worst experience and you might still want to do that because you know privacy is important and it might be that it's worth that privacy guarantee you to have a little bit of noise in other cases it might be that because we're trading changing the trade-off curve here and you can actually use you can use data that you might not have otherwise been able to use because the baseline requirement is that there's a privacy guarantee to even use the data that means that there's more data that's available that now we can happily use for the for the good of society for the good people while not risking privacy and so I think we really believe in this initiative as a way for us to use more data on the problems that are important to people in society without having to make a hard trade-off between using the data to solve those important problems and protecting people's privacy so with that I'm going to switch and talk about another family of technologies that can work in a in combination with differential privacy and so these are called confidential machine learning and as I mentioned it's actually a family of technologies and they largely you know are United around the theme of confidentiality and enabling you to do machine learning or competent or computation in a way that's confidential however there are different they have different trade-offs than they can be used in different ways and so the first and sort of easiest one to understand is having technology that's confidential from the data scientists so you can imagine cases where I want to be able to design my model you know code up what it will be but then I don't want the data scientists to be able to look directly at the data so I want them to be able to train a model on behalf of that data but they I don't want them to be able to directly inspect it so that's the simplest type of confidential computing and we we have that capability coming and so that will enable you to have that guarantee that the data scientists can't see the data now if you want to go a step farther then you can do confidential computing or confidential machine learning using encryption that is powered by hardware so in this case there's a hardware unit called an trusted execution environment that runs inside the CPU and all of the computation is in stays encrypted so this really completes the encryption life cycle where if I was actually I'd you know data is always encrypted at rest only now and it's encrypted over the network but when it gets inside the CPU you actually have to unencrypt it and so then you do have that data exposed to the operating system and the CPU and you have to trust that so in this case what this does is actually that you can you can instead keep the data and the computation encrypted inside of the CPU so now you don't have to trust the operating system you don't have to trust the cloud and only inside of this trusted execution environment will you actually unencrypted the computation and execute it but then your Rhian crypt before you put out the results and so this enables us to build you know machine learning models on encrypted data and and produce an encrypted model or or the same thing with training where now I can have a model in time that Enclave and I can send encrypted inferencing requests and get the response back and I have a much smaller trust boundary in terms of what I need to think about the other thing that's interesting about this technology is that it enables multi-party scenarios where now each of us need to trust the hardware unit and we can collaborate on building a machine learning model but we don't have to we don't have to expose our data to each other and we don't have to trust each other so there's a lot of interesting things that we can do on this hardware based technology and the thing that's great about it is because it's inside the unclaimed view actually because it's running unencrypted inside the unclaimed you can run a large amount of computation and different types of computation so it's a works in a lot of cases however you do have to trust that that Hardware unit and you have to have special hardware and so in some cases we want to go a step farther perhaps if you are Stewart's of someone else's data and you actually sort of have a mandate to minimize the the number of times it's encrypted it might be that you actually want to to look at ways where and you can do the computation and leaving the data completely encrypted and so that's where homomorphic encryption comes in and the idea of homomorphic encryption is that I'm going to leave the data completely encrypted and I'm going to develop math that allows me to actually operate on the encrypted values so in this case certain types of computation we can do it as encrypted 1 equals encrypted + encrypted to result in encrypted 3 and so this enables us to actually perform this computation without on encrypting anything and so you don't have to you don't have to have trust and the hardware in the way that you did with the the previous technology and so we have an open source library called Microsoft seal that contains meaning grade homomorphic encryption them's and so it can be used to implement a variety of homomorphic encryption scenarios but I'm gonna jump over and demonstrate actually how we can use this for machine learning so in this case I'm going to set this up and so what I've done is I've trained a model and my model sitting in in my model registry and Azure machine learning and so I'm gonna download my model from my model registry and then I'm going into to set this up in the cloud for inferencing so we can actually host it in the cloud and then send inferencing requests to it but I want it to do this using homomorphic encryption and so the difference that I need here is I'm gonna create my scoring file to to actually you know inference with my model however I'm gonna use seal in this case right so I'm gonna use my encrypted inferencing server and I'm actually going to build that into my scoring file so that now I can do my inferencing with homomorphic encryption so then now I just actually want to use Azure machine learning here and I'm gonna deploy my model using my azure container instance so now the model is going to be set up in the cloud so now I can we can actually you know move forward and start calling it so here let's test the service and what I've done here actually is I'm generating the the keys so I have my public and private key and I'm going to set up the the private key in this argument for the public key in the cloud so I can't actually do that now let's send a call to it so this is the the features that I want to send so I want to know if this person would be accepted for a loan however I want to to the cloud encrypted so I'm going to send that and so this is the value encrypted that I'm sending to the cloud and we're going to look at the response here so this is the response we received also encrypted so that way it's hard for the cloud or an attack or anyone to know what they're seeing here and so then now if we actually decrypt the results you know we can see the information so we can get the prediction and in this case this individual would be denied alone so this is great it doesn't work for all model types but it doesn't able us to now allow someone to actually use a model but without sharing their data to the model or or or to platform that's hosting the models so this is really a higher level of confidentiality so with that I'm going to wrap up here and say responsibly is a big topic and so we have a lot of great resources both on what we were talking about today which is privacy and confidentiality as well as some of the other topics I mentioned briefly and so you can go to the responsible a Resource Center and we have a lot of different sort of practices to help your organization get started in responsible AI development as well as links to all the tools and capabilities that we just mentioned I also mentioned that these work either you know built into Azure machine learning or on top of machine learning so you can go directly to AML and learn more about how to use our responsible AI capabilities inside of AML and if you're interested in getting involved in open DP or using our open source differential privacy system you can check out the open DDP community or join us on github we would love to have collaborators and contributions for the system and the same thing microsoft seal is available on github and we'd love to have collaborators and contributors there as well so I hope that this was a you know a great session for you and that you use these tools and that we see you on github so thank you 