 Christopher Penn: In today's episode, Jessica asks, who's liable for violations when it comes to marketing AI models or any artificial intelligence models, the company that hires the vendor or the vendor anything service agreements to look for. So artificial intelligence models, some really fancy word for software, right at the end of the day. Machines are writing software, but they're writing their own software. But it's still software is still a piece of software no different than you downloading a copy of, you know, Microsoft Word, or Excel or video game. And so the laws that apply to AI models, largely the same as the laws apply to standard software and, frankly, the laws that apply to anything. Companies have some level of responsibility when it comes to Product use and misuse to have to warn you of things that are obviously potentially dangerous or misleading. You know, when you look at the standard bottle of any kind of medication, it tells you, you know when to use this product directions to take it. Don't take it for these things. So there's this a little bit of responsibility coming out of the company. But other things, you know, don't have those warnings necessarily, because they're not really needed. Like, you look at the job of doing spicy peppers. There's no warning on here it says do not pour in your eyes. But people do crazy silly things like you know, tide pods. At the end of the day, when it comes to artificial intelligence, who's liable for the way that a model behaves is ultimately up to the end user if you download a piece of pre trained software pre trained model from Any vendor, and that's got a bias in it of some kind that is against a protected class and therefore not legal. You, the company and the user are responsible for what happens with that. Right? You are responsible if you use it and then and this model causes your marketing or your operations or your customer service to discriminate against somebody on a protected class, you are responsible for your actions, you are responsible for it just the same as you know, if you go out and you buy a knife of some kind and you use it in to cause somebody harm, it is not the knife minute manufacturers fault that you did something with the tool that was improper. You the user, were the person who did the bad thing, and you are liable for it. And that can mean civil liability, like getting sued or can mean criminal liability, like getting arrested. When you use any piece of software these days, actually, especially when it comes to the Downloading models and data from other people, other companies, there's a good chance you sign an End User License Agreement. Now whether you read it in its entirety is not the company's problem. But chances are somewhere buried deep within that End User License Agreement is a statement that where you waived all liability and you assume entire responsibility, whole responsibility for what you do with the software. Whether or not the software is flawed, whether or not the model is biased. Whether or not the training data was any good to begin with or not. There is a good chance that you have incurred 100% of the liability especially if you're getting it from a major commercial vendor like Amazon or Google, or IBM or anybody, there's a good chance that you are assuming 100% of the risk. Now, if you are publishing data sets, if you are publishing pre trained models, you definitely want to talk to Your legal department to effectively do the same like, hey, we've done our best. And maybe, you know, as as we were talking about, you list the ingredients and where you got the data, its provenance things. But you definitely want disclaimers of that, as well to somebody downloads your data, your software, etc. Your models, you want to say, hey, we've done our best. We know there may still be issues in here, but you the user, assume 100% of the liability for how you use this, then you cannot hold us liable if you make a piece of software that discriminate against discriminates against people who are, you know, transgender, guess what, that's not our fault. That's not our problem. from a legal perspective, it is your problem as the user of the software. It is your liability for you have to take ownership of your actions. In terms of service agreements, you've got to read them You've got to read them. Unknown: And Christopher Penn: especially before you pay money to anybody, you've got to read them to understand what they are and are not accepting responsibility for. And do that due diligence of look inside their documentation to the best extent that you can. If you're going to be paying money to this vendor, you can ask that as part of your due diligence to say, give me your data lineage. Give me your data, provenance. Give me your model governance. Show me how you're keeping the model from drifting Show me. Any known issues, you know, just like you buy a house. Yeah, have a house Inspector, and they come in and tell you Oh, yeah, it looks like there's a leak there. Like it's old, but there's definitely a was a leak there. And that's out of out of code. You want to do that same level of due diligence with any models or software that you're working with to say to the vendor. Hey, tell What you're known issues are disclose any material conditions that could cause issues at the vendor says we don't have that documentation. No, sorry, we don't have that. Don't do business with them. The same as they said, Oh, we our model is totally fine. There's absolutely no bias in it. So we don't have that documentation. Okay, that means you didn't do any checking, because almost every model a data set has some kind of bias in it, whether it's material or not, meaning has constant is consequential or not is is secondary, but even in the the weekly newsletter, I publish to the Trust Insights newsletter, whenever we publish a study we disclose the methodology and say like, you know, for example, with SEO, this is limited to the English language. So it automatically has a bias in it is that bias material, maybe, maybe not. But if every vendor who publishes any kind of data models, algorithms, should be saying these are the nodes biases in here. take that with a grain of salt, right? If If you feel like the data set needs to have more than just the English language and for SEO, then our data would not be helpful to you. Right? There's that disclosing of known biases, that's the sign of a more mature vendor. they've they've got their documentation order, they got the disclosures in order. They've told you what they know is wrong. And it's up to you to decide whether those things are important to you or not. So, really good question on liability at the end of the day, remember, it comes down to the the tool manufacturer is not responsible for what you do with the tool. You can do good things with it, you can do bad things with it, but at the end of the day, they're not liable for you are so that's the rule to keep in mind. Has any of this been settled in court? Not that I know of but I also have not done a whole lot of checking on fine law but based on existing software, Law based on existing general liability law and what and I'm not a lawyer and disclosure, I'm not a lawyer, check with your own legal counsel. I don't think aliens has come up in court yet. I'm sure it will at some point and it based on existing law, the courts are likely to rule that unless you were intentionally negligent or willful in the creation of your model, that you're the software manufacturer is probably not responsible for it. If you have follow up questions, leave them in the comments box below. Subscribe to the YouTube channel and the newsletter we'll talk to you soon take care want help solving your company's data analytics and digital marketing problems. This is Trust insights.ai today and let us know how we can help you 