 well hello my name is Eve Anderson I'm a director in Google AI and I work on these two areas that I'm about to talk about I've been at Google for over twelve years which is a lifetime in Google years before that I mostly had entrepreneurial roles I had a company based in Cambridge Massachusetts and I also taught at a university and helped develop the curriculum for a new computer science university so um my plan is to go through this material fairly quickly and then leave lots of time for Q&A because I think both of the topics that I'm going to talk about our both really interesting but also a little bit controversial and so Q&A is my favorite part okay so we're going to start with accessibility and accessibility is about making products that help people with disabilities 15% of the world's population has a disability that's over 1 billion people across the planet and these include impairments in vision in hearing in movement and in cognition so some of the things that we've been building at Google in order to help people with disabilities are very AI focused and AI dependent so I'd like to tell you about a few of them one is called lookout and we launched it in March of this year lookout is an app that aims to help people who are blind or who have low vision better interpret the world around them so it uses computer vision to identify objects and text in the environment and then it provides a continuous audio stream to the user so if you can imagine they might be walking around maybe one headphone in their ear and it's just talking to them telling them what's around them so how it works the camera on the phone captures live imagery of a scene continuously then computer vision is used to identify objects people and text in the then we score the items that are returned so we decide which are the highest importance one and ones in which are the highest confidence in recognition and we speak only the relevant items to the user to avoid overwhelming them because I think you can probably imagine if you're walking and trying to listen to the world at the same time if it's just spewing out irrelevant or too much stuff that would kind of suck so this is a really hard problem to solve there are a lot of challenges beyond traditional computer vision challenges one is that if a person can't see what's what they want to frame to recognize the images might be blurry they might be poorly lit the user might be moving as they're using it and so we have to use techniques like frame selection to avoid selecting images that are blurry or where there's a lot of movement for recognition another big problem is miss recognition in this example it's identifying the door as a refrigerator and if you're at home that might make sense that something like this would be recognized as a refrigerator if you're at work it's a little bit more likely that it's a door and so we have to use the users context to understand what they're doing at a given time and then only use the recognizers the computer vision recognizers that make sense in that context the third issue is latency if too much time passes between the time that the camera detects the objects and when it's read to the user then the information could be completely obsolete so what we've done is developed a whole bunch of on device recognizers and this also then of course helps for users who aren't connected to the internet or who want privacy in what's being recognized for example if you have it reading your envelopes at home you might not want all of that going straight to the cloud cognitive burden is one of the biggest problems it could be overwhelming if there's too much information or too much or wrong information so we have to be really just judicious in how we score systems in what we decide to present and we have to do some techniques like collapsing information for example we have an object recognizer and we have a text recognizer well you don't want it to say can of coke from the object recognizer and then say coke from the text recognizer that's redundant and annoying so we have to understand when things are part of the same thing and dedupe results lastly another challenge is extended use so we expect people to be using this for long periods of time throughout their day and we don't want them to have to pull out their phone and change the controls all the time that would be really annoying so what one of the the main ways to use is by wearing it on a lanyard like all of you have around your necks right now and so we built in control so that when the user covers the camera with their hand it just stops talking and then if they want to resume we use the accelerometer to detect if a user knocks twice on it and then it resumes so that way they never have to pull out their phone and it could just run and run and interestingly we've been collecting analytics on usage and we found that 37% of users are using it for more than 200 minutes a day so this is something that I think is really changing how people interact with the world actually one of my favorite quotes from one of our real users was I'm gonna paraphrase cuz I don't have it in front of me but it was something like I was walking around in a hardware store and I felt like I could see again I was examining all of the different products and I didn't have to ask anybody for help that made me made me really really happy to read that it's one of the reasons I love working in accessibility because you just like viscerally see how you change people's lives so another area of AI that's really useful for accessibility in many many different ways is speech recognition it's becoming so commonplace that people almost don't think of it as AI anymore once things aren't magical we don't call them AI anymore but it is I promise very similarly to computer vision it's taking in these signals it's running it through a convolutional neural network and then it's providing real-time transcriptions of the speech that are being said and this has so many applications for accessibility and here are just a few one of them is a product called live transcribe which we launched launched in February of this year this is a product that helps people who are deaf or hard of hearing so what you would do if you're deaf or hard-of-hearing is you'd put your phone out on the table and then when other people around you speak you see transcriptions in real-time of what they're saying and you can save them for later it can recognize many many different languages you know some people who are deaf or hard-of-hearing can speak but others can't or they prefer not to speak and so if you want as a user you can actually type what you want to say to the group and then it can read it out to everybody and I've seen this make a huge difference in people's lives for example one of the research scientists at Google his name is Dmitry really really smart person deaf since the age of three and he's a mathematician by training and because of live transcribed he was actually the impetus for us building it because he was having trouble communicating with his teammates so because of live transcribed not only is he better able to communicate with his teammates but he said for the first time he could have conversations with his six-year-old twin grandchildren speech recognition can also help with different types of disabilities for example something that we released the fears that go now but we continue to release new versions of it it's called voice access voice access lets you use an android phone without having to use your fingers at all so you're using your voice as a finger in a way so it's a little bit different from the Google assistant where you can do certain things which we refer to as high level commands rather you're doing low level commands like scroll down or click Next or whatever anything you can do with your finger you can do with your voice so it gives you complete coverage of all apps including ones that Google didn't write and I've actually found this very useful myself so when I started working in accessibility I pretty much believed that I didn't have any disabilities but I type way too much and I get sore and I've had really bad RSI actually for the last few months and voice axis has been a lifesaver for me especially when it first flared up I was able to just not use my hands at all and it made a huge difference from my own healing so thank you team if I had if I had known how important it would be to me I probably would have resourced that project here so an another thing I want to talk about is Google assistant which was not built as an accessibility application but it has had really big implications for accessibility for a couple reasons one is that it removes the need to so directly interact with technology in the way that technology expects you to which is a problem for people who for example have some kind of cognitive disability or for even if you're blind or you have a motor impairment and it's hard to type just being able to speak something is just so so nice I have a teammate named Kendra and her mom has I think it's multiple sclerosis her mom is in a wheelchair and one day her mom fell out of her chair and she couldn't get back up and she was able to ask the assistant to call her husband and her husband came home and helped her so it's interesting because a lot of things that weren't built for accessibility have accessibility implications and a lot of things that are built for accessibility actually become mainstream for example autocomplete for words that was built for accessibility and now I don't know about you but I wouldn't want to live without it I want to show you a video of a new project that we're currently working on so what I just told you about speech recognition works really well for people with clear speech without any kind of speech impairment but a lot of people with disabilities have a different way of speaking than some of the rest of us do and so let me show you a little bit about this project [Music] [Music] people whose speech is hard for others to understand they're not used in training the speech recognition models the game is is to record things and then have it recognize things that you say that aren't in the trainings your Demitri record and 15,000 phrase it wasn't obvious that this was going to work just sat there according to understand and the person who speak to them you can see that it's possible to make us be trapped and I start to work for Dimitri it should be possible to make it work for many people even people if you can't speak because they've lost the ability to speak the work that shun Sheng is done on you know voice utterances from sounds alone you can communicate but there may be other ways of communicating most people with ALS end up using an on-screen keyboard and having to type each individual limb with their eyes for me communicating is Steve might crack a joke and it's related to something that have been you know a few minutes ago the idea is to create a tool so that Steve you can train machine learning models him style to understand his facial expressions [Music] [Applause] to be able to laugh to be able to cheer to be able to boo things that seem maybe superfluous but actually are so core to human I still think this is only the tip of the iceberg we're not even scratching the surface yet of what is possible if we can get speech recognizers to work with small numbers of people who are in lessons which we can then combine to build something that really works for everyone [Music] so as I mentioned that's an active area of research right now we're actually seeing some pretty amazing results with some of the people who've signed up to be testers for that so I'm just so excited at the potential for helping more people interact verbally okay I'd like to switch gears a little bit and talk about ML fairness which is another aspect of inclusive ml a little bit over a year ago I think it was in June of last year Google released our AI principles and these are things that we live by as we create ml systems and they're all available online for everybody to read and I want to zoom in on one of them the second one is avoid creating or reinforcing unfair bias and what we mean by that is that we want to make sure that the algorithms that we create are not making it so that our products are worse to use for people if regardless of their gender or skin tone or religion or any number of factors and these up here are just examples and in different countries different examples are relevant for people who have been traditionally marginalized or who technology is not serving as well as it could sorry about the formatting here this says humans have a history of making product design decisions that are not in line with the needs of everyone and I'll give you one example of that until 2011 there were no female body type crash-test dummies for testing vehicles the weights the heights the shapes of the crash-test dummies tended to be male and because of this not because of any ill-will but because the testing was not comprehensive female drivers were much more likely to be severely injured in a crash and so this is also hard to read but it says these choices may not have been deliberate but they still reinforce the importance of being thoughtful about technology design and the impact it might have on humans now if we think about a traditional ml pipeline you're collecting data maybe you're having people label it or maybe the labels are based on what the end users are doing you as the developer you're choosing your objective function you're choosing which training data you're actually going to use you're filtering it maybe ranking it aggregating it from different sources users see an impact they might click on it they might not click on it they might do something with the data and your system is continuously learning well at every point in this cycle there is the possibility for bias to be introduced so here's one example of how to combat that so this is and this is an example of combating it by changing product design so Turkish is a gender-neutral language the words for he and she it's the same word it's O and so we found because of training data that the Oh beer doctor was being translated as he is a doctor but we all know that that is only one possible translation and that really reinforces many years of bias which actually in the real world is kind of evening out but because all the historical data is as it is the translations were biased and so what the team decided was to make a design decision to surface multiple translations in cases like this another example is fairness by data so getting an unbiased data set so we found this is using a commercially available classifier that the first three images here were being correctly classified as a wedding whereas the fourth one was not but it is it is a wedding and so what we did was we launched an inclusive images competition and to get more images from around the world and we've released a data set called open images extended a third example is fairness by measuring measurement and modeling so this shows what's called the perspective API it's a system that determines how toxic a comma is on the web to help moderators moderate the more toxic ones and so it was finding that some people are straight had a very low toxicity score whereas some people are gay had a very high toxicity score even though they should be neutral and so the first step is to measure the results among the different groups different identities and then you can use different methodologies in this case we used a methodology called min diff you can look it up we've published papers on it in order to improve the difference or decrease the differences in results for people with different identities so here are just some of the lessons that we've learned in working on mo fairness across the company now as with the Translate example identify places where there can be more than one correct answer think about the data that you're using for training and evaluation how its created is it representative but don't just measure the representative of the data but also the model because even good data can result in a bad model give users the opportunity to provide feedback no matter how smart you are or how inclusive you think you're being you're gonna miss things we do all the time so making sure that users have a way to provide feedback including users who come from a variety of diverse backgrounds we've created some tools to help people in their work these are all free tools that anybody can use one is called facets to allow you to look at the data the other one is called the what-if tool which allows you to look at model performance without writing code you might have heard already about our machine learning crash course we put in an intro to fairness module in there this is the same course that we used to train Googlers and over 20,000 Googler employees have already taken it and then we're promoting transparency through the concept of data cards and model cards which show distributions and performance of both the datasets and the models themselves so I think that yes that that is it and now we have a few minutes for questions you 