 - So, okay, I'd like to introduce you now to the moderator, Mollie Javerbaum, at the end there, and Mollie is at Google.org. She, fun fact, is very local. She went to high school just across the way, Palo Alto High School, and then went to Brown University, has a BA in International Relations, worked in consulting, I think traveled a lot, and then about six years ago came to Google. In her most recent job, she's part of a core team that was responsible for setting up this amazing new program, the AI Impact Challenge Grant Program, and she'll tell you more about it, but this core team, you know, had to do everything from the funding to selecting 20 grant recipients out of a huge pool of 2,000 plus and helping with capacity building and training. She's definitely a jack of all trades, and welcome, Mollie. Welcome, everyone on the panel. Thank you very much, really, for taking time to be here today, and without further ado, I'll let Mollie introduce everyone, thanks. - Thank you, Karen. Hi, everyone; thank you so much for having me. My name's Mollie Javerbaum. I'm with Google.org, which is Google's philanthropy. We bring together Google's resources, technology, and talent to support innovative organizations that are tackling big, big social problems, and at Google, we believe that data and AI and machine learning all offer significant opportunity for social impact, and we have seen, of course, AI's potential through our own products, like Gmail and photos, and we've also been doing work to apply AI for social good through our own research programs. We've been using it to forecast floods and most recently announced a breast cancer diagnostic, but also, we at Google know that we don't have all the answers, and I would even say we don't even know all the questions to be asking, and that's why at Google.org, we're really focused on finding super-innovative organizations that are actually on the front lines of tackling big social and environmental challenges and equipping them with the technology and the resources that they need to catalyze change in their communities, and so, as Karen mentioned, last year, we launched a program called the Google AI Impact Challenge to bring this type of funding and resourcing to the AI for social good space. We put out an open call. We really, you know, the space is evolving and nascent, as I'm sure you all are aware from being in this seminar, and so, we, you know, we didn't really know what was out there, so we put out this open call for proposals from around the world, and we got thousands of proposals. We got 2,602 proposals from 119 countries, so we certainly know that the appetite is out there, and the interest is out there, and then we selected just 20 organizations to fund based on feasibility, responsibility, and potential for impact, and I'm really excited to have three of the winning organizations here today. I think together, they represent a really diverse set of perspectives and topical areas that they tackled in their work, and so, without further ado, I would love for you guys to just introduce yourselves and share a little bit about the, your organization and the problem that you're tackling. - All right, so, my name is Nick Hobbs. My pronouns are he/him/his, and I work for the Trevor Project. For those who aren't aware, the Trevor Project is the world's largest suicide prevention and crisis intervention service for LGBTQ young people. With Google's help, we've really been trying to tackle a recent problem we've come across, which has to do with our scaling. Right now, our systems are very much first in, first out. If you're a young person in crisis and you contact us, right now, it's just a matter of waiting. What we're hoping to be able to do is to use natural language processing to be able to ask someone just what's going on, give them the opportunity to tell us, and then with that, hopefully be able to know how at-risk this person is of attempting suicide, and then with that information, we can then get them to a counselor more quickly, and our counselors then have the information they need in order to help that young person. - Hi, everyone, I'm Grace Mitchell, and I work for WattTime. I actually started working there about two years ago as a data analyst, and that was my first role outside of college, so being back in a lecture hall is giving me a little bit of PTSD, but we'll move on from it. (laughing) And the Impact Challenge application was really the first thing at my organization that I was asked to lead with a team of people, so I'm really excited and grateful to be here, and WattTime, we're an environmental nonprofit, and our mission is to give people the power to choose clean energy, and the way that we do that, we provide marginal emissions rates for grids around the world, and a user can query our API and be told if you shift energy usage, what are the emissions impact of that, and that whole mechanism is called automated emissions reduction. So, the project is a bit complicated because it's not furthering along our core work in a clear way. This project that we're doing is initially giving us the first step of how we enable automated emissions reduction at a high quality around the world, so this project, what we're trying to do is estimate generation and emissions for every satellite sounding, so if we have a satellite image taken of a power plant at a specific time, we wanna be able to predict what were the emissions. Carbon emissions are the priority, and what was the generation output at that time? - Hi, everyone, my name is Heejae Lim, and I'm the founder and CEO of TalkingPoints. Similar to Grace, so I graduated from the GSB five years ago. Our first philanthropic dollar was from Stanford, and our first big grant was from Google, so it feels amazing to be sitting here with the two kind of parties together. So, TalkingPoints, the problem we're trying to solve is the fact that children spend 85% of their time outside school, including sleeping time, and considering that fact, families and parents have twice the strength of predicting a student's success and how rich their family is, so how engaging is the family and the parent member? But for underserved diverse communities, much like here in the Bay Area, like San Jose, Oakland, and across the US and beyond, these families have very little means to engage with their children's education because of many barriers, one of which is language, so if you don't speak the English, then it's really hard to communicate with the teacher. They're also working double, triple shifts, so going to the school becomes really challenging, and they often feel like if I don't know maths, I cannot teach my child how to do anything, so the problem we're trying to solve is kind of limited family engagement through technology. So, TalkingPoints, we create a multilingual family engagement platform that helps parents and teachers to communicate with each other. We're two-way translator messaging, which is AI-powered, as well as personalize the coaching for parents and teachers to recommend content and what would help them build relationships, so we use technology to really empower and build relationships that empower that kind of power of love and human connection. To date, we've served about half a million families across the US and Canada, and a week ago, just in time for 2020, we had facilitated about 20 million conversations to date. (audience applauding) - Thank you all so much. Well, I'm curious to hear from you, turning to the topic of AI for good. When did you and your organizations first realize that AI might be a useful tool in furthering your missions? - Absolutely, so, like I said, right now, our queues are very much first in, first out, and that's a problem. We had looked at a number of different questions we could try to ask, and none of them really correlated well with the use risk level, so we reviewed our counselors and what they were asking youth, and we saw that we always asked the question, just, "What's going on?" It's a really open-ended question that gives a young person a chance to really say what's happening in their life and why they're reaching out to us, so as we looked at that, we started to realize this information provides a lot to counselors. It tells them what the topic's going to be about, and experienced counselors oftentimes can get a sense for how, how at risk that young person is of attempting suicide, so our hope now is that by taking that piece of the conversation, not when they get to a counselor, but before it, we'll be able to use that as a signal and as a prediction so that we're able to then say, "Yeah, you know, we were first in, "first out before, but now, if someone really is "at imminent risk of ending their life, "we'll be able to get them to a counselor "so that that counselor can then help them "and get them to a better place." - So, for our problem, we actually have the dataset that we're trying to create in specific locations, so the US, for example, the EPA provides this amazing dataset, which gives you, on the hourly level, the power plant, the unit, hour, what the generation emissions were at that time, and the only other place in the world where they have that dataset open source is Taiwan, which also doesn't include carbon emissions. So, we're in a scenario where we have this great dataset that we want to have everywhere, and the way that those datasets are generated is through very expensive continuous emissions monitoring systems, which other countries do not have the ability to afford. Even within the US, we don't have 100% coverage of every single emitting power plant. It's based off of programs and regulations for those programs. Those are changing all the time, especially in our current political environment, so we're thinking how do we make this dataset available everywhere, and AI was really the only answer to solve that problem, and especially on the global scale, so we thought, like, what do we have available everywhere for power plants? And public satellite imagery was the first step, so, the idea of can we correlate the data that we're getting from a satellite at a certain time to emissions generation was actually done or accomplished through a proof of concept by a partner that we're working with called Carbon Tracker Initiative. So, after seeing that proof of concept, our application was, like, "Let's just take this further and see "if we can really do this everywhere." But otherwise, you have to pay and regulate and maintain these continuous emissions monitoring systems, so AI seemed really to be the only way to go. - For us at TalkingPoints, I think there were two distinct moments. One was in 2016, which many of you may remember, when Google published a paper around significant improvements in the translations using neural networks. I believe the language was first in Spanish. And given one of, kind of the core pillars of our work is breaking down the language barriers, translation quality is extremely important. About 60% of the families enrolled on TalkingPoints are non-English-speaking, so that was when kind of wow, the power of AI. I didn't really know what AI was at the time. I just knew that it was a technology that would really push forward our mission in breaking down, kind of, language access, and then the second part was when we started realizing actually, like, there's a lotta conversations happening between teachers and parents, and we started collecting a lot of data without really knowing or intending to, so teachers would be often talking about which students were struggling in certain things. Parents were sharing information about their home environment, which is really critical to the teachers to know, so we started thinking about, "Okay, how can we leverage "this wealth of data to really push our mission forward "of helping students achieve, and how do we use that data "to be able to quickly read through the conversations, "set up some profiles of students and parents to be able "to personalize their experiences on TalkingPoints?" - Thank you, guys. I think it's interesting all of your organizations are using AI for the first time, which wasn't, isn't the case for all the organizations we fund, but it is the case for all three of you, I believe, and it's interesting because there's such different applications, like, they, you know, really run the gamut, and by the way, I forgot to mention this at the beginning, but we'll be reserving some time for Q and A, so please think about any questions you may wanna ask at the end. But, you know, so one thing that I know we've talked about before is the challenge of user research and of really centering kind of the human at the forefront of this work, and the work that you all are doing, obviously, affects people and the planet and, you know, vulnerable individuals, but sometimes the day-to-day is, like, a lot of data cleaning and a lot of, like, much more kind of removed types of work streams with the data and the models, so I'd love to hear from you all how do you think about the role of user research and building those connections with your users and specifically building trust, especially with the type of skepticism that we see around, you know, Blackbox AI-powered solutions? How do you help develop that type of trust and understanding of the user? - Yeah, I think that's a really good question because for the AI project the Trevor project's trying to do, there's kind of two groups of users. The first are definitely the young people who are reaching out to us. When we're building trust there, I don't think the young person is minding as much whether it's an AI or it's some heuristic we're using to get them there. The goal is just to get them to a good spot, and if they're really at risk, to get them quickly to a counselor so that the counselor can work with them to get them to a better place, so for the AI, that's just one part of the bigger ecosystem for our users, or at least for that set of users, but the other group is going to be our counselors because our counselors are aware we're working to try to build this AI to help them in their counseling services, but if we tell them, for example, that someone is really high-risk and they're not, that counselor is going to be going in ill-prepared for what they need to do with that young person and to try to help them, so for building trust, it's gonna be a lot of accuracy, and it's gonna be a lot of helping young people get to that good spot and get to that counselor when they need it most. As for user research, because we did have a big program before this started, we have a group called Crisis Services, and we spent a lot of time on the technology team talking with them, going back and forth, trying to understand, you know, what are your needs when you're working with a young person, and what can we do to make that easier? And by starting to combine both of these groups in terms of users and working with Crisis Services, I think we've really started to build that trust. - Yeah, for us, we're not at the point where we're having users test or interact with our potential data platform, but we have done a lot of research so far into potential users of the dataset, and it's very complicated, I think, achieving responsibility and interpretability are gonna be extremely difficult, especially because there are countries who have this data available, and they don't want to make it completely open source. They think that for us to provide this data using public satellite data could be a form of spying, and also, like, you know, we could be tinkering away, especially this is, like, a research project, and we could just be, you know, on our computers creating this awesome cool dataset that nobody uses or people don't understand what it means or they're putting themselves at severe risk for sharing data or trying to fact-check people who are publishing certain numbers, and if we say, "Hey, like, your, we don't think that you're correct. "Do you wanna work with us?" We don't really know how volatile that situation could be, but we know that what we're providing enables emissions reduction and is at the first step, you know, how can you reduce what you can't measure? So, we're doing even more user, just research into the different users for now, and we're definitely not gonna completely open source the data but trying to make sure that different levels of access are available for people who need it, and also, you know, it's not just the user interacting with the platform that then lives their lives. It's like this user does some analysis, takes that up to the next person, hopefully a policymaker, hopefully a government entity, and you wanna make sure that they're telling people, like, the truth. You know, how do we get them to understand what we're telling them, and how do we make sure that our confidence intervals and whatever else are really, like, making sure that the user understands the limitations of what we're trying to do as well, so through one of the many programs that we've had through this project, we've had a really great conversation that kind of made me think about UXR for the first time, asking, like, you know, what are the consequences of these, if you do it incorrectly this way or if your prediction is higher or lower? And sort of, we have to think really deeply about the consequences of that and make sure that the user for different locations, like, if we're generalizing, saying, like, "Oh, you know, we have very sparse data "in this location, so this is why our confidence interval "looks that way," and that's definitely gonna be a huge work in progress for us. - [Mollie] Thanks. - For TalkingPoints, I think that our users are really, really big, so it's actually the completely opposite problem from WattTime, so for us, you know, user research, we've always from the very beginning, we've probably spoken to about five users a week every week, so we've really baked in user research into our process to make sure we're not going off a tangent and building something that doesn't add value or impact the families and teachers we're trying to serve. In terms of using AI and data, we do have very sensitive data. You know, parents and teachers rely on TalkingPoints often as the primary communication platform, and the risk we have is recommending content that either might be offensive to a certain parent, so we have very culturally relevant content, so for example, if you come from a very underserved family, we don't assume any resources at home. We assume that you have no laptops, no books, no toys, so what kind of activities can the parents actually act out with the child to help with their learning? But if you were a parent who had those resources in the household, it might be offensive to kind of get that kind of content, and similar with the teachers, some teachers say, "Well, I can't tell the parent "and use this content that you're recommending "to find a quiet study space." This literally happened two weeks ago. She said, "You know, like, 20% of my students "are actually homeless, so this content is useless to me." So, I think there is a really sensitive area where we're recommending things in a pretty prescriptive way of TalkingPoints as a thought leader in kind of research-based practices, but that accuracy becomes somewhat important. Now, to counter that, we really are trying to take the users through the journey of our product development, so what that means is being really explicit about why we are doing these things, and when we communicate that, I think the risk tolerance actually becomes a lot higher, and that risk tolerance can go up because they trust us as a nonprofit, and this is where our identity as a nonprofit really comes as an asset because we are not a commercial entity that we use data for commercial purposes. At the end of the day, we have the same goal of really improving student learning and student achievement that both the teachers and families would want. - Thank you all. I think one thing that comes up every time, and certainly something we notice as we reviewed thousands of proposals is this idea of how do we make sure that something actually gets used? I think, like, there's, AI has so much potential, and you can see an amazing dataset or a perfect fit, even, between a problem and data, or so it seems to you, but that last linkage of, like, who's actually gonna use it, and what are they gonna do with it, and really making that connection and understanding how, how to, to achieve that follow-through is, I think, possibly, like, the biggest insight that we've experienced running this program and reviewing all of the proposals, and just thinking about, you know, where can AI be valuable, and how can we make sure that it actually is indeed valuable? And you guys got in a little bit to the topic of explainability, interpretability, but I would love to delve into that a bit further. I think, I'm sure everyone in the room is familiar with the importance of responsible AI, fairness, and interpretability, explainability, privacy, security, and I think we would certainly up here agree that the responsibility is all the more important when you're dealing with vulnerable populations, but also as we all know, it's really an evolving and nascent field, so I would love to hear from you all what has been your experience thinking through what it looks like to apply AI responsibly in the context of your projects, and what have you learned that you could share with the room? - Sure, so, AI responsibility is gonna be a pretty wide topic, so I'll try to just touch on a few points here. I guess the first thing is, at least for the Trevor Project and the populations we work with is to really look at some of the research around AI fairness. We really wanna make sure that any group we serve is going to be treated at an equal level, muchly, it's, like, based on their need and that we're able to succeed at getting them to a counselor when they are at high risk at the same rate across different groupings, and on that same note, you know, you wanna make sure you're making that same air, so I think that's often called, you know, equal opportunity or other topics like that inside AI fairness, where you're saying, "We need to make "sure that across of all of our groups, we're treating them "consistently, and ideally, also successfully." Now, that's just one part of fairness, but if we're looking at a wider topic of AI responsibility, if we just took the data and tried to just say, "How well are we doing?" Well, how are we doing how? You know, are we caring about precision? Are we caring about recall? Are we caring about a balance? These are really important questions when it comes to trying to get people who are at risk to a counselor. You know, is it better to try to capture everyone, or is it better to make sure that when someone goes to the queue that it's actually going to be someone who is high risk? So, there's been a whole lot of conversations internally, where we're trying to talk back and forth about which metrics really are going to make the most impact for our young people who reach out to us. Now, even with all of that, to touch on another point here, you know, we're dealing with a vulnerable population. We gotta be really careful. You know, data security, lots can happen, so we even try to strive to make sure any personally identifiable information's removed from the dataset. We use anonymization tools and others, truly strive to say the data's safe. We're going to make sure we're measuring the right metrics, and at the end of the day, not only are we gonna be accurate, but we're gonna be fair across groups, so that's everything we're striving for, and we're trying to be responsible with AI. - Yeah, so I feel like we have to go, like, meta here when thinking (laughing) about fairness because the project itself is trying to correct what we see as unfair, that certain countries can record their emissions because they have the money and resources to do so, and other countries that really, really want to can't, so that's one aspect of us trying to correct especially, like, the US and EU being kind of like the forefront for at least technology, if not climate policy and whatever else, but that is our objective, and I think that's how we're trying to keep in responsibility and fairness is keeping it through the global lens, and every technical choice that we're making, is this choice available to us in every country, and if it's not, we better be using it to leverage for the other countries, not trying to say, "Well, let's create a superior product for these countries "where they already have very similar data available," and then, there's also just the responsibility of, as you mentioned, making sure that there's equal representation in your training, and we can't really enable that because we have such limited ground truth data, so I think we're kind of in, like, a cuckold a little bit of how we're gonna generalize, but that's what we're really trying to correct, and every step of the way, that's what we're focusing on, and where every step of the way, we're just trying to do more user research and make sure that we can then explain to people the limitations. - For TalkingPoints, I mean, this, I can go on for hours on this topic, but I'll limit it to kinda two things. One is thinking about how do you use data and AI to change behaviors as opposed to reinforce because when you're, when you use data that has already biases baked in them and use a model to reinforce the behaviors that already happened, and those behaviors might not necessarily be fair, so for example, there is ample research now to show that teachers are more likely to communicate positive things to white and higher socioeconomically kind of core students, but they are more likely to communicate negative classroom incidences, suspensions, or things that are not good news for the parents for the students of color. So, if we observe our conversational data, most likely those behaviors are also reflected in our user set 'cause it comes, it happens subconsciously, and if we had to use an AI model to reinforce that kind of behavior and recommend content, we would only be reinforcing biased behaviors that we do not want, so how do we use the same dataset to be able to correct some of those behaviors, so something that we are thinking about at TalkingPoints. Secondly, I think to do with kind of vulnerable, marginalized populations. Many of our families actually cannot read. Their literacy level, even in their home languages, is really low. Many of them also do not speak written languages, so in terms of collecting feedback, and back to UX research in collecting data and feedback, we've had to really think about what other mechanisms by which we can collect feedback from vulnerable populations who might be technologically challenged and are not necessarily one of you or me and proficient at using a smartphone or clicking of a button or swiping to give us data, so these are the questions that we're still asking ourselves and definitely have not solved yet. - Yeah, I think that, that's been certainly our experience. We're all kind of learning together about this, and a lot of the time, it's really about the questions as much as it is anything, just having the right mindset and asking the right, the right questions, I think, in these applications. Well, so I think for our final question from me, and then we'll open it up to the audience, so we have a lot of amazing minds in the room. I assume a lot of folks are here because they are thinking about how they can apply their own talents and skills and technical expertise to, to make the world a better place, so I'm curious to hear from you all, what advice do you have for the folks in the room, and kind of parting words about how they should be thinking about doing this? - Yeah, I think one of the big things is as engineers, a lotta times, you know, we'll see a problem, there'll be a dataset, we'll process that dataset, we'll try a few algorithms, we'll see what works, what doesn't, and then we'll say, "It looks great. "Problem solved," you know, "we've run our test set. "Let's deploy it," but when you're working on a social problem, there's a whole other aspect that's gonna be really important to take into account, and that's going to be who's going to be interacting with this AI? How is it going to affect them? Did you make sure to account for the diversity of people this can affect, and were their voices heard in its development? So, if you're able to combine both of those, you're able to create something that yes, works well and that you're able to engineer and build, but you also took in requirements and understood the society and the context it's gonna be deployed, and I think if you combine both of those, that's when you're really able to do something that's good for society and does make AI for social good. - I would give a two-part recommendation, the first being clearly understand and define the problem that you're trying to solve. AI may not be the best tool to solve that problem. And the second I'm gonna borrow from Miss Frizzle and "The Magic School Bus." "Take chances, make mistakes, get messy." I know that that is the nature of the company that I work for. I know that we wouldn't have made it this far without doing those three things, and I know that I wouldn't have made it that far either, so find people who are just as passionate about the problem as you are, and see how far it takes you. I think you would be amazed, once you get the ball rolling, how many people want to help out, how many people could help you and answer questions that you didn't even think of yourself. - Totally plus one on what (Grace laughing) Grace said around focus on the problem and defining the problem. At TalkingPoints, we say, you know, we are, we solve for a problem and solve for an issue. That solution happens to be technology-based and not the other way around, and if that technology is not AI, whether it is with AI or not with AI, our users honestly do not care, like, they care that they, the product that you give them actually has an impact in their lives every day. Think the second one, I would just say is there are two sayings that we say at TalkingPoints, and I think this has been true from five years ago when I started TalkingPoints. One is borrowing in the words of Phil Knight. There's a Knight Center, I think, across the corner on campus. "Just do it, like, do not overthink things," and then secondly, related to that, it's always a prototype because you need to be always learning, and I feel like that kind of ties in with the theme of AI 'cause the more data and the more work that you do, you will be always learning, and your models will be always learning, which means as an organization, we should too. - All right, thank you guys so much for sharing your stories and those, those words with us. I think with that, we will open up to the room for any questions, if folks have any questions for, for our amazing panelists. Oh, oh, sorry, for anyone on livestream or recording, the question was we talked a lot about the benefits of using AI, but what are the trade-offs, and where is the greatest friction in terms of considering that as a solution? - Yeah, so there's definitely friction when you're trying to bring AI into an organization that hasn't had experience with it 'cause you end up having to have a lot of conversations. Before, you know, systems could be explained much more clearly. What is it doing? How is it doing it? If we wanna make a change, how do we make that change? So, it was kind of like, you know, original programming when you think about it. Here are the rules, here's the instruction set, here's how it works. Now you have to take it the other way and say without really good explainability, which can be hard to get in AI, how is it working? If you want to make a change, can you explain to the organization, "Here's what we'll need to do "to make that change," and if they have concerns, you have to be able to address all of those concerns, and that means usually you have a few people who need to go to that organization and say, "Here's everything that we could think of. "What are your questions? "What are your concerns?" And you end up holding a lot more conversations, so I think that's probably the main trade-off I've felt personally. - [Mollie] Thanks. - Yeah, I would say that (laughing) it's just hard. That's, like, the biggest con. It's really difficult even for an experienced ML engineer to really understand and explain why they think the model is working well enough, and even just defining accuracy metrics and getting into the technical leads of the problem. There's so many different choices to make, and it's very expensive to train different models and to optimize and hyper-parameter tune, so if you kind of, like, put your eggs in the wrong basket, that could really just restrict you, and I think also just the divide in skill sets for people that work in AI, so especially, like, we've had people from software engineering background work with data scientists, and it's great to have more people looking at your code and whatever else, but it's kind of sometimes hard, I would say, to find the balance between doing what's the most efficient, fastest, best thing versus, like, good enough for now. It's really easy to just lose sight of, like, what are we actually trying to do here? We're just trying to, like, build this crazy model and have these specific results that may or may not accomplish what we want, or, like, is this good enough for now? I think it's hard to find that stopping place, and I think that AI, especially working in the energy sector, can be sort of an exclusive barrier because people feel that it's something that's gonna inherently threaten their jobs. It's something that they don't necessarily understand. They have to take a leap of faith in using your data, and then also the datasets you have to work with, can you trust those to begin with? There are a number of problems, but. - I think plus one on the kind of the data issues, the people issues, and the fact that anything new to the organization will create friction. I think the one thing that I would add is in terms of making product decisions. It's honestly really hard to predict how much effort and therefore cost and therefore resources needs to go into building a model that is good enough, so we cannot predict the future, whereas I think traditional programming, you know, you have the requirements, you have the specs, and a team of engineers will be able to tell you, "This is gonna take us three months," so in terms of planning for product, that's been, and calculating the ROI in making decisions has been the hardest for us. - Oh, sorry, I'll repeat the question. (panel laughing) So, the question was, "What is, how do you know "when your model is good enough, "and how do you think about effectiveness?" - I think it depends on what model, right, so if you think about, thinking about our translation model, like, how accurate does the translation need to be, and how accurate that model needs to be actually depends on the conversation and the topic of the conversation. Is it a really nuanced conversation about student behaviors and potential kind of learning disabilities? That requires a very, very high level of accuracy, and actually we do not run that through machine translation because of that reason, but we are able to triage the topic of the conversation to be able to kind of route our translations accordingly. I think if it, if it's about recommending content for us and giving coaching content, it needs, you can be broader strokes, and actually we can have more false positives, so I think it really depends on what is the model doing, and what are the use cases, and what are the trad-offs in thinking about those models? So, it depends, is the answer. - (laughing) So, I guess for us, we're working backwards a little bit because we see this project as successful if we can technically achieve our objective, like, nobody's done this before. We're not even really sure if it's possible, so we're trying to take the top-down approach of saying, like, based off of this metric that we're defining, what kind of threshold do we wanna see, and at what time, for our purposes, it's easy to quantify. Are you saving emissions, or are you not saving emissions? Is this under or over-representation? Is that good or bad for things like the Paris Agreement, trying to scale emissions on a country level? Is it going to work for our partners who are using a completely different approach, but the same dataset? So, we'll never have a perfect answer, I think that works for all of our users, but we'll be able to see, like, what can we actually achieve, and as long as we're improving along that baseline, we'll keep going probably until the money runs out, so. (laughs) - Yeah, for us, it's very much, you know, our counselors spending more time with high risk individuals. That's essentially our baseline. If the answer to that (smacking hand) question is yes, then clearly this AI system is working for us. You know, we've also been working on a little queue model that can help us figure out, you know, if we put this model online, what will our new queue look like? Will we be getting more people, and will counselors be spending more time with people who are at high risk or not? So, always having that baseline (smacking hand) to compare against, I think, is necessary for any AI project, and that's what it is for ours. - So, is the question about, kind of, the way that technology is developing more quickly in the US and Europe relative to the rest of the world? Well, I mean I think I can take that one, and maybe you guys can chime in if you have thoughts to add, so I think that absolutely, the question about the way that technology is developing in certain contexts versus others when, you know, half of the world or, like you said, is below the poverty line, and how do we make sure that the benefits of technology are accruing to all of the world and also are being developed with all of the world in mind? It's an excellent question and certainly something we've been thinking a lot about. I can say, you know, our panelists on stage today are all kind of locals, which is why they don't represent the rest of the world. We did fund some organizations in Africa and in Indonesia and in India, so we're absolutely thinking about, at least at Google.org, about what it looks like to support the development of this technology and its application to the types of challenges that are being faced by other communities and other contexts, but you're right that, you know, the capital and the resources and the experience and the skills are not equally distributed, and it's certainly something that we've been thinking a lot about, and it's definitely a good consideration. - [Karen] I think it's a common challenge in nonprofits to find great data analysts, data scientists, AI specialists. How are you dealing with it, and also maybe a plug for, if you need any support this summer or on ongoing basis? - Excellent question, Karen. Thank you for asking. So, we have also a remote engineering team who's not based in the Bay Area but also rely on excellent volunteers. We actually have a team of Google fellows based with us, the seven people that we could otherwise not afford to hire, but Google has kindly donated to us, and we also, you know, kind of recruit from Stanford. We host interns. We hosted CS for Social Good interns for two summers. We also host Haas Fellows. There's two excellent fellows with us for this year, and we will continue to recruit in the upcoming year. We are constantly hiring, so if you go to talkingpts.org, that's where you will find us. - For what time, trying to think about hiring and all of that stuff, well, it is hard. Hiring for a nonprofit is hard. I would say, like, we've had a lot of struggles because we don't have a set person who's recruiting and doing all of these things, so hiring is something that everybody kind of helps out with, depending on who's gonna be working with that person the most, and in terms of finding great talent, we also have really flexible remote work policy (laughs) and unlimited vacation, so that definitely helps bring in people who would be, like, "Oh, well, I can't move "or I can't do things like that," and we also have contractors and whatever else, so I say for any nonprofit that's having struggles with hiring, I would definitely recommend having really flexible benefits and options for people. That's always a huge help, but I think also working for a nonprofit, like, working with mission-driven people, that's not something that you're gonna find at other companies or how much heart is going into the work and how much you believe in the work itself. You may not find somewhere else, and yet I hope everybody finds that in their jobs, but I think that's something that we have to offer that people really like, and in terms of internships and things like that, feel free to just email me, grace@watttime.org, or always reach out through the contact page on our website. We're super-small, so interns and things like that are just, like, as per needed basis, but yeah, feel free to reach out if you're interested, and we can let you know. - I'll be honest, as a data scientist, I'm not an expert at hiring by any means, (panel laughing) but what I can say is, you know, we do post jobs to our website, and our HR team does a great job at really getting those out there and trying to get applicants to apply. They're all right on our website, plus one to being very mission-driven, you know, being able to say we're working to really try to save lives. I think that does help to encourage people to apply for these jobs. We also are more than happy to always take in volunteers, so the Trevor Project always needs more volunteers. We're more than happy, so if you go to our website, you can find it right there, and we'd welcome you. (panelists chuckling) - Thanks, thanks, guys, and I'll make a, actually, a couple plugs of my own, so first of all, you can find more information on these organizations as well as the other 17 projects that we funded on our website at g.co/aichallenge. They're all amazing tech nonprofits that I think would all be delighted to hear from anyone in this room with any interest in supporting them, and then also on that site, you will see, you should see that we also published a report based on what we've learned so far. Specifically, we analyzed all of the applications, and we published a report about where we saw the most opportunities and where, you know, geographically and in terms of what the applications actually offered, in terms of, like, where organizations fell short or where there seemed to be a lot of momentum around a particular idea that might benefit from shared, shared collaboration, so please feel free to check that out. Hopefully it's interesting as well, and then the last plug that I would make just in, along the lines of Karen's question is that we worked with a partner called DataKind, which is a data science for social good nonprofit that helps facilitate pro bono data science engagements with amazing nonprofits, and they do a lot of great work scoping those engagements and really thoughtfully setting those up, so if you're interested in getting your feet wet and just thinking about exploring this space, I would also urge you to look them up as well. - [Karen] That's great, well, thank you very much. (audience and panelists applauding) 