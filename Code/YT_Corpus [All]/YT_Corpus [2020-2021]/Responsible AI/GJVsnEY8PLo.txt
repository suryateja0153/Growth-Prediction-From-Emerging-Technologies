 thank you for being here and I am going to be joined by four of my colleagues they have traveled from Beijing Israel Germany and Boston and of course Redmond and we also have a CTO from Danette who's going to be joining us presenting a demos I have four demos planned today ai is helping and enabling the transformation and making the businesses to rethink their entire processes and what I'll be talking about is about the cognitive services and a little bit about bots and how we are empowering scaling and building scalable solutions for enterprises such as you saw a little bit of progressive insurance in that video and Unilever and so on and so forth before I begin I want to step back 44 years we just celebrated about 44 years of Microsoft we started with altair 8800 a basic interpreter with just one vision in mind to put a computer in every desktop along the way Bill Gates kind of helped establish what is known as Microsoft Research in 1991 the three areas where he focused where vision speech and language one of the biggest reasons is because he dreamt about computers with the ability to see understand interpret for the products that we were building at the time on Windows desktop and office on desktop and so on and so forth and since then we have tested all of this and then we are rolling this out as finished products and providing this as a PS and a services all these rapiers are tested in various areas of Microsoft so very quickly let me switch here and then show one two three how many of you seen this intelligent chaos if you have not please visit us at the AI booth we are way back in the exposure I think we are hiding there is a big BMW car if you see that we are somewhere there we have some great demos in here this I'm going to spend 30 seconds and all my colleagues are gonna go come and do a deep dive here this has about 40 demos it includes all of the cognitive api's and I'm gonna show very very quickly what he can do we can pick up something like a vision API Explorer and then you can get a picture not my picture somebody better and do something like this ready ability to understand a celebrity in the picture understand the metadata and then all kinds of information and then you can go into the faces understand the faces you can look at objects he can do logo detection right that's kind of important thing so this is one huge capability the other thing you can do is if you go back to the demo gallery you can also do things like receipts understanding so so you can go look at a receipt I have in my pictures I have how many of you run into this when you scan a receipt you have it upside down so you try to upload to your expense tool and then you're trying to tilt and understand what this does is enable to understand the receipts using unsupervised learning we roll this is what is known as forms understanding as a part of the OCR APA this is available and many many more things right so if you go and then you can do with things like video understanding we're gonna go showcase all of this today and you can also look at some of these interesting pictures like nine how many of you are Boston Red Sox fan here alright so 1912 champions so this is kind of an interesting picture group of players so you can see all kinds of interesting faces about faces in there and then you can understand some of them are not able to recognize you so so things like this so switching back we'll be covering four parts an overview of cognitive services the first part we are going to do have three deep dives inside speech containers that's a first part inside vision we are going to talk about vision custom vision services video indexer and the finally we will talk about cosmic spark so you really want to stay to the last one because we are gonna take Hendrick Motorsports NASCAR data and then we are going to use cosmos DB and then we are going to showcase with the power of cognitive services the information that you can find so shall we get started that's it alright three part objectives right first one wanna showcase what we are announced what we have announced and what's new second part is about enabling cognitive services Center at enterprise scale we will be using a lot of our demos to showcase the scenarios and the last part is about gets you coding speaking of coding everything you're gonna see here will be posted here all the updates are available here if you have to take a picture of one slide there'll be this slide mainly because we don't only serve IPA and build we also serve api's so if you come to our booth you can you don't expect IPS but you can expect api's so with that we are seeing tremendous momentum right so dom amount of growth we are seeing is unbelievable there are about 1.3 million developers who have tried or discard Cognos services 60,000 customer deployments in cognitive services and the boss we are seeing about 3,000 bartz being created weekly and as I go through this I'll go through the details of this the biggest reason we are seeing this is because of two key investments one is our investment principles on productive enterprise grade and trusted last year we spent a lot of time making these ApS available in about 25 regions there are 4:17 GA services we announced a unified key to improve the developer productivity and then and got about 66 plus certifications to be HEPA certified PCA and so on and so forth and we will be bringing in V net capability very very shortly soon the second portion is also because of our approach being responsible that's super key for us there are smaller sessions are built on Responsible AI and also for a I forget cognitive services provide the most comprehensive capability their trainee services available and 17 of them are GA today we will be going and doing a deep dive on switch her speech and vision we will also touch upon what is known as anomaly detector that's a new service that we announced last month before I begin there's one big area that is a huge competitive differentiation and provides a lot of capability being searched we bring the entire corpus of worlds data and then provide this as an API so for your mobile and desktop scenarios you can go from spelling suggestions to all the way to related searches with a single call of an API and we are seeing this in our customers Airbnb is maintaining great listings of the power of Bing search api's grammarly to make sure the students are not cheating in the tests they use plagiarism checker with the power of being api's how many of you use what's up phew okay millions of images are searched in whatsapp through the power of being API so what did we announced we announced about ten capabilities are built and all of them and one new category called decision and we will be going through most of them at this talk the one big area I want to emphasize is personalization how many of you have used reinforcement learning for personalization few of you it's extremely hard to bring personalization to your site Netflix is a great example right I watch this movie how you personalize your content for yourself to watching the video we saw tremendous lift in xbox the use of personalization there is a great session by John Lankford and edges are ski tomorrow on personal I said please attend them to get the details of personalized there we brought this as an API alright so I talked about what's new and cognitive services let me go to the second part inside speech containers at Build we announced four new containers and I don't know how many of you saw Satya's demo on the speech Diaries ation pretty amazing demo where you can understand the speakers Diaries ation as well as your own live you know industry jargon right so those capabilities we are bringing them as containers very soon we have been shipping containers county services ApS as containers since December at Build we are announcing four new containers for anomaly detector farms recognizers speech to text and space text-to-speech bringing the total to about ten new containers will do show a demo of speech containers very very soon here is a quick workflow of how the containerized process that we are building internally right we build the models our developers and machine learning experts build models in different frameworks languages and so on and so forth once they get checked in we build them this is the ml ops pipeline we use internally we provide REST API agents wider and we deploy them as containers this is the same container that we are shipping externally that we are also using internally for example our own services when they use api's they use the container so this is the same pipeline that's being used and they are monitored and analyzed and so on and so forth few examples of how customers are using containers and are the containers provide all these capabilities the principles few examples Kroger Carnival Cruise as well as sa P are currently using containers in terms of Carnival Cruise they are essentially a data center in the water so they have very little connectivity and then they want to understand lost baggages no sentiment of customers and so on and so forth in terms of Kroger they want to map their entire customer journey from the time somebody comes into the shop to buy something and they use cognitive services containers for this yes ap today at this affair conference announced the integration of asset cognitive services with their digital platform so let me play a video that you can see some of the use cases of these things our vision was to bring artificial intelligence to the places where people never even imagined that they would be able to have a before we've got this great ruggedized one-ton suburban we can take this car in a disconnected environment there's a lot of different data collection devices that are within the vehicle but in-car camera 360 by 240 degrees spiracle camera we're able to deploy drones from the vehicle but when you really talk about power of the back is a little black case with the rugged Azure stack and AI that we're able to bring to the edge the data is amazing but it's the AI and the cognitive services that turn it into something that we can really use in the real world we create our own data processing in any environment that this car can get us into and disaster relief we have the ability to take in and start analyzing the infrastructure bridges roads we can find the most effective routes to be able to get aid into people we can help figure out routes to get the people out of harm's way from an enterprise standpoint increasing in efficiencies will save companies millions and millions of dollars imagine power lines we can send a single guy in a drone and a vehicle like this he's taking imagery of the power lines they could run the analytics based upon the models that are already built and now we see that this part can fail or everything's working fine being able to take the power of azure stack and AI to the edge to those people that are disconnected all over the world and bring them the ability to do things that we've never even been able to dream up before and now we can do that so want to go deeper into speech we announced two new capabilities the conversation transcription capability but along with the speech TDK provides a great speech theorization as well as industry focused jargon neural text-to-speech that be even GA noodle for this week um noodle text-to-speech we are very very close to achieving human-like parity and far better than any other cloud vendors over there and one of the interesting ways to showcase the neural text-to-speech is playing something like this so so I have a bunch of voices some of them are synthesized and some of them are recorded human voice so let me play the next one you can't know how long taro would have been mad because that was taken away from her you can't know how long Terra would have been mad because that was taken away from her which is recorded which is anybody yeah second one is a human voice okay not bad okay let's go one more nostalgic could still be evoked after only 20 years and many remembered when nostalgia could still be evoked after only 20 years and many remembered when yeah why don't you try again yeah second one is second one is recorded human wise so you get the idea right I think so so let me invite Archer to talk about this as he before he comes in we are some great scenarios in the speech and in this afternoon we are going to be covering what is known as what is new in speech so if you're interested please attend that session there are about five new scenarios that we are enabling with speech intelligent transcription call sentence transcription conversational voice agents connected car if you really interested come to our booth take a look at this modern voice generation I'm going to invite Archer here to talk about mainly on the speech architecture as well as on the conversational voice agent with that Archer take it away hi good morning I've Archer the program manager from Microsoft speech and language team from Beijing so may ask how many of you has been using Microsoft speech services could you really have okay great so because we have a lot of features this year and speech container is definitely one of them so I'll give you some details about speech containers and how we're going to deploy in your local host and how speech container can benefit the enterprise customers like you to gain the horsepower in your speech application okay so inside the speech container we have two standard container images speech to text and text to speech and you could connect the speech services simply using the clients with SDK or a restful api which is from the developer perspective is identically the same at the cloud services and of course all the speech AI components are packed up including the neural network models the settings the decoders and along with a series of pre-processing and post-processing technologies and your data like the audio files the transcription results are all stored in your localhost the connection to ager is only for building purposes okay so talk about the speech-to-text container is identical at the cloud services which means you can transcribe continuous speech to text file in the real time in a streaming mode of course you can transcribe do batch transcription from audio recordings we support 40 locales covering a Chinese man during American English British English German Italian Korean and there's more to come and the speech-to-text container supports worth activity detection the intermediate results the end of speech detection and automatic attacks were mating and punctuation and prevent maskings all the speech-to-text features you need and for the Tac Toe speech which means you can converse your text to the natural human sounding speech we supports 40 locales under the 737 voices you have different genders and personas to choose and we support the plain text input or you can use speech synthesis markup language aks SML so with the two kernel speech technologies in container there came a nice narrow that be benefited for example what is in my mind is voice agent we have a lot of customers in the list which is coming from the insurance or the telecom companies who's using the speech container in their voice agent systems because they require a high data privacy and our premises deployments so the typical structure could be like this you deploy a bunch of STD containers and the TTS containers in your local machine and you can use a container Orchestrator like kubernetes or a dog swarm so by hooking up with crm pipeline you desire for your business you could get the real time voice generation voice active was activities with your users and customers and of course you could use other container services like Louis for the better language understanding performance okay so how to build this it could be as easy as four steps to enable the speech container first you have to apply the container registry because it's in a previous stage and create a speech subscriptions in a jar then download the speech containers images to your localhost and run Nate use a standard docker command and speech SDK you're all set the usage could be some made to the ager for the billing purposes and we have some guidance for your machine setups and do remember don't forget that your machine have to support the advanced vector extensions the latest instruction size okay then we'd like to give you a live demo on how to set up the speed container in two minutes I'll give you Youssef so I'm gonna switch to my machine so to start the container you usually just log into the azure container repository there's just I already did that then we need to pull down the image so I'm gonna pull down the Microsoft cog the surfaces speech-to-text this is the latest tag which is the en-us model by default like Archer said we had multiple languages and multiple cultures available that you can download so by doing so I pull down the image next I do the run so the image is already downloaded so I don't need to do that so for the run we just specify the port optionally CPUs and memory and just the image and for cognitive services there are a couple of standard parameters which is the EULA accepting the EULA the building URL and the API key associated with it now the container is running locally on the machine now we need to setup a Python script to actually test this docker container so I've gotten a sample that uses the speech SDK as well as just a normal speech file so the only change is we remove this line which specifies the region and the subscription because this is not gonna talk to the container the cloud service it's gonna talk to our container we specify a subscription which is just no keys required because when you setup the container you actually give it the key in there so any string here would be fine and we specify an end point which is a WebSocket to the local host 5,000 and the path to get the actual path we need to look into the container so we would go to localhost 5000 and you see the blue screen of happiness and now the service API description would tell you just the different end points that we have there so once we copy this over and do this then we're ready to go so here is the demo and this is the video the audio that I'm using superpowers he relies on his own scientific knowledge detective skills and as you can see intermediate steps are shown and finally the whole audio is available and that's pretty much it back to your turn thank you thank you definitely a great demo so I want to emphasize again that we're you the speech container services through the SDK don't forget to exchange replace a region with your local end points using WebSocket okay so just get started and find more right a kms speech container thank you that's it I came back to and now so that was Archer who came from Beijing he's a software engineer at program manager at Beijing for the speech team and Yusuf was a software engineer based in Redmond thank you both of you took everything that we showed will be available in this URL as well as the URL I showed in the past so let me move on to the next one inside vision so I showed some of the interesting demos of facial recognition and so on and so forth earlier today and what use of just showed with a speech container you can do exact same thing for images with image containers and we are bringing more and more of those capabilities are build we announced two new api's one is ink recognizer and one other is form recognizer custom vision we just announced the GA of custom vision capability all of these things are enabling amazing scenarios enterprise scenarios how many of you attended Lila's session on the inside vision yesterday anybody okay so unfortunately you did not so I'm gonna is one person so I'm going to touch upon the areas that she covered on the five scenarios number one is industrial processing so how do you understand there is a security threat somebody's wearing a helmet or not a helmet in industry scenario real estate and facilities having a responsible facial understanding finance understanding so in it at Microsoft we have what is known as ms expense so when we go on a trip when we scan the receipts so we expense all of those things how do you automate the entire process using robotic process automation we use that within Microsoft using computer vision OCR capabilities and cosmos DB and so on and so forth how do you do that fourth one is media and entertainment we are going to go deeper into media and entertainment scenarios at this talk and then the last one is workplace accessibility how do you me make sure every person either you are impaired are disabled or whatever the reason you are you have the same kind of the access the rest of us have so when you look at the amount of the area that is growing the one area that is growing so much because of all the digitization capability is video content and our partners open text as well as media Valley they are providing this digital asset management of products so the only way you can get insights into all of this content is using AI and we have a great product what is known as video indexer anybody in the audience of used video indexer okay whew so we're going to go deeper into video index today video index that is built on top of Azure as a media services community services as you search it parses the video and audio files using ml algorithms it's available as a service as well as the platform you can get started free in minutes and when you ingest a video file it streams them into two areas there is an audio portion as well as the video and applies all types of cognitive api's and gets lot more intelligence into this and enables some great features as a part of this the best way to show this in depth is using a demo and I'm gonna invite Elan Elan has come all the way from ALDC Israel Development Center to present this Thank You lon after he finishes he'll hand off to andreas who is the CTO of Danette who'll come and present the next portion of the demo take it away everyone my name is Ellen I'm a software engineer for video indexer and today I'm going to briefly show you what what video mixer is and now you can use it as well so first what is video indexer and why is it useful manually tagging your all of your videos all your library is lengthy and expensive and can also be inaccurate video indexer automatically extracts metadata or insights as we like to call them using both the video channel audio channel and combination of both video and excerpt by the way award-winning product worth mentioning well you probably ask yourself what are those insights that I'm talking about luckily I can use our very intuitive UI to show you just that so if we'll take a look at the right pane you can see that we have a bunch of insights there we are able to identify faces that appear in the video we can show you exactly all the faces that appear in the video and where they appear as well moreover if the if the face belongs to a celebrity were able to identify the celebrity from a library of over 1 million celebrities for instance let's take a look at John F Kennedy that we identified in a video skip to the part and indeed John F Kennedy appears next we are able to identify the topics that are referenced in the video we use multi-modality topic inference model to do that and we're able to detect that nuclear weapons were mentioned international relation and so on once again you can see where exactly in the video this topic is related to we're also able to identify objects that appear in the video and again as all of our insights you can see exactly where they appear and we are able to identify military uniform and indeed there it is there we're also able to extract brands from spoken words in OCR that also appear in the video and once again show you exactly what they appear let's take Microsoft as an example you can see here that Microsoft indeed is mentioned one very unique feature that we have is the ability to identify the emotions that were in the video we're able to differentiate between five emotions fear anger joy sadness and neutral recently we'll introduce a new feature called scene detection we are able to break the video into its seems each team consists of many shots if we'll head over to the timeline we can see that we've also transcribed the old video and not only that you can also translate the transcription as well as all experience to more than 50 languages now we probably ask yourself how can I use video indexer well you can use the UI this is one option you can also you can also embed either the player pane or the more than insights pane to your app you're also able to use our very intuitive and well-documented API all the operations are actions that are able that you are able to do using the UI you can also use the API a service to do it and Leslie you are able to use logic at in fact it's so easy to integrate with logic apps I'll show you I will show you that right now logic at for those you don't know is a cloud service that enables you to automate workflows so let's say that our task in hand is that whenever a new blob is being uploaded to a specific container we want to index this blob using video and mixer so I have a previously set up logic app with the non video industry-related steps and now I'm going to add the video industry-related steps so first we need to generate an access token to access our account and we'll pick up the correct location the right account yeah we'd want to allow at it because we're going to upload a new video and all that is left to do is upload video in index which is a one-step process once again well chip is the location choose the correct account use the access token that we generated in a previous step is the video name we'll pick it to be the blob name video URL will be the web URL with sass that was generated in one of the previous steps and that's it now we can save this logic app we will upload a new blob once it will be uploaded great let's run the trigger manually for the sake of the demo and with a little bit of luck once i refresh no luck yet yep no we can see that the video is indeed being indexed well that's it for me allow me to invite and dress to show you one of our used cognitive services and video indexer for even more capabilities thank you thanks Alan so Alan gave you a great overview about what window to index I can do so I'm from a partner company called a net my role as CTO for mixed reality in AI and customers always ask to us oh that's great but they want more so we're looking at how can we even do a little bit more enrich what we do with video indexer for specific scenarios so in this case brandon sites our customers tell us hey we want to discover those brands in those videos but we also want to discover the logos the visual logos and we want to maybe onboard new customers of ours where there's nothing around there so what we looked at combining and adding some since some services we're using computer vision API and custom vision additionally why is it relevant right I mean we want to maybe find a spot in a video that's a good visual representation of the specific brand or we want to find an area on a video where an advertising customer might find their brand also in the video context and then boom puts it a video the advertisement right in so what I'm showing you here is not a sophisticated cloud architecture we really simplified the scenario for developers it's basically you WPF based on a sample we stripped down all the stuff we don't need we use video index API to get the basic insights into the video which also provides us key frames key frames are already good locations in a video to analyze cause they're not Murray for example and then we extract keyframes and a high quality from the raw video material with a uwp if when our tapi send it through the brand detection of computer vision with the c-sharp sdk and a specialized brand detection with custom vision that we build now let's have a quick look of how that looks like so this is a video that we've just seen with the car right and what did video n degrees have find find it found the Microsoft brand and it found it in two spots in the video right so here for example and here now the question is can we do something that can detect more so for that purpose again computer vision SDK and we also trained a custom model of seeing the Microsoft brand in the wild based on regular pictures we took so we found like wardrobe or cards on hats or to just be to really make sure we don't miss anything now let's run that little demo app we have here and it will go through that video keep them after keyframe after it gets the video index results and display the best thing is found now after a few seconds right we're right now looking at second 7 6 oh now 2nd 13 and we found something right in that on the right side we see the could the best picture we found the red squares are from computer vision and the blue ones are from custom vision looking back at with your indexer we found that at the time Stan's time was around 13 wow that's a nice shot of the car and the brand moving right through the image right now let's look at the code how is that done again we're running the sample the first thing we do is we call video indexer and we need to log in right so we get a basic type of session key after logging in into our account which has all rest calls by the way we're calling hey give us all the indexer results which is a really pretty big let me display it quickly a big jason that has all the insights s jason now what do we do what do we do with this now we could put it into an object and we actually extract the processing state with a DV civilized function of Jason convert part of the Jason tools for Newton soft another thing we also do we want to get to those keyframes really quick for that purpose we actually use Jason path similar to what you know from XML Express but for Jason and get a list of all those keyframes and then we start looping through them okay as the breakpoint will hit the next time is the conditional breakpoint when we found that mouse nice image right and this is a call through to the custom vision prediction SDK we don't need to do much we can really send the streaming raw data which is in memory stream in this case to the service here we get the account the specific service address from the custom vision portal and we get results back in the result set you see that one prediction was made we found the Microsoft logo with the color probability where the confidence about 92% that's one of the insights and if we continue we do the same thing with a computer vision API and the brand was found two times here as you've seen in the markings earlier one time was the actual text microsoft the other one was a brand logo with a specific confidence now this now runs through you have seen that already so I will switch back to the slides and the takeaway is it is pretty simple to develop those scenarios to extend the functionality of window video indexer right encourage you get to get started to build this out probably costs well maybe I want one afternoon for good developer and you can give something you to customers they probably didn't think they could get okay so I wanna thank both alone and Anders Ilan came from as I mentioned from Israel our team and Anders from Germany to come present the sim also he justs what we showcase is a bunch of digital asset management scenarios that day in it and his company company like open text and then media valet that they use to index large corpus of video data and then creating scenarios for brand detection many many many cases like this so the last part we always save the best for the last cosmic spark so one big announcement that we are making today is the integration of spark into cosmos and also providing a native notebook in cosmos TV so how many of you have used cosmos TV here great it's one of the fastest if not the fastest growing workload no sequel database at Microsoft if you think about the enterprise requirements right today the applications have to be available all the time and how to be served at low latency you have to be available to the users in the region they are and how to have a great SLA and fry in green elasticity and so on and so for these are all the common challenges enterprises are seeing to tackle this challenge you really need a completely different way of managing data and that's where cosmos TV comes into picture cosmos TV is Microsoft's globally distributed multi model databases provides fine 9 SLA guarantees and has support for Cassandra various ApS and now we are announcing the SPARC a peer support and provides the ability to scale both storage and throughput and in the matter of minutes you can provision this database across the world so we are available in all larger regions with cosmos DB and recently we announced the connector to spark DB before that there's a whole bunch of enterprises that have taken a dependency on the power of cosmos DB and and even at Microsoft we used cosmos DB across all our enterprise solutions last year we announced this native connector to Apache spark and also provided ways to architect for the land architecture today we are announcing this integration with spark api's the best way to showcase this is using a demo so what we have done with the demo is I'm gonna invite mark Hamilton here to showcase the power of cosmos TV what we have done this demo is we have taken the data from NASCAR Hendrick Motorsports is one of our largest partners and they have been pioneers and have been winning a lot of races mainly because of their use of data use of AI and what Mark is going to showcase is the depth of the new features of cosmides DB and then is going to showcase all the interesting things take it away tomorrow thank you very much none hi everyone I'm mark Hamilton and I'm a software engineer on Microsoft's applied AI team and today I want to show you how we can use our open source Apache spark machine learning library called Microsoft machine learning for Apache spark and cosmos DB's new announcement of a native spark platform to create a real-time racing analytic system for Hendrick Motorsports today we'll be taking a look at the triple-a 500 race on the Texas Motor Speedway this is about a three and a half hour race with 500 laps and every single second of this race we have about a thousand rows of data coming from the over 50 cars out there so it Henry Lee wants to do is to be able to take this large streaming data source and be able to understand it chop through it and figure out ways that it can improve not only its racing performance but also its safety and its efficiency so we can start off right here in the azure cosmos DB data Explorer and what I'm showing right here is a jupiter notebook this is basically like a distributed 5/9 SLA multi-model powerpoint that's a turing-complete so we can actually like explore it and pour the data and also run code against our cosmos DB query database right here so the first step is to read in the telemetry from cosmos DB and as a nod mentioned that this cosmos DB cluster is kind of enriched with spark right here so the actual compute nodes are right next to the data so it's about as low a latency you can get and furthermore this notebook is actually embedded directly within the cosmos DB data Explorer so we can come over here and take a look at some of this data that we have and we have a lot of fields to play with today we have the nut of the car number the latitude the longitude all the telemetry coming off the engine like the RPM the throttle the brake and all of this is that kind of a sub-second timescale for every single car so we have a lot to explore here so really quickly we can use a spark query to just a visualize the first few seconds of the race to kind of see what it is we're dealing with so we have this oval track and all these cars are racing around and we've highlighted a few from the Hendrick team and the winner Kevin Hart and so what's nice about this this kind of integration between spark and cosmos DB is that it's incredibly fast because it's only reading the data that it needs to if we come up and look at this query will see that while doing a spark filter up here and what spark is actually doing is taking this query and pushing it down to cosmos DB so it only reads the data that it needs and it's exponentially faster as a result so now that we had kind of a rough understanding of what our data set contains we can take a look at a more complicated query that can let us understand a little bit about the dynamics of the track itself so here I've taken the the braking and the throttling data and I've overlaid it on the actual track and we can really quickly visually identify areas where everyone's slowing down in order to get around a hairpin turn and areas where people are really slamming on the throttle in order to extract seconds not only can we chop up this data by the track we can also look at how individuals drivers perform and only a matter of seconds so here I'll take that data frame that we loaded in in our first cell I'll join it with a data frame of driver information so this is like the names and where they finished in the race and for every single driver I'll a granade their average time span braking and their average time spent on the throttle and I'll display it and an interactive chart so right here we can take a look at this and the winners are over here on the left and the losers are over here on the right and as you can see this nice downward trend which really shows that the more experienced racers tend to spend a lot of time on the throttle to really extract the most amount of speed as possible and what's nice is that this whole notebook is collaborative so others can view it and we can kind of explore this in real time and hone in on specific things and adaptively explore our data right here so this is great and all you know we've shown that cosmos DB can do a lot of queries and aggregations but what's nice about SPARC is that this is not just a query thing it's not just a database this is a full distributed computing framework it does MapReduce it does more complicated communications and even has connections with MPI so you can really have any kind of parallel computation embedded inside your cosmos DB cluster one of the things that we're excited to talk about is the azured cognitive services on SPARC this is an open source integration between all the cognitive services in the SPARC ml distributed machine learning library with this lets you do is take the cognitive services and all this cloud intelligence and embed it directly within your SPARC workflows so that it's easier than ever to kind of apply the cognitive services at scales and the terabytes and petabytes and you can learn a lot more about this at our website a kms / SPARC so here I'll show an example we're in a few lines I can create an anomaly detection algorithm I can group by the car number and apply this anomaly detection to each cars rpm values and in only a few seconds this will actually turn through our entire distributed database because we're leveraging the full parallelism of the of the spark cluster it's kind of DDoS these services and right now it's using the cloud API but as we mentioned before we have containers so other talks that we've given before actually embed these containers directly on the spark cluster so you can keep your eyes peeled for that and in order for our demo we'll take a look at a specific card namely number 32 and we can go ahead and plot this and so what you'll see here is we got a lot but we'll kind of unpack it slowly we have in the blue line is kind of the the measured rpm coming off of car number 32 the gray confidence intervals are coming out of our anomaly detector and everything that's outside of those gray confidence intervals are marked in red as anomalies and so without knowing anything about the data that we've plugged it in it's already detected a few key anomalies in a bunch of different places and so if we can kind of zoom in on one of these i've also labeled the Nascars like warning flags and so if any of you have seen racing you'll know that when like a car crash happens or there's debris on the track NASCAR will start a warning flag which will effectively say hey we have to everyone should slow down they should follow a pacer car and the anomaly detection algorithm immediately picks up that this is a deviation from the standard seasonality of laps at high speed and fires off a few detections here what I want to bring your attention to is this particular anomaly here at the end so as you can see we have a warning flag and our rpm goes to zero as opposed to something before that was still high but not that much and even after the warning flag is lifted this green line right here we're still at zero so you might be wondering what's going on here and luckily we have kind of the video footage from ESPN or something so we can go ahead and take a look at what's going on here peels out so this is why our rpm is going to zero because it's smashing into the sidewall as we speak so I kind of feel that technical demos and NASCAR is very similar in that you go like a lot of people say they go for looking at the sport and looking at the race but really you're just going to see the crashes right so hopefully that doesn't happen to us so in addition to having all of this kind of live telemetry coming off the cars at sub second intervals we Henrick also has a live audio feed hooked up to every single one of the drivers and this is pulling gigabytes of data in every single race and ordinarily that's a real pain in order to go through and index and figure out and they have to kind of do it manually with these large audio files and it makes it difficult if you want to pull up conversations you've had with your drivers or understand what kind of insights they're saying and so what we can also do is use the cognitive services on spark this time speech-to-text in order to take this large data frame of speech that's tied to every single time stamp and pass it through a speech-to-text actually extract this transcript in a searchable form that we can then store in the cosmos DB cluster and query with any of the different models like the gremlin API or the API and so we'll just take a look at the first individual results from this query and yeah so as you can tell this the speech is really the hard to understand even as a human being you know it's got it sounds like it's being composed through like 30 different walkie-talkies and but the the speech-to-text service is trained with like a neural speech model so it's even without any fine-tuning through custom speech we can immediately pull out this transcript we didn't train it with any jargon or anything like that we can just use it right out of the box for a lot of different things and so you know all of this stuff is great but what Henrik really needs is some it's not just for after the race after all the data comes in they want something that's real-time that will actually do this in single millisecond latency and keep up with a huge amount of data that's coming in life and so this is where SPARC can really help us out and that when you code using the SPARC API you can code and batch and then immediately switch your job to streaming and we've also added serving so you can turn it into a distributed web service but that's another talk so in this case we'll take our data frame and instead of saying SPARC that read we just changed one line SPARC dot read stream and we can now pull from cosmos DB in a streaming fashion we can do the exact same operations you know all the kind of sparks equal operations Python operations machine learning operations we want we can even join it with other data frames that can be streaming as well so in this case we'll join it with speech and finally it will pass it through the models that we created namely the anomaly detectors speech-to-text model to download the audio files and we'll write it out to a new cosmos DB table the effective result of this is that we're reading in unenriched data and we're passing it through a bunch of intelligent services in order to make it more usable for our end users namely henrik and so we can take a look at the the query status and we'll see that it's actively processing new data it's kind of pulling in that large chunk and as new data is added the new database will be updated in real time so we can take a kind of a step back and look at the full end-to-end workflow now we've started with telemetry from over 50 NASCAR cars that are coming in and thousands of rows per second we've put them in like the world's largest geographically distributed no sequel database with five nines and single millisecond latency z' we've passed these through spark sequel so you can do all sorts of different kinds of computations you can put in your favorite Python hacks you can use code and R and Scala and the entire ecosystem you can also directly embed cognitive services into your pipeline to add cloud intelligence and I didn't have time to mention this but there's also a full distributed machine learning library so if you want to train your own custom models on your largest of data sets you can do that I'm using SPARQL and also using tensorflow on spark and finally we can write this back out to cosmos DB in a streaming fashion to give a new more enriched intelligent view of the data that Henrik can build power bi applications on or build alerting pipelines or build web services on and the best part is all of this stuff is open source our cognitive service integration is open source or distributed machine learning line library is open source and you can check it out online thank you very much for your attention and I hope you have a great that's the dope conference I'll pass it back to and on hey it's Kate hey so mark how long did it take you to build the demo stay there stay in the architecture yeah so this demo came on the plate last week so all this kind of came together in the past like three or four days so it's it's really fairly easy to get started and I never used the cosmos DB notebook interface before so it's it's really easy to kind of explore this data interactively yeah so this is kind of the a oriented architecture that you're working with Hendrick Motorsports Marcus from our Boston office 23 years old in joining MIT the summer so congratulations and thank you very much Sean thanks guys so we showed a lot of things vision API speech containers pitch APs anomaly detector as a part of this Hendrick Motorsports speech to take Texas speech so on and so forth so please come back and visit us we have a large booth way back so it's hard to find so it's only thing we need a some kind of air to get you guys there way back if you look at a BMW car search for them we serve IPAs and api's so remember that when you come meet with us a whole bunch of sessions today what's new and speech services is this afternoon if you want to go understand forms recognition personalized sir and all kinds of interesting sessions so I want to end with a thank you and a video and I have time for questions so let me play the video and then we'll stay back and then answer as much questions you want so thank you for being here so let's go to the video today right now you have more power at your fingertips than entire generations they came before you think about that that's what technology really is it's possibility its adaptability its capability but in the end it's only a tool what's the hammer without a person who swings it it's not about what technology can do it's about what you can do with it you're the voice and it's the microphone when you're the artists it's the paintbrush we are living in the future we always dreamed of we have mix reality that changes how we see the world in a tie empowering us to change the world we see you have more power it's your fingertips than entire generations that came before you so here's the question what will you do with it thank you everyone you've been a great audience we will stay back here all my co-presenters to be here so come in the front ask questions if not I'll see you all at the booth this afternoon come back for IPS and api's thank you 