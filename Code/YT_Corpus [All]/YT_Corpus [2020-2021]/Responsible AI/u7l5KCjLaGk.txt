 So to moderate the next panel, I'm happy to introduce Tulsee Doshi, she’s the Product Manager of Google's Machine Learning Fairness Efforts where she leads efforts across Google to develop best practices and resources to build more inclusive and diverse products. So please round of applause to Tulsee Doshi and the panel. Aweome. Hi everyone. Thank you so much again for being here today. We're really excited to be your first panel of what we hope will be a number of really, really exciting conversations. Really excited to be here with all of you. As was mentioned I lead product for Google's ML Fairness Efforts and for our responsible research efforts which means I work with a number of teams and products across Google to think about fairness, transparency, privacy in our products and how do we make sure that we're both learning the cutting edge of how machine learning might both improve and detract from these experiences and how do we actually make sure that our products are giving the best experiences for all of our users. And so because of that I'm actually really excited to sit here with these three awesome change makers who are also thinking about responsible AI in the products and the features that they’re developing for their users. And so what we'll try and do today is talk a little bit about concerns of privacy, trust, fairness in our products and in the experiences that they're building and hopefully start a conversation that we can continue on through the rest of the day and through more experiences with all of us together. So with that, I will kick it off to these awesome panelists and what I'd love to do is if each of you could introduce yourself, introduce the product that you're working on and how you use AI in that product.  Hi everyone my name is Heejae Lim. And I'm the Founder and CEO of Talking Points. At Talking Points, our mission is to unlock the potential of families and parents and supporting their children's education. If you think about a parent as a child's first teacher, children spend 70% of their time outside school. There’s a ton of research to show that parent engagement matters so much in predicting a student’s success. But if you are a parent who doesn't speak English, who's new to the country, you're not educated yourself, then that's really hard to do. We've built a multilingual family engagement platform that lets teachers and parents communicate with one another with two-way translation, with personalized guidance on what they can do to support their children. Now we're reaching about a million families in 2020 and the way that we use AI is twofold - one, is to be able to improve our translation quality by augmenting machine translation with human crowdsource translation data that is more education domain specific. And second, to be able to give personalized, coaching and recommended content to parents and teachers depending on who they are and depending on who the students are. Hi. My name is Clara Northern and I'm a girl working with the famous Nada Malou. I'm the Director of the MSF Foundation which is an entity dedicated to fostering innovation within Doctors Without Borders. And the project has been quite well explained by Jaquelin, you did the job for me. But it's about tackling antibiotic resistance. Antibiotic resistance is about to the death of 10 million people per year by 2050. So to give you an order of magnitude, it's a death toll of cancer today And the app guy's laboratory technician, a non-expert library technician, who goes through the interpretation of the test that he gives to a clinician so that he can address the treatment for other patients. And we use AI with image processing to speed up the reading process and help the clinician and laboratory technician to identify the specific shape on the petri dish. Hi, my name is Nancy and I am part of the Crisis Text Line team  and we are 24/7 support at your fingertips - so in your pocket. And our goal is to make it easier to get help than avoid getting help, and our project has been about making sure that that help is as fast and as accurate as possible, so we're a human first company. We believe that if you're in a moment of crisis, and you can't cope, you deserve an empathetic human on the other side. We also believe that AI makes humans better. That I could do long division, but I'm much faster and more accurate on a calculator and the same is with our supervisors and our crisis counselors, that if we can triage the queue, if we can take the most imminent cases first, like a hospital emergency room takes the gunshot wound, or the person having a heart attack before the kid with a sprained ankle - that we should do the same thing as a hotline. And so we're grateful because every second shaved off is probably another life saved. Thank you. First off, I just have to say it's amazing to be sitting here with all three of you. I think the work that your organizations are doing is so amazing and directly impacting quality of life, right? Nancy I really liked your point of, you know, we believe that AI can make humans better and I think that's a really strong tenant of responsible AI right? How do we really think about how humans and AI work together. And so I guess what I'd love to ask each of you is how do you think about what does responsible AI really mean to you in the context of your organizations? Are there particular aspects of like privacy, explainability, or fairness that stand out to you as being particularly important and why? Maybe just do a quick, you know, here's what we think about when we say responsible AI. Yeah, maybe we can start on the other side and then go this way. So I'm going to say something, ironically, something predictable and then I'm going to say something not predictable. So let's do it that way. So something predictable is you obviously want it to be representative of real people. So thankfully, our corpus does tend to skew, but it skews young, poor, rural and diverse. 44% of the people texting us are LGBTQ. We do tend to skew young, we do tend to skew female, we also tend to skew Hispanic. Some of this does need to be balanced out because we want to make sure that algorithms are representative, and so thankfully Jackie - wave Jackie - spends a lot of her time doing that, oh there’s another Jackie. Jacqueline Fuller doesn't yet work for us, but we can work on that, Jacqueline. So that's the predictable thing that I'm going to say that I think everybody here cares about their algorithms - It'll happen, girl.  And so, everybody here cares about stuff being representative, but I’m going to say something now not predictable and maybe a little bit surprising. OK. Which is, I think people think that social change organizations should go slow and carefully and that the whole mantra out here of like 'move faster and break things' shouldn't be applied to our organizations because the work that we do is so precious. And so what I'm going to say to that is, - that! We should actually move fastest. [Applause] These are the world's biggest problems, we deserve the best technology and the best people. So we are hiring, just going to put that out there. We said backstage that we’re both hiring, so it was a race to get it in first. So that's why I'd say about responsible AI is like, work with us -  what's been so great about this project is the mentorship that our team has had and how we've been able to grow and try new things because this is lifesaving. It's not trying to get Chinese food at 2 a.m, or a car in the rain, you know, this is the really important stuff and so we need the best possible alogrithms and the best possible work. So... call me. [Laughter and applause] Clare, do you want to follow that lead? She's hiring too. CTO But me too. Oh, let’s compete. For us as a medical organization we had to set the highest standard possible and some of those standards already exist because actually we discovered, not discovered, but we are qualifying for an in vitro medical device. So we are under stringent European regulations about privacy, patient consent, how you manage your data and stuff like that. But the other path doesn’t exist quite yet, it’s about how you evaluate their performances and if the world is to capitalize on the potential of technology we have to have more of this open discussion, like today, about sharing failures as well, sharing the limits of algorithms. But also, to build up those frameworks, we have the help of the focus group AI for Health. It’s 2 UN agencies, WHO and ITU with the help of a German research society called the Fraunhofer Society, and they help us, just sharing experiences and protocol about how we thought of evaluating the performances of our AI diagnotic tool and then submitting to peer review, engaging in discussion, and the hopefully by the end of the year it will translate into a recommendation by the WHO. So it’s a healthier way we could think of toward responsible AI, but still a work in progress. It’s a very interesting time. For us, similar to Crisis Text Line, about 80 % of our schools are from underserved communities, very heavily skewed towards low-income communities. Many of our parents don’t speak English, many of our them can be undocumented, and they are discussing somewhat traumatic experiences on our platform. We actually saw a spike in the last election, the week after. So, it’s real that gets discussed on the platform, they’re often also concerning students and children who are minors. So privacy becomes really important to us. And secondly, I think you mentioned fairness, Tulsee. So one of our goals is to change behaviors and not not necessarily reinforce subconscious biases that we already see in the data and what that means is building algorithms and recommendations that necessarily don't reinforce the current behaviors that we already know are biased. So what's our responsibility as an organization in product development, but also as an organization who is a thought leader in the family engagement space, to really push that thinking forward. I mean that's a lot of responsibility and I think one thing that I think we know is that we cannot necessarily do it alone. And that's why we bring groups together like this, which is awesome. So I guess, both Nancy and Heejae, you touched on this idea of representation and skews in the population that uses your product and I know even with MSF there's questions of where do we get the data from? Where do we have data access? How do you think about, as you're trying to build a product fast and as you're trying to really get something out the door to help these communities. How do you think about representation and what are ways that you think about, you know, data collection, balance, diversity, fairness? And Claire, maybe I'll start with you. How do you think about that kind of the risk of bias? So about the main challenge we have, it’s a general challenge when working in innovation for MSF, is that, being on the film myself before you don't want to be begged to try some stuff on top of your routine. So now at the foundation we try to really make ure we are working on some things that might really work and be helpful to the field quite quickly. That's why the first data set, we took it from a neighboring teaching hospital in France just to make the proof of concept. But then quite early on in the process, we realized that if we want the product and the ML model to work on the laboratory where we work, we need to have access to those data. So we invest into proactive data collection and to be sure that it can capture all the specificities of the field, like resistance pattern, or prevalence, or the type of phone that they are using that are quite different. So it's something quite sensitive and we realize that there's a lot of differences, but we are only in Amman, so we are looking for opening more data collection in other in other contexts, so we can capture other diversities. I think that’s a really good point there too about even proof of concept being so different from what's on the field right? And how do we make sure that we think about every environment differently as we're rebuilding machine learning systems? I don't know, Nancy or Heejae, if you have other thoughts you want to add there? So there's research in parent-teacher communication or parent engagement that actually shows that teachers are subconsciously biased and how that shows up in communication patterns is, if you are a white middle-class parent, you're much more likely to get more communication around positive things that's happening in this school. And teachers are much more likely to communicate negative incidences or behaviors to black and brown students. If you think about - how does that show up in our data collection? And if we used AI to reinforce the current behaviors, you can imagine what kind of behaviors we will be encouraging, unintentionally. So as we're thinking about data collection or reinforcing behaviors, if we are building a standard type product to help users do things faster and easier and more convenient, to be able to improve engagement metrics, those would be the goals. But our goals are not that, our goals are to be able to reinforce the behaviors that make a difference based on research and encourage our users to behave in certain ways, differently, again based on research. So I think, thinking about data collection, thinking about how do you build models that will reinforce but also not reinforce. The bias that user might have without them knowing about this themselves, is something that we continuously think about and, honestly, I think it's a challenging task. Absolutely, and I think one point that was brought up was the importance of privacy. You talked about different groups of individuals having different communication patterns, potentially, Nancy you highlighted different skews in your population. How do you think about privacy especially with this kind of sensitive information? Education, healthcare, mental health - maybe you want to take a stab at privacy? 68% of the people who text us tell us something they've never shared with another human being. So they're coming out for the first time, they're sharing that they’re hearing voices for the first time, that they suffer panic attacks or that they have suicidal ideation - 28% of them say they're having suicidal ideation. And so yeah, if you think about that Corpus, it's about 150 million messages now, entirely sentiment, all unstructured, right? It's not a survey. It's not a branch survey. It's not a script and it's also labeled by humans on both sides. So almost a hundred percent of our crisis counselors fill out a survey at the end and 19% of our texters fill out a survey at the end and we've had an outside study to show that we've got imputation. It's representative. So that's juicy and we can do some really exciting things with that and where active rescues are concerned where we have to actually trigger an active rescue. So imminent risk situations. We actually have a baseline because we know when people like Sam Nadler who's here, one of our supervisors, has to call 911, which we do now about 32 times a day. We are in the position where we need to call 911 because someone has the ideation, the plan, the means and the timing to hurt themselves or frankly someone else and we need to call in for help. Here's what's great about the privacy and protection of those users - it's the only thing I care about. I don't care about like a stock price or pleasing like Unilever in an ad algorithm or like a VC who's all over me. Not physically, you know what I mean. [Laughter] We are laser focused on helping people - all of these organizations - all 20 of these organizations are laser focused on that one thing and when you’re laser focused on that one thing, and a little evangelical about it if I'm if I'm really honest, it's pretty easy. It's pretty easy to prize privacy. So one of the first things that we did was build an auto PII scrubber and it automatically scrubs about 96% of the PII. In fact it has been really challenging lately because we know that music is a top form of self care and we really wanted to find out which music like BTS, Ariana Grande? But Ariana Grande gets automatically scrubbed and so does BTS because they are names, so that's a bummer. But we’re laser focused on the user. In fact, I would say to the extent I just pointed out Sam Nadler. The way we started this work and on our team that's been part of this work and thank you Google for letting us have her be part of this work is one of our supervisors who’s a trained social worker who lives in Tennessee and she has been part of every one of these forums and treated with respect, frankly, by the mentors and by the other teams, and it keeps our product strong because you represent the user and for us to keep the user in the middle of this work is really important to us and we will never sacrifice that, we don't have to. No one's going to clap? Really? [Laughter and applause] I think you touched on some really important aspects there of just like putting the user first, and of course the importance of thinking about PII, thinking about, as you're building your algorithm, what information is it learning from? What information can it learn from, and what information it shouldn't learn from? Clare and Heejae, other thoughts on privacy? I think I mean +1 on what Nancy was saying. Crisis Text Line, MSF and Talking Points are all nonprofits and part of that, for us at least, for our educators and families, our incentives are aligned. Our mission is aligned. We will not use the data for commercial purposes. And I think that's a really clear statement and stick in the ground and branding that continues to reinforce our mutual commitment to privacy. And use of data to really further on the mission, as opposed to there's like, a side hustle. Actually, let me comment on that because the data exhaust is another story. And we should not - if I wanted to be a wedding planner, I would have been a wedding planner. It makes me nuts when not for profits have to throw dinners just so they can pay people. That business model is fakakta. There's no business model really for not for profits. And so, a few years ago we had a couple of companies come to us and say "Gosh, you know a lot about moving people from hot to cool just with language. Like, you can't coupon them, refund them. You're essentially doing customer service and if you think about it, everybody texting Crisis textline is miserable. It's basically customer service and we have to move them. You've learned a lot, can you teach us?" And I was like, we're not getting donors paying us and training volunteers so we can help a Fortune 100 company do better with customer service, but would you pay us for that? And they said yes. And so we spun out new company. We raised a seed round led by Floodgate and in a couple of weeks there will be another announcement about some stuff, but Lyft and Intuit are our first customers. They have no access to PII but they have access to general trends to compare with Lyft’s corpus of customer service data and I'll just put it out there that I gave all the founders equity to Crisis Text Line. And so now Crisis Text Line own 53% of this venture-backed company, that's leveraging what we've learned in the data, to basically put more empathy in the world that help more people. So it's called Loris.AI and I hope it becomes a unicorn so I never have to raise money. [Laughter and applause] I don't think we have time to talk as much about putting the user first and user trust and privacy but I know Di in the panel that's coming up later will talk a little bit about that user experience flow. So we'll let you talk to some of the other organizations about that. What I would love to end on before we leave is, we're here with an amazing group of individuals. We're all learning together about how to think about responsible AI. What asks or needs do you have from this group? Or what would you like? Other than hiring, Clare and Nancy are hiring. What last thoughts would you like to share with this group before we wrap up? Wow, what a shock - we're hiring too! Just so everyone knows. I think a couple of things, I mean, we also tap, the way that we collect data for translation work is through volunteers, a lot of volunteer community translators. So if you're bilingual you can log onto our platform, you can translate for us. Of course the PIIs are scrubbed so that you won't see any sensitive data. We have about 20 million messages now, that are being trained. So you could volunteer for us. I think everyone knows an educator or parent or a school administrator. Talking Points is free for teachers so please spread the word and lastly I think, join Talking Points and help us create this movement. I do think there is a bias where because we're a non-profit we don’t move as fast and we don’t use cutting edge technology. And in fact, I think we move faster and we do push the boundaries because we are so committed to our mission in serving our families to improve student outcomes. So I think just changing your mindset and reframing that a little bit is going to go a long way. AI is so technical and many of the domains in health are very technical. And we at MSF we don't intend to become a big software company. So I think it's really one of the projects where we can think of the power of collaboration and I think of the foundation of being able to become a kind of a catalyst for maybe other projects like that and ultimately manage to gather and share patients and underrepresented populations. Because for now it's more focused on the Northern countries. So I think there's a way to go there. Ben and Maggie are also here from our team and they left Amazon, each of them actually, to come work with us and I see other people in the room who have been doing this social good work for a long time. I see you Jim Frakterman, I see you Beth Kanter, so what I want to say to everybody here is - you are all Jedi, and your lightsaber could be red or blue. [Laughter] And what I'm saying to you is, make the choice to wield your expertise blue. I'm not being funny and we're all hiring and y'all look good. [Laughter] And on that very positive note, we will wrap up today's panel or at least this one of it. Thank you all and thank you all three of you for sharing your thoughts. I think we're still learning so much about fairness, about building trust with our users, about privacy. And I think these organizations are at the cutting edge with all of us. And so please share your thoughts and your learnings as you learn more about how we think about these issues in your own products and you know, find me after, find these folks after, and we’d love to continue the conversation. Thank you. 