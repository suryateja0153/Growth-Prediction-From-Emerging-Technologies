 hey hello everyone can you hear me someone who could hear the live stream let me know okay fantastic um let me share my screen here uh and i think we're going to be talking about responsible ai uh responsibility is a giant topic and i have like two hours of content that i'm going to tell you about in about 20 minutes so uh what i want to do here is kind of give you an overview of some of the things that we're thinking about and some of the things that we're doing and some of the tools that we have um as an idea for you to know kind of where you might want to go and to learn more or to dig in deeper and i'm happy to also have any sort of follow-up conversations around uh digging into any of these topics more deeply so my role i'm in the azure ai organization and i lead responsible ai for the cognitive services so um all of our services that are uh the pre-trained models that you can either customize and access through an api they also power a lot of our products at microsoft and they've also helped develop a lot of our responsible ai tooling so uh i like to put this side up as sort of a motivation for me and to talk about our strategy which is uh you know a huge realization that uh people are struggling with challenges right now most organizations are having uh either ethical questions or just responsible development questions regarding how they build and how they use ai and so i'm very much motivated for us to get the best practices and the best tools and the best technology in people's hands as early as possible even if it's very early even if they are imperfect and so you'll see a lot of our work is in the open source we're trying to be very transparent about what we're doing and what we're learning so that we can get people the state of the art as soon as possible so for us the foundation starts with microsoft's principles so we have principles of fairness reliability safety privacy and security underlying all of that is you know how we work where we want to make sure that there's the appropriate level of transparency at every stage of the process and all of us have accountability for for our part in in the process and in the development and in the use of the technology um but principles are just a start and so the rest of this talk is really how are we starting to put these principles into action in different ways and so the way we've been thinking about it in in my team and in our organization is the foundational step is it feels actually pretty different um as a technologist at least in the ai field is that we really need to start by assessing the the product and what are we trying to do envisioning the the impact of the technology and i think a lot of us start by thinking of all the the amazing things that this new technology that i'm going to make it will do but we don't often think about the cases where it might not work as well or if there's an error there could be an outcome for for harm and so uh building uh this process in from the beginning where we think about the the range about comes from technology and so then we're armed with that thinking and we can use that to decide uh what what to do in our design in our implementation and our testing and our deployment uh to help us mitigate any of the potential negative outcomes and really double down and enhance the positive outcomes and so we'll end up doing um work that i would put under the bucket of responsible development which is about how we build our technology where i need to make sure that this model works for multiple groups of people we also end up doing a lot of work around responsible use where the work is enabling others to adopt adopt the ai responsibly making sure they have the right information sharing what we know about how you might want to think about the technology how what's the best practice for deploying it how do you engage with people and so talking about ignite we we just released new spatial analysis capabilities for computer vision and in those we released a responsible deployment set of documentation based on research to help customers understand if you want to monitor social distancing if you want to deploy this in your building in your stores what are the different best practices for disclosure what do you need to think about in terms of privacy and different pieces like that so and the last piece of course is we're all learning this is a living system issues will happen and so designing our products to be reactive designing our products so that if we find an issue we can rapidly update the model we can learn from what happened and we can improve and we can also shut down the situation and we can build tests to make sure it doesn't happen again in the future so generally that's that's our process and that's how we think about it and we as i said foundational stuff is really envisioning that impact uh so we use um in our organization we use a practice that was developed by a group called ethics and society and the it's an envisioning exercise to help you really assess harm and think about all of these different potential harms and categorize them and as i said this is a piece that then allows you to go and decide what work do you need to do in building and and going from that and so um this harms modeling uh practice as well as others to allow to bring in different stakeholders um like community which is called community jury they're all available in our in our documentation as a responsible innovation toolkit and so we'll keep putting out these practices as we invent them and you can go and try them out and see if they're they're useful in your life cycle and helping you envision potential harm so with that i'm going to jump to the rest of the talk where uh once we kind of have those potential considerations uh in our head now we really want to look at some of the tools and technologies that are going to enable us to go and implement these these things and so uh in terms of our tooling there's many different ways you could talk about it but i kind of think of it in three categories which is uh one set of tools that we've built is really about enabling you to have deeper model understanding really understanding why is the model making an error why is it treating different people differently why did we see this outcome in this particular setting um because that will really help you have a deeper understanding and mitigate these these different issues and build trust in your model uh the second category is really i use the word protect but it's about uh protecting people whether that's the their privacy and the data or it's the confidentiality of the computation but making sure we have a robust approach to have sort of private and secure machine learning and then the last piece is is control where we want to make sure that we are using um a platform like azure machine learning that allows you to have a reproducible process allows you to have the right uh controls allows you to build governance processes on top and so a lot of our work and our work going forward will also be really about making sure that you have that robust reliable repeatable auditable process that you can trust in terms of building machine learning um so as i mentioned when i set the talk up uh we a lot of this is really early and so actually the way we've been doing this is we build a lot of the core technologies in the open source so that people can contribute new ideas very rapidly and people can directly consume them we work directly with microsoft research and other people in the research community to get the latest research algorithms and ideas in there as well as then have transparency about how things are implemented and allow us to move faster and collaborate but we've built all these tools to work on top of azure machine learning and in some cases be directly integrated into it so it's just part of your regular machine learning life cycle and so that you can have that last pillar that control that repeatable process that you can trust and so we have you know a dual approach here where where it both involves really having robust platforms but also having the agility of open source to allow us to develop um so with that i'm going to show you some of the tools um so as i said the first color is about understanding your models and the potential effects and how they make decisions and so uh to demonstrate uh this area i'm actually gonna i'm gonna jump over to a demo and i'm gonna show you the demo uh we have for making loan application decisions so i built a model uh in azure machine learning where i'm going to decide who receives the loan and and use this model for that and i want to ask multiple questions of this model right i want to understand is it fair um in this case am i giving loans differently to men versus women and then i want to understand why is this happening and how do i make decisions how does the model decide who it accepts and um who it rejects and so with this we're going to use our open source toolkit fair learn for the fairness questions we're going to use interpret ml to leverage the interpretability features to get greater model understanding and all of this of course is is backed with the azure machine learning platform so uh if we want to talk about fairness there's a bunch of different ways you can think about fairness and ai so uh that's this is a three-hour tutorial in itself um but what i want to say here is that the fair learn toolkit is specifically focused on uh what we would call quality of service fairness or you could think of it as model performance but it's about uh the question of is my model working better for one group of people than others or is it making different decisions for one group of people versus another so it does my speech wreck work worse for women or do i offer more loans to women these kind of scenarios so we've built as i mentioned an open source toolkit called fair learn that both allows you to assess differences between groups and really understand how your model is performing across different groups as well as uh it has built-in state-of-the-art mitigation algorithms and so if there is a difference uh disparity between groups and you want to algorithmically mitigate it then we have those capabilities built in um and so with that i think the best thing to do is jump over and show you my demo here um so in this case i'm using uh fairlearn and interpret ml as i said with census data and i've run this in advance because i end up training quite a few models but i just ran it slightly earlier today and this is a hosted notebook on top of azure machine learning and so all i've done here is i'm just processing my data and i'm actually training a scikit-learn model and so i just trained my model here and now what i'm going to do is use fairlearn to say is this a good model is this the one i want to put in production and specifically answer this question of how does it perform for men versus women and we know very much that fairness is a multi-stakeholder conversation it's not something that a data scientist would decide alone so we've designed an interactive dashboard that allows us to easily visualize the the results and be able to you know have a conversation around them so the first thing i'm going to do is i'm going to click in my dashboard here and i'm going to sensitive features my data set has so in this case i have two different ones i can look at i could look at sex or i can look at race and my data set has two sub values for sex and and five sub values for race so i'm going to click sex and i'm going to click through here um and then the second thing we have is the performance metric but depending on your problem there may be a more or less important metric in terms of how i want to think about is the model performing well for one group or another right it might be precision it might be recall it might be accuracy so in this case i'm going to click through to accuracy and what i see here is i have two different charts the first is the difference in performance right so i can see that my model is eighty percent accurate for men and ninety two percent accurate for women so i've got a twelve percent disparity between the groups uh i can also look at the over prediction and under prediction um i have a lot of under predictions so it could be that maybe i i can offer more loans um and if i move down here what i really want to look at is the the second chart here because in this case i'm using the model to make a decision i'm offering someone a consequential service i'm offering them potentially a loan which could have a significant impact on on their outcome and so in this case i'm offering loans to 19 of people but i have a 17 difference in how i'm offering loans to men and women so i'm offering loans to 25 of men but only about seven percent of women now the key is i don't actually know if this is a problem right it might be that i that there's something other very different about the men and women that are applying and this is a reasonable outcome so all i know now is that i need to go and i need to understand more um so if i jump back uh quickly here i'm gonna jump ahead this is where our next topic comes in which is interpretability interpretability is really a set of techniques that allows us to understand how is the model making decisions um which can be really important and so uh we have an open source toolkit for interpretability and our toolkit allows you to use a variety of different interpretability techniques together in one unified dashboard if you're familiar with interpretability there's different types you could have black box where you're going to be analyzing a model to get explanations but you're not really looking in the model or you could be using interpret ml to directly train glass box models so that you can inspect them and they're inherently interpretable and understand how they make decisions so we have text capabilities counterfactual capabilities lots of different pieces in here and so with that i'm going to jump back to my little demo here and i'm going to show you how we can use interpretml on this problem so what i've done is i've generated model explanations here and i conveniently also have a wonderful dashboard to help me explain i'm going to open in a new tab so that we can see it full screen and what our dashboard does here is allows us to to look at the model performance the data set the feature importance is and ask sort of what if questions but basically bringing the best of interpretability to ask questions of my model and my data so what i want to do here actually is use our cohort feature to look at women and men so i'm going to set up sorry filters here so let me go to sex and i'm going to add this filter and i'm going to save it so now i can see all of my women i'm going to add a second cohort here for men and i'm going to do the same thing okay and now we want one we're going to add we're going to save so now we have what we saw before but in much more detail the accuracy the precision of these two different groups so i'm using that same 80 80 92 percent and there's a lot more that you can do here to understand your model but i'm going to jump immediately over to the the aggregate future importance and um here i can see how my model is making decisions for these two different cohorts and what jumps out right away is that i am actually seeing that for for women sex is a major factor in whether or not you're offering someone alone and it's not for men and if i click and look at the individual points i can actually go and see that that for every single data point we are actually directly choosing to to add a negative value a negative contribution to their acceptance uh because of their sex so i'm pretty convinced this model is not a good one to move forward with and so if we had more time what i would show you in the rest of the demo is you can actually go and use fair learn to train a new model for you and you'll see different trade-offs so you'll end up with a i'll click here and show you you'll end up with a nice trade-off curve where you can explore models with different levels of accuracy different disparity between groups so that's our understand tools and so with that i'm actually sorry i'm gonna move on and talk about our other topic quickly um which is protect and as i said it's essential that we think about how do we really protect people how do we protect data how do we protect confidentiality of models and computation uh so we've also built a variety of tools for this front um one of the key questions that i personally have been working on is how do i protect privacy uh while still using data and so what many people don't realize is that even aggregate values uh in a computation like for example what we you know ultimately sort of produce with a machine learning model can end up leaking a lot of private information right you can have a smart reply model that memorizes rare sentences like my social security number is and then you know when someone types that sentence auto completes with the social security number it found in the data set and so it's really important that we make sure that we're not um sort of leaking information about individuals uh in order to do this and so to motivate this more i'm going to jump back to a demo over here and i'm going to show you our demo for differential privacy and so the idea here is i'm just i'm going to use the same census type of data set and i'm just want to do analysis an analysis on the aggregate values published so i'm assuming that i'm doing an income analysis here from the census data set um and in this case i just want to look at something like this like what's the distribution of individuals um with a high school degree in terms of their their income right so this is the the study i actually want to do but what uh what we've shown is if i know this information here and i also know a little bit more information about two individuals in the data set maybe this is public somewhere maybe it's an open data set so i know they're some of their education and their race um what i can do is actually taken off this shelf sat solver so i'm using a z3 here which is just nothing magic it's just a sad solver and i'm actually going to ask it to try to reconstruct uh the values in the data set and so we see that it is able to find a potential data set that lines up with the published results and since we know the real results we can compare and um what we can see is that actually in this case of 500 total incomes in the data set we've reconstructed 59 of them exactly and almost or almost a third of them we've constructed within 5 000 which could be a pretty significant privacy violation and so the the technique that we have been uh exploring and have developed and different techniques for uh to to work on this is differential privacy and so the idea with differential privacy is that it hides the contribution of the individual so the way this works is we're going to add a small amount of noise to each of our aggregate results and then we're going to also keep track of a budget to make sure that every query that's asked or a model that we've built hasn't extracted too much information from the data set such that it could be reconstructed and so we've built an open source toolkit jointly with harvard that is part of the opendp initiative that allows you to do differential privacy for machine learning and for analytics to allow you to mathematically guarantee that you won't be able to do this type of reconstruction attack so the way that this works is it's a system that you put on top of your data set or your data store or your we'll say data set as far as machine learning goes where then you can ask queries or you can train your model and we will manage the the budget and we will add the differential private noise for you so that you don't have to worry about attacks like this so if i go back to my demo what i can see here is i'm going to use our open source data set or i'm going to use our open source toolkit and i'm going to use that instead to produce the histogram so now i'll run that same attack but i'm going to run it on my noise histogram from a different from the differential privacy toolkit and what you can see is that we get a different result unsat which means there was no data set it could reconstruct that lined up with the published results so there aren't any privacy violations so the other thing we want to do is look at how well are we doing um oops yeah that's okay how well are we doing in terms of the analysis we want to do and so in this case the results look maybe less ideal um because it's a very very small data set so that i could demo it in my notebook for you but the idea is that hopefully what you'd see is that you can have results that actually only have a small amount of noise and look pretty similar to the regular results and um if we want to look at the utility here since that is less impressive um today uh the what we have here is that it's fairly inaccurate for for small groups but you get better results for large groups and that's exactly what we want because if you're very accurate on small group numbers then you would be able to understand something about their privacy and so that's the the general idea here and um we're using this in practice in a bunch of places inside of microsoft so with that i think i'm just going to briefly mention since we only have a moment that we also have a lot of technologies and a new release as part of onyx runtime focused on confidentiality and confidential machine learning so we have for example in azure machine learning capabilities that allow you to train a model remotely and make sure that um the information is confidential from the data scientists uh we also are building on top of azure confidential computing so you can train uh your model using encrypted hardware and and make sure that you have that higher level security uh we've also released open source capabilities for homomorphic encryption meaning you can do inferencing uh in the cloud without ever sending a confident and encrypted um an encrypted uh prediction and getting encrypted result back and never having to unencrypt anything and so we have a lot of different interesting capabilities that we've open sourced here and we've been building into azure and building building into azure machine learning uh and so microsoft steel is the name of the homomorphic encryption and we released how to use it on top of azure machine learning um and there's no sample notebooks to do that for encrypted inferencing so with that i am out of time so that was a little whirlwind tour of some of our tools and technologies um the microsoft responsible ai resource center is a great place to go and check out all of the resources there's tons of practices more tools research case studies all of that and then each of the tools also has a home page and all of this is also captured in azure machine learning as i said if you would like to engage on any of this i'm happy to sort of directly connect and talk more about any of these subtopics 