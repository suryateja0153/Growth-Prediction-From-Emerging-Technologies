 [Music] thanks everyone for joining in and thank you for having me i look forward to the summit every year and this year is no exception i'm very excited to talk to you about analytics and ai and how you're reimagining the boundaries of what it can achieve so many new ai capabilities have been talked about at the summit already and today i wanted to draw your attention to something that should be at the crux of all this innovation responsible ai practices looking at how the world has changed around us over the last few months and more so over the last few weeks has greatly amplified the need for us to ensure fair practices and ethical behavior across all sections of society to get a sense of the intensity of this topic cap gemini published late last year that nearly nine out of ten organizations have encountered ethical issues resulting from the use of ai these executives cited reasons including a lack of resources dedicated to ethical ai systems the failure to consider ethics when constructing ai systems and the pace at which ai is touching every part of our lives the statistic is alarming we as a community have a duty to address these issues head on and empower organizations to use ai with a sense of responsibility at microsoft we've also heard these concerns firsthand from our customers as a result we've invested in the advancement of ai driven by responsible principles that put people first our approach to responsible ai is anchored around the following focus areas keeping ethical principles at the core of everything we build enabling governance and operational transparency to drive right practices leading efforts for public policy at the highest levels and finally empowering teams with cutting-edge tools to encourage responsible ai practices now since the spark and ai community comprises majorly of developers and data scientists we'll focus on the tools in collaboration with microsoft research we are bringing the latest of research in responsible ai to azure over the next few minutes i will talk about how the new responsible machine learning capabilities in azure and our open source toolkits empower data scientists and developers to understand machine learning models protect people and their data and control the end-to-end machine learning process let's look at each of these three pillars in detail first let's look at the understand pillar it comprises driving an understanding of the model's fairness index and its interpretability but what does that mean one challenge with building ai systems today is the inability to assess and mitigate unfairness in the models to address this challenge we've recently open sourced a toolkit called fair learn there are two components to failure first focused on fairness assessment is a dashboard with both high level and detailed views for assessing which groups are negatively impacted second focused on mitigation is a set of algorithms for mitigating the observed fairness issues these strategies are based on a variety of supported fairness definitions such as demographic parity or equalized odds for classification tasks the supported unfairness mitigation techniques can easily be incorporated into existing machine learning pipelines and assess model fairness during both model training and deployment together these two components enable data scientists and business leaders to navigate any trade-offs between fairness and performance and to select the mitigation strategy that best fits their needs using fair learn developers and data scientists can leverage specialized algorithms to ensure fairer outcomes for everyone another important aspect of understanding model is the ability to interpret or explain its results this is where interpret ml comes in as the name suggests it helps interpret models and their results interpret ml is another toolkit we've open sourced and can be used for both glass box and black box models glass box models are designed to be interpretable they provide lossless explainability as they use interpretable algorithms like decision trees or explainable boosted machines interpret ml can help visualize the various features affecting the outcome of a model in an interactive dashboard and even allow for what-if analysis on the other hand black box models are based on more complex techniques like neural networks and can be a little tricky to interpret you can still achieve approximate explainability with black box models using explainers like lime or sharp entrepreneur uses these explainers to surface a dashboard with feature importance interpretability is needed to ensure that there is optimal transparency within models to assess and reason through the predictions it generates or the recommendations it creates with failure and interpret ml you can gain a very good understanding of your models let's now look at the protect pillar protect refers to protecting data against any potential misuse used to train the models to understand this better let's envision a scenario where a data scientist wants to use some sensitive data to create a model in a non-protected scenario this would result in submitting a query to the private sensitive data set and provided the right credentials that receive sensitive data as a response to their query well this is problematic now the data scientists can look at the sensitive data and potentially deduce pii information about an individual this is clearly not at all compliant or ethical now let's introduce a differential privacy toolkit within the opendp initiative that has been developed by microsoft in collaboration with the researchers at harvard's institute for quantitative social science this toolkit ensures differential privacy by first managing exposure risk by tracking the information budget used by individual queries and limiting further queries as appropriate in our example the query is first validated against the information privacy budget and processes only if it isn't overrun second it injects statistical noise and data without significant accuracy loss to help prevent disclosure of private information again in our example because the query response may be sensitive before it reaches the user it is injected with noise to make it differentially private in addition to differential privacy you can also protect your data on azure using hardware-backed confidential machine learning capabilities and homomorphic encryption and ensure your data isn't compromised at any stage of the machine learning life cycle the third pillar for responsible ml is control it refers to tracing the lineage of models and supporting a standardized format of documenting it for cataloging and reusability needs on the control side there are some features and capabilities that allow you to manage your audit trail and data sheets for the models within azure all the actions undertaken throughout the life cycle of a machine learning model can be tracked for auditing purposes this audit trail enables us to trace the lineage of the training data to operations performed on it its drift and most importantly the model versions and the various deployment endpoints now this is important for compliance reasons organizations can leverage this audit trail to trace how and why a model's predictions showed a certain behavior additionally once you have a model it is important to capture its properties and purpose alongside any other metadata that could be helpful from a reusability standpoint in azure we enable this with custom tags you can define for the models and capture all its information so that it travels with the model audit trails and data sheets enable accountability for models and add an integral part an integral component of responsible ml i've been talking about quite a few capabilities so let's bring them to life with a demo let me invite sarah bird to demo these toolkits and show how you can use them sarah thanks rohan hello everyone today i'm going to be walking through a demo of some of our responsible ai tools on azure as we progress through the ml life cycle i'm going to be demonstrating three key capabilities protecting sensitive information with differential privacy assessing and improving model fairness with fair learn and analyzing and explaining model behavior with interpret ml in this demo i'm going to be building a model for loan applications to decide which to accept and which to reject let's dive in here i'm using azure data bricks but you could also use a jupyter notebook or your favorite python environment like any good model building scenario the first thing i want to do is look at my data here i'm looking at aggregate information as well as individual features to help give me a feel for the data set and uncover any potential issues up front it looks like this data set contains a lot of great data to help me build my model it also contains highly sensitive information that must be kept private obviously i can limit direct access to the data with the data store however what many people don't realize is that aggregate values can still be used to reveal private information about individuals let's go to an example to see what i mean so here i'm going to demonstrate how we can use aggregate information to reconstruct the data set so in this case i'm going to assume that i know aggregates about individuals income as well as a little bit of individ additional information about two individuals in the data set and with that i can take it off the shelf sat solver and use that to reconstruct a data set that's consistent with the information that we know so if we run our sat solver here it's working to reconstruct that data set and we see that it can actually find a result and now we're going to compare that to the the real data set and see how well our attacker did so in this case it looks like the attacker did pretty well we're able to correctly reconstruct the incomes of individuals within five thousand dollars for more than 20 percent of the data set which could be a significant privacy violation so what do we do about attacks like these how do we use data but still protect privacy and this is where differential privacy can help so differential privacy hides the contribution of individuals in the output computation by adding a small amount of statistical noise to the query the noise is significant enough to protect the privacy of individuals but often small enough to still allow us to use the data differential privacy also calculates the amount of information that is revealed in the query and subtracts that from an overall privacy loss budget let's look at how this works so here i'm going to use our open source differential privacy platform and now i'm going to add that statistical noise to the the income information and then i can retry my attack with my sat solver and what you'll see here is that we get a very different result we aren't able to reconstruct the data set that's a consistent with the differentially private incomes and so we don't have any guesses at any private information however it's not enough to just protect privacy we still want to be able to use the data so let's look at how we're doing here in this case you can see that uh we're we're looking pretty good uh we're you know pretty close to the original information you can definitely see some noise added particularly because this is a small data set however now i can take this and go and use it in my problem knowing that i don't have to be concerned about privacy attacks so one of the ways that we can use differential privacy in machine learning is to generate synthetic data sets so here i'm going to use my open source toolkit and i'm going to create a data set that matches the patterns and trends of my original data set but doesn't reveal any private information and that way i can take that and give it to my data scientist without having to worry about privacy so i'm going to generate my data set here and now that i have my private data set i can go and build my model so here i'm going to build a scikit-learn model for my problem and now that i've built a model i want to actually analyze the performance and see is it good enough for my problem so of course i can look at traditional metrics like accuracy however since this is a problem where the a wrong decision could have a significant impact on an individual's life it's important that i go a step farther and i also consider the fairness of my model so in this case i'm going to use fair learn so that i can understand how well my model works for different groups of people and make sure that it's not using sensitive attributes to make decisions so in this case i'm going to use fairlearn and fairlearn has an interactive dashboard to help me better understand the fairness assessment so i'm going to click get started here and my data set has two values uh sex and race so let's click into sex and understand how well my model is doing but i also need to provide a performance metric which metric's important for my problem so in this case it's accuracy so i'm going to click through and the first thing i have is a graph here that helps me understand the difference in model performance across these two groups so i can see that my model is more accurate for women than men and it's about 85 percent accurate overall however i'm really interested in the outcome of this model what's the difference in who i'm offering loans to so in this case i can see that i'm offering loans to 19 of people overall however i have a significant difference in the number of loans i'm offering to women and men i'm offering loans to about seven percent of women that apply and about 25 percent of men that apply now i don't necessarily know if this is a problem it could be that these two groups have very different distributions however i do know that it's a sign that i want to dive in and investigate further so for that i can use interpret ml and its black box explanations so i'm going to explain uh generate explanations for my model here and then i'm going to use its interactive dashboard to explore those and understand how it's making its decisions so in this case i'm going to set up two different cohorts so let's set up one for women so we can understand what's going on there so i'm going to add a filter for sex and women are zero in this data set so we can add this filter and i'm going to save then i'm going to add a second cohort here for men and i'm going to select this and in this case i'm saying it for uh men and women since i want to look at this fairness problem but you can use lots of different cohorts like train and test to compare as well so in this case what we can see is uh the model performance just like we saw in fair learn but in a lot more detail however what i want to do here is uh jump over to the explanations understand the feature importance and how the model is making decisions and what i can see right away is that the model is uh using sex directly as a feature to to make predictions for women but not necessarily for men and if i click on this i can actually dive in and see that for uh for women for every single woman in the data set the value of sex being zero is directly contributing to the prediction uh that they are rejected which is pretty concerning and so what i want to do at this point is move on and explore mitigation techniques so for that i can go back to fair learn and use some of the the built-in mitigation algorithms that allow me to reduce disparity either during training or after so in this case i'm going to use the grid search approach where i'm going to retrain many different models by reweighting the data to try to reduce the disparity so i've run my model training here and now i can go back to the fairlearn dashboard to look at the best models so in this case now i have a range of models that are along the the curve of difference in predictions and accuracy so here's my original model up here and now i'm actually going to click through and look at this model which is a little less accurate about 82 percent accurate but if i jump down to the disparity in predictions i can see that i have a much different outcome now i'm offering loans to 15 of men and 14 of women that apply so this might be a much more promising model for my problem so what i want to do now is actually save all of my work so that i can share it with collaborators or show it to other stakeholders as we make a decision about whether or not we want to move forward with this model so i'm going to register all of my models here i can upload my dashboards and my explanations everything to azure machine learning then if i move over to my azure machine learning studio here then i can click on a particular run and so here's my different runs and i can click into one and now i can actually have my dashboard stored directly with my model as well as the fairness and so now i can show this other stakeholders or compared to previous models as i decide whether or not i want to take this into production back to you rohan thanks sarah it was so nice to see all of these machine learning tools come to life with the lone scenario just to add more color this demo is very similar to what eva is doing with these toolkits they're using failure and extensively to mitigate unfairness in models they've been able to reduce their disparity by more than 90 which is amazing and many more customers are using these toolkits to build responsible machine learning practices i want to take this opportunity to invite the community to get involved contribute to the open source projects powering responsible machine learning capabilities create scalable impact with the objective of having ethical ai practices and solutions all these tools are hosted on github and already have hundreds of downloads within the last few weeks if you want to try them in action please use the azure links to try them within azure machine learning or azure data breaks with that i want to thank oh wait did i forget something well i must let you in on a little secret this one about my presentation environment i'm sure most of you think i'm in front of a green screen here and there are some graphics people doing their magic in the background we thought of experimenting with ai a little bit and putting it into the use just for the summit first let me show you where i am this is not a formal studio it is a normal living space where i just came in to record the stock now let me show you how it was converted to a presentation stage for me we use kinect and the azure connect dev kit to bring ai and ar together to simulate the stage here's an architecture of the solution that produced the video for this talk amazing isn't it we use normal camera images and depth sensing kinect camera images to replace my actual background with the stage and design of our choice i thought about a beach in hawaii but opted for a bit more presentation friendly option this is just an example of how together we can stretch the boundaries of ai the solution has been published in a github repo linked here for you to use for your next meeting or video call again thank you for the opportunity to share i hope you have a great summer please stay safe 