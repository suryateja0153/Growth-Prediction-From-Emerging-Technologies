 and let's jump right into the next generation data science workspace and I'd like to start with some old news if you're attending this conference you probably don't need any more convincing the data and the I trend has been transforming all industries for a couple of years now almost all major companies are realizing that these investments lead to competitive advantages now if you ask me I don't know what those remaining twelve fortune 1000 companies are up to and as a result the job market has been playing catch-up to meet the demand three out of the top eight jobs in linkedin's latest emerging jobs report are data and the eye rolls congratulations to you if in any one of these categories I'd say you have some pretty good job security ahead of you now one thing you will notice is that there's a diversity of roles needed to deliver on these strategic initiatives it's not just one abstract data and the AI engineer solving the world's toughest problems really is a team sport and you need all of these functions working together as a team and not in isolation of course at theta bricks we call this unified data analytics and we've built a platform for that collaborative data science workspace becomes the focal point of such a platform it needs to cater to all of these users needs and bring them together today I'm going to introduce you to the next generation of our data science workspace an open and unified experience for modern data teams but before we do that let's look at where we're coming from and we've already come a long way in simplifying data and the I to help data teams in a bit faster on the left here you can see a MapReduce program to count words you should probably remember this just in case it ever comes up in an interview of course writing this code snippet is just the first step back when this was still cutting-edge you would still have to worry about setting up a cluster writing a couple of more lines of configuration and hope you get it all right thankfully today you don't have to worry about any of this in data breaks you just ride a spark sequel query and hit run the cluster in the background all the skills for you and you just sit back and wait for the result now of course this is a very simple example but it extrapolates to the full complexity of data engineering at scale but as mentioned this is only part of the solution where are we when it comes to data science today when it comes to production izing data science projects today we're dealing with a big mess if you've ever tried to do statistical data analysis or maybe train a machine learning model you know that the tools that are supposed to make a life easier are still very difficult to use in fact according to one study only about 15% of those Fortune 1000 companies have deployed the AI capabilities into widespread production the reason of course is that the tools available to these companies specifically in enterprise software haven't kept up with new emerging practices in data science and machine learning and as a result we are made to choose between three bad options that are all not that great the first and for many the most natural option is to just give everyone the freedom to do whatever they want on their laptop of course data scientists love that you have full freedom to install anything you need and you can move fast however you're pretty far away from your data you'll need to down sample and copy data onto your laptop and of course you don't have to sit in compliance to know that moving sensitive data onto your laptop is generally a bad idea and the folks who are maintaining your production systems are definitely not going to be happy to try to reproduce your local environment to address some of these concerns some vendors take the approach to just put those same tools to use on your laptop into the cloud essentially they're giving your virtual laptop now however just hosting two pa'dar and giving you a VM with scikit-learn or tensorflow pre-installed isn't that much of an improvement sure you no longer have to copy data onto your laptop but aside from security and governance there are no obvious benefits just more constraints and finally you may be asking yourself the question why not iterate on our production infrastructure directly well unfortunately those production hardened systems are not really ideal full exploration and most data scientists will not be happy if you try to teach them kubernetes so you're left with a hard choice full freedom of a laptop slightly worse experience with the same tools in the cloud or a fully production hardened system that no data scientist will want to use thankfully we're in a 21st century and with a little customer obsession and engineering we've been able to navigate these trade-offs our solution for modern data teams starts with the premise that developer environments need to be open and elaborate if our workspace follows opens our standards and provides a collaborative notebook environment on a secure and scalable platform next the industry has already figured out best practices for versioning and CCD and delicate pastes so we integrate our platform with this ecosystem and provide those best practices to data engineering and data science where reproducibility is becoming more and more important finally to reduce the time from experimentation to production the same environment can be scaled to production deployments allowing you to manage the full lifecycle within one platform and bringing all of this together I'm extremely excited to announce the next generation data science workspace on database and without further ado let me walk you through some of this innovation step by step before we give you a demo this is a screenshot of the current workspace navigation in de topics it brings together all of the components you need for collaborative data science notebooks clusters jobs models and access to all of your data at any scale for those of you are familiar with this interface you may notice something different we're introducing a new concept called projects to the left navigation panel one of the most common ways that data scientists start to work is to clone a kit based repository with projects you can bring all of your work to data breaks where you can access all of your data and use best-of-breed status or open source tools in a secure and scalable environment because participate based you can keep them in sync by pushing and pulling changes of course you can also switch between branches or create new ones this basic functionality provides you with a powerful set of capabilities to integrate your data books workflows with your CSID automation and then enables you to follow best practices when you move from experimentation to production and you don't have to learn develops tools now many of our customers have been waiting for this part it sweeter and I'm happy to announce that it is available in preview today now we have many more exciting features coming and I'll give you a quick sneak peek of those as well at the intersection of kit based projects and Environment Management is the ability to store your environment configuration alongside your code this integration will allow us to automatically detect any environment removing the need for you to worry about installing library dependencies yourself and you know sometimes saying it just works is the most powerful statement and in this case we make it just work following the same behavior you're used to on your laptop we give you an environment that matches your environment specification and make it available consistently on all workers on an auto scaling cluster so now that your environment is all set up let's look at your code you can already import iPads the notebooks - data breaks this allows you to convert you to paternal books - data picks notebooks and vice versa in the future we will store these notebooks in their native format removing the need for conversion this not only makes data picks more standards compliant but it also enables us to support alternative editors so if you want to we allow you to open these notebooks into pa'dar right here in database and similarly by the way for the our users among you we also support our studio however as mentioned earlier just providing you with cloud hosted open-source tools is not quite good enough so by default we will open these notebooks with the data picks notebook editor the data picks notebook editor can open to paternal books and in addition provide you with collaborative features like co-presence as indicated in the top right of the screen and real-time crediting as indicated by a colored cursor and to facilitate collaboration even more data picks notebooks also allow you to comment and leave comments for your colleagues all in one cloud-based environment now by allowing you to open to paternal books in the data picks notebook editor you will no longer have to make the trade-off between using standard formats and the collaborative features and benefits that database provides so in summary this is our solution for modern data teams I showed you how we provide collaborative notebooks based on open standards integrate with the kit ecosystem for collaboration and reproducibility and provide integrations with C ICT systems for a robust workflow from experimentation to production all on a secure and scalable cloud platform but you don't just have to take my word for it so let me introduce Lauren Richey who's going to give you a demo thanks comments for the introduction let's jump right into the demo for the purposes of this demo imagine I work at a big retail company we used to do our forecasts on a quarterly basis it's a big effort for our data scientists to come up with those forecasts using many different tools and once that is done we print them to PDFs and send them out by email that has led to lags and decision making because we use outdated forecasts of course these days the world is changing at a rapid pace and our leadership team has asked us to move from a quarterly to a weekly basis that will significantly improve the quality of our business decisions like how much inventory to order the amount of manual work involved in producing this forecast is prohibitive in doing this more frequently so as a good data scientist I'm determined to automate this process and provide our decision-makers with an interactive dashboard that always has the latest forecasts ready first I'm wondering if there's a better tool for those forecasts so I've searched google for Python forecasts libraries because I know that there's a lot of innovation and I find this library called profit which is open source by Facebook I read about it online and heard good things so I'll check it out of course there's a lot of time examples available online so I found one and forked into my github account here as you can see it comes with a data set and a jupiter notebook that shows you how to create a forecast this is great usually the way I go about this is to take an example like this and try to recreate it just to make sure it's not outdated so I click clone to get the repo URL because I'll need to in a minute so here I am in my database environment which we call the workspace in the past it would have been pretty difficult to get code from the git repository into data bricks however as Clemens mentioned we now have this new feature called projects which allows you to easily clone a git repository when you click create project you provide the path to a git repository so I just pasted the URL that I just copied earlier into this text box when you put create we clone this repository and make it available in your user folder as you can see it indicates which branch you're on and when you click into the project you see that all of the files were clones so the first thing that I'll do is to create a branch because I don't want to mess around with the master branch because that will be used to run our production job I open the get dialog and I can just start typing a branch name into this text field now I click on create branch from master and I'm ready to go as you can tell we're trying to make the most common workflows super easy without having to leave this environment now that I'm in my future branch I'll click on the jupiter notebook and you'll see that we open it in the data breaks notebook editor in addition to supporting standard formats this editor gives you several collaborative features that we'll highlight as part of this demo so let's get started data Brooks provides you with a scalable compute backends I can attach a notebook to an existing cluster or create a new cluster let me attach this notebook to this cluster called ml cluster that's already running now usually I would have to worry about the environment that is set up on this cluster and the libraries that are installed but with the integration of the projects feature and our runtime that's running on this cluster you will see that we automatically detect the presence of this requirements text and as soon as I run any cell in this notebook the cluster will make sure that the environment matches those requirements so what's happening now is that profit is being installed in the background you already preinstalled many popular libraries like pandas and numpy and we adjust their versions if needed so let's just run this entire notebook and see if it works as you can see this reads in the CSV file from the data folder and loads it into a panda's data frame the file has two columns one for date and one for the historic values of the column that we will try to forecast in this plot you'll see that we have data up until 2016 and then we forecast for another year after that this is great because that's usually how you get started find an example and make sure that it works however this is just using toy data and we have lots more data on our actual stores so let's see if we can adjust this example to actually scale to our needs I don't actually know where our sales data lives so I leave a comment to ask my colleague to help me out here okay let's see if he's online great coincidentally it looks like he's online and ready to help you can see he opened up the notebook from the indicator up here which shows which users are present in the notebook okay looks like he responded and he created a cell so I'll assume he'll just shared some code ok great he's actually using koalas koalas is an open source library developed by data bricks that provides the exact same API s but uses spark in the backend to scale computation this way I won't have to change any of the other code the data frame is still named DF and it should just work so if I run this cell you'll see that we're running a spark job in the background and hand you back a dataframe this is now different from the toy example before we have an additional column that indicates the store this data is from in this case we only have three for now San Francisco Amsterdam and New York of course I don't want to generate a forecast across all stars but I want to have a forecast for each store individually so instead of just running all of this through one big forecast I grouped the data frame by a store ID and apply the forecast as UDS what will happen is that we'll run a spark job group the data frame and then for each store we'll run this forecast and I can also delete all the other cells because we won't need them now the only thing missing is that I want to write the forecasts out to adults a table to actually use it in my dashboard I don't just want to run the forecast every time someone wants to look at it so we add this code snippet it takes the forecast and stores today's date and the store ID so that I can query by that later when I hit run will bring up some spark jobs in the background that crunched through those data and write out my predictions now this is great I just clone an example I found online data brings configured the environment for me and I could easily change it all to scale all of my data by using koalas and writing out forecasts so Delta now as mentioned earlier if for whatever reason you want to use Jupiter you can right-click on this notebook to open it with Jupiter right here in data bricks unfortunately the collaborative features you just saw are not available in Jupiter as a side note of course you could go back and forth between the two editors whenever you want okay let's go back to the data Brooks notebook editor of course I've been working off of my feature branch here so now I can go and check that code and to get I open the get dialog provide a commit summary and click commit and push we won't show this in the demo but usually you would go through the typical CI CD workflow to create a PR get it reviewed and merge it into master and here we set up our get automation to automatically check out the master branch of this repo in this production folder whenever a new PR gets merged from here I can just open up the notebook now this is the master branch version that I want to automatically run you can click on this calendar icon which allows you to schedule this notebook as a date effects job let's configure it to once a week and this way I'll automatically get the new forecasts written into my Delta table I'll click OK and we're done here now this is a full end-to-end lifecycle of experimenting with code in my own future branch checking into gates and pulling the master version into a production folder that is used to run a scheduled job let's quickly take a look at the dashboard that I put together in this notebook you can see that I use a feature of data bricks notebooks called notebook widgets I can just create two widgets that give me the female it's available on the Delta table those widgets will update whenever I get new data and then the data that is shown is automatically filtered by my selection I can create a dashboard from this notebook that embeds the table and visualization and also integrates the widgets to control the parameters of the sequel query so when I click present you will see that the version of this dashboard that I'll share with our decision-makers here they can select which forecast they want to see for which store all updated and without having to write any code and that's it for the demo so just to summarize in this demo I showed you our support for the native Jupiter format and how the data breaks notebook editor provides collaborative features like co-presence Co editing and commenting we saw the project-based git integration and how easy it was to start by cloning a git repository and creating a new branch for development and finally we productionize the forecast by pulling the master branch into a production project scheduling a job and creating a notebook dashboard to share the latest results with our business stakeholders now in theory I could even update these forecasts every day simply by scheduling the production job to run daily that's a massive improvement from the quarterly cadence that we were used to and with this I'll hand it back to Cummins Thank You Lauren for this amazing demo I hope that everyone watching is as excited about the next generation data workspace as we are to learn more check out data picks calm 