 [Music] thanks so much for joining us today we are here to have a really great conversation around AI and responsible use of AI I'm gonna start I'm Tracey frame the director of strategy for cloud AI I'm gonna start with just a few slides a little bit of history a little bit of future a little bit of everything but and then before we then have a conversation I also just want to make sure I introduce our fabulous panel that we have today last we have David not who is chief architect at HSBC Jay AG mech who's the VP at Google ai and fellow and then we have Nick Maguire who is the vice president and head of enterprise research at CCS in sight so you're going to be hearing more from them in just a couple of seconds so I wanted to start because part of a question that I get all the time is around how come the mixing of things like ethics and technology and the reality is these things have been mixed for quite a long time really since its inception because the story of AI actually doesn't start with technology at all the story of AI really starts with philosophy with thinkers like Lowell and Hobbes and dick Hart all of whom really wanted to apply order to thinking you know you can see dick hard really wanted thought to be as systematic as something like algebra or geometry and there was this vision that in the future machines could help with this sort of ordered thinking you could see some of this actually in where we take inspiration from biology and this is really the basis of neural networks is in fact the neurons in our brain this diagram is shows how the the neurons in the brain and they send out all their signals through the dendrites those are sensors right this was inspiration in many way for the Turing machine which is a interesting concept if you're not really thinking about how some of these things come together to really make AI what it is today from the 1950s into the mid-70s right around my birth time we had the both hype to AI winter so this is an image of the perceptron that was built in 1958 which you can't see in this screen is a 20/20 array of camera lenses and this was these were the camera lenses was meant to be the eyes this thing got so hot that you could fry an egg on it it was larger than a sub-zero refrigerator but it didn't work the goal here was to create a machine that could perceive something you could show it an image of something and it could then tell you what that image was because this didn't work there was then really no funding for neural networks for almost ten years this started to change in the 80s with new advances in physics and math folks life Jack geoff hinton who's now at Google and really understanding things like back propagation and this really started to change the game and bring us to where we are today and what might be possible with AI today in the 1990s up through the 2000s the big change that happened was a massive improvement in computing power so everyone here has probably heard of Moore's law what's crazy here is that it really just continues and we're actually going to talk about that in a moment and how that is specific to machine learning one of the things that that computing power has enabled is for the technology to get far more sophisticated so in 2011 the field of computer vision had about a twenty six percent error rate in contrast humans had about a five percent error rate fast forward just five years to 2016 and at that point computer vision has a three percent error rate in many ways more capability than the human eye that is a massive change in a very short time period and this brings us in many ways to where we are today where AI is now capable of doing so many things and that makes it even more important that we spend the time to really think about how we are going to use it in our organizations when I think about the rise of the Internet and how much that changed everybody's lives and I went to college at a time when the internet didn't exist and if I think about trying to do that again now it is pretty much impossible and when I think about what I've seen from what a I can do for enterprises for consumers for the world at large we really are on the cusp of something very very similar coming back to Moore's Law for a second there are over 90 machine learning papers published per day it is outpacing Moore's law so the capabilities here are growing at an enormous ly rapid clip and that's been the case for Google as well so over that let that time period of the last five to seven years Google has really transformed itself into an AI first company which is something you hear from us a lot about how we see the world it is really now infused in almost every part of Google's business and has made some of our most powerful technologies that everybody most enjoys possible because of how much machine learning has become a part of what they do ai has incredible capabilities to do phenomenal things for the world some of my favorite examples are things like being able to find manatees sea cows from a very high altitude and if you've had a chance to look at what some of those images actually look like when you're taking a picture of the ocean it's impossible to spot that manatee and yet machine learning can do that this example in the top in the middle is of a group of Japanese farmers who created a cucumber sorter using tensorflow so this is not a limited to the world of enterprises who can hire data scientists it is really available for it to everybody and this is something that we spend a lot of time thinking about in cloud AI and something that we're really excited about the more products we offer the more enterprises who work with us and the more that we can really do with companies around the world ai has the potential to really help humans make more responsible decisions so in 2015 and this is an ongoing partnership this example is from 2015 specifically we worked with the Geena Davis Institute we reviewed 200 films and analyzed them for both speaking and screen time for men and women really interesting things came out of that study the first is that women LED films made 16% more at the box office and yet men were both seen and heard twice as much as women on screen so you can think about how this can help really drive responsible decisions and create better business value for organizations because those organizations could be making significantly more value should they have better equality and that would then help set the stage for everybody to have the potential to see themselves in the future in media sometimes issues with bias in machine learning can really also help highlight how bias can be quite systemic so this study was of three million English words from public news sites from from Google News and it generated he-she analogies so for example if you were to use if I were to say to you Paris is to France as Tokyo is to Japan and if I said King is to man as Queen is to right so you would expect to hear Japan you would expect to hear woman and this is measured by the distance between words and so what this study did was it looked at those distance between words and then applied he-she analogies to those so for example you would say she is to sewing as he is to and the answer that comes back is carpentry she is to housewife as he is to shopkeeper she is to nurse as he is to surgeon so this can really help show just how pervasive and systemic some of those biases are and that gives us the ability to take a look at that and say okay that's not exactly what we mean how do we fix that one of the challenges with machine learning is that these mistakes if they happen can propagate very very quickly so if for example a there's a in these two images at the top it's not perceptible to the human eye but when you miss label it as a fish you can see what happens that suddenly then all of these images of dogs get labeled as fish instead and these are the kinds of things that if a mistake like that happens it can happen at great scale and with great speed and so it's very important to be cautious part of my job for cloudy eye is to look after the process we've built to support building our products in alignment with the Google AI principles the rest of my job is around how can we make cloudy AI successful and that's around how can we make it so that enterprises around the world really love our products and want to use them over and over and over again I see these two things as intimately tied because really when AI is going to be successful it's because it's going to be trusted and in order for it to be trusted it has to be trusted by organizations by their employees and by all of their customers and so for me a large part of what is going to make AI successful for any organization is when it's responsible last year Google released our AI principles the set of seven things that we believe a I should do and four areas that our applications that we said we would not pursue ourselves and this is something that we spend an enormous amount of time really diving into on a regular basis and ensuring that all of our products are built in such a way that they are fulfilling as much of this mission as we can and we spend time together looking at all the different things we can pull in to ensure that we have alignment with air principles and the products that we offer some things that we think a lot about when we talk to customers are around taking a human centered design approach and we have a number of resources available for how we might do that we talk a lot about how important it is to really know your training data and not just what's in the data but to be thinking outside that around the sociological effects around some of those systemic issues because without thinking about what is showing up in the data before it even gets into your systems it's impossible to really be thinking about unfair bias in a way that is really robust in order to help with that having interpretability and explaining ability making sure that machine learning can be understood both what's in the data and the decisions that it's making and this is an area where we're spending a lot of time you can see some of the outcome of this this was an example from Google Translate with Turkish where instead of making an assumption that he is the one who is the doctor offering different options between he and she but even this is something that is not enough because we you know that is not a gender-neutral experience and so how do we then expand on what we're already doing in as we learn more because the reality is this is not something that anybody can get one hundred-percent right this is something that we all do everything that we can and we learn as we go and we are always learning from our customers from our consumers from each other from all of the exciting research that's available this you can see also in something like smart compose where again some of those systemic biases showed up so a internal research scientists discovered that when typing I am meeting an investor next week the next suggestion was do you want to meet him and so instead of making that assumption for somebody in smart compose smart compose doesn't use gender-based pronouns in their suggestions at all and until we are in a situation where we may be able to actually accurately predict exactly what that person is intending that is something that would remain biases and data can come from a number of different sources they can come from bias data distribution from representation and from labels these are all things that are really really important to take into consideration as you are building and deploying machine learning models one of the things that we've made available is the what-if tool that allows you and I we won't show this video here today but I encourage everybody to find it online but this allows you to take different viewpoints and understand how a model is reacting from from different characteristics as we built our governance process in cloud around how we want to support the AI principles there were a number of things that were really important to us to ensure that we built for and one thing that's not listed on here but is actually extraordinarily important is that it shouldn't be dependent on the people who are currently undertaking this process so for me it's very important that this is something that lives on at Google because it is rigorous enough that it doesn't require me or the folks in who are currently in cloudy I in order to know how to go about doing it so it was really important for us that it be very very rigorous in gauging that it's efficient we're all very busy we want to make sure it's a good use of everybody's time so that when we have the conversations everybody has done all of their homework already and is ready to sit down and discuss what's at hand that they're effective that it leads to tangible outcomes and improvements in our products and that they're balanced in understanding both the needs that are out there as well as the requirements of any business we talk a lot about how technology is most powerful when everyone can use it but really at the end of the day it's even more powerful when everyone can benefit from it and this is the vision that we are trying to accomplish within cloud AI and with that I would like to turn it over we're going to have a discussion with these folks here so sit back down I'd like to start with both David and Nick as Jay and I were talking about this panel and preparing for it Jay had an excellent question which is that you know from within Google we are experiencing AI from the demand we hear from customers and it would be great to get a sense from both of you about the on-the-ground reality what is it like to be experiencing the AI wave happening both from within an organization and from an analyst perspective okay so thanks very much Tracy so um I think the way I reflect on it is to say that I think we experiencing HSBC is a large traditional Bank somewhat differently than you experience in Google you know we don't do original research to develop new ground-up technology for for AOA so we're a consumer rather than than an inventor what I'd say is that consumption is is kind of turning up to us in three ways at the moment firstly it's turning up through platforms like Google cloud platform so so that has done a huge amount to enable us to have a place where we can put they operate on it at scale and have access to a bunch of trainable models through api's that dramatically lower the bar for us in terms of skills and capability I think that why I didn't think of about 10 years ago we would have had to hire a bunch of PhDs you know six or seven years ago would have had to either PhD students four or five years ago who defied hats are really red-hot world class III engineers and now we can retrain our developers and engineers to to consume AI and build scratch to the cloud platforms are making a huge difference to us the other two categories we're seeing though are we seeing apologies to any sales guys in the room yeah there's no salespeople here the you know I get sold a lot to be Foxit the bank I get to make some choices about what we buy and what we don't buy so lots of people want to sell things to me I'm seeing a phrase that I learned from Nick III washing my tooth brushing we're products that have got some degree of modeling or analytics in there have kind of had the idea I label stock across them and if you are setting that please stop doing that if you have real AI built into your platform I'm all ears but please make it real so I'm seeing and I think that's the indicative of there being some hype in the marketplace right now then the third category I'm seeing is the consumption our direct engineering consumption open source frameworks I think our perception is still that there's a been a huge advance in the platform space you know places we can come and build models test models validate models cuz you may I I think the domain-specific space you know solutions that are specific to financial services is still quite thin so we're having to build that ourselves we don't mind building ourselves again if you're out there on your product company and you want to do something interesting then solving a bunch of problems in financial services and bring them to us as consumable solutions would be would be great then my final thought is that I still think we have a big I'd say we have two problems ones an education problem ones and imagination problem the education problem is I think that for most people in large enterprises that again different demographics of Google in Google you're mostly engineers and technical people people with a technical background and people who don't understand though I will feel compelled to find out what it actually means technically on the ground most of the people working for our company aren't technical people and don't necessary have the propensity or the skills to go deep in sir means so AI is just a term and they will see what happens in the press or what gets presented them as sales people so it's a pretty vague term that could mean a lot of things so grounding you know how people to ground themselves and what actually means of what it could do for a company is an education problem the other problem is the imagination problem where I think actually quite often people when you explain to them what what the current generation of AI really is it almost they feel slightly disappointed is little magical away it's not some big brain in the cloud is their models and algorithms and data and there's still hard work to be done so then the imagination comes in which says you well how do you put together that core predictive capability and use it to solve business problems so we're tackling both those things but I think it's still really early days we are enormously excited about the potential and all space you know banks all about data l space is a rich opportunity space for the deployment of AI we're really just getting started yeah I'm you know it's fascinating listening to David speak first of all thank you very much for for having for having me on the panel you know as I'm privileged in the sense as a research analyst I get to sit in between the kind of maelstrom of customer requirements and researching customers and speaking to customers around AI in ml on a regular basis we also do quite a lot of survey work as well get up to seven eight hundred companies across Europe in the US every year and we also look at employee attitudes as well so we can we can kind of cover that space but we also sit in between the big cloud companies and have their views of the universe and really settling all that down is is really kind of speaks a little bit to to what David was saying I mean what we see out there first of all is that you know our statistics tell us anywhere between kind of 60 and 70 percent of large businesses that we survey around usage of AI and ml have some form project research or proof-of-concept these types they actually a percentage of the Marcion production grade with certain applications as well so and we've seen that grow quite substantially over the last over the last couple of years so there's a lot of interest and there's a lot of thirst for knowledge and there's a lot of gravity obviously and many of you here in the room will attest to this around AI in ml at the moment but I would classify the market is still very experimental we see a lot of companies that are you know just starting to learn they're starting on small projects they're picking use cases where they can kind of start small get quick wins iterate and learn and go beyond into into that into that arena but you know what we're starting to hear more of the ore companies saying look you know what how do we take these kind of small experimental engagements and how do we think about a world where we've actually embedded machine learning capabilities into our organization into all many business processes and knots maturing the requirements that we're seeing in the last 12 months in particular when we ask customers ok what if you're you know what are your real barriers and requirements for for AI inside your organization and you know 12 months ago we heard a lot of Ron's get to the point that David's making we heard skills challenges we heard you know make the tools simple we heard use cases like I want to be able to apply this to my business you know we heard data problems in terms of preparatory work and so forth those challenges are still there but we're now starting to see our companies asking questions about risk and a lot of the questions and a lot of points that Tracy was raising are bringing up questions around ok how do i and how do I address security how do I handle compliance in a world where this is embedded into my operational processes and I think most of all probably the top question we're hearing now in the industry and I think the the the hyper scale cloud companies are coming to this now is around transparency and explain ability how do i lower my dependency on blackbox AI and how do i actually create and have instrumentation into my tools that allow me to start to think about how I build interpretability and more importantly to David's point it's not necessarily about interpretability to the data science teams or to the develop teams which obviously are leading projects at the moment it's more about the business how do I get the business engaged in supporting the technology and I think those are the questions that companies are grappling with now which is slightly different than say 12 to 18 months ago you know it's interesting one of the things that's making me think about is we've we talk a lot about how because AI has really only been able to be used by a relatively small part of the population for the last 60 years that's created a situation where there are really a lack of enterprise examples because organizations that have been able to leverage AI can do that because they've been able to hire in that scarce talent and then it becomes something where it's very proprietary and so nobody who's on the outside because those solutions have been built bespoke in-house has any sense of exactly what's happening and in hearing you talk you know I'm thinking about how that's actually now the experience both at a macro and at a micro level and it's making the requirements and need for AI to be far more explainable even more important because you need to be able now that you can start to understand how to use it if you're still unable to understand what it's doing and how it's doing it and if it's doing it in a way that you feel after you've assessed is is rigorously ethical and has is being deployed responsibly if you can't understand those things how can you trust it so - J you know you've been at Google for 14 years and I would love to hear and I imagine others would love to hear as well how have you seen machine learning change over that time and particularly with regards to issues of fairness and bias sure have we talked about that little apology in advance I've come down with a little bit of a cough perfect day to have it you know when you're speaking with an audience so if there are brief interruptions about it only there were a robot that could don't make him laugh yes so you know we were just joking about this earlier back when I joined Google about 14 years ago we could actually fit everyone in the company who had some background or spoke machine learning AI computer vision pattern recognition in a room in a room much much smaller than the room like this so it's been great to see that journey and I kind of compare it to phases of Technology adoption you know there are various models out there but all of them roughly fall in this four or five gays phase categorization which is that you have an early adopters phase followed by late adopters early majority late majority and laggards right and there they follow this kind of a normal curve of sorts and when you are in the early adoption phase the kind of questions that get asked the kind of projects that people find exciting is very different than the phase after that in the phase after that and this has been historically true for almost all the fields and I think machine learning is no exception in that regard you know back at Google you know whenever we used to have leadership gatherings I remember a decade ago people would walk up to me and their favorite question would be you know when would like computer vision work for real or you know name your sub sub field of AI when would that become real and then starting a few years ago when people walk up to me in these gatherings they usually come up and say here's Google photos it misclassified this picture it didn't catch the dog and to me like looking at a ten year period and how the nature of the question changes is actually really good reflection on what's happened in the field underneath right that we effectively went from taking this as maybe it will work someday sounds interesting into the actually curious too as you enter into the kind of late adopters early majority side right this question about quality reliability security Transparency all of these things you know it seems like they come out of the blue but actually no they're completely expected with regard to the adoption curve here and at Google we have tried to anticipate a lot of this you know the reason we came up with the I principles is we felt that as an organization that was in some way leading the way in in this field that what we set out as the operating principles as well as what we draw awareness to will have transitive effect kind of elsewhere and if you look at different aspects of the principles we draw out things there that we feel we really have to make progress on in order to make this early majority late majority part of the curve which means that and machine learning will gradually start to become critical part of societal processes like be it companies be it governance decision making and so forth and there all of these things become really important and that also explains why if you look at ml fairness it didn't it almost didn't exist as a field ten years ago and it started to become a field five years ago and it's kind of rapidly on the rise and I fully expect that about four or five years from now it will be a big field with kind of more mature cadence of its own so that's kind of the long-term view of things David something that many people may not know about you is that you have a background in philosophy and a PhD in FX can you talk about how the combination of technology and philosophy has helped both you're thinking about ethical implications of AI and then how that's impacted HSBC yeah thanks very much so I think if I follow Jays lead and say for the number of people who've studied ethics academically and studied I could probably so I think the way so the way is impacting my perspective is that it brings to me forty-two concerns and I hope so my first concern is that you know you showed your graph of the speed at which a is developing you what the Moore's Law curve philosophy does not move on that curve most of philosophy is having arguments with dead people who reigned their arguments over 2,000 years ago so you know I regularly have arguments with Aristotle so so that is not just often most of us don't live our lives as philosophers but that's the simple that's an indicator of the speed with which our general ethical thinking moves it does not move at the same speed as technology's changing at the moment so my first fear is the AI and other technologies will outstrip our ability to respond ethically to them the second fear and this is this it definitely comes from my background as both a 30-year veteran of doing enterprise technology background as a software engineer and as a business person and as a philosopher is the in engineering and in business and in architecture which is wanti for a day job we are problem solvers and solution builders and optimizers and we look to optimize outcomes and that's that's our normal mode of thinking at a normal mode of approaching problems and my belief is that it's not the right way to approach ethical problems is not actually normally a helpful way and just to illustrate this if you indulge me for thirty seconds there's there's a famous philosophical problem that gets written about a lot at the moment in the field of a I'm particularly in the field of autonomous cars many of you probably heard who's heard of the trolley problem there quite a few people again ten years ago probably no hands would have gone up so those of you who don't know the trolley problem is remarkable in San Francisco's is one of the few place in the world where this actually is still the trolley problem the cable car museum is amazing if you haven't gone yeah because most most places don't have trolley cars or cable cars anymore so the idea is the thought experiment is your inner Street is a trolley car coming down the street it's an out of control further down the street unaware are five people and they were absolutely will get run over and killed by this out-of-control trolley car fortunately you're standing by switch and if you pull the switch you can divert the car onto another track but the problem is there's one other person on that other track and if you divert them they will it will definitely kill that other person and normally when you presented problems were engineers at this point nobody starts inventing solutions and ways around it and could I throw myself from the car so you can't change the thought experiment you had to kill five people or you act and kill one person now I'm quite pleased as a thought experiment is in the wild now people are talking about it my worry is that I think people are approaching it in the wrong way they're approaching this is an engineering problem you can see lots of people telling up and go okay I've got this thought experiment now five greater than one say five kill one good job done right now I design my autonomous car and now I just need to know this number is bigger than that number the points that's not the point of that thought experiment the point of the thought experiment is the maximizing is maximizing is not a simple thing to do choosing to pull that lever and kill the other person carries ethical weight and it's not a simple thing it's not straight numbers game so the thought experiment is designed to express the complexity of ethical thinking but my worry is that right now is turning up and people are treating it as if I could just solve this problem figure out which number is larger than the other then we can go on and design our models so that's my second concern my hope is we're having this conversation oh so you know this is an ir ethics a our responsibility session in its full house that tells me that people are genuinely engaging with this topic and engaging with the topic thoughtfully so that is the first step some of the other analogy I have is with unconscious bias training in helping people to understand diversity and their own biases they carry unconscious bias training does not remove unconscious bias it just makes you aware of it currently because it's kind of part of our knowledge II ethics training conversations about AI ethics do not solve all ethical problems at least they remind us of war those of us washed imagine lob people in this room are trying to build solutions that we are operating in a context with ethical weight you're the things we choose to do have ethical consequences so the fact we're having the conversation gives gives me hope and then finally to answer very briefly your question on houses of HSBC so I've tried to have this go I have had this conversation HSBC we have in developing our own set of ethical principles being thoughtful about the fact that the banking equivalent of engineering is kind of compliance and you could write a set of compliance type rules that people could comply with and the problem with that is that would treat this as a solved problem and just be people set of rules they could follow the point for me is this is not a solved problem by any means so our principles the same as the Google principles already starting points for engagement and thoughts and again that gives me hope so that's the way we're trying to approach it and just to put a point on that I'm just curious how many people in the room have tried to create a long list these are the things that are okay and these are the things that are not okay from AI and ethics standpoint right so it's not a small number of people who've tried to do that because it's very tempting and it's it's really attractive to have that nice list that can be just a decision tree from your organization and to your point it's impossible and that so it is about then the conversations that you have and what you do about the conversations that you have and the issues that surface from those conversations and how you apply ethical thinking to those that's where you can make different you know if I can jump in on that I mean one of the things that I am seeing and it is to your point it is an iterative and it's an ongoing conversation but the ethical question these are macro kind of societal questions but down in kind of the traditional enterprise solving a specific problem with ml I think when companies start to think about governance to the point that you were talking about the the whole process of principles comes in and I think what one of the things that we've said and we've seen some companies kind of say as they look if if we can start with our company values and we can start to design things based around are first of all a risk profile you know and secondly the values of the organization that can get us moving into those into those areas I think you can get stuck way up here and I think that that is an ongoing discussion that's absolutely critical and fascinating but the company you know principles and values of the organization that's always a blueprint it's interesting to see now with yourselves and some of the other technology companies actually putting out their principles which are linked to their company values as kind of indication and help for companies doing that as well so just a thought in terms of how we've seen because we haven't published our principles you have actually principle number one exactly in accordance with our values because we were very strongly values driven organization look this is novel technology and it raises some novel moral problems actually grounding and our value stays the same yeah a very long time we're not shifting from them just have those in mind when you're making choices about how to deploy there's new technology yeah it's one of things we did I mean I think Google is quite clear on its core values and that's something that you know we as a group that works within Google feel strongly about but even ask how they I we took on a process to understand what are the core values of the individuals who work in cloud AI as an organization and and when when we do that what does that mean how do they relate then to the core values of Google and just to ensure that there wasn't any mismatch which good news there wasn't but the you know being able to have that also to then build our own processes on top of was extremely helpful right because it was specific to us the group yeah just a couple of days and what I'm hearing I mean I really agree with David on the part about not rushing to come to a spec for compliance because what the behavior that entails is that it gives us a false sense that we actually know exactly what we are optimising against right and whenever you have a compliance spec like any logical system optimization will optimize the system right up to the boundary condition mmm and then so debate on how do you move the boundary conditions I think we very much have to treat this as an open evolving space and don't over specify in places where we shouldn't that's right which is uncomfortable that's an uncomfortable space to be in for because there isn't an end but I think there's also an opportunity which is G the second part of what I want to say is that with Ian learning entering so many domains this is the first time that we are going to have a data-driven sense for what the lay of the land is and I think on equal parts caution I think there's an equal parts opportunity where we know that there are things that happen in society that aren't you know fair in some you know statistical definition of fairness or in congruence with our values but there is an enforcement gap between the two which in absence of a data-driven system was really hard to bridge and I think as a and machine learning starts to enter these systems we have a chance to actually make that bridge happen I think it's an opportunity in equal parts as well so Nick you talked earlier about some of the things you're hearing from customers particularly around explain ability and transparency are there any sort anything else that you hear that we should all be thinking about that customers are asking around responsible use of AI anything that we might be missing or it's just really top of mind yeah I think when we think about responsibility first of all it is becoming a very important aspect now of thinking within companies but what what what I think is coming to is actually organizations having a process for governance and more importantly I think quality assurance in terms of not only the technology but the process in terms of building ml based applications inside the organizations and that those really fall under kind of three key buckets of activity and you know two of which are really you know really really important today I think the first the first one of course is this topic of fairness and designing with principles you know right from the beginning and I think that's still an ongoing challenge the second one is this point on transparency and what's good about the notion of explain abilities to watch the level of which the technology and I know this is an entirely a technology-based discussion but the work that you know yourselves at Google are doing it's gonna be fascinating to see to the point that you made earlier in you know 12 18 months or sorry I think I think it was you Jay that made this the the instrumentation that will be that we have the disposal of companies when they're building out these applications that they can actually have you know interfaces and performance monitoring and user interfaces that can actually help them with a lot of that interpretability into into those applications now bear in mind that you know from what we see is companies don't necessarily think that you know specific use cases aren't gonna have full-scale requirements for explain ability and obviously explain ability itself is a subjective topic but what it means is certainly for as companies move to that operationalization phase and there's a lot of dependency on the technology you know think of banking and think of healthcare and so forth that actually there's going to be a need for some of that some of that some of that explain ability it's gonna be challenging as well based on deep learning and I know we're gonna see more recent owns a lot of research coming in terms of you know building more transparency into the way in which deep learning models operate and I think that's gonna be fascinating to see as well and then the last year is this whole area around security and I think this is something that we're gonna see down the road you know you mentioned the point in adversarial AI you know how do you protect pipelines how do you protect models of course the the infrastructure underneath so for me the what companies need to now start to think about is embedding some of these areas of governance that are all kind of coming together now responsibilities explain ability you know governance are all kind of coming together into one kind of topic area not in isolation and embedding that into into the design process you know before the first line of code is written right I think that's proper quality assurance that companies need to start to think about and one last piece on that is to think about governance not as something that we traditionally thought as it slows down innovation or it's a process that's mundane think about it because these systems are so iterative and it's an ongoing process it's actually the opportunity to to fine-tune performance and execution you know and it's part of this whole AI assurance and quality that I think is becoming more and more important as well apparently I missed the blue and black memo I'm just realized sorry I didn't anyways so you know one of the things that I wanted to ask Jay about is that a common response that at least I get when people start to explore issues particularly around unfair bias is well you know it actually we've solved that for our for ourselves this is not Google speaking and that instead we just don't measure things like race or gender or any of the sensitive subject aristokraft we're good how would you respond to that as a approach I think that's a very risky proposition to take I mean even in absence of air and machine learning there are plenty of societal examples globally on kind of not measuring these characteristics not leading to fair outcomes in in a grand scheme of things and that's because you know with any system whether it's run by human decision making or data-driven kind of assistive decision-making and there are always you know surrogate features correlated things that will effectively start to correlate with things that you are not measuring right and could then become factors through which bias starts to creep into the system again as I was saying earlier I think the opportunity here is that if we do measure these things we now have an opportunity to actually be rigorous against these characteristics in terms of truly measuring how bias these systems are and having a mathematical ways of controlling that bias you know eliminating it you know as the goal of the system and this doesn't have to happen in the overall notion of you know sometimes when I hear this debate it ends up as are we implicitly going to a quarter system right is that what fairness is about and actually it's not you know the kinds of fairness control that these algorithms will gradually start to deliver we're already seeing the first wave of those is for example ensuring that the behavior of a machine learning model that it's quality is sustained across these characteristics so not enforcing a precise code to our distribution on the outcomes but enforcing a quality bar that stays consistent and is uncorrelated with the sensitive characteristic that starts to say that kind of every data point every decision whatever be the context is treated with the equal amount of can accuracy in the system which is a very different control then trying to enforce quota level policies so I think as this field evolves more and more of these granular controls will start to show up and I think this is where there are opportunities where if we have a conversation on responsibilities I effects and the algorithms going hand-in-hand I think we have a new road to pave that can actually be really beneficial for society and so sort of extending from that if you had to name one thing in in fairness research or any of the m/l fairness work that you've seen that you're most excited about what would that one thing be that's coming down the pike let me make me named - okay one that is kind of already here in terms of the pipeline people who are building these systems could use it today and one that's kind of coming more and more from the research phase I really like the research that deals with post hoc calibration of machine learning models with regards to fairness outcomes so this is where imagine you're building some machine learning model for prediction and you want to ensure that it's predictiveness the predictive accuracy and calibration isn't correlated to some sensitive characteristics you're measuring there are several algorithms out there now fairly stable well you can take a pre trained model and a bunch of data with these characteristics label and recalibrate that model so that it performs fairly in fairness as measured by the definition I gave earlier across this so this means that you know even if the way in which you went about building the model had some biases creeping like to the extent your population is well represented you can undo those biases through a calibration process later on right which means that it's a it's a lightweight way of you know anyone building this ml systems to get on this exercise right as a first step and then move up from there so that's on the ready side what I'm really excited about on the research side is some early work at the intersection of fairness and semi-supervised learning where you know typically the hardest part about building these systems is getting data labeled across these sensitive characteristics and the amount of that data typically is what holds us back in a gooda system we could build there is early work that shows that a combination of little bit of labeling and massive amounts of unlabeled data we can start to get at outcomes that are very similar to having large amounts of data labeled and as those algorithms get more mature I think we are going to see it the next wave of fairness based workshop so just to wrap up here so you know David you spoke about HSBC writing its own AI principles and you know I'd love to hear from you what would you say to organizations presumably there's organizations in this room who are embarking on this process for themselves lessons learned things that you would suggest not doing doing more of any of those things and you know Nick I'd love for you to chime in on this as well from your experience okay thank you so I think first thing we've already said don't write the decision tree I think the best advice I can give is probably embrace respect and humility and what I mean by that is that this is a multidisciplinary exercise so to come up with our principles we engage people in technology team from the legal team from the communications team from the people who feel about customer outcomes from our risk team from our compliance guys because the end the day the regulations we comply with are really there to protect customers and protect the integrity of the financial system we operate so we try to treat this as a multidisciplinary exercise not just a technology exercise not just a business exercise not just legal exercise or compliance exercise and we all try to turn up to develop the principles with respect for the other people's expertise and self respect for our own expertise because you're turning up as experts but also humility in the face of the earth expertise that people had people had to show so I think that's that's the right way to approach it and then I think the other the other thought is anything you come up with is a start point not an end point and and you need as you design your iteration of that in your application of that you need to build in the continuous recalibration of the principles because as I said so the earlier humans are slow to develop the ethical instincts they need to respond to new technology this is a conscious exercise to try and accelerate but we won't always get it right you know I come back to what you highlight at the beginning and for me and what I'm seeing is that companies now paying much more attention to build a trust trust into systems and you know when we see when we survey companies and we actually actually when we survey employees independent of senior IT leaders the biggest barrier that employees say you know this is from an enterprise point of view not necessarily a consumer adoption point of view but from a business enterprise point of view the biggest barrier they have with AI in the workplace and the enterprise is trust I mean they just do not fundamentally understand or you know have you know an understanding of the you know where their privacy comes into play or you know with the accuracy of these systems you know how is it going to prove their life etc so getting adoption is a really important one and Trust is a key part of that so I think part of it is to start to consider these processes and this topic you know right in the design phase you know the challenge that companies could have had without doing this is that they have to they have to approach it afterwards and I think that could be those problems will get exacerbate because this technology moves so fast if you don't do it in the design phase that'll be my main recommendation we are over time thank you very much for joining us today thank you so much thank you [Applause] [Music] 