 hi everyone thank you so much for joining my session uh the importance of model fairness and the interpretability in the ai system i'm francesca lazari i'm a senior cloud advocate at microsoft where i lead a team of data scientists ai developers to build the end-to-end solution on azure specifically during this session we're going to talk about how you as a data scientist or a developer can build the end-to-end responsible machine learning solutions we have been writing a numerous article around this important topic the latest article that you can find is called the machine furnace i put a link there so you can check it out so there are also many um resources that we are going to talk about during this session some of these resources are open sources so you can see that we put several different github repo such as interpret ml github but also the fair ai furler and ai github repo so you can check it out all of these links and resources after this session and again if you want to talk more feel free to ask me questions so the agenda for today is going to be divided in in three main parts so we have the first part where we're going to talk about what's with the responsible ai what we mean with the responsible artificial intelligence and then we're going to look at the two main packages one is called the interpret ml package and the second one is called the defer learn toolkit as a you know we are in a moment of the history where we are all leveraging data in order to making the significant decisions that really are going to affect individual lives in different type of domains such as healthcare justice finance education marketing uh but also hr employment so it is very important for us and specifically for our customers to ensure the safe ethical and responsible use of the artificial intelligence we know that ai has the potential to drive bigger changes in the way we do business and on the other side like all uh great technology technological innovation in the past is also important that to keep in mind that this type of technological innovation is going to have a very broad impact on society as well um for all these reasons uh we think that is important when you build ai solutions and machine learning algorithms uh to ask yourself the following questions so how can i as a data scientist and developers design build and use ai systems that create a positive impact on individuals and society another important question for example is how do we best ensure that ai is a safe and also reliable how can we attain the benefit of a high but also respecting privacy so these are all open questions that developers and data scientists have and at microsoft we have building different tools that you can use and leverage in your existing solution or to build a new solution in order to assess the furnace level of your machine learning application uh moreover we also are seeing that many customers are fusing the same challenge there are many different reports here i'm putting a recent uh capture capturing a report that showed that in nine out of ten organization we know that they are facing uh ethical issues in the implementation of the ai systems so this organization cited actually different reasons including the pressure to quickly implement the ai the failure to consider ethics when implementing their system and also the lack of resources dedicated to ethical ai systems at microsoft we build a disa system that is a sort of a foundation to guide our thinking as a data scientist and we defined six ethical principles that ai system should follow so as you can see from this slide the first war are fairness reliability and safety privacy and security and finally we have inclusiveness so these are really key properties that each ai system should achieve the second two are transparency and accountability these are somehow underneath all of the other principles and guide how we design implement and operationalize the ai systems so let's see what we mean by transparency and how we can implement it so let's start with a transparency what do we mean actually with the transparency so we mean two main things the first one is that ai system should be explainable and also that ai system should have the algorithms that are um accountable meaning that you can actually understand why they are producing specific results there are a few cases of other machine learning interpretability we can define them following two different categories as you can see there is the first one that is a model designer's evaluation so this is more at the training time and the second one is the end user or providers of a solution to end users so these are at the inferencing time this is more the time where they are consumed your ai applications so there are many different uh use cases that are important to keep in mind for both categories like data scientists needed to explain the output of a model to stakeholders usually these are business users and also clients in order to build the trust another very important and popular would say use cases when data scientists need the tools to verify if a model behavior matches the pre-declared objectives um finally also data scientists need to also to ensure the fairness of their trained model uh other application and other use cases that are more like from the inferencing the time category are when again your ai predictions so the results that you get from your ai application need to be explained at the influencing time some of the most popular cases are in the healthcare and finance industry so like why a model you classified fabio like a customer id at risk for colon cancer another important question from the finance industry that we receive very often is why a specific client in this case we call her a rosin was denied a mortgage loan or why his investment portfolio carries a higher risk so these are all questions that somehow you have to know how to answer and specifically you have to know why you got you got the specific results so that's why um at microsoft um we developed the interpretability toolkit so here is a toolkit that really helps a data scientist to interpret and explain their model then we put together these a tool the toolkit in order to explain a machine learning models globally meaning on all the data or locally on a specific data point using the really state of art technology in a very easy to use way second we wanted to incorporate the cutting hygiene interpretability solution that are developed by microsoft but also leverage all the open source community so the solution so also this aspect is that is very important and finally we were able to create a common api and also data structure across the integrated libraries and integrate these with the azure ml services interpret ml is a diesel toolkit that you can find at aka.net interpret ml toolkit really gives you access to the state of art interpretability techniques through an open unified api and also provides you a lot of visualization that you can use in order to understand better why your model is predicting specific results so in with this toolkit you can understand the model so using the wide range of explainers and techniques using a really type of interactive type of the visuals you can also choose algorithms and experiment with different combination of the algorithms you can also explore as a data scientist a different model attributes such as for example if you are more interested in the performance or the global and local features and you can compare again different models multiple models at the same time so also this is very nice in order to find more information and you can again look at this github repo and also remember that you can run what if analysis as you manipulate the data and view the impact also of your model so why did we start this project in the gita repo you will see that the interpret community was able to extend interpret an open source python package from microsoft research that was used to train interpretable models and helping also to explain a black box system in just a few minutes we are going to see also what we mean with the black box system and so this was a so the interpreter community was able to extend this interpret capability with additional interpretability techniques and also utility function to handle also i would say a real-world data set and a workflow so with this package you can train the uh interpretable glassbox models and explain a black box system and also you can use this package to understand your model global behavior or to understand the reason behind each individual prediction as you can see azure machine learning is an sort of raptor so we call it azure ml in terms and it's really a wrapper because it helps you save explanation in run history remote and parallel computing for explanation on azure email computer so this is an additional capabilities that azure ml can offer for you and also is able to create the scoring explainer for you and most importantly if you want to push your model into production that can operationalize these explainers for you in the github ripple we will also see that there is a what we call the interpret tax bills on interpreter we have added extension to support the tax models [Music] and there are two different type of models that are supported as you can see there is what we call the glass box explanations these are for example explainable boosting linear models decision tree rule systems and we have also a black box explanation like lime sharp partial dependence sensitivity analysis uh the black box models are challenging uh in order to understand for example deep um deep neural networks so blackbox explainers can analyze the relationship between input features and output predictions to interpret models so as i mentioned in my previous slides some of the examples can include the lime and a sharp as well so talking about the shop let's take a closer look at it so sharply is a game theory approach to explain the output of any machine learning model so it connects the optimal credit allocation with the local explanations using uh what we call the classical shipley uh values from the game theory and also their related extension so let's see together how we can actually apply shuttle to a real sample machine learning use case so let's consider a black box that predicts price of a condo or an apartment based on all these features so as you can see there is a proximity to a green area such as a park and also the fact that the building itself is a pet friendly or not in this case the feature is a negative so with this in mind with these features our model predicts that the average cost of the apartment the average price of the apartment is 300 k how much has each of these features contributed to the prediction compared to the average prediction as you can see we have a different information such as the house price prediction that is about 300 000 euro we have the average house price prediction for all departments and this is about 310 euros so the delta here is negative is a minus 10 000 k um so these um are the simply values so as you can see we have a different uh features and let's start with the parts so how the parks contributed to these um result so we have a plus 10k and then we have the default category also contributed in a negative way 50k so the fact that the building itself is not that friendly also the size of that part of the apartment is a very important feature in this case and we see that he contributed that actually to um 10k and then we also see that there is a final feature that is the fact that the department is at the second floor they had a very neat contribution so these final attributes this final feature actually was not really impacting our model results so how uh did the uh calculate or how did actually sharply calculate all these values so um we will take uh features of interest uh for example cats ban and we remove it from the feature set second we take the remaining features and we generate all possible possible collision and finally we add and remove your feature of interested to each of the is a collision and we calculate the different uh the difference that he makes so this is really how sharp works so this is really the logic that is uh behind the shop of course there are some pros and cons that it's important to keep in mind when you decided to use a shop for example a shop is great because this based on a solid theory of uh and also distributes the effect in a very clear way um however on the other side you produce also contrastive the explanation what they call like explanation that some sometimes even instead of comparing a prediction to an average prediction of the entire data set you could compare it to a subset or even to a single data point but in terms of what are the cons for short computation time is uh it's possible that for example you can use like a 2k more or less possible collision of the feature values 4k features sometimes it's difficult to understand it so it can be misinterpreted and finally the inclusion of the unrealistic data instance of when the features are related is also some is also very possible so it's a risk that you should keep in mind if when you decided to use a sharper so as i said there are also different models that you can use there are different also interpretability approaches based on how you want to use these different models so in terms of the glass box models these are models that are interpretable due to their structure example are explainable boosted boosting machines linear models and also decision trees glass box models produce a lot lossless explanations and are editable by dominic's expert which is something very very nice to have when you want again to uh to use and leverage these uh glass box type of models um so in terms of the glm so this is a generalized linear model as you can see this is a flexible generalization of an ordinary linear regression that allows for responsive variables that have error dissolution models other than normal distributions so as you can see um the main characteristic of the generalized linear models is that are the current standard for interpretable models and also that they learn an additive relationship between data and response another example is the explainable boosting machine so here you can call them also ebm so this is a sort of the interpretable model that that has been developed by microsoft research it is a very interesting model because it uses modern machine learning techniques like bagging gradient boosting and also automatic interaction detection to improve the traditional uh generalization models these this is why actually um the um expendable boosting a machine are very accurate as and they are considered like a very good techniques like for example the random forest and also gradient boost and trees so in this second part of the presentation we are going actually to focus on fairness um we are going to see what are the different fairness principles which aims to tackle the question on how we can ensure that aics can treat everyone in a fair ways fairness has a main goal to provide the more positive outcomes and avoiding the harmful outcomes of the ai systems for different groups of people there are different types of harm as you can see from these slides broadly speaking i would say that we developed these different types of arms based on the taxon taxonomy that microsoft research created and there are five different types of harm that you can see in a machine learning system and while i have the definition of all of them in these slides for the scope of this project we'd actually just focus on the first two of them that are allocation as you can see this is the arms that can occur when ai system extends or i would say with all the opportunities resources for information to specific groups of people and then we have the other one which is a quality of service this is whether a system works as well for one person as it does for another person so the example of the face recognition from many different applications is probably one of the most important example of the quality of the service for this uh fairness part microsoft developed a new toolkit that is called the fair learn this is a new approach to measuring and mitigating unfairness in systems that make predictions serve users or make decisions about allocating resources opportunities or informations there are many ways that ai systems can behave unfairly for example ai can impact the quality of service which is again weather system works as well for one person as it does for another and also ai can also impact a location which is again there are harm that occur uh when ei system makes sense or with all the opportunities resources or information to specific groups of peoples so as you can see in the um in the toolkit that again you can find more at ak dot msl learn ai in these toolkits that they there are different type of focuses and different type of i would say capabilities so the main goal of this toolkit is to empower developers of the artificial intelligence systems to assess their system fairness and also mitigate any observer fairness issues most importantly it helps the user identify and mitigate unfairness in their machine learning model so with a focus on group fairness so now let's jump actually on a demo i want to show you how you can how you can use the interpretability to keep them so in this demo we're going to see how you can use the interpretability toolkit for tabular data in azure uh databricks um as you can see uh we are going to see what's the um toolkit that you can use and download uh for the explanation results from the explanation experiment and also visualize the future the feature importance uh implementing advanced analytics solutions in every organization and for each of our customers is i would say for a different step process so you first need to ingest the data from a different variety of data sources including batch and streaming data and as you can see there are different options here as these architecture shows us and then the most important part is that of course you need to take in and store uh disparate data that's being ingested regardless of the data volumes variety and the velocity we here you can do it of course with different type of products when you get into the prep and train uh stage you can use again azure databricks that just to train and deploy to your model so as you can see uh in databricks we have an option that is called the runtime now that includes a variety of popular libraries the libraries are updated with each reason to include the like new features so the database as a designated subset of the supported library as a sort of top tier libraries for these libraries azure databricks provides a faster update cadence updating to the latest package releases with each runtime release so this is very good for data scientists as well in terms of data set so we are going to use these breast breast cancer wisconsin data set that is a public data set here you can see that there are different attributes that we're going to use for this demo and uh not only in terms of the id number and diagnostics which are probably the most important attributes but also there are real value features that are computed for each cells uh necklace that we are going to analyze for this specific demo so first of all uh you need to um next slide first of all you need to install azure ml interpret and azure ml control interpret packages next you need to train a sample model in a local jupiter notebook as you can see you can again use a breast cancer data set and then you can split the data into train and test third you can call the explainer locally here you need to initialize an explainer object pass your model and then do some training data to the explainer constructor in order to make explanation and visualization more informative you can also choose to pass in the feature names and output classes that are names in output the class names for example if you're doing a classification uh these codes that you see in these slides actually show you how you can instantiate and explain their object with the different types of examples here specifically you have a data block spliner and pfi explainer uh in a local environment then if you want to explain the entire model behavior you can call what we call the global explanation so this is going actually to give you a sort of a visualization that you can again leverage to understand and interpret better your models so some of these visualizations again are produced from your python code just using these packages and i want just to show you some of the visualization that that this package can create for you as you can see um there is here an overall view of the tree model along with its predictions and explanations so we have the data exploration this displays an overview of the data set along with the prediction values then we have the global importance these aggregates features importance values of individual data points to show the models overall top k these are of course configurable type of k so you can change that number these are important features and also helps understanding of underlying models overall behavior then we have the explanation exploration so this demonstrates how a feature affects the change in the model prediction values or also the probability of the prediction values and it's a very good visualization if you want to show the impact of a feature interaction finally we have the summary importance so this uses a different individual features importance values across all the data points to show the distribution of each features impact on the prediction value by using this diagram you can investigate for example in what direction and the future values affects at the prediction value um another way to understand better what your model is actually doing is by using the local explanation you can see that here you can get the individual future importance values of the different data points by calling explanation for individual instances or for a different group of instances here we have a different type of visualization that are created first of all we have the local importance this shows at the top k important features for an individual prediction and it's very helpful when a data scientist wants to illustrate the local behavior of the underlying model on a specific data point then we have a day perturbation exploration it's a sort of what-if analysis as you can see this visualization allows changes to feature values of the selected data point and observe resulting changes to the prediction value finally another important visualization that i want to share with you is called the individual conditional expectation this visualization allows a feature value changes from a minimum value to a maximum value so it's very helpful when a data scientist needs to illustrate how the data points prediction changes when a feature changes again this was just an overview of what the interpretability toolkit can do for you and how you can leverage that on your um solutions i want also to share additional contacts of the product team who work in these um on this toolkit as you can see you can find their names and their emails there in case you want also to follow up offline with the product team again who put together all these toolkits that i presented today um again this is a one of the articles that you can use it to learn more and also to find some of the resources that i have been using uh used today during this session and in terms of the resources i just want to share those with you one more time and these are all the packages and the github repo that have been used during this session and you can also find me on twitter github and medium thank you very much 