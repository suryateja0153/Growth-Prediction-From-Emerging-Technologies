 machine learning are transforming the way that we do business but as we become more dependent on this cutting-edge technology we need to consider how to build positive outcomes while preventing unexpected results to talk more about this emerging responsible machine learning please welcome our esteemed panel we have willam we have hannah wallick and we have manuscript thank you so much all for joining uh on on build live thank you thank you for having us of course of course so i don't want to introduce you because i have i mean if i was introducing you it would just be each one of these people is amazing and that's just all you need to know but what i'd like you to do is uh each introduce yourself so willem do you want to go first sure yeah i'm willing i'm uh i'm from the netherlands i work as a technology advocate for infosupport and i typically help uh customers get started with ai machine learning data platforms this sort of stuff that's what i like uh and in my spare time i run the global ai community which is a very very large group of people across the world that are doing all sorts of cool things around ai trying to teach each other about machine learning about ai stuff and and yeah generally coding that's what i love fantastic and hannah can you introduce yourself yeah absolutely hey everyone i'm hannah wallach i'm a senior principal researcher in microsoft research new york i'm a machine learning researcher by training i've been doing machine learning for about 20 years now so way before this stuff was fashionable and nowadays most of my work focuses on issues of fairness intelligibility stuff like that as they relate to ai and machine learning i run microsoft's internal ether working group on fairness and i'm also a contributor to the fair learn toolkit that was released this week fantastic and we'll get to that very very shortly um but fears in the noosh can you introduce yourself for us as well absolutely my name is marine sumeki i'm from microsoft boston or really cambridge and i'm a senior program manager driving the product efforts behind two of our open source responsibility offerings interpret ml and fairlearn fantastic so yeah for my descriptions we have an absolute community hero in willem uh hannah just blows me away every time i see her speak about the amazing stuff she does around and ethics in ai and maneux is obviously pulling research into actual product so i just want to thank everyone for joining us we have a little bit more time than normal to quiz you all or or hear your perspectives on responsible ml so i would love to start because we briefly mentioned it in some of the introductions about some of the announcements that have happened at build so minute can you tell us a little bit more about responsible ml but also the part you took from the azure machine learning team yeah absolutely so as you mentioned we have had some major announcement in this conference which is pretty exciting we do have a two particular open source toolkit interpret ml and fairlearn one of them the first one is for helping you understand your models better how did my model makes its decision what factors went into the decisions of my model and that can be on the overall or global level of overall how this model is making its predictions or from the individual level so for this particular person what factors went into his or her loan getting rejected beside that we have a fairness assessment and unfairness mitigation toolkit that was also announced in this conference that is called fairlearn which has been based on the awesome research from hannah and her team mira dudek and basically that particular one is due to the fact that we understand that ai systems can behave unfairly and there are multiple types of harms that can arise and this particular toolkit is to enable you to really understand and assess the unfairness that is happening in your model and of course then a set of mitigation algorithms to mitigate that unfairness now we were aware that lack of access to actual practical tools in order to develop and deploy responsibly basically artificial intelligence responsibly has been one number one blocker uh for a lot of executives a lot of data scientists a lot of machine learning developers to really get get ai right um and that's why besides releasing them in the open so that everyone can get benefit of them and everyone can join the effort co-develop with us we also integrated it with azure machine learning because we only want the best for all of our azure clients to have access to this state-of-the-art set of tools to really put a lot of control a lot of understanding analyzing around their machine learning life cycle i know i was gonna say i i love this in azure machine learning i love this in the sdk um and i i just haven't even had the chance at the moment to go in there and take a look so i'm pretty excited and starting next week's first thing i'm going to be doing um but it's it's really really interesting to see that it's kind of coming out of research and then straight into our products and big fan of azure machine learning so love love love that it's in there and i guess actually that probably segues us very very nicely to hannah um hannah you you mentioned you've been kind of working on this open source framework fair learn and you also have a much wider scope of kind of ethics and fairness accountability transparency and ethics in um in your research space can you tell us a little bit about how the toolkit fits in the space and i'm assuming it's not a one and done thing there's more to it than this yeah that's right absolutely i'd love to talk a bit about this so one of the things that we've seen over the past few years is this growing realization that you know ai systems really can end up discriminating or disadvantaging already disadvantaged groups and in order to sort of really engage with these issues we have to approach them not just from a software perspective from a technical perspective but from a broader socio-technical perspective the reason why ai systems can behave unfairly is yes oftentimes because of the data or decisions that have made during the development and deployment life cycle leading to particular characteristics of models or systems as a whole but ultimately it's when these systems interact with the real world and the people who are using them or whose lives are impacted by them that we start to see that kind of unfair behavior and of course because we're dealing with this topic that is fundamentally socio-technical this means that we have to draw in this wider range of tools and resources than simply software so yes fair learners is incredibly useful for really understanding particular types of fairness issues with machine learning models specifically issues around sort of allocation of resources or opportunities and also issues around quality of service so does my machine learning system perform as well for one group as it does for another but when we start sort of digging into these kinds of issues and how they affect people's lives and maybe different ways that we want to try and mitigate them throughout development and deployment we have to use other things as well as software tools one of the things for example that my team has been working on is a ai fairness checklist and again i don't mean to imply that this is sort of a one-size-fits-all you know just use this checklist and you'll be done and in fact many of the items on this checklist are much more intended to provoke conversations and discussions and really to make sure that teams are sort of engaging with this space even though it is difficult and maybe outside the realm of things that technologists normally think about but we think that all of these resources together so things like the checklist things like fair learn um things like sort of broader even education in this space and helping people think about um these kinds of issues from day one of development is really the first step towards making progress another way to put all this is that when we're thinking about issues like fairness it's a bit of a culture change moving away from the traditional sort of tech first can i do this will it work all of that kind of stuff mentality and moving to more of a people first mentality so if i do this what will be the impacts on people on their lives on society as a whole and of course in order to make that kind of culture change you need a wide variety of resources of tools of materials and so on and so forth that's amazing and and i know you you you've been doing work in this for quite a while and so it's super exciting to kind of hear both the collaboration um into engineering but also that kind of statement of we're all on this learning journey right like the toolkit is going to just it's going to help us as those who are building models um to to hopefully rectify those mistakes but i love when you're saying like this got to be designed it's got to be talked about right from the start just like we say about security exactly it's a lot like security and privacy and that it's not something fairness is not something that can be left as an afterthought it is something that has to happen from day one and so it's super exciting to see microsoft firstly committing to this space and secondly to roll out a variety of different tools and resources to help developers really prioritize this one of our ultimate goals for fair learn is not to just have it sort of focus on assessment so we have this assessment dashboard component and then also this mitigation component these mitigation algorithms but to really expand this out into other resources as well like domain specific guides for when and when not you might want to use particular fairness metrics and also for when you might want to use particular mitigation algorithms we also want to see this expand to sort of handle other materials to help people think about other types of unfairness beyond just these allocation issues or quality of service issues so here for example we're really interested in moving to things like stereotyping and denigration how can we help our developers prioritize those they're much more socio-technical kinds of you know uh unfairness that allocation or quality of service which can be they're still socio-technical but you can often get more of a handle on them from that technical perspective but ultimately we want to be building out to really consider that broader space and help developers sort of navigate that landscape together it's also really exciting to see all this coming over research as well as a researcher myself it's just super exciting to see when things happen in the research world and then actually make it out to to the tools and the resources that we're putting out there for developers i was going to say it feels like we're hearing this more and more in fact i think three times today we've put we've spoken to people from microsoft research because they've so closely collaborated with azure with uh microsoft 365 or whatever it is so it is fascinating to see like this literally is cutting edge and immediately where can it be used right it's a lot more satisfying i have to say than just writing yet another academic paper it's really nice to see this stuff actually out in the world and people engaging with it and using it and having it make a difference that's amazing don't play down the academic papers i bet you've got a million of them so but um cool thank you so much hannah for that for that um initial insight in that space i wanted to turn the conversation a little bit to willam willem my bestie from the global ai community um so william leads the global air community for those who don't know um global ai dot community go there if you want to learn about ai this is the community to be a part of it is so exciting and we work super close with willem and his team um but willem what i wanted to know was you actually got a chance uh very just before build to have a look at these technologies uh that minute and hannah are talking about can you tell us a little bit about what you found yeah so i got a chance to try out fairlearn a few weeks before build back then it didn't have a lot of documentation so for me it was a uh actually reading the papers that uh hannah and others wrote so for me yeah that's that's the kind of thing that i like to do i like to try new stuff and especially in the field of ai it's very important that you learn to read those papers because they give valuable insights on how to use particular tools at fairlearn it's the same so i started reading the paper i started trying it out and very quickly i found one of my models was was sort of unfair towards female customers and it's it wasn't immediately visible to me because i i was very careful i i tried to exclude features like gender like ethnicity age all that stuff i tried to remove from my mod from my training set because i knew it would be sensitive there would be something that my model would pick up but despite the fact that i removed them my model still picked those features up and and that's kind of funny um uh when you're trying to do this kind of thing uh fairness is not something that you can can pick up right off the bat you know up front whether your model is going to be fair you have to train it first and then measure its fairness using fair learning then go back and try to improve that so it was kind of fun to do that that's amazing and obviously there is some documentation out there did you use documentation to kind of get started with that tech yeah so right now the website is up and running so there's a user guide that takes you through getting started with a couple of notebooks that explain step by step what you have to do to measure fairness and how to improve it afterwards and they also include a very extensive description of the api so once you work those through those notebooks you have sort of a sense of where you're going but afterwards the api will help you truly understand how to select certain features or what what all the options mean and it's really great that they uh they've been able to put up this website so quickly fantastic and obviously with uh there's other members right of your sort of uh main part of your the global ai community that they've got a chance to cover other technologies is there somewhere we can send our amazing audience to to go and read about them uh i don't have to link handy at the moment but we i'm pretty sure we can share the link afterwards um so we've got a couple of blog posts online uh that explain all the uh responsible ai initiatives we've actually looked at uh white noise uh so the differential privacy tool that microsoft announced we've looked at seal that's actually a tool that allows you to train models without actually seeing the data that's also pretty cool and pretty complicated um i've learned from sami who's been trying this out and we've looked at uh stuff around explainers as well specifically into nlp jobs um trying to figure out why is my model predicting a certain category for an email or a document or something like that it's pretty awesome um and and i'm pretty sure we can link uh cinderlink afterwards that's fantastic i was gonna say we do have comments to the right so maybe whether maybe we could pop there on my build.microsoft.com just afterwards and spam them with a few links just there um we have a really interesting poll going at the moment for from our audience and it is fascinating to hear to see all of the different types of systems that people are building with machine learning so we've got um pure prediction so maybe maybe more classical modelling and we've got quite a lot in the area of image a little bit in speech and personal assistance is an interesting one i guess it's been big at build with chat bots as well and actually thinking about that loco no code aspect and so yeah i just wanted to share that with you guys very very quickly um and then could we switch back to uh minus for a second actually here so i had a chance to um because interpret ml what is it is a github repository right all of this stuff is is open source it's available on github i had a chance kind of at the start of the year to dive in a little it's quite complex so brace yourself everyone but um yeah can you tell us a little bit more about it maybe i should have just come straight to you yeah um yes so basically when you land on interpret ml org under github you'll see a few repositories underneath that uh the main one is interpret so interpret holds a collection of what we call glass box models or in very simple words interpretable models they are linear models decision tree and of course one state-of-the-art machine learning model out of microsoft research called explainable boosting machine or ebm if you haven't checked it out you should check it out it is very very interpretable yet accurate and efficient so so that's one collection of interpretability techniques for tabular data that we have there also under interpret we have collected all the state-of-the-art basically black box explainers interpretability techniques that can explain any black box model what they need is the input features and the predictions they do not care what's happening inside the model they use bunch of approximations on top to infer how the model has sort of played the game on top of these features in order to come with that predictions so that's interpret if i want to put it short a collection of interpretable models or black box interpretability techniques for models trained on tabular data now another repo that we have underneath the interpret ml organization under github is interpret text we learned a lot about the need for expanding this repository to include text scenarios so our very first attempt was with text classification so exactly the scenario that william mentioned about basically classifying uh maybe an email to be spam or not spam what words contributed to this prediction of spam so that text classification scenario was our initial scenario that we started with and that's you that's where you find three different interpretability techniques under interpret text and then the other one is what we call guys or diverse counter factual examples and that is really really helpful for understanding and debugging your model so what it says is show me the similar um data points with different outcomes so marnutia's day alone has got rejected who is the closest person to manusc who has got approval on his or her loan it might be that oh amy is the closest person to my new same age same marital status same salary everything the same just her loan student loan is 10k less than manush that's why she has got approved and then using this counter factual points you can really see whether your model's decisions make sense for of course if my counter factual is someone the exact same feature said as me just gender flipped to male then then you know that there is a sort of a fairness issue that you need to go and investigate with fairlearn for instance so it's like a red flag to you so yeah these are the multiple ripples now in near future we have also interpretability for computer vision on our radar and we really want to expand our repository to include better debugging and air analysis tools nice nice i actually have i think quite an interesting question from the audience um and that kind of yeah i think manufacturing but you'll want to comment on this one possibly first but you know feel free to jump in everyone and what is the likelihood of introducing unfairness through actually tweaking your model and and introducing fairness in that sense uh maybe more unfairness i don't even know if that's a thing but is is that something that you that you're worried about or is that is there a way will this tool and help us to to clarify whether we're doing that or not yeah so basically with all of these techniques any you your model can behave unfairly so there there is very difficult with interpretability toolkit to really understand that fairness aspect of it but of course we have fair learn there um in order to bring a lot more understanding of okay this this change that i had happening to my model now take the outcome of the new model to fairlearn assessment phase and try to understand how that impacted the predictions for different demographics for different groups of people say females versus male versus people of non-binary gender so none of these toolkits is like magic for you to understand oh like exactly uncover everything but combination of interpretability and combination of fair learn with of course domain knowledge and context can really help you at least investigate and bring a lot more understanding around your model i know that hannah has a lot of thoughts there too yeah maybe i can just jump in there um just to say a couple of things i think you more or less covered it manush um i guess the the only thing i want to flag is that when you're working with any machine learning model it's of course entirely possible to make decisions that are well intentioned and intended to reduce things like unfairness but that might actually not help and i think willem's example of removing features like gender or race or age these kinds of features uh from your data set before training your model with the idea being okay this will get rid of unfairness this is you know then i'll be good to go is kind of a perfect example even with those kinds of modifications of course models can behave unfairly in large part because there are still signals about these kinds of things in other features as well and so that's why we thought it was so important to have something like the fair learn assessment dashboard that way people really can say okay look i think i've done something that's going to reduce unfairness but did it actually reduce unfairness according to these metrics the other thing i want to flag is that there's many different fairness metrics that you can assess your model with in that failure and dashboard and that's really important because these models all capture different aspects of fairness and unfairness you know fairness is this fundamentally societal concept and there's a lot of disagreement about what it might mean for a model to behave unfairly does it mean for example that the error rates should be exactly the same for different groups or perhaps does it mean something else one of the one of the things about fair learn is that we do have all of these uh different fairness metrics in there so that people can explore different ones and sort of say really how is my model behaving as a whole i love that i love that breakdown of kind of like interpretation and then and then fair learn here's the different ways that we can consider or are we building fair models is that is there something that i need to know something i need to do um so i guess a call to action here is just you know go read those docs get immersed in in what's going on with those dashboards and those interpretability kits because for anyone building these models i mean it's i guess it's innate in data science it's kind of a skill that data scientists have of um you know digging in and interpreting what's going on the data i sometimes make the joke and like the sherlock holmes of a data problem trying to like sift it down like figure it out infer things all of that kind of stuff so um i i love i love that you've really lined it up nicely um kind of splitting up the two the two different things and then um willem can i can i go to you for a minute so you've had a chance to look at the tech um do where do you see this being used in your day-to-day job is this something you're like god i've got to get on it straight away so yeah that's a good question um so right now customers are getting started with ml ops so they're starting to use azure machine learning service and similar tools to actually make sure that they have a reproducible model that's one of the other problems that we still have in the field that but we build a model and then we change the data and then we can't reproduce the results that we have before so that's that's what's happening right now and what i see um happening in the near future is that people start to wonder why should i trust my ai model an azure machine learning service in combination with interpreter ml is going to help people to understand why am i getting this particular prediction and might there be a feature that's extra heavy in this prediction that might indicate unfairness and then you can start to use uh tools like azure notebooks in combination with the fair learning dashboard to discover what is the unfairness in my model right now fairlearn is mostly a debugging tool for me as a as a data scientist and a machine learning engineer to take a look what's actually going on in my model why is it behaving this weird in this weird way so it's i think the combination of of envelopes on the one hand and then these tools on the other hand will only make things better um for the near future fantastic and i know if you can just put your other hat on for a second there willem or your community hat um is this is this going to be top of mind for some of your next community uh global ai communities virtual tours all of the amazing stuff that you do yeah so we've got a virtual tour planned on june 9. um during that tour we're going to explain more about fair learn about all the other tools that we've explored with the community and i've got a number of other things happening uh soon mostly online of course because of the corona pandemic that's happening around us um but sure yeah a lot of people are interested in this in these new tools and um i've got a lot of questions from people what does it mean uh to be fair what does what what do you think about ethics in ai um so there's sure a lot of stuff happening in the community fantastic and we have about a minute left on this panel so if i can do a quick fire round 10 seconds each of something and what's the key takeaway that you want people to take away for responsible machine learning um oh let's take one uh um i i just want to uh really promote that let's move away from treating machine learning models as black box interpret ml and further all started with the goal of bringing a community around all these toolkits and helping co-developing it together so definitely check us out and send us github issues sign up for contributing and we'll be super happy to grow this community fantastic hannah 10 seconds can you do it right yeah i think the main thing i want people to take away from this is that this is a complicated space that engages with broader societal issues and as a result software tools are not going to be a single one-size-fits-all solution you will need to sort of engage with these issues and learn more about them but we're starting to see a lot more resources out there to help developers really navigate that space and really understand it so fantastic i'm so sorry we have a hard stop on this session thank you everyone for more on how we can engage in responsible machine learning check out aka dot ms ms bill dash responsible ml next up a conversation to sure to inspire the technology and all of us it's our third digital breakout of the day this time scott hanselman luanne murphy who are inspiring next-gen code with make code go take a look a little bit of sleep and 