 [Ali Jadbabaie]: Okay, it's 2 p.m. So we're gonna start to stay on time. We have - welcome to the afternoon session. I hope you enjoyed the lunch. We have two wonderful talks and after the talks we're gonna have a one hour session as a business meeting to do further planning for future years. So, please do stay and it's a great pleasure now to introduce Professor Anca Dragan from UC Berkeley as our first afternoon speaker. [Anca Dragan]: Thank you. *applause* Hello. It's wonderful to be here. I work in human-robot interaction, and I find human-robot interaction really hard and fascinating and challenging, and part of it is because in interaction, we're trying to solve a substantially different problem from the typical problem we try to solve in robotics. So typically, what we do in robotics is, if we have some sort of task then we say let's write down some utility function, reward function, cost function, whatever you want to call it, and then the robot's job is to do something like find a policy that maximizes that utility and expectation. And so the moment this robot needs to do a task not in isolation but in an environment where there's a human as well also taking actions, now the problem definition changes fundamentally. Because now, I can no longer define a utility function that depends on just the policy for the robot alone. Now the robot's utility function also depends on the human's policy. It depends on how well the future actions that the robot takes and the future actions that the human takes end up meshing well together. And so for the robot to figure out what to do, it needs to start reasoning about what this human policy might be, and this is naturally, it's very hard to write down equations for what people do, and so naturally this is a place where we want to use data, we want to use learning, and then the question becomes how? What's the right way to incorporate data here, and so, if this is the space of all possible policies, let's say that real human policies are somewhere around on this slide. We don't know where they are, and so, what might we do? Well, we might collect a bunch of data and fit a policy to that. To do that, we have to commit to a hypothesis space, a class of possible models, and so because we have no idea where these human - a real human policy might be - it's very tempting these days to cast a very wide net, to have a very large hypothesis space of high-capacity models, and sort of, that way we make sure that whatever humans actually do, sort of, we're covered, we catch that, it's one of our hypotheses. The challenge that I have with this is that, what happens is I go and collect some data of human behavior, and then that data, if I use a class of high capacity models, will be consistent with many different possible hypotheses. There'll be many possible hypotheses, many policies that actually match, that are consistent with the human data. And this happens if I have a little bit of data, it also happens if I have a lot of data but it's all concentrated on some particular training distribution. And so then it's - chances are that I'm not gonna be lucky and learn this particular one, and then I'll be susceptible to things like covariate shift. And so, it'd be ideal to not cast such a wide net. I'd be ideal to focus the search a little bit when we fit a human model, and so the question becomes- when a robot does learning for human-robot interaction, what should this hypothesis space be? In other words, what's the right inductive bias to use when robots learn for human-robot interaction, and this is a question that my lab has been tackling for some time now, and to me, kind of, to start answering this question, we have to start thinking about - what might we think that, sort of, modeling people as following some sort of arbitrary, black box policy is not quite right. What is there about human behavior that's not arbitrary? So, to me, the answer lies in the fact that our behavior isn't arbitrary because what we do is driven by the stuff that we want to achieve. We have these things that we might call intentions and somehow, there's some magic, cognitive process that takes intentions and then produces the behavior that we exhibit, and this notion that humans have intent always seemed pretty fundamental and something that our robots should somehow be leveraging, be taking advantage of, but the problem is that this is a fuzzy statement, and what we need to do - and so these cognitive processes that turn intent into actions are a bit of a mystery, right now at least, and so in my lab we've been on a bit of a journey to try to figure out what's a useful way to take this fuzzy statement and turn it into actual inductive bias that a robot could use. And I drew it here as a nice stroll downhill, but you know, the reality of the matter was that it looked more like some sort of treacherous adventure with many kind of ups and downs, and I'll try to share a bit of a vignette of that today. Long story short, I don't know what the answer is. I don't know how to take 'humans have intent' and and say this is the right inductive bias to use, but I feel like we've come a little bit closer. So the starting point was to look at this notion of intent and formalize it. This is the starting point because I had no idea how else to start. To formalize it as some notion of utility. That is, to assume that there is something that people would ideally like to optimize for, and notice that I say 'would like to optimize for,' not that they actually optimize for it, but there's some utility that the person would like to optimize for this, parameterized by some parameters that we're going to call theta each in the stock, and the robot doesn't have access to these parameters. We're gonna assume that something like this exists, and so now this is not yet an inductive bias, this is just a statement. We need a way to link this to the actual actions we're assuming the person would take, and naturally to do this, if you look at economics, is to say - well, the actions that people will take will be the ones that are optimal of respect to this utility, but we know that people aren't quite perfectly optimal. We know there's a whole field of behavioral economics that teaches us some things, and so in practice a model that we end up relying on a lot is one of bounded rationality. One where we say that human actions- uH - depend on the parameters of d to the data h, but they're not necessarily the optimal actions in every state. They're - the human action is more probable the higher the q value of that action is with respect to d utility eta H. So this is one model of bounded rationality, and we'll come in a little bit of time back to it and and see where it actually comes from - but now, what's nice is that this gives the robot a way to anticipate what the person might do in the future at every step by assuming that the person is going to be approximately optimal, and instead of - and you can do that without having to learn the full policy by just learning - focusing learning on what's presumably a smaller space of candidate data ages of candidate intentions or utility functions that might describe the human behavior. And so this is, for instance, what we do here, this is in some work in collaboration with Claire Tomlin's lab, here we have Sylvia. Sylvia is wearing a t-shirt that nicely labels her as a human being for extra clarity. We have a quadrotor, and they need to navigate around each other. So they each have a goal - here's the quadroter and the human - and they each have a goal, and what you see is that the quadrotor is making these predictions about where Sylvia might be walking next, and making sure that it can actually stay out of the way, and so this is noisy-rationality at work. We know that Silvia has some sort of goal. We don't know exactly which goal that is, but the robot is making inferences over that, and predicting where Silvia might go. So - so far, so good. This works great for quadrotors. When we try to apply this to autonomous cars, it turns out we need just a little bit more. So, imagine that I have an autonomous car, and he needs to merge into the right lane in front of a person. So what happens when you apply this assumption of noisy-rationality kind of naively is that you'll predict that the car, the human-driven vehicle, will keep going in its lane and then the autonomous car would figure out 'well, if I go into that lane, then I'm gonna be intersecting with the person, that would be bad,' and so this leads to these autonomous cars that end up being a little bit too shy, a little bit non-committal, where they try to merge for a while and then people keep on coming and they keep letting the car come in and so on. And so, it was - if you think about human driving, I don't know about you, but - although, I hear some things  about Boston drivers - I don't know about you, but if I need to really get into a lane, what I'll do is I'll go for it because I know that the person behind will actually brake. So this teaches us something, it teaches us that human actions, whatever they are, are influenced by the actions that the robot would take. There's also this influence the other way around, which is interesting. So again, not sure about you, but sometimes if I'm in a hurry and I see someone trying to, starting to merge in front of me, I won't necessarily brake. I might actually accelerate, and that deters them from coming into my lane. And so the bottom line is that noisy-rationality and isolation is not enough, that the interaction is really game theoretic, right? That we actually mutually influence each other, and somehow we need to take that into account. By the way, another example where this game theory really comes out of me is in San Francisco, there's a lot of autonomous cars being tested, and so when I go home I usually inevitably, once a month or so, come at a four-way stop just a little after an autonomous car, And so it's the autonomous car's turn, but what do I do? I just go for it, because I know they're gonna hang out and wait for me to go, and so I get to get a little bit of a win against the robot. *laughter* So, the good news is that you can take the same notion of rationality and say it's not rational in isolation, it's not rational in the context of a single-agent problem, but it's actually rationality in a game theoretic sense, and so you can adapt the math to work with the game theoretic interaction as well, and that's what we found ourselves having to do. Now, the math works out just fine. What's challenging is the algorithmic side, where you have to start playing with approximation so that you can get these cars to make these predictions despite the continuous state and action spaces that they need to deal with. I could spend about an hour on this, and I would, but the point that I want to make is that we can use - we found that we can use tools from control theory and game theory to generalize this notion that's been pretty predominant, at least in my area of bounded rationality, to strategic interactions, and what we'll see here is a couple videos from what happens. So here's a car that's a robot, here's a human, and what's happening is that this car realizes that it can go in front of the person and then the person will slow down, then on the right hand side, we see that the car does this other solution where it tries to go in front of the person, but the person actually accelerates, and that's a prediction that the car makes, that this might happen and then it goes and merges behind. What you see here in these colors, the heat map, is the actual value function for the robot, and so we're starting to use these notions of - maybe people are approximately optimizing for something, but it's not in isolation, it's in the context of a game, to start being able to make these predictions in these highly interactive situations. Not just in these sort of simple situations in interaction, where you have a person walking and a robot that can stay pretty far away, out of the way, and that's fine enough. So on the one hand, I've been really happy with these ideas, because they've enabled us to do a lot in a bunch of different applications. On the other hand, part of me cringes every time I write down these formulas because essentially, if I'm being totally honest with myself, what I'm doing is I'm taking people and I'm saying - kind of throwing my hands up in the air and saying 'I don't know psychology. I'm not trained in that. I don't know how to deal with people. I've been training robotics, so I know a little bit about that, what the robots do. They optimize for stuff and so let's just kind of squint our eyes and pretend that that's what people do as well. Let's just treat people as if they're robots optimizing for stuff.' That's pretty much what we're doing here, and what I've been finding recently is that there's different degrees to which this works. So if you have a zero-sum game, this is pretty good as a way to go about the problem, because essentially in a zero-sum game where you have a robot, say, competing in a game of Go or something against the human. Well, sub- optimalities of the human actually work in the robot's favor compared to what the robot expected. So that's good. It works okay in general for some games like the course interactions that we were just talking about, but what we've been finding is that it's often times not such a great thing to do where you have a human and a robot on the same team collaborating, trying to do something together, and I don't know about you but that's what I'm excited to see a lot of robots be. I want robots that actually help us to do the stuff that we want to do. So, sometimes what we find in these - when we try to use these tools in collaboration contexts, is something like this, where here is - this is an agent trained for self-play So this is a robot that expects the person to work like a robot, and this is - in a collaborative task, this is sort of the reward that the robot expects to get. So this is the report that robot gets when you actually pair it with a rational agent, with another robot, and then you pair it with a human and it gets about half that. It's a pretty drastic difference, and then when you pair it with - so, what's even worse is that when you actually fit a black box model - given a lot of data off this particular task, there's caveats, but you can just fit a black box model and give that to the robot, and then when you pair it with a human it actually does better. So, this is really disappointing to me, because the whole point - remember at the beginning of the talk, I said 'well, black box models are really tricky because there's gonna be distributional shift, we're gonna have all these different problems, so let's find some inductive bias' and if the inductive bias is this notion of rationality, what we're finding is we might not actually be gaining that much. In fact, we might even be losing, depending on, sort of, the amount of data that you have access to, and so I've been thinking lately that this is a pretty fundamental issue, that with noisy- rationality, as much as we've benefited from it, it's also kind of kicking the can down the road. So this is a diagram of, sort of, you know, how a robot works. You feed in a dynamics model, you feed in some reward parameters, you run some optimal planner, it produces the optimal q value function and then you know what to do in every state, and with noisy-rationality what we're doing is, we're adding some noise up to here where we say 'eeh, what the person will do is not necessarily what the q value function thinks is the optimal thing to do but eeh, there's gonna be a little bit of noise' and we're doing that not because that's a good model of people, but because that's papering over some issues that are happening sort of earlier in this deck. And so it felt like it's kind of time to not to stop papering over things and to actually start fixing it, but the problem again is I don't know things about psychology, I don't know how to model people for real. I only know robots. So what did I do, I started thinking about what do robots suck at? When our robots really bad at - when do things go wrong with robots here as well? Because if something is hard, what - if it's a situation where it's really hard for a robot to be rational, maybe the same kind of thing happens with people, too. So let me give you some examples. One of them is that sometimes robots plan for a very limited time horizon as opposed to a full time horizon. It turns out people do that too.So if you incorporate that as part of how you think the person is optimal, that - now that explains the data much better. Another thing is, we're kidding ourselves in robotics if we think that we actually know the dynamics, the - sorry, the parameters, the reward parameters that the robot should actually be optimizing for. It turns out, that describes people too, that we people sometimes are still learning about what it is that we want, what we prefer, what we like, what we dislike, and that if we start incorporating that in, we start seeing some more progress. And then the actual example I want to get into today is - robots don't know the dynamics model. They have to still be learning about it. It turns out we humans also don't always know the dynamics model for how the world works, because these tasks are complicated, because we have intuitive physics models that are not a hundred percent reliable and so on. So this is a great example of this. We had people played this Atari game called Lunar Lander, where you have to take this aircraft - spacecraft, and control it to land it in between these two flags and if you look at human behavior here, it's kind of - it's tempting to say it's terrible and it's random. It looks really noisy, so maybe, you know, the noisy part of noisy-rationality is fine. But what we've figured here is that people aren't just randomly operating this thing. They're actually trying to do something and their actions sort of makes sense, it's just that they don't understand the dynamics of this aircraft very well. So their control inputs only makes sense for what they - how they think the aircraft works, not for how it actually works. At least, that was our hypothesis. We thought that maybe people look random, but they actually are approximately optimal, just for the wrong dynamics model, and we thought that instead of assuming that people are noisy-rational, maybe we should be really be assuming is that yeah, they're noisy- rational, but not with respect to the ground truth world dynamics. Maybe with respect to some other internal dynamics that they have in their head, and so what we did here, and this is some joint work with Sergei Levine and our students at Reddy is we mate the dynamics some free parameter, and then what we wanted to do is to run inference to figure out which dynamics model eta best explains the actions that we've seen the person take. Mathematically you can just do this, you can run this inference algorithmically - again, a little challenging because it involves being able to differentiate through the q value function. It changed dynamics, the q value function changes, and so in order to compute the maximum likelihood estimate, you need to be able to actually differentiate through that. And so the workaround we found for this was to kind of split this problem into two parts. On the one hand - so we made the q function an actual free variable - and then we said, on the one hand, we want to make sure that the q function explains the actions that we actually saw the person, so that's the right-hand side of this. On the other hand, we want to make sure that the Q function is consistent with the dynamics, and the reward parameters, and what we mean by consistent? Well, we mean that on sample states, it should have low Bellman residual. So the difference between the q value function now and what it is if you do one Bellman update should be low. And so this gives us now an estimate, having seen human data about what internal dynamics parameter the person might be assuming, and now what do we do with this? Well, if the person is commanding the lander and saying 'hey, apply control uH,' if we actually apply that control, well, you know, the lander would crash, it'd be pretty bad. So that would mean passing it through the real dynamics of how the real world works. So we don't do that, what we do is we don't listen to the person, we take their control command uH and pass it through this estimate of the internal dynamics that we've learned. So this gives us an estimate of where they think the lander is going if the lander were to actually execute uH, and so then what do we do? Well, we find a control command to execute by inverting the real dynamics such that the aircraft actually goes to the place where the person wants it to go, as opposed to the place where it's actually - where they're actually commanding it to go. And this is pretty neat because now when we do this - it's not that people are great, but they're actually landing the craft between the two flags and not crashing left and right. And this is not total proof, but it's some evidence that this hypothesis, that maybe people are more rational but with respect to some weird internal dynamics model, it might actually be true. And so we go from trajectories that look like this, where red means failure and green means success, to trajectories that look like this, where people are actually landing their aircraft properly. So, what have we done? Well, we said it's bad to cast a really wide net, because that makes us prone to all sorts of problems. Noisy-rationality took us to something that is way less flexible, perhaps we went from something that's too big of a space - hypothesis space - to something that's too small and wrong, and so, what we've started to do by taking all these steps like making the assumptions flexible with respect to the dynamics, allowing for the fact that the person might be actually learning about what they want, what they like and dislike, and so on, we're sort of - and looking at the strategic interaction, we're sort of finding ourselves broadening this assumption and broadening this hypothesis space while still being able to leverage the fact that there's this intentionality thing going on with people. Something that I worry about is that we'll never be done. People are really hard. So we can broaden this and try to fix it as much as possible, but we're also - will always encounter these situations where we're interacting with a person that really falls outside of this hypothesis space. And that's something concerning. Here's an example of this, where, let's see, that there's some coffee spilled on the floor and now Silvia instead of going straight to the goal is avoiding the coffee, but then what does the quadrotor do? Well, the quadrotor's modeling does not know anything about coffee and the fact that people might care to avoid coffee that's being spilled, and so the quadrotor keeps on thinking - and I'll play this again - keeps on thinking at any step, despite the fact that Sylvia's not following the straight line, keeps on thinking at any step that Sylvia will just turn and start heading towards her goal, because that's the approximately rational thing to do. And so that's what leads to crashes, because the robot is making now the wrong prediction about the person. I'm going to skip this slide... I don't know how to skip slides, I just switched to keynote... So what's happening here is that we have a person that's going to one goal, and that goal, and now the person is going to some unmodeled goal, but a quadrotor that models the person as being approximately rational is never really gonna pick up on this. So it's just gonna keep expecting the person to correct, and so one thing that we've been looking at is, can we at the very least - there's no way for the robot to actually be hypothesizing 'ooh, there's coffee' and - or, at least I don't know how to make that happen, but I at least want to make the robot realize that its assumptions are no longer holding, and so this is what we've been doing. Recently, there's this particular parameter in the noisy rationality assumption that controls essentially how rational we're expecting the person to be. If you set beta to zero, then you're modeling a random person. if you set beta to infinity, you're modeling a person that is perfectly rational, will take the optimal action. We used to just set this as a kind of a tunable thing that we would play with. It turns out this is one of these beautiful moments where it doesn't have to be a hackey parameter, you can actually make it as part of the inference, and so that's what we did. I'm gonna skip because I'm out of time, but the idea was just simply if you fit this parameter and it tells you that the person seems irrational, then chances are that the person isn't broken but that your model is broken and that should be a way for the robot to pick up on this fact that 'ah, my assumptions aren't holding,' and more broadly, if the person appears to be suboptimal respective to any model that you have, then chances are that the space of hypothesis, the space of model is done, is wrong. And I have been given signs to shut up, so I will skip a few things here. I'm skipping animations. So here's what happens. You have a person that actually steps towards an unknown goal, and all of a sudden the robot becomes confused, and that's because it's realizing that its model doesn't make sense. It's actually treating now the person as being more irrational and it's becoming more conservative. So with that, we've been thinking about what the right inductive bias is for HRI. We think that it's important to actually take into account that humans actually have intentions, but it's hard to actually capitalize on that. We think that rationality is okay, but we need to actually be broadening it and make it more flexible, as well as be ready for it to be wrong, and with that I'd like to thank my collaborators and some of the students are actually here. As well as Claire and Sergey. Thanks. *applause* [Ali Jadbabie]: Thank you so much. We have time for one or two quick questions. [Anca Dragan]: I left time for, like, one minute of questions. [Audience Question 1]: I'm just curious, at the game where the aircraft was landing, how many trials did you give the humans to try to learn that, because I think humans are pretty spectacular at learning very nonlinear, dynamical systems. So, just curious how many tries would you give them, how many blocks? [Anca Dragan]: Yeah, we had a training phase where they sort of seem to be plateauing at some point. I'm to have no doubt that if you give them days upon days they'll eventually get better, but for the purposes of the in-person trials kind of study... *mic cuts out here, momentary cross-mic interference* [Anca Dragan]: Okay, I'll yell. So I was saying that we gave people a training phase and they were plateauing so that's part one. Part two is that we didn't do it in this work, but in other works we actually started looking at how we can actually take advantage of the fact that the person is actually still learning and adapting, and that's a little harder to incorporate because how the heck would do we know how people are learning, what their update is? So we started studying a little bit - how much does it matter to be capturing this? What our assumptions on their learning that are required in order for the robot to do well, and so on. That's a paper that we had on HRI, thank you. [Audience Question 2]: Hey, great talk thank you very much. So, I was just wondering, it seems as though for a lot of these kind of, like, human interaction types of algorithms, it's very hard to kind of design algorithms just because your methods are going to be very data hungry and then you're gonna be collecting data that's gonna be dependent on the actions of the humans. So I guess, in your experience, have you - do you have any recommendations for that? Do you kind of get over that through simulation, or maybe just a lot of, like, human research? [Anca Dragan]: Yeah. So in a sense, that's been the journey, right? Is to find something that - some assumptions that we can make that are flexible enough so that we don't have to get a ton of data and we don't have to get a ton of diverse data, that's really the killer. It's just - you can get a lot of human-human data but then you have covariant shift the moment the robot is starting to use that model to act and model responses that people would have. It seems to be particularly a problem in some of the domains as opposed to others, and it has to do with - is it cooperative, is it competitive and so on. And so, the journey has really been - clearly, the rationality alone is not flexible enough. Clearly, we have to do something different, but it turns out that if you add - at least in the domains that we've looked at - there's always some amount of flexibility that you can add, so that you can get away with making that assumption and not costing you too much in terms of the sort of the asymptotic reward that you converge to. Thank you. [Ali Jadbabaie]: Thank you very much. *applause* 