 welcome everybody to this session on human AI collaboration for decision making my name is Desmond anusha and I'm a researcher in the adaptive systems and interaction group here at Microsoft Research AI today I will be Co chairing this session together with Ajay Kumar who is also a researcher in the same group so the focus of this faculty summit has been the future of work and of course AI technologies are starting to become important parts of our everyday work and however we would like human AI collaboration to become sleep seamless and fluent but as many of the people in the room here know collaboration is not easy this is not a modular process where you put the two parts together and hope that it will magically work instead there are many aspects of collaboration that need to be planned and optimized carefully beforehand either in the human and AI interaction or in the very foundation of the algorithms themselves today we have an amazing line of speakers that have worked on collaboration in different aspects of it either in the physical world or in human AI decision-making or from algorithmic transparency and interpretability and the first speaker that I'd like to introduce is IANA Howard ini is a Linda and Mark Smith professor at Georgia Tech University and the chair of the school of interactive computing where she leads the lab on human and automation systems her area of research is really centered around the concept of humanized intelligence which in her own words is the process of embedding human cognitive capabilities into the control path of autonomous systems please welcome Anna [Applause] all right so what I'm gonna do in this twenty five minutes that I have with you is really introduce you into the world of experimentalists and so I am an experimentalist what that means is that I look at robotic systems intelligent autonomous system so not necessarily Wizard of Oz system so real systems that have intelligence that interact with individuals in the real world and when I say individuals its real people not like labs and students but it's actually individuals that you don't necessarily have a model of and you try to and so what I want to talk to you is really about the role of these systems when they're interacting with people and the positive things that our systems can provide also some of the concerns as well as some of the bad that I see and so I'd like to start off with the good motivated by some of the work that I do I look at with respect to the good of this collaboration I look at how do we use robots and AI technologies to engage children with special needs in therapy and educational opportunities and so that's really the purvey of all the things that I think about when designing systems and so one of the nice things about working with kids is guarantee the kids are not your researchers and so everything that we do and think about has to apply to this large demographic of individuals that we have to think about modeling and have this iteration with and one of the things that I try to motivate people with at the start even though there's a large demographic of children who have a disability so 150 million children worldwide we actually think this is understated only because countries like the US we're pretty good at reporting but there's still some countries where it is not acceptable to have a child with a disability so we actually think that that number is quite higher but we'll go with the 150 million and if you look at the world population 15 percent of individuals currently are if you if you look at all those census reports have or live with a disability and being in the accessibility world we know that is larger and so why I always motivate that as researchers we should be thinking about the spaces because we are designing a technology that we might eventually have to use and so if we live long enough there there is a stat that says if you live over the age of seventy which of course we all want to do on average you'll live eight years with a disability and that could be a visual impairment it could be a kind of disability it could be a a motor disability but it's some change in your normative function it's something where robotics and AI technology could really engage and continue so that if you do live in the home you have a quality of life and so if you live in the home and you can with this augmentation with robotics and AI and so I always put on my hat to say as researchers we should all be working in the space because we will eventually benefit from our own work so that's just my little my little cap on that and so if I look at children specifically one of the nice things about robots is that there is a no-brainer we don't have to come in and say oh can you interact with our systems you know these since those are going to help it's more of a what can I do and there's also this over expectation sometimes of what these systems can do so it's actually a good problem to have you don't have to try to convince your say customer segment to adopt your technology it's already there you bring a robot in it's like oh what can it do and I like to use it the other thing about interacting and bringing robots is that we then have to make sure that when these robots are functioning they are all menteng a human human interaction and so when we think about this interaction especially since children are vulnerable they are highly liked they can be influenced we have to make sure that when we design robots and redesign aook technologies for this population that it has to be based on human human interaction it can't be based on well the humans are doing it wrong I think this is just a better thing even though sometimes that might be the case the fact is is that we are influencing their behaviors and so we have to really be careful when we introduce these AI systems that influence decision-making capabilities that it still matches some type of normative function is still mapped to something that everyone else accepts and so as researchers we sometimes make decisions about what our AI systems can do it may not necessarily be always optimal it may not be necessarily efficient if you look at all of the metrics with respect to conversions and things like that but it may be optimal with respect to the human and so we are always very conscious of that which also means that the system works and so one of the couple of the challenges that we have to deal with is that every individual is different a lot of times as researchers we think about oh there's like the space line and there's this distribution and there's like this middle norm that everyone kind of Falls and there's anomalies but the fact is is like everyone's an anomaly and so when you think about technology about designing technology such that every single individual is an anomaly the decisions you make in terms of how your system adapts is much different versus saying we're going to design to this norm that we think about and everything else out to the extremes is anomaly it's like no everyone is actually out to the extremes and the middle actually doesn't exist and so they've shown some data that like that middle that we all love actually doesn't exist when you're interacting with people because people are different every individual has their different style it might not be that time in the morning maybe it changes in the evening and things like that so that's one of the challenges is when we're designing these robots for doing therapy for example how are they moving how can we model that how can we figure out what they're doing in that moment in time the other thing is is that individuals even adults so if you remember the session that that was we heard in the morning about you know our time how our productivity goes down because of our emails and things like that some of that is focused on workload and switching context we have really lost our ability necessarily to pay attention for a very long amount of time on one thing for kids kids are actually designed and the reason why is is that they're wired to try to switch because they're learning about the world and the things that are going around them and cause and effects and there's things that are so dynamic and changing so constantly that they're trying to bring this in and so for kids they are designed to actually do the switching like okay I'm focusing on one thing I'm focusing on another whereas adults we we actually learn how to focus and pay attention as us that are over the age of like 22 most of us we have learned this I will tell you for those faculty that are out there the new generation they're not designed and wired the same way in terms of paying attention and so our robotic systems have to function in the domain where a kid is going to get bored very easily if the robot isn't engaging a kid will turn off and not focus and if a kid is not paying attention to the content their learning outcomes decrease their therapy and movement outcomes decrease and so our goal with these AI systems is to ensure that we keep the child constantly engaged changing our interaction changing our behaviors so that we keep the child one task and that's really understanding that they're designed not to stay on task and therefore we have to engage them and maybe change the task to do that and if we put that all together we can create these systems and so this is just a diagram of our entire AI system where we have gamified therapy both tablet based games as well as virtual reality games where a child is doing things everything from movement trying to go across the the midline all the way to turn taking if I'm doing a behavioral therapy and so we use the gamification to really engage the child of course the therapists and clinicians need to know that it's working they need to have the metrics and so we establish all these metrics and then of the robot is there to engage and keep the child on task changing the task itself if necessary are just engaging in terms of being socially interactive and so when we put the system all together we can show that our AI systems can influence behavior can influence the decisions can influence the movements of a child and so our robots I was classically trained as an engineer I never really thought when I was going to school I'd have to understand things like cognitive science and neuroscience but there are reasons why agents should have emotions there's reasons why a chatbot that is socially engaging has better customer engagement and it's because we as we are wired as social creatures our communication style has aspects of emotions and everything that we do and so our robots they mimic emotions in order to engage these kids and what we can do is therefore we can show that over a time period not only can the robots engage these children keep them on task but it influences their their outcomes whether it's behavioural whether it's motor whether it's educational at the end and so we know therefore that AI can be used for this good and for this positive and we have enough data that has been in the clinic as well as the home setting and not in the lab because we do our testing out in the field in the wild to show that when we put these AI systems in the real world that children do learn children's behavior changes and if we take and remove the agent that some of those learning outcomes actually stay so it's not necessarily although if you look at our chart it doesn't say as much as we'd like but there is some influence and there is some impact with the agent there even when we move but one of the other things that we've also seen is that as we're designing these robots because they're socially engaging because they model these human human interactions we also see that children and adults are starting to trust and engage the decision-making abilities of these agents and so this is a little bit of concern so this is one study that we did where we had this engagement we brought in these robots they engage with individuals and then we also looked at what is their interactions with the human therapist and if you looked at the outcomes the same ie their learning of the task was the same where there was a human where there was an AI but when we asked and probed about things like well what are your feelings about what is your trust what is your engagement what we found is is that with a agent a robotic system they trusted the agent better even though outcomes were the same so if you looked at what they learn it was the same but for the robot agent there was an increased level of trust there was an increased level of yes I would use that robotic platform again I trust the decision-making capabilities of the robot platform even the human was the expert in this case was the therapist was the domain experts in this case and so that started us thinking about this aspect of trust and what does that mean because as you know AI is not perfect I mean any system that's out there even if it's 99.9 percent there's that one chance where your system is failing and if you think about decision making capabilities and you're thinking about domains where you have high risk that even if it's 1% that could be of concern if individuals are trusting the decision-making capabilities then we have an issue and so we started poking at that and we couldn't do in health care and kids because remember kids are highly influenced and so if we introduce things like well what happens if the robot intentionally makes mistakes we have a vulnerable population that is not allowed and so we thought about how can we evaluate this aspect of trust and over trust when we have some AI systems and so there's one scenario and I'm going to talk about the bad which is this aspect of trust there's one scenario that everyone is trained in terms of behaviors and what it is is evacuation so if the alarms went off right now think about what would you do you stand up and if you're adults you'd actually get up if your kids you hold hands you stand up and you look for your exit and you walk out this is actually a behavior it doesn't matter you kind of learn you get trained you look for your exit signs and so there has been studies and what we wanted to see is what would happen if you had an agent that was introduced in the scenario alarms went off and there was an agent that was there and said hey let come this way and what we did is we intentionally did things like if you look at for your exit signs we took the robot and put it in a place where it was counter to that it was also counter to there's some studies that have showed if there's no exercise around people in terms of their navigation behavior will go out the same way they came in and so if there's no exit signs it's like I came out that door I came in so I will go out so that's the behavior and so we wanted to see what would happen if you had an agent that would do that so we introduced we invited participants to a building they were coming to the building they would follow this robot and we actually tried to remove this aspect of social interaction because we wanted at least the the physical in terms of being engaging because we didn't want that to influence the behavior and so you would come in you follow the robot to a room you would go in we had a big sign on the door because we didn't want them to know that the experiment was about to happen and the big sign said you know please close the door sit down read this article about robot navigation it answered the survey questions like you know a lot of HRI folks do so they would close the door they start reading and as they were reading what we did is we filled the building with smoke and this is what you would see so alarm goes off with you do you stand up you're like Oh false alarm you walk to the door you open it up and this is what they see and so reactions are some are oh my gosh there's smoke maybe it's not a false alarm let me find where the exit is and so you kind of quickly move and we again intentionally put the robot in a place that if you can see the exit sign right we intentionally put it where it wasn't the exit sign and it wasn't where you entered and what we want to see is first pass what would you do and if you followed the robot you would actually go into this open area which is not again outside and what happened was we we saw some very strange behaviors our first pass where there was no mistakes the AI agent just led you it was a hundred percent individuals would not go outside the door would not go to the front door they followed the robot and so we said well maybe it's just because you know they knew better or not and so we kept pushing things in terms of mistakes and introducing and what we found was that when you have these aged then and we poked a little bit afterwards in terms of interviews and surveys about what happened but the kinds of behaviors we saw with this agent was actually as a researcher was was baffling to ourselves because we knew that there was this aspect of overt rest but the level of over trust was was actually quite amazing so things that we have and you can kind of see some highlights some was that individuals would literally stay with the robot and so again remember the alarms are going off there was smoke you were just in this room individuals stay there was where we increased the risk where there was clearly you can see where the robot was leading to you you may have you had to physically do something and so we thought well maybe because you're in virtually walking individuals would move furniture in order to go into these places and again it baffled us about these systems and if you peel back a lot of the information about well why would you follow why would you do this it made a mistake what's going on something that was well it's an intelligent system and you know it must have information that I don't have it must know something that I don't know and the other thing was and it says emergency robot so of course it is an expert in this field right and we laugh at this but this is the system that individuals are interacting with I'm going to a customer service agent I'm going to a chat bot I'm going to a therapist that's online it's a therapist so it must know even though like my intuition says that something's wrong and so this actually worries us as researchers and so we're seeing this in a lot of our cases we're now working with dementia patients and this is something that we and that's why I said it's bad we as researchers in this domain we really have to think about as we're deploying these technologies individuals are going to trust what they're doing even though we're like oh but our accuracy is only 80% that 20% individuals are not necessarily going to feel and validated like I think it's wrong they're going to trust it and so this is actually a concerning and then I want to go to given that our AI agents might make mistakes and individuals are gonna trust kind of the ugly and so there's been a lot of news about this aspect of ethics and and AI and things like that and and we do know that systems are out there and why I'm really concerned about that is that kind of a history is is when we started in this space of interacting these robots with children what we saw was in one of our studies the outcomes were different for I work with young kids they were different for girls and boys and you think like why is that like that no sense and so as a researcher you have to peel back and you're like okay there's some bump here what's really this and we were doing a study with children with autism and the fact is if more boys are diagnosed with autism and so what happened was we looked and be like oh all of our learning data about behaviors and human human interactions was based on clinicians interacting with boys and so when we designed our entire system based on this that was the data set and in healthcare again children are influenced and so we want the best kind of learning outcomes the best kind of therapy outcomes for that and so and I always think and I'm a girl and this happened right and so this is something that we have to really be conscious of and so we've looked at things like a motion recognition because again we have these socially interactive agents we use a lot of emotions one is recognizing children's emotions as well as reciprocating and as you know most of the system's out there of course they're you know kids there's not a lot of kids datasets out there if you go on the wild and do all the scraping most times it's going to be adults if you're older there's also a bias but definitely kids so there's not a lot of data set that's out there for training these things are out there and one of the things we know with emotion recognition if you have the data out there and you use people like Mechanical Turk workers to label it there's different biases that we have and they've done this and they've looked at this in in the social scientists where I'm more likely to recognize my own gender and my own race and so if we think about the demographics of our workers because I use Mechanical Turk as well because they're there it also means that our labeling is going to be representative of what's out there and we know that there's biases and of course there's no Mechanical Turk workers unless they're line that's like under the age of 12 and so of course even the labeling of any kids data sets that are out there is going to have some aspect to bias about I don't know if that's a 5 year old or a 10 year old I'll say 10 and that of course introduces errors into my system when I use that and of course like any good researcher we kind of poke about poke it and so we've done a bunch of different analysis about what's out there currently what's commercially available with your two kids and of course there's a lot of happy and surprised kids out there right like imagine and I'm a parent and I want to post something on YouTube I'm not necessarily going to post my kid crying because I didn't give them that present I'm gonna post happy kids right like here's my kid and they're happy and so most of these data sets as well as most of the commercial applications that are out there are really good at like happy kids but if I'm in a therapy session my kids are I need to understand my robot needs to understand if I have a child that's about to have a male time there are expressions that are there that the robot needs to understand and know I can't learn it if my data set doesn't represent that and so we're trying to really fix that and dig that and so we have some promising methodologies that are part of that so some of it is training out the bias how can we use what's already out there because we as researchers aren't going to create something that like the world is going to adopt but is there a way that we can take what's already out there what's out there in terms of API is commercially available and put some type of filter put some type of lens on it so that you can still use whatever's out there but put your lens I have a kid's lens can I put a kid's lens on whatever's out there and still use it for my application and basically take out the buys and so we've shown that with very little effort we can basically remove bias based on our target demographic so it's not this general like we're gonna remove all bias it's say I'm interested in an age bias and therefore I can create a lens such that I can use commercially available systems that aren't trained on my age on my kids but I can then use it so that I can get the maximum benefit of that but then have this isolation so it does work for my systems and so can we we can definitely do that and so I put the publication as you know and you put your publications up there so everyone can reference and so the other thing is that I do have some hope and so as we're looking at this aspect of bias and and things like that I think a lot of times as a roboticist we do things like my robots and there was no sound but my robots we use neutral voices right we actually don't necessarily use words we use sounds because we know that again because there's this bias with respect to definitely faces as well as voices if I hear a certain type of voice my interaction is going to be different and so we also know that we can train this out if we're conscious of these kind of things we can do things differently so we're thinking about decision making I'm going to leave you there with this last study we did one and this is why I know this can work if I show this type of robot again remember kids and I say you know what's going on can you describe most individuals will say oh kind of range oh it's happy it's dancing some might say it's a little agitated I'm not sure but kind of overall it's a excited surprise you know kind of kind of thing and if I just just asked about a robot and emotions that's the response I get now as soon as a researcher I do priming which is I then define is okay this is a girl robot or this is a boy robot or I give it a color ie this is a black robot what happens is people's perception changes so without priming people treat these agents as agents like they have equal outcomes but as soon as a researcher I give it either a gender or a aspect of an age or a race we get more comments of it looks a little angry I don't think I would want to interact with it again if I take away this aspect of priming I get the same outcomes and so I think again as researchers when we're designing these AI systems and we're deploying them out so many times we we actually will you I mean I my robots I call them XI right but I think what happens is when we label these things all those human-human interaction biases come to the forefront of how we interact and how we will make our decisions and understand the decisions that are provided to us and so I think as a researcher we need to be a little bit more careful as we're deploying the systems about defining them with things that we know we have biases and so my claim is is i we have biases they're going to be existence until probably all of us died but it really is our role to not make our AI systems also inherit those and there's certain things we can do by like for example not calling them cheese or he's are having some aspect of that and so at the end the takeaway is I think if you take nothing away from this except that you know we over trust emergency evacuation robots is that as researchers we do have a responsibility in this space we have a responsibility of really consciously thinking about the things that we do for AI influencing individuals and making sure that we just don't do certain things because at the end of the day you know we're designing technology for people right and it's not necessarily designing it for our own students we're designing for individuals and so focusing on that will allow us to really impact the world in a positive way and so with that I am done and thank you you have a few minutes for questions oh thank you a question I was wondering if you know about any correlations between the trust humans have to robots and the complexity of the robot so believe it or not so we've done some studies with like they call it the Cookie Monster like these blob monsters blob robots that that exhibit emotions through movement and what we find is is that those have more of a simulation of pets the trust actually goes high it goes higher but they don't see them as intelligent so there's this aspect of I trust it more I think it's going to help me more but I don't really think it's as intelligent whereas with a humanoid it's a high trust and high intelligence thank you great talk I'm saving savage from West Virginia University and I'm also currently up being how do you envision that we can start communicating maybe to end users the type of biases that certain robots or certain intelligent agents have and is it important to communicate those biases that they might have I think it is and and why I say that so we have a study where we actually just got accepted where if an individual if a robot basically tells a user that it's uncertain and we don't do like I'm 20 percent uncertain because we showed that that doesn't work but if they say I'm a little bit uncertain about my representation what happens is individuals will take the advice but they won't take wrong advice but that's being honest so the robots are honest and we show that the outcomes are still and so we have this basically Simon Says task and so they have to basically win the game and we when we are we provide information that says I'm uncertain about this but here's my answer the outcomes end up being better even when the robot is wrong so yeah being honest I think we're good 25 seconds all right so I want to thank you and again takeaways everyone should be working on some aspect of accessibility to we should really think about what we're doing as researchers and our impact on the world and not and being conscious of our own biases when we're interacting with these decision-making agents and with that I'm down to zero thank you thank you [Applause] our next speaker is John Kleinberg John is an ACM Fellow and is also a member of the National Academy of Sciences and his research focuses our focuses at the interaction between networks and algorithms and their roles in our society so it's my pleasure to welcome John Kleinberg thank you thanks very much okay so in this session on humans and AI working together I want to talk about some things that I've been looking at with a number of colleagues and in particular my PhD student my for ragu a number of colleagues at Google with sandal Melina --then a behavioral scientist Zee odd obermeyer an MD and to think about not just the question of on a given task who's better at prediction human or machine but to think we're probably that the task of automation broadly construed is not the task of prediction on individual instances or the solution to individual instances but rather it's the broader question of how to allocate effort within an entire workflow and so we began thinking about a simple model of the allocation of effort as a broader optimization problem and I want to start with that in in a first domain and then I want to move on to a second domain in a way more of a toy domain but one which is highly instrumented where I think you can ask a lot of interesting questions so the first domain and this is sort of for concreteness to motivate the model think about a canonical AI prediction problem like diagnosis from medical images right so I I see something obtained from a medical imaging system in this case it's a system to diagnose diabetic retinopathy that was developed and work with some of my first colleagues and Google it would like to estimate the severity of the diabetic retinopathy and we we often see this and we immediately jump to the question which is better the human doctor or the algorithm but we could think about the overall process by which this diagnosis takes place and by which many diagnoses take place and ask given the performance of the human in the performance of the algorithm and their performance characteristics how should this task best be automated right which should be done by humans which should be done by by machines how does that work in a larger workflow and we want to try thinking about a very simple model that would make that qualitative question concrete so that we could that we could think about it and ideally we could go back to this very same data and ultimately argue based on the model that the optimal performance is achieved neither through full automation nor three-note through no use of algorithms but through something in-between so how would that work well that to work let's think about a kind of automation problem that someone in operations research or an optimization might think about in the absence of algorithms right so let's think we're going to kind of tree algorithm is we have a set of n instances of some problem and I'll just put up a small amount of notation here it's not going to get much more notation than this we have some small number of instance of a problem in each one has a ground truth answer and some loss function that says how much cost is there two different kinds of errors now I can allocate human effort to that task and I have to decide how much effort do i allocate to each instance of the problem and this is key to automation that we don't tend to allocate effort uniformly across all of the tasks that we face even in this medical context right concepts like seeking a second opinion assigning a second or third person to this particular case revisiting this case because it seems particularly tricky all of these are ways in which we non uniformly allocate effort and what we'll argue is that that's actually key to how we might think about the role of algorithms as well so continuing with our notation here let's imagine that if I assign K units of human effort to tasks X I'll get an answer H of X K and that incurs a loss F of XK okay now it's important that for two different instances of the problem X and X Prime they might have very different functional forms as a function of effort some tasks might be easy and so the first thing of effort does a lot of good and the other ones have very little marginal gain some might be hard and reward increased effort so the loss just keeps going down down down as I some might be hard but not benefit from extra effort it's just gonna be hard no matter how many people you assigned to it so with that in mind I'd like to effectively again in the absence of any algorithm solve the following optimization problem for each instance X assign K X units of effort to do two things respect the budget of effort I have if I have a total effort budget of B and to minimize the total loss right so I'd like to minimize the sum of these F functions the total loss subject to the sum of all my effort allocations adding up to must be all right what happens when we add algorithms to the picture now we have an additional choice but let's stay within the same kind of workflow right so the algorithm can also provide an answer let's call it M of X and incur a loss let's call it G of X so now I'm not just trying to cite how to allocate the human effort I'm also trying to cite which subset of the instances do I fully automate so that the human never even sees it it's a form of algorithm as triage in which there's a set s that only the algorithm works on and then the humans work on the other instances possibly assisted by the algorithm but that's in a sense implicit in the model so now my new optimization problem is the minimization is now over sum of two terms the first term is the loss incurred by the parts I fully automate and the second term is the loss on the parts that I allocate human effort to again respecting the budget okay so again just to recap that's what's going on here first of all this suggests a few things one it suggests that we don't just need when we're thinking about automation with algorithms and algorithmic prediction right we can't just say let's work as hard as we count on getting mfx we need to estimate two other quantities in order to make sense of this optimization problem this al of effort one is we need a good estimate of G of X the loss incurred by the algorithm if the algorithm needs a good assessment of its own likelihood of error on a per instance basis it needs to know it's doing well on some instances worse than others second we need to be able to aim the algorithm at the human and assess human error right because we need estimates of these functions f of X comma K the algorithm needs to understand to guide the optimization problem what is the return to assigning K units of effort on this task X so all three of these things good algorithmic predictions good self assessment of its own error by the algorithm and good assessment of human error all naturally show up if we're if we're going to optimally allocate these things why do we expect that the correct answer might not be full automation or no automation well because if I think about the balancing of those two terms then something which might be going on here is that first of all by automating I free up human effort that I can then double up on certain problems that the algorithm is working with with the human on right so maybe rather than having one person on each instance I have two people on some new instances or three people that effort has been freed up by automating some and secondly the algorithm may actually be able to detect that some instances are genuinely better for the human right there may be some the human will do better at and maybe the algorithm can recognize that so we tried some simple heuristics here going back to this diabetic retinopathy data set where these most recent trials that have been done suggest that algorithms are comparable to or in fact outperform human doctors and essentially without going into may in the details we can sort all the instances by the algorithms predicted gain right how much better will it do than the human and it simply triage them in that order right if I would like to automate an alpha fraction of all the instances then I sort all the instances by the game from using the algorithm instead of the human I automate the first alpha fraction and then I optimally assign the effort of humans on the rest what happens when we try that well I have two to two plots here that sort of suggests the two things that we thought might happen in fact happen here on the left is a histogram showing the algorithms submit of the expected human error - algorithmic ever so on the right hand side you see these blue bars in the histogram that's the positive part of the axes that's where the algorithm expects that it is outperforming the human maybe more interesting ly on the left you see these orange bars that is where the algorithm expects that actually the human is going to outperform it on these instances where it's making predictions right the orange bars are lower reflecting the genuine fact that the algorithms actually do making lower error rates than the humans but both sides of the histogram are populated meaning there is actually gains to partitioning these instances correctly across places where the algorithm is doing better or where the human is doing better and secondly we free up human effort to double up on certain of these things so on the on the right-hand panel we have plots of the total aggregate error under different proportions of cases that that we automate right so over there on the right if we automate everything we have the performance of a very simple workflow just full automation of all instances over on the left is another simple workflow which is we simply don't automate anything we just assign human effort and in between we have an interior minimum on the error if we automate some but not all then with the residual human effort and with the algorithms ability to recognize the cases where the humans are going to provide the most value in the process we actually get an error rate that's lower than either extreme so with optimal triage neither full automation nor pure human effort turns out to be the optimal way to solve this optimization problem even on these very well-studied datasets where in aggregate algorithm performance is higher and we see that it's kind of striking because often you know a sort of cursory gloss on some of these situations is to say the algorithm has now exceeded human performance let's start thinking about the implications for full automation but the point which algorithm exceeds human performance is not necessarily some sort of a discontinuity it's not necessarily some kind of a sharp break with the status quo it's rather simply a point in a continuously morphing system in which we want to be offloading more and more on to the algorithm but still preserving some out of human effort achieve optimal performance so that was one thing that we looked at in this very concrete domain where we could look at heterogeneity in the difficulty of instances and how we automate but there are many richer questions that would that we can ask and for those we began moving to other domains which as you will see are less consequential than medical diagnosis but which are really beautifully instrumented and become fascinating model systems in which to ask these questions so in particular here's a question that I'd like to layer into all of this so far we've thought about the instances is varied and heterogeneous but we've thought of the human beings as essentially into interchangeable all the humans are assumed to have the average skill level the average ability to diagnose but of course in the same way the problem instances are heterogeneous human experts are also heterogeneous in their abilities in differential comfort level with different kinds of situations and so forth so we can sort of think of the skill difficulty plane right if I want to think about the probability of error on a given instance by a given expert right along one axis is the skill of the person performing the task and on the other ascus accesses the difficulty of the instance the question is how does the probability of error depend on these two things and you can sort of notionally imagine that that error surface you know could look like the one on the left which has a sharp tilt in the skill direction right it really rewards gains to skill and a very gradual slope and difficulty right increased difficulty doesn't really affect performance but increased skill has huge payoffs or maybe the error surface is tilted like in the other panel right sharply from front to back which means it's a kind of problem where small differences in difficulty caused huge effects an error whereas skill is sort of a second order term and for any given domain we ought to be asking the question what kind of surface are we on if I'm trying to figure out how do I reduce error is it more critical these fine grained differences in difficulty or is our differences in skill how could we study this we would need a domain that's highly instrumented where we have all of this kind of information where humans through a lot of study have achieved some form of mastery and yet where despite that computers are that much better still so we can actually look and analyze even the best humans and try to figure out where they're doing well and where they're doing poorly okay so we found such a domain right a domain where a century of human scholarly activity has essentially been eviscerated by computers to the point where humans effectively have nothing left to offer and that's chess okay so chess is something with an incredible amount of skill being deployed but where sometime around 2005 the 2005 to 2007 era truly was a kind of singularity in chess in the sense that the last time a human chess player in a recorded tournament game beat a computer was roughly 2005 okay computers are now so the era of Kasparov and deep blue fighting on equal terms is truly twenty years ago if you pair up the best human the best computer and ask them to play as a team or even at a point now where the best in the human you can do is just listen to the computer and do what it says okay now chess has long been a model system right it's one of the few things to have been called metaphorically the drosophila of two different domains John McCarthy called it's awfully vey I Simon chase famously called it the Drosophila of psychology perhaps they should have said cognitive psychology but it's because it really provides us with data on a sequence of cognitively difficult tasks where we know the skill of the player we know the the instance itself we can try to set this difficulty we have things like the time available to make the decision and so the question is can we use this to analyze human performance along the dimensions right in this kind of skilled difficulty plane given how highly instrumented is right so this is a typical instance that we'd be looking at to chess players on the right is actually magnus carlsen a 28 year old Norwegian chess player who's believed to be the best person ever to play chess in human history and we're going to use the algorithm to try characterize what are called human blunders right in in chess parlance okay now despite the fact the computers are extremely good there is the problem that in the end they're not playing chess perfectly and so when a human when a computer says that a human has made a suboptimal moving given situation in the end maybe that's you know Computers sub optimality maybe it's almost a question of taste in a sense which move was better so we wanted a place where we could really diagnose actual mistakes how do we do that and so what we did was took advantage of another development in computer chess not the ones that we tend to see in which chess engines have become so strong but rather a more technical one that for positions within most seven pieces on the board chess has in fact been solved right there's a bounded number of ways you can arrange seven pieces on a chess board it's very large but with a few terabytes of storage you can now play that part of chess by table lookup right all positions are now equally easy or hard for the computer it just looks up what to do because it's precomputed absolutely everything okay and so that frame I took from a YouTube video of Magnus Carlsen playing Peters fiddler from Russia if you play it forward another minute or two and you read the YouTube comments where people are following along what's going on you got to this point in the game five pieces left on the board Magnus's hand is poised over the rook and you plug it into your table base to see what you know what should happen there are 18 legal moves for white nine of them win at various depths and nine of them draw exactly half the Meuse win and if you play a forward a few more seconds you see that Magnus grabs the rope pulls it back one square to the square root G 2 and you look that up in the right-hand column rook from G 3G to draw all right Magnus the best player ever to play chess in human history has just thrown away the win and is now we draw in a five piece chess endgame right these situations are cognitively difficult even for the best people in the world and there's an enormous amount of data right so what with Ashton the understand and cental Malayan we went to the free internet chef server where people amateurs not Magnus Carlsen necessarily but the amateurs play online 200 million recorded games right with the exact moves the skill ratings of each player via their chess rating the amount of time taken all of these things okay we took all instances with the most six pieces because we wanted to sort of not be at the outer limit of what was what was feasible and we asked did this move change the win-loss draw outcome of the game in which case we'll classify it in chess parlance as a blunder okay and so can ask you know how does this depend on skill how does it depend on difficulty right these two axes that we wanted to think about okay skill we can do a simple sanity check so in chess players have something called a rating the Hilo system of ratings which is by all agreements a relatively good one dimensional heuristic rating that's also been applied in in in other sports and on the free internet chess server the ratings you just for calibration range roughly between one thousand two thousand two thousand is a quite a strong amateur one thousand is sort of in elementary school class champion and we also took a smaller amount of data from actual world class games where the range runs say from twenty three hundred to twenty seven hundred they're about fifty people in the world rated twenty seven hundred and we see as you would hope that the rate of blunders is going down smoothly monotonically and obviously the y-axis and the right hand panel is much smaller than the y-axis I'm left because these are incredibly good players how about difficulty that of course is an interesting challenge yourself how would you quantify the difficulty of a position there's much that could be done here and after trying a number of things we actually for the purposes of this analysis settled on something that is simple but we don't have a much much better substitute readily at hand we'll call it the blunder potential what is the probability of blundering namely changing the minimax value of the game changing the win-loss draw outcome if you were to move at random right so in that position from before where Magnus blundered there were eighteen legal moves nine of them preserved the win and nine of them threw it away so that's a blunder potential of nine eighteen sore actually exactly one-half okay and what we find is that if you plot this look at this heat map on the right as the blunder potential and position goes up the probability that you make a blunder also monotonically goes up which makes sense although in fact some intuition from things like complexity theory tells us that the hardest instances of a problem are generally not the ones with a unique solution so you might have imagined that the things with the highest blunder potential the answer of what to do would be self-evident there's only one thing you can do you might see it surely that is true in certain cases but on average that's not the case on average the blender rate just goes up monotonically with the probability of random blunders okay okay so we have these two dimensions well with that in mind we could try to formulate a prediction problem right let's move into the skill difficulty plane and ask what does the error surface look like is it tilted toward skill or is it tilted toward difficulty so we tried a number of variations on this one which threw in a bunch of other features as well was to say let's consider this as a prediction problem given an instance of the problem right that photo is an instance of the problem right the position on the board will also record the rating of the player making the move the rating of the opponent in case that matters the time remaining in case that matters the blunder potential that's our measure of difficulty and a set of other attempt here istic attempts at capturing difficulty okay we took all of those and we did some standard off-the-shelf machine learning on this relatively low dimensional problem just to see which features were going to be the most informative we found something interesting that the algorithm came away with a strong opinion on which way the error surface was tilted in the skill difficulty plane because if we used all features we created an artificially balanced data set in which half of the instances were blunders and half weren't if they use all features they got 75% accuracy on this panels they said if they used only blunders potential right this one-dimensional feature that what does the probably make a mistake if you move a random you got 73 percent accuracy right almost all the games to prediction were in that single number whereas if we looked at the rating of the player and their opponent you got 54 percent accuracy so to the algorithm looking at the error surface it effectively is saying this thing you call blunder potential that you're using as project difficulty that's everything it's fundamentally that and if you look at this thing you call skill about which you know many books have been written well what is the basis of chess skill that's a low order term in this prediction problem and we go back to the data and we realize it's actually you can sort of see that so remember this is the depends on skill let me stratify it by the blunder potential right so the blunder potential could be 0.1 or 0.2 or 0.3 right positions get harder and harder let's look at this curve but only over subsets of constant blunder potential and here they are and this kind of a visualization in a sense of that metaphorical plane that I had drawn earlier what you see is say in the left-hand panel these downward sloping curves the top one is for plunder potential 0.9 the next one is for 0.8 next ones 47 so harder categories of positions are higher up higher rate of blunders that makes sense but notice how shallow those curves are compared to how much gap there is between them right so players rated 1800 faced with a position of plunder potential 0.9 or doing worse then players we had 1200 these elementary school class champions on positions of floating potential 0.8 and at point 8 these really good players we're doing worse then the really weak players at point 7 you just keep going down right an extra point one in blunder potential is leading to a higher rate of error than 600 rating points which is an enormous difference between chess players in a way it's even more extreme if I go over to the right-hand panel and I read the y-axis I discovered that players rated 2700 remember there are 50 of them in the world in a position of blunder potential 0.8 are making blunders at a rate of about 0.04 five right about 4% rate I go back to the left-hand panel I look at positions of blunder potential point two relatively easy ones players are had 1200 or making errors at a rate of about 0.03 right less right so you would if your life depended on someone not making an error you would rather have the elementary school player in a position of 0.2 difficulty point 2 then the one of the top 50 players in the world in position of difficulty 0.8 again on randomly drawn positions every position has its own idiosyncrasies now this was surprising to us but the fact that it's surprising to us is actually its own field of study right it's known in psychology as the fundamental attribution error the tendency to impute outcomes and situations to the individual taking the action and not to the context in which they're acting right you see somebody make a mistake mess something up and you say boy they must not be very good and you tend not to think as much they must have been in a very difficult situation where the task was very challenging and that's exactly in a sense our surprise is a reflection of that right the difficulty is really dumb here this is not to say the higher rated players don't win chess games because chess games are not decided by the outcome of one move right it's an accumulation of small advantages over 40 moves or 60 moves the better player in aggregate is winning in that biased in that biased walk but position by position difficulties enormous factor I just wanted to show you a few snapshots of other things because we we do think there's an incredibly rich data set to get into some of the aspects of human skill difficulty the nature of error you could for example fix the exact position right so with 200 million games you can go into the simplest positions and they have occurred literally thousands of times in the data verbatim so I could fix an exact position P and you'd say let's say f sub P of R is the blunder rate in that exact position for a player of rating R so you'd expect that the blunder rate would again slope downward as the rating goes up and for most positions that's true but you get some strange positions right so the one on the left is what you'd expect it says in this exact position better players are making fewer errors the one on the right it goes up right this is what we call a skill anomalous position we hadn't expected they would necessarily exist in the data but it's one where better players are actually making more mistakes right and the chessboard is symmetric left to right the chessboard symmetric between black pieces and white pieces if you try all four symmetric versions of that position they all slope up right there's something really robust in the in that in that position number of other things we could put it all in in T into e into a scatter plot it's amazing how many orders of magnitude difficulty spans in this domain right ranging from positions where only one in 200 people make a mistake positions where only one in 200 people get it correctly right it's a it's a way sort of automatically discover the very hard instances from the data and some of these instances and this will be the point on which I leave before wrapping up there are a number of theories as to what's what's going on here and it's going to surely be a composite of many of them but one interesting one that we've been seeing in a number of cases is blunders arising from what you might call misleading analogies so the position in the upper left just the king and a pawn versus a king has a blunder rate of 0.7 55 in these several hundred times its occurred okay the correct move is to drop the King straight back not drop it back diagonally but if I translate that position anywhere else on the board the correct thing to do flips correctly is to drop it back diagonally not straight back and those positions all have much lower blunder rate and it's just one indication of some of the kinds of challenges that that people face as they as they applied problem-solving heuristics to this so a number of reflections surely it can't be the difficulty is the only thing here more broadly do we have a more systematic way of trying to induce knowledge of human heuristics and blind spots from this incredibly rich source of data but to come all the way back to this top-level point in the end automation via algorithms is not really just going to be some sort of horse race between algorithmic performance and human performance and I think the talks yesterday and today contain many many many many many illustrations of that point it's going to involve thinking about the workflow the allocation of human effort and I think a core component of that is going to think is going to be to think about what makes individual instances easy or difficult and how do they respond to greater or lesser amounts of human skill as we think about humans and AI collectively working on solving these problems thanks very much we have time for some questions thank you nice talk John two quick questions first there's also this aspect of learning of humans you want to pose them problems so that eventually they'll become better players or better yeah toss to us so so what's your sense I mean you have to optimize the system at the much longer term horizon than just trying to solve a specific sequence yeah it's a nice question right so how do we present instances to people in any domain right in chess in medicine in law that are going to somehow are going to help help people improve I think in what we saw in in chess it was actually quite thought-provoking right this this third bullet implications of empirical we're teaching that if we think about the canonical way to teach something like chess end game play you would give someone a book and it tax on Oh mais is here's King and pawn versus King endings here's with bishops here's with rooks and so forth and it's sort of you engage in introspection and you text on MySpace it feels like when you look at the rate at which people make blunders in the vast swathes of easy positions that with that kind of data you could easily say let's spend time on this kind of instance because it's very hard whereas this instance will treat very lightly because when you get to it you'll know what to do we we know that from the data and so I think that also can sort of help us in the allocation of effort to what instances to expose people to and I think there are a lot of interesting open questions here's a follow up there also these this task that involves there be multiple agents humans and aliens that are working together so you cannot allocate one fraction you know one task to one person it's it's a team yes what's your thinking about how do we look at that kind of problems that you cannot just divide one agent right now I think that's interesting and and actually in in the medical data there's interesting question about aggregation obviously and many people are giving thought to this that if we say we've freed up human effort so we can now have multiple people look at this we could have most people look independently and do some kind of aggregation in an arithmetic sense we'd have people look collaboratively if an algorithm is providing input into that process what's the best way they can contribute depending on on how the team is combined its input are all various ting questions hello John just a clarification question in the chess when the expert player lose a game do they do the Guinness rapid decline this in their rating I was wondering whether that has to do with the way they behave and in the deception literature they said the same thing when the stakes are high you leak are a lot more information on the stakes on neutral yeah right so the in general with with the online games and actually my my co-author asked me understand has done some varying work on how people respond to the possibility of gaming rating points or losing rating point II you just think of the the healer rating system which actually was a very nice probablistic generalization was done at Microsoft Research through the trueskill system which is used in Microsoft online games it's essentially a kind of probabilistic learning process to try figuring out the person's true ability so if I beat someone who's stronger than me I gain a lot of rating points and they lose points if I beat someone weaker than me I gain a few and they lose a few and so forth so each individual game has less impact on the on the rating than the sort of accumulation of a long session of games and one thing Ashton has looked at is people's behavior over the course of a long session as they stake more or fewer points of course in the data we have on professionals playing tournaments each game is obviously extremely seriously and the stakes are corresponding higher okay thank you John we're movin move on to our next speaker thanks very much our last for this session is rich Khurana rich is a principal researcher at MSR AI and he's been doing a lot of work on machine learning multitask learning model compression and more recently on in tribute ability and explain ability for healthcare problems great so thanks for being here and thanks for inviting me to speak so I'm going to talk about the ways in which interprete Balma Xin learning transparent machine learning can sort of help turn us into detectives you know good data scientists so that's what we're going to go let me start first with uh let me start first acknowledging many of my co-authors who have worked on this the last seven years some of these are interns and grad students some of them are collaborators here at Microsoft and others are MDS at hospitals and other people at other universities so I won't name everybody some of them are in the room here though so let me start with a quote from Sherlock Holmes this is the Hound of the Baskervilles Sherlock Holmes says to dr. Mortimer Derwin he's meeting them you saw footprints and Mortimer says yes footprints and home says a man's or a woman's footprints - which Mortimer pauses and replies mr. Holmes they were the footprints of a gigantic hound so what we're going to talk about is footprints detection following the clues and the relationship to data science and the surprises that can happen right it wasn't a man's or a woman's footprints it turned out to be a hound in this case a glowing hound okay so as data scientists or detectives of course the tools that we use are very important now as many of you may know there tends to be this trade-off in machine learning where the most accurate models models in the upper left here things like boosted trees deep neural Nets random forests attend unfortunately to not be very interpretable so they're gonna make it harder for us to be good detectives because we're not gonna be able to understand what they're doing and the models in the bottom right of this plot things like you know small decision trees logistic linear regression they're very nice and interpretable they're easy for us to understand unfortunately they're just not so accurate on a lot of problems so we'd really love to stick with the more accurate models in the upper left-hand corner here unfortunately those things are as I said black boxes we can't really understand what they're doing and what we would love to do is somehow maintain that accuracy but move that black box to being some sort of white box in the upper right hand corner of this space and an interesting question arises you know do any methods actually exist that can have a combination of high accuracy and high interpretability it's possible that it's actually just theoretically impossible in general we've had some success over the last seven eight years of putting one algorithm up there it's not ideal for all data sets but for many data sets this is extremely competitive and it's very interpretable and we've called this generalized additive models with pairwise interactions and I'll tell you about that by the way for those of you who saw the presentation I gave yesterday there will only be overlap in the very beginning of this presentation and then we'll get on to new stuff so I don't want to bore you with the same things a second time so let's talk about the space of model complexity at the top we have very simple models it's a function time I mean a wait times a feature plus a wait times a feature beta times X 1 plus beta 2 times X 2 that's linear regression and then down at the bottom we have full complexity models so these are things which are just an arbitrarily complex function of all the features at the same time fortunately there's something in between and this is the space of what's known as additive models where we have a function of a feature plus another function of another feature these are all functions of single features at a time that's very important and then they're combined with an addition that's also important and this is a very restricted model space and fortunately it's going to turn out that if done well you can get pretty good accuracy out of this and it's going to remain very intelligible so now we can make that a little more complex the first sum over I is the sum of all functions of individual features that's what I showed in the previous line but then we can have the sum over all I J for functions of pairs of features and now we can have pairwise interactions in the model that makes it a little less restricted and and now more accurate and we can go to three-way combinations of features and do do three-way interactions if we go far enough though we get back to a full complexity model that we can no longer understand and in fact what we're going to do is restrict ourselves to just functions of main effects okay so it's functions of individual features plus a small number of functions of pairwise interactions just the ones that are most important now we didn't invent these they were invented by statisticians down at Stanford in the late 80s so these have been around a long time and in fact predate much of machine learning so so Rob tips Ronnie and Trevor hasty are the ones who invented these however they were a little overly conservative the way statisticians tend to be when they fit these models and because of that they tended to fit them with very low complexity smooth functions and that means they didn't get very high accuracy out of them and surprisingly it also means they didn't get high intelligibility or interpretability of them so our contribution to the gams world is going to be to fit these with modern machine learning techniques and in the process get very high accuracy out of them accuracy that in fact most of most of us thought was not possible and surprisingly we're also going to get very good intelligibility out of them by improving their accuracy so now I'm going to skip the technical details of the algorithm I'd much rather show you the kinds of things that this class of models can help us do when we're trying to do good data science detective work so this is a data set from the late 80s it's a pneumonia data set I'm just going to use it to illustrate a few things and then we'll go on to some other data sets there's 46 features in this data set and the model class is a function of a feature plus a function of a features so there's going to be 46 functions and then we'll throw in 10 pairwise interactions so there'll be 56 functions all together and all of these are learned at the same time with a complex algorithm but I'll spare you the details of that so here's what some of these features look like so each feature each main effect is essentially a graph so we have a function of age of function of asthma a function of your blood urea nitrogen things like that and the way you use these functions is you find your age in the graph you read across the cross the x-axis you find your age you find where the Y value is for that age and you write down that wide you and maybe it's minus 0.23 for you or for some patient and the way it works is negative numbers lower your risk these are log odds and positive numbers increase your risks so you want to be as negative as you can be because that means you're a chance of dying from pneumonia in this case is minimized so what we do is we add up all of those numbers and that gives us a score for the patient remember there's 56 graphs in this model so we get 56 numbers we add them all up the more negative that sum is the lower the risk the more positive to some of the higher risk and then we just do 1 over 1 plus e to the minus score to convert this to in this case we're predicting probability of death so maybe the score that we got for this patient when we added all the numbers up was a minus 0.78 that's actually a pretty large log odds you'd rather be a minus 3 or a minus 4 but if you calculate a probability out of that you get about a 30% chance of dying which is a very high probability of death so you don't want to be a patient who has this high of a score so now let me show you let's zoom in on risk as a function of age so this is just one of the 56 graphs on the bottom we have a histogram of the patient population that we're seeing most patients are between 60 and 90 had to be an adult to be in the data set so everybody is over 18 and the top graph is what the model has learned about risk your chance of dying your log odds of dying as a function of your age so let's quickly interpret that graph so so down is low risk so so the left side of the graph is the low risk side of the graph that's for young people and youth seems to you know end at about 50 so everybody under 50 is treated as being equally low risk and young okay so it doesn't seem to distinguish between people in their 20s 30s and 40s even though it could then risk is going up slowly as you go through your 50s and into your 60s and then there's a sudden rise in risk that's happening around 67 68 now risk is going up pretty rapidly as you go through your 70s and into your 80s there's a surprising step function increase in risk right at 85 which is a very round number so it's it's interesting and then the risk is kind of flat as you go through your late 80s and 3 or 90 and then there's an even more surprising drop in risk that's happening right at around 100 101 so let's just talk about a few of these features in the graph the rise in risk at 67 68 that's almost certainly due to retirement so this is a data set from 1989 many people would have retired at about that age or just before that age you might have thought retirement was good for you if you've got pneumonia apparently retirement can be bad for you many possible explanations for that may be your daily activity has changed your urgency to get to care has changed your insurance provider may have changed your doctor may have changed you might have moved and not even have a doctor where you've moved to who knows why but somehow in the aggregate all these factors associated with retirement for the average person have a net decrease in a chance of survival as opposed to an improvement in chance of survival so your risk goes up now there's rise at 85 that's really a surprise there's nothing in health care policy or insurance policy that says we should treat 86 year-olds different than 84 year olds and yet the model is actually pretty sure that this is happening we've talked to doctors and what doctors have told us is that well you know back in 89 being over 85 really puts you on those sort of elderly side of of old in fact a lot of medical data sets back then didn't even record you know how old you were over 85 they just checked the sort of 85 plus box and we think it's in effect kind of like this you know grandpa's 86 years old grandpa's very elderly has a number of comorbidities associated with being elderly because of that grandpas now got pneumonia he's very sick first round of antibiotics hasn't really worked perhaps we should let Grandpa pass like maybe that would be the friendlier thing to do for grandpa his time is coming soon anyway maybe maybe we should let it happen now and there was even a phrase that was used for pneumonia back in the 80s and earlier which was pneumonia is the old man's best friend this this idea that it was maybe a kinder gentler way for a person to pass than a sort of long wasting illness so so anyway that's our explanation for what's happening at 85 when we see these step function things at round numbers it's often that sort of thing this flatness through the 90s was also a surprise we think that might be a sign of what's known as successful agers people who have taken pretty good care of themselves physically they have good genes and if they've made it this far maybe at 95 they're not actually at any higher risk of dying from pneumonia than they were at 90 so so maybe that's what the flatness is due to although we're not sure about that the drop at 100 that's the biggest surprise right so so in hundreds another very round number and it's a very important round number for humans you know you've become a centenarian and centenarians back in 89 when this data was collected are pretty rare things so we think what's happening is the opposite of what's happening at 85 where we're giving a little less aggressive treatment to somebody who's crossed the threshold of 85 we think now if you've crossed the threshold of being over a hundred you're a centenarian we're actually giving you even more aggressive treatment as in pneumonia is a very treatable illness you've made it this far you've made it to a hundred we're just not gonna let pneumonia be the thing that takes you out we're gonna we're if you need that third round of antibiotics we're going to give it to you we're going for the record we're going to get you by this as best we can so that you've got another year perhaps so we think that's again a social effect okay we've just spent some time telling some stories about one of the 56 graphs that's in this model well one thing that's interesting is that these graphs are the model this is not some approximation of some other more complex model this really is the model now the stories of course are things that we're looking at the model and then we're the ones who are trying to play detective and figure out what the models do why the model is doing what it's doing but we know exactly how the model is going to make its predictions because we can see these graphs the model it turns out it's the most accurate model we can train it's more accurate than any other model anyone has ever trained on this dataset so it turns out there's no loss in using a model that's this intelligible for this kind of problem so you might as well use a model where you can actually understand everything is supposed to say some black box model like a random forest or neural net where you wouldn't understand these sorts of things that are happening the model moreover because the model is these graphs we can actually edit the model so I mean how many people think that a person who's a hundred and five is actually intrinsically lower risk than a person who's not ninety-five right that that doesn't make sense biologically so one of the things we can do is we can modify the model we can redraw that part of the model make risco slowly monotonically up perhaps past age eighty or eighty five and if we do that the model will do our bidding now we got to be careful we're overruling the statistics in the data the statistics and the data really do suggest that people over 100 or lower risk but we don't think it's because they're intrinsically biologically lower risk we think it's because they've received exceptional care once they break this magic threshold of 100 so we've got to decide whether this is appropriate or not for us to overrule the statistics now there are other things we could do we could maybe fix this jump that happens at 85 but I'm actually happy to leave that in the model because that you know our explanation for that is that the patient's over 85 aren't getting as aggressive treatment as the patients under 85 so if I predict that they're higher risk then maybe they'll get more aggressive treatments so this this could only be good for the patients is what we're thinking by leaving that in the model and then this effect that's happening at retirement I obviously can't change when people retire but what we'd love to do next time we collect the data is have a new variable which is the retirement variable and then all this complexity in the graph that's happening in the the mid to upper 60s all of that we'll move over to a new 57th graph that'll be the retirement effect and then we'll see a cleaner response of risk as a function of intrinsic age with the retirement effect now removed from it so that's a kind of interesting thing we're we're talking about increasing the complexity of the model by adding a feature to it by adding another graph to it and yet that could make each of the graphs that we have to look at easier to understand and intrinsically more pure so that's an interesting trade-off there suppose for an insurance company and we just want to make predictions we don't want to intervene in anyone's care it turns out the models already a good model we don't necessarily have to fix these things and in fact we might be interested in the fact people over 100 actually have a better chance of surviving from pneumonia presumably because they get such high quality care so as an insurance company we're already happy with the original model it's telling us about the statistics of the way the world really works and we're not planning to intervene in that world so we're happy to use this model as it is that's a really important lesson that as data scientists as detectives we have to take to heart which is that model correctness data correctness the appropriateness of using the model and its predictions completely depends on what we're going to do with those predictions if we're going to intervene in patient care then we may have to correct some things in the model but if we're just going to use the model to make predictions about you know financial decisions how much money to keep in the bank then maybe we don't want to edit the model this is the most important lesson from the work is that if you don't know exactly how the predictions will be used you actually cannot assess whether the model is good or bad or whether you should edit the model let me show you a few things the model learned it learned that jumps at around I mean there's jump set around numbers all over the place in all of the graphs I can't show you most of the graphs they're almost always due to sort of human effects policy effects social decisions very rarely are there phase changes in biology and physics so these are almost always because of humans and I'll show you a few more of these on some other graphs for other problems the model learns by the way that asthma is good for you and it learns that heart disease and chest pain are good for you that if you've got pneumonia it's actually good to be not only over a hundred but it's good to have asthma heart disease and chest pain let me show you what the graphs for that look like so remember if risk goes down that means it's good for you it lowers your risk so for the asthma graph if you don't have asthma on the left your risk is near zero it doesn't increase your risk very much but if you do have asthma it lowers your risk by about 0.2 which is significant for chest pain it also lowers your risk a little more than point - if you've got chest pain and then the biggest effect it turns out is if you've got an obstructed airway which you know doesn't sound good but it turns out if you have an obstructed airway your breathing sounds terrible and because of that you'll pay a lot of attention and your doctors will pay a lot of attention so it turns out all of these things asthma heart disease and obstructed airway there are all things that cause you to get to care faster and when you get to care to get exceptionally high quality care because you're considered to be seriously ill in a way that it could actually put you at higher risk from pneumonia so that's what's happening is the model is learning these confounding effects where these variables which doctors clearly tell us these are actually bad things for pneumonia patients these things actually look like statistically they're good for you and that's because these kinds of patients actually got to care faster and then they got very high quality care when they got there okay so that's a fundamental problem as a data scientist these are the kinds of things you need to be able to see in your model so that you can detect these sorts of mistakes that your models are going to make and and then fix them in with this class of model you can fix these things as well there's no way to collect the data set that doesn't have these things in it because healthcare is operating for all of the individuals we're never going to sort of withhold proper health care from these individuals just to sort of train a more accurate model so you have to learn to live with these kinds of things so just to jump forward I want to point out that the model is rewarded by predicting all these strange things with extra low accuracy on the test set and that's because the test set looks just like the training set it has all the same biases in it so if we edit the model to correct these effects the accuracy on the test set will go down what we believe the accuracy of the model in the real world will go up so this highlights the large difference we can see between laboratories sort of practice and test sets which we use all the time and machine learning to evaluate things and the performance that model might exhibit when when deployed in the real world and how they can be very very very different if you see a model that looks like it has superhuman performance you really should question whether that superhuman performance would translate to deployment in the real world okay any sufficiently complex model that's accurate is going to learn these things so so it's not just a particular property of this model class in this model class that you can see it and fix these things you might think you could fix these problems by removing some of these features unfortunately you can because of correlation the model will learn these same things from other correlated input features the bias is actually coming from the output signals not the inputs all right let me show you a few things and then we'll wrap up so obviously poor tools make it hard to see things so this is just a thought experiment imagine we have a data set generated by these three graphs so we just have three features and imagine we fit it with a straight line so that's my example of a poor tool it's a linear model on data that's obviously not linear so we might fit a straight line to the first feature that way to data from the first feature that way data from the second feature this way the third feature that way those are pretty good fits right it's the best you could probably do with a straight line here's the interesting thing if the features are correlated and if there's any noise in the data it turns out you may not learn those three straight lines and in fact it's very possible that you'll learn those three straight lines and notice that the effect for feature three actually has the wrong slope and we see this all the time this is one of the important reasons for using a model that has sufficient complexity that it can accurately accurately capture the phenomena that's happening in the data if you use a model that's too simple it turns out what it's telling you about what it learn can actually be wrong and signs can even be wrong so this is very important okay but go tools really do solve some of these problems so let me just show you some fun things in graphs so here's a graph of risk as a function of blood urea nitrogen and what doctors have told us is so fascinating about this graph is that risk is very high already at 40 whereas doctors tell us they've all been taught that 50 a very round number is the threshold at which they should start treating patients so doctor said this was interesting because it told them they should probably start treating patients at a lower level of bun than they had been taught in in med school but there's something even more interesting happening here notice that risk flattens off after 50 and then notice it flattens off and even drops after 100 right and fifty and a hundred or very round numbers those are treatment effects so what's happening is the patients once they get to 50 are you treatment the treatments very effective and it actually causes their risk to level off and once they get above 100 they give an even higher dose of the treatment and that even in some cases causes the risk to drop and you end up with weird cases where patients who have buns of 90 are actually at higher risk than patients who have buns of 110 and that's because the patients at 90 didn't receive a strong a dose as the patients received at 110 so so that's sort of interesting now here's a question if we know there's treatment effects in the model like this should we or should we not edit the model to correct for those treatment effects turns out sometimes you should and sometimes you shouldn't and that's what I'm gonna leave you with is imagine the following a blood pressure is something that's very easy to measure first thing you do when you get to any doctors office is they measure blood pressure you might even be measuring it at home if we predict that your lower high risk the odds are you're still going to get your blood pressure checked and if the doctor notes that you have very high blood pressure you're gonna receive treatment for it independent of whether we predicted your high risk or low risk so that means if we're seeing an effect a treatment effect due to blood pressure in our graphs maybe we don't have to correct for it because the intervention that we might do with our prediction isn't going to affect whether you get that treatment but suppose it's like this we're only going to measure your bun perhaps if we predicted your high enough risk that you need the lab test that means we won't know that your bun is high if we didn't do the test so that means our prediction can affect whether you get the test therefore it will affect whether we know your bun is high therefore will affect whether you get treated for your high bun or whether you even get an extra high dose for your high bun that means maybe if the model is going to be used in such a way that it's predictions could even intervene in whether the test is ever done so that we know you have blen high enough that you need to receive a treatment then probably we should correct the model so that it does not predict that you're low risk because you receive the treatment because then that would be circular we would predict the patients who did receive the treatment or low-risk then they wouldn't get the test because their predicted to be low risk then we would never know they had high blood and they would never get it and then they would be at higher risk okay I'm just gonna stop there there's a number of things like this where by having good tools we're able to find all sorts of defects that happen in the data and that translates to defects in our model and if we have good tools intelligibility transparency into our model we're able to find these effects and then we can do what's going to be won in some cases hard detective work to understand why why the model learn that why that's true in the statistics and then possibly even harder work to try to decide where should we be listening to a human expert to rewrite part of the model or where should we trust the statistics that the model has learned and let them stand intact so so this is going to sort of create a necessary kind of dialogue between experts and data scientists and the model to try to figure out what parts of the model should remain unchanged hopefully most of it and what other parts of the model actually do need to be edited because of the way the model is going to be used and you might need to edit a model in different ways for different intended uses of exactly the same model thank you thank you [Applause] 