 hello and welcome to our webinar people centered design principles for AI implementation I'm Allie McDonald a senior editor at MIT Sloan management review and I'll be your moderator today the audio for this event will be recorded and will be available to all attendees approximately three to four business days after the end of the live event we welcome your questions for our speakers today to submit questions please enter them any time in the questions module on the go to webinar control panel or you can submit questions on Twitter using the hashtag MIT ASMR event will answer as many questions as time permits in the hour if you're having audio or other difficulties please check the help link in the upper part of your console our speakers today are Rey Wong and David Bray Rey is CEO of constellation research in Palo Alto California and David is the executive director of the people centered internet coalition and senior fellow at the Florida Institute for human and machine cognition our thanks go out to SAS for their sponsorship of this webinar David and Ray we're looking forward to this discussion today and we're talking about people centered design principles for AI implementation to start off we actually want to hear from you our audience first and we have a poll question for you to answer this question is what stage of AI adoption is your organization in you can choose from the following options which are you're just starting to explore you have funded an initiative or you are using AI in your organization now so ray and David based off your experience where do you think most organizations are positioned currently ray do you want a that one first would you like me to yeah good so first thanks for the opportunity and my perception is it's kind of an interesting dichotomy we're probably a lot of people feel like they're not fully embracing AI or they haven't really put their their toes in the water yet but I think when they really begin to look underneath the hood some things that they may not be calling AI that may be either expert systems or decision support systems that are sort of the previous wave of what AI was in the 70s and 80s they are using those systems I mean even just sort of intelligence sort of search or business analytics and so it's this interesting thing we're probably people feel like they really haven't fully embraced this most recent wave but I think when they actually sort of look underneath the hood as what they're doing they would be surprised that some of the things that they they would not consider to have some dimension of a high they probably already are ray your thoughts yeah no I agree and I think what what is happening is people are using AI in different ways the catch-all of AI is such a broad category what they're starting to realize is that they're seeing different levels of AI being use and the exploration part is happening we just did a survey of 80 early adopters and it's the same thing they've just become the process they're getting to see some levels of ROI but they know there's a lot more and there's a lot more ahead of them so it's going to be very exciting terrific well that's really helpful to know some of your own experience with this and then based off of our poll results that are now in it looks like 2/3 of our audience are just starting to explore 12% have found funded and initiative and then 20% are actually using AI and the organization currently so that's also encouraging too so that's that sets up sets us up very well so I will turn it over to you Ray and David for your presentation yeah he thanks a lot so let's spend some time talking about where AI is we're going to talk like the progression and so let's move to the next slide well we can I talk about where we are in this path to AI now let me interesting things about this path to AI is that AI is often known through many different names and different categories and the way we're using AI right now in this conversation loosely is to talk about a category of different ways that are getting us to augmented or artificial intelligence now most people are starting out with predictive analytics big data that's the foundation but then over time what we're going to do is we add machine learning and we will get to at some point the neural networks the cognitive computing and then way way out at some point narrow AI and general artificial intelligence now what is hot at this moment really is the predictive analytics and big data bringing that data assembly in a place where then can be used and then of course the machine learning as we apply different algorithms and patterns so that we can understand what what's in that data most people have lots of data it's not about having more data it's about getting better insight and what we're trying to do is go from data to decisions and in that process of going from data decisions what we're thinking about is how do we take that data how do we align that to business processes order to cash it'll hire to retire incident to resolution campaign to lead and once we align them with those business processes we take those information close and we start looking for patterns once we find those patterns then we have something we can take action against and so we go from data to information flows information flows to insight and then insight to action and that's the data decisions paradigm that we're trying to do the techniques over time are coming through predictive analytics and big data machine learning neural networks and cognitive computing and then over time these systems will start becoming autonomous and building their cells their narrow AI and the general AI so David anything you want to add to that as well I think the folks would be to recognize that we've been through different waves of the 50s with others that I really was trying to create programs and algorithms that could basically play games or perform human tasks as good as humans and then fast forward to the second wave that's where it was really looking at sort of the idea expert systems could a machine if you told it enough room answer as good as a human expert in terms of whether it was health care trying to do identification say cardiovascular disease but also it was called decision support systems and group decision support systems and that way it was really trying to figure out how you could collectively make decisions better as a group with the AI also being present and so we continuously strikes with that as well the most recent thesis of the I they've been able to actually in just the last two years create algorithms that can actually be better than the top five percent or more doctors in the world and identifying diseases of the eye by doing retinal scans and other images so really this third wave which is really sort of the idea of machine learning neural networks and what goes from beyond there this has really become possible in part because of cloud computing and other computational capabilities we know how to storage we now have the memory to do what was quite frankly algorithms that were identified in the 80s but just weren't feasible because we didn't have the cloud service in the cloud infrastructure necessary to do this now and it's just worth noting that will probably see other ways of AI in the future as well but we're I know experiencing what I would call the third wave of AI yeah that's a great point I mean this has been around for a long time and and you can take the algorithms that were developed in the 80s that didn't have the compute power and pass them and a lot of people are doing that right now and so a lot of the algorithms are there now what's interesting about this as we go to the next slide is what you're going to see is really four things that people are trying to accomplish with AI one is a level of automation as you can tell that's a Tesla plant but the funny thing about this automation in this Tesla plant is while this looks like a fully automated plant they decided that they needed to add more people to improve quality yes I think I said that they actually added more human to improve quality because there's only certain things machines can do they'll do it exactly the way they're being told but that level automation allowed them to free up in place to become more strategic so that they can actually think about what else to do or how they could actually improve quality while those machines were there now if we go to the next slide what you're going to see is this notion of augmentation and so after you get to level of automation you're thinking what else can we do and that augmentation is really talking about the fact that we are now able to build upon those skills build on that so that we can do more and a lot of notion of having artificial intelligence place is allowing us to think outside of the box apply these algorithms apply these new tools so that you have time to actually do more it's not going to make a decision for you but it's going to help you make a faster decision and then the last the second the last one is discovery and what's interesting about discovery it's about our building to find patterns that we wouldn't have seen before you apply a topological data analysis and something to discover there were patterns that you didn't see before right they're kind of interesting clusters that you wouldn't have been found well that discovery is great on multiple fronts and identifying new things but also being used to identify areas from fraud to threat detection to security and these are kind of in place in a continuous level and then well the next class one is really about risk mitigation and compliance right and what we see in this risk mitigation and compliance is the ability to really identify those threats and in some cases prevent a threat and in other cases making sure that you're compliant with a lot of regulations compliance is a big area and also reduction of risk is also a big area that we see in businesses and so this really is kind of a great way to think of the business rationale to think about a ad but when we actually go to understanding what people are doing there's a progression of actually outcomes that occur and so in this next slide what you see is really we think about seven business outcomes that occur with AI we start by understanding hey what's happening in front of us that's perception and so perception actually has replicated all five senses our ability to see hear smell touch that is actually happening right now and it's being digitized as it gets digitized we want to understand what's happening give me an alert give me a workflow give me a task what is that notification right and so we start by identifying those notifications and then over time we realize their patterns and we say make a recommendation for us make a suggestion and then over time we can start automating them now the automation is going to be hard but it's going to happen right we're going to be able to figure out our false positives or false negatives we're going to understand why those exist and over time that training is it going to happen and when we get to that training we can get precision decisions in that automation portion when we have some level automation we get better at the prediction aspects so we what can we expect them back our prediction our planning forecasting that becomes a huge area and then prevention is the one where we actually see the most opportunity in prevention you have the ability to do the risk mitigation I'll give you a great example I'm not going to pick on a brand but there was a famous phone that basically you heard in every airport that was banned and part of the reason was a hundred and forty phones actually had batteries that caught on fire now let's think about it 140 phones for this manufacturer was really 8 Sigma in quality 8 Sigma in quality which is like 99.9 mmm I ate cygwin quality took out eight billion dollars for this manufacturer over an eight-week recall imagine if I could fly machine learning and AI to identify the root cause of that issue and if I could go from eight weeks to one week that would have been a billion dollar recall if I could go from one week to one day that might have been 100 million dollar recall what's five to ten million dollars on an AI project to prevent a hundred million dollar recall or even an eight billion dollar recall that's the power and that's how you build a business case really prevention we see is one of the huge areas and in the last case we may get to somewhere where we get to gonna have situational awareness but we're probably talking ten to twenty years out here so or maybe sooner what do you think David yes heated interests as well as it was called AI winters and so some of you think that we're talking about are probably 2025 years out at the same time if an organization doesn't start the journey now with some of the more early technologies in particular using the data they have Rapha lines in the day they have making it so that's even understandable by a machine if they wait too long others will actually disrupt and actually gained the leapfrog ahead and so it's one of those things that the journey of a thousand miles begins with a single step we should recognize that just because some of these things are more on the horizon for 20 or 25 years from now we do have to start the journey now with the things that require us to make sense of our enterprise make sense of our processes and make sense of the data we have great thank you so much David and Ray so as we get into this next portion of their presentation which we'll be getting more into what people centered AI means for organizations we have another poll question for the audience so that question is how important is it that your organization is people centered the options being is it very important somewhat important or not something that the organization has considered yet so again we'll give it one a couple of moments to respond and Bri and David what are the common reactions that you receive when posing this question in organizations yeah I would start by saying that people centered approach is kind of a Design Thinking philosophy starting by thinking about how you put the individual at the center of that design thinking process I'll let David jump in with more details but I think just so people contact us are thinking about this poll yes so Doug Doug Engelbart who is well known known as being the inventor of the computer mouse but he's done more much more than that back more than 50 years ago in 1969 he gave the mother of all demos which actually demonstrated the graphical user interface he demonstrated the the idea of file file versioning the fact that you could talk to a computer or the goal that you could talk to a computer and actually that the initial Mouse that he built was actually a wooden Mouse and and really was this idea that technology should should recognize the ways that humans can interact with it so that it's actually not that that foreign to us and you have to remember of course back then 1960s you're mainly dealing with postcards as the interface for both feeding in programs and definitely graphical user faces because he and more importantly overcome the challenges of what he called the need for communities to learn and adapt the recognition that and their way going forward if we actually had this internet working technology at the time it wasn't called the internet wasn't there yet but this ability to share insights and learn from that that different communities could try different things and learn from there and adapt and so when we talk about people centered it really is trying to extend that vision that our human strengths recognizing and possibly adapting the challenges and the community as a whole and so if you're in a business context is thinking about how he makes sure that it's actually something that's community-centric not just not just a point of ever just a transaction if you're in a case of a non-profit or a government organization how do you engage your supporters in a way that is actually empowering and thin as opposed to necessarily distancing them or having them not feel like they're part of the process so whatever type of organization you are think about when we talk about people hundred API the idea that a aia coming that's done with the community as opposed to done to the community or something where they don't feel like they have some involvement in how it's designed and how its implemented so so I'd be understanding the poll results if you can actually share them with us selling yes so I am good timing and that's really helpful context for understanding you know what you're talking about when you're saying people centered being sort of you know across external and internal consideration so the poll results are in and encouragingly 78% say that this is very important within their organizations 15% have reported that it's somewhat important and 7% are saying that this isn't something that the organization has considered yet so that's helpful as we get into this next portion of the presentation and I will turn it back to you both thank you all right so now as we move forward what we want to really share with you are things that you can do in your organization now to have people centered approaches to data and the implementation of a on and the first thing we need to recognize because there's been a lot of conversations and you've seen some some rather than two thing figures trying to bait whether an AI is good or bad is it going to be I would say that I think you'd agree to is at first we've got three thousand years of human philosophy thousand plus years and we still have the universe I don't think somehow miraculously is going to be the same they're all together and we figure out a common need to recognize that at end of the day in some respects especially the third wave of machine learning learning it's only as good as the data that's actually fit into the Machine if the data itself has biases that you don't want or it's not representative of the questions you're trying to ask or the way you court that the data or the decision the way you implement the decision from the machine themselves is not ethical then conversation with ethical itself is actually very challenging and so these are all things that we need to think about when we try to move forward when we're trying to think about how to do people approaches and raise ourselves yeah no I definitely agree and I think that what we have to understand is given the fact that people are very different outcomes and views these ethics are going to be different in every country every value set and so you do have to account for it we always have this funny question like you know you know if who lives if to autonomy equals get into a crash right is it the doctor is it the politician is that the banker is that the single mother with three kids is it the teenager is it the teenage sports star but is it the policeman or first responder I and it's a crazy question right because like who gets to decide do we play God in that and it actually they're very simple answer I'll let you guys think about this but if we know there's going to be an accident and we know who's going to be in those vehicles for that accident why don't you tell them to stay home and then send the cars out and let them have the accident so but but it's nothing is that easy right it's going to be very difficult so but let's talk about the things that we've been talking about how people can build these design principles in a I get these early in so that we can avoid and actually and that should prevent any kind of overt ethical issues sure and I think that's really what we're striving to is recognizing that at the end of the day ethics distinguishing that there's which is socially defined its normative its social when you internally determine there's times when Essex and Mauro's can actually be in conflict with each other sample for some personal world it may be a physician to assist someone even if they have for stage cancer something that with life that would be unethical by different professional societies and the physician may actually respect that actually a patient wants to and therefore stage to have that they need to pass away but that points out that morals and ethics can be in conflict that ethics are socially defined there's interesting research that shows it takes in general 10 to 20 years for humans to actually sort through ethics of any new technology that's about a generation and it's worth pointing out that back in one supposedly the British actually thought that some greens from unethical forms of warfare because they're underneath the water fast forward to World War two we know that almost played out into the sense that the British were not as far advanced in using submarines other countries were or two months taken a different stand so so so really what we're trying to do is separate from the conversations of ethics and morals what organisations can do just to in general be always in a learning mode as these technologies move forward and as societies figure out many people centered way and the first principle would espouse is transparency specifically wherever possible share the high level implementation details and what you're trying to do I'll think about this is we already have especially for for-profit companies you have to do 10k filings it would really be great if we had something similar you don't have to do it but you do something somewhere where you say here are the different data sets again not detailing them to the table and row level but really just saying we're bringing in we're bringing them together to actually run whether it's a neural network maybe it's a deep learning assessment or something like that whatever it is share the approach and say we're doing this to reach decisions involving X Y & Z and that sort of transparency at least helps people understand what you're trying to do and also what you're not trying to do and so it can help remove some of the suspicions or the doubt that's out there and actually engender trust radio thoughts I definitely agree with you and I think has parents is one of those things that we do it we put it out there because you want to be able you want people to understand like if you get to a point where there's some level of bias or there's some level of like an algorithm that's being tested or being able to figure out what's happening you want people to be able to freely test it really see what's going on and and and I think that helps with the other two that we're talking about later yes and one thing I wanted to say that Adams transparency is the idea that really that there may be cases where maybe are given of proprietary for me the information the data itself may involve some some privacy and that needs to be respected as well panel outside experts that doing disclosure that where they're brought in and serving as outside observers to make sure you're using ability because as quickly or as scale explained ability is really trying to help people understand what what is the data that come in the different data sets are being brought in and the decisions being made from that so you may not be able to specifically say why the machine is now keying in - now's a good time to do X or by one eye or in Bethany but you can at least say we've brought these data sets and here's how we're using them here's how often we refreshing them and then here are the decisions for driving from that radio section the one good yeah and the one thing I would add is look I'm bias is not a bad thing do you make decisions based on factors so it's okay if it's explicit bias and you're okay with it but what explain ability does it also helps you uncover unexpected bias and unintended bias and when you do something let's say we discriminate against left-handers with like purple hair and you don't know why and you didn't intend to do that that becomes a problem and that's why I explain ability is important yes recognize that when you are training a AI implementation you are actually training it to have a bias and that's actually how it becomes an expert but what you need to do is make sure you're intentional in the biases you want to have present versus the unconscious biases or the social biases that you don't want to bring in for example you know we know already in our current society there is unfairness and how people are paid based on either demographics involving gender or or even height unfortunately the taller you are the more your gait which maybe make sense if you're a basketball player but probably for most professions that doesn't make sense and so a lot about what advice III yeah you never know but I raised that because when you teach a machine to become an expert it is developing biases but what you want to do make sure is that you're explaining which biases you want and what you're doing to try and actually not incorporate those biases you don't want to continue which Chrysler's next point which is real important yeah exactly go ahead brain you can use it it drives us to our next point which is yeah it's very important it was just the ability undo that bias so how do we reverse this right we've got out and said oh yeah okay left Sanders with purple hair yeah we don't want the week that's wrong how do we fix that and that's really what reversibility is about yes and I think it's it's recognizing that in some respects this may actually be where a I can overcome human weaknesses because even as humans it's hard for us to unlearn something even when we become consciously made aware of it I often ask people and actually the listeners on the call could this experiment right now which is I'd ask you to question her and once you've crossed your arms if you could then do the reverse and cross them the other way instead of training the way that your reversibility is actually something that we humans actually have challenges with and we just need to make sure that when we start to use machines to make decisions and automate certain things in the workplace and other organizations that we need to be able to unlearn things if something that the machine has reached the conclusion of is not something that we want to continue so this leads to what are those three steps if those are the three principles what are the three things that you can do an organization and the first that we would actually advocate for is actually having data advocates it needs to you need to recognize that if garbage in garbage out that's a term from computer science and that if the data itself is not representative and diverse of the questions you want to ask of it the implementation itself is going to be fought and so this is something the question is whose responsibility is this I know some organizations have created two state officers well I think that's an interesting step that's putting it in fact it really needs to be stakeholders from across the entire organization because while the data officer may have our data scientists may have a lot of expertise the different business units will actually understand their data better because there at the end of how it's being done and so it's really trying to think about who your organization takes responsibility for understanding the possibly even understanding the risk of your data because it could be if the data itself has personal information in it or sensitive information that if there was a breach with some of that having data advocates not only helps you do but it actually helps you improve the risk profile of your organization as a whole to actually really quite frankly you should actually find ways of appropriately removing from an organization because maybe it's out of date or it's not something that you need to hold on to any more rain yeah yeah no I definitely agree with you and I think having data advocates I mean you can make every you can make every role update advocate it's not just someone that's in TAC or someone that's you know traditionally working with the data it's everyone in the business side as well so something to think about in functions of finance data advocate I see a customer data advocate an HR data advocate so just think about it it's cross-functional so and again steps that you can do with them consider actually having your data advocates across the organization work on a code of ethics that's people centered and actually have ways for people to elevate their concerns within your organization both outside your organization of the concerns about your products or your services but also internally as well also think about how we can actually do this all together and as Ray and I are both trying to emphasize at the end of the day we are the Calvary it's got to be all of us it cannot be relegated to just a few people in the organization this now leads to then the next step that we recommend to people which is called mindful monitoring and and really what mindful monitoring is is the idea that you need to think about different pools of data there's really going to be three pools of data the first pool of data is that which is the trusted data that you're currently using to train your AI implementation it's data that you consider having gone through some process of review you consider it it's good enough to teach the Machine to begin to reach conclusions and make recommendations as to what your organization is going to do but then there's also two separate other pools as well there's a cute pool which is potentially worthwhile data that you've not formally vetted whether or not it should become trusted or not and then there's also a naysayer pool which is really trying to help sort of identify that data which is maybe out-of-date maybe be that's no longer relevant maybe market conditions have changed or organization conditions have changed or unreliable data and this is increasingly important because unfortunately as we've seen already it's increasingly possible to generate large amounts whether it's data text audio or video that quite frankly isn't real or is fake and this could be done either intentionally or unintentionally but if organizations aren't careful if they start doing a implementations and they're not getting their data whether it should go in the trusted pool or not or whether it's no longer reliable they may find they teach their machine things that are actually dubious at best or actually lean it down a blind alley and so to sort of put this in a table form again we're talking about the data pool that is trusted that's the data that's actually that it to fit your AI training systems and then the two other ones again queue data it's data that might be useful but has not formally been vetted and then the naysayer pool that's it's actually trying to actually say you know devil's advocate this data is out of date so I'm sorry do you have thoughts on this as well I think that's important it's mindful monitoring is something that people haven't always thought through and I think you also want to take in mind that you know data isn't necessarily good or bad or high quality or low quality you have to just think about data being purposeful like does it solve the purpose of what it's doing because so folks that are data advocates and data oriented to begin with they know that they capture all the data because you never know if you need to use it somewhere at some point in time and so that that's something I think about as well right and I think we see some people that say well why don't you just why don't you at the point of collection because maybe it's not relevant or maybe it's not useful maybe it's even obscene or something that and the challenges is at the point of collection it's really hard to determine whether something is relevant or not and in some respects it may not be relevant at that moment but it may be six months later twelve months later and so you hit the nail on the head that oftentimes organizations are collecting the data but what we're saying is don't automatically pull that into your trusted pool just because you've collected it and then have some process to decide whether it goes into the trusted pool or it gets discarded or it gets gets put in the naysayer pool and said yeah it's not relevant right now maybe it will be in the future or we don't think this will always ever be relevant but that's sort of thinking through you need to recognize that if you're not careful your orientation itself may actually find that misinformation disinformation is not something that just impacts humans but could infect your AI system as well and at least have a final and third thing that people can do is actually provide bounded expectations for yours whether it's your outside community stakeholders they need to understand sets are being used to training and the way that could be informed can actually be listened as to what they're doing the idea is if it builds upon what we're already talking about which is have data advocates have people that are actually champing how data is used in your organization but it's going a step forward almost like a rather than a daily allowance food list of things that are being done with this data in this company include the following and it's making that visible in a way that can be seen by others ray you have thoughts as well yeah no I definitely agree with you there I think these bounded expectations are important one of the things that you're going to realize that a great place to actually start with a I train with AI is in heavily regulated industries like healthcare or financial services those are insurance those are great places because the expectations are abounded and you can accent them very well defined so I think that helps a lot yes and to give our audience a sort of a framework to help sort of what's called the framework and the Auris framework really is just sort of for a question what are your obligations to society to your customers in terms of the AI implementation and we can think of the cases where maybe there's some high profile large companies that on their bottom line picture about what they're doing might impact not just their bottom line social cohesion trust that that it's worth doing at least a 360 what are your obligations lower your overall obligations to your shareholders stakeholders and society as a whole once you've done that then the next is the egg which is your bias these are things that you don't really have the ability to see maybe because you don't have complete clairvoyance in the future in fact I don't think any of us do babies the recognition that you know you're coming from a perspective that is maybe us centric or Asian centric or maybe is that you're familiar with your current market but you're not from there of other sectors that you might spill over to or start to work with as well but whatever it is we'll have biases in part because we're experts in certain things but by becoming experts we also help under stings as well and then from inner the two remaining steps are the are for responsible actions based on your obligations to societies and dance on your acknowledged unknowns what are the proactive things you want to do with your implementation since you're always going to be learning I think that is so important because the reality is again technology's moving quickly societies with effects that are happening we always have to be learning and what you plan today I'll be changed and adjust not just in six months and twelve months but 18 months and 24 months onwards and then finally the last the S is for safeguards again we've already given you some ideas which is safeguards can include a data advocacy function and we have always included outside people that can actually provide input but together this simple framework of identifying your bound of expectations with regards to your obligations to society your acknowledged unknowns your responsible actions with the implementation and your safeguards can at least put you miles if not might years ahead of others when it comes to being people centered in your AI implication before we go to pivot Paul here actually is think about having an Ombuds function I mean we've seen how those functions for other things in the past but really this is a way where as you grow and move forward in your implementation have a function where concerns opportunities risk can be raised and be shared with your board and involve of your board and your outsiders I'm thinking about how the data you're using how the things you are producing using Rai services impact people ray yeah I know I agree with you and I think the important part here is when we think about this and then we apply these design principles the most important part is to humanize AI one of the interesting things we've learned over time is that it's artificial intelligence isn't there to replace us it's not us really learning from the AI it's actually the AI is learning from us and it's a inverted approach when you think about this terrific well thank you David for that and Ray as well so what that presents a really I think practical framework for people in our audience to start thinking about being people centered in their AI implementations within their organizations and as David mentioned we do have a final poll for our audience before we get into some questions for Ray and David so that question is what are the biggest barriers to adoption of AI in your organization is it assembling the expertise needed for AI assembling the data needed or being willing to adapt and work differently so we'll give everyone a chance to respond David and Ray again what are what are your some of your thoughts here I'm sure there's in some ways some combinations of different barriers but is is there one that you feel is very common right now in organizations I can start I'll tell you that like the hardest part right now is really assembling the expertise that's the part where we see a lot of scarce resources the data is out there it's question of having the right data but I don't want to bias the advice what people are gonna do in the poll and then the third one is the one that people think about a lot I don't know certain one is something that people don't think about is the fact that you do actually have to work differently so I'm going to cure it I'm not but here's a penny not everybody get to the data my perception is expertise is hard the data is out there but I will say that a lot of organizations don't realize how old stovepipe their data is Asians they were locked up in legacy 19 the data is there but it may not be in a form that's exactly and I think the last thing I would say is change is hard you know everybody says they want change unless it actually involves em changing or I've seen many places where I parachuted an organization they say we want you to be innovative just don't disrupt anything exactly results are in and I think it is fascinating at these are.this this poll has the most spread out results we're seeing with 34% of our audience feeling that the expertise is Rea touched on is right now a major barrier 21% are saying that their data so maybe it's they have the data and it's not in the best shape or maybe it isn't gathered yet and then 44% so our or winner in this poll is actually being willing to adapt and work differently so I think that does speak to what you were both just mentioning that it is a major change in paradigm shift to be adopting some of these processes and ways of working and organizations so that concludes our introduction to this topic and our initial presentation but let's continue the discussion with our Q&A portion so we've received lots of great questions from our audience and will continue to take questions for the remainder of the hour we have about 18 minutes so we'll try to get through as many as we can a reminder that you can submit your questions by entering them into the questions module and to start off we did have several questions pop up that were about sort of action and you know gaining sort of gaining you know pushing against the status quo that might be happening in an organization so how can individuals within a company who want their organization to be more people centered about data and AI elevate these ideas and approaches to senior leadership so I'll leave that to Turay or David whoever wants to kick this off my screen so I think the first thing you need to do is is you need to identify what are the strengths but also the weaknesses in the organization that you are trying to help embrace this change and I find the easiest way to do that just start off with you know if you're not in a position where you are specifically tasked to do this but you're choosing to do this organize some very informal Brown banks within your own company do something formal lunch and learns and ask people what they're thinking engage them using Socratic questions listen as much as you can because that will tell you where are people excited where you have those early adopters but also the conversations may either overtly tell you where their concerns or fears or the people that aren't present will tell you something as well because I've seen many an innovative effort say you know what we went off and we went with only the people that were excited and we left out the people that were naysayers and I'm like that's not gonna work well because if you leave out the people that are naysayers early on when you try to bring back your innovative effort to the rest of the company you will either get shot or it will just never really do is do informal listening and learning tours whether it's who lunches when super formal meetings but do whatever you can to assess the strengths and weaknesses of the organization prior to doing the AI implementation which was going to act question of an awesome answer well terrific sorry I was muted there for a second but I will that I think that's really helpful and I like I said that question kind of surfaced in several different questions so I hope that's helpful for our audience and another one to pivot here a little bit more back towards our ethics conversation so we we did have some specific questions about you know initiatives and guidelines that are already sort of coming in to you know the for instance the European Union has ethic guidelines for trust for the AI one of our participants asked how do you think we can despite different stakeholder interest successfully implement these into binding you know binding laws and decisions and organizations and how will epics sort of play a role there ya know that's a great question right when we think about the EU guidelines on AI as example right there's so much of this right it's about having human agency and oversight which means you begin something with the human you end a process with a human right there's also some aspect of making sure it's safe right and they also talked about you know transparency diversity along the way and so I think you're it's going to be subject to a lot of different like if data sits in a certain region or if a transaction that's working with certain set of citizens unfortunately we're going to have to get that flat level granularity in the future because every country is going to try to legislate some level of their version of ethics in AI and so the people-centered design principles that we're talking about today are probably going to be universal and the mega set of ethics but there are going to be things that are a little bit more granular that we might not account for and you may see that in different regions in terms of their perspectives on what their expectations of artificial intelligences will do ranges said I mean the reason why we give people the Orr's framework that includes what are your obligations your obligations will vary based on the markets you're operating in as already mentioned if you're in Europe you obviously have GD P R which obviously is asking you to respect the person's actual proactive consent you can't assume consent it must be proactive consent to use the data for purposes of moving AI at the same time if you're operating in China they've actually indicated that's starting fairly soon where there's going to be December or January it to to have data that is not available to the government is actually going to be illegal which obviously is the complete 180 from Europe and so we spell out the Auris framework to say what are your obligations to society based on the markets and the societies in which you're operating because it will vary based on the different laws of different countries and regions have terrific that's very helpful and I think one other area that goes back to the framework and I think something that you were touching on David about having data advocates within organizations that aren't necessarily only technical roles or you know involved in the creation of some of these algorithms or systems we did have some questions around then what are maybe tools or methods that people who aren't software engineers or technically you know capable of creating these systems be mindful of and maybe be aware of when it comes to detecting bias in an algorithm or something that might have negative implications for down the road sure I'll tackle it first and then Ray you can add on I would say the first step you need to always ask is does this even pass a human test which is if I had to answer this question would this data set be informative enough that if I sat down with pen and paper and enough time would I be able to answer this question so does it make sense to be using this data set to be answering this question if not then I think that begins to raise a red flag which is why are you somehow trying to correlate X with Y if it doesn't make sense but then to you need to begin to dive a little bit deeper and say if it makes sense to use that data set how was this data collected is it you know can I look at the representativeness of it involves people you know were for some reason where Native Americans left out or where minority groups left out or were people at certain ages more involved in others and so ask ask whoever you can to say give me the different sort of sections of the data to show is it representative of society or am i seeing a skewing where unfortunately maybe it's all white males or something like that in which case I've got a problem so first assess for does it make sense to be using the data to make sense as to is it representative of the questions you're trying to ask and then the third thing is what are your checks to make sure you are comfortable with the decisions that are coming out of how the AI is using this data for example the AMA eventually comes to the conclusion that having for bid but it may say that people of a certain type are more at risk of defaulting on loans well that may be something that the data shows but it doesn't mean ethically that you should then target certain groups for higher percentage loans or something that because that's not ethically right even if that's what the data is showing and it may be other reasons why it may be because that's just how things are have been historically it doesn't mean as a predictor of the future but you need to recognize that just because the data shows a correlation does not mean you should act on that if it goes against your principles as a society and as an organization yeah and I think a lot of this can be done done technically because if you've got the ability to rigorously challenge your assumptions and understand any cognitive biases or pattern biases that's not a technical function that's really about you being gonna be critical and dealing just in critical analysis but if you've been in analytics roles or critical analyst roles you're trying to figure out is the bias coming from the source is someone trying to influence you in a different way right if that data even actually accurate right now is there posit is there potential that that data it was faked right those are things that are not technical sure effect I think that's very helpful and yeah I think something that when people think about it from you know a human standpoint answering that question themselves from their own you know guideline of ethics within their organization that that frames it in a way that feels approachable so one other question we did have that got maybe into it it was surfacing and a lot of different questions was that the idea of once this has this this idea has passed and as you know senior leadership has bought into this idea of doing more people centered AI where do you start and what do some practical examples look like right now for these types of implementations do I go first right do you want me to sure I was going to say that you know if you apply these design principles and run a test against those design principles and building it then I think the next part really is what David was talking about in the last three years it's really the culture and the change that's behind right and I think that's where the training and training around data advocates becomes very very important because what those data advocates are doing is they're they're actually spreading the word that is actually carrying on the culture but they're also doing it in a way that's important for each organization and having those data assets is important but doing it on its own you still have to monitor what's happening right to make sure that you're not going astray or not going in a long way and I think that that those two things work very very well with each other and then the third part was talking about bounded expectations I think that is important because that helps gives you some guard wheels as to what's to start to test but I believe it's going to get harder and harder as you expand out larger areas that go closer to narrow AI or you're actually making decisions and because for this one reason the precision of decisions is going to be super important 99% accuracy manufacturing were like the ads is great it's going to be okay 99% accuracy and healthcare and I don't know about that right so I think that that's kind of where we're going to see some of those limitations but it's important that people do not get frustrated because the curb and I don't know which truck oh yeah hold it this way the curve is actually going to look like this and then it's really gonna wrap up pretty hard in terms of the return but the problem is somewhere along the way it does if you're putting more money in then you're getting any ROI out of but that's okay because that's part of the training piece to get to that level of precision and decisions it's going to be hard and I think it really has to be a conversation that says that people are in it to do this right and well it needs to be approached as an experimental learning activity and so I wouldn't make it your you know I wouldn't take everything you're currently doing and stop doing them to do AI you should keep on doing what you're doing but have AI as a parallel effort because as I mentioned there is going to be a period of learning there's going to be a period of diminished returns before you start seeing them that exponential gain where it actually benefits the organization and so framing that expectation internally but also externally is key and also again we've talked about the data gets all these steps we put for you they're all good the first thing I would actually recommend is as quickly as possible work with your board and your leadership to put together that ORS framework even if it's just draft because you're going to eventually face some challenges where you gonna have to make a decision I've seen cases in the past where let's say you're doing something involves public and involves public you know people can provide comments or data and unfortunately some people post comments that are either nonsensical maybe they're inflammatory they're not really contributing the conversation but then you have the question of do you move them or not and some people might say well you're removing them because they're inflammatory or they're not helpful but then other people say well wait that's censoring or that's something you know speech or something like that if you can plant that flag early and say here's what we perceive our obligations are for example our obligations are to hear everybody you if we disagree is that where we don't understand what they're saying that makes it easier to be more proactive as opposed to reactive to what's happening and so having that Moore's framework as you move forward in that always be learning mode of doing an AI implementation I think it's so key because there will be unexpected things that happen but being able to at least be grounded and saying here's how we've chosen to position ourselves as a company in as an organization it'll sort of serve as your North Star as you move towards that final goal terrific that's really helpful and one quick question that I jogged my memory when ray touched on the example of narrow AI just for our audience because this did come up early in the presentation could you give a quick summary of the difference between general AI and narrow AI sure so yeah so narrow AI so the challenge of most AI systems right now is they're fairly brittle beyond the domain in which they've been trained and so with expert systems you can create a system that's as good if not better than the top 5% of doctors in the world with cardiovascular disease or disease of the eye or a cancer detection but then the moment you try and shift to something that's maybe you know something about the lungs or about muscles the AI just falls apart because it's just not been trained in that and so the holy grail of sports the metaphorical holy grail that everyone's looking for is a system that is not just narrowly trained on whatever specific data it's been exposed to but much like a human it can have a much more diverse conversation so we can switch from not just different medical topics but then suddenly if you decide to ask it about you know how about that sporting event or how about that stock return or something like that you can have a fairly robust conversation and and provide insights across a wide range of topics as opposed to just a narrow range of topics and one of the things that's going to happen over time is you're gonna have a bunch of narrow AI capabilities like it aggregated that become part of your general AI and so it's all working towards that but I think it's really important understand that certain tasks are so different than others it's about putting those together I mean it makes you wonder like how the human brain actually works and it's done below all we can do so many things what actually a new theory coming out that actually while we think we have one conscious voice that makes up our brain there's actually probably many and that really loosen up a conscious voice and so very very well be the solution to generally I may be to have many different narrowing eyes and then some bridging function across all of them in the future well something else the filter of the voices in my head I mean the yeah as long as we don't get that brain computer interface for social media I think we're all good so everything's good malfunction you're hearing more more voices than you normally do well this has been fantastic and David and Ray I really enjoyed first I really enjoyed your article and having worked with you both on this I've personally learned so much more about where AI is right now within organizations and this really helpful framework for thinking about how we can be more people centered in our approach so that the projects that do get off the ground within organizations are you know really working for all stakeholders involved so that's actually all the time we have for today's Q&A session so over the next few days please look out for a feedback survey that will be sending via email and we greatly appreciate your thoughts and opinions so we're also a reminder that the recording for this program and the slides will be available within three to four business days so look for look for an email with that and that finally concludes our program thank you for attending thank you so much to our presenters Rae Wong and David Bray and thank you again to our sponsor SAS 