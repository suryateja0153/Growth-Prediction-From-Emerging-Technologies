 [Music] my name's Andrew machete Solutions Architect with Google clouds healthcare and life sciences team and one review for you today some of the tools that we have for doing different types of genomic analyses on Google cloud so first a brief introduction to our team the healthcare and life sciences team is a multifunctional team investing in building products that are specific to these industries we also work to help deploy these solutions with customers and then scale through partners to help expand and expedite that development today we're going to focus on a number of tools for biomedical research ranging from public data set hosting to our pipelines api product some workflow orchestration tools for doing both genomics and other research workloads we'll cover a few open source tools that we have and some integrations in the space as well and finally we'll get into some large-scale analytics at the end so on the topic of public datasets GCP hosts a number of public datasets from a variety of industries some of those from genomics these public datasets are available for you to query up to a terabyte a month for free and then as a data host you also have the ability to bring your own datasets and we have a public hosting program that you can get in touch with if there's something that you think is beneficial or you can also just host your own make it public if you'd like or access control that however you'd like to control who you want to give that to and when you're hosting these public datasets whoever is doing the actual usage so the analytics is going to be responsible for any of the costs of the querying or the egress costs if they're in buckets so as a data host you know that you only pay for the storage and you're not responsible for any of the computation that someone might do so in genomics we host a number of datasets including thousand genomes the Illumina Platinum genomes TCGA the missing database for autism simon's genome diversity project as well as some others and we provide these in both the raw files that are available in Google Cloud storage buckets and then we provide them in bigquery as well where those datasets are available for you to do analysis or to actually compare those with some of your own data sets that you could bring in the similar space we also host a number of annotation sources including clinvar and cosmic and these can be used for sample annotation pipelines and there's a number of sample queries available online as well showing how to join these with other datasets such as those that you might bring on your own so this is our page for the genomics public datasets page and we'll take a look at it here in a second but at the top you can see we've got links to both the bigquery and the storage buckets or we can actually browse it by specific datasets to see a little more detail so if we look at this here and we go to that dataset page again the top has both the storage and the bigquery and in the bottom we can go to one of the individual data sets so if we go to thousand genomes for example you'll be able to see all the specifications of of which tables we have in there broken down by where they are what versions and then what's in a storage bucket versus what's available in bigquery if we go into that here you can see for example the phase 3 of thousand genomes we can go to the VCF s-- and go ahead and browse through that full sample size the full sample set and these are available for you to test your own workflows or to use as well in bigquery we can go here to see the variant annotation datasets and this actually includes both the annotations as well as the other datasets so on the left here you can see one is the annotations the other one being the variants and the various tables that are a part of each of those so next I'd like to cover one of our main products in this space and that's pipeline's API it's also referred to as genomics API which was an earlier name of the product the usage being almost exclusively for genomics to start the product now is available for any sort of research workload and it works well for genomic pipelines but also can handle other things as well so even though we can do other things one of the more common uses of it is to do secondary analysis and in this case we can use it to run standard tools such as gatk or other optimized machine learning tools such as deep variant by using pipeline's API we can optimize for time and cost through a number of different ways these include things like parallel execution being able to use GPUs or TP use our tensor processing units persistent disk and different disk types as well as different instances such as preemptable instances which are instances that are much cheaper but may get shut down if there's a spike in demand we also work with a number of workflow engines to incorporate directly with pipeline's API and you can see some of those partners here and then also some partners who provide their own solutions in this space such as fire cloud and Terra DNA stack seven bridges BC platforms and a number of others the work that we're doing in this space is informed by the interest that we see in the industry so if there are things that you're using or partners you're working with that aren't available reach out we'd love to hear some feedback on that so we know what we can do to help drive the industry forward so some of the workflow engines that we integrate with include Cromwell D sub next flow is a newer one that we now incorporate with and then we're working with a number of others as well to bring that support to the other workflow engines now the advantage of working with a workflow engine is it abstracts away the process of you having to start the VMs manage the whole process of that and what you provide is the container that has the task you want to run you include the VM and so any of the specifications you have for that VM ranging from a very high-level definition where you might just say you want a standard one instance all the way down to the point where you can configure the number of CPUs which CPU architecture you need if you're taking advantage of some of the newer Intel chips the ability to attach GPUs and TP use and specify which ones you want and different disk types as well which can include SSDs even locally attached SSDs if you really need that throughput so when you're using the workflow engine now it's easier for you to run these in various environments and not have to change everything just to move to Google Cloud you don't have to learn the intricacies of the API because we've done that work with the workflow engines themselves so you just define the tasks you have and the workflow engine will make those calls for you another benefit of this is that you don't have to monitor the status of all of the tasks as they go through and the workflow engine will track when an operation starts what needs to come next when it should start one and if it needs to retry a certain task due to failure or preemption so when we use pipeline's API it starts these operations using a container for each task again you define the resources that you need for each task and this is something that you can specify at an individual task level instead of just the entire pipeline so you can really start to optimize for each task and you can realize which parts need high throughput which parts need memory and maybe only a certain part that needs an attached accelerator like a GPU or certain parts of it that may be more suitable to being preempted if you can restart them so we'll take a look here at a demo of using Cromwell to run the germline snips and in Dells from gatk best practices and the demo we're going to look at is actually two separate pipelines and so what we have here is Cromwell in a container and the first pipeline we send the container that includes Cromwell and the inputs that we provide to that or the Whittle the workflow definition and then a list of the inputs and options we want for the run so that first VM we start is actually a VM with Cromwell once that's up and running it then will start the full pipeline here of gatk and then it'll run through all of those again the one with Cromwell when it's finished it knows that then it'll shut down as well through pipelines so it allows you to manage all of that without having to install something specifically now if you have a working instance of Cromwell installed somewhere else you can definitely use that but this is an opportunity we provide to show how you can do it without needing to install anything on your own so to start this here we're gonna clone a couple of repositories with the code we're using here and set some environment variables about where we want our input and outputs to go and then the command that we start here there's a few options if I know it's a little small here but we include things like the region in the zone where we want the job to run the input files the output locations where we want to save our logging as well and once we run it at the bottom we get an operation ID so that number there this allows us to follow up and track that operation if we need to troubleshoot or maybe come back later and in the UI here if we go to the genomics part of the of the cloud console you see a list of operations here and we can track when they've been completed we can look at the tags to see what part of the tasks they were so once that's all completed we can now see the output of these in a Google cloud storage bucket and this includes you know our final output that we were looking for it includes any of the logs from different tasks throughout as well as standard out and standard error so if we need to go back and troubleshoot a particular task we have the ability to go back and see that and then any of the work intermediates as well that we may have saved either for checking or doing QC and QA so here we'll go to the storage bucket and as we go in here you'll see that in the work we've got this job ID and this is what correlates to the job we just ran you can see here a different folder for each one of those calls so each of the tasks we used and coming in here again we can see the output from those and the errors and logging so if we need to come back and check that going to the output folder we have all of the different outputs from the various tasks that were part of this this includes some metrics and reports that were generated as a part of that process and then also we can go through here to see the index and our VCF as well which is kind of our end goal the next example is using d-sub and so when we use d-sub here we're actually going to install d-sub in the cloud shell so again you don't have to install anything on your own machine and you can run the entire thing just from the cloud so the example we're going to use here is using data from thousand genomes to do indexing and we're going to use Sam tools in a docker container to do that so we're going to start again by cloning the repository with what we need and I'm going to install d-sub in a virtual environment again this is running in that cloud console so I don't have to worry about installing or maintaining that the command to run it here is very similar to the last one if you see some of the options here we provide a zone we provide an input and output the container that we want to use and then once that's running we can actually go and track the resources that it's generating as well so if we come to the compute engine page you can see this is the instance as well that's been started that's actually running that job and then going to storage we can go ahead and see the output from that as well as some of the logs such as standard out and error if we need to troubleshoot some of that next up is deep variant so deep variant is a snips and in Dell variant collar and that uses deep neural networks excuse me and this is developed as a collaboration between Google research and verily life sciences and the cloud healthcare and life sciences team partnered with them to optimize this for cost and speed running on GCP and if you look at the documentation pages for deep variant in our cloud documentation there's a couple different settings we provide there of a version that's optimized for time and a version that's optimized for cost and we can do this by using different resources attaching GPUs for example and by changing that we can find the right trade-off between cost and time that works for your business or your research so in this case we're going to run deep variant from a docker container very similar to the last ones so we'll take a look at the script here that's going to start deep variant again we set a number of variables at the beginning such as our project the input the reference genome that we want to use the zone where we're going to have this run and then at the bottom is the command that's making the call to pipeline's API and that will look very similar to the last ones as well so once we start it we get that operation ID and this one will jump ahead here and we can go ahead and look at the output in our storage bucket and get to that VCF file as well one of the newest additions we have here is next flow who recently added this a couple of months ago if you're not familiar next flow is a multi-platform workflow engine that supports a variety of backends ranging from various cloud providers to on-prem to HPC clusters to even running locally and what we built in here in working with the community to develop this is the ability for you to choose Google pipelines and Google cloud platform as the backend that you want to where you want to run those tasks so this allows you to take an existing workflow that you have that may be running somewhere else and you can now move that to run with pipeline's API by changing the executor and then a couple of the settings in there as well again the things like project machine types and then in this case it'll use pipeline's API as the backend for that and so all of your computation then we'll run through pipelines on Google compute engine there's also an option with next flow where next flow can set up a cluster directly on GCE with configurable master and worker nodes so that's an option you can use as well the example we're going to look at here is with running an RNA seek proof-of-concept that's using pipeline's API as the backend again we're going to do this whole thing in the cloud shell so we don't actually have to install the next full application on our local machine although you can if you'd like but we'll show a demo here of doing it all in the cloud and then we're gonna use that installed version of next flow that's in cloud shell to go ahead and run our pipeline so let's take a look at the config here our next flow config and you'll see at the top there where we set Google pipelines and this is where you would change that depending where you want it to run we then set a couple of specific options such as the instance types we want to use in the project and then we'll go ahead and start that version of next flow right now running in the cloud as the tasks go through you'll see an ID on the left there and we can use those to track the resources like we saw before going in to the operations and compute engine to actually see which instances are doing which which tasks and follow up on those next up we'll get into variant transforms so variant transforms is a tool that we open sourced for transforming and processing VCF files to bring them into bigquery to do analytics now variant transforms runs using dataflow which if you're not familiar supports the Apache beam SDK in both batch and streaming mode although in this case we're going to use it in batch and then variant transforms also supports both direct runner and dataflow runner and you can pick which one of those is ideal for your for the operation you're running so some of the challenges that we face in tertiary analysis include trying to analyze large amounts of data and join that with public datasets or perhaps with data sets that we're receiving from a collaborator or at a different facility the ability to join those variants with different annotation sources where we're trying to understand an annotation specific to that organism we have tasks like QA and QC that we're doing we may have multi sample processing as well and then doing things like Association analysis population analysis G wasps so for all of those things bigquery is a great tool for that we'll get into that in a second here but to be able to support that we really need a way to import this data into bigquery in a reliable reproducible way with a standard format and variant transforms is what does that for us so variant transforms uses parallel batch processing and data flow to do this reading in VCF files either single files multiple files and then we'll end up dumping those into a bigquery table now in addition to importing we realize that sometimes you may have a tool that still needs VCF files the reality is a lot of things have already been written and they may not have an integration with bigquery and so you may need the VCF files back variant transforms also supports pulling things back out of bigquery to give you a VCF file so you can maybe identify a subgroup from your data that's in bigquery export that to a VCF that you can then put into your other tools and really use bigquery as that storage as well another recent addition to variant transforms is support for doing annotations and right now we have support for doing vet annotation either importing something that's been annotated or performing annotation on a data set that you already have as we continue this development we're looking at other annotation sources as well so if there are others that you're interested in or something that would be useful again we'd love to hear the feedback there and see what would be useful for you at the end of the talk we've got a mailing list and I'll point that out but if you've got feedback or questions feel free to send it to that group and we'd love to hear from you so this example here looking at this screenshot is running variant transforms on the 2500 samples of thousand genomes and this particular set is about 81 million variants so it ends up being an 81 million row table and the screenshot we're looking at is the data flow console in Google Cloud console and you can see here this this task is about a hundred days of compute time and we've completed it here in just under two hours but you'll notice we could go faster if we look at the right that blue line on the chart where that kind of plateaus so that line is the number of workers that we had working on the task and you can see it hit a limit there which was the quota that I have assigned and in this case I have had a quota set for four disk space for how much SSD I was willing to use at the same time so you can use things like these quotas to help control costs or figure out how much you're willing to do at a single time in this particular case it scaled up to about 400 workers and then that's where it stopped if you wanted to increase that we can increase the time it'll be done faster and again that's where you get into the balance of cost versus time now in running it with more disk space you're obviously going to pay for more disk space but you'll have much less compute time and so that's where you start to balance the differences in cost of compute purses cost of storage so the next sample we'll take a look at is basically running this on a smaller subset of data that's about a hundred variants we use that just for scale so when we take a look at the script we're going to use to run variant transforms we list things like the input bucket with our VCF the output table in bigquery a temp location and then some information about our product at the bottom you'll also see a Google pipeline's command we're actually running variant transforms through Google pipelines just like we did before so once that starts we get that operation we can come to compute engine and we can see the main node here that we've started and this is the worker that's actually going to kick off that dataflow job so then coming into data flow we can start to get into the specifics of that run we can follow it to see where it's at in the process understand the resource consumption and once it's done we'll go into bigquery to see the output coming into the dataset in table you'll see the schema listed here and this is the schema that we use for variance and you can see the information as well as the types for example so you know how to query that field we can look at some details about that showing us the size of the table the rows the location where it's stored and then this preview field lets us see a sample set of the data so it shows us how the data is stored and if you're more of a visual learner this is a spot you can come to see what that looks like before you start writing your queries so once it's there bigquery is our server less scalable data warehouse and bigquery scales to petabytes of storage it provides a standard sequel dialect for doing queries so you don't have to learn a new language to use it one of the features is that it actually separates compute and storage from each other and so in this case particularly in a research environment is very important because as a data host you know exactly what you're gonna be on the hook for you upload a data set you're gonna pay the storage cost of that and you know you're only gonna pay the storage cost you're not worried about what analytic someone else is going to run against that and as a user you know that you're paying for an analysis that you're doing and we show you how to actually how much you can expect to pay for those as well as a data host we also offer a lot of different ways to do access control and you can do some fine grained access control as well if you're providing your data set to different collaborators for example one of the ways that we do that is through using authorized views and authorized views are a way to let you share your query results without actually sharing the underlying data so let's say you have a table with some data set and you want to give access to a subset of that whether that's certain columns that aren't relevant for that person maybe it's certain rows or do two different consent or usage agreements you're only allowed to provide certain samples to a particular user in that case you write a query that pulls out that subgroup and you save your results as an authorized view the end user that you now give access to the authorized view queries that view as though it were a table so they don't know anything different they think that's the table they use the same syntax and everything as though they were querying the table but they don't get access to the underlying data and you decide what they get to see some other features in bigquery our query validation so on the right here you can see this green check box and that's going to let us know that it's a valid query we wrote if it's not valid we get a little red X if you hover over that it'll start to give you some feedback as well and let you know what's missing in the syntax give you some tips as to how to maybe fix that and then next to that the green text is well tells you how much data it's expected to process when you run this query so the cost the pricing model for bigquery is storage and data processing so time is not something that you're charged on but you're charged on the amount of data that gets processed and so we'll show you here how much data we expect to process if you were if you were to run this query and it lets you understand is this really a query you want to run or is it something where maybe you need to optimize your query before you actually run that we provide a number of different ways to actually integrate with bigquery we have the UI here so if that's something you're more familiar with you can run your queries from here you don't have to leave your browser window open when you run them your queries will get started as a job you can come back and find them later if you run the same query again results are cached for an amount of time as well so that if you're running the same thing you're not necessarily getting charged every time you do that we also provide a command line tool for running your queries and so in that case that's something you can run on your local machine through the Google Cloud SDK or you can do it through the cloud console which already has that tool installed there's also an API for it as well as a number of client libraries the client libraries we include right now our Java Go Python Ruby nodejs and c-sharp the advantages of this mean that if you've already got an application in one of those languages you don't have to write a separate integration for bigquery you can import the bigquery library for your language have your query happen right there and get your results in the query so when we're using bigquery as a variant store one of the things we need to standardize here is the schema that we're going to use and this schema here that we have is based on a standard VCF and the way we represent this through a few different data types so we have simple fields such as integers and strings we have nested fields which show up in here listed as a record and then we also can have repeated fields as well so in this case we may have multiple alternate bases and within alternate bases there are different ones we want to keep track of so we represent those as a repeated nested field we also store the information from the VFC header so things that are in format and info for example you can see at the bottom here under alternate basis those are things that we've pulled out of the header and then we'll go ahead and store those again we can also put those back into a into the VCF if you need to pull that back out of bigquery this is suitable for storing single sample or multiple samples and variant transforms when you're importing is also able to validate a VCF before you import it so for example if you have a lot of samples you're importing it once the last thing you want is to get all the way to the end and then have it fail because you have some malformed records in one of your files so we can actually do a quick validation step where it'll go through before and let you know if there's any issues and you can decide what you want to do at that point if it finds a malformed record whether want to try to guess what to do if you want to ignore it if you want to abort the task though that's something that you can configure there and then again as I mentioned the the ability to export that back to VCF as well because we have this standard schema we can get it back to that same spot if you're querying multiple independently repeated fields in that case you're gonna actually need to unnecessaries and the sample set here is just over 10,000 samples which is about 361 million variants that we're using and the initial import of that is a 23 terabyte table now these were imported using variant transforms and actually did this in four batches and that snapshot we saw earlier from dataflow was one of the batches where I was actually adding 2500 more samples to this existing table now when you're doing those imports again you decide if you want a new table or if you want to append it to an existing one and when you append it to an existing table it's an atomic operation so nothing is actually there until the whole jobs completed so you don't have to worry about it failing at the end and ending up with a mismatch of records or if someone's running a query while you're updating the records they'll get one version or the other depending when they run the query but you're never going to get a mix of both so we're gonna go through the next few slides and look at some tips for optimizing queries and the things we're gonna optimize for our run time storage and the amount of data that we process now as I mentioned the cost of bigquery are related to the amount of data you process and the storage so those are easy things we would think to optimize because they drastically can reduce the cost that we pay for this in addition run time can also be important depending what you're using the output of this query or if you have other processes in your business or something else in your lab that depends on this output there might be times when the run time is actually more important to you than the cost or equally as important so we'll look at how to optimize there as well so this first query we're going to look at a fairly simple one and we're just counting all the variants and we're gonna order them by chromosome and so I do this because this is a query that shows where we're actually gonna have to go through and we're gonna have to process all of the rows because we're going to count all of the variants so we're gonna go through and count everything where there's a genotype greater than zero and then we're gonna group those by chromosome and so in here you'll see that group by reference name and that's where we'll go ahead and order those and once they're ordered we'll actually go through and we'll count them in each group and report them back so when we run this on the initially imported table it processes about 13 terabytes of data and takes a little over four minutes to run that is expensive and time consuming and we can do much better in both of those and we'll show you a couple of techniques of how you can deal with that so the first optimization technique we have is partitioning and clustering of tables now in this case at the bottom you'll see that we cluster this table on reference name which is the chromosome and then the start and end position so essentially we're taking all the variants and we're actually lining them up in order and that's how we're going to physically store the data so we're actually going to bigquery is now going to co-locate the data that's in the same cluster in the same place so if you have certain queries they'll be optimized by knowing where to go and which data you have to access in this case it can reduce scans of unnecessary data or in the case of this particular query the benefit we get is that now the data is already grouped so when we try to go ahead and order those and group them everything is already clustered together so it's a much quicker process to do now you'll notice here in this one the table is still the same size it's still a 23 terabyte table but we'll expect the speed increase due to the way that it's stored so in this just like before this one's going to process 13 terabytes of data so again it's still a fairly expensive query for us and we're not filtering in this query which is why we're not saving any of the processing there but you can see the time that it took here is just under 45 seconds so it's over five times faster by having that data together in the same place the next type of optimization we're going to do is starting to optimize for the types of queries that we're running and so in this case we take the clustered table that we have and then extract the call sets for the homozygous reference calls because we can infer that from the reference so we don't need to store a duplicate of that and then for the no calls we actually pull that out and just store the no call as a boolean as a true or false of is it a no call so in that case we're now not going to have to unnecessary that we can check the boolean and save on the amount of data we're processing and the amount of data we're storing so this table now is about three times smaller than the last one coming in at just over eight terabytes and when we run that same query on this clustered and compacted table we don't have to process the entire call record to check if there's a genotype greater than zero we can figure out which records those are and in this case we did it by processing less than a terabyte of data so in this case we process 18 times less data than the original meaning the cost of this query is five percent or so of what that original one was not to mention the time we look at this query now we run it in under ten seconds which is 25 times faster than that original table that we imported so let's look at some of the other advantages of clustering so this query now we're doing the same thing we did before we're doing that variant count but we're only gonna check chromosome 1 and the way we're doing it is with a where clause now the where clause is not actually going to change how much data we touch it's still going to have to go through and check the where clause is great at restricting how much data we get back but it doesn't really help us with how much data we process generally and so if we look at that when we run it you'll see it still processes 13 terabytes of data now we did this in 30 seconds as opposed to the 4 minutes because we didn't actually have to do the group and the count but we still had to go through all of that data so when we start to use the clustered table so now where it's actually ordered and bigquery knows where to go to find different sets of that we can see on the right here that this one says it may process up to 13 terabytes still now the number we give there is a worst case scenario we don't know all the time how much data we're gonna access until we actually do it we can give you a worst-case guess of what we may have to touch so in this case it says that it could run up to 13 terabytes but when we actually run it you'll go ahead and see the difference so in this case we actually only process one terabyte of data because bigquery knows where chromosome 1 has been clustered it only has to hit that particular section and it knows it can ignore all the rest of the data in the table because everything that's co-located together now it runs in about the same amount of time around 30 seconds which lets us know that the majority of the time here is probably the counting and not the actual finding of it so the times the same but processing it is 13 tests 13 times less data again leading to a direct relation to our cost reduction now if we keep optimizing here and we use that clustered and compacted table the one where we had pulled out the reference and the no calls remember that originally this processed 13 terabytes it took about 30 seconds in this version we're now only processing 60 gigabytes of data so 200 times less data that's barely a blip on the price sheet at this point and we're doing it five times faster still in coming in at just under six seconds to do that same that same count so next if we look at a little more complicated query if we're checking the transition to transversion snips this is a query that will require accessing almost all of the data so we expect that we're gonna actually have to touch most of our entire table on the originally imported data this processed 20 terabytes out of our original 23 terabyte table and came in at just over 8 minutes so we'll look at a couple different things here of using those same optimized tables we had before to see what the impact is on this particular query so again this calculation still requires accessing the same amount of data so on a clustered table it didn't save us anything on our actual cost of computation we still had to touch about 20 terabytes of data but we did cut the time in half and now ran it in about three and a half minutes instead of the original eight so when we start to look at what's important to your business for a particular task this is an option where even something like clustering can still help with some areas of performance and you can find what the best optimization is for the types of queries you're using when we run this on that clustered and compact table we've now cut the time to be less than a minute and in this case it processes about seven terabytes now remember this was an eight terabyte table to begin with so we're still touching almost the entire table but we're it's it's still a significantly lower amount of data that we're having to touch and again about three times faster coming in at just under a minute so as we go through this we're currently looking at reviewing some of the schemas we have as well as some of these optimization techniques to go ahead and see what we can do to provide optimized schemas or provide best practices for certain types of queries so again this another area would love to solicit feedback if you've got certain queries that you're running or things that aren't working well or things you're looking to optimize we've got a mailing list that we'll discuss at the end that's got a number of users of the product as well as a bunch of members of our team and we can help try to figure out what's the best solution for you and see if there's something we can bring back to the community there as well so one of the other advantages with bigquery is using the client libraries such as the Python client library to integrate with Jupiter notebooks so in this you can connect your Jupiter notebook directly to bigquery which will use bigquery to do the compute and all the heavy lifting of any of the filtering and ordering to give you your data back and you're not depending on your hosted runtime to actually do the hard computations so the way this works you paste your query as an argument when you've imported the bigquery library you run that you get the results back in your notebook and if you've got something in there already where you're using a data frame for example you can get that data back from bigquery put that into the data frame and not have to change the existing notebook you have and keep using your same analysis just changing the data source that you're using on the backend some different options we have for running notebooks here include collaboratory which is product by the research team at Google that they provide for use and this is hosted on a Google back-end although there are limitations to the the runtime and what you can choose since it's a hosted environment there's cloud data lab which is a product that you run in your own Google cloud platform project and allows you to host a notebook there that you want to connect to and then we've got a product that we announced this week as well that went to beta called AI platform notebooks and this is essentially Jupiter notebooks as a service and this supports a variety of different frameworks and packages including tensor flow and pi torch as well as a lot of packages pre-installed that people are used to like numpy and whatnot now a our platform notebooks also has a built in integration with github so we can use that for syncing or backing up our notebooks and also using that to share among other people so if you need to share your notebooks with collaborators either in your lab in your company with others you can do that through github in a way that you're already used to and we can also attach different types of CPUs or attach GPUs to those form notebooks as well so if you've got tasks where you're using tensorflow for example you can choose which GPUs you want that are appropriate for that and then of course you can always host your own jupiter notebook as well so if you already have your own environment set up and you've already got hosted notebooks bring in the bigquery client library and you can now utilize the benefits of bigquery with the notebooks that you've already got so let's take a look at a couple other tools we have here as well for doing analysis and we're gonna look at data proc being one of those and then we'll get into a couple of the partner tools as well that we have so first off one of the partner tools that I want to mention recent addition here is that we now have support for para bricks through the marketplace in Google cloud platform so para bricks has written a GPU optimized version of gatk best practices and deep variant and in this case they're able to take what was about a 30 hour run and turn that into 45 minutes so this is now available for you to launch in cloud marketplace it runs in your environment and allows you to bring services like this to run in your own in your own project the other part to note is if you're a developer who's building tools like this this is a way that you can actually provide that and make that available as well and simplify running that on Google clouds infrastructure so that your end customers don't have to figure any of that out so let's take a look at going through here we'll start by pulling it up in marketplace and go ahead and launch that on compute and we can choose the types of GPUs we want like a V 100 we choose how many we want to attach and go ahead and start that once it's up and running we're going to SSH into our instance and in this case we're going to copy our license over and set a couple of inputs here and a reference as well and those are all coming from GCS buckets we're then going to go ahead and run that through this connection and as this is running again it's going to process it save our output to its storage bucket that we defined and again this shows it in a way that you can now bring this directly into your project or for the developers of this tool this is a way that they can deploy and monetize that as well so lastly I want to talk about data proc at a high level here so data proc is our managed service for Apache spark and Hadoop environments and this can be used for a variety of tools one of them right now that we're using is for hail so using this to run a genomic analysis framework doing things like gee wasps and we're able to run that through data proc which will handle all of the nodes for you and again as we look at the world of notebooks we can also then connect the notebook to that as well so if we want to just submit our job directly to that cluster or if we want to connect a notebook to that cluster we can then run the job through the notebook as well so I hope that gives good introduction to some of the tools we have in this space and helps cover some of the questions you might have some of the resources that we discussed here that are available with links as well and we've got links here to some of the documentation and the tutorials that we used and then as I mentioned as well we have a mailing list at the top and that Google genomics discuss mailing lists you can join that from our genomics documentation page and this list consists of a number of users of these products from around the world as well as members of our solutions team and our product and engineering teams so if you have issues if you have questions if something's not working if you're trying something new that you can't figure out it's a great place to go and see if there's someone who may have already done that if there's other contributors in the field who can help out or if you've got questions for the engineering team as to how we can help better improve these products [Music] 