 Hello everyone and welcome to the Genomics onboarding for Carpentries instructors interested in teaching the Data Carpentry Genomics curriculum. My name is Erin Becker and I am the Associate Director with The Carpentries and I will be leading today's onboarding session. This onboarding is primarily intended for people who are already certified as Carpentries instructors and are interested in teaching the Genomics workshop. It's also suitable for trainees who have gone through instructor training but are working towards their official certification, and also applicants who have applied for involvement in the instructor training program and are currently waiting for an opportunity to be involved in that program. Please do note that this onboarding is not a substitute for the two day long instructor training workshop or any stage of the instructor checkout process. If you're not already a certified instructor and you would like to teach at a centrally organized Carpentries workshop, please do complete the instructor training and the checkout process. If you're not already a Carpentries instructor, that's perfectly fine, please do continue with this webinar. Non-certified instructors are more than welcome and encouraged to use and adapt any of the Carpenties lessons for other teaching contexts. All of our materials are licensed CC-BY attribution. So please see our license for usage conditions, but we do highly encourage members outside of our community to use these lessons and adapt them for their own contexts. Also if you are a non- certified instructor and you're interested in teaching at a Carpentries workshop, you are able to teach at a self-organized Carpentries branded workshop alongside a certified instructor. So please check out the requirements for running a  Carpentries- branded workshop through that link. Advantages of completing this onboarding -  what is this onboarding intended to do over the next 45 minutes or so this onboarding will help prepare you to teach at a Data Carpentry Genomics workshop. This is a 2 day long Genomics curriculum, so we will not be going over the content of the workshop in detail within the next hour here, but we will work on looking at the curriculum, its overall structure, what are some of the key confusion points for learners, some of the things that instructors have found issues with in the past, and also getting an understanding of the data that is used in this lesson. So we'll give you an overview and dig into detail where appropriate. Completing this onboarding will also give you priority placement for teaching at Genomics workshops. It's not a requirement that you be onboarded in order to teach at a centrally organized Data Carpentry Genomics workshop but we will give priority to instructorstors who have gone through this onboarding. What kind of background do instructors need in order to teach this particular curriculum? So instructors will need to have some experience using bash, which is the default shell on Mac and most Unix, excuse me, most Linux platforms. You will have to have some experience writing your own bash scripts. They don't have to be long or complicated scripts, but some sort of short bash script and also running programs that others have written through the command-line interface and you also need to have some sort of experience working with FASTQ formatted genomic sequencing data. It's useful if you also have some experience working with and logging on to a remote computer - whether that's your University cluster or some sort of cloud computing platform - so having that experience to log on to and manage your connection to a remote computer will be very useful in this workshop but is not a required prerequisite for the instructors. You do not need to have had experience using the specific command-line programs that are taught in this lesson. I will go over what some of those programs are, but it's much more important that you have experience and comfort working on the command line and working with genomics data then that you've used any particular program for working with that genomics data. You also do not need to have any experience working with the particular cloud computing platform that we use for this lesson which is Amazon Web Services. I'll talk in just a bit about how we host the lesson data and where learners actually do the computation of the workshop, but it is not necessary that you have ever used Amazon Web Services before. The Carpentries staff will take care of all of the backend setup of the AWS instance for your workshop and I'll explain a little bit more about how that happens in just a moment. The very first resource that I want to introduce you to for this workshop is the workshop homepage. All Data Carpentry lessons have this workshop homepage that gives an overview of the entire 2-day curriculum. In this particular workshop homepage there are some additional resources. We've developed a set of frequently asked questions for hosts and for instructors that you can browse through, particularly the instructor questions, that may answer some of your questions about this workshop and those are linked directly from the workshop homepage. We also have information here about the setup for the workshop which I'll be talking about more in just a moment, and also the data that is used to teach this workshop. The homepage gives an overview of the lessons that are taught in the workshop with a short one or two sentence description of that lesson and also talks about any optional additional lessons. In this case, for this workshop, we do have an optional introduction to R and Rstudio for Genomics lesson that is not part of our core offerings yet as of June 2019 and so will not be covered in this onboarding materials. There's also some more information about the teaching platform that's used for this workshop and again I'll talk about that in just a moment. If you look also at the extras section of the workshop overview page you can find additional instructors notes and information about how to launch your own Amazon Machine Instances for working with these materials, but again if you're working with them in the context of a workshop, The Carpentries staff will take care of that behind the scenes work. Going back to our presentation, the overall workshop structure of this material - it is divided into four lessons. Two of those lessons are much longer than the others. So it starts out with a short lesson on project organization and management that is similar in some respects and different in others to the Data Carpentry Data Organization in Spreadsheets for Ecologists lesson. I'll go through each of these lessons in a little bit of detail and point out some of the similarities and differences with other Carpentries lessons you may have taught and also point out how they are unique. So the project organization and management lesson is approximately an hour and a half long, the introduction to command-line lesson is taught second and is very similar to the Software Carpentry UNIX shell lesson, but adapted to working with this particular genomics data set. That takes about four hours to teach and could potentially be taught in a little bit shorter time period if you have a more advanced group of learners who have experience working on the command line before. The third lesson and kind of the core part of this workshop is the data wrangling and processing lesson which has no parallel in any of the other Carpentries materials and is very specific to working with genomics data. That lesson takes somewhere between four and six hours depending on the audience that you are working with. It's better to assume that it will take longer, as there are a lot of important concepts covered in this material and it's good not to have to rush it. And lastly the workshop ends with a short introduction to cloud computing for genomics which again has no parallel lessons in the other Carpentries materials that have been published. It's about an hour long and it really is quite a introductory lesson. Speaking about setup and getting started this workshop is taught on an Amazon Machine Instance the reason that it's hosted on an AMI is threefold. One is that it simplifies the setup for learners and instructors. There is a lot of software that needs to be installed for this lesson to work properly and having learners do all of that specialized software install is a distraction from the learning objectives of the lesson and also can take a substantial amount of time away from the core content of the lesson. Because it's not the specific command-line tools that were actually teaching, it's the way of working with those command-line tools and those can generalize across other other programs, other platforms that people will be using in the future, we don't want to focus on the intricacies of installing these tools, which can be quite complex, but want to provide this preset environment for learners to work in. Second reason to use this remote cloud computing service is that we are dealing with a very large dataset. It's over a hundred gigabytes ,excuse me, gigabases of data and so it's very useful to have that stored remotely so that learners don't have to worry about their storage space. Most laptops probably wouldn't be able to handle the computing that we do on this workshop. And third, and I think very importantly, this provides more of an authentic experience for learners than having them do all of this setup and working with the data on their private laptops. In their own work they're going to be working with large datasets. They're going to be working with potentially tools that other people in their lab use that are installed on a centralized computer so that everybody uses the same workflow potentially. But datasets keep getting larger and larger and so it's very common to have to work with genomics data remotely for simple compute processing power issues. So we want to provide that authentic experience for learners. For the instructors point of view, the staff, The Carpentries staff will create the Amazon Machine Image and the instance that you will connect to. And we'll provide connection information for you, for your co-instructors, your helpers (if requested your helpers) and also connection information for all of the learners in your workshop. Once you have logged into the AMI, which you do through your bash shell with a single line of with a single command, then the rest of the lesson becomes almost indistinguishable from working on your local computer. So if you haven't used AMI before, if you haven't done much remote computing at all, this hopefully will not present much of a barrier to being able to teach these lessons. I know that I, I do have a genomics background, I did my PhD in microbial genomics, and I had never worked with AWS so I was a little bit intimidated about teaching this lesson the first time, but it really ran very smoothly after logging on to the AMI which was pretty straightforward. And staff can help walk you through that. I want to dig now into each of the four lessons. In just a little bit of detail. So the first lesson is the project organization and management which is partially adapted from the Data Organization in Spreadsheets lesson for Ecologists. The similarities are primarily that both of these lessons include a discussion of common spreadsheet formatting problems. So it starts out with a very messy spreadsheet and asks learners to identify problems that they see with how the data is organized and then there's a generally a very nice interactive group discussion about what those issues are. It's a nice icebreaker because people like to identify problems with other people's data, with other people's analysis, and so it really does provide a good discussion. There are several differences between this lesson and the Ecology spreadsheet lesson. This lesson does include a discussion of metadata. So most genomics, most types of genomics data, have metadata standards that are required when posting that data to whatever the relevant database is. There's some discussion of the existence of those standards and pointing learners towards where they can get more information about those standards, but not going into great amounts of detail. There's also an additional part of the lesson that discusses how to access data on the NCBI SRA. So this is actually quite nice it takes the learners through, once you have a journal article, how do you find the information in that article that will allow you to access the relevant data sets on, in this case, the SRA. To make room for those additions, this lesson has gotten rid of some of the additional parts in the ecology lesson around data validation and quality control. Those are reintroduced in the data wrangling and processing lesson. So we don't lose the idea of data validation and quality control it just comes at a later point in the workshop. We also do remove the dates as data lesson, sorry episode, from this lesson but there is a little bit of a discussion in it in one of the exercises so it still comes up as a concept. I do want to make a point about this particular lesson includes quite a few points of discussion for learners to talk with their neighbors or with a small group about a set of questions and it's really important not to skip those discussions as this is the first lesson in the workshop and that really sets the tone for the workshop and getting people to interact and realize that this is going to be an interactive space will help a lot with motivation throughout the rest of the workshop. If you get people talking to each other early it will make your workshop more successful in the long run. Moving into our second lesson, which is the introduction to the command line, this is the point at which your learners and yourself as instructors will connect to the AMI using the connection information that's been provided. After they've done that connection this lesson is incredibly similar to the Software Carpentry UNIX shell lesson. I'll walk through some of the similarities and differences in just a moment, but I do want to also take a point here about motivation. There is a section at the beginning of this lesson titled "what is a shell and why should I care?" I know it can be very tempting as an instructor to skip the motivation and just dive straight into code, but that motivation is really important for your learners especially if they've never worked on this shell, if they've never worked on the command line before. Helping them understand why it is so important especially for working with genomics data when often you have to work with a remote computing setup and the only way to do that is through a command-line interface and also when very many of the commonly used multi-purpose tools are either only available as command-line programs or have additional functionality that can only be accessed through the command-line interface. So that motivation is really important. Make sure that you get that in at the beginning of the command line lesson. So some similarities and differences between this and the UNIX shell lesson in Software Carpentry. This lesson covers file system navigation so pwd, ls, cd, which are quite standard in the UNIX lesson.  It covers working with files and directories, so creating a file, creating directories, removing files and directories, moving, copying, renaming them. It covers redirection, tab-completion, and wildcards. It does not get into as much depth about redirection as you do in the parallel Software Carpentry UNIX shell lesson, but it does touch on it a bit which is then important for the third lesson in this workshop which talks which uses command-line programs that rely on redirection in some cases. Similar to the Software Carpentry lesson this lesson uses nano as the text editor. It's straight forward as much as these things are. It's certainly one of the easiest to use text editors and it provides the help information directly on the text editor screen. So regardless of what you use as your personal text editor of choice, nano is what's installed on the virtual machine and it's what the learners use for this lesson. This lesson also covers for loops and writing bash scripts, which are expanded on in more detail in the third lesson, and it also includes some of these miscellaneous commands that are important, so history cat, grep, which is used in quite a bit of detail in this lesson, wc, man, and less. Just to make a note that the man command produces a little bit of different output on the AMI then it does at least on my personal machine because the installation of Linux is slightly different from what I have on my computer. So if the manual page that you're looking at looks slightly different and includes less information than your manual page on your personal computer, that's okay. I like to redirect learners' attention to the utility of googling things if the manual page doesn't provide enough information. There are two additions to this lesson in comparison with a Software Carpentry lesson. One is file permissions. There's an exercise where learners have to create a backup version of their data and change it to to be read protected, sorry write protected, and there's also a section on moving data from your computer to the remote computer and vice versa and also scraping data from the Internet through curl. Going into the third lesson in the series, the data wrangling and processing lesson, this is not similar to any of the other Carpentries lessons. It introduces the primary data set that the learners will work on that the learners work with. The previous two lessons in this series worked with smaller versions of that data set that were used in particular ways. In this lesson learners kind of dive into the fuller larger data set and so this lesson walks them through every step in a particular variant calling workflow and ends with them writing a bash script that allows them to perform the complete analysis through executing just a single line of code. So the idea here is that we are building up an automatic, we're building up the capacity to do automatic analysis of the data and teaching them each of those steps and then combining them all together at the end to show the importance of automation for repetitive tasks. This lesson, this slide kind of shows the workflow for this lesson so we start out with sequence reads that are in FASTQ format, do some quality control, align to the genome, do some alignment clean up, and then variant calling. The tools that are used in the current version of the lesson for those steps are listed here. So we do some quality control visualization with FastQC. The learners actually transfer the, this generates an HTML file for each of the sequence files, and learners have to transfer that between the remote computer and their home computer to open the HTML. So that provides some practice with that with SCP with transferring secure transfer protocol across computers. Then they get into trimming and filtering reads with trimmomatic, do some alignment with bwa mem, file format conversion with samtools, variant calling with bcftools, and then filtering and reporting SNPs with vcfutils. I know that was very fast because I want to emphasize that the particular programs and and tools that we teach in this lesson are not the important bit of information. It's the fact that we're walking them through a workflow and teaching them how to automate it. In a year or two years we may no longer be using trimmomatic, that's something that will be determined with time, but the general workflow and the idea of automating the workflow the idea of for loops and bash scripting will still be relevant and the learners will be able to take those tools and apply them to whatever workflow they need to perform for their own research. So don't panic if you don't know and haven't worked with these particular tools before, it's okay, you can learn them. I would recommend walking through this entire lesson on your own. if you're familiar with any or all of these file formats and you have the appropriate background in genomics you can walk through these lessons and have a good enough understanding of how these tools work to be able to teach them, because remember the thing that you're teaching is not so much the tools as it is the process. You can learn them, it will be okay, I promise. And the takeaway here is that the data wrangling and processing lesson reinforces the things we learn before around for loops, setting command parameters, bash scripting, and above all the importance of automating things that are repetitive and error-prone. Two things to note that are important, because this lesson relies a lot on a workflow, a pipeline, each step in that pipeline is dependent on the previous step and so if a learner gets behind or had to leave the workshop and come back there's a good chance they wouldn't be able to catch up with the rest of the class and so we do provide the solutions, all of the solutions files, which is all of the output files for each of the steps and all of the scripts that the learners are writing throughout the lesson. Those are all available in a hidden solutions folder so you have to notice that period there, that's a hidden file. So you will have to point learners in that direction if they need help catching up. And this is something also to alert your helpers for the workshop to so that they know if learner is irretrievably behind that they can point them towards the solutions files. Similarly if the learners aren't able to pull the data files that are pulled in the lesson directly from the SRA maybe there's internet connectivity issues or some other reason, those files are available in the hidden backup directory which is also at the home directory level. There's one more lesson that is used in this workshop and that is the introduction to cloud computing for genomics lesson. It again is not similar to any of the other existing Carpentries lessons. It provides additional motivation for why learners would use cloud computing including the discussion of some of the advantages and disadvantages of using cloud resources and different cloud platforms. It can't be a comprehensive discussion of all of the cloud computing platforms out there but we do touch on a couple of the popular ones and also talk a little bit about the similarities and differences between using cloud computing and a university compute cluster. And the core part of this lesson in addition to that kind of general discussion is technical tips for how to make sure that your environment when you've logged into the cloud is what you expect it to be and how to make sure that you stay connected to your cloud instance. So that covers the actual content of the workshop. I want to spend just a couple of minutes talking about how you can prepare to teach and what to do if you have any additional questions. So the first step in preparing to teach is to read through the instructors notes for the lessons that you're teaching. They're linked here in order. I'll just look at the data wrangling and processing lesson \to show you an example of what these instructor notes look like. They start out with discussion of the motivation behind the lesson and the overall learning objectives and then get into the lesson design, talk about why the lesson is structured in a way that it is. There's generally some technical tips and tricks here that give you information on problems that learners might encounter and what you might have to do to help them get through those difficulties. You'll notice that they're potentially some empty sections in some of these instructor notes. Those are places where we really encourage instructors especially novice instructors to contribute their experiences either as an issue or as a pull request. You can click the improve this page button and it will open up a text editor for you to suggest an improvement to the instructor notes which will then be reviewed by the maintainers for the lesson and they're always very excited to get contributions from instructors who've actually taught these lessons in a real setting. Going back to our presentation, another thing to do before you teach in addition to reading the instructors notes is to make sure that you go through each line of code in the lessons that you're teaching or each demonstration in your lesson. In order to do this you will need access to the AMI, which will be provided for you by the Carpentries staff about one week before the workshop. If you'd like to have more time to work through the materials and would like access to the AMI earlier please contact team@carpentries. We can provide that earlier access if requested. And last but not least you're welcome more than welcome to join a community discussion session which is a weekly, I think we have them twice weekly now, gathering of instructors and other community members to discuss teaching Carpentries workshops and other work that we do in the Carpentries community. So that's a great place to ask general questions about the logistics of teaching Carpentries workshops. You have now completed the genomics instructor onboarding, so you now have priority when signing up to teach at genomics workshops. Make sure that we at The Carpentries staff know that you have completed this onboarding please by sending an email to team@carpentries.org It's the same email that's listed here just to let us know that you completed the onboarding for genomics so that we can put a note on your instructor profile and make sure that you get priority status when you sign up to teach. In order to hear about teaching opportunities you'll need to sign up for the instructor mailing list which you can access through this link. Through that list you'll get regular emails when teaching opportunities arise. If you don't want to wait for somebody else to organize a workshop you can organize one at your institution. If you click this link you will see some resources about how to host a local workshop, including checklists, email templates, information about how to talk about the code of conduct, venue specifics. So we tried to kind of provide as much information as possible there to help people organize their own local workshops. And we'd also really like to encourage you to please leave feedback for the lesson developers and for other instructors who are teaching these lessons using issues and PRs to the lesson repositories. If you don't know what an issue or PR is, that is okay, we have an explanation of how to contribute to the lessons that you can access using this link here as well. Lastly I'd like to just talk about how to get help, so the instructor notes are a good resource to start with. You can also contact the DC genomics Slack channel which is linked there. You'll have to sign up for The Carpentries Slack workspace, once you do you'll have access to this channel and it's a great place to ask questions. There's generally always somebody on there who can provide some answers and I'd love to see more discussion also on there about teaching experiences. You can also check the frequently asked questions for this workshop to see if the answer to your question appears there and if it doesn't and you have more questions and you'd like to chat with someone about the workshop please contact team@carpentries.org and that will go to the appropriate staff member to help you out. So that's all of the information that I have. Thank you everyone for watching this onboarding video and I hope to see you engaged as an active member of the Data Carpentry Genomics teaching community. 