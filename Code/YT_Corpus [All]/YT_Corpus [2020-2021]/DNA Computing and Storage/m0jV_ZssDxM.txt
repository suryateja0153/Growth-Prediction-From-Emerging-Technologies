 hello everyone and welcome to this session on genomics I have the great pleasure of introducing Roberto but before we get started I just like to quickly go over the Microsoft code of conduct for this event many of you have probably seen this before if you've gone to other sessions at build but I just want to make sure we all take a quick second to skim through it read over it make sure we all understand what the code of conduct is in summary the idea is that we all just want to be nice to each other create a friendly professional welcoming environment or everyone can feel comfortable all right so the wonderful person we have joining us today is Roberto he is a bioinformatician on the Microsoft genomics team and he is here to give us an intro to genomics and talk about how you can use Azure to analyze genomic data taken away Roberto thank you so much Jenna as pointed out my name is Roberto yes I'm a senior bioinformatics scientist on the Microsoft genomics team within Microsoft Research for the next few minutes we're gonna be focusing in on genomics and particularly how we can fully harness to the power of genomics in the 21st century I'll then give you a brief introduction into how genomics analysis can be done on the azure cloud using our Cromwell Azure implementation and last we talked about some of the work that we at Microsoft genomics have been doing to really push the boundaries of genomics and integrate genomics and health data science be able to put all that data to really a drive the field forward so for those of you who may not remember from bio 101 we are in world of genomics we're studying the information encoded within DNA or deoxyribonucleic acid think of this as our molecular hard drive and that hard drive contains 3.2 billion letters of information or basis as the refer to DNA if we were to think about this in the context of a novel which has on average standard not novel has about 1,300 pages opportunity words for age it would take 2.5 million pages to encode all of the information that's stored in every single cell of your body in a single novel that's a lot of information and so this presents a unique set of challenges for us and ultimately the really demonstrates where the power of the cloud truly lies it's also really important to remember that this information can really power the research as well as medicine and a whole number of different fields that can benefit from this genomic information that's encoded whether that be for personalized medicine identifying infectious agents or predicting whether or not someone may actually have a possibility of inheriting disease but dealing with data at such a large scale is a unique challenge that all genomic scientists have to work with and this challenge really breaks down into two segments first and foremost when we sequence a genome that is reading back the information that's encoded within individual persons DNA or an individual samples DNA we generate over a hundred gigabytes of data for that individual sample in order to be able to do this at any significant volume to really be able to study populations we generally enormous amounts of information so thinking about how we move that data and how we store that data is croute essential we also have to then consider how we analyze that data and current algorithms because of the nature of the data and we'll talk a bit more about this in particular momentarily required hundreds of core hours on the table to understand that information and for example it would take over nine million four hours of judgment to analyze all the genomic data we would generate we simply had a full Madison Square Garden worth of samples that we were analyzing and lastly we have to think about how we actually integrate and structure this data in a manner that is just simply focusing on the genomics information itself but how we interpret it and how we add other pieces of information that can be valuable in to that particular into that particular data set so let's talk a little bit about what a typical genomics analysis pipeline looks like start off with taking tiny pieces of DNA and those are interpreted through a number of different technologies that ultimately generate tiny pieces of information of that 3.2 billion base pair DNA sequence that represents each the genome of yourselves so first and foremost we have to move hundreds of gigs of information per sample that are coded in files that are called fast Heuer bam files and further we have to then consider how we can actually add some structure can make sense of all the random pieces of data that are there and I mentioned here that this is what we call in computer science called np-hard problem and if you're not familiar with that term it's just a fancy term in computer science it means that something is extremely difficult to do in order to run once we have this order put on the information we then have to the interpret for individual samples and across in pieces across the individual sample where there might be similarities of differences from what you would normally expect in that DNA sequence to be and we call this in the genomics world variant calling infiltration and currently this field already utilizes machine learning models to accurately spot a changes in the DNA sequence or mutations we then have to think about how in the information actually impacts what we're looking at in terms of the overall genome and the genomic information that's encoded but in order to be able to do this we actually have to then take insight from a number of other pieces of information whether that be data that we obtain from the clinic and a reference databases that tell us whether or not a particular mutation may be advantageous or disadvantageous image data a whole other pieces of data working on Andrew we have the benefit of being able to leverage of other existing tools such as the fire API to be able to bring that data into the cloud in a secure manner structure it with Azure products and services and then ultimately we have to finally be able to present that data back and use it for different individuals who may be interested in learning what that information encodes whether that be our researcher a physician a patient or anyone else that may be interested in looking at that data from a computer science perspective all of these different steps encompass a number of different research and exploration avenues and so I encourage those of you who may or may not have actually thought about doing these genomics research within the context of our broader computer science background to really think about it because we have to understand the data as well as since this is the probably the most precious piece of information one could actually have a look at we have to make sure that that data is being protected and that the data can actually be interpreted and utilized which encompasses a number of different fields moreover the different fields of the different tools that people are used to interacting with at that data we have the benefit here at Microsoft of having all of those available at our disposal but ultimately we have to be able to figure out and properly pair of the right tool kits and the right tools with the right steps and the appropriate users before that and so within Microsoft genomics it's fundamentally our goal admissions you'd be able to try and make sense of this process and to be able to streamline and automate this process as much as possible to enable our users and researchers physicians and scientists to make the most use of this information so let's now dive into a bit of some of the individual work that we've been doing and we'll be in understanding how pieces of DNA ultimately are related to one another so it took these old said old newspapers and I asked you to tell me what happened on June 27 2000 but I decided to make it more difficult and move those newspapers up before I handed you the remains and ask you to tell me the information we as computer scientists know that we can take the random pieces of him for information those short snippets that would remain after the explosion and use our problem-solving skills to start piecing back together the information that's encoded and just as a primer for those of you who may or may not be aware June 27 2000 were actually is known as the DNA day of the day that the human genome was first published this actually represents a huge computational challenge and what we are doing every time we analyze an individual sample of DNA sequence data we looking at between tens to hundreds through even thousands or millions of copies of DNA from a sample that are all randomly fragmented because the fact that DNA sequencing instruments don't have the ability to read all 3.2 billion bases at once so we have a random assortment of pieces of genome represented in our dataset about 2.2 to three billion reads about information we then perform overlapping step and use other bioinformatics tools to be able to reassemble this information to recapitulate the actual reference sequence that were interested in looking at to identify potential changes that may exist ultimately the more data we have the more chances are that we have enough pieces that overlap with one another to be able to rid of recapitulate the sequence and so thus the resultant references of higher quality the problem is that in order to be able to do this this requires a huge amount of computing power and storage capacity this process environment from attics is commonly referred to secondary analysis and it's ultimately taking unstructured genomic data and adding some level of structure and understanding to that to be able to identify what may be happening in an individual sample and one of the best practice pipelines that's currently been implemented by the Brown Institute of MIT actually performs this analysis it performs an initial overlapping alignment step a number of quality control steps and then fundamentally didn't defy sir mutations the current average compute time for this is approximately 30 hours as part of a collaboration of the brode are Microsoft genomics team has actually optimized the service on Asscher and we have can take an average phone would significantly reduce average compute time for a sample to be processed and so this service ultimately is currently available but it does one specific pipeline and the world of genomics is rapidly evolving first and foremost if we look over the last 15 years or so those of you in the familiar in computer science would be familiar with the term Moore's law well genomics is the one field found out there that actually breaks Moore's law and the cost of human genome sequencing thanks to an explosion of new technologies has actually resulted in the cost of a genome rapidly depreciating over the course of the last 15 years and currently sits between 100 to 300 dollars depending on what technology you're using moreover this field is constantly evolving and adding new layers and interesting pieces of information innovations such as long read sequence English allow us to look at make your pieces of DNA single self sequencing that allow us to actually use molecular tags to keep track of each individual cell in a sample and spatial sequencing which actually allows us to keep a actual location information actually within a tissue to be able to understand where DNA and sequence is actually coming from so these other pieces of information in combination with rapidly lowering costs and sequencing means that as a team our goal is not only to simply enable specific pipelines but rather our focus is shifting to actually building a rapidly scalable platform that's elastic to allow for the continued innovations in research while enabling once you've made them identified those insights people to actually deploy those at scale to actually benefit the human population and the first component of that is implementing a workflow engine that was developed at the Byrd Institute called problem Cromwell ultimately has the ability for us enables us to be able to run any scientific workflow that is containerized using a leveraged Azure batch resources to provision individual virtual machines for a given task to process that data and so we have authenticated access and users can either supply data directly using REST API s or they can you use actors or Active Directory services to actually be able to upload data to blob storage and ultimately initiate and automate analyses and ultimately have that output be reported back to them as a brief demonstration on how Cromwell and Azure actually works is there are do with his video i apologies for that q that audio apologies for the the technical difficulty there but what we'll do is that Oakland forward l200 forward for sake of time I said encourage everyone to come back to this momentarily [Music] so fundamentally what we're looking to enable of utilizing this Cromwell on Azure of crumble on enter pipeline is not only to be able to automate genomics analyses and be able to plug in play specific secondary analyses for example likely just Microsoft genomic service on chroniger but also be able to in facilitate the transition and the preparation of data into what we call tertiary analysis to enable us to actually drive machine learning and data science analyses of genomic data integrating that with other pieces of information that a user may have and then fundamentally present that with power platform services so this end this is an example of a polygenic risk for a prediction dashboard that we've developed in collaboration with a customer well we're actually and this is actually a real-time dashboard and I would encourage anyone who looks through the materials after to really actually dive in this is interactive and can be examined but actually as take a look at four different machine learning algorithms that have been strange trained on a number of different snips based on their prevalence and their statistical evaluation in the G of individual set of samples to understand a population that has been sequenced why is this important because fundamentally what we're looking to do is it's not simply just research we're actually looking to empower healthcare and empower physicians to be able to actually make a difference on the Microsoft web utilizing the platform that we're building and so since the audio seems to not be streaming properly over the teams app I will skip claim this video but I would encourage you to a story about one of the collaborations that we currently have with Seattle Children's Hospital looking at building a model for predicting sudden infant death syndrome or SIDS so with that I'd like to open the floor to any questions that users may have and thank you all for attending thanks so much for Berto I'm going to send your video to the audience as they can see you while you are answering some awesome questions we've got so I wanted to start with some questions just about you and your background and genomics at Microsoft which is not maybe what most people know Microsoft for could you start by maybe answering the question what exactly is a bioinformatician and what do you do at Microsoft certainly so now I said I am NOT actually a computer programmer traditionally by trimming I am a buyer permutation and so my background is actually I've spent just as much time working in a lab with leeches and with C elegans which is a small micro semi microscopic worm and with a human-headed net cancer samples as i have programming and fundamentally bioinformatics is the fusion of computer science in biology and so I've spent time both on a bench actually learning molecular techniques as well as I also have some traditional computer science training as well so that a bioinformatics aesthetic so really interesting is in field and still or one that's very much in high demand so if you have an interest in biology as well as you're obviously so you're tending build so you have some interest in computer science as well bioinformatics is a great field where you can actually total line between and ultimately you're enabling researchers to understand data and biological data using computer science and programming techniques awesome thank you and a good explanation I'm curious what jobs are there out there for people who maybe also don't have a traditional computer science background or maybe even for people who do but are really interested and the sciences like biology or chemistry or physics um all those great life sciences that's an excellent question so fundamentally all of these called hard science and soft science tech deals generally and continue to generate lots and lots of data whether that is genetic data or information about metabolites and chemical data and such and so fundamentally there's a huge demand for individuals who can and are act as any people of working with large amounts of information on a computer and so in order to get into said bioinformatics or the informatics field in general you're definitely sick you're gonna want a strong computer science background because you have fundamentally be working a lot on the command line and also working with high performance computing in the cloud and such to do that but you also want some understanding of the basic science field that you're interested in so in my case I have a lot of background in genetics in biology and biomedical science but there are bioinformaticians that are really well focused in protein biochemistry which admittedly is completely beyond my expertise but similar big data challenges as well and so you really you wanna said you have to start with your computer science core but then you can start you know and collaborate with the basic science field of your interest and that's really how you can then transition us it pretty easily into this field awesome thank you that's great we have just a ton of questions coming in I want to ask a couple we had some questions at the beginning about different programming languages in bioinformatics so one of the questions was what is the preferred programming language for pursuing bioinformatics angel as a question and has rapidly evolved even over the course of my career which it's not that long the currently Python is the dominant language in the field if you're more interested in hard you know the really hardware acceleration and more focused more on the the computer theory aspects of things a lot of I said that world really works in c-sharp and in some of the more machine focused languages I said in that regard I have friends and collaborators that are f-sharp programmers a set for example and then if you're looking more manipulate data and be able to interact with data and particularly in care you understand it is that python is always a good go-to but there also is a very rich development for R and so being an our programmer is you can have a very successful career in terms of understanding data looking at working with are there's a few other languages that are really kind of up and coming Russ is one that really seems to be starting to take off within the bioinformatics world and then it's always handy to have some curl or and general extracting knowledge as well because you're always working with different environments awesome okay here's a question more from the biology side of things someone was curious why is it required to fragment the genome before you kind of start this process of analyzing that data that's an excellent question so we're dealing with each individual cell 3.2 billion bases and those bases are only in you know are the only thing that's really structures those are in the notes pieces of them are the individual chromosomes that are in your genome so you have 23 pairs of chromosomes in a human genome so and each of the chromosomes varies in size unfortunately from a technology perspective we just don't have the ability to see to actually read an entire genome at once moreover molecular techniques make it extremely difficult for actually keeping all that DNA and if those DNA's solidly in one piece together so you have two challenges both from the molecular side of biology in terms of keeping the DNA intact particularly since you're generally harvick deep harvesting DNA from it happen ultimately have to kill whatever you're harvesting the DNA from and the moment you kill something a pop-top processes create out of introduce enzymes that will actually start cutting DNA and then you have the technology to actually be able to read that data so we just said we just simply haven't bridge that challenge we're making strides there with long read sequencing and the stone technologies in that regard but we still can only look at things that are kilobases to mega bases so tens of hundreds of KT to mega bases in length is still kind of the upper level upper barrier to that so that's awesome thank you I'm gonna do one more quick question and then I think we'll switch to our last slide one that just came in Linux or Windows for genomics which which is the better platform for genomics so I am actually a Linux trained first and foremost and actually kind of my go-to you know sorry Windows but you know the fact that you can have Linux embedded now within their PC is pretty awesome in that regard but yeah most tools are kind of generally developed with Linux OS in the background in mind so I'll say windows subsystem for Linux for the win now we can have it all yes okay I'm gonna switch us over to our very last two resources slide if you could flip to that one fantastic ok thank you everyone for coming thanks for all the amazing questions we're so sorry we couldn't get to all of them there's just so many coming in I will say if you enjoyed this session please do go ahead and fill out the survey for this session you can find that at AKA OMS students own 20/20 survey and the session code for this one is com2 15 these slides and other resources will be available for everyone at AKA thought ms / students at build thanks again for coming I really appreciated all your fantastic questions here's one more slide I put these in the Q&A as well at the very beginning so these are some great links to learn more about genomics - thanks everyone 