 - How we might use nucleic acids to store data? What is data and what is the cloud? This is a photo of the Facebook Data Center, it's about 1,000,000 square meters, about $1,000,000,000 to construct, and it holds all of your data, yet 90% of the data is never accessed. This is what we have learned from Facebook, from Dropbox and from a number of studies. By using storage at this rate, by 2025 we'll run into at least two problems. There will be more data generated than the amount of silicon that can be used for memory being mined. And second, data centers will take about 20% of the global world electricity production. So clearly, we need to do something else. DNA is incredibly dense, it's easy to replicate, and it's something that we will read forever as long as we have access to a DNA sequencer. The world's largest most advanced USB drives store about one terabyte of storage. Now if you take the calculation of how much DNA is in your biological thumb, you can get 100,000,000 times that storage. And that's something that we can read at any point. We're not the first to think about this idea. Many others have tried this and they've demonstrated that any sort of bits can be easily converted into DNA and recovered. But there's one critical problem of cost. Microsoft will have announced a gigabyte of storage in DNA, and at list price of $3500 per megabyte, you can guess how much they've spent just to buy this material. So let me tell you a little bit about a DNA data storage schema. Essentially, what you have is a bunch of bits, you have a software codec that converts these bits into a series of DNA sequences. We make many of these in parallel on a synthesis array. Once you have it, you can store it, you can replicate it, you can randomly access portions of it by sequencing, of course, and then you have a piece of software that converts these DNA sequences back into your base. So that represents the whole cycle. Now, the critical problem is the speed and the cost of writing this polymer. So let's take a look at why it's so expensive and what we might do. There's a critical difference between what you might call DNA for biology versus DNA for data. So DNA for biology, needs to have very high fidelity. And what is completely secondary is the speed, cost and the scale. Now that need is completely flicked when you talk about data, of course, when we want data storage we want speed, cost, scale, and fidelity is actually secondary, because we can use software to perform any sort of error correction that we already do with modern information systems. The two polymers are either what we call bio or data. Now, in bio, if you think about how you might need to make nucleic acids, you might need to make a very precise ATCG and see, in order, you can store the same amount of data in a polymer which I have in this row called data, which has a variable number of letters at each position. And once you sequence it, you can perform a series of very easy software tricks to actually compress those repeats and get exactly the same sequence back up. Here's one of the pieces of data we generate. So we wrote, Hello World, but essentially, what you can see in the colored histograms here, for example, under H01, you'll have a median of about four As followed by 10 Gs, two As then three Cs. And this AG pattern that you see at the beginning of H01, H02, and H03 is qualitative, if not quantitatively quite similar. In addition to that, we've also created software that allows us to use imperfectly synthesized strands to actually store your data perfectly. If we're trying to make the strand in the top left, we might come up with any number of sub strands that may have errors in synthesis. And you can see that many of them are actually missing nucleotides that we mean to add. But through our software and our error correction process, we can actually recover that data perfectly as long as we have multiple strands of the same sequence. And of course, when we're working in molecular biology, we often are working with many, many strands at the same time of the same sequence. So this is actually free redundancy that you can exploit. We've also taken this error correction codec, and we've simulated how it might scale if we were to start storing gigabytes or petabytes. From our simulations, you can see that it can actually perform the right error correction so that you can still have a fairly lossy synthesis process, but your data will be kept perfectly. (audience claps) (soft music) 