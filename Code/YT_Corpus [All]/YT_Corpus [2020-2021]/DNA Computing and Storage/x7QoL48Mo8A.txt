 hi i'm lynne langet cloud architect and developer and the first google developer expert or gde lately i've been working with bioinformatics clients building cloud-based genomic skill pipelines in this talk i'm going to talk about patterns for implementing effective pipelines on the google cloud platform there are four parts what is needed in this area why do we need to have patterns and how do you learn them how are you going to use pattern information and how can you learn more first we need to understand the scale and scope of the problem in my work as a cloud architect i've worked some pretty big data sets in fintech in new york and london in ad tech in la but genomic scale data sets are a whole new level the amount of data that's being generated by mostly human genomic analysis requires new patterns for building effective analysis pipelines on the cloud now when i talk about scale what do i mean can i give an example yeah i will a recent client that i've been working with is the broad institute at mit and harvard on average they've been putting in 17 terabytes per day into the google cloud this is an astounding number think about it in your own work in data pipelining are any of you working at anywhere close to the scale yet if not i think you will be even if you're not in genomics and the reason for this is the amount of data that's being generated i've had other projects in the iot area in ad tech and they come close to this but genomics is at the cutting edge so what are the challenges in building effective gcp pipelines at this scale well there's a lot of them first cloud practices which services should our researchers use should they use virtual machines should they use bigquery should they use higher level services how are they going to understand devops practices how are they going to work with configuration as code and really importantly how are they going to learn about service cost control you can make a very expensive mistake very fast with this amount of data speaking of data how are we going to try out and learn these services the data that many of my clients work with is personally identifiable human health data how are we going to work with down sample data sets these data sets are huge they have three billion letters and they do analysis across the genome speaking of the genome researchers really want to work with genomics examples most of the examples in learning cloud services are generic hello world and that really doesn't resonate with my customers on top of that in genomics there is a increasing approach moving towards tools and libraries such as jtk the genomic analysis toolkit and the widdle or wdl workflow definition language the next flow library in language nf and galaxy project there are many others so what i've done to address this knowledge gap is i've created an open source course on github it is 100 free because it's on github and it consists of notes markdown pages screencasts code examples and sample data importantly all of the examples use genomic data it's called gcp for bioinformatics so this course introduces researchers to working with raw google cloud infrastructure or systems for genomic analysis built on the google cloud including teradot bio from the road institute in berlin variant spark library from csiro bioinformatics in sydney australia next flow i o from the group in europe galaxy project which is global and others now to get an understanding of the complexity let's first consider batch compute for analysis if you start at the bottom of the diagram although today we're talking about the google cloud many of my researchers are faced with the additional complexity of collaborating with researchers who work with a different vendor cloud so we have a number of public vendor clouds on top of that they'll build their analysis on clusters of virtual machines and increasingly containers and on top of that many of them are now starting to use orchestration layers such as spark or kubernetes on top of all that cloud infrastructure researchers are often using these bioinformatics tools and libraries that i previously mentioned such as tarot dot bio nex flow galaxy and more those libraries have associated languages such as whittle wdl cwl common workflow language next flow nf or ga and those tools often have orchestration layers such as cromwell for terror.bio or mini widow pipelines api is actually a google api that works in conjunction with a lot of these higher level libraries does this look confusing well it is there's a lot of different tools and libraries to understand it's one of the reasons that i made this course to break all these services into parts and pieces so that they can be learned and understood individually and then combined effectively now in addition to batch compute you'll notice a similar but not identical kind of pattern here and this is interactive analysis now this keys around the center square in the center of the center square the i python notebook or i p y and b it is becoming a pattern in genomics to create reproducible research in the form of a jupiter notebook and so an important pattern in pipelining is scaling those notebook environments using jupiter hub or google collabs so again there are a number of parts and pieces to working with these pipelining infrastructures and my course covers all of these on the google cloud so drilling into the highlights of what my course covers it really is designed to be consumed the way that you actually work with the cloud there is a level zero for setting up your account and setting up cost control level one is files and data because of course we're computing on files and i cover core services such as storage buckets google cloud storage i cover public data sets that are available such as thousand genomes and other data sets for genomics that have restricted access services like bigquery which provides serverless sql querying on top of files and can be used for genomic type files such as vcf or variant calling format also i cover higher level services such as teradot bio data sets in the compute area i cover topics such as virtual machines running containers of course clusters of each of these and also include information about serverless or functions at the workflow level but also the google life sciences api which is called the pipelines api next flow and higher level services such as tarot dot bio additionally to cover the interactive pattern i have information about working with and setting up jupiter notebook environments on the google cloud platform i also include modern devops practices such as cicd or continuous integration and continuous deployment and i have information about machine learning that's specific to genomics covering examples of libraries like deep variant tensorflow io and google nucleus another aspect of the course design is that my examples are designed to be quick one of the challenges that i found when i was learning genomics tools as a cloud architect as many of examples reflected real world in real world can be huge amounts of data so a hello world can take minutes or hours or even days to run and that just wasn't a way that i wanted to work so i provide example data and i provide specific steps to try out each service my course is designed to run on a trial google cloud platform account and if you run with my example data sets it should be at nearly no cost i never pay more than a dollar or two and importantly most examples run in five minutes or less so you can quickly get up and running and you can find out if the service is going to work for your particular analysis needs another aspect to the way i've designed the course is i've designed it at three levels for each of the service areas so for example for files for data for compute so on and so forth so the three levels are infrastructure as a service and this is for researchers who want to set up control and manage everything the level of data for example they might want to set up their own rdbms or relational database they might want to set up their own no sql environment on virtual machines or on google cloud storage buckets at the compute layer these researchers would prefer to set up their own clusters of google compute engine virtual machines or docker images on google kubernetes engine there are other researchers who would prefer to work at the platform level they would rather run their analysis over managed services and let google handle more of the underlying infrastructure so data this looks like data lakes that are running on gcs buckets google cloud storage buckets and use of public reference genomics data at the compute layer this often manifests by using services such as dataproc or the google life sciences api which efficiently handles orchestration of groups of virtual machines allowing the researchers to dynamically spin up and spin down compute resources and it allows them to take advantage of aspects that are specific to google cloud such as preemptable instances so that they can save lots of money which is really important at this scale still other researchers prefer to work at the software as a service level and one example of this is using the terra.bio system which is a web ui that was designed by the road institute in verily that currently runs on top of the google cloud platform in this case rather than interacting with the gcp console or using tools such as gsutil my researchers will go to the tarot.bio platform and they can access both their own data and publicly available genomic data through the tara ui also they can spin up and spin down jupiter notebook instances and compute clusters that run on top of the life sciences api through the web ui now that's a lot of different areas to think about so i think at this point will be helpful if we actually take just a brief look at the course on github in this demo i want to show you my open source course that i built gcp for bioinformatics so first we're going to go out to the site on github and this is where all the artifacts for the course are located you can see that the course is organized by topic area and it's set up in folders so they're numbered in a logical order you would of course start by setting up your gcp account then you would work with files of data then you would work with virtual machines docker containers or functions so compute and multiple instances you may add machine learning you also might use code and cloud service tools such as continuous integration and continuous deployment container registries and other tools in each section the organization is set up as follows at the beginning of each section there's a readme so there's a main readme i'll just scroll down here and you can see that i've consistently used emoji throughout the repository to make it easier to scan you can see the list here as it says this repo includes content you can read watch or run so you can read pages of the repo you can think of them as notes you can watch linked youtube screencasts or demonstrations you can try out the computational examples using jupiter notebooks there are linked github repos and there are additional advanced topic links at the end of each section on each page there will be a linked youtube video so you just click the graphic to go to the video if you prefer to learn by watching screencasts you can simply go to the playlist that i've created and linked and as you can see there are 27 different short screencasts mostly around five minutes some of them are six or seven that cover the topics via demonstrations now just to get an idea of how each topic is set up we'll take a look at one topic area just so you can understand the structure which is consistent throughout the course so here we have in the files and data section in that section we have a topic called sql questions and this topic uh is covering using the bigquery service with a bioinformatics or genomic example so you can see first there's a little explanation about it and then these uh notes pages are very action oriented in the design in that they're created so that you can actually try out the service with a genomics example within five to ten minutes and get a result that quickly as well it's a little bit unusual it's one of the reasons i made the course because i've seen a lot of bioinformatics examples that take hours to run and i really in course design wanted to create examples that would cost basically nothing to try out because of the tiny sizes of the sample data sets and would be just really quick and easy now in the case of bigquery i've included a number of screenshots in this notes page because i'm assuming that for most people taking the course this will be a new service so you can see for example i've used this screenshot in the beginning of this page now this page actually is a link out to an additional course and this is a course on using the ansi sql language against a sample genomic data set and if you just go down the page this page is a little bit longer than some of the other pages is it's a three-part course and you can see for example we've got sql queries here that you can actually run now just to show you how this is intended to be used the course is designed to be used with a trial gcp actual account so you can run the lessons and and learn by doing so for this particular one if i scroll way down here for a second and i'll just stop here this is an example using a concept called a self join on a very very small but a domain specific example and and so the idea here is to return a hierarchy and the lesson as you can see is write a sql query to return the grandparent category of glycine binding so um to show you the environment where you work with this this is bigquery in gcp and for this course i've created a publicly available really small sample data set so you can see here i'll click on this experiments table and i can actually preview it and it has just five rows again really tiny examples here one of the concepts as i mentioned in talking about this course is that you can explore these services with examples that come from genomics that are tiny so you can do it basically at no cost and in just a shorter amount of time so the course is completely on github so it's completely free it's completely open source i really welcome any suggestions from the community the best way to do that is through pull requests sort of the typical github process and i really look forward to the contributions and the feedback of the bioinformatics research community it's lynn langett and of course is gcp for bioinformatics thank you 