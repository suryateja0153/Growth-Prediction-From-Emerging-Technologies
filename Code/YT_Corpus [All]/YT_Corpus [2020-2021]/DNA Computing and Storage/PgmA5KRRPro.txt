 (soft music) - Good morning and good afternoon. And welcome to the first webinar in the series of the State of Data Science in Africa. This is hosted by the National Institutes of Health Common Fund. My name is Maggie Dugan and I'm here with my colleagues from Knowinnovation. We are also known as KI. That's the KI in KIStorm, You used to get here. We're a facilitation team that specializes in accelerating scientific innovation and we are very pleased to have been asked to help support this virtual event for the NIH Common Fund. Briefly our agenda today, some welcoming remarks and tips for engaging successfully in the webinar. We'll have an overview of today's session on biomedical imaging, acquisition and analysis from our moderator Paul Pearlman. This will be followed by 10 to 12 minute presentations from each one of our panelists. I'll let Dr. Pearlman introduce them and followed by questions that will be answered by same panelists. And then we'll have a plenary conclusion. This is all followed by an interactive networking session. Okay, this is an opt in. So we hope you will choose to join, it'll be an opportunity to engage further with the panelists and with each other and continue to interact with everybody and dig deeper on the topic of biomedical imaging and data science in Africa. So, incidentally I want to show you just a few slides to let you know that today's event is associated with the funding opportunity. This is the NIH program on Harnessing Data Science for Health Discovery and Innovation in Africa. So there are 4 different opportunities that are currently open: Research Hubs; Research Training; Ethical, Legal and Social Implications; Open Data Science Platform and Coordinating session. If you want to learn anything more about any of these funding opportunities, we invite you to visit KIStorm to learn more about how to apply or also to go to the NIH site. The application receipt dates are between the end of November and early December. So we encourage you to explore this further and find out more about how you can be part of this program. So before we start, just a few tips. We are recording this program, so if you want to see anything again, you'll be able to find it here on KIStorm or on the NIH site. If at any point during today session, you get bumped out, you get kicked out, you have problems with Zoom you can't find where you're supposed to go, you can always go onto the KIStorm "Need Help" page. Someone will be monitoring and we can help you find your way back to where you need to be. In this webinar, we have the opportunity to ask questions. So you'll notice at the bottom of your screen, there is a little Q&A button with some thought clouds, some speech clouds. You can click on that and add a question and as well, you can see what other people are asking. And if there's a question that resonates with you, particularly, you can say, "I like that question too," by upvoting that. And I would just like to introduce to you our moderator who is Dr. Paul Pearlman from the Cancer Institute. Paul. - Thank you, Maggie. I'm going to keep this brief so we can get directly into our talks. So our session goal today is to highlight novel work at the intersection of Biomedical Imaging and Data Science. The session will consider both a variety of imaging modalities and topics in Imaging Science, as well as the applications of machine learning, artificial intelligence and computer-assisted decision support and image analysis. As we're starting to think during the session about the broader DS-I program and maybe potential applications to that program, I want you to think about the interpretation of biomedical imaging data, often being subjective, and that the goal of introducing data science methods and approaches is often meant to yield objectivity. So with that in mind, what are some of the examples where such methods could have a transformative public health impact in Africa? Furthermore, what are some of the unique research opportunities at the intersection of biomedical imaging and data science on the continent? Finally, what are the challenges and barriers that hamper research at the intersection of data science and biomedical imaging? So today's panel consists of these 4 speakers: Dr. Michael Kawooya, Dr. Sameer Antani, Dr. Celia Cintas and Dr. Dan Milner, who I'll introduce prior to each talk. So as Dr. Kawooya is getting his slides up, I will go ahead and stop sharing and introduce him. So Professor Michael Kawooya is a general radiologist with a special interest in diagnostic and interventional ultrasound. He's engaged in clinical work, research and teaching at the Ernest Cook Ultrasound Research and Education Institute in Kampala. With that, I'll turn it over to you, Professor Kawooya. - Good afternoon to you from Uganda. - The talk I'm going to handle is going to be basically on imaging. And we are going to look at Biomedical Imaging Acquisition and Analysis. The objectives - to define radiomics and relate to other "omics." Explain the process of extracting radiomic data from images. List challenges in processing radiomic data in Africa and how to mitigate these challenges. Discuss potential for Africa to apply data science in biomedical imaging with Uganda as a case study. Radiomics is a method that extracts large amounts of quantitative features from all forms of radiological imaging using machine learning algorithms. And there's also what we call radiogenomics, which combines genomic and radiomic data to characterize disease phenotypes. And we call this Multi-omic data. You may also combine this with text data from electronic health records, if that is necessary. Curating radiomic data for use in an ML system. Data from imaging studies has to be organized and homogenized in that PACS system and this is usually in the DICOM format. We organize the data from electronic health records and imaging reports. And this is usually in natural language text. The data has to be de-identified and then also has to be linked to the "ground truth," which may be histology or microbiology. Because it is data which is very heavy, we have to reduce the pixel dimensions, so that then we can be able to process it with the computer we have. Otherwise it requires a very high speed computer. And then the data can be stored on a local server or on a cloud. Now how do we process radiomic data in a machine learning system. We usually use supervised machine learning technique. We write, train, validate and test the algorithms to obtain a model. Then we use that model to predict variables which maybe diagnostic or prognostic. And artificial intelligence for radiomic data utilizes segmentation methods to analyze the radiomic features and we segment region of interest. And then after that, we characterize and quantify descriptive features, which are like shape, size and texture features. Now there are challenges with dealing with radiomic data, data from radiology in Africa. Organizational challenges on this table we see radiology departments are not aligned to data science application, because this is new in Africa. So data has to be reorganized. We have to install a PACS system and also electronic health records. And these are not all over the country. All over the continent, many departments don't have PACS systems. The data has to be stored on a local server or in a cloud space, organize and homogenize it. And then it's important that the imaging protocols we use should be standardized and uniform. So that data from different centers and from different machines can all be processed together. We also have few imaging experts, and this one you can overcome through collaborations, or you can use remote reading of images. And also it's important that we should train locals to do the work and not only to rely on remote reading. In many departments, there's no expertise in computer science. And so there is need for the department to form linkages and collaborations with computer science departments or institutions where there is computer science so that they can access machine learning and artificial intelligence facilities. Now the computer science and artificial learning infrastructure requires very high speed computers. These are not usually found in radiology departments, but you can collaborate with the computer science department. And here one is usually likely to find high speed computers, which can process data for machine learning and for artificial intelligence. And the quality of the images in many parts of Africa still needs to be improved, and so there's need to purchase and maintain up to date equipment and also design and apply uniform standards for imaging and train the users. And this way you can ensure good quality images, which you can use new artificial intelligence systems. There are operational challenges in this area. Absence of large training datasets. And so this one you can overcome by combining data or images from different imaging facilities, but to ensure that they are using similar imaging equipment and also similar protocols. And one can also access open source tools, but these are usually heterogeneous and have several other complications with them. Now there is need to link the data, the images to the "ground truth." And so here we collaborate with colleagues from pathology, from microbiology, so that when we see an image, then we can be sure that either this is tuberculosis, or this is cancer through use of either pathology or microbiology. That is the "ground truth" to our imaging data. The data has to be extracted and quantified and here we use experts from computer science who can help us to extract data from these images and also they can have computers with segmentation software to segment the images. And then we can be able to have this data processed in the machine learning or artificial intelligent system. The writing of algorithms, training, and testing, and validating data is not a skill which radiologists have or people in radiology. But then we utilize our colleagues from computer science who have the expertise to help us to write these algorithms, to train, and to test, and to validate the data. Now some hints if you want to undertake a project in data science in Africa. You choose common diseases where the data science will have an impact. For instance, many cancers like cancer prostate, cancer breast, infections like tuberculosis. These are areas where data science may have an impact. We choose areas where imaging is applicable. For instance, brain, lung, breast, here we can apply imaging for the brain like MRI and CT scan, chest X-rays with and CT scan for the lung, and mammography and ultrasound for the breast. So that intersection between common disease conditions and diseases where you have imaging applicable, that's an area where you are likely to pick a topic to do your research in artificial intelligence or to apply artificial intelligence and machine learning. And then you choose an area where you have expertise, which is available. For instance, experts for interpreting brain MRI or brain CT or experts for interpreting lung CT, or breast CT or breast mammography and ultrasound. And at times you may also need to link with colleagues from molecular biology for the molecular biology aspect of the tumor. Also with colleagues from oncology and also with colleagues of course, from computer science. So choose an area where you have expertise and you can undertake this project, when you have all the capacity to do it. And then it's important to collaborate, collaborate, Collaborate is strength. Collaboration is strength. You collaborate within the facility, you collaborate with other disciplines. You collaborate with other universities with other centers and also other continents. And then through this, you can have all the skill, the right skill mix, which you need to undertake a data science project. There are opportunities for many countries like Uganda. Here we have the modern MRI machine, CT and ultrasound equipment. So this can give you the set of images for you to use your machine learning or artificial intelligence model. And also in Uganda, we are fortunate. There is an artificial intelligence laboratory at Makerere University, and these are very friendly and they are welcoming, and you can have collaborations and do projects through them. And we also have a Molecular Science Laboratory at Makerere University College of Health Sciences, and with very good modern equipment, like DNA sequencing that can be done. And so these are all opportunities. There are areas, a lot of disease conditions in Uganda, common cancers like prostrate cancer, like breast cancer, where you can undertake a project in data science application for innovation and for research, and also for clinical application. So in summary, application of data science in imaging for Africa is possible. There are challenges with applying data science in imaging for Africa, but they are also solutions which can be devised, especially through collaborations. There are opportunities for applying data science in imaging for Africa and these should be taken advantage of. And this NIH grant is a great opportunity. So I encourage you, I invite you to come and join in and see how you can make use of this opportunity. Thank you so much. - Thank you, Professor Kawooya. I want to remind everyone that the Q&A box is live and you can go ahead and start putting in questions as you think of them. So while Dr. Antani is getting his slides up. I want to introduce our next speaker. Dr. Sameer Antani is an intramural scientist at the US National Library of Medicine with research interests in medical imaging and machine learning for a variety of clinical problems, such as cervical cancer, HIV and TB, sickle cell, cardiac, myopathy, and others. - Thank you very much for the introduction Paul and welcome everybody. Good afternoon to everybody in Africa and good morning to the people in the US and other parts of the world as per your time zones. I'm going to talk about several aspects in biomedical imaging AI that relate to data science, to do with data, design and decision making and the interaction between them. And I'll draw on some of my experience as an intramural scientist at the NIH, working on some of these biomedical imaging AI problems, and to help you think about your problems or your challenges as you embark on this journey. So before I go too far into my talk I would like to acknowledge that what I'm drawing on are my experiences that I have derived from many, many collaborations over the past 20 years. And my views are personal and they do not necessarily represent the NIH or any collaborator in particular. And as this is a rapidly evolving field, my opinions are subject to change as I learn more and we all learn more in this area. And the collaborators are many, and they are in varying from students who come from different parts of the world, to research fellows and other organizations. And within the NIH, I must acknowledge my collaborators at the National Cancer Institute, the National Institute of Allergy and Infectious Diseases, Dental Craniofacial Research, the Eye Institute, and the National Library of Medicine of course. So this is a rather busy slide in images, but I'd like you to think about various imaging modalities that you see and how one would process these images. On the left you can see, top left, a histopathology image, that's from a biopsy, of the uterine cervix to look at cervical cancer, and I'll touch upon that a little bit. You also see a small swatch of a microscopic image of, from a cytology, from a bit of Pap smear, liquid Pap smear perhaps, or a traditional photograph of the tissue. You go into chest X-rays looking for disease patterns and onto 3D volume images from the brain, the usual diseases that are exhibiting some kind of change in the substantive matter. Or in terms of functional responses as our brains process visual and auditory and sensory data. On the top, you can see both an ultrasound image of the heart, where you'll see normally the ultrasound data, which is sometimes in terms of small snippets and as video. But also an EKG that is embedded in there. That would be one dimensional signal data that one night might need to process. And the processing can be done using of course super computers or desktop PCs equipped with GPUs for processing, or even mobile phones as exemplified in our ongoing work in detecting malaria using smartphones. And sometimes there are data tools that are involved in multiple dimensions. And once you have collections of these images, one might want to access them using textual means if you have metadata tags, or you may have patient health records or you might want to recall them by their appearance. It's very natural for us as we look at pictures to be able to recall what something looked like and that is the field of image understanding or Content Based Image Retrieval, which is something that the organized system tries to do in trying to connect words and pictures, which are natural to us as humans. So with that background, I'll take you on a journey, which is to do with detecting cervical cancer or more importantly pre-cancer since that can be treated. Now what you're looking at in the left is the picture of the uterine cervix as seen by an gynecologist or gynecological oncologists during the routine exam. And what you're seeing there is the dark region in the middle is the os, which is the entrance to the uterus. And what you're looking for are changes in the epithelium of the squamous tissue there. And there are 2 tissue types, the columnar cells and the squamous cells. And it's at this junction that you will begin to see the invasive effects of Human Papilloma Virus, certain types of them that are particularly oncogenic and can lead to cancer. And on the right, there are some regions that have been marked by an oncologist for us, grading them as a stage one, stage two, and stage three. It's after stage two and beyond is what we start labeling as pre-cancer where one would want to treat and using thermal ablation perhaps or cryotherapy, so that any further damage is eliminated and the woman can resume her normal life. This is called a Visual Triage but it's not so easy for human observers. And there is great subjectivity. And of course this presents a good opportunity for computers to intervene and perhaps highlight regions. Those who may have been less trained to provide some objectivity on how to detect disease and whether it can be treated. So there was some work done in the past, well, over 2 decades now, but more recently we had a breakthrough in using a deep learning algorithm. Which is a neural network algorithm also known as an AI algorithm that looks at an image and is able to provide an AVE or an Automated Visual Evaluation severity score that predicts that an image is either a control that means normal, or is a case which is deserving of treatment. Now to train this algorithm, of course, we had a large collection from a 7-year study conducted by the National Cancer Institute, NCI, in the Guanacaste region of Costa Rica. And several thousand women or nearly 10,000 women were followed over 7 years. And the collection of images from every visit were then grouped into training images that were truthed, or there were those that were indeed cases; there was a biopsy truth that was applied to them. And those selections were used to train the network and tested them so that it has very high performance. And the performance is shown on this chart here. This is known as the ROC or the Receiver Operator Characteristic Curve, which plots the sensitivity of a test versus the false positive ratio on the X axis. And you want this curve to be as much of a right angle heading toward the one here on the top left. And as you can see, this algorithm has a very high "Area under the Curve" or the AUC of 0.94. That means it is making very few mistakes. Now that said, if you were looking at many deep learning papers that's what you would be trying to track. However this alone is insufficient. It's also important to see where you are having the thresholds set up so that you perform well. And in this case, the classifier confidence was reduced to 0.2. Normally you would want the classifier to be very confident to be able to achieve this high a score. And that is a factor that's important. Also important to note is the variety that different cameras can produce on an image. And you would want to train your classifier with as many samples or from different camera sources, so that you have a sense of robustness and reliability in the performance of your AI algorithm. And this also alludes to different kinds of data that you need to use to train your method so that it doesn't do very well on a small kind of a set of images, but a broad variety of images that represents the population that you are building the algorithm for. Then you want to be able to explain to the clinician why the AI thought the image was worthy of treatment. In this case, you're looking at a pre-cancer image on the left, that's an acquired image and there are 2 different visualization methods. That means there were 2 different kinds of algorithms, AI algorithms. Looking at this image, both correctly classified the image as deserving treatment. But interestingly enough, each AI algorithm was highlighting a different region of the image, which of course introduces doubt. And that's why it's still an area of research, but AI can help you classify something correctly, but it's also important for you as a researcher to focus on why it is classifying an image as a case or the control, and what region is highlighting that is worthy of treatment. So if you think about this problem now, from the left to the right, you can see an image has been acquired and there is an HPV DNA test done. And if the person is HPV positive, then we would like to be able to decide if the person should be treated or not treated that means it's a case of control And then if it is treated, there would be a separate algorithm that will be determining if the region should be excised or terminally ablated. And so you have different kinds of algorithms that have been trained on different kinds of data images, as well as HPV data, which may be textual in nature that are informing the clinician to make a better judgment and provide good treatment. So what I'm trying to highlight here is the high interdependence among the data, the AI design and the decision that is produced so that you have a highly reproducible and highly reliable outcome from the AI that can help affect clinical action. And in thinking about this, we use the word data science, but I like to start to think about - Is it the science of data? It's acquisition control and the visual quality of the data? How large your set size is for treating? Larger the set size, larger the breadth, the variety. And this queue in the data, if you have too many controls and too few places, you might not get the representation necessary statistically speaking to get the reliability and robustness from the algorithm. You also want to think about any racial or gender or socioeconomic variety that might be introduced there, which can affect the quality of the images. And in terms of thinking of the AI design, what is the intended application? Is it for discovery, this is research or is it a diagnostic thing? What kind of networks I'm going to use? And this is an example. - Where you have different kinds of networks looking at just at this chest X-ray as an example, how each network or each AI algorithm ended up showing different, highlighting different parts of the image. All had disease, yes and all were correct. But they all were highlighting different aspects and so the application is extremely important as well. If you're going to be using synthetic data, does it really represent reality? How are you hyper-parameters screened for your deep learning algorithm and would you learn incrementally? That means humans correct machine. The machine learns to adapt and evolve over time is also a technique that needs to be researched and considered in designing an AI algorithm. So I'll leave it there and here's my contact information. But before then I invite you to join in the network lounge and in fact, and have a chat. Thank you for your attention. - Thanks Sameer. As Dr. Cintas is getting her slides up, I'll go ahead and introduce her as well. Dr. Celia Cintas is a research scientist at IBM Research Africa in Nairobi. She's a member of the AI Science team at the Kenya Lab. Her current research focuses on the improvement of machine learning techniques to address global health challenges and exploring subset scanning for anomaly detection under generative models. Celia. - Thank you Paul for the introduction. Good morning, good afternoon. In this talk, we want to discuss fairness in dermatology and how machine learning models are built for a skin disease diagnosis. And to see if there is an equal skin tone representation in these datasets. This work was generally done with the College of Engineering of CMU, Rwanda, IBM Research, both Yorktown and Nairobi Labs. We know that in dermatology, as in other medical fields, disparity exists across ethnicity. For example, in Black African American population, melanoma is often diagnosed at an advanced stage. Survival rates of ALM were lower in African American compared to Caucasian populations. Due to COVID-19, dermatologists started an international registry to catalogue skin manifestations of this disease. This registry compiled 700 cases, but only 34 of them were Latinos and 13 from African American patients. Professionals are raising concerns regarding the issues in the medical space. An excellent example is "Mind The Gap: A Handbook for Clinical Signs in Black and Brown Skin" written by Malone Mukwende. As we know, machine learning systems may place certain groups of people at systematic disadvantage due to the bias of data sets. So we want to see how these disparities are reflected on the healthcare machine learning models. First, we want to check is the standard datasets used for machine learning tasks in dermatology are biased with respect to skin tone and can we quantify this? If so, does the dataset bias lead to unequal performance of the disease classification and can we provide an automatic report of medical literature regarding these ethnicity examples? From the machine learning perspective, recent advantage on computer vision and deep learning led to breakthroughs in the development of the skin image analysis and diagnosis. For example, in melanoma, we have benchmarks models that outperform trained dermatologists. Since 2016, we have the ISIC challenges that show problems on nation segmentation, clinical diagnose patterns, and lesion classification. On the other side, we leverage work on predictive inequity in computer vision regarding skin tones. This was proposed by Joy Buolamwini and Timnit Gebru. So what we want to propose is a framework to estimate skin tones and their effects on classification performance in dermatology datasets. This work was led by Tim and Newton. They will be presenting in much more detail at MICCAI 2020 on next month. But the main idea to have this framework ongoing is based on 3 steps. First, we need a segmentation model to be able to extract the non-disease skin regions, to be able to estimate a skin tone metric. And after we have this information, we can do a stratified evaluation to see that the skin disease classifiers are working correctly across all the skin tones. So to evaluate this proposed framework, we used 2 common datasets in dermatology. ISIC 2018 with multiple dermoscopic images across 7 different disease labels, and a subset of images with "ground truth" of segmentation masks. In the other hand, we have SD-198 who has multiple clinical images over close to 200 disease classes, but with no segmentation data. For our purposes, we need to run some manually pre-processing before using them such as removing classes with images that we don't have observable non-diseased skin area, as you can see some examples in the slides over here. We also removed specific body parts such as inside the mouth, as you can observe here. And we manually segmented a subset of these images to create our segmentation mask "ground truth." Our model for segmentation to obtain the non-diseased region is using a Mask RCNN. This is a particular deep learning model that takes as an input an image, as an output is going to be a binary class for us, giving information of which pixels correspond to the non-disease and which pixels are from the diseased area. As our datasets are small, we need to use pre-trained models as we can find for Mask RCNN. We only need to fine tune these to our both current datasets. As they open up this network is a gray scale mask, we actually need to apply some thresholding techniques to extract a binary sample as we see in the bottom right. After we have our non-diseased region of pixels, we need to characterize them with a skin tone metric. Particularly we use individual typology angle that is highly correlated with melanin index as you can see in the bottom it is scattered to the right. ITA is an angle metrics, so we need to move from pixel information, transform it to another corner of space and extract luminance information, and b that quantifies amount of yellow in that pixel. We transform these to an angle and after we have these ITA values, we can bin them into ranges. Previous work used 5 different categories to bin these ITA values, but we extended to 8 to have more fine granularity of different skin tones. As you can see lower ITA values, go to tanned and darker skin and higher ITA values go to lighter skins. So now we have our skin tone information. Now we want to see if the skin disease classification model performs equally across all the skin tone spectrum. For this, we replicated state-of-the-art models, particularly here we're looking at Densenet, that we used for pre-trained model on ImageNet and basic regularization methods such as dropout and early stopping. You can see that across both datasets, we have similar accuracy to the ones in leader boards or reported in other papers. Let's look first at results on segmentation of ISIC 2018. Here, we can see a high accuracy and a low mean absolute error in ITA computation. What this means is how good is our method to detect which pixels belong to a non-diseased skin area against the other ones and how well can we compute or estimate the skin tone based on these pixels. For the second dataset, we can see that our accuracy goes a bit low and our mean absolute error for ITA computation score high. Some explanations are that this dataset doesn't have any standardized way to collect their data. We will have different body parts here and various regions and lumination settings. So further work can be done around these type of dataset. The 2 skin datasets are highly biased towards lighter skin tones, as you're getting fair. Only a few samples for tanned and darker skin in both available datasets. So all the dermatology algorithms that are being trained currently are heavily on these ranges of the skin tones. But their bias in the datasets doesn't seem to affect the classification result for each of the skin tones. But the caveat to this result is a very small sample size for darker skin tones. So further research should be done, on these part of the dataset and for running some experiments. Some extensions that currently Hannah, one of our members is working on, is how can we use the same framework, but to dermatology textbooks. So currently our framework runs for dermatology datasets but we want to expand these to textbooks or academic papers. So professionals can have information on the representation distribution of examples in these documents. We still have a lot of work going on. We are wondering how can we create datasets with more evenly distributed samples across all skin tone categories? How can we improve the segmentation process into images such as textbooks or papers? And how can we collaborate with dermatology professionals here in the continent to avoid only sources for US-centric in dermatology? Here are all my colleagues that are working on fairness and different healthcare applications. Thank you so much for your time. - Thank you, Celia. While Dr. Milner is bringing his slides up, I'll go ahead and introduce him as well. Dr. Dan Milner is the Chief Medical Officer for the American Society for Clinical Pathology based in Chicago and works with more than 20 countries on pathology capacity building, including telepathology and personnel. So this morning - thank you so much to my fellow speakers who have set a wonderful stage for what I'm going to discuss. And what I'd like to do is dive really deep into pathology very briefly, because this is a rich source of images, someone in the Q&A has already asked and pointed out that. And so I just wanted to go into that in a little bit more detail. So to begin, I just want to talk about pathology images and first how is a pathology image made, which I think is important for everyone in the audience to understand the complexity of that process, before we actually get to that image. Then talk about the layers of a pathology digital image from the lowest level resolution to the highest level and what that means for research implications. And then lastly, talk about additional data, which has been touched on briefly by my fellow speakers. So let's talk about a pathology image from a patient to a data file. So for those of you who are familiar with pathology, this will be a little bit of review. For those of you who are not, keep in mind, that there are 3 main sources of, or 3 main types of samples that you may use in Africa for pathology. Those are cytology which is the study of cells, surgical pathology which is the removal of tissue from a live person to make a diagnosis of cancer or infectious disease. And then autopsy pathology, where you remove tissues from a dead patient in order to make an anatomic diagnosis after death. And that whole process here illustrated by an example of taking a sample from a breast tumor is not dependent on pathology, it's dependent on the clinicians. The proper way that the sample is taken, handled, processed, labeled, and fixed before it is sent to the pathology lab, can have massive impacts on the quality of the image that's produced at the end. And so this is a component of research or a program that has to be well understood and documented. Once that sample is collected and sent to the laboratory, it's in a fixative, which has to be of a certain type, it's put on a tissue processing system. It's then embedded in paraffin wax, that paraffin wax block is then cut with a razor to reuse a slide. That slide is then stained using a variety of different staining techniques, and then you have a stained image. And this is the classic histology slide, or even cytology slide that a pathologist would look at under the microscope and read. And as you can see, there's already a lot of steps in this process that can go wrong. If fixation is wrong, if the tissue processor isn't functioning properly, if the tissue embedding isn't done with highest quality, these images are not going to be ideal. Now we have our glass slide and we need to, we need to scan it to create a whole slide image or a digital image that we can then analyze with the computer. Well, at this point, we have lots of options. There's not just one way to scan a slide. There are at least a dozen different companies that make whole slide imaging system. They use different background processes. Some of them like Phillips use Blue Ray technology. Most of them use basically a microscope inside the system that scans either by moving the slide or moving the microscope objectives. And they can scan at different resolutions. They can scan at 20x or 40x and have optical zoom up to 80x or 100x but all of that is variable and needs to be understood before you try to use this for research. And then lastly, once you have scanned that image using that system, you now have to look at that image or study that image and there are a variety of viewers at that point. Each company that makes a scanner will have a different software viewer. There are some agnostic viewers that are available as open source software and all of those use different file formats. They process the image slightly differently when they open it. And so those are other aspects that need to be understood. So just to recap this, as we go from the patient to a digital file of a pathology image that we want to analyze in our research, there are all along the way, steps around quality, around assurance, et cetera, that you want to make sure you're keeping up with and accounting for in order to make sure that that final image you're looking at is of high quality. And this is not something you can take for granted. If I had the time, I could talk to you for hours about the challenges of pathology in Africa. This is what ASCP works on. We work with multiple laboratories to help improve their quality, to provide equipment, to provide immunohistochemistry. And the goal is to produce high quality images for diagnosis, but also high quality images for research. What about pathology image analysis? Now my colleagues have shown you radiology, gross image of the cervix as well as skin, and those are very complex, as you could see the process that they were using. Pathology is slightly more complicated than all of them for a couple of reasons. So if we think about the image that I'm showing you here of 4 boxes, 2 red, 1 blue, and 1 black, and we want to teach a computer how to figure out what those are, the orientation, et cetera, it wouldn't be that complicated. And this is very similar to trying to understand a radiological image, which is in black and white or possibly even a skin image, which is almost two dimensional, the way that you look at it. But with pathology, we have all those different colors, but we also have layers. A histology slide or even a cytology slide can be many microns thick. And so from a given image, the camera can actually take multiple layers and have multiple slices or Z stacks that we have to look at. And in research, you may need to account for that. Then when we finally look at our image, for example here, this is an image within the thyroid of a parathyroid, something very simple. You can see at this low power that the surrounding tissue is different. And the tissue in the middle is unique or as of one type and for a pathologist, they look at this slide and instantly think, that's thyroid with a little parathyroid in it. But for a computer, it first starts and says, there's a light area, there's a dark area. And then it does pattern recognition to figure out where it is. And then it can finally say, this is likely a structure within a thyroid gland and looking at the color of the nuclei, et cetera, it can determine using AI or comparative metrics that this is likely a parathyroid or some other structure. But this is at a low power. If we now zoom in to a higher power, as in these 2 images, these are from completely different cases, you can see that in the top image, there's a lot of structure. This is a very structured picture, it's got orientation. It looks very unique, and almost like something that anyone should be able to recognize with a little bit of training. And this is in fact a part of the retina of an eye. And so you can see the rods and cones and different parts of the eye. But the lower image of histology is just from a random part of a tumor. And what you notice is that there are some blood vessels in there, that's the red stuff. There's some purple nuclei, there's some pink material, but it's completely disorganized. We don't have that structure that we see above. And it's very high power. This could come from the slide in the middle, but it's such a high power that we don't have a lot of context around it. And so when a computer or an AI algorithm is trying to learn, it can learn at low power, but doesn't have that same learning using the same learning techniques at higher power. It's very important to understand how you're going to sort of approach a case and what you want to learn from it. So just to recap that, if we just wanted to have our computer or our AI algorithm recognize that this was a parathyroid within thyroid tissue, you can imagine that it's not as complicated as trying to teach it to recognize, for example, the orientation of the rods and cones and the retina, or even to tell you what kind of tumor that is on the lower left, I'm sorry, on the lower right. Because of the lack of orientation, the lack of structure, the AI really needs to study those cells to study those nuclei and even look for details that we may not have, in what my colleagues have been calling the "ground truth." That is, a pathologist may look at that slide and say, "Oh, that's a "X" tumor and I recognize that from morphology", but they may be incorporating all the information about the patient, what the low power view looked like, what the high power view looked like. But AI is only looking at this one image and has to be able to also integrate that data. And then lastly, there's all of the additional data that goes along with the pathology. So we've got that digital histology in the middle, which I've just walked you through, but then there's all the ways you can analyze it. You can do handcrafted or hand annotated features, where a pathologist or a technician goes in and marks off different areas. You can use machine learning, where you, as Celia was describing, where you feed the machine thousands and thousands of images, which have already been annotated, and then it learns to interpret new images that it's never seen before. And then you can do deep learning or neural networks as Sameer was talking about where you teach it to understand the interactions of cells on a slide using multiple different layers of information, for example, different stains or different color techniques. And then as Michael was talking about, you may have radiology data that you need to correlate back with histology. And he was referring to histology as an example of "ground truth" to base on radiology, but understand that histology also needs a "ground truth" or a basis to understand exactly what's going on there. And then there's all the omics data, transcriptomics, any other kind of proteomics data, et cetera, that has to be related back to that histology. So there's two aspects of that. One is that histology must be high quality and absolutely perfect for all of these things to be integrated correctly, to give you the best result. But then when it is really exactly what you want it to mean or what you want or need it to be, it needs to be integrated correctly to all of this data using annotation and tags and case tracking so that your omics data on RNA expression is matched to the proper set of histology images. And then when we think about all of this, there are 2 ways that this data can sort of come together, especially in data science or big data projects. We can think about individual patient data, where we have 10,000 patients from multiple countries that all have a same cancer or similar cancer or infection. And we're feeding all the data about those patients into a database. And that's sort of creating working models or AI models for how to do things. Then there's also population patient data. The demographics of the population, epidemiology, cancer registries, et cetera, which may be anonymous data. It may be more about a region or a population than it is about individual patients. And that's important because for those of you that are thinking about projects that are going to have impacts for policy or how you are going to inform the government of spending money, et cetera, you want to think about population patient data, because you're trying to do what's best for the population. But if you're in a program or you're doing a clinical trial, or you're trying to treat patients, you're going to look at individual data to say, what are we learning from the system to go back and help that one patient as we move forward. And that uses very similar techniques and integrates the data in a similar way, but it's just a different level of analysis and a different level of understanding. So I think with that, I will close and turn it back over to Paul. - Great, thanks Dan. So we should be bringing all of our speakers up right now for the panel discussion. I'm going to go ahead and kick it off with maybe a couple questions that were hinted in the talks. So this one is initially for Professor Kawooya, but I think was relevant to everyone. How should we think about applying machine learning and artificial intelligence when our datasets may be small or as Professor Kawooya suggested, some of our datasets have poor image quality. - Thank you Paul, for that. When our datasets up of poor image quality, it becomes difficult for us to apply machine learning because it would be difficult to train the algorithm with images of poor quality. And so I think it's very important that even as we begin, we must ensure that our quality is uniform and that the method we are using is also standard and homogenized. That's all that I can say because the algorithm will just get confused with images of poor quality. - And I can also comment, Paul. I think one of the things that is really important for our audience to understand is the state of the disease they might be wanting to work on. So if you want to do a project on the data science of tuberculosis or HIV or malaria or breast cancer, one country or even one center may have more than enough cases to produce a big dataset, especially if you think about sequencing or other omics that are going to go with it. But when you start thinking about rare cancers, for example, or rare infections, multi-center, or even multi-country datasets may be needed in order to have enough images of a given disease in order to actually do that work. And so I think as you are embarking on what you want to work on or what you're thinking about, consideration of the epidemiology of that disease is really crucial for sample size, as well as the amount of money that you have for these particular grants. - Also on the machine learning side. As you saw in this talk, we use pre-trained models. If you're using image based data, you can actually use training models to base on other datasets more large and you fine tune to your small datasets. So this is a technique that works very well on images because usually these deep neural networks work on the same idea of finding edges or finding features that are common across all images. So you can fine tune your models when you work with small datasets. Regarding quality wise, if you have a match set of low quality and high quality samples, this will be an ideal setting. You can actually train generative models to learn what a high quality image representation looks like. So any low image dataset quality can actually be transformed to high, but for this, you at least need to have the pair of good and higher quality. Thank you. - Potentially another one that we get in most technology related panels is, as we started to talk about the need to develop a sort of new cloud infrastructure, and as people start to think about these grants, what opportunities exist for sort of leapfrogging over some of the fits and starts we've seen in sort of developing standards and other issues in terms of generating large datasets? Are there opportunities to learn from our past mistakes elsewhere? - I'll key off on the word generating datasets. Is that, if you are embarking on a new study and you have the opportunity to acquire new data, then establishing protocol, I'm speaking, not as Dan alluded, to scanning histopathology slides, which is, there are many standards in place in selecting the appropriate scanner and scanning technology may address some of these challenges. But if you're thinking of acquiring pictures or you're thinking of sampling the population, then considering the epidemiology aspects, and then considering the clinical protocols that are in place so that you don't have the variety that would disturb your training. The same time you don't want to make it so artificial that when you revert back to a natural setting, you have a highly skilled and "perfect", in air quotes, dataset. That is not a representation of reality. So striking a balance between the two yet, if you're pushing the technology forward so that you have a good quality set, giving consideration to this. And one thing we noticed is different imaging devices tend to have different imaging characteristics and lighting conditions will influence how the picture appears. And human eyes are amazing at adapting to all the variety, but computers are still ways away from that. - I think initially this one's going to be for you, Sameer, as well. I want to re-ask one of the questions that was asked and answered in the chat. And that's, what are the major limitations of deep learning? You described it and I'll add to that, when we think about deep learning, how do we think about some of the issues associated with the fact that we don't always understand why our algorithm is making a certain decision that is to say, convolutional components. - Right, that is almost almost entering the space of deep learning art than science. And this is there is science that you have to visualize, what the network is looking at and what it's focusing on. Now, you can artificially influence the network to look at certain regions using other deep learning networks. So we think of deep learning networks as decision makers, where you can cast segmentation, which is drawing a boundary around an object of interest, or recognizing an object in an image as classification tasks, then also selection tasks. So having a chain of deep learning networks, one can influence them and therefore you are sort of handcrafting a little bit, helping it along, in a sense, building an end-to-end system that given an image it would percolate through your various networks resulting in an answer. But if you think of it, each individual network, one can put in probes to see if it is being disturbed or distracted from looking at the object of interest, and not by what you want to look at. The only caveat I'd put is if your goal is discovery, that means you have an existing collection and you want to find outlier patterns, then this strategy might be the best strategy. - So we have a couple questions have come in: looking at the intersection of multiomics data and imaging and understanding sort of the value added through adding multiomics data. I think this question could really go to almost any of the speakers. Anyone? So, okay. So potentially? - Could you repeat the question? I'm trying to look at it, but - The question is what sort of multiomics data has been used in the context of imaging and what is its sort of value added been? And then sort of what other omics data would be a priority or valuable to increase the prognostic value of sort of a machine learning or AI tool. - So let me start off, maybe I'll prod Dan a little, if he can jump in. But I think he was alluding this toward the end of his talk, where for us at least, omics data as in terms of coming from radiomics, genomics, or proteomics provide a sense of truth, absolute truth that helps us train an image. So looking at an image alone, you want the AI to step in and provide a high standard of truth at a very low cost point, entry point. So if you think of the cervical cancer problem, well, I would like to use a common handheld mobile phone, let's say. To be able to determine if a woman is in a precancerous or a stage at least where she needs more repeated frequent visits, the disease is progressing. Now, the only way we learned that was true, we know that the images themselves don't provide the truth. Humans are gloriously bad at looking at and classifying this. And if we throw the same set of images with the same human truth standard at an AI, it will arrive at the same level or slightly better perhaps than human performance. But if you want to advance the AI, then the idea of introducing other omics data as truth standards. So we are not showing the truth standard to the machine, we are saying that this image truly is positive. And now figure something out, see something more in the pixel patterns that we are sort of extracting away from so that we can help it. So that's one way is providing absolute truth standard. If you're going to now expand this further and include other health record data, along with other omics data, then you are expanding the kinds of inputs to a much more glorified, deep learning networks then. So that it can take in multiple input sources and provide a better level of decision, the other end. That's one slant at it. - Yeah, and I would just add that pathology images, for example, diagnoses of cancer or some types of cancer, myotonic counts, classification of tumors to a very high level degree, are often used as the "ground truth" or the gold standard by which to then look at Omics datasets or genetic datasets, et cetera. And I think early, early studies, for example, of transcriptional profiling of tumors demonstrated that the transcriptional profiles matched up pretty closely to the histology, but then when those datasets grew and became very large, it was clear that within, so for example, adenocarcinoma of the lung, there were subsets of transcription profiles and that's where new therapies, new diagnostics have actually come from. And so I think you always start with a gold standard and histology is obviously an excellent one, if you have high quality as I was describing, but the goal is for the AI or the research that you're doing on the image analysis to supersede that and to tell you something new beyond just what that gold standard told you. And I think as you add more omics datasets to a given grounded dataset in pathology, you learn more and more about those images and ultimately come up with better diagnostics. - Thanks, I think this one is for Celia. So this is a question around sort of how you approach achieving an equitable AI tool, that is to say, and the question goes further to ask whether you use a multi-model sort of per country or per population approach or a single model attempting to be more general. - That's a good question. When we learn at least for our specific problems it's always by population base. You will not have a 1 single model that will tackle all different populations. Regarding fairness and equitable in machine learning, I think the first step is be transparent about how the model was trained. What is the population that was working on, as when other researchers in other cultures or other side of the world are adopting this already trained model, they can know to what extent this model will work for my problem. So the first thing it will be to be transparent. And then one very interesting comment is how can we introspect these models? One of our panelists showed the GradCAM visualizations. These are things that help you understand how the models makes the decision. So you can have a notion on why that decision was made. So I think introspection of the model, be transparent on the limits on how it was trained, and the performance system is super important to build better machine learning solutions. - And thank you. And our next question is for Michael. And the question asks, sort of how big of a data, sort of a collection of data, do you need to start thinking about image analysis and AI diagnoses in the context of ultrasound? - Ultrasound can be very tricky, depending on for instance, if it's something like a gall bladder stone, where it's a yes or no, then you may not need so many images to train that. But if you're looking at something a little more complicated, for instance, if you're looking at breast cancer where there are different varieties, depending on the biopsy you would certainly need several hundreds of pictures to do that. So it will depend on whether it's a black and white, like gallbladder stones, are they there, or they are not there, or if you are measuring for instance, biparietal diameter in a fetus or the femur in a fetus, that's a little more straight forward. But where there is a little more heterogeneity where the image is more complicated, then we certainly would need more models because there are several varieties of that disease. But usually the ratio is to have about 70 training datasets to about 10 or 20 of testing and validating. So that ratio, one should attempt to have a big training dataset. And then about 10 or 20% of that as a testing and validating dataset. - Thank you. I think this next question could be for everyone. And it asks whether there are international standards for characterizing images that could be helpful for having good datasets. And then there's a second part to the question, where the person asking the question is concerned that based on the discussion we've had so far that each research group will really have to collect their own initial images to make sure that they're sort of clean, or conform with, what is needed for the algorithms. - This is Dan, I'll just comment very quickly. For pathology there are standards for how to use those slide imaging systems to collect images. There are also standards for histology quality, cytology quality, et cetera. And those are sort of large and complex. But at the end of the day, when you are doing, for example, an oncology project or a clinical trial, this could be done in this grant program and you need a correct diagnosis, there are standards for reporting of cancers, which include all the different features, that come from the image, that they aren't about the image itself. They're about all the other things. So there are quite a few standards out there. And for the second question, I think, if you are going to collect your own data, and I think Celia's point is really well taken. If you're going to collect your own data, likely someone has collected a dataset like that before, and so seeking out those existing datasets to use as a comparator, to use as a high quality gold standard against your images is probably the best way to normalize your dataset and correct any problems early on. - Okay, thank you. So our next question is, and I think this is for the whole panel potentially as well. What are your thoughts on the utility of crowdsourcing or other similar strategies for annotating imaging data or pathology data, so that the potential role of citizen science. - I'll just add really quickly. So part of what we do at ASCP with our telepathology virtual support system is each country where we place whole slide imaging equipment has a team of 15 US-based pathologists that review those images with them. So if that site decides to engage in a research project, they have automatic crowdsourcing of 15 experts to review their cases. For those of you who are planning on or thinking about pathology, I can offer that ASCP would fully help to support you in that regard. If that's something that would be helpful for you for image validation. And I think it is a very, very vital way, to make sure your images are high quality. If you have one pathologist look at a histology slide, they're about 60% accurate. If you have 2, there are about 85 to 90% accurate. If you have 3, it's a 99% accuracy on the diagnosis. So crowdsourcing is really crucial, especially for complex cancer cases or difficult histology. - Right, any other thoughts? - In Africa where you have limited expertise certainly would need to have some other person look at it and validate the imaging. And that's why it's important for us to have colleagues let's say in the US or elsewhere who have experience in that disease condition or who see many of those pictures so that together then we can have a higher degree of accuracy or validity for that image. So for us in Africa, certainly it's important that we team up with another university where, or another research center, where they are also seeing many of those images and have more experience than we have. - And a small addition to this will be to always keep inter and intra observer errors measurements, where we use crowdsourcing, to be reported next to the dataset and the labels. - Okay, I'm afraid Dr. Milner is going to have to drop off of the panel from this point. I want to jump into our next question and that's, it seems, so the question is that it seems that good machine learning productions can often disagree. And the question is whether any new disease mechanisms or insights have been derived from ML machine learning predictions that allow pathologists to assess sort of new "ground truth." - I can answer for pathology, Dan's dropped away, unfortunately. But... ...in the limited experience I have with, or finding, so question is the way I would cast this question is if different ML predictions disagree, and we agree that what it has found obviously deviates from "ground truth," unless the ground truth was an aggregate label. So you don't know where the disease is, but that the entire image represents disease somewhere. Then a human expertise could step in, and verify if the highlighted regions are indeed diseased. And if there are any correlates there. However, if you do have localized truth, then the immediate response is that, or the naive response might be that, if the machine has found some other region that is a surrogate or an alias of the true region, is it really a false positive, or is it something indeed new, is it a correlate? And that again goes back to, away from computer science and data science, into the interaction with clinical experts or biomedical experts to see if this is a prevalent condition that is seen widely across the population of that after being analyzed, or is it an outlier? And if it's an outlier, are there other information in other records, other tests that might suggest that this is a meaningful find? - If I understood the question, in imaging, for instance, there's been a lot of progress with glioblastoma, which is a tumor of the brain in ML. And we see now that ML may help to make a difference between the different type of glioblastomas, the mutant type and the wild type using machine learning. And the naked eye may not be able to pick those differences between those mutant varieties. So in other words, machine learning or artificial intelligence sees what the eye cannot see, and this correlates very well to biomarkers in molecular imaging because the biomarkers for the mutant type of glioblastoma are different from the biomarkers for the wild type of glioblastoma. And artificial intelligence can help us to sort out this, whereas the human eye may not be able to make that difference between these 2 varieties of a glioblastoma. So there are areas where artificial intelligence has gone a long way to see what the eye cannot see and helps us to be able to predict what that molecular type that may be. Whereas the eye cannot predict that. - So I've another question and I want to start with your view on this, Celia. The question is where individuals can go to learn to build some of these machine learning models? And is there computing capacity in Africa for building and interpreting machine learning and AI solutions? - Yes, definitely. So our lab has a strong relationship with the School of Engineering in CMU in Rwanda. We always have great Masters students from the University. So I will definitely recommend those programs. IBM also hosts several courses where the machine learning basics also blockchain and other seminar topics. Hopefully year by year, we will see that all these models are being able to run on cheaper and cheaper hardware. So this is great for everyone that doesn't have access to big clusters and so on, that can still with a few notebooks or a small CPU, they can still run experiments. Also, all the pre-trained models that we can access, all those neuron networks have been running for weeks and weeks in millions of millions of samples. And we get those finetuned matrices volumes in our computer with zero computation cost. And usually we will train with the smaller datasets. So I will say yes for small runs of course we have the capacity and for bigger, of course we have the cloud. - Thank you. So I'm going to ask one last question. The question is, why by and large are many of these machine learning and AI tools, not yet being used in clinical settings? - That's a tough one to answer because a lot is, I think, between technology, that is the developing a machine learning tool, developing even say a product, to it fitting into a particular clinical protocol. There are many steps, they include approval from in the US, the FDA, or appropriate health ministries, agencies in respective countries. Acceptance by the community of how they will translate results from a machine learning model to clinical practice. Is it merely a classification score or is it a prediction of risk to the patient that will suggest an appropriate intervention for the right member of community? Is it, for example, from a pathologist standpoint, it might be as naive as highlighting regions to look at, which of course it uses the time and there is a performance metric there. And, but they, if you skew away from missing any cases, you allow false positives, you still have saved time, but you will not risk losing the patient. However, intervening from a surgical standpoint, when the patient doesn't need to be intervened, or you are in midst of a surgery, you can imagine a live system as an AI system that is serving as a guide and you have an unnecessary incision. Now, the risk is much greater. So there are several factors that play into it that mostly allude to risk approvals of an acceptance by the community and other factors there. - Anyone else who'll add to that? - Yes, I think for a machine learning perspective in clinical settings, I think we will get to the point to help professionals to make decisions. But currently we cannot allow machine learning models to make their own decisions. As we don't have clear transparency on how the decisions are being happening or under what population this model was trained. And given that health is such a critical thing, we can hope for these type of models to assist professionals to see different patterns that maybe they wouldn't have access to, or see other "second opinions" let's call from machine learning models. I think that is going to be more viable, but as any other process here in Kenya, you need KEMRI approvals. You need to go over a process of ethical consults to see that you are complying to everything as any product, as anything that gets to a clinical environment. There are a lot of regulations and we need that. The models that we deployed are being transparent and know the limits of their support. - So I think I'll address. - I also add that the artificial intelligence and machine learning is something very new to Africa and many, many doctors, many practitioners are not yet used to that. And I think, we'll have to see how can we get this into the curriculum for undergraduate and postgraduate training so that they know the benefits of applying this in routine clinical practice and where it's applicable. So I think that training is important. And then secondly, it's also the relevance. If it does not address a clinical challenge, a clinical problem, which I encounter every day, then I'm not likely to use it. But if it addresses, if it's relevant to my setting, my healthcare setting, then I'll take that up. Plus of course, the ethical, legal implications as already noted. - I guess I would take the risk as a moderator of adding a thought as well. And that the regulatory science is really developing in parallel and only recently have we seen sort of machine learning and AI specific regulatory pipelines (indistinct) from our FDA. And so a lot of this is still happening in real time. With that said, we are out of time for new questions. I want to invite each of the remaining speakers to maybe add a final thought, if you have one on the session as a whole, before I turn it back over to Maggie. - Well, I think I'll quickly end the silence there. I think this was a wonderful session, I particularly enjoyed it because it provided a broad spectrum of opportunities, challenges, the close interaction between data and design and the bridge between computational sciences and biomedical sciences towards improving patient care overall, that is the ultimate goal. And there are many steps toward that, which are broadly data science. So I hope that the attendees have been inspired and the opportunities that NIH is providing would be taken advantage of over time. - I also want to say that yes we have embarked on the process to bridge that gap between the high and low and middle income countries, with regard to data science application. It's a small first baby step, but I think it's a very important baby step. And we should have as many people on board, as much collaboration as possible to make sure that this works, this very first baby step really works. - Some of remarks here on my side. Thank you very much, the panel was super interesting. I learned a lot too. Something that I will highlight and reinforce is all these machine learning solutions are done with the main experts side-by-side. As computer scientists, we cannot sit alone in a lab. I get a bunch of data, match it to our deep learning model, and publish that. That is not sufficient. We always need a domain expert showing us new patterns on the data or new patterns even on the insights of the models. So my highest recommendation is always to work with the main experts in any problem that you are trying to solve, and especially with the community that you're working on for sure. - So I want to thank our speakers again. And I want to thank our attendees as well. We had well over 100 participants in this session. So I think there would be a booming round of applause happening right now, if we weren't virtual. And with that said, I hope to see all of you in the Networking Lounge, and I'm going to hand it off to Maggie, who will explain how to get there. - Exactly. Yes, we have come to the end of this webinar, but we are not necessarily finished. You are invited to continue the conversation by joining the interactive networking session in the Networking Lounge. And I am going to give you a little demo of how to get there. So I am going to imagine that you are probably, if you go back to KIStorm, I would imagine you're on this page. This is today's page for this session. This was the Zoom button that you joined to get here. And if you just go down 1, 2 pages, you'll see the Networking Lounge. That's where I invite you to go after you leave here. And when you come into here, you will see that there is a button to enter the Networking Lounge. It will take you into another interface called Spatial Chat. You can choose for your camera and microphone. I would encourage you to do that. You can do microphone only if you want, but it's nice to do the camera as well. You type in your name here and then you can join the party. and then you see that you are in the Lounge. If you hit the little negative sign here, it will make the screen change for you. So you can see that there are several tables. You can also see who else is here. I got a couple of other people here with us already. But we've asked our panelists to do is to place themselves at tables. So Sameer is going to be at the orange table, Celia will be at the green table, Dan will be at the pink table, and Michael at the blue table. We'll have some other people in there you can see. And what happens is, if you move your avatar close to someone else, you can see them and then you can hear them. And if you move away, then you can't. So can have conversations and you can go to different tables and chat with different people. So that is how you will get to the Networking Lounge. But before you go there, I just have a couple of other things I would like to share with you that are important information for you to know. First of all, we're really interested in your feedback on today's session. So in the chat, we'll put this link as well, so that you can easily click on it. We invite you to just do this very quick survey to let us know how today's session went for you. I want to remind you about the hashtags. If you think that there was something interesting that came out of the day that you want to talk about in social media, we encourage you to use these hashtags. There are some more series coming up for the State of Data Science series. Next week, we have one on Leveraging Data Science Approaches to Address Environmental Health Challenges in Africa. following on. We have Biomedical Informatics, Maternal and Child Health Care. The following week The next week, we have two Infectious Diseases. Using Data Science to Fight COVID-19 And then later on, the 21st Health Metrics. So we encourage you to put those in your calendar. You can learn more about all of them. If you go to KIstorm we have updates on when they are, who's going to be speaking and you can get more information and put them in your calendar. And as we reminded you before up next, we have the Interactive Session. That's happening right now in the KIStorm Network Lounge. And we invite you to see you there, and we thank you very much for your participation today. I want to thank Dr. Pearlman, Dr(s). Antani, Milner, Cintas and Kawooya for joining us. Thank you very much. And we'll see you in the Networking Lounge. Please do fill out our feedback form. It really helps us to know what you thought of the session. Helps us make sure we design the next sessions to be interesting and informative for all of you. See you in the network session. (lighthearted music) 