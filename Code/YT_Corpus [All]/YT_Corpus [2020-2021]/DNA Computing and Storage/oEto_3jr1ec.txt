 are you suffering death by a million paper cuts hopefully not because that sounds awful but if that's a metaphor for your small file uploads messing with your performance then you're in luck today on cloud storage bytes we're diving deep into the world of optimizing your small file uploads so stick around let's say you're a biotech company and your main product is a software that works with DNA across multiple distributed machines everything is great but whenever you add a data set your clients are stuck waiting and waiting to begin computation you've checked your architecture and it's pretty straightforward a new data set comes in gets divided and uploaded a cloud storage triggering a pub/sub alert for your clients to start computing so what's the problem here this is the tricky bit the amount of work each client is responsible for changes depending on the number of clients connected to do work so having more clients leads to smaller work blocks as the clients balance out that workload this means that your software will take ages to upload all the files to Google Cloud storage in regions with the highest number of available cores so let's say you've got a data set with files of various sizes and you upload each file to a regional bucket over and over and over again if you were to take a look under the hood while this upload takes place you'd see that as file size increases so does the throughput speed meaning smaller files are gonna bog you down like a death by a million paper cuts which once again sounds rough the reason for smaller files slowing things down is because of the amount of transactional overhead for uploading those files the smaller each individual file is the more times this transactional overhead adds up and gives you issues there's good news here and that comes by way of parallel uploads as we hinted at in an earlier episode you can use the - M option in gsutil to perform a parallel copy and dramatically increase the performance of your uploads if you were to revisit your example of the data set from earlier and take another peek right under the hood this time using parallel uploads you'd see significantly higher throughput as that pesky transactional overhead is mitigated across the multiple parallel threads problem solved and paper cuts avoided but how does this help with large file uploads we have answers but that's a totally different episode if you want to know more about parallel uploads for small files check out the link in the description and don't forget to subscribe give us a like and tell us what more you want to learn about Google Cloud storage thank you for joining us for this quick byte of cloud storage see you next time [Music] 