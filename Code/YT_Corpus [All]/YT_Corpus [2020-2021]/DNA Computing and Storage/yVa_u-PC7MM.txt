 [MUSIC]  Hello everyone. We are going to get started. So please take your seats. We're going to talk about storing data in Azure. This is one of the fundamental sessions. So it's an introduction to Azure. If you have never worked with data in Azure, you're in the right place. I have to do like that to see you but I'm going to stop because it's tiring. If you've never worked with data in Azure, you are in the right location. If you have already worked with data in Azure, you can stay anywhere, I'm sure you will find some things that you may not know. We are going to talk about storage, accounts. We are going to talk about Blobs, Files. We're going to talk about NoSQL and SQL Data. Then we'll even talk a little bit about Data Warehouse and Data Factory. Cool. First of all, the agenda why would you even store some data in Azure? So that's going to be the first few slides. We'll talk about the advantages and what is really convenient when you put data in Azure. Then I'll show you Cloud storage. Cloud storage is really a fundamental offering of Azure. In fact, every time that you create for example, Azure Functions or an App Service, Website, or a virtual machine for example, you will need some storage. You will need some places to put your files. So very often, creating a storage account is a very first step that you do when you go into any service in Azure. Then, I'll do you a demo and we'll create a data store in Azure. We'll use File share, but I could just in the similar manner, I could use Blob, I could use Queues, I could use even NoSQL data in tables. Here we'll use File share just to show you how you can connect to that. Then we'll talk about Cosmos DB which is another very fundamental offerings that we have. It's our NoSQL offering. It has multiple APIs. It is globally replicated. It's pretty awesome. So we'll see an introduction to that. Then after that, I'll show you a demo. We'll do a run-through into one of the Cosmos DB instances that I created, and I'll show you a few features. Then we'll go into SQL because let's face it. What would Microsoft be without SQL, right? It's one of our very important services. So we'll talk about SQL Server. But we'll talk also about the new offerings that we have which is called SQL Database Managed Instance. I'll show you a demo, and then we'll talk also a little bit about PostgreSQL, MySQL. We have different ways to store data in Azure like that. Finally, we'll go into Azure Data Factory and SQL Data Warehouse. Data Warehouse is like a big data storage options that we have. Azure Data Factory is a way to build some ingestion pipeline and I'll show you a demo about that. Then we'll do a quick summary at the end. Cool. So, why would you even put data in Azure? Why would you even put data in the Cloud? So probably, my favorite reason why you would do that is the reliability, the high availability and the security. Okay. Because every time that you put data in Azure, this data is going to be replicated automatically at least three times. So you're always going to have at least three replicas, which means that if one data server would break for some reason, we are going to automatically fail over to another one and your users are barely going to notice something. Okay. You even have the possibility to configure those replicas to be in a different region. So like this user is really a disaster in one region and those things happen. Okay. Last year, we had a case where in Texas, it was a combination of very very hot temperatures. Then the electricity broke down. Literally, we couldn't cool down the data center anymore and the data server starts melting, like literally melting. So that would be what we would call a disaster. Okay. Then you can have an automatic fail over to another region where it's colder. I don't know. Netherlands for example. Then after that, basically your users are barely going to notice a change. So that's really cool. Security also super-important. We'll talk more about that a little bit later in the session. But basically, encryption. We also start by default, its rest and also during the transport. So when your data is on the server, when its being transported. But also the mere fact that in our Data centers, most of the times, there is nobody around. So the floors are most of the time empty. Sometimes you have some maintenance people who go there. They are accredited. Basically, you have a high-security, a high probability that nobody is going to steal the data center which seats under your CEO's desk. So that's typically something. So high accessibility because of all that. Also the fully managed services that we have. So for example, if you put your data into SQL database or into Cosmos, we have the possibility to suggest you some improvements. Like if we see that you have some queries which are long running, we're going to make some suggestions on how you can improve that. We're going to make suggestions for cost also. We're also like to how can you save costs basically? We are going to make some security assessments that's optional. But if you switch that on, we're going to tell you, okay, we think that you had an SQL injection attack. We think that you had an unusual login activity. For example, a user try six times to login and it doesn't work, these types of things. We have different storage tiers. So you can take advantage of hot tier, cold tier, archive tier, which are differently priced. Of course, archive is cheaper. So this is also a good thing to have. Automated backup and recovery. So if something happen, your data is backed up and you can recover that in a nice manner. Also something which is really nice is that we have one common storage system for different data types. So for example, if you are acquiring a firm or you are getting acquired maybe you have to integrate different things. Maybe some people use Blobs or they use Files. Maybe some people use SQL data versus NoSQL. In Azure, we offer you-all that. Then you can have services which allows all systems to communicate. So it is pretty cool as well. We also have of course the global access, so you can reduce latency for example by putting the data closer to your users in Australia or in Asia for example. So those are just a few of the advantages that come to mind. If I summarize that in three big categories, I would say cost-effectiveness is definitely something which is nice in the Cloud because we are going to scale you up when you need more capacity, bandwidths, RAM, CPU. But we also scale you down, which means that if suddenly the peak demands that you have is disappearing, we're going to scale down and then you pay less. Okay. So it's very easy to have more data servers but also less. So that's nice. On the other hand of course, if you buy a Data server, it's there forever and even if it's not used it's still going to cost you money. The other thing is reliability because of the backups, the load balancing like I said the different replicas that you have. These are store recovery scenario fail over scenario. This is all built-in to Azure, and it's all available as a service, as a managed service if you wish, and storage types is the fact that we have different types of storage that can easily communicate together. In fact, we even support hybrid scenarios, where you can have some data on your premise, some data in the Cloud, and have some communication system between them. Good. So if we go into storage accounts, like I said it's a very fundamental offering that we have. Every time you create an Azure function, a Website, a virtual machine, etc, you're going to start by creating a storage account. We have four different services plus also disks, which is a kind of service if you wish. So Blobs it's one of the service we have, Blobs are binary large objects. They can be literally anything you want, they can be video, pictures, PDFs, text files, really anything. Then you can upload that to a Blob container, and then you get access to that for example via Rest. In fact all of our services have Rest APIs, so it's very easy to build some clients using the Rest APIs. We also have SDKs like you would expect for example for.NET, for Java, for JavaScript, so it makes it a little bit easier to access but everything is accessible via Rest anyway. We also support streaming scenarios, so if you have some big files for example, you can stream them which is convenient. Another storage option, another service is File. So you can create a File share in Azure, and then you keep getting access via Rest like all the services but here we also offer access via SMB, which is a protocol which is supported on Windows, Linux, and Mac OS, which means that for example on Windows you get access to the File share directly in Windows Explorer and I'll show you a demo in a moment, so that's quite convenient. If you have an existing File share on your premise, and you want to move everything in the Cloud, it's a simple copy paste in Windows Explorer and then you're already migrating your data into the Cloud. Another option we have is Queues. Now, Queues is a little bit special because it is made for messages. So it's a system where you can have multiple subscribers, multiple senders and basically when a message arrives, the subscribers are notified and can do something. The messages can be 64 KB in size, so they can just carry a payload. One scenario which is really nice in Queues for example, is that you can use that to have your On-Premise services communicate with your Azure Services back and forth simply by sending Queue messages, so that's quite convenient. Also, Azure scenarios that we support is Serverless like for example, if you do have, let's say Blob storage but also for Queues, every time that you get a message, you can execute an Azure function, which is a Serverless offering that we have. So basically, this function is going to run and do something for example, it could, I don't know forward the message to a signal or a service, which is then notifying clients via notifications or maybe doing a push notification to an Android or an iOS client, all these kind of things. The fourth service that we support in storage accounts are Tables. So Tables is a simple NoSQL way to store data, it's basically a key value type of Table. It's really very easy to access, it's very resilient, you can store terabytes of data into that, you can even store petabytes of data if you shelf them. So it's quite convenient, it's also fairly cost effective. Now, they are not as powerful and they are not as globally replicated as Cosmos DB. So for example, if you start with Tables, you have a migration path into Cosmos DB because we also support Table API in Cosmos DB, so that's quite nice but Tables are really a nice way to get started. In fact, when I create mobile applications, that's very often what I do and I want some storage in the Cloud, typically I start with Azure Tables because I find them quite nice to use. You can via the SDK for example, say I'm going to take an object and just send it to the Table and then it's automatically sterilized in the Tables, so it's very very convenient. Then we have Disks. So Disks are of course fundamental for virtual machines but you can also create a disk for something else if you want to have some storage. Here we have different options, for example we have Premium SSDs which are faster, they are also more expensive. We have Standard SSDs and we even have HDDs, which are of course more cost effective but a little bit slower, still very convenient, very easy to use. So when you create a VM, you're going to start by creating a disk automatically for the OS but then after that, you can add some data disks if you want for your data, so that's quite nice. Now, the nice thing with storage account is that it is built on a unified system, which means that, for example, if you learn how to work with Blobs, the way to work with Files is very similar in the SDKs or with the Rest APIs or the way to work with Tables. So you have SDKs that you can install but they all look very very similar. Also, you have a lot of things which are automatically available such as encryption at Rest, your data is always going to be encrypted when it's not transported. During the transport, you can also switch on HTTPS, TLS etc, this is all available by default. Also, they strongly consist in replication, so if you have multiple replicas, we are going to take care of the consistency for you, fault tolerance, load balancing, etc. So let's talk about encryption because this is really important, I think you agree. So encryption is always available by default on Azure. Everything that you put on Azure in terms of data is going to be encrypted. This encryption happens at Rest with a symmetry key, which means that it is faster to actually access your data using symmetric keys. By default, we use our own keys but you can of course replace that with your own keys if you want and then you have the total control over the encryption this way. Now, we also support of course encryption during transport. For example, if you build a website it's going to be by default HTTPS, if you use a custom domain on your Website it's going to be HTTP unless you add of course a custom certificate in which case again it's HTTPS. The transport between database for example, we support things like ExpressRoute, VPN, which means that again your data is going to be encrypted and during the transport. So all that is available by default always and then you can also configure it by adding your own keys and by supplying your own certificates. Redundancy and Replication, now that's quite important because I just told you that you can have multiple copies of your data. So it's good because you may want to put your data in a location which is closer to your users to reduce the latency, to make it faster to access the data. But also for higher availability in case one data center would have a problem, then we failover to the other one. So we support four different ways to configure Redundancy and Replication. By default you're always going to have at least a Local Redundant Storage, which means that your data is available three times. Now, those three replicas might be in the same building. So it's within one region and even if we have multiple buildings inside the region, we don't control where those replicas are going to be. So you need to take this into account for example, in terms of latency etc but it's nice to have three replicas in case something happen in one. Now, if that's not enough for you, if your data is really sensitive and you're afraid that there is a disaster, we also support other things like for example, Geo-Redundant Storage, in which case, you will select another region which can be very far away. Then you will have six replicas of your data, three per region and in that case, we have a GRS where you have one primary for the read and the write and then the other one is used for backups and for failover, so that if the first one disappears, we balance the load to the secondary one. Now, if you want to also reduce the latency for your users, you can use this solution, which is a Read Access-Geo Redundant Storage, where basically you have one primary for the write but then you have two secondaries for the read and of course that can make the Read Access faster if your users are very far away. Now, in certain regions, we also support something called Zone Redundant Storage. Now, this one is quite popular, it is being rolled out as we speak. So you're going to see it appear more and more in different regions. This one says that, you're going to have your three replicas but they are going to be in different zones. Now, the definition of zone differs a little bit depending on the region where you are. So it can be that it's a different floor in a building, if the zone is really small but usually if it's a big zone like for example, the Western Europe one, zones are going to be in different buildings, and those buildings are going to be far away from each other, so that they are still within the same region. So the latency is small but on the other hand, if I don't know the building would burn for example, the other buildings are safe. So it's an additional securities that you have for an interest in price as well. Good. So I'm going to show you a demo and for this demo, what I'm going to do is go and create a storage accounts in Azure. So to do that, I'm going to go into the web portal which is a nice way to get started, but you can of course do everything I'm going to do here with different tools. So we support things like the Cloud Shell. Cloud Shell is really nice. It is a way to start to shell here directly into a web portal. Then inside there, you are going to, sorry about that, you're going to have access to Bash or PowerShell. So as you see, I can switch here. So, even if you are far away from your desk computer and you want to quickly access your resources to make sure that everything is okay. You can do that here. Now, another way is of course we use PowerShell on Windows, you could use even Bash on Windows, if you have the Windows subsystem for Linux, very convenient. Or of course, you can use things like Visual Studio. We have the possibility to create a storage account directly in Visual Studio. So here, I'm going to use the dashboard. So, every time that I want to create a new resource, I start here in the top left corner with "Create a resource" and then you're going to see the Marketplace and you see our most popular options. For example, if I go down to storage account, so here we have different storage options. We have SQL Database, Azure Cosmos DB. Here, I have storage account. Those are the most popular, but you have a lot more here, okay? Every time you have a Quickstart tutorial. So, those are really good to get started. They are going to be some small five-six minutes Quickstarts. Usually, those are articles, sometimes there is a video with that as well. For example, here you see how you can create the storage account in the portal, in PowerShell, in Bash, and here you see how you can get started with DotNet, Java, Python, JavaScript, etc. We have tons of those. So very, very convenient to get started. So here I'm going to say, I want to create a new storage account. Here we go. Then this storage account, I need to configure it. So I need to enter a few information, and it's always pretty much the same type of information that you enter when you create new resources. So you start by seeing which is a subscription, where you want to create your resource. So this is basically where you're going to be built, and then you need to create a resource groups. So resource group is going to be basically, a group of all your resources. The name is self-explanatory. But really what it means is that it makes it easier to maintain, to manage your resources. For example, if you delete the resource group, all the resources inside the group are also going to be deleted. So it's easier to manage your applications. Good. So in that case, I'm going to create a new resource group, and I'm going to prefix it with my initials, lb. Let's say, simple storage, for example. It's going to quickly check if it's available, that's fine, and then the resource group is created. Then I'm going to enter the storage account name. So here, same thing. I'm going to say lbsimplestorage, well. Finally, I need to select the region where I want to place it. So here in my case, I'm going to go into Western Europe, which makes sense because we are here. Other options you can select as you see, we have standard or premium SSDs. We have storage V2 which is just the version of the APIs that access. Here we have the redundancy that I mentioned before. So, we have Locally, Zone, Geo, and Read-access redundant storage. In that case, I'm just going to choose "Local" because it's just a small demo. My three replicas are going to be enough. Then we have Access tier, and those govern basically how hot your data is. So if you choose "Hot" it means that server is going to run all the time, the access time is going to be very fast, okay? Very short access time. If you go to "Cool", that's good for data which is not access as often, okay? Like for example, you have data which you know, I don't know, some reports where people upload reports and then you access it like once a week to create a report at the end of the week. So, that is cheaper but then, of course, it takes maybe a little bit longer to react. Finally, on Blob container but only on the Blob container itself, we also have an Archive tier. What archives means, it's something which is not accessed very often. For example, grades for backups. Okay? If you have a backup, put it in Choose a blob and save it as "Archives" then it's cheaper. Good. So in that case, I'm going to say, "Review and Create". It's going to validate everything I entered. If everything is okay, I'm going to create a resource. Now this is going to take just about a minute, a little bit less than a minute to create, and you can always follow the progress here. If you click on the "Notification" over here. So basically, this is where you see all deployments that you have in progress. This is going to create a storage account with the four services that I mentioned; Blobs, Files, Tables, and Queues. But in that case, we are only going to use the Fileshare, and I'm going to show you how to connect to that from Windows Explorer. So, we are just going to give it a couple more seconds to get started. All right, couple more seconds. Here we go, it's created. So now I can go to the "Resource". Now you see that I am inside the Web Portal in my Storage account, okay? It was created. I can see the Overview here. But I can also configure different scenes. For example, I can say, right here, I can configure some access keys. If you're using some SDK in the application, this is going to say basically, the address of the storage account if you want. You can configure Geo-replication here. So here I have a locally redundant storage. So I have just one region, but if you had Geo-redundant storage, you could select different regions. We can configure encryption. So in that case, you can use your own key for the encryption if you want. We have tons of things which are quite interesting. Then if you go further down here, we have the services. We have Blobs, we have Files, Tables, and Queues. So in that case, I'm going to go to "Files". Here, I'm going to create a new Fileshare, and I'm going to call that "My files" which is super nice name, and of course it needs to be small. Here we go. All right, so now it's good, and then I'm going to create a quartile of one gigabyte to get started, but of course, I could go higher. I'm going to create this File share, and then, how do I connect to it? Well, I'm just going to click on it. Then you have here, a "Connect" button. So if I click here, and you see that right now it's empty, I can connect for example, in Windows using the Z letter, and here you have a PowerShell script which allows you to connect. So why don't we do that? So I'm going to copy that. I'm going to start PowerShell here. Then I'm just going to paste that here. It's just going to take a few seconds to actually connect to the drive, and that's going to connect to my Z drive letter. So now, if I open Windows Explorer and go down, you see that here I now have a File storage and notice that the address is the address that I gave before. It'll be simple storage. So now it's cool. It means that I can go and create, for example a new text document and let's call that Amsterdam.txt. Then I can go back to my portal, I can refresh, and you'll see that after a short wait. Okay. Now the file is available, that's cool. If I add the directory here, "Hello folks. " All right, let's be inclusive, "Okay". Then I go back here, you see that my directory is available here as well. So basically, it works, which means that anybody who has access to this subscription can connect to the Fileshare and then can use it to share documents between a team, for example, something like that. So super easy to use and very convenient. All right. So that would be a quick demo of the storage option. Now, I'd like to talk to you about data. So let's go back to the slides and we'll talk about Cosmos DB. So, Cosmos DB, like I mentioned, this is a fundamental offering of Azure. It means that, in every region where we have Azure, you'll have access to Cosmos DB. It's also a service that used to be very pricey but went down in price quite a lot. So now, it's very reasonable. You can use it for a very reasonable price, and it has a lot of advantages. For example, it is globally distributed. So it has been sought from the stat really as a global service, which means that you can very easily configure multiple replicas of your data and you can also govern the consistency models, say, okay, I want to read to be far, sorry, a rights to be fast but then I'm going to wait a little bit for the propagation. Or I want to read, I want to write, sorry to be a little bit slower but then when a write comes back basically, I know my data is available everywhere. So you we have these kinds of things. We also have elastic scale out, which means that it's very fast to scale up and down a Cosmos DB instance. So if you suddenly need more capacity, more bandwidth, more whatever, because you have a peak demand, for example, that is going to react very fast. We guarantee low latency, high availability. We have those SLAs, which are service level agreements where basically, if we don't fulfill our promise, we are going to give you some money back. So, we support multiple models, key value, for example, like a Table Type thing, Column-family, DocumentDB. We also support Graph and also one thing which is really nice is that we support multiple APIs. So it means that, if you have an existing application for example in MongoDB, very easily, you can take the data, put it in Cosmos and then you just switch the connection string, and you don't need to modify your application because we use the same APIs. In fact, tomorrow I will show a demo of that in my Migration session. So it's Mig 20 data migration. It's 1050, and I'm going to show you how to migrate Mongo to Cosmos, as well as SQL Server to the Cloud. So that should be a fun session hopefully. All right. So Cosmos DB is used to power global solution. like I said, most of Microsoft released running on Cosmos these days. A lot of Azure is running on Cosmos as well. So the data is distributed, you can very easily configure an additional replica, you don't need to change anything in your application, the load balancer is going to take care of selecting the database which is closest to your users. We are also able to build real-time experience because we have such a low latency, we guarantee less than 10 milliseconds for read and right, and so you can build systems like for example, some bidding system, okay? Because in a bidding system, you want everyone to be notified as soon as there is a new bid to avoid fraud, or to avoid these Gaussian. Also, fraud detection like I mentioned. Now, this is also great for things like gaming, or IoT, or e-commerce, because we handle the spike. So if you have certainly a need for capacity, or a need for bandwidth, or a need for any thing, we're just going to scale you up, and then when these peak demand disappears, we're just going to scale you down like this, you don't pay for what you don't use, okay? Another thing is, of course, we have a lot of offering working natively with Cosmos DB, for example, Azure Function serverless offering, you could have very easily an Azure function running every time that something changes in your Cosmos DB database, for example, if you want to clean up your data modal before you run some AI on it, or if you want to create some reports for example, that's absolutely possible. We also, of course, enable all normal Azure services, which means also analytics things like, Spark, Hadoop etc, you can configure that. Finally, it's great for lift and shift, because like I mentioned, we support multiple APIs, which means that you can literally take a MongoDB, take a Cassandra application, put the data in Cosmos, and basically you don't need to change anything in your application. So global distribution is such a fundamental feature of Cosmos DB that I want to underline this again. Why is global distribution good? First of all, the high availability, right? Because the data is available in multiple places, in one place would disappear, the other place is still available, but also as a low-latency, because we cannot even at Microsoft, go faster than the speed of light. We tried we failed, and so all you need to put the data closer to your users, and like this you're going to reduce the latency, and we have those SLAs, which is basically guaranteeing a low latency. So let's see a demo. So what I'm going to do here, is show you an existing Cosmos DB instance. Let's go into the portal. So here I have one, and to create that Cosmos DB, as I did before with the storage account, create a resource, then I selected Cosmos DB, I entered some information, and then here we are, or I could also create it in the Shell, or in the CLI, or in Visual Studio, etc. So here you see that I have my Cosmos DB, it is own line, it has one collection which is in fact the collection I'm going to use tomorrow in my migration demo, so that's why it's already here. As you can see, I have only one region which is configured, which is in Eastern US, and that makes me a little bit uneasy because things happen, right? So maybe I want to have multiple replicas of my data. So it's very easy to configure. I'm going to go here to replicate data globally, and then you see that I can select different locations, for example, I'm going to say I want to copy in Western US. I'm going to put the copies in western Europe because it's close to us, and then I'm starting to have customers in Australia, Singapore, and here in Hong Kong. So that's cool, and I'm going to just save, and basically this tells a replica, okay? Now in your applications, you don't need to do anything, because the connection string is actually pointing to a load balancer, and this load balancer is going to be intelligent enough to know when's the data is available. So it's automatically going to go to the copy of the data which is available, which is the closest to your users. So you have just one connection string to worry about. Now, if for some reason you want to access a specific location of your data, it's also possible. But typically, we don't do that. Typically, we just address a load balancer. So now the replication process started, okay? Few other things I want to show you. Here, you have your connection string, so you have all the information which is important for when you build an application. Here we have the Data Explorer, which allows you to go into your data and explore the data, so that's pretty convenient. For example in that case, this is a Mongo API. So if I click on documents, you will see that here I have my documents which are appearing, and those documents are stored in, if we could see that, in JSON format. So this is a standard for Mongo, so that very convenient here I could also interact with my data, or if you prefer we also have something called the Azure Storage Explorer, which is an application that you install, and then you can click on that, and it's going to start the Storage Explorer, and then you will see your Cosmos DB accounts as well. So that's quite nice. Now, the other thing I want to show you here, you can configure all kinds of things like for example, firewalls, etc. But you can also check the matrix, and basically, you can see how your database is doing. So in that case, we see that we have the two replicas which are starting, okay? It started with the Western US, and right now it's not available yet, but eventually, in a few minutes, we'll have the data available everywhere. Here, I also have the possibility to check how my data is fairing in terms of SLA's. So if I go to latency, here it's going to tell me, how my reads and writes are performing. So here I see that we guarantee 10 milliseconds or below for a read, but we also guarantee 10 milliseconds or below for a write, and right now the read is around two milliseconds. The write is between, let's say five and six millisecond. So it's quite important to observe that, because if you see that we exceeds those values, we are going to give you some money back. That's what the SLA says. Another important matrix, availability. So right now we guarantee 99.999 percent, okay? Which means that the database may only be out of work for just a few seconds per moments, and here we are actually at 100 percent, so it means that everything is running fine. I really want to see the availability go to 100.0002 percent. But apparently, it never happens. So all right. So that's what we have here. Other things, write, consistency, etc. So important to keep an eye on your matrix, so that you make sure that everything is working fine. Okay. So that would be a Cosmos DB solution. Now, let's go back to the slides and we'll talk about Azure SQL, which is of course, very important as well. So Azure SQL Database is basically a way to store data in a relational manner in Azure, Okay? You can migrate for an on-premise SQL server to a Cloud SQL Server, and the nice thing is that, because it is an SQL Server family, you can do that without changing your applications, okay? Now, in Azure SQL Database, we see I had a few incompatibilities especially with old versions of SQL Server. But now we have a new offering which is called Azure SQL Database Managed Instance, which is extremely compatible. We are almost 100 percent compatible even with SQL Server 2005, and so basically migrating is super easy these days. Now, Azure SQL Database in the contrary of VMs, so very often what people do and they want to migrate to the Cloud, they do a backup of their data, they create a VM, they restore the data in the VM, and this is what we call an IaaS, an infrastructure as a service, okay? I would say personally, I prefer Azure SQL Database because it's platform as a service, which means that it is a fully managed service. You're going to have offerings like performance assessment, where we are going to check your performance and offer some solutions if you're too slow, okay? We are going to propose how you can basically optimize your costs. We also check the threat detection that I mentioned before. It's also a service, you can switch it on, and then you can make sure that your data is safe. So all that is offered in a managed relational Cloud database service. Also, and however, I would say, because it is NoSQL offering, all the tools that you're using, all the SQL tools are going to work. So if you are used to use SQL Server Management Studio, you can use it if you're used to use Visual Studio, you can use it, you're just going to direct to a different connection string and then everything's going to work in the same manner, okay? So that's quite convenient. Also, like I said, this is a fully managed service so we have scenes like, maximizing performance, reliability, data protection, etc. So quite convenient. Now, also another thing which is nice with SQL database is that, if you have a lot of data, sometimes you want to run some scripts directly on your data, and for example, optimize the data, clean it up, or maybe you want to run some machine learning algorithm. So now you have the possibility to do that directly on the database, which means that you don't need to first transport the data somewhere else, run your scripts, and then put the data back. So basically, it's faster to do so, and it's first of all, of course a lot less in bandwidth. So we have for example, R directly integrated into the database. So this is great for data scientists, because if you want to create an algorithm which is going to clean up your data or which is going to do something with the data, you can do that directly on the database. It's good for DBA's because you can manage your data with scripts directly on the database or business analyst, so that's quite convenient. Our integration, like I said without moving the data, and the cool thing is that, once you create predictions, for example, you create, you run an AI model, you create your predictions, you can also export your predictions as a stored procedure, and then if you have a client application, you can basically calls that, and take advantage of the data using the built-in stored procedures which is exposing the prediction that you created with your model. So it's quite a nice way. Good. So when you migrate database, and that's going to be the main topic of my session tomorrow. You have typically those three phases, assessing, migrating, and then optimizing the data. So assessing, we have different tools whose had for example, the data migration are systems which I'll demonstrate tomorrow. But then, when you do the migration, very often, so you're going to move the data, the re-hosting basically, and then you'd need to refactor your applications a little bit, like for example, switching the connection string at least. But then, sometimes you need to re-architect your application or maybe even rebuild your applications completely, because maybe you are using features that are not available in the Cloud or maybe you are moving to another platform and it's not available. It's a little bit annoying to do that. Now, the cool thing is that we have these SQL Database Managed Instance, which is a new flavor of SQL Database if you want. This one is really a premium offering, so it's for enterprise clearly. But the nice thing is that it is going to offer extremely high compatibility even with super old versions of SQL Server. So that allows you to really concentrate on moving the data, refactoring your obligation lightly. But in most cases, you don't need to re-architect or rebuild them. Also, because it is a managed service, because it is a platform as a service that we have, you are going to be able to run optimization. For example, coursed optimization, either during or after the migration, or maybe performance optimization, or maybe the security detection etc, which is an area where a Virtual Machine is not offering you the same flexibility. Good. So that would be this. So what is really SQL Database Managing Instance? Well, it's a new flavor of Azure SQL Database, if you want. So if you are used to Azure SQL database, it's going to be very easy for you to work with Managed Instance, and Azure SQL Database is still available. So it is using the same infrastructure, which means that you can use the same tools, you have the same benefit. But in addition, we have this increased compatibility, and also it is very secure. It has been built from the start with the understanding that certain enterprises want a database without any public IP address. So in an SQL Database Managed Instance, all the IP addresses are private. So if you want to connect to that, it's within a virtual network. You can use express route, you can use VPN. But basically, it's not open to the public at all, and so of course, that increases the security quite a lot, so it's nice. Now, it is, like I said, the premium offering, so it's more expensive than Azure SQL Database, but the nice thing is that we all see the cursor transparent. For example, if you decide to have more core or more storage options, you can do that in the portal. You see immediately how much it cost, and like this you don't have bad surprises. Good. So when you do the migration, typically, what people do is that they say, "Okay, I'm going to do a backup and restore, and it's an offline migration scenario, where basically you need to switch off your website during the migration." Now we have these new offering which is called the Azure Database Migration Service or DMS, and I am going to demonstrate that tomorrow in my migration session. Basically, what it does, it's really cool because it allows you to do that in a fully managed way. So you can script it, you can configure it in a bottle if you prefer, you can run it, and then as long as the DMS is running, your data, your source, and your destination are kept synchronized, which means that you don't need to switch off your website or your application. You can start the DMS. You do the migration. You can test your new data, you can run this, maybe a staging environment against a new data to make sure everything's running all the time. The old website, the old data is still running. When you're ready, when you're comfortable, you switch your connection string, and then you can switch the Database Migration Service Earth, and then leaves your shirts that you didn't lose any business during that time. So that's quite nice. Minimal downtime, and you can use that to migrate from different services to different services. So for example, you can migrate from MongoDB to Cosmos using that. You can also migrate from SQL Server to SQL Database or SQL Database Managed Instance. You can even use it to migrate from AWS hosted databases into Azure SQL Database or even from Oracle. So we have quite a lot of offering here and we keep adding using. So that's pretty good. Multiple sources. We also support MySQL, PostgreSQL inside the DMS. So tomorrow MIG20, I'm going to show you more about that and we'll do a demo as well. So come and see that.Talking about PostgreSQL, MySQL, I want to mention those because this is also a new offering we have in Azure Database, sorry. So PostgreSQL, MySQL are some very, very popular open-source database systems. They are used widely, and now you have the possibility to host those in Azure database directly, which is quite nice because it is all score's going to improve the availability and performance, because again, this is replicated, this is highly available, this is secure because it's in the Cloud. It is also going to be easy to manage and monitor because it's a standard Azure offering so you can use all the Azure monitoring tools to make sure that your data is running fine etc. You can run some AI you'll need, so the AI prediction that I mentioned before. But also, because it's about of Azure it is totally integrated in all our services. For example, DevOps management tuning, performance tuning, Data Visualization which is about AI, for example, etc, and of course, the database migration tool. Security and compliance, Azure is compliant with all major organization so a nice way to put your data there and to be pretty secure. Good. So let's talk about Azure SQL Data Warehouse, and then, we'll go into another demo. So if you have a lot of data like what I showed you right now. It's great for terabytes of data. But let's say that you have petabytes of data. Like you have a big data scenario. Maybe you have an IoT solution. You are monitoring multiple buildings or maybe you're building, gathering some data for some AI models. Then Azure SQL Data Warehouse is a good solution. You can scale up to petabytes of data like I mentioned. Also, you can run some parallel processing only. So we have some massively parallel machines that you can switch on and you can process the data, for example, in order to clean it up or in order to run some AI models etc. Cool thing is that you can also pose those compute machines. So like this you save money because those are, of course, quite expensive to run. So you want to run them only when necessary. You can run queries directly on all non-relational data, and so because it's an SQL offering, it works with all the familiar SQL tools. For example, SQL Server Managements with your Visual Studio etc. Again, because it is an Azure offering, it's going to integrate with all the services you would expect. Azure data factory, I'm going to show you a demo in a moment. It's an ingestion system. But also things like Stream Analytics, Hadoop for reporting, Machine Learning, Power BI for a dashboard and revolting etc. So nice solution here. It works with a number of sources. For example LOB, CRM, graph data, image data, social data, IoT. So basically, as soon as you have big amount of data, you can put it into Azure SQL Data Warehouse, and then you can integrate, for example, with Hadoop. You can store the results in Blob storage directly. You can run some machine learning models. Either your own models or our creative modals owns that and have some nice AI on edition. Good. So let's talk about Data Factory. So this data needs to be transferred in a way and Data Factory is a convenient way to do that. Like for example, if you need to ingest some data, it's not really a migration. But you need to ingest data on a regular basis. Like you say, "Well, I don't know I have some salespeople, and every Fridays they send me their sales report and they put it in a folder somewhere and I want these to be integrated into the database automatically, and then I'm going to run some analytics on that. So it's quite nice because with Azure Data Factory, you can ingest from a large number of sources. We'll see that in a moment. But also you can run some scripts and some algorithms to reduce the data. Like for example, MapReduce, High-speed Spark, etc. Also things in C-Sharp, and then when these data is clean and trusted, you can basically store it and then you can use it, for example, for analytics, for AI, etc. So it's quite a nice way to do things, and so I'm going to show you a demo now, and for this what we are going to do, is go into here. We are going to go into the portal, and I'm going to show you that we have here a "Storage Account". So it is exactly the same type of storage account that I created before, except that this time I'm not using files. I'm using a blob storage, so I have blobs here, and then you will see that there is a "Blob container", which is here. If I click on that, inside there, you see that I have a text file. It's transactions are TXT which is somethings that I imported from somewhere. So basically, it could be, for example, my point of sales service which uploaded that through an application, and then if I edit the blob, I see the text file. It's comma-delimited that any test some dates and some values. That seems reasonable enough. Cool. So this is what l have, and now, the destination for this data is going to be an SQL Database which is running in the Cloud in an SQL Server. This database right now is empty. I have nothing into it. In fact, I just created it, and if I go here into "Query Editor", you see that I need to login to my database. I'm going to go and take the super secret password. Nobody look. Ali right. Good. So now I'm logging to the database, and if I open the tables here, you see that it is empty. So literally, I have nothing in these database ETL. So I'm going to create a new table. So to do that, I'm just going to run here a small SQL Query. Run. Now, I'm going to "Refresh" here, and now we should see that now I have a table, but this table is empty because I just created it. Good. So now, how do I ingest my data? Well, I'm going to use a Data Factory in that case. So I just went here on Azure. I created a resource. I say, this has to be a Data Factory, and I'm going to start here as a website to author and monitor my pipeline. Just going to take a few seconds to start, which is great because like this, I can drink a little bit of water. All right. Right in time. So now I'm going to say okay, I could create pipelines so I could create something from a template, in that case, I'm just going to copy the data, and I'm going to do that is you can see here I'm going to do it once for the Demo, but I could also run it on a schedule so I could say okay every night at, I don't know 9:00 PM. For example, I know that all my sales report are in. Please insert the data, and then after that do something with this data. So in that case, I'm just going to run it once. Let's click on "Next", and I'm going to configure the source data. Okay. So right now I have nothing, so I'm going to create a new connection. In here when I create the new connections, this is all the sources of I mentioned before. So you see that we have a lot, we have Amazon, we have Blob Storage which I'm going to use. We have Cosmos DB, we have thongs like MySQL. We have Cassandra, Concur. We have of course Dynamics, FTP even we have Hadoop, we have HTTP, Hive etc. Even things like Mongo of course Oracle okay SAP somewhere in there also. So basically, we have a lot of different sources that you can choose. In that case, I'm going to configure an Azure Blob storage as a source, and then the next step, I'm going to say where is this Blob Storage located. Well, I'm going to use here an Azure subscription, and I'm going to go in to my "Ignite the Tour" here we go, and then the storage account name is going to be "lp6s5" here we go lp6s5 storage. I'm going to say all right, now it's going to basically create the connection gets started, and then the next place I'm going to say, where is my data somewhere to browse, and inside here I have these Blob Containers that I showed you just before, and here I could easily select the whole Container so that he teaches every single Container, or just a file. I'm going to say that's correct. Now it's going to automatically detect the formats, or for example, it say, okay this is a comma delimiter, and you see that the data is already been parsed, so I can quickly double-check if that's correct. Let's click on "Next". So this is my source, then I need of course a destination. So this nation I'm going to say, all right let's again, create a new connection in fuzzy destination. Again, I have quite a lot of choice, but that case, I'm going to choose an Azure SQL database. I know that you could even choose here in this SQL server on On-premise SQL server if you want it to do so. Here I'm going to take my Azure SQL database where I just created the table. Same thing as before I need to say, all right let's select the subscription "Ignite". Let's select the server name "lp6s5" SQL Server. Now inside the server, I should have two database or I want to select the one that I just created. I'm going to login with SQL authentication, which means I need my username and I need my password [inaudible] and then when everything's ready I'm going to say finish, same thing as before it's going to connect. All right. Now I'm going to say "Next", and then he's going to tell me where do you want to put this data, in that case, I have these transactions tables that are just created I say yes. Next step I can change the mapping for the columns if I wanted to, but hearing that case I don't need to, and here I can also run some scripts on ingestion for example, to clean up the data, and here I can also say what's happens when something doesn't work. For example, I can either aboard the activity completely, or I can just keep the roles which are not compatible just to make sure that everything's working fine. All right. Let's click on "Next', just a summary that's good. Now I click on "Next" the ingestion is actually running. Everything's good and now if go back to my database here, and go into the Query Editor, I can say select star from transactions, let's run this and quickly you see that now it is actually working my data has been ingested. So here I run the important ones, but I could that out on a schedule like this for your salespeople for example. This is a process to import the data is as easy as just dumping them in a file share, or dumping them into a Blob Container via an application. For example, these things of course coming directly from SAP, directly from different systems. Good so that data factory for you. Now I just want to give you a quick tour of an SQL Database Managed Instance, because those are new like I said. So this is an SQL manage instance that I have here, which I'm going to use tomorrow in my migration session, and here in that case, you see that this database has already a little bit of data inside. It is running correctly, and what you can do here is that if you're not sure how to get started you can do a quick start, and then you can see what's happening here. So we have different tutorials. How do you connect to the machine using for example remote desktop, or you can connect using SSMS. Here we have all the connections strings. So you have everything you need, to get started with our ADO.NET, JDBC, ODBC, PHP etc. Now surprising, like I said this one is a premium offering. Okay. So it is more expensive than Azure SQL database. It's an Enterprise offering. But the cool thing is that it is highly compatible even with very old versions of SQL Server. So even if you're using some exotic features I would say you're going to be safe here, like cross database joins or maybe you have some compound CLR running on your database, or maybe you have some special stored procedures, for example, we're almost 100 percent compatible, and also another thing is a high-security. So this machine is running totally isolated in a VNet with only private IPs, and if I want to have for example, an application service, I will also put it in the same VNet, and then I have high security basically. So here it's totally transparent. If I increase the number of score you see what's happening. So you can check the price before you do. Now this button is really nice. It's what we call our "Save money" button. Which is the one that our accountants don't like but we love it, and basically if you click "Yes" you are going to leverage your existing On-premise SQL Server licenses, and then you can save up to 55 percent, and you see that if you click "Yes" and suddenly you pay a lot less. So make sure to click "Yes" if you do have SQL licensees. Like I said our accountants on those happy but honestly who cares. All right. So that would be this, the last thing I want to show you is the threat protection. So we have here advanced data security, and if I click here for 15 books amounts I can turn it on, and then I can configure which threat I want to detect. So here for example, SQL injection, Data exfiltration, Unsafe action, Anusual client logins etc. So this is all available in these managed service quite nice to use. Again, premium offering so maybe tried to go to Azure SQL database. But if you see that you need more security or you need more compatibility. That's definitely a good place to be. All right. So we are reaching the end of the session. So I'm going to show you, first of all a quick summary and then we'll have a few resources. So why would you put data in the Cloud? Well first of all, like I told you right scalability, we are going to scale up but also down so that you save money when you're not using the resources. Reliability because of the multiple replicas, the automatic failover all right. The security which is built in with the encryption and everything. Simplicity to use it the same SQL tools, is the same MongoDB APIs in Cosmos. For example, it's the same Casandra APIs etc. Built-in intelligence as managed services, and all sorts of global availability, which has different advantages. If you want to learn more about Azure storage, you can go to Microsoft.com/learn, and there we have quite a few learning paths, where you can learn for example, about storage, about NoSQL data in Cosmos, about SQL data in SQL Server etc, and also about securing your data. So it's quite nice. Microsoft LAN is a nice offering we have, you can gain some points as you go. So there is also some gamification. You can tell you about hey look, I did something and then, I need a raise for example, or just compete with your friends and your colleagues for offering people who's paying coffee or something like that right so it's fun. This is why you will get the slides and hearing the session I didn't have code, because I did everything in in the portal, but that includes the slides are available here. Please evaluate this session it's really super helpful for us, because we were runs these tour over and over again in different location, but also we'll come back to you with more sessions in the future, and we'd love to know what you sort. If we can do something better please let us know, if you saw It was awesome please let us know. So that's good for myself esteem. All right. So thank you very much for your attention. I'm going to stick around a little bit more for questions. Thank you. So if you have questions please come forward so that I can leave the room, and also tomorrow from 12:00 PM, I will be at Media expert if you want to talk about the session, I remind you that the 10:50 AM, I have my migration session. So have a great evening and hopefully see you at the reception. Bye bye. [MUSIC] 