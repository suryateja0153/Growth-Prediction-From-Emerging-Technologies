 [APPLAUSE] JENNIFER PRIESTLEY: All right. Thank you so much. So I'd like to talk about why data scientists need to understand ethics. This is an ongoing conversation not only in the classroom, but in conferences across the country. It's certainly been a topic of conversation here at this conference. I really can't walk into a meeting, walk into a coffee shop without somebody asking me questions related to the ethical issues that data scientists are facing both in corporations, as well as for public servants and certainly in the classroom. So as I've been thinking about these topics related to ethics in data science, it actually made me realize that there were some parallels related to ethics in data science with the Manhattan Project back in 1939. So if you're not familiar with the Manhattan Project, this was a group of scientists-- physicists-- who were brought together to examine the possibility of weaponizing nuclear energy. So this group got together in secret without any consultation apart from a few government officials. And their ultimate product resulted in the bombing in 1945 using the first atomic bomb that ultimately brought World War II to a close. So you might not immediately see what was happening with the dropping of the first atomic bomb with the issues that we're currently facing as it relates to ethics and data science. But I think there's some clear parallels here that we can use to inform the conversation. So the first point here is that a few people can do an immense amount of harm. And this is going to be a theme that we're going to talk about as we examine some of these issues that we're facing related to ethics and data science-- that a few people working in a small room, a dark office somewhere can actually cause an immense amount of harm across the globe. The second point is lack of consent. So if you go back to 1939, 1940 when these physicists were first examining the options and potentially the downstream implications of detonating the world's first atomic bomb, one of the issues that they literally had to talk through was the possibility that they could have burned off the atmosphere for the planet-- burned off the entire atmosphere, ignited the atmosphere of the planet. That would have completely decimated every form of life on the planet at that time. None of us-- or certainly none of our parents or grandparents-- were consenting to that experiment, right? So we have this issue of lack of consent. There's a lot that's happening as it relates to data-- our personal data, corporate data-- where we are not engaging in a consensual conversation. And so that's the second point that we're going to explore. The third point is this concept of unknowables. So certainly the physicists, these scientists that were engaged in these conversations related to the first nuclear weapon could have no full appreciation of what was going to happen downstream. Ultimately what they did impacted the whole of history of humanity. And that's certainly something that they couldn't have anticipated at the time when they were examining the opportunity to develop these weapons. And I think that, again, is a parallel with what we're seeing right now with data and with the ethics related to some of the decisions that we're making today that have incredible downstream implications that I don't know that we're fully anticipating and fully understanding and accounting for in terms of how the decisions we're making today with data are going to impact our children and our grandchildren and the people that are coming in behind us. So this is a framework that I think makes sense when we're having these conversations related to ethics. So before I jump into exploring these three points, I thought it might make sense if we talk about how we got to this point. So data science is still a fairly nascent field. It's still a fairly nascent discipline. So whenever I want to understand how topics, how disciplines, how concepts have evolved, I go to the well of all things, being the Google search engine. And so this is what we're seeing in terms of Google searches on data science. And so you can see back in the early 2000s, there's a little bit of trickling in terms of people looking at data science. But these are pretty much just people wearing hoodies out in Silicon Valley. And then around 2011, 2012, we start to see this hockey stick in terms of this explosion on searches on data science. And you might say, well, what is it that was happening at that time? And we could certainly have conversations around how the types of data were evolving, the size of the files were evolving, the computing power was evolving. And things were moving to the cloud. And bigger platforms were happening. And certainly the methodologies were evolving. And all that stuff is true. But really, the main thing that happened was there was a Harvard Business Review article that was developed and published by Tom Davenport that said that the data scientist was the sexiest job of the 21st century. And so it's kind of the revenge of the nerds at that point. So everybody that couldn't get a date in high school all of a sudden was cool and sexy. So that was part of what was driving this. But all these things were converging at the same time. And I'm sure all of you in your respective companies-- if I asked everybody to raise their hand, who has a job opening right now for a data scientist, my guess is everybody's hand would go up. So this trajectory I don't think is going to slow down anytime soon. But then this actually raises some interesting questions, certainly for those of us in academia in terms of what we're teaching. So we spend a lot of time teaching students what they can do in order to get these jobs. And we're not spending enough time talking about what we should do, which gets back to these questions related to ethics. So lest we think too much of ourselves, this is an overlay of Google searches on Kim Kardashian relative to data scientists. I know she produces data in her own way. So you can see the searches on data scientists certainly are subordinated there to Ms. Kardashian. So coming back to how we found ourselves in this place and thinking about how data science has evolved as a discipline, again, if you go back to the far left side of the graphic and you think about what was happening in the early stages of data science, it really all comes back to the data, the evolution of the data. And so the first kind of data that we all worked with, whether you're talking about an academic perspective or you're talking about the early days of consumer finance, which tended to be the first place that we really saw data in any meaningful way-- that data was what I like to call small, structured, and static. So it was data that you could drop into nice rows and columns, Excel spreadsheet-type stuff. And I like to say this is the happy place for statisticians. We like to work with data that is small, structured, and static. And so the traditional tools there are descriptive statistics and predictive modeling classification. But as the data continued to evolve, the data wasn't small, structured, and static anymore. The data that we now see on a day-to-day basis is large, unstructured, and in motion. And this then leads to the needs to develop skills that look different than the skills that are needed in that first box. So if you're dealing with data that's large, unstructured, and in motion, you're going to need to learn how to work with sensor-based data and IoT data. You're going to have to learn machine learning skills. You're going to have to learn concepts like deep learning and support vector machines. And the things that you need to know to translate large, unstructured, and in motion data into information are different than the skills that you need to translate that's small, structured, and static into information. And then as we continue up that trajectory, then we're moving into where we find ourselves today. And that's data that's massive, integrated, and dynamic. Now we're talking about systems of data talking to systems of data. And this is really where we're starting to get into these conversations around artificial intelligence. And we can talk about the skills that people need in order to move up that trajectory. But importantly, the issues related to ethics are very different in that top box versus the issues that we would be facing down in the bottom box. So the issues related to data that's small, structured, and static look very simplistic now versus the ethical issues, the different types of questions that we need to be asking as we move into data that's massive, integrated, and dynamic. So this brings us back to those three points-- so the parallel with the atomic bomb. The three points were that a small number of people can do a great deal of harm. The second point is that people don't typically engage with consent with these issues. And the third is this issue of unknowables. So coming back to that first issue of a few people can do a great deal of harm, I took a look at the major data breaches that have happened over the last couple years. And I just wanted to just briefly bring those up. So last year in 2018, Mariott had a data breach. I hope none of this is news to everybody. I think most of this hit the headlines. So Mariott had a data breach that impacted 500 million people. If you've stayed at a Marriott, your data was probably compromised. And it included information regarding your hotel stay and your financial information if you had a credit card on file. And so there was a breach in 2018 where that data got out and is now floating around on the dark web. 2017, of course the Equifax breach that got a lot of press, a lot of headline. Actually, the number of people that were actually impacted by the Equifax breach was less than the Marriott breach. But it got a lot of headlines because of all the financial data, obviously, for the people that were impacted that were then exposed. In 2016, you may remember the Ashley Madison breach. This is kind of a questionable website where people that were married that wanted to engage in relationships with other married people could go out and exchange information. There was a breach on that. You can see close to 500 million people were exposed. All of their email addresses went public out on the internet. So you can imagine all the implications to personal relationships that happened as a result of that. 2015, Anthem had about 100 million people's records exposed. The reason why this was important is because this now is medical data. So these are people's medical profiles, their doctors' profiles, the type of medication they were taking, et cetera. And all of that went public. In 2014, eBay had a breach. Again, about 100 million, 200 million people. Again, predominantly financial information. And then you may have recalled in 2013, Yahoo had a massive breach where 3 billion people were impacted. So if you had any touch points with Yahoo prior to 2013, it's almost assuredly that that information got out on the internet. Actually, the bigger question there is, who knew there were 3 billion people that engaged with Yahoo? So the point here is that a small number of people can do a great amount of harm. My guess is that when you start to add up those numbers, pretty much everybody in the Western world was probably impacted by at least one of these breaches, which means that your information, your data is probably out there for somebody to have access to. Again, a small number of people can do a great deal of harm. And this actually raises an interesting ethics question that we actually had to face within my own institute. So should researchers utilize hacked data sets that have been released out onto a public forum? So within my own university, we had some students who very serendipitously were out on the website for the elections for the Secretary of State for Georgia. And they were able to download the voting records of every person in the state of Georgia. We knew who they had cast their ballot for. We knew their addresses. We knew their names, everything. All of that data was out there. We pulled the data down. And we immediately started to see some interesting patterns. And then we very quickly realized that that data was not supposed to be public. That data went back off of the website very quickly. So then it raises an interesting question. Do we have the right to use data that was made public through a hack? So that becomes an interesting ethics question and one that I know many professors around the country are facing. So going back to this idea that data that you think is private ultimately can probably be determined. They can probably go back and determine who you are. So here's an example from 2006. You can see the headline, "A face is exposed for AOL searcher number"-- and you can see the number up there. So what happened? So AOL, thinking that they were doing a good thing by putting anonymous data out there for people to do research against, released records of 20 million web searches from across their search platform. They did not identify who the people were. All they did was they identified what they had searched on. Well, guess what? With a little bit of effort, if you can identify what somebody is searching on, it doesn't take a whole lot of effort to go back and identify who that person is. In fact, if we know your gender-- which is pretty easy to find if we can see what you're searching on-- if we know your gender, if we know your age, and we know your zip code-- three pieces of information that we can get pretty easily off of any of your web searches-- it's pretty clear that we can go back and identify who you are. Depending upon what study you look at, the probability of us identifying who you are as a person ranges between about 60% and 80% just with those three pieces of information. And so this poor woman ultimately was kind of unmasked, if you will, when they identified that she had searched on things like "numb fingers," "60 single men," and "dog that urinates on everything." And so they were able to back into who she is. So there's a privacy researcher named Cynthia Dwork. She's done a lot of work in this area of differential privacy. And she's been very clear to say "de-identified data isn't." Because if we know really any of these key pieces of information about an individual record, it's almost a certainty that we'll be able to go back and identify who the individual is. OK. So an another example of this-- I'm sure you're all familiar with the concept of credit scores, FICO scores. And so the FICO score has traditionally been used to assess somebody's likelihood for repayment of a loan. Well, this set of researchers-- this is actually a fairly recent paper that's out there from the FDIC-- have been able to identify that if they can identify a 10-item digital footprint on you and use that to then determine your probability of repayment or your probability of default on your credit, it actually has a substantively higher predictive accuracy than your FICO score. So again, if we know basic things from your digital footprint-- things as simple as, are you accessing the internet from a computer? Or are you accessing it from a mobile device? Are you accessing it from an Apple product? Are you accessing it from an HP product? Are you accessing it from an IBM product? All of these key pieces of information can determine your likelihood of repayment. So why is that interesting? The regulators have been very clear on what we can and cannot use in terms of developing credit models. So things like age, things like gender, things like race, things like zip code to prevent concepts like redlining have all been very heavily regulated to prevent discriminatory action using traditional FICO scores. But when we get into things like digital footprints, that's completely unregulated. So we have all kinds of ethical issues now where we know it's more highly predictive but ultimately is subject to discriminatory practices. So coming back to this concept again of lack of consent, I'm sure you're all familiar with all the shenanigans that have been going on with Facebook. So one of the things that Facebook engaged in was this experiment. And they called it the massive-scale emotion contagion experiment. And so basically what that meant-- and if you were on Facebook at the time, you were probably part of the experiment without consent. They purposely fed you in your news feed highly extreme points of view, particularly incendiary parts of the news feed because they were trying to elicit a reaction from you to see if that impacted what you were posting on Facebook. So they were curating your news feed with the purposeful intention of seeing if that ultimately impacted the way you interacted with the rest of your network. Were you given the opportunity to consent to this experiment? No, you weren't. But ultimately, that was a very broad-based experiment that has now been used again and again and again, impacting news feeds to generate very specific reactions from people that ultimately bring us in many ways to where we are now in terms of this highly polarized society. So again, lack of consent. Some of you may be familiar with the Target modeling issue. So here's what happened. So some data scientists at Target pulled together all the transaction behavior of people that were going into the store, identifying what they were purchasing. And then they developed probability scores for certain life events. One of those life events is pregnancy. So as you're preparing to have a baby and then as you have a baby and then as you're caring for a small child, that's a very profitable customer segment. So being able to predict if somebody is going to have a baby potentially is a very important marketing objective. So these data scientists working with Target data developed these scores. And they identified particular people who they thought were pregnant. And then they would send them coupons to come into the store to buy things. So what happened? So they identified a 17-year-old girl in Minnesota. They sent her coupons. Her father got the coupons and said, how dare you think that my daughter is pregnant. He goes into the Target. And he starts yelling at the manager. He brings in the coupons. And then a couple of weeks later, he had to go back and apologize because, in fact, she was pregnant. Target knew before her father that she was pregnant. So that raises all kinds, again, of ethical issues. Did Target break the law? Absolutely not. And you can see Target's response to what happened were very conservative about compliance with all privacy laws. They didn't do anything wrong in terms of their legal compliance. But even if you're following the law, you can do things where people get queasy. So did they break the law? Absolutely not. Did they violate the spirit of the law? Maybe. Did they do something that was creepy? Absolutely. But then that starts to raise all kinds of interesting questions about if we can predict somebody's life event based upon their transaction behavior, and we can do it with something as simplistic relatively speaking as pregnancy, could we then use that to predict if somebody is going to engage in illicit behavior? If somebody has the probability of developing a mental illness? If somebody has the probability of engaging in a behavior that ultimately would lead to incarceration? And so those types of scores are floating around the internet. And organizations purposefully reach out to data scientists to build those scores to determine probability of outcome. And those are used then for decision making. Is that wrong? I don't know. Is it unethical? Probably. And so again, going back to this idea that we spend a lot of time certainly in the classroom talking to students about what they can do. And we're not spending enough time talking about what they should do. So here's another example of lack of consent. So increasingly, people are very quick to put all of their DNA out on things like ancestry.com. So you spit in a little thing. And you send it in. And then they can tell you who you're related to. And so you may remember a couple of years ago, there was a serial killer in California. And they had some DNA. They were having trouble identifying who the suspect was. But they had some DNA evidence. And they created a fake profile on ancestry.com. And they were able to use that to identify who this person was related to. So these people-- again, lack of consent-- who were one, two generations or two steps removed from the killer-- their DNA was then used to identify who the ultimate suspect was. Did these people have the ability to consent to have their DNA used in this way? Absolutely not. Was the outcome ultimately positive for society? Yes. We had this terrible person taken out of society. But ultimately, the people whose DNA was used to get to that point did not have the opportunity to engage in consent. Again, it brings up an important ethical consideration. OK. So the third issue is related to this question of unknowables, of working with data in such a way today to solve one particular problem but potentially generating unexpected outcomes, unexpected consequences that we're not anticipating. And so that raises this question. Will this algorithm do what I think it's doing? So I'm sure you're all familiar with facial recognition algorithms. So Google and Facebook and Microsoft, these organizations that have spent so much time and energy and money and resources on developing these facial recognition algorithms, are very quick to talk about how accurate they are. They come in and say it's 98%, 99% accurate. And that's typically true if you have very light skin and you're male. The darker your skin gets and as you move into women, and certainly if you're a dark African-American woman, the accuracy on these facial recognition algorithms drops down to just over 50%. That has enormous implication. When we start having these algorithms that treat people differently, that's an issue. So I'm going to pick on SAS just a little bit. So I know some of you guys who are in this session when they were talking about their facial recognition algorithm-- which, by the way, is brilliant. And SAS has been making incredible investments in AI. And ultimately, I think that's going to benefit all of us. But I was disappointed to see that when they brought up the people and they tested the facial recognition algorithm there publicly, pretty much everybody up there on the stage had very light skin. It would have been, I think, more powerful and more impactful if they could have demonstrated a much wider variety, a much wider continuum of skin colors to prove to the audience that yes, in fact, their algorithm works just as well regardless of what your skin color is. OK, the perpetual lineup. So I'm sure you guys are all familiar with this idea that when a crime has been committed, they'll gather up some suspects. And you've seen it in the movies. And they'll have the witness that's behind the glass. Do you recognize any of these people? Can you identify the perp in this context? And that has been going on for hundreds of years. Well, now we've moved into an era of the perpetual lineup, where we are all involved. So across the country, police organizations and law enforcement have been gathering pictures from your driver's licenses, from ATM cameras, from street cameras. We are all now part of the perpetual lineup, where when a crime is committed, then they-- instead of bringing seven or eight people in, now they're bouncing that up against the entire database, where we're all actually included. Now, that again sounds good in practice. But when you go back to that idea that these facial recognition algorithms are much better for some demographics, much more accurate for some demographics than they are for others, again, that brings up all kinds of ethical issues that I don't think that we've fully resolved. We're definitely in a place where the technology, where the algorithms are far ahead of where the ethical conversations are. OK. So when we talk about algorithmic biases, when we talk about, will this algorithm ultimately do what I think it's going to do? Will this AI technology ultimately do what I think it's going to do, I think there's three examples of this that probably deserve specific attention. And I want to be very clear. When I talk about algorithmic bias-- and I hate that term "algorithmic bias." And here's why. Because ultimately, algorithms are just a series of mathematical executions. I mean, addition is not biased. Algebra is not biased. Algorithms are really just a culmination of multiple mathematical operations. And so the algorithms themselves aren't biased. They just do what we tell them to do. So the bias is coming in from the humans. The bias is coming in the way we're bringing in the data. The biases are coming in being introduced by humans, not through the mathematics. So when we talk about algorithmic biases, there's three elements to that I want to pull out. The first is this idea of a pre-existing algorithm. And so you can see the quote up there. A pre-existing bias in an algorithm is "a consequence of underlying and institutional ideologies that may be explicit and conscious or implicit and unconscious." Basically, the idea here is that if you have a bias and you're writing the algorithm, your bias is going to show up in the algorithm. And typically, most frequently, that's unintentional. I'm going to assume most people are not bad actors. You're not intending to be biased in the way you've written the algorithm. It's just a function of latent biases that you probably don't even recognize. So here's an example. So Google has been called out over and over again for having gender bias in their algorithms. And so there was a study that was done recently in 2016 where a female graduate student simply went into the Google search engine, and she typed in the word "CEO." That's all she did. Well, about 26% of the Fortune 500s are run by women. So you would expect something roughly proportional if you're doing a search. But ultimately, what came back was less than 10% of the pictures were of women. So I actually tested this. So getting ready for this talk, I went in. And good news-- Google has fixed it. Bing has not. So I went onto Bing. So I tested it on Google. And sure enough, it was about 20% of the pictures that came up were female. And when I did it in Bing, this is literally what I got when I did the search on whatever day it was, on Sunday or Monday. So this is literally hot off the press. So there's 54 images up there. There are two women. And so just doing some simple math, that's 3.7%, way underindexed relative to 26%. I did that math in my head. Thank you very much. And so the idea here is that I'm going to assume the people behind the Bing search engine are not just blatant misogynistic in their views. But they may have some inherent biases in the way they were developing the algorithms. And so this has, again, implications that we need to be cognizant of. So this is a paper that's still in progress. You can see it's from the National Bureau of Economic Research, February 2019. And these researchers, who are predominantly legal experts, are actually identifying the fact that these biased algorithms, these algorithms that are generating biases related to gender, related to race, related to age, et cetera, may not actually be a bad thing because it's allowing us to uncover latent biases within society that we may or may not actually be cognizant of. And so you can see that quote. "Algorithms are not only a threat to be regulated; with the right safeguards in place, they have the potential to be a force for equity." We can't uncover biases latently if they're not being brought up to the surface. And so having biased algorithms may actually be the first step into addressing some of these issues. OK. So the second kind of algorithmic bias issue is related to technical. You can see the point. Technical algorithmic biases emerge "through limitations of a program, computational power, its design, or other systemic constraint." So what does that mean? So one example of this is the algorithms that are used by lawyers in jury trials. Typically, algorithms are used to predict how a jury is going to vote. Is the jury going to come back with a verdict of guilty or not guilty? And then based upon the probability of that outcome, the lawyer can then inform a decision to his client to say either, yes, you should plead, or, no, you should hold your ground and not plead. And all of that's fine from a mathematics perspective. But that algorithm cannot take into account things like human emotion. The technical limitations are that they can't incorporate how the jury is actually viewing the suspect or the person that's actually on trial. So there's a limitation there in terms of actually replicating fully embracing the human element. Another example of that is the fatal Uber crash. So you guys might have seen this recently-- "Uber crash shows catastrophic failure of self-driving technology." So what happened? So with these self-driving cars-- and I'm not necessarily picking on Uber. This is true of all the self-driving car algorithms-- is they have to calculate when they see something in the road, what is it? Is it an animal? Is it a person? Is it a tree? Is it just a piece of trash? So they have to classify whatever this thing is in the road. And so what was happening was this car had identified a person, but only saw them from here. And so for whatever reason, just given that they only saw that part of the person, all they saw was a white sock. And so the algorithm kept going back and forth. And it couldn't make a decision-- is it an animal? Is it a piece of trash? Is it a human-- because it couldn't classify it fast enough. And so by the time it ultimately came to the decision that it was a human, it was too late for it to stop. And it ran into the person and killed the person. And so that's, again, a technical issue going back to these algorithmic questions. The third point I think is probably the most disturbing. And this comes back to the nuclear detonation in 1945. And it really is this idea of emergent or unknowables. The things that we're putting in place right now-- we don't necessarily know where these things are going or what's going to happen. So let me give you two examples. So one is unpredictable correlations. So going back to the Target example, so we put data out there related to our financial transactions. We put data out there related to things we could be doing online. And when unrelated data sets are brought together to then make predictions, that's where we start to have a problem. That's where we start to get into some real issues related to privacy, real issues in terms of impact to society. So again, we brought in these unrelated data sets. And they were able to predict if this woman was pregnant. But you can imagine, going back to the example I was talking about previously, you can bring in unrelated data sets that have things like your digital footprint, your activity online, standard data related to your voting patterns or things that you've donated to. And all those things are brought together. And now we can develop probability models on your emotional health, on your mental health, on your likelihood of developing cancer, on your likelihood of engaging in certain types of illicit activity, your likelihood of becoming addicted to methamphetamines or opioids. And then those predictive scores can then be used again to make decisions related to hiring, related to allowing you to rent an apartment, et cetera. So those unpredictable correlations with unrelated data sets becomes an issue. The second point is related to feedback loops. So feedback loops are where we curate data to give us more refined, more relevant information. So the most benign example of that is if you go on Netflix and you look at a movie. And it comes back and it says, if you like this movie, you might like this movie. Another benign example, match.com. Hey, you dated her. You might like her. But the more insidious examples of these feedback loops, again, comes back to these self-curated news feeds. So as you start to look at certain types of news articles, that starts to curate your feed. And then you only start seeing news articles that are related to your particular or the things that you've expressed an opinion about that are relevant to whatever it is that you're engaged in. And then it becomes increasingly more extreme. And you get less and less influence from things that are outside of your news feed. And then obviously, we find ourselves where we are today. So feedback loops, emergent-- I don't know that when Mark Zuckerberg first developed Facebook when he was at Harvard he could have anticipated where we are now in terms of curated feedback loops for our news feeds. OK. So all this comes back to, now what? Where do we go from here? And so there's been a lot of conversation around regulation and what's going to happen in terms of regulation. So if you look at one of the federal agencies that I think most people embrace and accept and appreciate, it's the FDA. So why do people like the FDA? The FDA makes sure that the drugs that you take do what they say they're going to do. And the things that could happen as a result of you taking this drug-- it's disclosed. And so the question now becomes, do we need an FDA for algorithms? This has been a point of conversation in a lot of conferences and a lot of meetings that I've been in recently. Well, guess what? This is hot off the press. So this is April 10, so just a couple days ago. There's actually now been-- the Algorithmic Accountability Act has now been introduced into the US Congress. And you can see it's being sponsored by Cory Booker out of New Jersey. And the idea here is that corporations that develop algorithms that make decisions around what you're going to see, credit decisioning, using nontraditional modeling, any of these approaches, anything that impacts you as a consumer-- they're introducing this bill to hold corporations ultimately accountable for the algorithms that they're using. So this is effectively the first step to developing an FDA for algorithms to make sure that the algorithms are doing what we think that they're doing. OK. So as an educator, as somebody who works in academia, I look at this through a slightly different lens. And it all comes back to, how can we do this differently? How can we make sure that the data scientists that we're producing today are making ethical decisions in the future? How can we change what's happening to ensure that people are treated justly, fairly, and equally in terms of the algorithms? And so I was actually sitting at my kitchen table. And I was talking to one of my kids about this. And my son, who is 14, said, you know, Mom, I think everything just comes back to math. That's what you always tell me. It all comes back to math. And actually, I think he was right. Don't you hate it when your kids are right? I actually think he's right. And so the more I thought about this, I thought about it in this context. And I think it makes sense. And I'm going to say something a little controversial. I think business schools are part of the problem. And here's why-- because we're very quick to give people a black-box solution to doing data science. So you don't actually have to understand the algorithm. All you have to do is point and click. All you have to do is input the data and point and click and generate the results. And then you can tell the story. And you can put it in front of the client. And so I think we've skipped a couple of steps. So if we just jump straight to machine learning, I think we're doing the next generation of data scientists a huge disservice. My opinion as an educator is that there needs to be a hierarchy. So if you're familiar with Maslow's psychological hierarchy, the idea is that you have to move through these psychological gates before you can really move to the next level. So the point here is that you're not going to care if you're wearing purple or mauve if you haven't eaten in two days. So you have to move through basic needs as you continue to move up to higher psychological needs. So I think there is an opportunity from an educational perspective to develop a similar hierarchy for data science. You have to understand the basics of the math. We have to teach students basic math. They have to understand mathematically what's happening in these algorithms. And then after they understand the math, then we move them into the statistics, to the modeling. They have to understand the basics of supervised modeling and classification and these basic core skills that are typically embedded in a statistics class, some basic programming. They have to understand how to write some basic scripting languages. And then they can move into understanding the higher-level concepts of modeling and algorithmic development. And then they can do the sexy, fun things related to machine learning. And then they get into the visualization and storytelling. And if we do all those things right, then some of these ethical issues hopefully will be resolved on their own. And so I make this point frequently that I think we're doing our students and the next generation of data scientists a huge disservice if we're cutting out, if we're short-cutting the mathematics and the statistics and the computer science. So I'm always hesitant that business school professors are going to go cut my tires out in the parking lot. But I do think it's a relevant point for conversation. So again, thinking through, we've identified what these issues are. And the Algorithmic Accountability Act certainly needs to be part of the solution. But the government can't be the only solution. We need to work as a community to figure out what this is going to look like. So as an academic, as somebody who works with students in the classroom, at a minimum I think students have to understand the law. So the students have to understand the basics of things like the GDPR. In Europe, they certainly have to understand the current laws on the books. We don't want anybody going to jail because if they're in jail, they can't donate back to their alma mater. The second point there is just creating awareness, just ensuring that the students understand what they're doing. They get so deep in the weeds that sometimes they forget that ultimately the algorithms that they're developing, the things that they're doing could have massive impacts to people's lives and certainly have downstream implications. The third point is engaging institutional research boards. So within the context of universities, we all have these IRBs where we have to go and have our studies reviewed by objective third parties who think through these issues. And it's not an end all be all, but it's just another checkpoint. And I have to believe if Facebook had taken their contagion study through an IRB, it would have never been approved. So there just needs to be additional checkpoints there. And then that fourth point again just goes back to this idea that I think we're doing our students a disservice if we're shortcutting the mathematics and statistics and taking them straight into a black-box approach where they're simply generating results. So those are my reflections on the ethics of data science. I encourage you all as members of data science and analytics community to continue these conversations. So I think we have a couple minutes. I'd be happy to take some questions. UNIDENTIFIED AUDIENCE MEMBER: Thank you very much. It was very enlightening. And it's a topic that needs to be discussed. I was wondering if you could tie a couple threads together. One is you said that as part of the unknowables, will this algorithm do what I think it does? Can we go maybe one level below that and say, with respect to the Algorithm Accountability Act, will the Algorithm Accountability Act do what that act Thinks it does? In other words, as a data scientist, certainly I'm aware of this, the ethics. And I've actually had to encounter it with respect to assisting a police force in determining people they can target to assist. And so for me, the idea that I as a data scientist would be answerable to the ethics that the federal government put in place is kind of scary. Maybe some sort of self-government, self-regulation within the industry would be good. And certainly you're contributing to that by having it be a conversation piece. But what would be your thoughts about applying what does this algorithm-- does it do what we think it will do with respect to potential legislation? UNIDENTIFIED CO. REPRESENTATIVE: Yeah. It's an excellent question. So I think the Algorithmic Accountability Act is-- it's just recently been introduced. So it hasn't been passed. I suspect that ultimately, what does pass is going to look very different ultimately than what has been initially introduced. They say you never want to see sausage or laws being made. So I suspect that there's going to be a lot of iterations versus what's been initially proposed. But let me answer your question more broadly. And that is really, can we just count on the government to make these decisions for us? My opinion is that the answer is no. And let me give you an example of that. So when you're working with highly computational students-- and I'm sure this includes many people here in the room-- they're really smart. And so if they've got an objective, if they're trying to accomplish something, and I tell them, well, you can't do it this way, they're going to figure out how to do it this way. So we do an exercise in our in one of our classes where they have to build the equivalent of FICO scores. And so they know that they can't use gender as a decision variable. But if they've got all the transaction data, they can create some pretty good proxies for gender that map pretty closely to gender. And so then they can use those synthetic variables then in their model. Have they broken the law? No. Have they violated the spirit of the law? Absolutely. So again, I'm going to make the assumption that most data scientists are not bad actors. I'm going to assume most people are good and just and have the intention of making good choices. The problem is they don't understand the choices that they're making. They don't understand the harm that they can do. And so again, as we're teaching the next generation of data scientists, I think we have a responsibility not just to the students, but really to society to help them open their aperture and fully understand, at least to the extent that we can help them with this. Everything you're doing potentially has downstream implications. Every algorithm you're building, every model you're building, you have to think more broadly about the people that are going to be impacted by this. If we just put laws in place, they're going to figure out how to get around it. So it can't just be the law. It has to be a combination of the law and a broad-based understanding. And again, I think part of the issue that we're seeing, some of the reasons we're seeing, the ethical challenges we're seeing is because people just haven't gone through the rigor of learning what these algorithms are actually doing. And so I think the academic community is part of the problem. We've had this race to get out certificates and this race to get out my tracks and analytics programs and business analytics programs. And you can get a data science degree in six months. And you come in. You're a poet in six months. Take our class, pay us $30,000, and you'll come out with a data science degree. I think this is part of the problem. We've shortcutted the hard work. And so I really feel strongly that in order to produce ethical data scientists, an important part of that has to be the math. We have to go back and teach them the mathematics and the statistics, the basics of how supervised and unsupervised models work, how classification methods work. And they have to understand what's happening inside that box. So that's my answer. UNIDENTIFIED AUDIENCE MEMBER: I was wondering how you felt about people who might voluntarily surrender their privacy for a good cause and if this is always ethical within limits. An example might be someone is taking a new drug and says, all right, just track me through social media. We could probably find out what they're eating, if they have any adverse health effects, and even whether they die. And they just said, no, it's fine by me. Do you think that's OK? Or would you put limits on people voluntarily surrounding the privacy? UNIDENTIFIED CO. REPRESENTATIVE: So I'll answer your question in two ways. So your question is really one of consent. So the question is really, I'm OK with it if people understand ultimately what they're volunteering, which again goes back to that downstream implication. If you're voluntarily putting your data out there and you're willing to sign the documents that say, all of my data is yours, that's fine as long as it's truly an informed consent, so long as you truly understand what it is that you're doing, what it is that you're giving. So there's been more than one conversation that I've had recently where people have talked about this issue of effectively creating a value of your data. And then you can donate that data as a charitable contribution to an organization. And they can use your data and other people's data to make money for whatever the organization is. So like a university-- for example, if alumni come back and say, well, you can use my data in this context and you can sell it to this particular type of organization, and then the funds that come in need to be used for charitable engagements within the university, that will ultimately benefit students. It's an interesting idea. I think this idea of valuing our data is definitely coming down the pike. Gavin Newsom, who's currently the governor of California, in his inaugural speech recently talked about this idea that Facebook and all of the other big tech companies need to pay a data dividend back to the consumers. So for example, Facebook can't exist without your data. And so there is a value that can be calculated if you take Facebook's value and then you divide it by the people that ultimately are on Facebook. That math becomes pretty straightforward. And that becomes the value of your data. So when Gavin Newsom was doing his inaugural speech, he talked about what the value is for Facebook data. He talked about what the value is for Reddit data. He talked about what the value is for Twitter data and then made the point that the consumer should receive some of the economic windfall of their data being used. So it definitely is a point of conversation. And I suspect it's going to continue to be a point of conversation. OK. I think we're almost at time. I'm getting the hi sign. I would be happy to talk to you after I come down. But thank you so much for your time and for your attention. 