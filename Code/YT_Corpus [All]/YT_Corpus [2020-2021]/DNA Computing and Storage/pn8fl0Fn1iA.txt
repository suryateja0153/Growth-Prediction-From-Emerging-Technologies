 All right. Good morning to those of you on Central,  Mountain, and Pacific Coast time and good afternoon to, no it's still good morning to  those of us here on the East Coast. Thank you for joining us today in the NCIC speaker  series. I am Warren Kibbe, the Director of CBIIT and I've got a couple of items to mention. Today's presentation is being recorded and it will be available via the CBIIT website at  cbiit,nci.nih.gov. And you can find out more information on future  speakers on our site and by following us on Twitter, at NCI_NCIP. And today we are very happy to welcome Dr.  Alexander Szalay, cosmologist, and Bloomberg Distinguished Professor of  Astronomy and Computer Science at the John Hopkins University. And he is going to be talking with us about the Fourth Paradigm: How Big Data is  Changing Science. With that I would like to turn over the floor to Dr.  Szalay.  Thank you very much for the invitation. It is a pleasure to be here and.  And actually the microphones are in the ceiling. So if you want to sit, or if you want to stand.  No I think I will kind of move around and  [inaudible]. So it turns out that although my day job is in  cosmology I'm spending more and more time over at the medical school these  days because the astronomy has led me to start  working on big data before it was so fashionable. So we really started to do some pioneering  projects and it is clear that, today's talk is about what we learned anyway from  astronomy that is how it changes the way we do science. So what is generally true is that the data is growing  exponentially in all science and the exponential growth is really  coming from the improved detectors which are all based  on solid space manufacturing. This could be cameras actually they are the same  cameras which are in our telescopes and our satellites that they are in universe or in the general geometric sequence there. And basically the computers are capable they are based on the same technology as we get bigger and bigger. Because of course our computers are also getting  faster.  Excuse me you're going to have to speak a  little bit closer to the mic because I can't hear you well.  Yeah okay so I will try to speak clear. And the so basically we can keep up with the  linear data flow. As the data come up the detector we can  processes it. So typically the data is roughly doubling every year  in science because we are on a flat budget in most of the sciences. But there are areas like genomics where actually  the spending into genomics is increasing so therefore the  doubling rate of data is probably every five or six months. As a result most of the sciences are becoming  increasingly data-driven and I will elaborate a little bit more as to what I  mean by that. And this is all happening very rapidly right in front  of our eyes. There is an additional challenge. So it is not just that we collect data and it goes into  a silo. There is an increasing pressure to have all this  data and make it public. And not just public, but actually accessible and the two are very different things. So data can be public but incredibly cumbersome  to accept it and that makes it really useless. So we have to make is accessible and it's caused  us a lot of non-incremental changes at such a rate, we're  growing at 5% a year or 10% a year we could probably keep up doing  more of the same buying a little bit more computers and a little bit more disk space every year. In the doubling we have to start doing things  differently especially in the analysis of the date. How we actually turned the data into knowledge. We see an additionally interesting thing  happening. So for a long time science was actually breaking into smaller and smaller sub areas. In these days we are seeing through the computing  and statistics to the conversion of physical and life sciences, people are starting to use the same  methodologies. So people who are working in computer science  and brain research are now, a lot of them are looking for computer science and  physics PhD. For example to work on the problems, because they already are familiar with the tools. Some of the simulations in systems biology are  starting are starting from the same first principles as for example some of the larger numerical simulations at the national laboratory and physical science. When we think about data, it is not just a small  number it's a huge data set, you know the petabytes of concern, the large collider. But also we have millions and millions, if not  billions of small spreadsheets, which are generated in the labs and coming off the instruments, which are all residing on little computers,  scattered, disconnected from each other. And there's a huge amount of information on it. But to properly annotate all this data so that it is  usable for others has basically defeated us. So we still don't know how to tackle this problem. In any case use of the data, there is a true  scientific revolution is happening and how discovery takes place and it is happening  right in front of our eyes. And it hasn't happened all that many times since  human history so we live in a very special time. You can't open "The New York Times" these days  without actually seeing something like this in the headlines. "That DNA Sequencing is Caught in Deluge of  Data." So this is one part but look at the target  advertisement above it. So the people who look at this article, there's a huge need for people who are interested  in big data. And so basically I think these agencies are actually  leading the hard sciences these days. So there is kind of this other subtle message  there. So there's also a big data problem there. And so how is science changing? So thousands of years ago science was empirical. We tried to basically capture nature in some  written and graphic forms. So you know the Chinese and the Greeks created  old star charts. And we the Bethlehem star was recorded and that was the first super nova that was  recorded. The turbulent flow was we couldn't do simulation of  hydrodynamic but Leonardo was actually drawing a remarkably  accurate drawing of a turbulent water flowing [inaudible]. And this was the way we did science, at least collected the science in a descriptive way. Until actually Kepler used some of the data  collected by Tycho Brahe to capture the abstract, a simple analytic equation  after the data that describes the motion of the planets, in a particularly elegant and simple form. These are Kepler's Laws. And then this kind of grew to Maxwell's equation and then through the special and general  [inaudible]. So these were the equations that people also  could solve analytically. So we tried to find other big solutions to them. And then there came a time at around the 900  Project where some of these equations were simple to write down, like the Navier-Stokes equation for viscous fluids. But yet even up to today, there is no analytic  solution for those equations, so we try to use computers to solve them. And thus, entered computer sciences. And this was the first such application was done  by [inaudible]. And then today what we see is that there is  computational biology, there's computational physics, computational chemistry so there was a  subdivision of all the sciences during computing. But this was really in many ways becoming  increasingly [inaudible] when people talk about computational science. People advise the Circular Tower of the Cirque. What's happening today is that the computers are  entering kind of every phase of the scientific process, from the instrument to  computer-driven. Computers collect the data they process the data and then they have an inherently [inaudible] and so here we call this e-science, or data  intensive science. Which is actually very synthetic in its nature. So it synthesizes theory, experiment, and  computations with statistics and interestingly as much as for example this  required people to understand mathematics to solve the equations  in an actual way, we couldn't go to a mathematician every step  along the way when they were trying to solve [inaudible] equations. And here also scientists learned use the  computers [inaudible] computer sciences. So here I think scientists will also have to learn and become native in this area. And note the accelerated base. So it took thousands of years to go through this  phase of science. It took hundreds of years to go through this. It took a few a few decades and now this is  happening basically over the last five years. And the human genome project and everything are  starting to put out so much data that we really don't know what to do with it. Data is everywhere. And the doubling really means that if we hope that we can actually collect all the data and all the  scientific data in the world it's not possible. So if the doubling means that over the last 12  months we collected every scientific data from the beginning of science. So that is a lot and if you wanted to move all this  data into single locations in the meantime there will be new projects  springing up which are by the time we move the data they are just  generating as much as we are trying to collect. There is another problem. So I actually was mistaken a little when I said that the computers can't keep up with the data flow. They can keep up with the data flow as long as we are processing everything through a linear  pipeline. But if we want to make sense of the data, we're going to for example compare to cluster  analysis on it. The most brute force operating systems  algorithms for cluster analysis require I take every data item in  my bucket compare it to another, every other data item, and if they match I put them into some separate pile. This is how I build clusters. This scale says the number of data I accept. So the cost of this comparison. So if my data is doubling every year, my computer speeds are doubling every year, but the cost of my algorithm is actually n square. Next year I will I'm already the cost of my competition next year will be four times as  much but my computer s are only twice as fast. So basically these operators will shortly be untainable, but in many of the traditional statistical operators if  I have n data points and a big n square covariance matrix, inverting this covariance metrics is an [inaudible]  operation. And this is the traditional sort of minimal variance  estimator. I have to do this. But it turns out that the nature of errors is when we have very large data that is changing. When we have hundred data points the statistical  errors sort of driving problem, we really needed to do everything to minimize the  covariance. When we have billions of data points, it's a  systematic errors in our instrument that are driving the problem. So it makes no sense a that point to try to solve  the minimum variance problem, which was optimally the old word but it was  designed to involve data. They need to devise new statistics. Therefore for example the cost of computing is becoming the driving force in the analysis. So we need to devise operators where the  accuracy of the operator changes as we compute more and more. And then we can decide when to trade up the  computer time versus accuracy. And or when do we get the system together all our  instruments because it makes no sense to compute any further because we do not gain anything. So it's very different from Google searching. So it's always you know, bothered me, so okay so  what are we doing at University as [inaudible] when Google is spending millions of dollars on big data analytics. But what we do is fundamentally different. So in solving the problem they are looking for  causal connections. And we do exploratory searches that the cost to devise that trying to look for possible correlations in the data but the correlation number replace the causal  factor in the connections. But for Google it is okay to stop at the correlation if they see something correlates they post an ad  and they make money. But we want to find a cure for cancer or the causal  reasons for the cancer in the genome. We cannot stop there. If we see something as a possible hypotheses. We have to devise other experiments and this is  what scientists do. We tried to come up with clever other [inaudible]  experiments which really than do the confirmatory, collect confirmatory  evidence. So this is very different so this is why we can still  compete in this world and continue doing science and we cannot just  hand it over to Google. Our architecture that we used for computing are  increasingly inadequate to handle big data problems. Because the CPUs are still a multicourse, they're getting faster and faster. But the hard disks are not changing basics very  much in technology. So they are is still the same mechanical devices. And so the [inaudible]. Most of the scientific data houses today still done  in small to midsize clusters, typically in broom closets. Done out of faculty startups because very often it is  difficult to actually, so you know the [inaudible] have to spend the time  trying to buy computing elements, we have to write an MRI. Generally, the faculty don't understand the problem  they want to be cheap spend all the money on the computers  put into the broom closets, which overheat. The graduate students installed the software, it  gets infected with viruses. The second day [inaudible]. So by the time the machine becomes operational  it already amortized [inaudible] or half its value really. So it's a system that is not very efficient. They keep buying more computers and the  managers two years ago burn down the transformer in the physics building or  next to the physics building. So there was a surge and they were operating so much against the limit that it melted down. We were operating for about two months on the  generators brought in from Virginia on tractor trailers. They run cables as thick as my arm and to the  building and I touched them and they were hot. So then I started to appreciate that you know this  is it, we cannot put more computing equipment into our buildings. And of course there was also a furnace building  taking 250 kW to melt and build high temperatures for conductors which  was operating at the same time. All these things basically led to the meltdown. But the bottom line is that we need to start to think  creatively and organize things differently come up with new operatives and we just cannot throw. So generally, scientists try to throw brute force into these computer problems for a long time. We have to do something differently. So I had the privilege to work with Jim Gray from  Microsoft. He was a Turing prizewinner computer scientist who invented transaction processing. So every year the admission is running Jim's algorithems and he wrote prominent books on this. I met him in the mid-90s and I worked with him all  the way until he passed away. And we tried to formulate after he died, you know  some of his ideas and terms of laws like Moore's Law. And so one of his the first time when we work  together came out that scientific computing is really revolving  increasingly around data. And it's not about supercomputing any longer. And this is clear you know [inaudible] having the supercomputers and so on. So this is the other point is that we need a gal out  solution for analysis. This means that we should not try to build a  superduper supercomputer but we need to actually build it out of very cheap  elements. Jim wrote a paper and again we organized this. That he invites us to work where we will build our systems out of cyber  bricks. So he said that we will have to have disk drives to  store the data but the controller system, the disk drives will be so  powerful that data will be able to do all the processing of the  discharge. They are actually there. So essentially the same ARM processors which  are in our cell phones are the disk controllers of the newest solar disk drives. So we actually can do that. And this is precisely what Google, Microsoft,  Facebook and everybody is doing. Buying cheap computers and just buying millions  of them. And this is how they build up their system. When the data is too large into and movable  taking the analysis to the data instead of taking the data to the  analysis, which is what everyone was doing in the 90s. We had this big computer centers and basically  we took our data sets there, because they were small, we did the computing  we [inaudible]. You can't do this anymore. You have to actually build the computing right on  top of the data set. So that you can use very short path networking  very, very high speed networking. Yet the one way to communicate with scientists so  instead of writing 100 page requirements document. Just give me your 20 questions you want to ask  from the data set. Okay, so he did this when I met him the first time I  started to work on an astronomy project. And so I said ha okay this is ridiculous. I'm a scientist, I want to ask everything and anything while you bother with 20 questions. Then Jim smiled and said, "Okay, so just give me  20 questions." I said okay. The first five was easy. I did that in the first five minutes. The second five took an hour and a half because it had to be different and then I said okay I have to go home and think  more. So he taught me humility in two hours [laughter]. And then I sent out an email to our astronomy  collaboration that okay what other questions can you think of a  very little came back. So then, in the end, I made up another 10. And then over roughly 15 years that have gone  sense. We found another 15 patterns actually that people  use so they were not that many. And it turns out that they are also not that frequent. So the first five are really those patterns that we  use that essentially every astronomers is using every  day on their data system. But this is huge. So it looks so simple, but it is really important to  teach the scientists and the users to actually what our priorities. What their own priorities are. And the last thing this is always going from  working to working. So we use to design systems in such a way that  the [inaudible] sat down and design basically a requirements document,  before, it took several years. And then it took several years to implement. And then we built it, [inaudible], And the whole work was different in computing. So basically computing operates down at 2 to 3  year back scale. Every two or three years that they new computational paradigm we went to the computational grid. We went to mid services, left services. Now everything is in the cloud. What is guaranteed that in another three there will  be something else coming up. And so whatever we do today, we will have to  adopt and change in three years. That is almost guaranteed. Anyway, so there was a book that we collected  after Jim was lost at sea. That to each of his collaborators wrote a chapter of  this book and it's available from Microsoft, so it's  downloadable. And so, "Fourth Paradigm" this is the new  paradigm of science. Okay so what are the challenges? So these are kind of the typical phases of the  scientific analysis, data collection, discovery and analysis, and  publishing. And what is changing here in the data collection, we have exponential data growth on here we have  New World in this paradigm we have to have data federation,  not just single data sets, but we try to take multiple data sets. We are moving the data and we also have a new  publishing paradigm because we can't control the data into journals. Traditionally, this is the way it went. So the scientist collected the data, did the  discovery, analyzed the data, and then published it. Okay today what is happening in many of the big  projects like, [inaudible] or the human genome project, that you  collect the data and you essentially have to go through the data  scrubbing and cleaning and publishing this before you can even start the  analysis. So we are actually reverting these three  fundamental steps of science is actually done now in a reverted order. And this is very important. So we have also this non-incremental changes. So we need to come up with new computational and statistical tools and strategies. So this incremental algorithm where we basically  can trade up computer transverses accuracy. We need to, in order to make the that meaningful,  we need statisticians, we need computer scientists because it has to be  computable. We need the domain scientist, the genome  experts and astronomers. And the other thing is that this is where I usually get a lot of flak for this sentence. But and it should be, this is short and polarized. It should be kind of intention is to promote  discussion. But what I mean here is that essentially in a lot of  sciences, in particularly in life sciences a lots of a priority  hypothesis, which were sometimes derived from theory. And what is happening that so they are still  hypothesis driven but increasingly, these hypotheses are derived from correlations. So essentially we create a lot of correlations from  data than the human mind and the scientist with his expertise, looks at the  context and tries to weed out basically the irrelevant and multiple and looks  at the things that actually make some sense, in course of way. But this is how we generate basically another set,  a new set of hypotheses. And so this is I think what is coming out of the  analysis of this big dataset. In many ways. I don't know if you know what this is. This is the first microscope right it was built by Zacharias Janssen. Okay and so he built a microscope and basically a  whole new world opened up. And we are at this roughly at this point where we don't quite have the microscope but we  are trying to build one. And when we do it a whole new world will open up  in this world of data, that all those patterns will emerge in science that  we don't quite yet see. They are kind of scraping the surface. So why is astronomy interesting or in a sense, so what I found that the astronomy community was  remarkably fast to adapt with this new way of thinking. And why. So it turns out astronomy has always  been data-driven. So when the Bethlehem Star went up, and so the  [inaudible] the sky, we really didn't have a hypotheses that this is the  end of stellar collapse, we didn't know even what those objects in the sky are. And it took 1000 years to figure out. They find basically observations deriving new  hypotheses doing a new observation, combining this with new nuclear  physics until [inaudible]. And when this telescope there was another  supernova found in the space [inaudible] data. Then it was a detection, and it took Evan Raetz  [assumed spelling], my colleague in the Department of physics at  Hopkins who actually made the connection that such a bright supernova at such a distance means the universe is actually  axillary and it must be filled with dark energy, a very [inaudible] in which one supernova crashed. So then it is a difference between detection and  discovery. And it's the human scientists who actually  machines can do the detection, but it's instruments will make the discovery. So this is Jim Gray giving a talk at Microsoft, at the Microsoft workshop about that the  astronomy data set that we are working is really big [laughter]. Jim was a senior guy at Microsoft. So he reported there to Bill Gates. So he could do whatever he wanted. And he lived in San Francisco and South Seattle. But every year he still had to do a yearly  performance evaluation with Bill. And Bill asked him why are you spending for years  now, half your time working with an astronomer? And Jim said I love working with astronomers  because their data is worthless. It is so exciting because we don't need [inaudible] and we don't need the lawyers. There are no privacy issues with astronomy data  so we can play. And we can try out new ideas how to analyze the  data. So basically, there was no such thing that I think a lot of health data on medical data, it kind of makes it particularly cumbersome to  experiment with. Anyway, so Bill asked Jim and Jim was allowed to  work on this. So I got into this through astronomy, so we started in 1992 a project called the Sloan  Digital Sky Survey. Which started pretty much at the same time as the cosmic genome or human genome project. At the [inaudible] library there was a group at  Hopkins that at that point was running the Hopkins part of it. So we talked quite a lot at that time about their experiences. In any case, it started in 1992, Princeton, Chicago,  Hopkins, University of Washington, Institute for Advanced  Studies got together to the technology was just arrived to build very large [inaudible] a specific camera. And they also had the computer power to actually  do it. It was supposed to. The whole system construction was supposed to  finish by 94, 95 they were going to run for five years and  finished by 2000. As it turns out we did not even start taking data  until 2001, 2000. So there was a huge underestimation. The project was projected to cost $25 million. Roughly 10 million for the telescope, 10 for the  instrument, 3 for the mountainside and 2 million for the  software. So today I would say ha. So in the end it was a little over 100 billion it  finished in 2008, and everything so the telescope, the instrument, and the software and the computers to run the  software was about $30 million. So this was the biggest mistake we made we  grossly underestimated the software implication. We thought that we could just recycle existing substance. And on such a scale where it turned out to be  many, many terabytes. You have to have such level of such robustness  and structure in the software that was just not us. None of the astronomy software that was up to  standards. I looked at it recently we had to write more than 1  million lines of code. Which is small compared to Windows but for an astronomy project, it's very large. It was going to take about 2 1/2 billion pictures of  images. In the end we took actually 5 billion. And all data was supposed to be 10 terabytes. Now I have to crate actually a safe set of all the  data that we collected and process it with well more than 100 terabytes. And we projected the database to be half a  terabyte. It is now 35 terabytes. Had we stayed on time, none of this would've been  possible because computing and disk drivers have actually fallen by factors of  two every year, so we could become much more ambitious about  the whole thing in the end. So we built, with Jim we built a database and on  top of the it, that interface, which in many ways, was quite novel. And it was heavily influenced by David Litman at  NCBI. So Jim and I went and spent a week at an NCBI  and talked a lot to David. And he had a lot of really good insight. First of all through Google forms, he showed us a short paper which since then I still haven't been able to find. It was a letter to the editor from someone very  famous in molecular biology who said that if you build a form-based interface of  the data built by a video programmer, then your the best scientists are still limited by the imagination of that programmer. So you have to have a back door that the  scientists, the clever clever young kids can go in and do  whatever they want with the data. And then, so this was a part of this. In any case, so in 12 years we have 1.2 billion web  hits, 200 million external SQL queries that people went  into through backdoor. We have 5000 reference papers and 200,000  citations. All from a community of 15,000 people. Through the community, only 15,000 roughly a little  bit over. But we have 4 million distinct IP addresses then of  the user. So they are hitting. A lot of other people than just the professionals we  are hitting, [inaudible] and scientists. And they keep coming back. I expected the usage by model distribution, so the short [inaudible] is for the outside people and the long one is for the astronomers. It is a power roll all the way around. [Inaudible] to look at the usage data that recorded  every mouse click since 2001, we have I think a trillion and a half  terabyte of data just through the logs. And it's a works most users around the facility today, more than Hubble and any of the satellite data, or  the [inaudible] telescope. And they also built a collaborative server site  around this point, that is used by half the astronomy community  everyday. They can actually collaborate, store in intermediate  results and share data with each other. And this is just showing citations, [inaudible]. This was an interesting spinoff. So summer of 2007, a post doc [inaudible] had the  idea to look at the bindingness of galaxies whether they are  clockwise or anticlockwise, repeating on the sky. But we have 500 million image collected in the  database, just too much. And the software is not very good in doing this. So we posted the site where we asked the public  to actually make a decision, and this was called Galaxy Zoo. Brian [inaudible] from Queens who actually was an astronomer who quit half the year before his PhD when  [inaudible] became big in the 70s. He finished his PhD that year, at the age of 60. And then he helped us to publicize this site and also was the designer. And so in two days we had 300,000 people  signing up. And this is now becoming really big so the Galaxy  Zoo is now Zooniverse. So that our, science where people transcribe [inaudible] fragments from plato and so on that they mostly are [inaudible]. They are transcribing the ship logs from the British  First World War, British warships where they had to measure the [inaudible] pressure every four  hours. And they are reconstructing the temperature of the  ocean around the First World War, out of this. Anyway, but what is interesting, that in one of the  images there was this splat which is like a splat on the windscreen. And the telescope software classified, the  [inaudible] software classified it as scattered life in the telescope. And the Dutch science teacher, middle school  science teacher wrote a blog about those this that, "This has a little bit more  structure. I don't think this is actually scattered light, I think it  is rea." It turns out that this is one of the most unique  objects in the universe. And it happens, the Sloan has this different vented filters in red and infrared. And where the two filters overlap, there is a little depth between the two filters. And all this light comes out in a very narrow  imaging line which happened to full fall exactly into our blind spots. So it appeared to be very big on the Sloan  images. Anyway since then this object has been observed  by space telescope, with a very large radio telescope array, that was featured in the contact by ultraviolet  satellite. So and he was the first author in all those papers. And set of computer gamers have discovered a  new class of [inaudible] that they named [inaudible], because they're very  compact but still with a very active star formation. So people are making original scientist  discoveries. People who are not scientists by profession, but they actually have a science background. The data is made available. We took down the [inaudible], the whole strings  that we build for the Sloan database and we use it now for  radiation oncology. This is actually turning into the start of a company  with Toshiba at Hopkins. This is an environmental science. We are putting we are already having an  operational database for the thousand genomes or a part of the  thousand genomes with in order part search engine. These are computer simulations. This is Galazy Zoo. We are storing server simulations. This is the big infrared survey in the United  Kingdom. This is the Hubble space telescope. So you can see that it's kind of spreading  everywhere. And of course, this all started from something  called the Terra server. This was Jim Gray's test project, so this is the  reason why we met. Jim was building essentially a set of a cluster of  servers which were storing images from the USGS and the Russian spy satellite for Microsoft. This was 90 to 95. And Microsoft didn't want to fund it, so they said  okay, who the hell wants to look at their own backyard. So today we know much better. What Jim basically saw this as a torture test for Microsoft fostering technology. And this is actually the beginnings of our  database. So we started out with the Terra server, modified the script and now [inaudible]. So this is basically the OncoSpace. There is [inaudible] some of these principles, data analysis of the data was started with the  [inaudible], and do all the data manipulations inside the  database, which has GPUs in it. And it is now running at several universities as a  test. Okay so this was done by Todd McNutt and John  Wong. So when we go to hypercomputer simulations of the traditional supercomputer simulations that's  the whole New World. They are starting to put out so much data that they  can't deal with either. And so this creates a whole bunch of challenges. How do we interact with this data set. I don't want to dwell too much on it just to show a  few statements where a clever idea can actually change the game. So this is the Navier-Stokes equation, [inaudible]  said this, "That this is the last unsolved problem of classical  physics." And a friend of mine was doing the simulation at  Hopkins and a student was running a large simulation  where they were putting in a bunch of test particles on top of the simulation which  were moving with the fluid, but as the turbulent was chaotic and particles  never added where they were supposed to. So he wanted to put a bunch of particles around a  vortex tube, that is set tornado that is forming and figure out where they came from. And it was sort of the same side of the Sloan  database at the time so Charles asked me after a business week, could we actually store most of the time sets in the  database and then just run the particles inside the  database? I said sure no problem. And this was also the time when the movie  "Twister" was about, where guys were driving up against a tornado and  shooting and [inaudible] into the tornadoes. And basically this is what they do. So take your laptop shoot 1000 test particles into  the tornado in the database and these are [inaudible] data stream and move  with the flow. Essentially, you don't have to download the  simulation files. You don't even know whether it is 20 terabytes or  20 petabytes behind it. You basically get the results back. Anyway, so this is a new paradigm for all simulations. But in a few years we delivered now 12 trillion  sensors to the community and people have written several hundred papers  from Caltech to Princeton to Italy to France on this data set. Because they can do experiments with the laptop. They don't need a super computer to do it. And same thing happening in cosmology. So people so there is a simulation and a frame of mind Gerard Lemson built basically  a database of the cosmology simulations which simulates the  Sloan database. You can observe the simulation the same way as if  it were the real sky. And it became immensely popular. And the fourth year and the people in all this  turbulence in the simulation databases people are using the  database as a musical instrument. And they're making beautiful music. Much they do things that we never imagined that  would be possible with that. So they've come up with incredibly creative ways. So once you hand them an instrument. They learn how to play it. And so this is the common kind of fact. So what do we have to do to it. So we are now taking the lessons learned from all  these exercises. So basically how can we do scalable data  intensive analysis. We still have to have the data on hard disk. But they are becoming like safe drives. Basically, using the desk occurs really fast. As soon as I have to move the disc, then data  becomes very slow. And so basically everything becomes a streaming  problem. We have to handle data frames very very fast. So we have to formulate our problems. Essentially processing data streams on-the-fly because the data will always be larger than the physical memory we can store them. And how much does this cost question. I found this figure actually and 2009, but by and  large, this is still true. This is from a company that sells storage space and they also publish server update design. They have the design online and you can order a  had the manufacturing company built this for you. So they said the worldwide for a petabyte costs about $81,000 today, it's about 60. So it hasn't actually the prices haven't changed all that much sense the [inaudible]. So if you actually put it all into proxies and servers  and motherboards, you can build a petabyte for 117,000. Today my bath number is about 80,000. If you order from Dell it was about 800,000. It maybe, came down a little bit to 600 today but it  is still a huge gap there. Sun was selling a server for about $1 million Oracle bought them up to 1.5 million [laughter]. Okay, so in that if you actually put it into the cloud. It is attractive, but when you add up the storage  fund of Amazon or Microsoft it is about $2.8 million to store a  terabyte in the cloud. Anyway, so this is the big problem with the cloud. So the result is that we are still building our own. And this is something I built with a grant to build a system to demonstrate that we can build a streaming data  streaming engine. So the purpose of this was not to build a general- purpose computer but build something with lots and lots of storage 6  1/2 petabytes for about 800K that has an enormous data bandwidth that can  exchange the data from the disk to parallel and we can solve streaming problem. In the idea is that people can write a one-page  proposal and then bring 100 terabyte data sets to hear and then eat essentially analyze it for a few months and then take the results home. So for example, one of Carol Griders' [assumed  spelling] students is now analyzing about 300 terabytes of TCGA data on I think for these machines. And I get a phone call about six months ago from someone at the [inaudible] Center. That's okay. So I have this 120 disk just illumina. The cost is $10 million today. They are 1,000 genomes of asthma patients of African-American heritage. And I think that this would be really useful beyond  this. But what do I do with it. I said okay drive over and let's copy it over so that  you have another copy. He said I don't dare to drive what happens if my  car gets hit along the way. It is 5 miles from the medical school to our lab. And so we ended up because that's the only copy  of the data so we ended up running an optical fiber in the end, from the  medical school to her office and then copied over the data. But it was several over 300 terabytes. Another person calls me that she has about 1000 genomes, again related to pancreatic cancer. So within Hopkins there are people individuals, I would say probably at least five individuals, and each of them have about 1000 genomes  today. It's more than a terabyte. And they don't know what to do with it. Not even to the point that okay how does it get  from this pile of this on my desk to actually something that is usable. Not to mention that illumini actually analyze or align those thousand genomes in three different versions of the software so that  the data is unusable. It has to be basically recalls realigned and  recalled. Okay anyway. Long tail and I think to the end. So this is the other big problem in particularly large  sciences, and so the integral of the long tail is big so all the data and all these little parts, which are basically [inaudible] scattered  everywhere. It is really quite large. At the same time all at times of forcing people to  publish their data with a few exceptions like gene bank and protein  bank. Those actually work pretty well. Have been a large mainly a failure because we  force people, the creators of the small data sets to [inaudible] complicated data creation form. And after two attempts they basically refuse to do  so. Because there is no value proposition in at for  them. So what do I get out of it. And so there are a couple of lessons in the outside  world. One is Facebook. So where Facebook started and bring seemingly  unrelated useless data onward to a server about [inaudible] and essentially  Google had saw that this would take off but once all this data was in  a single place so that you can look for correlations and patterns and they grew to a critical size. Suddenly [inaudible] happened and the question is what is the science equivalent? So we would do something. The other lesson is DropBox. Where DropBox interface was so simple and in terms very Apple like so that this was a very crisp design, with absolutely  minimal interface. You just clicked on it, installed and it worked. And you didn't even have to think about it. And generally when people create very converse  of interface, then scientists aren't in charge of software project. I've seen so many times where they do something  very complicated, they start to do everything for everybody. And the result is it does nothing for nobody. So we wanted to do some kind of experiment, and we got some money from the Sloan  foundation. So basically set up a storage system which is full  tolerant and put it in a DropBox like interface. A share and drop interface into it. And offer them free storage for acts years. But more than DropBox offers, though more than a  few gigabytes so that it's able to take some reasonable size data sets, but  require no metadata but we require basically the basically the  username we require an email address. Today there are so many obstruction papers  published on the archive that I can go to, if I know the email address in the  proper author ID of the people I can go to the archive and find out  what the papers are and what are the tables in the papers and give a very good description of the data. I can look at the social network, [inaudible]. So in a sense where it is very difficult to write, to define an ontology for a whole field, it's much easier to find a limited ontology for a  small number of a people in a automated specian. And so we hope that we can actually grow to a critical size might actually things start to click  so it's an experiment. Okay working obviously genomic at Hopkins I see  strong parallels between genomics today and what astronomy was  20 years ago. So today it is all based on files and pretty much  manually run scripts. Where a lot of the information is just encoded in  the long file and there maybe a couple of other lines in data. Everybody is obsessed with running their aligners. This was again people and astronomy used to have their own favorite benches on the sky. They were largely used the same two or three  software but everybody was using different parameters, so you couldn't compare anything. And this worked with a small number of genomes, but it will break when we have thousands or  hundreds of thousands or 100,000, like the British sequencing project. Then we want to do studies to create a search for  patterns to database. So what we learned in Sloan so first of all, nobody believes that we are going to [inaudible]. Basically there are these 4 or 5 rich public, or  private universities. They got all this money they take this data and they  will run. They will not release it. Until this point our NSF acceptance rate was 1/6  of that of the field. So we were punished for doing the project. After that when we put the data out people didn't  believe that the data was good enough. So they tried to actually take our raw data and tried  to use a analyze that process it with their bandwidth tools and they  wanted to prove that they can do better. They found out in the end that they couldn't. It was good enough. And in the meantime, a lot of other people were  turning up papers so they started using the date and what and I think  this will happen and genomics as well that once people agree on a  common level of practices that it's good enough in the data is comparable  suddenly people were able to share and use it efficiently. But I think this is I don't think this [laughter] will be  spinal tapped, we don't have to go to 11, it's okay to 9, so we don't have to go to 11. So there is a center and interplay between  technology sociology and economics. The technology is changing very rapidly. So we have Moore's Law, and basically this  exponential group of technology and cell phones and so on. And the trend is given by the changing generation  of technology through our cell phones or digital cameras. Sociology is changing and very unpredictable  ways. Why is it that YouTube took off and MySpace didn't  when there was a time when the two were almost directly comparable. But there was kind of a little bit of an advantage to  YouTube and it turned into a runaway and exponential runaway basically  peer pressure. So it's work's thinking about when, so the  statement that build it and they come doesn't hold any longer. In general, people will not necessarily use the new  technology only if you offer something entirely new and or  substantially cheaper and or substantially simpler and all of our friends are  using it. So the peer pressure is becoming an increasingly  large advantage. And the third part is economics and the funding is  essentially level if not decreasing again as we all know very well. So we have to these things are sort of creating a  very interesting interplay. This is just a quick one slide overview at Hopkins. So we have basically physical science and a  whole bunch of different projects. These are in life sciences. Here we have the data conservers and the  libraries. And we just got a bigger ground to take the sky server and its own archive and more formally turning into a set of reusable building block that the data incrypted is a building block. Hopkins actually put a fairly large investment into  the so-called discovery and safe grounds and George Hopkins actually  very helpful in being one of the referees [inaudible]. And then they also are increasing the building of  the hardware. So this is a new spin from that we are building in College Park of at $13 million computational facility that will have all sorts of 22 terabytes of storage. So the last, but one slide. So what we see is basically a whole bunch of  trends. It's very hard to predict the future. Okay so everything is changing so fast. But we see this conversion of physical and life  sciences. So data collection is happening in ever larger  collaboration, think again of the human genome project. And then that data is made public and analyzed as  much by individuals or very small groups. And this will be the way to be big science by  individuals in 2020. We see the emergence of the [inaudible]  sciences. There are generally very sophisticated people and  we will see even more when the baby boomer generation retires. we have lots of interacting data that nobody has ever bucked out. I think a lot of people will actually do interesting  research from home simply because they love it. Or they can act as tutors to younger kids. But we need to formally start training the next  generation. Today we are training people who are I shaped. When I mean by I shaped is they are extremely  deep and narrow. This is our whole higher education system is  focused on this concept. And as we get older and older. We actually tend to build a bar on top which means  that we gain kind broad but shallow knowledge but in this new world we need people who are much more shaped like a greek letter Pi So again , this is a transition area. So this happens when every physicist have to learn mathematics now basically I think  everyone in science will have to learn a little bit of data science more than amature. So this is what I mean that they have to have  [inaudible]. And the whole core of this will be once this becomes a part of the general education and a sense we don't even call it separate. This will be part of just knowledge in general. But we need to in order to do this, we need an early involvement in computational  thinking. When you get your PhD and get at your job. It is too late. There is nothing [inaudible] basically. So, summary science is increasingly driven by  data is a change in sociology. So probably the right way to call this really the  data-driven science is really more like data-driven discovery. So that the discoveries are increasingly driven by  data. We need new instruments so we need to build the  equivalent of [inaudible] . So which you shouldn't think about it as new  computing systems. These other microscopes and telescopes of their  time. today and it should be able to opporate. There is a major challenge on the long tail that we still haven't been able to tackle. We have this new course science emerging and I  am proud to say that the Sloan survey has really been at the top of  this transition, but it now is it's [inaudible]. And I thought it would be appropriate to finish with  a Henry Ford quote. "If I has asked people what they wanted, they  would have said faster horses." We cannot just buy faster computers anymore. We have to change our ways. That's it. [ Applause ]  So thank you very much Dr. Szalay. I'd like to make. Just an observation that I think you point out a lot of  wonderful things that certainly I think all of us have been feeling that are doing cancer research. Something maybe that is directly relevant to some of what we do have geonomic data column we have been building simply focus right now is to really opening that up to the whole community and letting people  deposit data next to it. And something that you brought up as one piece  of that is that having this data pipeline where everything gets annotated in a good enough  way. So we'll see if it really is good enough. So this time I really like to open up the floor for  questions. So if you have questions at the great time to ask. , Thank you for a fantastic talk so looking at your  success stories and some of those more challenging projects that  you were talking about. It seems like the common theme is building a user  centered interface board infrastructure. That was part of the success. Who do you think should do that should be project  based like yours or funding agencies or universities?  Says building one central node has the danger of having a single point of area. So if you have [inaudible] and a couple of nodes, both geographic the data that works out. So this is good to have some sort of a redundant  system. Even then, system design to sell that power don't create the tower of Babble, but to have like probably start off with a  few alternate system and eventually narrow it down. Even if you build it as agency specific. Then I was still not limited just to a single place. I would have at least three heads in the system so  that you can be aware or creation of their laptops but I would not hesitate against computer science. So needs to be driven by science and by people who actually understand the science so that it is  capable of supporting the new discoveries and audible.  So I didn't add for folks on WebEx that they have questions they should be raising  their hand and WebEx. I think we have enough time for maybe one more  question.  [inaudible] last year you indicated you worked with emergency service department and the  imaging aspects of the correlation of the last year?  Good so lots of the focus of that group has  recently gone into working with Toshiba so it has been invented. So it has kind of hunted the phase where they are  really very self-supported. So for example I went to some of the early  meetings with them, and that it was clear that they are well on their way. So in a sense, yeah. So it is going very fast. So Toshiba is now [inaudible]. They had a couple of [inaudible]. So it's taking off and this is kind of the idea. I was calling this disruptive assistance so that we  have that group where we work very intensive for six months and then a little bit less intensive for two years and  now they're on their own and they are running it and they are doing things in  a very different way. Now we are trying to work with genome.  We need to close the official session. thank you very much for coming and thank everyone for attending the  lesson today. And we will talk with you. I will meet with everyone. The next time we have a next speaker set. Thank you [applause].  Thank you. 