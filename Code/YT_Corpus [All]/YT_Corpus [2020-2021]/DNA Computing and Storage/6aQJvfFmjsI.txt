 I win last pet care I'm Kobe here with George and we're data engineers for the global pet care data and analytics team within Mars so Mars petcare is made up of about 50 different brands 85,000 associates hitting over 55 countries and really we just cover a wide range of pet care needs so our talk today and we're just going to go a bit through what our data platform looks like and some of the reasons that we've chosen to go for a data bricks and Sparky tail solution and date Delta Lake has both an internal tool alongside that as well as our output our analysts so our platform we are building out a delta data Lake which is like a global Lake with all of the brand's pet care data or put into this one place so then you've got a consistent format you've got a single location for all of these data assets and that gives you that one source of truth which all the analytics teams across all of these different brands can tap into and use together and that also means you've got shared documentation on all of these data assets and collaboration across teams that just wasn't possible before when it was all managed individually by the brands some of the brands that we've got we've got a series of vets like Banfield blue pearl BCA and they're really fun because we get a lot of medical data you get medical notes diagnosis things like that so you really get a view on the health of the pets and the things that come out of checkups and then we've got a lot of nutrition brands pedigree Royal Canin Iams and so a lot of the time we can tell kind of what foods they're eating and you can combine some of the information from them together to then see how food is affecting their health and we've got some pet tech companies like whistle you do GPS trackers for dogs and wisdom panel who do genetic sampling and then Center like Wilson which is a pet science center and they're really heavily into the research around pet health and how to improve pet lives so the landscape currently is our sources come from all of these different business units who operate as their own businesses so they've got their own source systems their own datasets their own models and that's all managed by them using whatever tools they have chosen and set up with which means there's a really varied approach to data collection and the structure that they store that data in in this massive diversity and data quality data format is quite difficult if you want to swap between them because you have to then learn about all of these systems and have access to tap into them so that's kind of the source that we're dealing with so our ETL tool takes all of that and then standardized at certain fields we have the same date format we provide the data all in the exact same way for users on the analytics platform so they don't have to deal with all of the disparity kind of from these systems and then on top of that we will say optimize these tables for analytical user output rather than the kind of more operational format that the data's then great so let's dive into a little bit about our solution how we built this framework using data BRICS and spark so just give you some context to talk about our text axe so Mars have chosen to use as your so that's the platform we're on this is great we've got access to a lot of tools and obviously this plays very nicely data bricks so the typical tool for for general ETL on on Azure as as some of you probably know is ADF data factory this is great this is a great tool it's got a lot of features straight out of the box and this is what we were originally using in our team as well what we found over time and linking back to the to the slide that Kirby was just talking about was that since this data is controlled by the separate business units within Mars and and we we kind of have to adhere to their schemas and Eskimo changes their export capabilities whether that's fully up to scratch or not that we were adding a lot of custom logic in via data breaks notebooks into those ABF pipelines in order to handle these error cases and these changes and schema revolutions and everything like that so we got to a point where we just look to what our pipelining system was like and decided that we could just build this from the ground up and have a much more scalable solution using using the powerful tool we had at our disposal which was spark and a spritz so then moving into their breaks using that as our basis we then had this ability to directly connect to all sorts of systems because of the built-in JDBC connectors and and libraries that are packaged as part of runtime and on top of that we have access to the entire open source library of - using mainly PI spark as our as our choice of language we could access any open source Python library to kind of do some more clever stuff around automated passing of unstructured data like CSVs or Jason's it's also infinitely infinitely in inverted commas scalable you know we can scale out with those clusters as wide as we want depending on how much data we need to push through this push through the system and then finally we really integrated Delta Lake you know in our framework both as the metadata config layer for the frit for the pipeline itself but also as our output layer that we give to our analysts for running models and analyzing and producing output from the data so here's a sort of high-level diagram of what our ETL framework looks like we it's called kind so if I use that name that's what I'm referring to this is a typical run of a single flow of a single table through kites so at the beginning we have the source taking up the data using these connection templates the way that we approach this is with sort of digit up connectors that we can then reuse so rather than a separate pipeline or specific code to connect to a specific data source we have a connector for something like Azure blob storage or a connector for you know database systems and then we can reuse the back connector with the metadata coming from those Delta Lake configs in order to pick up the data from the right place without having a bunch of excess rear it repurposed code so those Delta Lake as metadata configs are really useful then because we can drive that entire pipeline using those as a source and not have to hard code any of those values our values into the actual pipeline itself and then beyond that connection template step the rest of the pipeline is then identical and it doesn't matter if we picked up a flat CSV or if we picked up a table structure table from a database that the ETL framework within spark that we've built is then identical for the rest of the pipeline so that's what it looks like for a single table but sometimes it may look a bit more like this right so as I was mentioning about scalability we have the ability to massively paralyze this and we're paralyzing this at both the kind of spark level with the data transformations and you know the partitioning of the data as we go using spark but we're also paralyzing it at another level on the driver in order to run things run separate sources at the same time and run separate tables at the same time as well so how do zhing spark a database gives us that ability to make it as small or as big as we want so now that we've got that framework and we we've built that sort of framework in data bricks and in spark we can then go bigger than that and we can build on top of that so using the data bricks API we've managed to kind of make make things a lot more user-friendly and the first is this this video that's playing on the screen here which is our custom dashboard for our each framework this is called control it's it's kind of a place for monitoring creating jobs checking the status of our data in the lake and all of this is is completely hinged on using the extensive data breaks API they library if anyone hasn't used it before it's great you can do just about anything you can do in the data breaks UI and on the web program probably even more I would say and it can enables things like this where you can hook in to set runs off and receive data and view the config tables again so it can it can allow us to build this this kind of tool and another really useful thing about we use the database API for is our testing so being in Azure house we're using as your DevOps for our general source control and deployments and we can tie in our testing of of our data Briggs pipeline directly into that because we can use the data breaks API and so by you know specifying what size clusters we need for certain tests because we can know that in advance and know what size they need to run on we can build these tests into automatic continuous integration pipelines as well as deployment pipelines to just go off and cool these runs and return a result directly into that so then moving into a kind of specific part of the pipeline just to understand how it works a little bit and a little bit so date Delta lake itself and parkade which it's built on have a schema revolution capability but we've kind of added on top of that a way where we configure it up so we have complete understanding of what schema revolutions are happening to the tables over time that we're dealing with so this this slides just gonna go through a little bit of detail around and around how we deal with that so the day one we've got this initial load it's just you know a set of columns with a set of data types we detect that as with Peter and we store it in our Delta they config metadata as our ground truth so day to day two rolls around we get a same table for this this next increment and it's got another column as we see here so we compare this and we say okay that is the the same schemer except we've got a new column now we know that Delta Lake can handle this and so we say that's absolutely fine we make our ground truth this new schema with the extra column de 3 comes and you can see that column 1 has now actually changed to an incompatible datatype we've gone from an integer to date time and we can't reconcile that so what happens here is that we compare this to our ground truth it says no this isn't gonna this isn't gonna work and we can error out with us with the correct error at this point in the pipeline so that we know what's gone wrong we can then go back to the source and get this fixed because clearly something's happened a4 rolls around and again we've dropped that new column that we had but the schema kind of lines up what's important here is that we're comparing to the ground truth from day two and not from day three because we never put that in because it never successfully processed so we're allowing this schema revolution which is typically a real troublesome part and we're allowing it to be relatively lenient and then utilizing the capability of Delta Lake to merge scheme the way our pipelines are set up means there's a lot of dependencies between notebooks and as a team there's a lot of collaboration happening on this is we've got kind of one source code for a single pipeline that runs hundreds of tables have to be a bit careful with dependencies and overlaps so we've kind of set up our own workflow for using it with data breaks notebooks and that's built off of some open source codes that I'll put a link in the chat to this on so we manage all of our build and release pipelines and all of our git repositories through as your DevOps good integration real easy to spin up so we actually work in a really traditional git workflow we have a master branch and then you feature branch off I think how a lot of teams work what we do is rather than using the built-in link between dates bricks and as your DevOps we have a middle step which is we manage I'll get kind of an our local machine where you pull down the entire repository so the entire project code and then you can deploy that into your user works based on data bricks and work there so that lets you have a branch of the entire project rather than just a specific notebook which i think is more works better with as your data factory compared to orchestrating through data bricks so having this this defined way of working on a whole project level means that when you have those dependencies between notebooks it doesn't get mixed up you can test them because you've got your own project going on and it means checking in there's a lot less issues with people having merged conflicts and overwrites so really it just opens up the entire team being able to work on the same pipeline at the same time without breaking links in between so over these next couple of slides I'll just explain a little bit more detail around how we're working with those Delta lakes as our config metadata and our approach to doing that so firstly to talk about the benefits of using Delta Lake we get the asset transaction benefits which means when we're doing concurrent writes this is thread safe and this is super important as I mentioned in a previous slide around how we're parallelizing that driver process in the in our pipeline we need the safety of being able to concurrently right to that config at the same time across multiple threads once we have that safety what this means is that rather than having to spread out our configs in two different locations maybe per source or per table we can just have a single master conflict table mate I mean one per environment but really there is a single master set of those configs and everything can interact with that one set this is great because it means you know we all hope that things don't go wrong but if they do it's really easy to manually change we don't have to go and dig out where that individual configures and we can have we have a whole process built around changing the conflicts and there's also initial set up in doing our pipe you know adding a new sauce or adding tables where we need to manually change and so having that one master set lets us just create a process for that rather than digging around in in common in a complex nest of files so we're basically getting that traditional benefit of a kind of database system you know we've got that ability to concurrently write and read and that ability to have a transaction log but with zero extra infrastructure so we don't have the complexity of spinning up the server whether that's cloud or on friend it just files on a link with that metadata layer that lets us do this and then another added benefit on top of that is that since we're using data breaks we've got that really nice kind of interaction between Delta Lake and data bricks where we can really visibly see that metadata layer you know look at that transaction log directly view the size of the files and everything like that the other point men is versioning again we hope that things don't mess up but if they ever do it's super easy to just use time travel on Delta Lake and revert back to an old version if something does go wrong rather than again using the example of many files dotted around and you're overwriting those files over time because you know you can't merge as such that's a lot harder to roll back on so we know that's a really useful benefit and also the benefit of being able to not necessarily rollback but at least see and track those that transaction log you know web who or what has caused the most recent write merge or append and to be able to track changes like that so this diagram is kind of a bit of a visual one on our approach we have this need to keep those keep our config sort of source controlled and the core data that lives inside those conflicts even if it's just the schema of that config itself but it may include the manually inputted data as well we want that source controlled and we want that to be able to interact with as a human so we actually have that in Jason and that Jason contains the data for the itself but it also contains the deployment instructions on how those tables get deployed into the Delphic config metadata so maybe that's just a simple overwrite take what's in the JSON and override the Delta make tables but it might be a more complex merge where you only want to merge on certain columns and update certain other columns so this JSON file is really easy to interact with as a person and we're planning to surface that in our UI in the future that gets get that gets pushed into get through a PR and then get will go off on a pipeline to go to deploy that into the Delta so that goes and uses the data bricks API that I mentioned earlier to cool this notebook this script that is a config deployer now that script will run through and on most runs and all the successful runs it will run the overrides run the appends run the merges into our actual Delta Lee conflict tables that a pipeline is interacting with and at that point we then need to go back to get an update get with that version of Jason so there's kind of a double master system going on what the point there is is that whatever sits in the Delta eight tables is mirrored by what's sitting in the Jason so in cases where this doesn't work and the deployment fails for whatever reason maybe an error in the human error in the Jason or maybe you know having two columns with the same ID or something it will fail that deployment will revert those Delta like tables pre-deployment as it were and then also revert back to pre deployment so in this way whichever whichever happens whether they're success or failure the point is that we always end up with a mirroring between the Jason and the Delta lake table so this slide just to give a bit of kind of in depth detail if anyone's interested on the left is uh the what the deployment part of that JSON file looks like so at the very top you can see a very simple override for a key value table and the kind of second that Jason shows what the more complex merge looks like so it's defining those merge rules which columns we're merging on and which columns we're updating and all of that is then taken forward and deployed in the right way by the script on the right hand side what we have is what a table definition looks like so you can see that they're the schemas laid out the identity columns and then just a dummy row to show what about would look like you know super human readable really easy to understand and really easy to add and change as well without you know one-off updates happening to those Delta make tables we're controlling it all in a source controlled manner by using Jason so George's are spoken about kind of Delta Lake and how that's good for our kind of back-end processes but that's also what we choose to store the data as for the analyst to access and use so that's our kind of end point of the actual data that flows through all of this pipelines and we tried out a few different things but found that with our user base it was definitely both easiest and quickest well if that comes from being able to optimize and the kind of data we use it's quite big it definitely needs spikes it hits it and by partitioning on kind of common filters like dates you can then just skip chunks of that and we said order a lot as well by drawing keys and the performance aid on that is noticeable so what that means is when you're doing a project using this data and just a lot quicker to get through and you don't spend ages waiting for your for your queries to go which is what used to happen on some of the more smaller sequel storage systems and we've also got that version data which isn't only good for rolling back when stuff goes wrong our end is also good for analysts and researchers to use because you can recreate projects that have already be done on the exact same data set and so we're non-traditional database has been updated and you lose that history all you have to do is say I want to look at what it was a couple of months ago and read in that it means you can then validate those models you can do comparisons between what we had before and what we've got and yeah being able to track that history sometimes when you're looking at specific use cases is really something that you just can't do under the systems we've also found just being able to freeze a table that you're working on without writing out your own copy of it is very good so you're saying I'm starting a project but I don't want the data to change underneath me as it gets updated I will always start my project by reading in as of version six and that it's my project version and then later you can take that away and run it on a more up-to-date date set without having counts change and stuff like that how's she going so we found that that's definitely an analytical bonus and then having the metadata accessible and visible is also very good being able to see when a table's last updated and being able to see the partition columns very easily means that the optimizations we've applied are accessible and usable you could say what's this partitioned on okay I'm going to filter by that specific date because that's the one that it will speed up and that other people have targeted as a reliable date column and so we surface some of our tables through the hide med store into 8 bricks using unmanaged tables which again if you just want to look at the data quick and do the exploration or you just click on the data tab and then you've got an example you've got the schema and you've got all of that like the data types and update history there which means if you're not sure which tailor you're looking at it's quite easy to dig in and get a quick view on them so that's kind of how we surface everything to our users everything we do is quite an action team is kind of for the advantages that come out of the projects that get done so thought we just finish off with a few examples of the stuff that our data goes into and so mentioned walsim era Science Institute really heavy heavy research stuff going on sizzle analysis machine learning stuff so they've actually built an AI tool which can predict kidney disease in cats up to two years before before the like tradition all tests that would be done in a bet so this is called renal tech and it's rolling out at the moment in the yes through Banfield Pet hospitals and just being able to look at the amount of data that we have as a whole using spark kind of opens up these kind of models which can just really revolutionize how we diagnose pets and pick up on early early signals of common diagnosis we've also got the pet insight project who are looking at pets activity behavior from these GPS trackers and pairing it with health health issues which recorded in the vet medical notes and that's another way to look at early detection of things so the potential for it is you can pair up dogs behavior at home like how far they're running kind of activity like that paired with common diagnosis again gives you just a much deeper insight into the pets life which means the diagnosis on the other end is just the more you can base that on the more accurate you're likely to be and then another one of the pet tech tools that we've got is the wisdom panel which looks at the genetic health of dogs so if you can take a sample through fernfield for example on their puffy plans you can get a genetic sample done as part of just your pet's health care routine it then means that before they have any chance of developing something you can be aware of what the most likely issues for that exact breeder so if you have a certain breed type that's likely to get back diabetes for example that might be flagged to you to look out for the warning signs a lot earlier nearly catch stuff the better it is for that pet so really all of this data the outcome is really building a better world for pets a mess the platform that our team is building thanks everyone for listening and we'd like to open up the floor for any questions you 