 hi everyone i am nimanushi and i'm a customer success engineer at databricks today i will be talking about reinforcement learning applied to the problem of financial portfolio optimization the ultimate goal would be to design and implement an automatic trading bot with spark however i figured before i jump to the implementation i go through the theory of portfolio optimization and how ai and in particular reinforcement learning can be can be applied to it i will be talking about the specific implementation in the following talks which i will be presenting at upcoming spark and ai conferences well today's agenda is the following first we define the financial portfolio optimization and discuss how it has been approached with stochastic optimization methods started by markups in the 50s then we talk about how we can actually translate the problem of portfolio optimization into a framework which uh reinforcement learning can be applied to namely a markov decision process here we talk about some specifics of financial markets that bring some challenges in the general formulation of the mdp after that we talk about model-based and model-free reinforcement learning and talk about how these models can be applied to the optimization problem and we discuss some of the pros and cons of those algorithms which puts us in a good position to start with a decent implementation in in the following sessions um okay what is financial portfolio optimization well it is a subcategory of the broader class of fund allocation problems imagine something like an index one which tries to construct a portfolio according to some predefined index weight or even a more straightforward scenario an equally weighted portfolio which rebalances the holdings of the portfolio each day such that each individual asset gets an equal share of the initial capital these are basically some examples of capital-based fund allocations one can think of another or other allocation methods which are not focused on the dollar amount of positions for example instead of allocating same amount of capital to each individual asset to come up with the equally weighted portfolio one can think of a portfolio which is fully diversified with respect to some measure of risk here instead of having the same dollar amount for each position we have equal amount of risk in general one can optimize some function to construct a portfolio right imagine that you want to have a portfolio which can extract the maximum value of returns or you want to maximize expected return for volatility units at the end of the investment time frame because you do not only care about the return but you want to manage risks so we can formulate the generalized problem as follows like given a student given a set of assets like m assets with a defined price history uh and a an initial portfolio or an initial endowment that you have at the at the time zero that you want to find an allocation which maximizes some objective function which is denoted by gamma here this function could be simply expected return which actually maximizes your wells at the end of the investment period or it could be expected return divided by some measure of risk like volatility or any other function which basically describes the investment goal objective functions are in general some function of the projected distribution of the asset returns at the end of the investment horizon this means that the problem of portfolio optimization naturally is a stochastic uh optimization problem uh one can solve it like in two ways yeah you can think about in a static optimization where you project you project the the the estimation of the next period uh returns all the way forward to to the end of the investment horizon and you come up with the distribution of your returns at the end of the investment horizon and solve the optimization just once for that or you can estimate the next period returns dynamically and solve the optimization problem sequentially to basically set up a dynamic optimization paradigm it was markowitz who pioneered the attempts to solve the stochastic optimization problem described in the last slide this framework is actually consists of two steps the first step no matter which objective function you have you just need to solve the mean variance optimization problem it is a constraint quadratic optimization which tries to minimize the portfolio variance while constraining the expected portfolio return at a target level solutions like this for different values of target mean value define a curve in the mean variance plane which is called efficient frontier once you have division frontier one can solve a one-dimensional optimization problem of optimizing the custom objective function along that curve in the mean variance plane um so one one might think uh to use this method to periodically solve this static optimization problem to make it a dynamical setup right well it seems that it is a viable option unless you try it in the real world because in the real world you have the transaction costs the problem is that the solution to the optimization for two periods might be so far away from each other that rebalancing the portfolio and the costs of transactions lead to a sub-optimal policy in fact myopic optimal actions can cause sub-optimal cumulative rewards at the end of the period um so that now that we talked a little bit about uh the opt-in the portfolio optimization problem and how it was formulated in terms of the stochastic optimization and the attempts that were made to basically solve those those uh problems we can discuss now a little bit about how we can formulate the portfolio optimization as a markup decision process and applies some of the the methods in the reinforcement learning to solve the portfolio optimization problem so first of all how is it an mdp defined right um let's assume a setup where at each time step an agent starts from an initial state takes an action which is uh some kind of interaction with the environment and the environment gives a reward to the agent and changes the state right if the state transition probability which is determined by the environment is only a function of the current state and not all the history of the up up to this point of the time the dynamical system is called markovian decision process okay how does it look like for a trading agent right at the beginning of each period the agent has to rebalance the portfolio and come up with a vector of the acid holdings this this basically defines the action so the action of a trading bot would be the directly the the portfolio weights that is uh is coming up with at the end of each period what about the reward what is the reward function of the environment in general identifying reward is a is a little bit more challenging and what is rewarded report basically is a scalar value which fully specifies the goals of the agent and maximization of the expected cumulative reward over many steps will lead to the optimal solution of the task let's let's look at some examples taking games for example the goal of the agent is very well defined in in games right either you win or you lose a game and it could be well divided into separate reward signals for each time step if you win a game at the end of the step you got a reward of one if you lose again at the end at the end of a time step you get a reward of minus one for example and you get a reward of zero otherwise so very well defined and very well divisible into separate time steps however take a trading agent for example who wants to maximize the return but at the same time do not want to expose this fund to extreme market downtrends and crashes he does it for example by managing the value at risk of his portfolio so the the objective of the agent is clearly defined by but dividing this objective into sequential reward signals uh might be a very challenging task um now let's talk about the state and observation at any step we can only observe asset prices right and the observation is given by the price of all assets this is clear we also know that when one period prices do not fully capture the state of the market so this is something which is known i mean you cannot basically predict the whole state of the market by just looking at the prices of yesterday for example this makes uh financial markets a little bit more challenging and they in general financial markets are not a fully observable markup decision process and they are just partially observables because we can only as agents uh observe the prices um so what it means is that the state that that an agent has is completely different from the state of the environment and there are some solutions to basically uh build the whole environment state from the state of the age the most obvious solution is we can we can build the state of environment from the whole history of the observation which is basically not scalable or we can approximate the environment state by some parameterized function of past observations when we were working with time series uh as as we're as we're doing that in financial markets it is natural to to assume that the state generating function is not only a function of observations but also a function of the past energy in states right so with we think of some some models which has some kind of memory um let's look at some of the examples garsh models so these are these are these are the models which are widely used in quantity finance and they are basically constructed in this way assume that the state of the market at each time can be fully represented by the volatility of individual assets this is the assumption that basically says if you know the volatilities you know the full state of the market if you assume that garch models can build a rather simple mapping of past volatilities and current observations which are the prices to generate the volatilities for the current time step and therefore they can fully build the state of the market from the observations that passed from the past observation and past states we can look at uh in a con we can look at other models like in in continuous domain stochastic volatility models they do the same they basically build volatilities which are hidden states of the market as um by by just fitting a kind of stochastic process to to to the volatilities in this way they are able to basically generate the hidden states which are volatilities and generate a full representation of of the market but obviously one can use a more sophisticated featurization of the of the hidden variables or hidden state of the market so it shouldn't be as simple as just volatilities one can have a complicated representation of that and neural networks for example can can build those kind of complicated models of of the market state but the common thing among all these models is that the state of the environment is built used using the past observations and past states and the the state of the agent at the current time is not uh enough to basically come up with the with the whole state of of the financial market or basically the returns for the next period okay now that we talked about the mdp formulation of portfolio optimization a little bit i want to go through some of the main components of the reinforcement learning in this part to basically put us in the position to come up with some algorithms that we want to eventually be implementing using reinforcement learning um policies so policy is simply mapping from a state which an agent experience to an action that it takes it could be deterministic policy which means that if an agent finds himself in a certain state he will always take a certain action or it could be a problem probabilistic policy which means that he will choose a certain action from a spectrum of all possible actions with some predefined probability concept of value function so what is value function value function is defined as the expected amount of reward one can get from an mdp starting from the state and following a certain policy for example if we define the reward of a trading bot to be just log returns of portfolio returns at the end of each time step the value function would be the expected amount of cumulative return at the at the end of the investment horizon and models what are models models of just agents represent a representation of the environment and it defines the transition probabilities of the state and development for example if you assume that the next step returns of the financial time series following gaussian distribution the model of the environment is fully defined via the transition probability of a gaussian distribution so now that we have all the ingredients in place we want to talk about the model based in reinforcement learning import for the optimization how the setup looks like and how we can basically build algorithm algorithms based on these setups we start from our familiar mdp setup where an agent interacts with the environment and gets rewards based on the action it takes but now the idea is that uh the agent first tries to learn the model of environment from the transition uh he has been experiencing so he's not going to to to optimize the policy directly from the experience but he first tries to learn some model from the transitions that he's been experiencing and then based on that model he will try to to solve a kind of optimization so at each time step uh the agent first predicts the next state because he has a model for the employment so he he predicts the next state and the reward he will be getting based on the action e2 he took then he observed the real transition the real rewards that he got from the environment and then he can basically incrementally update his model because he has a model and he has a loss function that he can basically train uh train the model upon so what are the advantages of that uh that kind of paradigm so um there are some advantages especially in in financial uh portfolio optimization the most important one is that there has been a lot of studies about the behavior financial markets and the properties of the financial time series data it is very easy to basically implement those findings directly into a model-based reinforcement learning paradigm right so you basically can put all those findings explicitly into a model and then have a model that best describes uh the financial market transitions so things like volatility clustering things like heavy tales of the returns tail dependence among different assets existence of jumps and non-stationarity can be directly modeled and learned from the data but then obviously there are some disadvantages because you have an explicit model that that you have to first to learn there are uh some sorts of errors and approximations coming coming right so you you first have to to learn a model and if your model is not a an accurate representation of the environment the the optimal policies that you learn based on that that model won't be optimal at all because you have a model which which cannot or is not basically describing the the market as good as if it can or it should um so let's formulate everything that we've been talking about the model best reinforcement learning um what should we do um well in general if you want to basically use reinforcement learning or model-based reinforcement learning we need to gather some experience by interacting with the environment and figuring out the model from those experience that we have been gathering right but in finance it is a little bit much easier because uh the interactions that we make with environment which are basically the the transactions that we make uh do not uh affect uh the state transitions what do you mean by that is that any time that we buy or sell any asset in the market we can assume that this kind of transaction does not change the prices so that we can basically separate the whole action from the whole transition and we will have a setup which which which has only the transition of the prices so basically we can look at the history of the prices or the returns and we can basically train a model based on that or supervised model based on that so the the whole approach will look like something like this we pick a parameterized model which predicts the next state transitions or comes up with the probability distribution of the next time uh period uh returns we pick an appropriate loss function uh so that we can train that model and then we find the prime the the parameters which minimize that loss function and we basically can train the whole the whole model on our uh on our on our data set let's put all of this into a generalized algorithm that we can use for any type of model based reinforcement learning input of this algorithm is simple you have your trading universe of m assets so basically you have to define what kind of assets you want to trade in you need to define the parametric model which you think predicts that the returns of the market the best and you need to come up with a loss function which describes the deviations of the model predictions from the observed uh returns for example you can have an armed guards model with non-gaussian innovations and the corresponding loss function would be like likelihood you could use maximum likelihood estimation on a batch data set basically to to to first initialize the model or learn the parameters of the model and then jump into a kind of online online training setup in the reinforcement learning so the rest of algorithm is simple you use the batch data that that that you have basically gathered this is simply your history of the prices um you use that to learn the the parameters of the model and then you start to to to iterate over the time steps you start to predict the next state from the model that you basically have learned on your batch data you observe the returns and the state not the state you you observe the return and and the prices or the returns by just stepping forward you build your state from the observation and the state's uh history that you have been uh gathering this is part of your model so basically part of your model is is responsible for building the environment state from the observation that he has been making and then you calculate the deviation from the state that you observed or or basically built upon the the observations that you made and the state that you have been predicting and then incrementally incrementally learn the parameters or or change the parameters based on the gradient of that loss function and rebalance the portfolio based on uh based on the model that you have you can use any kind of model model based control basically to solve the the optimization problem so uh as as soon as you have a model your for your environment you basically can sample from that model for example in a monte carlo setup so you can have a sample of all returns until the end of the investment horizon and basically start to uh estimate the the the objective function like expected returns volatilities whatever you you basically put as an object as a as a function objective function to optimize for and then solve the optimization problem uh for that uh for that one to call a sample as soon as you have a model you basically can can uh can control uh your policy basically using different policy methods or just subscribe to follow and then uh reiterate until you basically converge so this is a whole parting whole scheme of of of using uh model based reinforcement learning to uh to to learn the model at the same time uh use that model to to to plan and come up with a with the optimization at the same time okay instead of learning a predictive model of the transitions first and then use that model to come up with the optimal policy one can start to learn the optimal policy based on the value function directly right assume that the value function can be defined as a cumulative return you will be getting at the end of the investment horizon then you can use a generalized function for the policy which parameterizes how you rebalance the portfolio at each time step and then at the same time you can use another function to parameterize the amount of cumulative return you will be getting if you rebalance the portfolio accordingly so this is a typical actor critic setup which is one of the state-of-the-art methods modern model of reinforcement learning and could be directly applied to the problem of automatic uh trading bots using using one of reinforcement learning so here in the graph i still i i have pictured basically how how it could look like the networks basically the actor network will get the observation which are the prices based on those observations we will first build the state of the environment and then use that state to come up with with some action which is basically the portfolio weights and then a critic network at the same time uses those those uh those weights those portfolio weights and of course uh observations uh of the prices at the same time he builds a state upon those again and then uh come up with a with a value right with value uh how much that rebalancing will will basically um give you a cumulative return at the end of your investment so basically we'll roll it out until the end of the investment horizon and look at the the returns that you will be getting and give you an estimation of the value you will getting from that action that you picked from your actor network and these setup can be jointly trained we are different state-of-the-art algorithms i just put a generic ddvg so deep deterministic uh policy gradient algorithm and it could be applied to to to this specific problem and this is something that i will be trying trying to do alongside a model based reinforcement learning to be able to to show how we can implement those in spark and use some of these spark features to basically paralyze those those model trainings and and come up with the ideas how a full implementation will look like so let's uh briefly talk about the kind of challenges and the problems that that all these kind of models that we talked about have so um as as i said before i mean uh it is very important and crucial to to reinforce my learning algorithms and mdp formulation uh to have a clear reward function signal right um it is kind of challenging for uh for a generalized portfolio optimization uh framework to come up with um reward function generators if you have some some sort of complicated risk functionals like valid risk um or any other quantile based uh risk measure of of the of the portfolio returns it might be a problem to basically uh engineer a reward generating function the other thing is about the the environment the financial market environment it is a very complicated environment it is a lot of features which basically make it very hard for the models to to learn effectively and on top of that there is a general general theme in financial markets so basically the the ratio of the the signal to the noise is pretty low compared to other other other other areas which basically where reinforcement learning has been successfully applied to so things like games um things like uh image processing text classification stuff like that so basically the nature of financial markets and the nature that is a very noisy noisy noisy environment makes it very hard for for the for the reinforcement learning algorithms to learn it [Music] added to those problems there are some specific problems with with model 3 and one of model based reinforcement learning financial markets um for example if you want to use model 3 there are limited amount of training data so you know that the financial time series if you for example want to to to to learn a model which uses daily um return data or daily prices you basically have like 250 data points for a year and then for i mean if you want to to to train your model uh on a history of 10 years you will not have more than like 2 000 to 3 000 data points to basically train your model and this is very very small amount of data um which basically means combined with with the fact that the the financial markets are very noisy environment will make the models to be very prone to overfitting and not being able to generalize well for the out-of-sample out-of-sample data in a model-based reinforcement learning you could have some of the some of these uh a specific specific characteristics of the financial markets modeled explicitly into your interior algorithm but then you will have to come up with um with ways to to to cope with model uncertainty changing of the models inaccurate models and and your hyper parameters of the models and these will directly affect your optimal uh optimal portfolios and optimal solutions that you will have at the end of the day so there are some some ideas that can basically use the benefits of those both worlds to to make make make reinforcement learning a kind of viable option for portfolio optimization kind of hybrid methods that you basically start to learn a model at the same time use model uh model three to to generate samples and augment uh augment data basically to to to to come to to basically cope with that limited amount of training data problem and try to use model model 3 reinforcement learning under generated data from the model 2 to to get more accurate type of type of solutions but these all have to be uh to be tested and taken uh very carefully into the account so um it was my uh presentation so the first part i just wanted to give you a theory of how it will look like what are the challenges of using reinforcement learning and um trying to understand the theory behind it and then the next part i will be trying to implement a fully integrated solution based on based on reinforcement learning algorithms different type of algorithms and spark to to to train a an automatic trading bot which can basically come up with the optimal optimal portfolios at the end of a certain investment horizon i hope you enjoyed it and thank you for your time you 