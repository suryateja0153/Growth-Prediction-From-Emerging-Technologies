   [MUSIC]   [NARRATOR] Welcome to Unite Now, where we bring Unity to you,  wherever you are. [ERIC] Thanks for tuning  into this talk on ArtEngine and AI-Assisted Artistry. Since you're watching, you've most likely heard that Artomatix has joined the Unity family, and you're probably curious  to learn a little bit more about who we are at Artomatix and what this means for Unity. The goal of this talk  is to answer these questions and give you a wider context around where we see  the industry going and how Unity plans  to lead the charge. Before we get into that, though, I'd like to introduce myself. I'm Dr. Eric Risser,  the founder of Artomatix, where I've also led  the technology development for the past eight years. The reason why I'm giving this talk is because Artomatix's history, especially in those early years, is largely entangled with my own. You see, before founding Artomatix I was already working in AI Artistry in academia. My research topic  sat at the intersection between machine learning  and computer graphics, where I focused on synthesizing new unique images,  sounds, 3D shapes, based on man-made examples. Over the years, I've published a couple of SIGGRAPH papers, and I put together some pretty nice prototypes on the topic. So then when I finished  my doctorate, I noticed that  this kind of technology didn't really exist in any strain in a meaningful way, and I was a little worried that unless the people who were really pioneering the field in academia led that charge, it never really would. Also, I noticed there is a pretty big problem in the industry, and I wasn't really sure how else it was ever going to be solved. Digital art has gone through an evolution over the last 20 years. But as the quality has gone up, so does the time it takes to make it. And it's not just  the main characters we care about, it's all this other stuff in the  background that takes forever. This has gotten so bad that  Grand Theft Auto V took five years and cost 280 million dollars to create. It's not even Grand Theft Auto V  that's the problem, it's Grand Theft Auto's I through V and the curve that forms. You see, it actually mirrors the  median video game development costs almost perfectly  over a 20-year span. Or if you were to put that  another way, games are taking  significantly longer to make. The reason is that art  creation is still an almost entirely manual process. There are more artists now than ever, and art tools are getting way better and way more efficient, but relative to the explosion  and demand, there is just no way  for an inherently manual process to keep pace  with the exponential increase in computer memory and power. That's where our mission  at Artomatix over the past eight years  has kind of been. We want to build  a new breed of artistic tools that introduces automation. We call this  the example-based workflow. But what does that mean? Well, the idea is that you start  from example data, which can be a scan you took, a quick texture you drew yourself, anything really. Then you add human intent, so this is what you want to have done. And then you add  some kind of automation to that or the computer does through some kind of advanced algorithm, say, some kind  of a machine learning approach or possibly something  from data science. Basically, it executes  on what you intended it to do. Then there is also an optional  fourth feedback loop step, so you can keep iterating on your design. The goal of the  example-based workflow is essentially to make  every artist an art director. So you can create through curation. So this workflow is great for rapid prototyping, for mass production or just getting an artist  to the 80% mark super quickly, so they can spend their time focusing on what really matters to them. Now, the example-based workflow  wasn't created by us. The industry is already  familiar with this concept. We've just been calling it the photogrammetry workflow up until now. With photogrammetry  you take a bunch of pictures, as the example here, Then you add the human intent, which is I would like a 3D model, and then the computer  automates that process. So here's a picture  of our first product ArtEngine. It's a reimagining  of that classic procedural tool but based around machine learning. Thanks to this, it's able to automate a lot of the time-consuming  and less creative work you do today. Now, I just said a big statement. Our technology is based  on machine learning, and that makes it  fundamentally different from the procedural tools  you've seen before. I think it's worth taking a moment and explaining what this means. At a high level, you can think  of procedural and ML Artistry as the opposites of each other. In a procedural workflow,  you write an algorithm, and that algorithm guides  how the data is processed. On the other hand, in an ML Artistry workflow, you provide the data, and that guides  how the algorithm works. If you really think about this in terms of like an AI game character, so in a procedural workflow it would be kind of like  if you were to program a whole bunch  of IF-statements and equations that would tell that enemy player how you want it to behave in certain pre-predicted scenarios. Whereas an ML equivalent  or alternative would be like starting  with a blank slate but giving it the rule  that dying is bad, and then you play against your ML enemy, and it over time learns from you and figures out how to get  better and to die less. So let's start  with a simple example by looking at ArtEngine  Seam Removal feature. Given this input, ArtEngine reimagines what the seam region  could look like, but tileable. I'd like to highlight how this is  different from procedural seam removal techniques by  walking through how one of those might work. So let's say we start  with this input image, and first we would translate it  by 50% along the X and Y axes, wrapping it around the borders as we go. So, cool. Now we've got  the original image on the left and the wrapped image on the right. You can then grab a plus sign  shape patch out of the middle, and then you can blend that with the wrapped version. This trick can work pretty well  for messy textures like this one, but you can notice a lot of ghosting around the lichen. There are ways to make  this problem less noticeable. You can use graph cut tricks and things like that, but, ultimately, it's a limitation  of the procedural concept, and you ultimately  run into problems with more structured textures  that have more distinct objects. The key difference between  our approach and procedural is that procedural  does the same process regardless of what  the input texture looks like, which doesn't really make sense. Like if a human were doing seam removal, they wouldn't manually do  the same operations for a grass texture as they would for a brick wall, so why should the algorithm? Because it's such a core part of our workflow and frankly because of the way every single art tool is going to work in ten years, I'd like to take a moment  and tell you a little bit about how machine learning works. Don't worry, I'm going to get a little bit into the weeds here, but I want to emphasize that this stuff isn't that scary, and you don't need a PhD in AI to use these tools or even  to appreciate how they work. I just want to offer you a little behind-the-scenes understanding so you feel more comfortable. The main idea  behind machine learning is that algorithms are able  to improve automatically through experience rather than their direct programming. Thus, ML algorithms are given data, and they customize what happens  based on that data. Machine learning at its core  is statistical in nature. This is important because it lets us leverage a whole body of mathematics  and algorithms to modify and interpret probability density functions. Let me show you an example. This is a visualization tool I threw together real quick to help illustrate what the machine learning process is doing under the hood. Going back to the seam removal  example I showed you, here's what the problem  looks like to a computer. First, the computer needs  to reframe the problem into something it can relate to  mathematically. That's why the image on the left has been turned  into a point cloud on the right. The green points come from the green pixels in the center, and the red points come from the red pixels along the seams. So what is  this point cloud exactly? It's really just a summary  of what the image looks like around each pixel's location. So it's kind of like taking  little crops of the image around each pixel. If you think about  a 10x10 pixel size crop, that's 100 pixels, Then, when you multiply that  by the three-color channels, you're looking at 300 unique values that you need  to store for each crop, or if you think about it  the other way around, that's a single point  in a 300-dimensional space. If we were to gather these crops  for each pixel in the image, then we can treat the image as a point cloud  in our 300-dimensional space the same way we have a point cloud in 3D-dimensional space  here on the right. Now, there's a couple of problems  with using crops directly. The first big problem is that they use a lot of memory. At 10x10, it's still kind of  manageable for our algorithms, but what if you were to increase  the size to 100x100 pixels? That would be 30,000 dimensions, and it'll probably make our computer explode. The other problem is that each pixel in the original image translates to one point in our now 30,000-dimensional space. So that space is going  to be mostly empty. 30,000 dimensions is just  way more capacity than we need, and it actually makes it harder  for the computer to understand how each point relates  to the cloud as a whole. So, in this example, you're seeing the space condensed to 3 dimensions, so we can make an X visualization. But it's also a good idea to store information  to a space that's both smaller and more meaningful to the problem. This is the first part  of the process where we start to use  machine learning, and there are  a number of ways to do this. Now, again, don't worry, you don't need to understand  any of this to use the tool. The only reason why I'm talking  about all this stuff is just to dispel some of the mystery around machine learning, so  it's a little less of a black box. So, for some problems,  you can go for a supervised ML algorithm that does classification. This essentially means  you're teaching your neural network how to recognize  what's in the image. By training it to do that, you're actually computing  a transformation function that turns that 30,000-dimensional  raw pixel data space into a perceptually meaningful  lower-dimensional space where different axes  relate to common image features, like shapes and gradients and that sort of thing. Alternatively, you could also use an unsupervised  machine learning method which would reduce the data stuff without having to be pre-trained. This is also known  as dimensionality reduction, and there are a bunch  of different algorithms to do this. This comes in handy when you have data that doesn't necessarily relate directly to human perception. So, basically, that's what we're showing you in this visualization. The image features around each pixel on the image on the left has been transformed  into a point cloud on the right. This is a nice way to frame our problem for the computer because now we can observe  something very interesting. Well, some of the red points  overlap with the green. We also see blobs of red points sticking out from the rest of the cloud. Statistically, these points  are called outliers, and they come  from our ugly seam lines. The reason why they show up  in our point cloud further away from the center is because they visually look different from the rest of our image. And this is basically what we want  to fix in seam removal. So now we can rephrase  our seam removal problem in a way that  the computer understands. We want to change the pixels  in the red portion of our image, so the corresponding  red points in our cloud match our green points  in the cloud a bit better. Now, this is easier said than done because keep in mind, each pixel in the image actually  contributes to a bunch of the crops since they all overlap  with each other. This means that if you update a single pixel it'll move a bunch of the points  in our cloud all at once. This is a hard problem to solve, but, again, that's where machine learning  comes to the rescue. The end result, after all these algorithms do their job, is that the red points  are turned into these blue points. You can see in the visualization  that the blue cloud changed the shape a bit and condensed. So the outliers shouldn't be  too extreme anymore. If we bring back  the green points in the center, you can see  that the new blue points actually do fit the distribution pretty well. Now that we've seen the difference between how procedural  and machine learning works, I want to point out  that the two aren't enemies, and you don't have to choose  between one or the other. In fact, the two  are actually really compatible, and their combination actually gets you more than some  of their individual parts. Well, procedural doesn't get you the kind of quick automation wins that you get with ML, and while it's fundamentally not good  at working with example data, it does get you precise  role-based control over your generated art project. This is very important for making an ML approach practical in a real-world production, and so it's guided  our design choices all throughout  the development of ArtEngine. Our Head of Product UI is going  to walk you through some projects to help illustrate how all these concepts work together in practice. [MAN] So, we have a few use cases to go through to show you some of the power of ArtEngine. Before recording this, we wanted to capture real-world textures to use, so we grabbed our cameras and did a little bit of trespassing. With that done,  here we are in ArtEngine. And you could see  the image of a wood fence which has been scanned  or photographed, just by an iPhone 11. This was captured using  a photogrammetry process, which was done very quickly, the idea being that we just really wanted it to be pretty easy just to get some data in here. And so now what we're going to do is convert this image  of a wooden fence into some floorboards, and you will see this whole process start to finish. So we started with an 8k image, and the first thing to do  is to crop it down a bit to the edge of the desired details. We can fix any skewing while we're at this. Also, 8k is a bit of overkill for this example, so we're going  to reduce that down to about 4k. Now, starting our cleanup process, we can remove this gradient and switch back and forth and see how that looks to even things out. We can also perform some de-lighting using something called albedo generation node. You can actually  blend these two together to have more control over the de-light process. Fluctuate it, to get it where you like it. And that's an important thing  to highlight about the node graph. We have full control  over the process: the order of things,  the order in which we do things, how they kind of work together. That's all really something  we can choose, we can pick. Next, we can mask to remove all the spaces  in between the boards. Since this is going to be a floor, you're not going to need those spaces like a fence would, unless it's a really crummy floor, but that's entirely up to you. Now, the colored Mask node is going to allow us to easily remove  some of these imperfections like these knots that wouldn't exist in a floor. Just masking that data out will leave some gaps in our images. We need to put something in there. So we'll use the content-aware fill. And that will cover up those imperfections and fill that area with part of this wood grain that makes sense and blends seamlessly. The next thing  we're going to look at is a bit of a procedural workflow. It's going to allow us  to remove seams, so we can tile this floor  more than once. Obviously, that's important for a floor. Now, we have nodes to remove seams very easily, and we're going to look at that  in this next example. But I also wanted to show you how you can clean up this tricky organic seam as well. The reality is there's a lot of ways to achieve this stuff, and we have a lot of options and nodes that are going to get  the results that we want. Now, with that done,  we have this seamless texture, and we can use  this mask paint with lines node to assist us in the generation  of a normal map. We can blend the floorboard lines with other normal map data generated directly  from the diffuse map. This leaves us with a very  realistic-looking normal map. Again, the important element  here really is that we were able to take  a diffuse map, a photo, really, from a photogrammetry workflow and generate a cleaned-up albedo  and normal map from it. From here, we can use color match nodes to create variations, and a lot of weird, interesting, cool possibilities. And, again, all from an original dataset that is much lighter and really captured a lot faster  than what's usually required. Before we wrap up this example, while we're looking at a few others here, another thing to highlight is the level of parametric control available that really enables us to go, "Wow, we can change just all sorts of stuff with this." And we could continue  iterating on this or, if we're really happy, we can move on to the next item on our list. In the next example, we're going  to look at some concrete. This time, we have  a diffuse and normal map, and we can see  that it needs a little cleanup, starting with some  gradient removal. Next, I want to show you  something cool. So we can actually  use the Mask tool and then invert it to basically select a sample of our image. In this case, we can select  the sharpest parts of the image. Then, using the mutation nodes, we can actually apply sharpness from the selected areas to the entire texture. So if I zoomed it, you can see the area  I took the original samples from. And you can see  how that's been applied all over this texture. And not only is this applied  in an intelligent way, it also has removed the seams from our texture. Once we're done  with this general processing, grabbing the parts that we want, we can further refine to get us  exactly to the end solution that we're looking for. So even though  we already have a normal map, we can go so far as to generate  a second normal map from it to create some variation and make some  very interesting changes. All of these options,  of course, are controllable through these node parameters. Next, I think we'd like to generate the height and roughness map. Now, we could do that  in some old boring way, but I prefer to use  good old-fashioned trickery. So, to do this, we can actually  generate noise here by applying a color mask  to the normal map to take out some of the highlights. Once we've done that,  we can actually blend that with the grayscale of the albedo, and the result  is this new roughness map that highlights the high points of our edges and gives us  this really cool visual effect. Then we can generate a heightmap  directly from our normals. Again, we can see an example where we bring in  some data from the field, gathered very quickly, and just figure out  how much we can clean, modify, refine it. And, again, these were captured with a cell phone camera, which for anyone  who's been doing this a while is absolutely insane. So, on to our last examples here, and I really want to focus on how ArtEngine  deals with patterns. So you can see here  that we've brought in some maps from a sort of weaved wicker fabric thing. This is something you might find  in some IKEA furniture or, I don't know,  other fine furniture outlets. I don't know. [LAUGHS] Anyway. So we're using photometric stereo, basically bringing in eight images, and then we want to prepare it. We're going to crop it,  we're going to smooth out the gradients of the standard stuff we've looked at in the previous two examples. This is looking better,  but there are obviously some seams with a pattern like this. So we can use these nodes to actually guide the AI to recognize the pattern  in the material. By giving it just  a little bit of information, it's able to flatten this out, and that in turn allows us  to make it seamless. Once the AI recognizes the pattern, it can repeat that infinitely. From here, we can add things  like a heightmap. In this case, we're going to smooth  that heightmap out a little bit, soften it up a little bit. And in the end, we have  a very nice render of this wicker basket material stuff. We take this stuff; we bring it in, and we make it production-ready. Kind of easy, better living  through AI-assisted artistry. I'm going to quote that. Now, we have some wool, which is a cool example because it's something that would take us a long time  to make procedurally. There are lots of little hairs  and strands and threads that make up this wool and would take us a really long time to generate this. So, again, we can crop this down to the parts that we want, and, again, that obviously is going to leave us  with some seams. As we saw before,  we can flatten this image. This time we're going to flatten it with a pattern unwarp node. And then, just like before,  we can guide the AI, say, "Hey AI, here's what we want to do. This is the pattern,  hope you recognize it," and it's going to do  the rest of this work for us. I'm not really an expert on wool, but I think that's some  pretty high-quality looking wool that we have right here. From here, we can apply  a few modifications: change our sizes,  again, create a heightmap, and, again, remove any gradients that happen to exist. And in the end, we have  some really great looking wool. We brought it in, a little bit of work, and we're good to go. By providing  some information and context to the AI and ArtEngine, it can assist us  in generating a lot of elements that would take us a very long time to capture or generate by hand. So we work with the system, and it speeds up our process, allows us really to create  some really cool stuff. [ERIC] So now that you know  who we are and what we do, let's jump back into the big news: Artomatix has joined Unity, and this is a big deal for our users  and the industry as a whole. For a Unity user, this means that ArtEngine and all the stuff  we've shown you in this talk is about to get a whole  a lot better. You're gonna see some new art  creation tools coming out of Unity, and they're going to help you build your game faster from start to finish. That also means that you're going to have access to machine learning tools for the first time ever. The other big takeaway is that you're going to start seeing some of this technology show up directly in the game engine itself. Now, just to be clear, this doesn't mean that the game engine  is going to replace ArtEngine. More that there'll be some new powerful features under the hood. In addition, we're working on ways  lines to bridge everything together, so when you work  on Material on ArtEngine, you see it automatically update  directly within Unity. I would love to tell you about  some of the new standalone tools we're working on, but it's still a little early  to publicly announce them just yet. Instead, I'd like  to switch gears slightly and talk about  the behind-the-scenes discussions that led to Artomatix joining Unity. As many of you know, Unity has been placing  more and more emphasis on artist tools  and content creation over the last few years with additions to the engine  like VFX Graph, ProBuilder, and other really awesome stuff. This has been going really well. Unity wanted to move faster  and felt that it made sense to create an entirely new division to focus exclusively on standalone art creation tools and technology. Unity reached out to us about this time last year to pitch a vision they have for turning Artomatix into this new division. At the time, we'd just released  the commercial data for ArtEngine, and you're seeing some  really nice adoption in the AAA. But we're also only 20 people  working out of a small office, and we felt we didn't have  the resources to go after our big vision. So it was really a perfect timing  and a perfect fit. Having now joined Unity, we're in the process  of moving into a new building where we have the room and the headcount to grow to over 100 people  by the end of the next year. This will be five times the size  we were a few months ago. So we'll be able to move faster  and build new products. Unity has entrusted  this very exciting mission to us because we share the same vision,  and they believe in our expertise. This is a belief that's been built over years. While you may have just recently  heard of Artomatix or haven't heard of us at all, we've been quietly working away in the background building this technology  for a really long time. Lots of research and development  had to go into making ArtEngine something ready for industry, and Unity has been helping us  along the way to do this. Here's a screenshot from a project we worked on together  almost four years ago. So now that this technology  is ready for the industry and the industry is ready  for this technology, we thought it was the right time  to formally join Unity and step into the light, which brings us back to ArtEngine. Let's go a little deeper  into what we have to offer today. As I've mentioned, our first product is a Material authoring tool based on combined machine learning and procedural workflow. And I'd like to talk a little bit about the design philosophy  that went into that. I've already mentioned  that the example-based workflow is a three-step process. First, you gather example data. Next, you provide human intent. And finally, you automate. I'd like to go a little deeper and talk about each  of those three steps. On the data side of the workflow, you can really just keep doing  what you're already doing. You can gather data from libraries, scan or pin it yourself, whatever. Our workflow  is really more focused on adding human intent  and automation. I've mentioned that we combine machine learning with procedural functionality, and that procedural component is really where the human control  comes into play. It lets you script  the ML features together with more complex operations and room them out with pure procedural functions like noise and shape generators. We also have Unity standard  image processing features like pipeline, all that kind  of good stuff we're used to. The other way artists can provide  intent is through painting masks that help guide the ML operation. So you can think of this as it lets you direct automation in a painting-by-numbers  kind of way. The way that really makes  ArtEngine stand out is the AI functionality. And this can be broken up  into three main categories: Enhance, Expand, and Transmute. The reason  why you'd want to enhance is if you have old  or low-quality data that you need to bring up  to some modern standards. Upres is a great example of this. Lots of older artistic content  still looks great but needs to be remastered. We offer two modes for this: tune mode and realistic mode. Sometimes when you grab images off the Internet to use as raw materials, they can suffer from some pretty bad JPEG artifacting. We've trained a neural network to clean those right up and bring back  the original details. So Enhance is for any asset that needs these types  of fix-up operations. Expand, on the other hand, is for assets  that already look great, but you just don't have enough of them. Expand starts from a small example  and makes more of it. It brings the richness and variety  of the real world into the digital realm. There's plenty of examples  of Expand features in this presentation already, so I don't want to bombard  you guys with more. I just wanted to highlight one that I think really  stands out above the rest. It was created by Pete McNally, a tech artist at Havoc. So Pete went to a cliff face with his photogrammetry gear and scanned  this beautiful rock wall, and then he went to ArtEngine and mutated it out to cover a much bigger space. He then went  into a 3D-modeling program and made this  kind of warpy cylinder and applied his material on it and wrote a blog post called "The PBR Cave in under 60 Minutes". I think this is a really cool example because it really highlights how you can get  from 0 to the 80% mark superfast using  the example-based workflow. In any case,  that's enough about Expand because we've already  covered that quite a lot. Let's move on to Transmute. Transmute changes some basic nature or characteristic of an asset, tailoring it  to some specific purpose it wasn't previously really suitable for. These features come in handy when you have high-quality assets, and you have enough  of whatever it is you needed. But it's just not fit  for purpose in some way, like if your game were a puzzle, these features help make the pieces fit together. A great example is color transfer. This lets you update the color  and Material properties so that one Material  imitates another. You saw an example of this  in the wood fence demo where multiple images were used to change the look  and feel of the final result. I want to show you another feature that helps you control your colors, but in a more precise way. I also want to highlight that ArtEngine isn't just a tool  for modifying scans or even just for editing Materials. The plan is for ArtEngine to become a robust artistic tool for everyone. As such, I want  to show you an example of how you could use ArtEngine  to help make an 2D game. First, I grab the Sprite Sheet off of the Unity Asset Store. I really like  this little dragon warrior guy and want to play around  with him a bit. Since I just took a screenshot of the Sprite Sheet, it's full of JPEG artifacts. So the first thing I should do is to run it through an Enhance node to clean up the image. Cool. So now that it's sharp and less noisy, we can add a color transfer node, which automatically generates  a color palette for the image. By default, it chooses the top five most representative colors  for the palette. But you can set this  to any number you want. You can also override  the palette colors that ArtEngine chooses with your own. Anyway, the default color palette looks fine, so let's start modifying. To start, let's change  the dragon skin. Instead of orange,  I prefer dark green, so I just select the orange palette  in my Properties panel and select the color wheel. I move that over to green and select. And cool. Now that the skin's green I think I'd like the wings  to be dark red. So I'll just repeat the same process. And lastly, that brown hair  would look better black. Cool. There we go. All done. So let's zoom out  and look at our Sprite Sheet. Because this now updates  the color space itself, all the little dragons  were modified together. If they are broken out into individual images, that would be okay too because we could still batch process them with our new batching node. So now let's switch back  to our original image so we can see the difference. This is something we could always do before an image-editing software, but now we can do it a lot faster in just a few clicks. It's a great feature  for taking assets from the library and tailoring them  to your specific game, 2D some kind of 3D thing from the Asset Store. This is also an important feature to highlight because it's not actually using  any machine learning. It's still modifying point clouds, like we saw earlier, but it doesn't require  any learning to do so. Color space is only 3 dimensions, so there's not really  any need to reduce that. And by modifying  the palette colors directly, the user is telling ArtEngine exactly how they want the point cloud to be transformed. Now, that's not to say this feature isn't powerful or complicated or wasn't difficult to implement, quite the opposite. I just want to highlight  that while we use AI for a lot of the stuff we do, there's no rule that says we always have to use AI for everything we do. Our only guiding principle  is to be pragmatic and do what's best for the user. To that end, I'd like to  highlight our unwarp node. We already showed this off  in the cloth demo, but I want to highlight it again since it came directly  from a problem we observed with people trying to scan cloth and other textiles. It's a hard problem to solve  with general imaging software, and one AI makes a lot of sense for it, since it's essentially  a computer vision problem. I also want to highlight pattern unwarp node to highlight the other main use case for ArtEngine. In addition to Material authoring, ArtEngine has a lot to offer  to anyone who wants to scan. To this end, we've got  some nice scanning features built into ArtEngine already, and we're working on more. First, there's image  to Material generation that you must have  for any Material-authoring tool, and of course, we're always  improving this with AI. To that end, we've got  an albedo generation and hard shadow removal nodes, and it can de-light real-world photographs. And we've added support for multi-light source scan processing. So we're working  on some new features, and we should have more  to show you at the next Unite. Which brings us  to our 2020 roadmap. I'd like to share  some of the high level plans for the rest of the year. One big push  is for Smart Material support. Right now, ArtEngine loads bitmaps and saves bitmaps. In between, you've built a graph, and that does some processing from input to output, but the graph only lives in our mentioned project. We're implementing  a compound node system, so you can take  large and complex graphs and break them down  into single nodes, and then use them within other ArtEngine graphs or export them as Smart Materials  to be used in other tools. This is going to fundamentally  change the way we'll use ArtEngine, both on its own, as well as in combination with other tools we're building. Let me give an example  of a Smart Material. We grabbed these two  marble textures from the library. They're super nice scans, but in their raw form. There is only so much  we can do with them directly. Our goal is to quickly turn this data into an infinite marble floor Smart Material. First, we need to do a little extra  work to the scan on the right. It was taken from a real-world  marble floor pattern, and the tile is actually  two distinct marbles, and we only want the texture  on the outside of the borders. So if we highlight  this area with a mask, we can use a mutation node to create a new unique  4k type of material from just those bits. Just as a quick throwback to that point cloud  visualization I showed you. This brings us to our other  big development for the year: combining the example-based workflow with painting. We've already developed  a form of our mutation that can be gathered by a mesh, so the new synthesized material  will conform to a UV-space, and you won't see any stretching  or seams on the model. This will let you paint your Smart Material directly on the mesh itself. Here's an example  of an off-the-shelf asset being updated quickly. It makes texturing your models super quick and fun. Anyway, this is what we'll be up to  for the next while, and we're looking forward  to showing you our progress at the next Unite. To wrap things up, we'd love to get your questions on the forum. And if you'd like to get started with ArtEngine, please reach out. And lastly,  thank you for tuning in.   [MUSIC]   