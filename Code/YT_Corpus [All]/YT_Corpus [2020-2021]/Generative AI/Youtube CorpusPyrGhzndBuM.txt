 hi my name is Joe Hicklin I'm a senior developer at the math works I'm going to show you what generative adversarial networks are I'm gonna show you how they work and how to make one using mathworks deep learning tools ganzar generative because they generate something in this example we'll be generating images they're called adversarial because we use two separate networks fighting against each other each one improves in sort of an arms race as the other one improves and it's this improvement that will allow us to train the generator to generate the images we want to make training again is pretty interesting we're going to use a setup like this we start with two networks here's our generator and here's our discriminator we'll start out just by feeding a random noise into the generator now the generator hasn't been trained yet so he'll produce images but his images are just random data he'll produce a whole bunch of images we'll take these images we're gonna call these fake data and we're going to send them to the discriminator and he's supposed to say whether they're real or fake but he hasn't been trained yet either so his output his garbage he'll call some real and some fake right next we'll take some real data and we'll feed that to discriminator but still the discriminator hasn't been trained so his output is garbage and so this is our first step but now we're ready to train them we want to train the generator to fool the discriminator so here every time the discriminator has called the input reel the generator has succeeded and every time the discriminator calls it fake the generator has failed so that gives us an error signal we can feed back to the generator and update its parameters to do better at the same time we want the discriminator to label all the fake data as fake and all the real data is real so everywhere the discriminator called the fake data real that's an error every time it called the fake data fake that's getting the right answer and the opposite for real data everywhere it calls it's real that's a right answer and what it calls it fake that's a wrong answer so that gives us error signals that we can feed back the discriminator to update it all right so we go back we update each Network and repeat the process and at first the networks do a very bad job but slowly they improve and each time we go around the passing in the data and doing the training the networks get better and better and better and after a great many times around the loop we wind up with a generator that's doing a pretty good job generating our artificial images and a discriminator that does getting pretty good at telling them apart usually we'll throw the discriminator away when we're done and just use the generator generate our images now that we've seen how gans work let's create one in MATLAB I'm gonna create and train again that generates artificial images of sunflowers this process is gonna have five steps we're gonna have to organize our data create our two networks generate a gradient function write a training script and finally train the two networks the gradient function is the gans specific part of this program and i'm going to start with that training a neural net is an optimization problem we're trying to minimize the errors in the output of the neural net this is usually done through gradient descent if we can find the rate of change of the errors with respect to the learner Bowl parameters we can change the parameters a little bit in the direction of the gradient and hopefully improve a little bit and we repeat that many times and hopefully converge to a good solution the core of this is finding the gradients we need to write a function that will calculate the gradients we need to train these things we need to write that function and it's already written right here the gradients function will take in the generator network and the discriminator network the real images and a noise vector and it's going to return the gradients for the generator and the gradients for the discriminator that we'll use later in the training loop now this function is going to work just like how a gans network works that I described earlier first thing we're going to do is pass the noise into the generator and get some fake images out of it we'll pass those fake images into the discriminator and get the discriminators predictions on the fake images we'll pass the real images into the discriminator and get the predictions on the real data once we've got that we're ready to calculate the errors the error in the generator is his error in the prediction on the fake images the error on the discriminator has two parts it's the sum of his errors predicting on fake images and his errors predicting on real images and we'll just add those two together now that we've got the errors we can call DL gradient DL gradient we'll use a form of automatic differentiation to do the heavy lifting for us and he'll calculate the gradient of the loss with respect to the learn herbal parameters in the generator and we do the same thing for the discriminator and its loss and that gives us the gradients that we're looking for from this function so there's our gradient function now that we have our gradients function we're ready to do the other four steps and here I've got a script that does all of that first I'm gonna get my data together into an image data set like we usually do then I'm gonna load the two networks from disk I built these networks earlier using deep network designer I'm going to specify training options and these are the same kind of training options you've seen in a lot of examples finally we're ready to train the network now normally you call train Network to do that and it has a training loop inside of it and does all that for you but you can't in this case because we're using a different gradients function and we need to we need to treat it specially so we're going to write that ourselves but it's the same sort of thing that's usually happening the first thing we'll do is generate the some noise that we're going to use later for validation keeping track of our iteration count then for each epoch for each time through our data we're gonna reset our image set and then we're gonna move through the data as long as the image set has data we're going to take the next batch of images this skips the last batch because sometimes they're not the right size so we'll just skip them and finally we're ready to go here we're going to take our sunflower batch of sunflower images and put them into DL arrays and move them onto the GPU now we're gonna do the same thing we're going to generate some random noise and put that in DL arrays and put that on the GPU and now we're ready to calculate our gradients we're gonna call DL F eval passing in our gradient function that we wrote earlier passing in all the other arguments to generator just and the images and that will calculate our gradients for us now we're ready to update the networks Adam update does that we give it the learner bowls for the network and the gradients and it will produce new learner bowls same thing for the discriminator we'll pass the other discriminator learner bowls and its gradient it'll give us new learner bowls and that's it we've taken one step this little section of code here just draws some random output every so often for our entertainment and then we repeat that loop and so that's all it takes if I start that running it'll move through all these steps and we'll see the output changing every few seconds as it calculates now it'll take about an hour to get a really good job here and we'll zoom through that through the magic of video so we let this go for an hour and you can see it's done a pretty convincing job of producing images of sunflowers we've seen how Ganz networks worked we've seen how to implement them using math works deep learning tools if you want to learn more about how to do this check out the links below 