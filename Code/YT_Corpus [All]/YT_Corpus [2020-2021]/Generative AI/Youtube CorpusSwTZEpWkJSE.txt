 Andrey: Hello and welcome to Skynet Today's Let's Andrey: talk our podcast, where you can hear from AI Andrey: researchers about what's actually going on of the Andrey: eye and what is just click bait headlines. Andrey: I am Andrey Kurenkov a third year PhD student Andrey: at the Stanford Vision and Learning Lab and the Andrey: host of this episode. Andrey: On this interview episode, you'll get to hear Andrey: from Tom Hennigan, a member of a technical Andrey: staff at OpenAI, working on the safety team. Andrey: Tom, along with Gerard Coplin, Moar Katz, and Andrey: others authored a recent paper scaling Andrey: laws for auto reggressive generative modeling. Andrey: He completed his Ph.D. Andrey: in a physics department at Stanford, where he Andrey: studied atomic motion in solids, advised Andrey: by David Rees. Andrey: Thank you so much, Tom, for making time to be on Andrey: this episode. Tom: Thanks for having me. Pleasure to be here. Andrey: So our focus will be on Andrey: your paper, which you co-authored with many Andrey: people at OpenAI we can say right away, Andrey: "scaling laws for auto-reggressive generative Andrey: modeling", which just came out a few weeks ago, Andrey: following up on a few other papers from Andrey: OpenAI, including "language models are few Andrey: short learners" which famously introduced Andrey: GPT-3 and also "scaling laws for Andrey: neural language models", which came out Andrey: also this year. Andrey: Before we dive into any of the details, Andrey: how about I just let you provide kind Andrey: of a summary of Andrey: what the paper is about and what are its main Andrey: conclusions. Tom: Yeah, so, you know, I think a lot of Tom: the field of machine learning is focused on Tom: getting state of the art results, and so people Tom: are trying to find ways of Tom: tweaking things to improve loss, Tom: accuracy or whatever their metric Tom: of choice is to to Tom: get a new state of the art result. Tom: And a lot of that happens sort of on the edge of Tom: technological progress or what's possible, making Tom: a model a little bit better, bigger, adding a Tom: little bit more data, those kinds of things. Tom: And so I think I view this as a way Tom: of kind of trying to that Tom: focus of this work is trying to kind of zoom out Tom: and say, OK, well, what is what are the trends Tom: in performance look like? Tom: Not if I just increase the data set Tom: or make things two times Tom: bigger or even ten times bigger. Tom: But what if I look over something like five Tom: orders of magnitude? Tom: Is there some sort of macroscopic trend Tom: that's happening there that Tom: might be informative? Tom: And somewhat surprisingly, we've been finding Tom: that in the case Tom: of measuring the test Tom: loss, it seems that the Tom: test loss as a function of one Tom: of either a dataset size or the Tom: amount of compute you invest in training or Tom: model size increases Tom: or decreases with Tom: power law, plus constant offset trend Tom: for any of those three things, so long as you're Tom: not bottlenecked by the other two. Speaker: So, for instance, if you have plenty of data Speaker: and you have lots of compute so you can train to Speaker: a convergence, loss as a function of Speaker: model, size seems to be parallel Speaker: plus constant offset for Transformers. Speaker: We first saw this in language, as you mentioned Speaker: in our paper, Speaker: neural scaling, scaling laws for natural language Speaker: models. But the emphasis of this paper Speaker: was seeing if that generalize to Speaker: other domains. And it seems like that is Speaker: the case. Andrey: I see. Yeah. Thank you for that great summary. Andrey: Yeah, I was just looking over a figure one. Andrey: And as you say, the idea here is that you do this Andrey: for a few different types of models. Andrey: So you do this, I think, for images, Andrey: for language, for text to image Andrey: tasks, image to text video. Andrey: And if I understand correctly, you have the Andrey: same kind of architecture. Andrey: The transformer architecture, Andrey: which is also a basis of GPT-3. Andrey: And so you apply the same Andrey: model with the same task of Andrey: the same loss of cross entropy. Andrey: And so we've kind of the same constant Andrey: across different types of data. Andrey: You get this parallel Andrey: relationship, which is basically saying that Andrey: as you change one of these variables Andrey: of let's say if for instance compute Andrey: every order of magnitude, you see a sort Andrey: of linear decrease in the loss. Andrey: Is that a correct description of the Andrey: main outcomes? Tom: Yeah. So I guess I would say the Tom: if you increase the compute Tom: by a factor of ten, you always see the same Tom: fractional decrease in the loss. Tom: But how big that fractional decreases, Tom: whether it's 50 percent or 30 percent or whatever Tom: it might be. And I'm making those numbers up, by Tom: the way, there probably aren't the actual Tom: depends on the domain. Tom: And so it's a it's a Tom: parallel relationship. Tom: So it looks linear on a on a log log plot just to Tom: just to be persnickety. Tom: But just to get that right. Andrey: Of course. Yeah. It's important to get to the Andrey: details and maybe now we Andrey: can dive into a bit of the details. Andrey: So you also have Andrey: in describing the results, pretty interesting Andrey: idea of reducible and irreducible loss. Andrey: And so this spiral relationship, I think Andrey: the main results are for reducible loss. Andrey: So can you try and explain to listeners Andrey: what are these reducible and irreducible loss Andrey: and how you get those? Tom: Yeah, so Tom: so we were we've been finding that the Tom: relationship between the loss and compute Tom: or model size or dataset size, whichever of those Tom: three it might be, is a power line plus constant Tom: offset fit. Tom: And so for those Tom: familiar with information Tom: theory, really, that suggests Tom: this kind of really nice interpretation, that Tom: the constant in that power love plus constant Tom: fit, so that the value you're approaching Tom: as you go to infinite data, infinite compute, Tom: infinite model size Tom: is sort of the true entropy Tom: of the data you're Tom: trying to model. Tom: It is the sort of Tom: the best, most uncertainty, Tom: a perfect model of that of that data Tom: could achieve, whereas Tom: the power component. Tom: So you have a constant that's the constant that Tom: additional power component is the, Tom: quote, reducible loss, which is that the Tom: component of the of the loss that can be learned Tom: and actually represents that the scale divergence Tom: the Kullback-Leilble divergence between Tom: the the models distribution Tom: of the data and the true data distribution Tom: itself. Andrey: I see, if I understand correctly, basically, it's Andrey: saying you understand the irreducible loss Andrey: is even if you had the perfect Andrey: model that could perfectly learn Andrey: everything from a training set just Andrey: due to the nature of the data, it can't Andrey: get to zero loss because there is some amount of Andrey: randomness that's inherent and you're never going Andrey: to overcommit. Andrey: And so you can you can find to some Andrey: extent this irreducible loss and that becomes Andrey: a constant offset in your Andrey: power relationships. Andrey: On the on the graphs, there's a line Andrey: that's log-log linear. Andrey: And so what you're plotting there is Andrey: the irreducible loss, Andrey: sorry the reducible loss, which doesn't include Andrey: this impossible to Andrey: get away from irreducible loss. Andrey: Is that right? Tom: That's exactly right. And what we're Tom: suggesting is that perhaps the the reducible Tom: loss is really the important quantity Tom: here is sort of telling you how close Tom: you're getting to modeling the Tom: true distribution of the data. Tom: And sort of if I can give an anecdote Tom: of how I sometimes think about the irreducible Tom: loss, they say that their language, Tom: though, in this case, these are utter Tom: transformer's. So you're just trying to predict Tom: what the next words are going to be. Tom: If I read the first chapter of a murder Tom: mystery. Tom: No, no one in the world and no one in the world Tom: could say with 100 percent certainty that they Tom: knew who the murderer was, that they knew it Tom: was Professor Plum in the Tom: study with the candlestick, Tom: they would have some probability distribution Tom: over who the likely murderer Tom: was. And there's just some Tom: intrinsic limit to how Tom: how calibrated or how how good you can make that Tom: predict how sorry there's a limit to how Tom: accurate you can accurately you can make that Tom: prediction. And so that Tom: represents the irreducible loss. Tom: So that's sort of like unachievable to do Tom: better than that. So perhaps what's the really Tom: important metric that we should focus on is that Tom: is the component, the component of the loss that Tom: can be learned. So the reducible loss. Andrey: Great. Andrey: Hopefully is clear to voters, I think Andrey: that makes a lot of sense to me. Andrey: And then again, a little bit Andrey: more into the details, I think there's a few main Andrey: kind of quantities or quantitative Andrey: results you have, and those are Andrey: for loss as a function Andrey: of model size, loss as a function of compute. Andrey: And then also you have something quite Andrey: interesting, I think, which is Andrey: the optimal model Andrey: size as a function of compute, which Andrey: I think you call and see for a given Andrey: compute budget, finds the Andrey: optimal model size. Andrey: And you also show that this optimal Andrey: model size can accurately model Andrey: as a pure power law. Andrey: So can you just tell us a bit more about these Andrey: different quantities and how Andrey: consistent they are and what have been main kind Andrey: of exciting things for you? Tom: Yeah. So first to Tom: just briefly describe what what we mean by Tom: the optimal modifiers for compute budget. Tom: So you could imagine that if you Tom: use a very small model, Tom: you and invest a lot of compute Tom: and it's to train it for a very long time, Tom: the loss will go down only so much because Tom: your model capacity is limited and a small model Tom: can learn only so much. Tom: But in contrast, if you use an exceedingly large Tom: model and invested, Tom: say, the same compute budget, you might only Tom: be able to take one step because you have so many Tom: parameters and need to do so many floating Tom: point operations that you're only able to look at Tom: one batch of data. And it's it's hard to Tom: imagine that a model could learn much by only Tom: looking at one batch of data. Tom: So for a given compute budget, there is some Tom: Goldilocks region in between Goldilocks Tom: choice of model size where Tom: you're able to look at enough data for the Tom: loss to drop significantly, but is going to Tom: before the loss starts to level Tom: off and asymptotically approach whatever Tom: the converged performance is going to be. Tom: And so you can using these results, you can Tom: extract for the trend. Tom: Again, for decoer only transformer and Tom: these different domains, images, videos, Tom: math, language. Tom: And a surprising thing is that Tom: it looks like the optimal model size is a Tom: function of compute, budget is a power Tom: law. And not only that, but the power laws Tom: are surprisingly similar for Tom: all these, seemingly, to me Tom: at least pretty different. Tom: Romain's all of them have an exponent that's Tom: right around point seven now. Tom: And when I say similar, I mean some of them, the Tom: actual values might be different by an order of Tom: magnitude. And so but when Tom: you look at it on the log plot, Tom: these lines are seem to be Tom: all almost on top of each other, which is Tom: yeah, it came as a real surprise to me. Tom: And it feels like it wants to tell us something Tom: about you. Tom: I don't know some some theoretical thing that we Tom: don't quite understand yet which which I think is Tom: exciting. Andrey: Definitely. Yes. Interesting to see these sort Andrey: of trends across different data types, which I Andrey: guess is a whole kind of exciting thing about the Andrey: paper. Andrey: Moving on to a little Andrey: more specific detail, Andrey: which I found interesting. Andrey: You say Andrey: in a paper that when generative image models Andrey: are fine tuned for imaging at classification, Andrey: you find it law for classification loss Andrey: versus model size. Andrey: So again, basically it's saying that for Andrey: if you increase the model size of a 10, you get Andrey: some fractional decrease, 10, 20 Andrey: percent for classification loss consistently, Andrey: which is pretty cool. You can basically do better Andrey: and better. Andrey: But DL here is that Andrey: that happens even beyond the model size where Andrey: you approach irreducible loss for generative Andrey: modeling so you can go beyond Andrey: use of loss for genitive modeling. Andrey: And you also say that you conclude this approach Andrey: to irreducible loss does not necessarily indicate Andrey: diminishing returns or representation, quality or Andrey: semantic content, which is interesting. Andrey: So the point, if I decide correctly, Andrey: is that you might interpret Andrey: the law as being diminishing Andrey: returns basically Andrey: as you increase by 10. Andrey: So you go from a million to a billion to Andrey: Trillian, you get the same return every Andrey: time. Andrey: Right. Andrey: Which maybe is bad because that means that Andrey: once you get the billion, it's hard to get to Andrey: trillion to get another 10 percent. Andrey: So can you speak with more about this image Andrey: classification and the question of if this Andrey: indicates diminishing returns? Tom: Yeah. So I think for me it relates Tom: to the constant in that Tom: parallel political. Tom: Equation and which, Tom: as we said earlier, corresponds to the Tom: irreducible loss. So if if Tom: you were looking at the reducible loss, then Tom: what you said would be true, it might be the case Tom: that every time you 10x your investment Tom: and compute our model size or whatever Tom: it might be, maybe you would improve Tom: your decrease your loss by another 10 Tom: percent or 20 percent or whatever it is. Tom: But actually when you have the power line plus Tom: constant offset fit, those even Tom: that fractional return begins to diminish because Tom: you begin approaching the irreducible loss Tom: and asymptotically approaching that. Tom: And so it might be the case that you're Tom: increasing your model size by one hundred times Tom: or ten times, 10 times, 100 times. Tom: That loss is only decreasing by maybe one Tom: percent, three point one percent. Tom: And so from that, if you Tom: weren't looking at these macroscopic trends, you Tom: might conclude that, oh, well, Tom: that these I'm Tom: as I'm increasing my model size, my losses and Tom: going down by very much. Tom: So I think my model isn't really Tom: getting any better. And my downstream past Tom: performance is also probably not improving. Tom: But but actually, that's not the case that Tom: as we see here, when we fine tune it for the Tom: classification objective, the Tom: performance continues to improve in a smooth Tom: power-law kind of way, both for Tom: the classification loss and for the Tom: classification error rate. Tom: And so I think this, again, suggests that Tom: maybe the important quantity is not the Tom: total loss, but specifically the reducible loss. Tom: Because if you were if you did that same looked Tom: at that same trend, but you were looking instead Tom: of at the total loss, the reducible on the Tom: irreducible loss, excuse me, you'd see Tom: that that was consistently decreasing Tom: by 10 percent every decade. Tom: And so that would maybe be a better indicator of Tom: what sort of downstream task performance you Tom: should be expecting. Andrey: Mm hmm. Yes, and Andrey: yeah, I'm curious in particular about image Andrey: classification, because this is a pretty concrete Andrey: task we can focus on. Andrey: And, you know, to some extent, Andrey: you say matters in products and so on. Andrey: Already that image classification Andrey: for this is what I know is the image Andrey: classification saying what is in this image Andrey: and a lot of online APIs exist to do it. Andrey: So I would imagine Andrey: that the companies providing these Andrey: APIs care about performance and want Andrey: to try and do better as possible. Andrey: And so it's very interesting that you have Andrey: this Parlo of as a model increases, Andrey: classification also Andrey: improves by some consistent Andrey: ish number. Andrey: And additionally, you have, I think, even more Andrey: interesting results here. Andrey: So you have something that Andrey: I have also shown to some extent. Andrey: I think that as you go to bigger and bigger Andrey: models and Andrey: here you are pre-training them on this generative Andrey: task of, you know, learning representations, Andrey: then you find using them for classification and Andrey: you find that larger Andrey: pre-trained models fine tuned significantly Andrey: faster. So you need to look at less data Andrey: to achieve better results. Andrey: So, yeah, in some sense, it's easier to optimize Andrey: larger models and actually Andrey: perform better over time. Andrey: So, yeah, I'm curious if you can highlight Andrey: some results, which you think maybe from a Andrey: programmatic perspective, from an operational Andrey: perspective, of wanting to know Andrey: if you're building a product, if you're building Andrey: a new one, that to do some task. Andrey: I think this is one of the results that seems Andrey: interesting, that it's easier to optimize larger Andrey: models maybe for imaging as you Andrey: just go big around. Andrey: You have a results like that working from Andrey: this that you think you could highlight. Tom: Yeah. Tom: So I guess I Tom: would start by caveating that the way we Tom: do the training is that, you know, these Tom: model, we're not achieving any thing close to Tom: SOTA. Right. So like, I don't know on this graph, Tom: that classification error rate is maybe Tom: 30 percent or something, which is not exactly a Tom: state of the art for imaging at these days. Tom: But but I think what you're alluding to is Tom: like maybe these macroscopic trends might Tom: that the results here do maybe suggest Tom: some practical tips and tricks that could be used Tom: for for for practitioners. Tom: And I think that the point you raised Tom: about bigger models being Tom: more sample efficient is something that shows up Tom: again and again and in these in these Tom: works. Tom: And so and I think Tom: that the generative free training did Tom: it. We can see that it appears that generative Tom: pre-training did give Tom: these models. Tom: Did result in these models having good Tom: representations that transferred well to then the Tom: downstream Task Classification and that they Tom: picked up on that classification task pretty Tom: quickly with relatively few samples, especially Tom: for larger models. Tom: And I guess one other practical point Tom: is you might think, oh, well, then a bigger Tom: model must be is just the way to go. Tom: But a practical point for Tom: many companies is that a lot of their Tom: compute cost is not in training, but also at Tom: inference time. And a bigger model Tom: will cost you more at times. Tom: That's just so you have Tom: to weigh those two things against each other Tom: if you're at all of Tom: your cost is in training. Tom: And you're not worried about the constant Tom: reference time, then? Tom: Indeed, it can in many instances make sense Tom: to use a larger model. Tom: But if you're primarily Tom: constrained by inference costs, Tom: you may be more frugal Tom: with how big of a model that you want to use. Andrey: I see, very interesting, yeah, Andrey: and speaking of that Andrey: against the images, one thing I found interesting Andrey: is you have a section here called The Andrey: Inconsistency in Compute and that assize Andrey: counting laws. Andrey: So for some data and I think here it Andrey: was particularly images Andrey: you showed that actually here as you Andrey: scale the dataset size or the amount Andrey: of data seen by the model, it's not quite Andrey: a law. So you get a Andrey: linear relationship for a while and then it sort Andrey: of tapers off and you get almost like a natural Andrey: shape for Andrey: loss as a function of dataset size. Andrey: And that's something that's quite interesting Andrey: also for practitioners. Andrey: And if you say a company and you're collecting Andrey: data, you want to know how much more data Andrey: to collect, you're going to get more payouts Andrey: for additional data. Andrey: And as a law would be very interesting. Andrey: So can you speak to what you found with Andrey: respect to loss as a function of Andrey: dataset size and maybe the inconsistency Andrey: that you have? Tom: Yeah, so for Tom: we found that the loss is a function of dataset Tom: size was a power-law plus constant offset. Tom: And so the L shape you're referring Tom: to, the bending the curve Tom: on this log plot is a result of Tom: that power approaching the Tom: what we're calling the irreducible loss, which is Tom: causing it to ascend towards this constant value, Tom: which is the constant in the Tom: in the power law plus constant Tom: equation. And we think represents the irreducible Tom: loss. And the Tom: inconsistency you're alluding to is something Tom: that we saw four images Tom: here in previous work. Tom: We also saw it for for language where. Tom: So if I can try to describe it, Tom: it's a little bit of a mind work, but so Tom: you can plot the loss you're going Tom: to achieve as a function of dataset size if you Tom: allow the model to look at as many epochs Tom: as it wants. Tom: Now, if you're limiting the dataset size, Tom: eventually it will overfit. Tom: And so here we're just using early stopping Tom: to tell us what the best achieved Tom: loss was. Tom: So that gives you one curve of loss is a function Tom: of dataset size. Tom: Another way you can get lost as a function of Tom: klodt loss is a function of data. Tom: Set size is to Tom: look at the loss that a model Tom: would get if you let it look at that dataset Tom: size but only go through one epoch. Tom: So you only let it look at each example once. Tom: And you could do that for a handful of different Tom: data set of a handful of different model sizes. Tom: And so the smaller models will plateau at Tom: larger values of the loss because they're Tom: constrained by how many parameters they have and Tom: then has bigger and bigger models Tom: will have lower and lower loss. Tom: And we spoke about earlier, they seem to also be Tom: more sample efficient, so efficient. Tom: So the loss drops faster. Tom: What's interesting is that we weren't able to Tom: actually see them. Interesting. Tom: If you extrapolate those two trends out, it looks Tom: like they're going to intersect, which would Tom: suggest that at some point. Tom: Yeah, I don't know. Tom: You can interpret it as we will. Tom: I think I Tom: might guess that at some point. Tom: You were going to be yeah, maybe maybe Tom: in the long run, it's going to be that Tom: this sort of loss is a function of data size Tom: is going to be constrained by the multi epoch Tom: case where actually after the model Tom: has seen some amount of data, Tom: some fraction of the epoch that looking Tom: at more is is actually giving diminishing Tom: returns. Tom: But it's hard to say. Tom: I have to say it's a it's an interesting Tom: question. I don't I don't know what's going to Tom: what's going to what would happen as Tom: if you were to go to larger Tom: models with with more data. Andrey: I see. Oh, I think I mischaracterized Andrey: it a bit, so inconsistency is basically Andrey: if you plot all of these linear parallel Andrey: lines of loss is a function of data set and Andrey: loss of function attribute. Andrey: They are slightly different slopes, which means Andrey: that at some point one of the slopes has to, Andrey: the last of which is compute will Andrey: have to maybe overcome or Andrey: overtake the data. Andrey: So. So that is interesting. Andrey: As you see, it's Andrey: then I think, as you say, I think Andrey: kind of it's not too much research Andrey: of this vein. Andrey: There is starting to be more of it. Andrey: And you do cite a lot of the relevant literature. Andrey: But these kind of microscopic or microscopic Andrey: trends that are empirical Andrey: but seem to hold are Andrey: definitely interesting to to be aware of. Andrey: Yeah. Tom: Yeah, I think for me, you know, coming from Tom: physics, you know, you see you see a power Tom: line, you think, oh, it feels like it wants Tom: to tell us something. Tom: And I think there's a lot about AI Tom: machine learning, how neural networks are working Tom: so well that we don't have a good grasp Tom: on theoretically. And so, yeah, I don't know, Tom: maybe maybe results like this will help us make Tom: some some progress in that in that direction. Andrey: Indeed. Actually I think now maybe Andrey: is a good time to acknowledge once again, this Andrey: is a big team project, which is Andrey: I think two dozen offers, Andrey: a large number of offers Andrey: and maybe I don't know. Andrey: I'm curious if you can zoom out a bit from the Andrey: results and can you tell us a bit of Andrey: what was involved in getting this paper together? Andrey: I mean, infrastructure wise, experimental Andrey: design wise, all of the different details. Andrey: What are some of the things you can think Andrey: of that were really big efforts on this? Tom: Yeah, I mean, obviously, we Tom: there's, you know, Tom: a fair amount of engineering Tom: that goes into building these Tom: models and making making it Tom: possible for us to to train them. Tom: There's a huge there's a huge effort. Tom: And this work Tom: really wouldn't have been possible without all of Tom: the people really working Tom: out the details there to make that make that Tom: happen. Tom: And I think I was also Tom: fortunate to have really Tom: great colleagues and Tom: mentors in terms of research direction Tom: who thought these were interesting Tom: ideas to pursue. And I think it Tom: wasn't as obvious to me and maybe at the Tom: beginning of the project. But in hindsight, I Tom: think it's really, really cool. Tom: And I'm glad we did it. Tom: It's. Tom: Yeah, I mean, I feel pretty Tom: lucky. I do feel like it was a pretty Tom: collaborative effort and and yeah, Tom: it's it's been great. Tom: It was a great experience. Andrey: Great to hear. Yeah, definitely. Andrey: When there's such a big team and it's good to Andrey: hear that it's kind of all came Andrey: together. Andrey: And I do think you got Andrey: some very interesting results now Andrey: speaking a bit about architecture Andrey: and sort of results. Andrey: I was also wondering about some of the things you Andrey: might do next or what you thought of doing Andrey: as far as some of these empirical studies. Andrey: So one thing that I find interesting is Andrey: you evalu hear of this transformer architecture Andrey: and you show it with different models, sizes Andrey: and yeah, I wonder if if you've thought Andrey: about maybe going and evaluating Andrey: these trends for, Andrey: let's say, programmatic design choices, the one Andrey: thing I was thinking is for very large models, Andrey: maybe it's not usable at Andrey: deployment time. Andrey: So people would use like pruned models Andrey: or optimized models. Andrey: And there to be interesting,to these transformer Andrey: or do these proven models is inherently Andrey: worse. Andrey: So, yeah, I was wondering kind of what Andrey: sort of things where you're thinking of perhaps Andrey: looking into next week, possibly Andrey: improving models or anything else, really? Tom: Yeah, I think I think pruning is Tom: is is definitely an interesting Tom: research direction here. Tom: You could you could ask what Tom: how how is this how are these Tom: trends changed as as I Tom: pruning the model after the fact more and more Tom: and there's a lot of practical Tom: knowledge to be extracted there. Tom: I think that would be interesting. Tom: You know, I think one obvious thing is Tom: we only did the Koetter Tom: only transformer's here. Tom: And in many of these Tom: problems, that's probably not the Tom: best architecture to be using language being Tom: an exception. Of course, Transformers of Work, Tom: you know, maybe is not the best, the Tom: only transform. Tom: Obviously, there's other things out there like Tom: Bert, those sorts of things. Tom: But something in that vein works seems to work Tom: quite well for language, but for generative Tom: image modeling. Tom: The decoder on the only Transformer probably Tom: doesn't have is not the best choice of Tom: architecture. Tom: And in our prior paper Tom: on the scaling allows for Tom: language modeling, we looked at comparing Tom: the trend for the Transformer Tom: and the LSTM. Tom: And I think that was informative in that Tom: if you looked at loss as a function of models, Tom: this model size, they Tom: seem to have roughly the same exponent but Tom: in the power law. But the Tom: LSTM had a different multiplicative constant Tom: in front of the power law. Tom: So on a rug plot, they were parallel lines. Tom: But at every point the CM Tom: had higher loss than Tom: the transformer. Tom: And so seeing it Tom: and that I mean, that's another thing that's Tom: surprising that the two exponents were the same. Tom: But then naturally, another natural Tom: question is to wonder if we use something more Tom: natural for generating image modeling than it Tom: decoder only transform if we use maybe pixel Tom: CNN or something. Tom: In that vein, would it have the Tom: same content and be offset in the same Tom: way? Tom: I'm pretty curious about that. Andrey: Exactly. I was actually going to ask that next Andrey: is if you're thinking of going into our Andrey: architecture architectures like a new one, that's Andrey: so glad to hear that. Andrey: Sort of also the obvious next Andrey: step. Andrey: Yeah, absolutely. Andrey: I think that's pretty much all I had Andrey: to ask was a ton of interesting Andrey: details in the paper. So, again, just to mention, Andrey: the title is scaling laws for Andrey: auto aggressive generative modeling that you can Andrey: find that an archive and take a look at yourself. Andrey: It's it's quite readable despite, you Andrey: know, a little bit of technical. Andrey: I think it's the empirical results are pretty Andrey: easy to get. Andrey: Is there anything else you'd like to mention or Andrey: highlight from the paper? Andrey: We haven't touched on yet. Tom: Yeah, I think those are the big things. Tom: I mean, I guess I would echo what you just Tom: said, which is that this is this paper is Tom: primarily focused on empirical results. Tom: We do make some conjectures about what? Tom: About how to interpret some of them Tom: excited about Tom: others pursuing Tom: theoretical work related to this, to Tom: have some ideas for why all these things Tom: might be Paola's. Tom: My colleague on the paper, Jerrett Tom: and Some is one of Tom: his students has worked on. Tom: Trying to a theory for Tom: as sort of a first pass Tom: theory for why this might be the case, Tom: but I think there's a lot of more interesting Tom: stuff to pursue, not only in Tom: this line of empirical work, but also seeing if Tom: we can extract some theoretical Tom: understanding from it that I'd be excited about. Andrey: Definitely. That'll be exciting to see. Andrey: Well, in that case, I think we've got Andrey: pretty much a good overview of the paper. Andrey: Thank you so much for joining us on this episode, Andrey: Tom. Tom: Yeah, thanks. It was a pleasure. Andrey: And thank you so much, listeners, for being with Andrey: us on this episode of today's Let's Talk Andrey: AI podcast. Andrey: You can find articles on similar topics Andrey: to our newscast today and subscribe to our weekly Andrey: newsletter. That's kind of the dotcom. Andrey: Subscribe to us wherever you get your podcasts. Andrey: And don't forget to leave us reading. Andrey: If you like the show, be sure to tune Andrey: into our future episodes. 