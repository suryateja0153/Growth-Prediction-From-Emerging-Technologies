 hi everyone it's nono here today i want to share with you a video lecture delivered by nate peters at the digital futures workshop that we organized with jose luis garcia del castillo back in june 2020 in this lecture nate is going to talk about his master's thesis that he did at the harvard graduate school of design when he did a design technology master's degree and he's going to discuss the methods that he used to develop an ai-powered web application that produces architectural floor plans from user drone shapes without further ado let's get started so quick about me my original undergraduate university studies were in architecture so i had a bachelor's of architecture degree from iowa state towards the end i sort of jumped onto the the computational design bandwagon we uh had just opened up a kind of robotics research lab my my final year so i pivoted and decided not be an architect and go to grad school i went straight from there to the gsd and i was a master of design studies tech student for two years the project that i'll show here was my master's thesis uh and then in the middle of that i started at autodesk as an intern i think right as no no starting full-time um and i think jose luis there's like a weird kind of like orbit between the the group we work in that autodesk and the schools in boston um but i was there as an intern and then right after i graduated i started uh full time and i've been there ever since so my and before i dive into the research step i'll give you just kind of a super quick overview of what uh what i'm doing kind of day to day now uh so i'm in a department at autodesk called the architecture engineering construction generative design group um which essentially says you know we're building kind of new cutting edge tools for for people in the ac industry for workflow optimization machine learning uh the project that i've been working on started as a beta called project refinery and then just within the last uh three or four months we uh we've launched as kind of a rebranded now actually real product tool called generative design so red it's the first place that it's available uh but essentially it's a uh it's a multi-objective optimization tool i have a quick video i can show you guys but uh the idea is that you know there's there's a large group of people now like us that have you know computational design skills when they go to dynamo go to grasshopper but there's still a large uh a large portion of architects engineers that spend a lot of time in modeling software but don't have any engagement with design automation so the kind of high level concept of generative design is the tool is first building a platform that someone who understands automation and optimization they have a way to kind of share that knowledge and share that work with other people that don't necessarily engage with it and another major goal of this is that if you have you know repetitive problems or uh kind of small scale things that you do every day as a designer and engineer that consistently require kind of reevaluation um but are at the same time automatable it makes it easier to kind of bundle up these these optimizable problems in a sense uh and reuse them across projects and avoid kind of the fatigue of constantly managing dynamos groups and grasshopper scripts and updating them from projects and having to manage them and kind of all the overhead that comes along with that so this is uh you know v1 or almost v0 this it's only been out for a couple months but we're super excited about it but that is not what we'll be talking about today um so the thing i want to focus on for this section is projected back in the gsg that was focused more on machine learning and participatory design so participatory design is essentially the the process of design that actually engages the end user in the process so with an architecture you know the person that's going to live in the house or use the space actually has some agency in determining that space it's not a super common problem you know it's not a common way of engaging the design process normally because it's complicated enough as it is uh but the the kind of overlap that i was interested here was uh proposed systems for mass housing that should have engaged with participatory design but for whatever reason really didn't and there's a really long and sort of interesting history of you know kind of like failed utopias like failed mass housing projects um you know architects like uh gropius and frank lloyd wright and jonah friedman and essentially everyone you can think of has had some sort of entry into this realm um and they'll have a lot of kind of shared overlaps the most interested in um kind of fall into separate categories the one how can you guys still hear me i just got the good okay yeah you've crossed for a moment you're good zoom uh okay i got the your internet is unstable pop up um so what i was interested in first was the physical system that the gropius designed it was a panelized kind of lego like adaptable architecture system that they actually manufactured for a few years but built very few examples of uh an issue is that there was really no consideration given to how someone who isn't an architect would customize it you know for all the effort they put in and making it to be something that could be reconfigured and reused and sort of you know freed from the bounds of traditional construction problems they didn't account for the fact that you still only have so many architects and a much larger body of people that would you know need houses uh and this is you know the 1940s it's very very far pre-computer but it's it's interesting because the the efforts in this realm really haven't changed even in close to 100 years now that you know the prefabricated house still sort of looks the same the second project i looked at for a long time was by iona friedman um and it approaches it from the other angle it's very theoretical but it was looking at the system you know how would someone who isn't an architect design a house for themselves if they don't understand some of the underlying uh you know professional expertise i guess for doing that and so even in the 60s he had this kind of cool retro concept that he called the flat rider where it was this sort of glyph like physical typewriter they didn't actually stamp out uh kind of coded pixels like a like a computer punch card that would act as a sort of blueprint that you could hand to someone who's engaging in a self-built architecture project and they can you know sort of keep everyone within the constraints of the system but actually design things that are new um which also i think has a bunch of really really interesting concepts built into that but it was theoretical and there was never really a physical system to connect it to and so nothing was ever realized you know beyond the drawings and so the place that i sort of inserted the research that i did was you know taking all of that conceptual background now that we have computers and the internet and actual ways to disseminate information like this and to build collaborative platforms what would it look like to take a constrained kit system and make an interface that someone who isn't an architect could actually potentially use so i built kind of two things one was the the neural network side of the project the the actual prediction algorithms and the second was kind of a prototype for an interface um this is kind of a small preview i'm going to show the other back end first but this is the final product um it's a javascript 3.js interface where you can drag the zones and essentially define an area that then an algorithm will fill back in with some kind of prediction for how an architect might program the space kind of based on the data that it's seen before but before we dive into that i'm going to look a little bit kind of further into you know how the neural network can actually understand a thing uh so jose louis talked about this a little bit um and usually in these lectures everyone has some version of like the number handwritten data set um but the thing that was kind of a lightbulb moment for me was that the you know the way that someone who knows how to read sees a number and recognizes what it is is something that you never really lose you know if you if you learn what a four is you can look at most things that are supposed to be for and and sort of classify that for yourself the way that that works for a neural network is that it has to see many many examples of things that are fours and understand them at a much more granular level to be able to reliably do that kind of at scale and so there's this this kind of shared process in you know image algorithms or in image neural networks where we're taking input data and they're breaking it down into lots of little tiny pieces and sort of understanding you know adjacency relationships about what's happening you know from this pixel to this pixel on all sides and then kind of mapping windows across the entire object and after doing that you know lots and lots of times you sort of build up this kind of innate understanding that's totally unique to just the way that neural networks operate and so at the lowest level you know this is what when you know you consider uh an algorithm seeing an image this is sort of what it sees so that nine is really just two greater values from zero to one and when you're putting something in and getting something out you're just getting an equally sized prediction that is a different matrix of zero to one um so kind of building this back up into fix depicts the things that we're using it's just essentially this concept stacked into three layers which is just rgb values so an image of essentially anything you can imagine can be kind of interpreted in this way just splitting into channels looking at it purely as numbers and then learning relationships about you know mapping images you know a to image b um or really in whatever order you want and so the you know a lot of what i was looking into was also strongly influenced by jose luis and no now because they were looking at some of the stuff around the time that i was at the gsd and in addition to the stuff that i learned from them there was this example that's now super well known kind of associated with the pix depicts project that's this facades experiment and so what's happening here is that they have a hand tagged data set so someone has gone over a bunch of pictures of elevations of i think it's google street view data uh but they've gone over it with sort of a tagging tool and they've drawn colorized zones that represent you know what types of architectural elements represent these things on the building um and what's interesting here is that this actually isn't training data this is what comes out of the network so these are pictures of buildings that don't exist sort of imagined by the neural network and so when i was you know thinking about what the the logic kind of the system behind my my drawing interface could be um i was originally imagining it would be something more procedural you know something more like a grasshopper script but once i was exposed to these sorts of you know applications of creative machine learning i decided to sort of pivot and see if you could convert this concept into something that worked with floor plans so for the experiments that i did the the high level kind of process was you know start with a batch of houses that sort of look like the the type and size of of plans that i was hoping to generate build up a data set from scratch um and then the end result the way that someone would interact with the model is then fill in a solid region that just represents the the site plan or the kind of zone to fill with rooms and the neural network would give back some prediction that would be some kind of combination of the examples that it had seen um so in an abstract way it's you know based on all the houses that i've looked at before this is how we think an architect would divide up this space assuming that you know the houses that it's been trained on were initially designed by architects or someone who was a professional the other thing that's changed dramatically just in the two years since i was doing this which sounds weird because it's like not a long period of time but in machine learning time that's like the decade uh there was really no publicly available data sets for floor plans uh someone asked in the chat earlier um there there's one now like a large one that i'm aware of called cubica but just for the type of data that i knew i was looking for there really wasn't anything out there so i ended up building one from scratch which wasn't as terrible as it stands it was like two or three days it's like a lot of coffee but i ended up with this data set of about 110 floor plans that then with some augmentation and um you know kind of creative python trickery ended up being about three or four hundred houses um i'll kind of buzz through the process of that so for building up the data set you know this is an example of like one kind of raw image i got from google i found a bunch of different websites that have house plans so it's a combination of stuff but essentially i was looking for single-story one-bedroom houses and the idea was that you know the the model space is sort of bounded to a certain scale and then to make the predictions as accurate as possible i made sure all the data sort of fell within that scale as well um so all the houses are usually within like kind of a 40 to 40 foot bounding box and represent like usually within a like fewer than three or fewer bedroom house that would normally exist in the us so with the scraped image it starts at kind of a random scale and the the time-intensive process was essentially going in there actually reading the drawings or reading the drawings to find the scale and then actually scaling these into world space so i was using rhino and i if you're interested actually i have the model and all of the tagging data in the git repo that we'll share later but the process was kind of getting all the data into a shared uniform scale and the function of that is that at the end when training the model you know all of the even once it's been rasterized it's an image and kind of no longer a drawing the the pixel still represents some kind of real world approximation of space so bigger houses that take up more pixels actually have more rooms and the network actually kind of learns the relationship between the number of separate objects and the the size of the space that you're filling and so once everything was scaled um it was just drawing rooms kind of color tagging everything i made some extra kind of experimental representations and actually at the time i didn't really do anything with this but one of the new experiments that we did in the past month actually overlaid these two things to kind of create room separation and actually produce totally different interesting results and then the the input data set here so these are your a images and these are your b images and that's the kind of the pair that we then use to generate new predictions the the color palette was not a design decision this is also common to a lot of like image-based kind of prediction projects that you'll see the the idea here is that you know the colors are as far apart from one another and on the spectrum as possible since you know these things will get mapped from integer 255 value pixels down to a space of zero to one that flattening process there's some loss and so the idea is that if the the colors are kind of equidistant if you can imagine them on a color spectrum the the network won't get confused when when predicting if you know a pixel is this type or this type so this was the results of like one of the first models that i trained um and i think the you know the takeaway that i had after this project was that like like a neural network isn't a very good architect but it's a really fun tool to play with if you want to explore designs in a way that you don't normally do so it you know i would recommend anyone like go like build the next like holistic modeling platform with pix2pix in the background like i don't think it's like that's the takeaway from any of this stuff but it it is a super interesting kind of mode of designing and then working with uh working with constraints that's totally unique to machine learning and what i find really cool about you know actually kind of looking into the results and seeing what's happening here is that sometimes they make a lot of sense sometimes they don't make any sense at all um but you can sort of fill that middle space with you know human intuition and so as you as you place more objects and kind of block in more of the system the algorithm usually gets kind of smarter about then how to make the next decision in the chain you know so for this one you know the training image was that there's a bedroom here a hallway two bathrooms a kitchen and then a large deck area the prediction for that same shape was a gigantic closet on the top um a huge kitchen a deck in a place that makes sense but you've still got a bedroom the bedroom beds closets and everything's like walkable so it i'm always just interested so as you draw things that either are similar to what the model is seeing or if you draw things that are you know nowhere near what the model has seen it'll still occasionally or actually can often do something that is still functionally workable as as a as an architectural plan um but it doesn't know what's a good plan so that's kind of where human intuition still comes back in and there's there's the need for a feedback loop and so these are some you know totally imagined shapes you know so kind of intentionally drawing things that the network hasn't seen before and and seeing what happens so this is what the interface looks like so on the you know kind of stepping back from the back end the intention of the project was that you know the user doesn't really interact with the model in that way you know they they kind of are influenced by what the model is doing um but see it is more kind of cleaned up uh representation um this you know is kind of half prototype you know prototypical the you know as you could tell with some of the images you know there's a huge variety of how noisy or or high quality that the data out of it is so the process of actually interpreting that data automatically and cleaning it up and then building clean geometry like this is like probably at least as hard as doing the ml work in the first place um so this is you know these representations are sort of aspirational or that they were at the time um but the the concept is that you know you can use uh you know a machine intelligence like like a neural network to take the constraints that someone may have you know so it could be you know i have a lot that has the shape that's very atypical and would require the expertise of an architect and instead see what you can sort of synthesize from from the model oh shoot okay this is on can you guys see my youtube screen cool um and this was another sort of demo i put together back at the time and the idea is that you know so even if you can't get the perfect answer every time which no one really expected but when sort of presenting things in this way and you talk about automating any kind of you know manual process when you do it and you say like well no this doesn't really fix it you still have to do some of the work yourself but usually you know people are confused by that um but i i always thought it was sort of interesting this again this kind of new potential way of engaging with these models where it might solve 80 of my problem or it might give me an idea to approach the problem in a way that i wouldn't have otherwise and then you can you know you as the human user at the end can then interpret the results and sort of fill in that last 20 10 um so this is you know kind of an especially noisy output that i picked and instead of saying like oh you know this one's no good i'm gonna look for something else in the design space that's more more clean or interpretable you could instead say well i'm actually just gonna use this as an underlay and trace over and kind of fill in the gaps for the model and that's kind of what closes the idea of the feedback loop yes yeah the user would draw in their their shape whatever it is and then they could use the the tools that are built into the front end to trace over it and eventually end up with something that they could kind of take on to the next phase of the design process theory is there anything in the chat i don't know jose luis that questions or anything there are some questions um you may want to open it up for five minutes to answer questions i've been responding to some questions that have been okay um i think well yeah so this was yeah this was my last slide um somebody was asking somebody was confused about when you showed the slide or you had the floor the av and the prediction somebody was a little people were a little confused about what was the input was the output and what was it was so oh sorry yeah that probably wasn't the ground yeah did you clarify that a little bit so the the actual image pairs so for the the model that was generating those underlay predictions and the ones that i showed the a image you know the input image was the black infill and then the b image is the tagged colors um well and actually as i say that the you can you can tell the model to train either way it's not the directionality isn't really important you just you tell it to go b a or a to b so if you're thinking of the facade sort of as an example the you know the relationship is the same so the the color tagged image is the the input and then this is the actual training pair so if i'm going to draw a shape and i want a prediction it's going to convert that that shape into an image that that looks like this one here it'll just be white pixels with the black infill and then the pixelpix model will process that black and white image and it'll output you know the the kind of predictive matrix that has some you know idea of what the room program would be yeah there was somebody was asking also about data augmentation and about the idea of flipping and rotating images to yeah so that's um i think even in the just the entry google pix2pix uh demo collab notebook they're doing some of that in there the github repo will show at the end also has the code that i wrote for doing that there's a kind of a nice library for for doing that in python it's i most of the pix2fix examples i've seen do some version of that and mostly it's just a strategy for kind of um generalizing the data set and to avoid over fitting um in it you know probably it does or doesn't make sense in some context based on like what types of images you're you're training on but for you know this example it doesn't really matter you know even if i mirror or rotate this this floor plan or the representation of it it's it's still a valid floor plan you could you could take the house and you know the doors will still work and the rooms are correct to one another adjacent um and i think yeah so it worked well for me to do that but it might not in all cases yeah imagine so for example if we're gonna see some data sets now of flowers so in a data set of flowers if you have a flower picture so it makes sense that the tail is at the bottom of the image and then the flower is maybe at the center or on the top so that's what the model will learn if you would like to have the model also learn maybe pictures that are flipped where the tail is on the top and the images on the bottom you will have to do augmentation in the sense of rotating the image right so in the data set you have normal images and then images are rotated so it's about as nate saying uh not having the model overfit which is just learning the patterns that are only on your data set and trying to augment it with more you know more types of inputs asking how large your training set was um so the number of like original unique plans that i tagged was 104 i think um i had like 125 130 images and then there was just you know once you get up close and realize that some are bad you end up kind of cutting some out so i think the actual set was like 104. which is a very slow it's a very low number for training right and i think it works well i guess you can decide if it works or not i thought it worked but it worked in this context uh because the you know the the variety of plans in a at a high level is really small right it's like you're not showing it hospitals and offices and mansions and tiny houses it's like a very you know kind of a narrow range of architectural style and even within that you know i think just based on what you can find you know images of on the internet it kind of trends towards like you know contemporary modular houses and so even within that i think it's a pretty small range um i think now one of the things that i was realizing recently is that now that again the tools have gotten so much better and it's easier to train these things it would be interesting to see how far you can push the scaling of this um yeah i still i think the biggest limit is probably the the spatial constraints you know the fact that there has to be at least in my mind there has to be some relationship when using images between like the amount of space in a relative sense and so it could be difficult to train a model that accounts for both hospitals and houses of this size but at the same time you know this stuff is under very active development and improvement i'm sure something cool will happen relatively soon 