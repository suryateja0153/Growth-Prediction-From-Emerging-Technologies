 my name is evan i work for a wonderful company we're based in munich called data insights we do a lot of cool things with data the company has also been very kind and they let me get up to all these really interesting projects so thanks if any of you guys are listening in i'm gonna be talking about a really cool application i put together um it's fairly recent like i just finished it up i think about three weeks ago so fresh research it's to do with gans specifically i've worked on a sort of invisibility cloak you'll see what i'm talking about so to jump in um let me first explain the technical stuff behind these gan things um for anyone who hasn't seen them before gans are sort of these uh interesting little machine learning interesting machine learning novelties um in which you have basically two components you have this generator component and a discriminator component so what's going to be happening here is the generator component is going to be working to try and make up some stuff and that stuff could be image video text all sorts of things it's going to try and make realistic looking things the discriminator on the other hand is handed sort of stock photos or stock video or stock text and is going to try and identify which one is the fake and which one is the real so it turns out that if you do this many many times eventually the generator gets really crazy good at making fake looking things so one of the first applications uh was this style transfer thing um this was where you'd give it a real photo and it would sort of change it into a painting like photo and it could do it to the point you really can't tell that this wasn't avango painting so it gets really good at these things but there was a bit of a sense that you know yeah you know these can things they're kind of just this novelty but it turns out they can really do all sorts of amazing things and people are realizing that now um one of the most amazing things is uh these faces that the person doesn't exist this is just a fake face that the program has put together you can see over time it's really really increased at a crazy uh impressive rate to the point that now i wouldn't be able to tell that this person doesn't exist this is from a website actually called this person does not exist dot com you can go check it out uh with deep fakes most people are probably familiar with these deep fake videos this is another huge thing there's a specific article that caught my eye this year where a politician had used deep fakes to put his face talking many different languages by getting speakers of other languages to sort of give his um his campaign um speeches so it's at the point now i think also that gans are infiltrating into our everyday lives and we should be aware of this uh really really interesting applications but also you know there's big questions they bring up too so google does other things with them google does somewhat more banal things sometimes but uh this is one i like it's uh this satellite image automatically translated to these map images if you're curious about how google maps works the gan is behind it another thing we've seen is a photocolorization so you see black and white photos that can automatically be colorized you can teach gans that trees should be green light should be blue blah blah blah my favorite is uh the scan that learns to change horses into zebras it can also change as you present to horses and it can also do it in real time so you can video tape a horse and it becomes a zebra in the video it's pretty amazing it's just amazing a team sort of put effort into this but i love it that's my favorite gan application so far so to summarize why you should care about gans um hugely creative technology very young i quoted before 2014 we've definitely not explored all the things they can do they're relatively easy you can run them on your video laptop i've tried my laptop uh like was said before with uh this sort of modeling for the covet it whines and it makes appalling sounds and i had to prop it up near the window so it didn't overheat but it can be done as opposed to like deep cnns where you're just never going to be able to um another thing is i haven't touched on this because this is such a big topic that i don't have time to go into it but as data moves from cloud to edge because of things like gdpr and also just the fact that edge tpus make sort of these applications on edges feasible there's going to be a big push towards gan created synthetic data when you're training big ml applications and some companies have actually already started using cans for these things so with all this in mind um my company we said you know be cool uh if we had again i said that specifically actually and then convinced other people it was a good idea but the point is um i started thinking okay what kind of gang can i build to sort of show we know how to use them and well thinking that i came across this whole sort of deluge of these um videos online specifically i saw them all over linkedin where people were claiming that they'd invented ml invisibility cloaks so i kind of felt it was a little disingenuous they were holding up um pieces of cloth and saying look invisibility machine learning but it's a green screen effect is what it is uh green screens have been around for a long time i did some research and i think the first one was used in like 1940s so they're really really old technology it's also not machine learning the idea is you just have an if else if there's a green pixel here show the pic the picture behind basically like the the previous layer that has already been registered so you already have information about the back one but this did get me thinking maybe we could actually make like a machine learning invisibility cloak like kind of a real one that didn't know what was behind a person um to give you a bit of a macabre example professional photo editors back in the 1940s removed nikolai uh salman's friend here and head of the secret police um when he fell out of favor with stalin and to do that they very astutely said hmm this sort of um little banister in the back or perhaps a wall it probably continues through him and there's probably ripples behind them so you can infer what's behind a person you can never be sure but you can sort of extrapolate with what you know you can't be sure there's not like a duck in the water but you can get a fairly believable idea of what might be behind him so i was thinking is there a way to basically get gans to do this sort of like a true ml invisibility clip and yeah kind of um so what i started with is i started with okay i need pictures to train it in which i have someone in the picture and then i have the same picture without someone um it's fairly hard to find pictures where you have that and they're exactly framed identically but there's also a lot of these stock photos online of just people with no you know background just transparency there's also a lot of photos online unsurprisingly of places without people so you kind of put the two together with a little python script and uh you can sort of resize and put people on the photos and sometimes you get ones where it's kind of unbelievable like people just magically levitating and stuff or this person who's like just built into a door or something but the idea is these could be edge cases it's not necessarily a bad thing to have some unbelievable photos in your data set because you might have someone jumping and you might still want to take them out of the photo so with that in mind i ended up with this as my data set uh the source image on the left and the destination on the right i had hundreds of pairs of these um and yeah at that point you're more or less ready to get into training so to train it i used an architecture called the pix2pix scan it's a fairly new one that came out in 2017. it's lightweight and it works great i loaded it all into databricks i spent a long time sort of whipping data bricks to do what i wanted it to do including things like trying to get the notebooks to communicate and changing parameter handling um so a little bit of work there but eventually i saw epoch 1 epoch 2 and when you're in the machine learning world and you see that you're like oh yes yeah it's working and so finally i got that um so one thing to note is when you're doing something like let's say image classification tasks or even basic nlp applications you train a network based on a mathematical concept called a loss function it turns out that for gams loss functions are really non-trivial trying to tell again when it's doing a good job is very very theoretical and there's a huge body of research trying to figure out the best way to do it with that in mind you kind of ask yourself okay how can i kind of quantify this how can i tell my gan when it's doing a good job how do i know myself um the best way to do it is you kind of track it and you make sure visually you see that you're going down the right path so to do that um i have this interface running it's called wisdom visiting was developed by facebook and what you can do is in addition to seeing a few metrics that are relevant to the training you can also actually see the images as they're being put out by the generator so it's really useful because you can say okay it's kind of figuring out what to do or no it's going the complete wrong track it's not at all working so this is the final architecture um for the wisdom server i hosted it on an ec2 instance uh the notebooks that are actually doing this pix2pix scan code they're running off databricks i have the model being saved to the databricks file system as well as data being ingested from there and for some ci cd kind of like shiny stuff i have git integration so um i'm going to try and get it running real quick i think it should all be working let me just pull this up okay so this is where i'm going to have wisdom running i might just really quickly because it uh it eventually times out so doing a live demo demo in a lightning talk is a little bit crazy but i think it's gonna work i just need to copy this command and then it should all be good so first of all what i'm going to do is i'm going to ssh into yeah here we go so now i'm inside my ec2 instance so in ec2 i'm going to boot up the wisdom server so when you boot it up you see this here you see it go it's alive so when it says it's alive it means it worked so now if we go here and refresh it i should be online here yeah so in the corner i see online which means my wisdom server is working so now i'm going to run my notebook um i have a bunch of sort of notebooks to read in uh my deathclash is going to sleep i'll take one minute to start so yeah we're going to just bring in a few things so specifically i have to um bring in the training options these are things like where it's being trained also you have to link in the training options to the wisdom server and whatnot and then other things here i also have to sort of read in the model itself um and then down here this is sort of the magic code here that actually makes everything work so i'm gonna let that load it shouldn't take more than like 30 seconds or one minute um and then if i pop up this come back there okay so at first what you're going to get is you're going to get things like this just to sort of summarize what we're looking at here the person on the images on the left this is what the generator um sorry this is this is a the image that you have being put into the generator uh the image on the right this is the image without the person that's what the generator is kind of trying to get to but it's not actually able to see this image and in the middle this is what the generator outputs so you see at first what the generator is doing is in trying to get to this image it's um just kind of fiddling around just kind of making everything in that image look a little fuzzy so it doesn't really know what it's doing after a few epochs you can see here it's kind of figuring out where the person is at least so this is after only about four eight bucks you can see pretty consistently it's figured out the person's there and it's trying to wipe them out a little but it hasn't really figured out how to do it convincingly if i jump back i think we might be able to see it working now ah dev cluster's still loading yeah i forgot it goes to sleep after an hour okay but that's okay because i have some results to show you here so after a few hundred epochs you actually get pretty convincing invisibility uh mind you this is on training data so it always looks better and i'll show you some test data later but could you you should be able to see that after a few hundred epochs um it's kind of figuring out how to extrapolate the background shapes for example this door frame and also the the carpet and stuff or in this case this little girl like the the shelves are kind of visible in the table too so it's kind of trying to figure out how to to do it um you see here it's getting pretty good this person has no no legs just because that's the way that the photo was but the floor comes through uh the walls it's kind of gotten this one's pretty good this one you can see this uh this woman has more or less completely disappeared you can just barely see a little bit of her legs there and then i just have two other images to show you so here also again you see the floor is coming through pretty convincingly there and the very one the bottom one here again it's done pretty good job with the back of the bed so not bad we're kind of getting there um still running the dev cluster okay so with that in mind um when you see these images it looks pretty cool and you know we think yeah we're there but we have to keep in mind this is training data so this is data it's already seen while it's practicing which means obviously it'll perform a little better on those pictures when we get to test data um so these are images it has never seen before these are images that were just pulled off the web these people are real people in these images they weren't uh you know sort of stock photos that i just slapped on so it doesn't do exceptionally well but one thing that's reasserting is it's it's finding the person consistently so consistently it's able to figure out where the person is and also to kind of extrapolate the background somewhat you can see here this guy's head is almost gone uh i'll zoom in a little because i have a zoom in on this one so you can see here that again the background is kind of being used to extrapolate this person's shoulder the bad pattern is somewhat coming through on the leg so it's getting there um and also here i like this one too because you can see the carpet pattern the carbon pattern is somewhat being extrapolated through the legs here again so it's kind of figuring out what to do to justify why it doesn't do um exceptionally well i would say first of all um i only had a well i said a few hundred before i had about 200 because it actually does some data augmentation but uh i only had you know on the order of 100 images if you're really putting this to sort of the full test you'd want to definitely trade on a few thousand began also was only trained for 200 ebox this is nothing compared to how long you should be training like a full-scale game for there's no reason you couldn't run it for thousands with other things like gradient decay another thing is all things considered the network being used here for the generator it's 15 layers which is very shallow when you're dealing with for example a typical residual convolutional neural network which is used oftentimes in sort of these heavy deep learning image classification tasks the ones i've used before you're going up to like 150 or even more layers so there's no reason i can't push this further and at some point i might another thing is hyper parameter tuning i just did it sort of in an afternoon and played around a little there's no reason i couldn't push it further still all this to say that um for sort of like a fun in-house little prototype i there's not really a need to push this extremely far and really you know gobble computation time um i'm pretty happy with the fact that you know it seems to be working and i'm pretty confident that if i really push the project i'd be able to get it to work but i kind of need a use case to justify that um let me just quickly pop back okay we're running now so really quickly this should just take a minute but i'll just run these real quick and now we should be able to really quickly see and then we'll open it up to questions while i'm running it also if anyone has questions right now feel free to jump in okay well those are all running in yeah go ahead oh i was going to say i think there's one question in the zoom chat um from uh yes mean i believe uh she wrote oh go ahead sorry i don't know what uh which language everyone speaks uh uh man i'm sorry i started speaking german i'm like my name is jasmine jasmine is fine or jazz yes i get a hundred pounds on evan oh okay uh my question i noticed in the training data uh it might be easier to explain with an image here so yeah sure if you just think of the access sorry this is really low tech i'm an engineer uh so this is our y and notice in your if you go back to your slides with the um it was a person in the house and the background was blue and it was a male walking through there's three images um i think you're probably talking about probably this one or yeah that's actually that one works uh go down uh um uh 30 yeah there we go um so if you look in this might just be my perception but in the bottom row on the second uh image you notice there's a slight protrusion i'm calling it y equals a like as in uh we'll say if you uh right near the the light in the middle image it looks like there's almost a slight like push towards like the like z plus access if that makes sense like it looks like it's protruding outwards so what i was trying to assert is is there some type of is there a known distortion when you're doing these images on train like is there something that we could derive from any type of distortion we know because like i can see uh slight differences between these images but i don't know if that's me differentiating that or if that's right the gan model yeah so um okay so i i think i think i kind of get where you're going with this so you're saying you know that we we perceive depth right and we can tell clearly you know this person is removed from the background right okay yeah yeah you might be right there it might be not being there and i'm perceiving the color of the of like the brown light closer to me yeah yeah yeah that's crazy in this case uh one thing to know is um these crop files you know they're not perfect um when you look at the files sort of i was using the stick on top of these photos there's always a little bit of sort of gif artifacts or jpeg artifacts or whatever around them um and definitely began is using that to his advantage like again is totally figuring out that hey uh you know like i can see there's there's some you know blur in the background behind this person there's a few pixels that that are different color so it's definitely one of the reasons why i perform so much better on training data um in the real world obviously you don't have that so when you look here it's not as crisp like it's able to figure out decently you know where this person is but uh it doesn't do nearly as good a job and you see also cases where sort of this person on the on the right like it didn't get his head at all um this is to be expected because you don't have those those little little tiny image artifacts for it to go on interesting yeah um before i i take any other questions i think this should be working out yeah cool okay so now you can see it in real time uh this is the in browser wisdom server um i'm only printing out one image every i think 50 so that gives a sense of how fast it's like rocketing through these images on the bottom you can see the training metrics um and they all sort of with gans again they all have different meanings and i won't go through them in this talk but uh their interplay is another indication of how well you're doing and uh yeah already i mean we're only 18 images in and you can see it's starting to start to figure a few things out like in this image it's kind of getting the the color this one didn't work at all but uh yeah after you after a hundred two hundred epochs you uh you get pretty convincing results um so yeah so i'll just finish up i just have one very final slide and then we'll leave 10 minutes or so for q a uh the very last slide here um yeah so this is sort of where i want to take this project uh first um databricks has mlaps i've used demo ups before when i've been doing a computer vision problems but i haven't used it for gans and i'd like to try and implement it uh also our company has an aws deep lens i think it'd be super cool to like upload this model to the deep ends and like uh deep lens and do real time and visibility like people could sort of like dance invisible it'd be kind of cool uh and then also in general this is advice to all of you like when you're approached with company use cases i think it's a really good idea to remember that this technology exists and i've tried to sort of push that around our company and say you know this is something that we are able to do this could have great um sort of uh for example data anonymization uses or a hundred others uh so it's a good idea to keep this in mind and i haven't put a fourth point here but another nice little thing to sort of tip my own horn is i don't think anyone's actually done again in databricks before so i think this is a first i haven't found any info online about previous attempts so and that's it so go ahead with questions if anyone has any karen if i'm sorry i wasn't sure if you had a question first and you just saw mine but i i didn't want to jump in if there was other questions oh no worries i i wasn't sure if you were going to ask it um shoot away audio or not so i i was just going to repeat your question but but thank you i appreciate it i'm glad you joined us uh evan i think let's see uh there's a question in the platform chat from robert and he is asking can gans be used for movie special effects yes absolutely in fact um so one of the things that are so great about cans is they can learn to do sort of these very general like photoshop pro level techniques uh and do them very efficiently and fast so um an example is let's say for whatever reason you did um have a movie where for whatever reason you wanted to get rid of every instance of a person in the movie uh you'd have to definitely train this a little further but if you pushed it to the point that it really convincingly edited people out you could run that on a movie on every single frame and it could just you know launch through them it it can sort of create these uh these images in which the person has been removed in a fraction of a second um so you could very quickly run it through the entire entirety of the movie and uh it could sort of do do that on every single frame and it doesn't have to be that like with gans again there's there's so so many applications you could teach it for example to add mustaches to people or to sort of like you know change people's faces or something to do with the background it also doesn't have to be of people there's been a game that changes day scenes into night scenes and you could easily apply that on movie footage as well so huge implications across a lot of different uh domains um sorry i was in a i just graduated data science school and thank you i'm i'm hoping to get a three-year phd but uh my my thing is um i'm in a group it's like an analytics group out based in india and we've been trying to get project ideas in for uh image recognition now i don't know if you would classify this as image recognition like i i just call this computer vision but my question is where are the boundaries of like what could i add in for example would i be able to utilize this um i know let me let me let me actually use databricks as a base um my question is would this be applicable to images say that are saved in a different like saved on aws and then using um either lambda functions or i guess sage maker to manipulate those images uh use case um say a company you have you own a bank uh and the company sends you a bunch of like images from atms and unfortunately it's become vulnerable and someone messes with the code and now they've manipulated the images is that a use case that's applicable to the model that you just showed to present to a business to say hey this might be a way for us to prevent attacks by learning how people could manipulate images does that make sense so um there's there's a couple questions in there so the first one regarding like the aws um and lambda functions absolutely in fact um generally when you're doing uh deep learning or machine learning on data bricks it's a good idea to go for s3 as your image and data source uh it's a little bit better for security reasons from what i've heard um i'm not exactly sure why but this is what i've been told uh previously when i've worked with cnn i've done it with s3 piping the images to databricks uh the second the second thing i heard there so there is a bit of a like a wishy-washy boundary between when you should be using cnns or sort of traditional computer vision and when you should be using these more recent sort of can applications uh generally you're a little safer unless it's a very specific gan use case going for the sort of cnn side the reason being is because um within computer vision there's a there's a lot more history i mean cnns have been around since the the 90s uh they're better developed they tend to be a little more simple in terms of their architecture keep in mind i have to use a generator and a discriminator so i'm actually juggling two different networks here um for things like image recognition fraud detection classification uh image segmentation these are things where computer vision definitely like triumphs you definitely want to go towards more red residual cnns uh cnns units things like this uh where gans come in is where you have a huge amount of photos and you need to change them somehow something that's not as you know simple as just putting a box around the object or segmenting the image when you want to do something for example like remove a person that's a little more complicated and that's a place where gans would excel um so then you want to go for gans if you're doing something that's a little more out there a little more i'd say creative but that's me being biased so that's where sort of gans come in yeah okay but you can always do with cnns which and we've started with cans what you can do with cnns almost always they could if you wanted them too okay um my last piece on the sagemaker thing um i i guess i'm asking this for what you just said the anomaly detection and with regards to s3 being more secure it is because it has better encryption possibilities but there could be the issue of using for example amazon kinesis where number one you know i think the images come in and they're they never left they'd never leave the server like even when you remove different versions so that's problematic but i guess um i'm just testing your understanding so i can understand better uh to bring this back to organization for you know like you're saying anomaly detection broad detection these are the things that are like really hype right now in uh cloud computing so just understanding like what is what is the best practice like a go-to for phd when it comes to using uh machine learning data bricks specifically and uh with the use case of anomaly detection or fraud detection so if um one thing i'll add is just because i think we only have a couple minutes left but just uh to add one final point if you're looking to do um fraud detection or anonymous um anomaly detection on uh images that are being piped in look into semi-supervised learning uh that's what most people probably go for uh you just actually embed you don't really even need you know a second half of the convolutional neural network but uh that tends to work really well i've seen people using that along with um clustering techniques and you can actually not only like automatically detect um anomalies with untrained data you can also sort them by different types of uh fraud or anomaly or whatever you have so it works really well semi-supervised with the front end of a convolutional network embedding and clustering beautiful thank you so much you just answered so many of my questions thank you yeah so i think uh i think karen actually disappeared but i think we're like up for time karen actually had to leave so she made me co-hosted wrapped into session did anyone have any other questions for evan i think that's a no i i just wanted evan you're really brave for doing a live demo during a lightning talk i wish i had that courage uh the cluster wasn't it didn't shut down i forgot it after 60 minutes usually it works yeah you still you still got 90 up in there so yeah sure yeah well thank you again evan so i really want to take the time to thank evan uh for for giving the talk today and all the audience members for attending today's session uh i think the slides and recordings should be available fairly soon um if you have any other questions feel free to email us but that'll be a wrap for today otherwise great and you can also add both of us on various networking things so that's possible too yes uh evan if you want to leave your linkedin in the chat or there's not not too many people called eminem so you can probably just track me down based on that yeah i'd love to see your work or your notebooks or um you know this is this is groundbreaking work so this is cool stuff yeah really cool stuff thanks a lot and also thank you it's really cool 