 all right can everybody hear me great ok let's get started for today again somebody closed the door in the back so welcome to lecture five on deep unsupervised learning we will do they cover implicit models and a lot of it will be generic adversarial networks who has looked at Wilson's code for homework one raise our hands okay cool which one of you has given Wilson some feedback on his solution for a homework one nobody yet well solid solution Wilson that's great I recommend actually so about 10% of you looked at it I recommend everybody look at it I think the best way to learn something well is not just to try to do it but also check how somebody else solved it and that might be a good solution to check and see how it compares to how you did things that it might actually help you a lot as he work in your homework too because you might see tricks Wilson used that might be helpful for you as you solve your homework two and maybe also your research and so forth your homework to by the way is already out and is due this coming Tuesday so you have about a little less than a week left to finish that up any questions about what you six so the slides that you'll find online are still bit in draft stage for the full deck but are more or less finished for today's lecture we'll have two lectures to cover implicit models and gans because just a very big topic and we need to full lectures to get through that in fact you see here the menu we're going to go through it's a lot to cover all the way from motivation through the whole progression of ganz different ways of looking at ganz and then some other considerations will consider at the end today will hopefully get til somewhere in the middle of the game progression that's our target for today and it will pick up the rest of the game progression next week and see the other views on ganz next week all right let's start with some motivation let's see one thing you might have seen in terms of image generation is this tweet over here from Ian Goodfellow who actually is the first author on the first gam paper so some since the inventor of Gann and 2014 is when this paper came out and kind of looks like a face but not necessarily high-resolution not particularly realistic and then from there we see a progression just five years to extremely realistic faces automatically generated by ganz then resisting over here began this is actually a video for began let me pull up the video here is the video or watching here is for watching interpolations between real images so there's real images and you can interpolate in latent space Afghan between those images and then as interpolated legends please generate the corresponding images that the chanter generator would generate from that latent code and the thing to look for here is that actually a very large fraction of the interpolations are very meaningful meaning that some how is this covered a lot about the structure that is in our natural world around us and what makes for realistic images because many things in Layton's space will map to realistic images and if you interplay between two real images you get something that's somewhat meaningful in between most of the time not just an overlay of just averaging pixels you can actually get different viewpoints on objects and so forth so our goal is to better understand how to get these things to work and maybe also see what the future might be actually they're interesting that happened is in 2018 somebody took some code available in github trained again on paintings and then actually marketed really well and sold them at Christie's which is a well it's in London it's where people buy art and you bid and actually somebody bid and paid over $400,000 for this AI generated artwork with the signature actually the equation that we'll spend a lot of our time today on is the equation that effectively designed this painting funny thing by the way is also that the people who sold it did not invent again they did not even write the code to run again they did dig out the right training data so you can train again that generates interesting images that you can market and sell so through the generative models and as firstly what we've seen so far is other recive models made pixels CNN RNN gated pixel CNN pixel snail we've seen flow models autoregressive flows first and real MVP glow flow plus plus we've seen latent variable models that was last week the common aspect here is that all these are likelihood based models the first two categories are exact likelihoods that were optimizing against and the third one latent variable models we had the variational lower bound an approximation of the likelihood that we would optimize for training and so basically we've seen so far you might expect the next thing we're gonna see is also going to optimize their likely we're actually not gonna do that to narrative models you might want a sample from them evaluate the likelihood of a new sample or a given data point train them and find a good representation some latent space but what if all we care about is sampling what might that open up may be in terms of new opportunities well if all you care about is sampling how about this thing over here well will this thing do any thoughts say it again all right this is just indexing this is just indexing into your training data and then displaying an image from your training data so this is gonna generate extremely realistic images and so all you care about is realistic images being displayed you're good to go but of course we want more than that this is not as interesting what do we want from our sampler where she want somehow a generic model that can understand the underlying distribution of data points and as a consequence interpolate across the training samples in a meaningful way I would sample similar but not the same as training samples so it generalizes when the output samples are representative the underlying factors of variation in a training distribution an example would be digit data set and you might want to see another rendering of the same digit but with different strokes different thickness thinner stroke maybe shift a little bit to the left or the right a variation of might not be exactly in the training data would be better than just putting back out training data examples for faces you might have seen faces and different poses but for some faces you might have seen a certain pose if you can generate that pose also it means it understands something more about the underlying distribution and that's what we're after in implicit models we're going to sample Z from a fixed noise source distribution so Z will still come from uniform or Gaussian we pass it through a neural network outcomes X this is just like what we saw in flow models and in latent variable models what's going to be different here is that we're going to try to train this thing with explicit density estimation we're just gonna focus somehow on the quality of the samples generated and try to optimize it for quality of sample generation let's make this a little more explicit what does it mean to be an implicit model so let's say we're given some samples from a data distribution P data that's how our training data then we have a sampler Q Phi which takes our latent code and turns it into some X some output some deep neural network turns latent code into some output Z comes from some simple distribution uniform or Gaussian X Q Phi of Z will actually induce a density function just like we saw in flows when you went from one space to another space you can use a density function but in implicit models we did not have an explicit form for distribution of our data or the distribution we use for our model we can only draw samples that's our assumption so this becomes very different when we've seen before we're not going to have some kind of objective we can optimize we just have some Z that sampled and suppose we turn into a X space sample that is similar to the training data and so what we want to do is make our distribution that we learn the model distribution as close as possible as a data distribution by learning some appropriate Phi but we're never going to have explicit access to P data or P model we're gonna have a samples from our underlying data distribution and samples from our model and somehow we want to compare those samples and train Q Phi to make those samples close together so it's quite different from maximum likelihood so we now need a different way of measuring how far these two distributions are within C models we could just use the kale right we had kale between the data and the model and that was the same as maximum likelihood optimization minimizing kill maximizing the likelihood and now we can't really do that because we will not have a P model in a parametric form we will just have samples from the model so we need to come up with a new distance measure and within Ganz we'll see that there are actually quite a few different distance measures people have come up with variations on the same theme but not always the exact same distance measure and so some examples are maximum main discrepancy which we'll cover at the very end of next lecture the chance Jason Shannon divergence which will see approximately maybe gets somewhat optimized by again the regular gand and then earthmovers distance which matches with what will later call Wasserstein again so we don't know yet how to do this but that's going to be the overarching picture we somehow want to measure the proximity of two sets of samples and some back propagate through that proximity measure to improve our sampler to get samples closer to the data samples yes yeah very good question can we say a bit more about this thing over here the actual the big takeaway here is that we're not going to explicitly represent a probability distribution the way we have done so far there's not going to be ap model of X that assigns a number to each possible data point X instead implicitly that's the implicit part implicitly we know that if we sample these they will result in X's through the mapping that we saw in the previous slide so this mapping will turn these into axis so we know that implicitly by choosing a mapping from Z to X we are inducing a distribution a model distribution but we're not going to have a form for it in the sense that we're not going to have someone else's oh the probability of this X is this much instead all we're going to have is a lot of samples generated this way and we're going to compare a lot of samples generated this way with our data and we somehow want the two sets of samples to be close together and when they're close together that means your Q Phi over here is doing the right thing so gonna optimize our q5 to bring the sets of samples together while keeping in mind of course we don't want this kind of sampler where you just draw data points you want something that generalizes that can interpolate that draws variations on the original data so it's not just about oh can we find something that's as close as possible cuz then you just use your data it's more subtle than that you want the notion of generalization so for now that's a little subtle and they'll become more clear soon but I'd say the important takeaway is to have this P model thing we will not have any expressions of the type P model of X hi and then some log in front of it and try to maximize it it's not gonna happen here okay so let's take a look at the original gaen paper came out in 2014 proposed a new framework for estimating generative models be an adversarial process okay simultaneously train two models generative model G captures distribution this created model D that checks how different the distribution is that we're generating first is the data distribution that's the high level idea experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples and since then that potential has definitely been filled in by many follow-up papers I think this paper might be cited 5000 times by now or more so what does the math look like we're going to formally as follows we're still going to try to find a generative process G and there's gonna be a discriminator D and it's gonna be a game between the two of them so a minimax game they're playing against each other let's first look at G what is G doing G appears only here G is turning Z into X so G of C is X lives in our image space or text space or speech whatever you're trying to generate so G of Z generates then what what's the discriminator doing discriminator is trying to classify this community is trying to classify it which axis our real versus generated so here's our real data distribution I'm going to try to maximize the log probability under the discriminator in Scribner that will have a 0 1 output well a sigmoid output probability of being real and so the screener will try to drive these try to drive these two one as high as possible so one is the highest probability you can assign to something being real and then here it's doing the exact opposite it's saying discriminator is going to try to drive this to zero because those are fakes which if this criminal drives is to zero to subtract it out that New Zealand one minus and actually the whole term will become as large as possible so the discriminator is effectively trying to be as accurate as possible at assigning zeros to fakes and once to reals and it's doing that in a probabilistic way two of them play a game against each other you can imagine what happens that convergence if if you're a Nash equilibrium of this game meaning that nobody can improve anymore well the generator should generate things that look real because then the discriminator cannot do anything about it that's ideal discriminator at that point will output essentially 50/50 the way you optimize this if you don't know what it is you gotta hedge your bets you gotta sign a 50% probability to each side that's at full convergence not saying it ever fully gets there but that's how this game is set up so this villainess you actually now don't need don't need explicit paralytic evaluation model for your pea model you can just try to solve this game and the game is defined us try to generate things that are as realistic as possible what might it look like so you have some sense two pipelines you have X coming from your data goes through D and D tries to assign a probability of one of it being real so that's a supervised learning problem right there at the same time is another surprise learning problem here for D to sign essentially probabilities as close as possible to zero of artificially generated samples being real and then in here there is a generator and so what effectively happens if you look at the math - well we can look at the algorithm you gonna iterate you're going to sample some Z's and generate let's say images to be concrete then you're gonna take a mini batch from your data these are the real images then you're gonna come you're going to look at this objective you're going to optimize your discriminator to maximize its classification accuracy this is just straight-up supervised learning you train a classifier we maximally accurate on classifying real versus fake you have some real data some fake data just train it then the generator going back to if you look at the overall objective which is this generator only participates in this part which is put back over here and so the generator will try to minimize that term in the objective and that's great in steps on that part and you just iterate over those two until hopefully you get to a good spot so any questions about general setup ok so then let's take a look at a demo so what do we have here we have Gann lab demo we have a real distribution shown in green over here then as we start training what's gonna happen as they start training both discriminator and generator will be training so let's hit play stop displaying it correctly it seems fortunate let me plug in my laptop I think it should display better demo so we have real data and green fake data samples generated in purple this is before training or let's reset so it's completely before training and then let's run the play and so what do we see happen here is what we would hope to happen is that as the training progresses the purple samples will match up more and more closely with the green ones and then when a discriminator tries to distinguish which samples which individual data points come from real versus fake have a harder and harder time figuring that out and at some point generator will sometimes win the game and be able to fool discriminator initially in this case sex work pretty well the purple samples you are getting pretty close to the green ones which means it's time to capture the general gist of this distributions is a mixture of gaussians and is captured by this sampling process again there's no explicit density model that we can evaluate is just we want the samples generated to be similar to the data distribution that we have then if we look at the thing on the right over here what's going on is thickly what's happening is for each sample that was generated you have something saying they'll just been moved more in this direction or that direction would actually have been a better sample that would have done a better job at flowing discriminator you see the samples move around accordingly here let's do something a little harder so this is actually working pretty well I can see learning curves here you now have two losses that you look at by the way because the discriminator loss in the generator was and hopefully both of them converge at some point it's a game it's a little more complicated to think about it's not like I just see a curve that goes up because there's two playing against each other and so there's no such notion as you have in likely training as you see it go up and asymptotes to some correct number then let's make it harder let's three clusters Russia let's do this circular thing and see what happens you see some oscillations on the losses here that's pretty typical that sometimes discriminator does a little better actually often a discriminator tends to do a little better then a generator it is a little better and this can go back and forth this is of course that pencils and expressiveness of your discriminator network and your generator network but this is the underlying dynamics happening when we train for realistic things to now be able to visualize it this way is now going to be in 2d is going to be high dimensional image space but something similar will be happening where somehow these samples and the real data are getting lined up so they become indistinguishable for your discriminator network this can also sometimes work really poorly like you have a lot of oscillations happening here now it's not really getting to a great convergence point yet and so that's actually one of the big challenge with Gantt training that the you don't get this typically this clean monotonic improvements that you're used to with training supervised models or other unsupervised models okay so let's go back to our math that's behind this so here are some samples from 2014 the last column are real and the other images are generated yes question well it's a good question for against it works so well I mean you know what what says do they work so well so sometimes they work very well in generating realistic images right that's that's in many ways their their strengths and so part of that is that their objective is set up exactly for that all right because you have a objective that literally says make your generated samples as indistinguishable as possible from data samples and so gans are very directly optimizing for that and so part of what this hints at is that just optimizing for likelihood doesn't always 100% aligned with optimizing for looking like real images and you see that I mean of course some live models are getting very close and so that distinction has been actually reduced quite a bit but in the early days Ganz we generate much more realistic images because they directly optimized for it as for a lot of people intuition is that likely models when they're not getting very high scores then you know it's might not be perfectly aligned with realistic images but once they get very high scores do really well they are also going to be starting to generate realistic images can you add AG an objective to any model yes you can have multiple losses absolutely if you want to if you add it again objective it would likely improve the sample quality because it's it's directly optimizing for that that's a good question I mean I think as soon as what you're getting on his dad throw in the kitchen sink at something is a great idea very often because you throw the entire kitchen sink at it you might get better performance and doing something more clean and I think educationally it's more interesting to see the individual parts see what their strengths and weaknesses are rather than just saying let's take everything everybody's ever invented put it in one model and put it in front of you so that's that's the reason and same with writing papers you don't want to write a paper where you bring a thousand things in and then nobody can understand where it comes from it's really about generating insights and understanding but at some point you're just an engineer and when you're an engineer not a researcher you might just want to say I want to bring everything to bear I don't I don't care you know I'm not trying to generate it inside I'm not trying to make anybody understand what's going on here just gonna bring everything to bear and see what comes out of it and that's you know that's a different mindset it's a good question you could you could combine everything and for your project you could maybe combine you know all four classes of models we've seen so far in some way question yes that can be a problem the generator might memorize some examples in principle the discriminator should understand the same principle if the generator generates let's say five of your data samples every time the discriminator should learn that whenever it sees those five out of maybe a thousand training data those five are always by the generator and that also not always but most of the time and so the discriminators face that those will say Oh likely generate it even though sometimes it's real data most the time will have been generated and so that's the counter force to that it's not perfect I mean you will see that this kind of similar things do happen if you're not careful but in principle this objective gets around that if it's optimized completely by being able to recognize that if you can memorize the trend data perfectly that would be a breakdown and so you don't want essentially you don't want to set up a generator network that would memorize it train did it do you want to step your chair and network to take in let's say Z sample from a Gaussian distribution and turn that into some continuum of images not a discrete set of images but yeah principle you except a network that turns your C into one of your training data images you don't want an architecture that could do that so how to evaluate this if I wish to forget there's have to still be a bit of an open problem because you cannot just report the kind of numbers we've been reporting so far one thing you can do is say well if I can generate samples I can think of it effectively as a mixture of gaussians in some sense where around every sample I put a Gaussian blob this in turn yields a density model and now I can evaluate for some test data what is the likelihood of some test data under this mixture of Gaussian kernel density estimator model and report those numbers some playing you have to do with the bandwidth even for one the examples you could see four different bandwidth of your kernel you'll have different smoothness of curve so the reference is shown in gray you want to be as close as possible to the gray the black those stripes at the bottom are these samples and then depending on the bandwidth for a very narrow bandwidth you get the reddish curve which is very high frequency for green you get something intermediate and then for blue you get a pretty smooth curve so that's a wider bandwidth and will have a higher likelihood score on your test data because it's much closer to the actual distribution of your test data you only will care about and people who did this and the early papers people would compare this course the big issue with this of course that it only works for low dimensional data because for high dimensional data putting these gosh and blobs around generated samples it'll still not really fill the space over which we wanted to fill the space you got a big very high standard deviation gaussians it's just not a great metric but in principle you can use it and report on it here's another reason why I might not be the best metric on the horizontal axis here is number of samples on the logarithmic scale and this is I believe for 6x6 6x6 patches so not even that high dimensional images and this all 6x6 patches and you'd then have a generator and you generate samples from it and you see how what the log likelihood is of your test data as a function of how many samples you put into your sampler to build your kernel density estimator and you see it's not getting anywhere near where you want to be and it's not going to get there any time soon so that's for the 6x6 36 dimensional space so you can imagine for megapixel image is going to be even worse turns out in the taste paper they actually play a little trick you can go check in the paper down if you play some weird tricks you can actually get a higher score than you're supposed to be able to get with the actual underlying density so you can go check that out well people have mostly switched to is something else something called inception score and then another score we'll see soon so how do we size step this high dimensional density estimation well good generators should generate samples that are semantically diverse so semantics predictor could be a train Inception Network v3 so you're trying to work on the image net and you've fixed that and then Y given X where Y is one of the thousand image net classes of what this thing outputs so what happen then is each image X should have distinctly recognizable object so if you train an image net and now generate new images each image you generated should somehow belong to a class that's the intuition behind this and that's what it's going to score and you should also generate all class it shouldn't be that you always generate the horse disk is then you're not generating a very diverse sample set so high level intuition a classifier should have high confidence on the samples generated that means they are look a lot like the data the situation which the classifier also has high accumbens on the samples and the sample should belong to many different classes so the inception score does exactly that give the inception model pre-trained nothing to do with again just a classifier model you look at the marginal label distribution so you generate data you look at how often you get each label and that gives you a thousand dimensional histogram in some sense and then the inception score looks at the KL divergence between the conditional of Y given X and the marginal so the marginal ideally will be uniform and the conditional ideal will be highly peaked around the one out of a thousand classes that you just generated and there'll be very large Cal divergence so there's a case where your Cal the various actually if it's high it's a good thing you're hoping for a high QL divergence here generally speaking our talked about the extreme case where it's generating exactly what you want generally speaking the the less the less uniform Y is the more difficult to have a large-scale divergence here and the less well you classify also the harder it is because if you have a very poor closet let's say you data you generate is not clear what class it belongs to and I have a close to uniform distribution maybe what it might be and then these two become close together again and so this is really optimized for the two things we care about clear images in terms of class and making sure all classes are covered about equally another way to think of is that is measuring the entropy in the class labels you get out of this process and compact out the entropy of Y given X you want very low entropy of Y given X which means it generate a very clear clearly classifiable image and then this thing will be very low so here are some images generated with different methods so this is the real data this is the again proposed and this from the improved again improved training of gann papers so images from that some previous models and what we're looking at here is that the more realistic the images become the higher the inception score and so we see a good correlation between inception score and realism of images now does it capture everything this inception score thing well there are some corner cases it really doesn't capture so close to your corner case imagine you only generated from thousand classes he generated one image of each class and that's all you do then and you there clear images from each respective class we have no other variation in your data you generates from your game it will have a really good inception score in fact perfect score uniform over classes and clear classification for each image generated so there's another score it has become popular for a shain inception distance which looks at as you generate an image process it through a confident that might again be pre trained with a classification procedure then somewhere along the way take a hidden layer look at that vector and think of it as a representation of that image and now look at on your training data and on your generated data what is the distance between the mean of that feature vector that you just generated and also compare the covariance matrices so you look at covariance / generated data you look at covariance over training data and you compare the two covariance matrices here to see how close they are and if they're close and the means are closed and there's a good sign that you're generating data that is a lot like your underlying distribution obviously these are not perfect metrics as I said on the first slide about metrics for gans is still no open problem to have really good metrics if you had a really really good metric you could probably use it as an optimization criteria optimize against it the two we just described are not good ones to optimize against if you optimize directly against them you'll you'll find solutions are not exactly what you want but assuming you're not optimizing directly on them there can be a nice independent measure of the amount of variation and crispness of the pictures that you're generating and so in the fresh a inception distance paper they compare it with the inception score and we see here that as you disturb more the inception distance goes up which is a good thing because it means you're not doing as well so you have a worse score up is worse here whereas the original inception stays constant doesn't really change and so you definitely say this is a worse generator than that one and forshay captures it but original inception or does not similarly here for making it more blurry very clean curved refresh a not so clean for the original inception score and there's a few more comparisons here with again no is introduced fresh a captures that this is not as realistic gives a worse score whereas Inception does not so key pieces of Ganner fast sampling you just go sample as he from Gaussian or uniform and then turn it through a neural net into a sample we're not doing inference so we're not going the other way not anything we've described so far next week we will look at how we might be able to go the other way but for now I haven't looked at that and then the notion of optimizing directly where you care about which is perceptual samples that's what's in the optimization objective which is a minimax game so now we've covered the basic algorithm let's look at some theory behind it again fear of ganz is not perfectly understood these are some just some hints at how you might be able to better understand what is behind ganz so one question we could ask is what is the optimal discriminator given a generated and a true distribution any thoughts think about is in a very simple kiss or a very simple case you might have two distributions may be a Gaussian another Gaussian and you could actually ask the question right what is the best way to distinguish where my data come from Gaussian one or Gaussian - well if they have equal standard deviation then you'll say ok on one side here it's 1 on the other side it's the other that's your best guess we can actually even compute a posterior you can be basing about this for any point I'll say a point over here you could look at the ratio between these two and from that understand the ratio of probabilities that came from one or the other so in principle that's possible if we have two processes we should be able to infer the probability the exact probability came from one or the other assuming we had access to the malls for this theoretical analysis we'll assume we can somehow write out the model and do some analysis with it ok so let's take take a look at look at what we can do here stepping through the math this is our objective that we look at for our discriminator all right we're trying to find out to maximize likelihood of this thing for our discriminator which is here and here let's write this out explicitly these expectations as integrals here respect to the data here respect to our Z's now Z our dispersion over Z results in a distribution a general distribution for X which will change a variable here generate our for X once we have both integrated respect to X who bring that out and we have this quantity over here and so we now have an optimization problem where we have P data x PG X and then D appears here in here we can now take the derivative respect to D of X and find for a given X was the optimal D of X here was the posterior probability so let's do that that's really a problem of this format over here and we find is that this is the solution and so our optimal discriminator will be this the probability I assigned for this coming from the real data is probability it was generated from the data divided by probability trades from the data plus probability generated from the generator so we have a form here and what we have here may be of a data distribution a generator distribution and we can then know exactly what our optimal discriminator would look like so now we can do a bit of extra math what we can then do is we can say we have an expression for our optimal discriminator if we fully optimize our discriminator we can fill that expression into our original objective and see what if the inner loop is fully solved by discriminator what it looks like so we're going to fill into the discriminator here optimal discriminator we fill in that posterior probability if you look at this expression over here it's some kind of expectation spec - data that some log ratio expectation respect to the generator some mod ratio that's your remind you of carroll divergences if this submission the Cale where the data comes first some care where the generator comes first but then and indeed we see data and then data at the top that's exactly the KO we're seeing here same thing on the other side but then on the bottom we see a sum of two distributions so it's really an averaging distribution sitting here same over here so we see is that actually if we look at it carefully assuming the discriminator fully optimizes and that's an assuming that's Khmer does that I mean it's not easy to say oh to do that and we'll see we might not want to do that but assuming the discriminator fully optimizes what happens is that the generator is trying to optimize a type of KL divergence between distributions it's called the channel channel chansons channel divergence which is a kale between the data and the average between data and generator and between the generator and the average between data and generator so actually we're still optimizing something like a kale but a different one that we looked at before remember if we did maximum likelihood it would have just been kale P data PG this is the this is maximum likelihood so this is we this is a little different let's take a look at what this comes out as when we optimize for we have some data in 2d it's our actual data if we do KL so maximum likelihood we get this thing over here what is it doing it's ensuring that there is a lot of probability over here where mostly probability masses but it also spreads out over the entire space because if you don't cover some data point it's gonna give you a negative infinity score and that's that's bad so you got to cover everything this one we'll skip for now we'll come back next next week then the Jensen Shannon divergence we'll see how she sit over here it decides to assign pretty much Hollis probability mass on the biggest mode and that happens to be the optimal thing to do here why is that happening well it has these two terms and one of the terms is a lot like the maximum likely term but the other term is actually a reverse Kail it's where the data and the generator are swapped when you swap let's say you have a real distribution as two modes maximum likelihood we'll try to find will hopefully find this thing that's the best scoring distribution you can have but the reverse scale will actually find this thing over here as a higher score for focusing all on one mode rather than spreading it out equally over both modes now the jetsons shannon average is not exactly reverse kale it's something in between reverse kale and forth kale and so it's not zoning in on the mode maybe as much as a reverse Cal would but it'll still have that property of liking the zoning on it here's an example Alec Alex made to show difference between the three so our data distribution here P of X is shown in blue says bimodal with a large peak on the left and then a smaller peak on the right the maximum likelihood thing over here does the normal kale will find something that kind of spreads out just probability mass in a decent way reverse kale we'll put it all over here now though of course is still Finnegan but this constant as much as possible right over there that gives you the better score then the Jensen's channel divergence does something in between but most of the mass on that main peak but still puts a bunch of mass out there onto the other peak more so than the reverse kale would do and so you might have heard of things like mode collapse and cans and things like you know what would it mean to have modes that instance disappear but happening is that well let's go back to this this picture for a moment the chance D this is metric will prefer zoning in on specific modes which means that this mode if you look at your generator will pretty much not appear and I should be optimal it's what it's optimizing for whereas if you had a maximum likely model it'll really have to cover everything and we talked about this with a VQ ve where we saw samples on vqv that had a wider coverage than what we saw in the big an samples at the time so for compression you would actually want to share all points in the data description are assigned some probability mass otherwise you cannot compress the ones that are out of distribution very well for generating good samples blurring across modes actually spoils the quality of your samples because as you start blurring across and bleed leaking between two modes the things in between are not going to look very good and by covering the space in between you get not so good looking samples picking one mode without a sample mass and points outside can just give better-looking samples quick caveat here the reason we had to make a trade-off is because what we're fitting is a unimodal Gaussian - a bimodal Gaussian distribution if we were fitting a bimodal Gaussian to a bimodal Gaussian distribution we could fit things more closely and we would not have to make this trade-off and so as you have more expressive models you are likely to suffer from this less because your model is able to capture many modes while not needing to put probability mass in between where you would get unrealistic samples but in practice I mean depending on you know the complexity of your data high dimensionality of your data and the neural nets your training you'll often still see behaviors like this where your target might be in this case eight modes but then during training you just jump around from mode to mode to mode which what's happening is you're here now the discriminator is like well will you generate something there it could be real there's only one eight of the samples that are real over there but all the fakes land over there discriminators gonna say well if I see this I'm going to say fake and then you get a bad score here and you gotta run somewhere else where the discriminator hasn't caught up with you yet you're in another spot and then someone discriminator catch up so well if you generate here that's no good I know that it's generated because only 1/8 of the real samples will come in that spot you're getting these cycles where chatter there hops around from mode to mode 2 mode discriminator catches up in generator hops around again so that's one of the challenges with ganz the mode collapse and making sure you can cover modes another thing that's actually quite challenging is well let's look at this objective are here this is our discriminator objective on the inside it's just a classifier is it feasible to run the inner optimization to completion we ran all the way till we get that Bayes optimal classifier well is it well if a trainer interrupts inner loop for long enough you could actually get there with an expressive enough network you will get there and get the Bayes optimal classifier would it create problems if we're able to do so it actually will create problems the reason will create promise is that once the disc remainder becomes very confident there will be no gradient left so discriminator here we have in this plot we have y equals Sigma of X then this is our objective log 1 minus Sigma of X inside of here x equals G of C right well so 1 will come later than some sense the X here is most of the discriminator of G of Z and then Sigma comes at the very end o discriminator and so what lives inside here is the generator objective when the discriminator is extremely confident and this hundred percent sure what you generated is fake you will knock it and any small change to your generation process the discriminator remains a hundred percent confident it's fake that means there's no great in signal and you cannot improve your generator if you look at it specifically here discriminator once it's becoming very Fernand which means for this criminal to be Compton means that when we are getting a fake image that's we'll worry about there's a fake image being generated discriminator confidence is fake so fake image discriminator is confident means it generates something close to zero there on the zero side or in the X space X would be going to negative infinity so the sigmoid turns that into a zero probability of being real at that point you're all the way out here where this objective is extremely flat and now if you look at the derivative derivative which is on this plot over here is zero so you get no signal from your discriminator about how to improve your generator to get better samples in the future this called discriminative saturation is this notion where if you discriminate reigns for a real a long time or not even that long time is just expressive enough to be really good quickly what will happen is the gradient for the generator will effectively disappear and so we really can do instead or should do instead is an alternating optimization don't optimize your discriminator all the way till the end till it's near perfect in your current generator because then you will get no signal anymore that's one solution another thing you can do that's often done is reformulate your objective to be non saturating a little bit of math here but let's actually look at the plots here is our y log of 1 minus Sigma X and instead we can maybe look at y equals minus log of Sigma X this is a slight tweak to the objective it's looking at the other side in some sense you have two terms in your objective log of Sigma X and then log of 1 minus Sigma X and the log of Sigma X is living in the front of the two terms the other one is log of 1 minus Sigma X by flipping this 2 minus log of Sigma X you are living in a different regime so you live in them you know what the problem is the problem is always when discriminator confident leads to this X going to negative infinity so that led to the problem that this is flat here and so your derivative is zero and instead you say I want T I'm gonna optimize the negative log probability of just Sigma X you get an objective that looks like this which has a slope here and becomes flat on the other side and so but this decide we care about so where the discriminator is very confident that it's a fake and we see we get a derivative of negative 1 which is not 0 and we get all the signal to propagate you might say well isn't it bad that we have a derivative of 0 now on the other side because we essentially we're choosing with this process either we have a no signal when the discriminator is very confident it's fake or no signal when discriminator is very confident about it being real in practice once the discriminate is very confident it's your curated samples are real you actually don't have as much of a problem we're now generating real samples so you're kind of good to go for your generator the problem is when initially you generate bad stuff you get no signal to improve upon it and so by changing the objective from the original log 1 minus Sigma X into minus log of Sigma X you get that effect and here it's done for the overall objective originally we had this just a minimax game and we're replacing the second term here only for the generator with instead this thing over here and now we keep getting signal no matter how confident our discriminator is with it's not capping off the signal anywhere any questions about this yes say it again oh good question so at the top here we have a minimax game that has a well-defined we might not be able to find it that has a well-defined minimax equilibrium Nash equilibrium once we switch to running this optimization here our discriminator has a different objective that is working on than the generator so this will not be a clean min Max two-player game anymore it is still a two-player game in some sense but it's not a min Max game the way it used to be and a zero-sum instead the utility function for the generator is somewhat different and not going exactly against the objective function of the discriminator so it has different properties when you run this you can't claim the same things about you know Nash equilibrium that you could claim for zero-sum games because nonzero-sum games have not the same kind of properties and practice though typically works better to do this because you don't lose the signal ice typically the difficulty in training ganz is that initially the generators not good and then the Scream inator is so powerful and now I get no signal and you can train yes yeah that's a really good question I mean you're referring to the algorithm here right where is it over here original algorithm the paper I mean they have like four K steps do this which is discriminator up this is say we use k equal 1 so they don't do too many discriminative it's just exactly 1 but in principle you can very down same with the generator you can in principle vary how many steps you do on the generator so yeah there's a lot of hype a parameter choices and what you'll see as arvind steps through the whole progression of ganz is that there's a lot of hype a parameter choices that can make a difference it's a lot of architecture choices that will make this much better behaved and so bringing all these things together as well as a resulted in the results we see today just naively running it as formulated in 2014 is is actually not going to give you that great results you need to be really careful about all the details to make this work yes sure let's maybe hold that question for the end of next lecture because I'm gonna see a lot of variation essentially asking about variations and the remainder of this lecture and next like there's pretty much all variations so let's see what what the questions are at the end of that yes yeah so the question is can you play some hacks where maybe you know even though the derivative is near zero maybe you can boost it up and still recover some derivative I suspect you can make that work it might not be super convenient but there might be a way to make that work and but in fact a lot of variations we'll see we'll also actually have a specific formulation to avoid that saturation so sometimes people build it into the objective in the work that's happening now rather than trying to after the fact correct for it arvin wanna switch over to you cool so you've seen the basics like the foundational math that lets you understand ganz but as Peter pointed out if you just go to the old Goodfellow Tiano code and just try to run it now so you're not gonna get the kind of results that you're familiar with the latest scans so so let's actually look at what has helped make this progress and it'll be a lot of emphasis on architectural and engineering details but at the same time we'll also give you some foundational ideas that have also enabled these kind of things to happen so the first step is we look at this architecture called DC gal or deep convolutional generative adversarial networks this was a paper by Alec referred and collaborators and way back in 2015 basically their motivation was that they wanted to do representation learning it was not particularly to generate images Phil look at the abstract they basically show that like unsupervised learning doesn't has not with convolutional neural nets has not received much attention which is really true at that time because people weren't really pushing on unsupervised learning because CN NS was just working and there was a lot of things to do in supervised learning so they basically are proposing DC Ganz's available or learn good features do good unsupervised learning and use these representations for from the generator and the discriminator and show that they can actually be pretty useful and captured or useful aspects for the image so a big part of the DC Gann architecture is the adoption of convolutions everywhere so it's not that the original Gann was not deep or it's not that the original Gann was in using convolutional layers it's more more that it was not deep enough and didn't use sufficient conversion layers so and that's really how deep learning is progressed over time you just add more layers and you just use better layers and you get better not better results so if you if you starve out so these are just this is a hundred dimensional Gaussian that you sample this is a noisy sample and you project it up with a dense layer and you get 16,000 let's say you get 16,000 384 units and you just do a reshape and make it spatial now so there's usually you reshape it to 4x4 by 1024 and then after that you don't need to use any dense layer it's just gonna be a bunch of D convolutional layers for those of you who are not familiar with this word it's just the transpose cun it's just basically what happens when you do a backward pass of a normal Kong which is the transpose of that and so that lets if you use it with the stride greater than 1 it lets you up sample since because the backward of that would downsample so so from a 4x4 by 2024 you every layer what you're doing this you want a spatially up sample and and you want to check do channel down sampling so this is really the inverse of what you're doing a in a supervised model we're in a supervised model you start off with a big image where very few channels because you have RGB channels and then over layer by layer you're basically reducing the spatial dimension but you're increasing the channel dimension so the inverse of that happens in a generative model because you are starting with just noise and you're trying to get all the way up to the actual image so you can see that you go from 8 by 8 16 by 16 32 by 32 to 64 64 at that time 64 by 64 generating a 64 by 64 image was considered a big deal so you could imagine like you know you I mean it's not that they didn't try to do bigger images and you got you can also see that the channel dimension is being reduced by a factor of two until the final layer which gets three channels the final thing I want to tell you is that images are pixels right like they're RGB values are pixels so just 64 by 64 by 3 that just that tensor it's not going to be an actual image unless you force it to be so what people usually do is they normalize your input pixels to line the range minus one to one that's just obtained by you you just you just take your image and you just divide by one 27.5 and you just subtract one right so so so so or so basically your your your pixels are normalized to lie between minus 1 to 1 so when you try to generate it you apply a tannish non-linearity at the end and that lets your model output that lets you generate or output values that are lying between minus 1 to 1 this is very crucial because at sampling time you should be able to go back to RGB values so there are lots of more details just just like just that architecture is this fundamental part but not the only part so usually in it so in a discriminator there's a normal classifier so or you can interpret it as a classifier so for that architecture you're just going to use a regular convolutional model that you use in supervised learning so in such models typically I do in those days people used to use max pooling to to go to and probably mean pooling in the middle so those are not particularly good for generative models so these are removed from the DC Ganz discriminator and whenever you do the up sampling in your in your generator using transpose convolutions and when you do down sampling in your discriminator because your discriminator is gonna take a 64 by 64 by 3 input and try to go all the way up to the single value which says like real or fake right so you be down something in your discriminator exactly the like mirroring the degenerative whatever resolution you got at the end it'll be in the beginning in the discriminator so there you're going to use strata convolutions to do the down sampling and you use average pooling only at the end before predicting this value just like having a classifier you just use mean pooling at the end you know you never use it in a bit in between and for the nonlinearities in your architecture it was very crucial for them to use a leaky value or non-linearity in a discriminator with a slope of point two this is a leaky value looks something like this where that the negative x part is not not zero but it also has a slope but it could be a different slope from the positive x and usually it's a smaller slope so this is important because your gradients could become really sparse for training something very complex like again so they wanted to use something that doesn't zero out very quickly and so they use leaky earlier in the discriminator and then finally so this is this is actually a difference from the original gang models because the orginal ganon models used the max out non-linearity and like I said there is a tannish for the generator and the discriminator is a binary classifier so you use a sigmoid output at the end so those are the output nonlinearities and most importantly in DC Gann Vash normalization is used for preventing mode collapse this is not a principal solution by any means it's an architectural solution and but it was crucial not to apply it at the output of the generator or the input of the discriminator and this is not a random choice it's because batch nouns are usually applied before a rally or a leaky rally non-linearity it's always like you you follow up a batch alarm with a rally and at the output of the generator you have at an edge non-linearity and input of the discriminator there's known like well your your your your generator outputs are actually they can in a normalized range already so and your input from the data distribution will also be normalized already rely between minus 1 to 1 so there is no need to apply a non-linearity there so it's a pretty principled choice not to use the batch norm at these two locations finally in terms of optimization details they use an atom step size of 2 e minus 4 and the they don't use a very aggressive momentum so that's probably telling you that how how like optimization is any big issue and like you know having too much momentum or no momentum is gonna like make your models not train that well so do you they pick a very reasonable momentum value and the trainers are reasonably big bad size 128 was probably considered big at that time and all these are very crucial details to get these results so DC gain can be summarized as basically taking the promise of a new technique ligand which was not proven yet and figuring out lots of details and getting results for the first time which were amazing and we'll see why there like we see some of the results so one final point I want to make is how you apply the batch norm to the discriminator so if you your discriminator looks at both ex coming from your generating your generator distribution and your ex coming from your data distribution and if you construct a mini batch that has some real and some generated and you pass into your discriminator your batch statistics are going to be average across all of them and this is generally not a great idea but it is both interior and Contra interior you could think about it in both ways in terms of helping you could think that it could try the the batch normalization is averaging the what's relevant at the moment for the generator so you could think about it in a helpful way well in a non helpful helpful way what he could see is like the the statistics are getting corrupted with some noise and like so it's not really learning to figure out the actual statistics that matter which is your real distribution and so it's a pretty messy choice but turns out that when you actually implement models when you write code when you do a particular forward pass and you're just writing min for a last discriminator for and then you have another step which is la you if you're using the non-saturated version you're maximizing the discriminator last with respect to the generator parameters you just do separate forward passes and this is implicitly in short but this was sort of like the Attar it was a bug but it's actually not a bug it was pretty useful so that's another subtle point about batch nom and the discriminator so in terms of results DC Gann was really really good for generating images of bedrooms that's not the goal of course but it was where the bedrooms data set is this Elson data set a particular subset of that as bedrooms there are lots of other subsets but this is a pretty constrained data set it doesn't have a lot of variety but it still has sufficient diversity enough for you to like train generative models and see what's going on and the bedrooms look pretty realistic and pretty pretty diverse because so that shows that DC gang could work and produce realistic images for the first time for a gang model and they also scrape datasets of celebrity faces from the internet and they trained it and you could see that it's actually generating pretty pretty pretty good pretty promising phases for any generative model at a time and the nice thing is you could also interpolate in the latent space so you could take a particular noise vector Z one and another vector Z two and take a lot of points in between and as you keep moving along the Z you could keep getting different X's and you could see that it's basically changing the viewpoint of a bedroom or like changing you know it's sort of like a camera that's moving around basically so you could think of like one of the latent variables as basically the position of a camera that's capturing the scene so it's implicitly understanding the formation of an image the process of forming an image even without any geometric constraints baked into the model one subtle point about this is it depends on how you take the Z's in between you could you could imagine taking a high dimensional z1 and z2 and you could take all this straight you could all the points in a straight line or you could think of all these coming from a spherical Gaussian and you take one particular point in another particular point you just take the circle that is in the sphere and it turns out that you can produce better samples if you sample from the circle instead of taking just a straight line this is not used in the DC gang model but in the later demos of sampling or interpolations the people prefer to use the spherical something and for the first time actually any generative model produced some reasonably good looking samples on a big enough data set like imagenet which has one one point to eight million images but they only trained it on a 32 by 32 down sample version prior to this mod prior to this paper there was another paper pixel RNN which also is shown to work on 32 by 32 image net but the samples from DC Gann are like more realistic looking and the pixel RNN samples so a most interesting aspect in the DC gain results was that you could do vector arithmetic so a lot of you who are familiar with the word to act models in language you you would know that people used to do these very interesting aspects like taking two different words taking a third word and like subtracting the embeddings and seeing what it gives like what is the closest neighbor a neighboring word in your data set and like figuring out all these nice linguistic and understanding and turns out the same thing can be done in four images with a DC ganbaru where you take latent you get a smiling woman and then you get you subtract the embedding for a neutral woman and then you add the embedding for a neutral man and so that way the neutral gets subtracted out looked like you're adding and subtracting it and you're ending up and you're also getting subtracting women so you're ending up with a man because you added a man so so these are like nice latent properties it's very clear that this Z vector is able to capture like the correlations between the Z and the pixels that generates is able to capture these high level semantics without any label data alright so that was very interesting because that means that these models are actually capturing some are doing actual and supervised learning and there are more results similarly like a man with glasses you subtract man without glasses and you add woman without glasses you get of them in with glasses and these are various different samplings right you can sample with adding different noise and then you can see different samples it's whereas if you do it in the pixel space if you just take the image you just subtract the pixels and add the pixels of the other image the results aren't that good they look pretty noisy right so it's very clear that the space is so crucial finally like I said the interpolations really worked well in spite of just doing it in a straight line and you could see that it's understanding the concepts of like poses right like if you could imagine this if you if you think about the G that transforms Z to X think about this gsm graph like the equivalent of a graphics engine that's been written with neural networks right so you're feeding in some input parameters and it's rendering the actual image and as you keep changing some of the values in your noise it's behaving as if it's changing the pose in the renderer right and then is slowly changing the texture and going to the target face so that's really cool and that's really one we what do you want a generative model to do in terms of the representation learning results which I believe was the main focus of the authors they took the discriminator that was trained in the DC gain process and they put the dip they a sheet try to put a classifier on top and even though the DC gain model was trained on image net unlabeled data they took that DC gain model and and and and put a classifier on top and try to Train it on C far where the convolutional layers a have been trained from the Gant training process and they showed that it could do better than a supervised model comparative supervised models at a time and you'd get eighty two point eight percent top one accuracy on C 4 C 4 10 so that was pretty cool that was very encouraging it's not as good as like so this was a self supervised method exemplar exemplar based method and that was doing better than their model and there are other models like you know the work from atom codes from Stanford he'd done a lot of k-means based methods for C far but it was pretty competitive with those methods so yeah I was just so Peters question was would it just finding the discriminator the answer is yes you just take the model you put a classifier on top and you just find unit they also tried I think they also tried just freezing the features and training an SVM on top so both of them work pretty well any any questions so far all right so DC can produce incredible samples for any generative model for the first time like know even though pixel are in an existed at that time DC can samples were more diverse bigger images bigger resolution way more realistic and so it was this was exerting for any generative model and this was exerting for ganz because like ganz was tarlof like an academic paper with a weird idea I was just very you know very fancy but not working but then you know with a lot of hacks and architectural details it started working I was perceptually good the interpret inter inter interpret interpolations I guess yeah your interpolations were great and then the representation learning aspect was also very exciting the problems to address was that the training was really unstable like if you look at all these hacks the all these hacks exist because they were not able to get it work without those hacks so a lot of brutal architecture and hyper parameter choices and one point point at one point that I'm that is not really a negative but I would like to make is because he used bash Nam and the discriminator when you Sam and in the generator you need to sample at this time in a batch mode right so what this means is in supervised learning if you use batch norm you use a running earning average of your best statistics at every layer and then you use this to statistics a test time but in generative models what would would be DC generators they didn't use test the the training time statistics they just used they just sample a at back on batch mode at this time so they would sample a batch of ZZZ and then they would just pass it forward to the generator and get the images and the batch numbers just going to happen to whatever samples you feed him so it's got your samples that you get or be quite correlated across your batch and also it's going to be very much dependent on what your batch sizes because you usually want to use a bigger batch size for bathroom to work really well and it's also really likely to you know very be very different for different samplings so that's not a technically it's a it's fine to do like this but in from in a principled sense you would want the generator model to be a sampler no matter how many samples you want to see so that's that's like a sort of a negative market actually so given all these negatives the next next paper that was pretty significant in Gantz was to address some of these negatives try to propose fixes so just in the DC gain since it was also a bunch of another sort of some principle and heuristic hacks to make Gantt training even better and that is this paper called improved techniques for training Ganz was written by Tim Solomon's and collaborators from open AI and one of the authors in this paper is Peter Chan is one of the instructors and or he was one of the originators of this class too so like so basically they proposed a bunch of techniques like feature matching and mini-batch discrimination historical averaging virtual bash numb and one-sidedly loading all these are techniques so we'll go into details just like how we dive into the details of DC gang and we see what they are doing and why they are useful so feature matching we see Peter mentioned that if you train the discriminator really to optimum it's not very useful because you're not going to get sufficient gradients but then if you don't train the discriminator to optimum you're you're discriminator is not good and just purely focusing on optimizing this objective it's not really great for the generator because you don't really have a good discriminator so instead of just trying to make the generator optimize the output of the discriminator the idea was what if we looked at something else so what if we took an intermediate feature F in the discriminator and you you passed your real data forward and collected the statistics the hidden layer from your intermediate feature layer the discriminator and you do the same thing for your generated samples you sample a bunch of Z's you get your generated outputs and then you pass the generated outputs to your intermediate layer discriminator and you force the features of your generated samples to match the features of your real data sample real data so this way are not just purely optimizing the output of your discriminator you're also making sure that the generator produces samples that are consistent at various feature levels of the discriminator as far as matching the real data is concerned and this so this is something like making sure that you you hope that the discriminator has learned some useful edge detectors and so on and so you're just trying to make sure that these Lola somewhat semi low-level statistics of the generated samples master discriminator and so the generator can't produce some garbage to cheat the discriminator anymore the second technique was a very interesting very interesting technique called a mini batch discrimination so Peter referred to some problem called the mode collapse problem in in in Gant training where when you train again your it's very likely that you're all this noise and your actual noise distribution is basically being mapped to a single point by the generator and the discriminator is not really average as it's happening because it's just looking at each of these samples independently during multiple forward passes and it's not aware of the diversity of the samples that are being produced by the generator and it's just happy that it's able to confidently discriminate between the real and generated samples yeah yep yeah so that's that's why we're going to talk about the minibar discrimination you mean to say are we doing this for are we ensuring that the statistics of the features match we are right you're taking the first order so this is the mean it's a first order statistic yeah yeah you could like you could try to match higher order or like even the sound deviation it's just that it's more expensive like you would then calculate a covariance matrix yep no so that's a great question so the question is this looks very similar to the fresher inception distance which is actually an evaluation metric for ganz so this paper came out even before that and they were not trying to design this objective with that in mind and I'm not sure if anybody's actually tried to run this objective after that metric was proposed and see the correlation between optimizing for it and not as far as the evaluation goes yeah you could think about those it's not actually a gradient flowing from that layer the gradient is going to flow normally it's just that you have an additional loss and so those weights which are you know trying to get you up all the way up to F and I'm gonna get two gradients one is from your normal discriminator output and your this is the extra loss yeah it doesn't change it doesn't add new gradient paths but it gives gives you more gradient for the parameters involved in getting you all the way up to F so the second idea is in this paper is called the it's called the mini bar discrimination idea and and and basically the the the the motivation here was that you want to make the discriminator aware that is it it needs to make sure that the generator produces a diverse set of samples and not just a few realistically looking samples and just passing the generated samples in multiple independent mini batches is not going to let the discriminator be aware of the kind of overall diversity being produced by the generator so one way to ensure that is to incorporate some site information about what the other samples in the mini batch being generated by the generator are so what what what what what the artist proposes the following let's say your X I is one sample in your mini batch and it's producing a feature vector f of X I so they use a make tensor a times B times C let's say that this is an a dimensional tense vector and they use this tensor to project f of X I to a matrix M I which has dimensions B times C right so now this matrix has B rows and si dimensions per row and now you consider two separate samples X I and XJ are mini batch and for a particular row B so there are 0 to B minus 1 rows and for a particular row B you're just going to take the difference between the vectors in that row for these two samples you take the Elven norm and you take the next potential of that so this is a distance metric between the corresponding rows of the corresponding matrices of these two separate samples and you do this for you you basically do this for every single sample in your mini batch right so you have a mini batch and J is just one other sample you can do that for each respect to I you can do this for every single sample in a mini batch and you add add all those values I add all those distances and that gives you a metric of how differentiated the particular sample I is for the corresponding feature vector B index as being your matrix and if you concatenate all these this is just a single vector and if you can carry in a single scalar and if you just concatenate all these different scalars for different rows you get a bunch of distance measures right so and this is a cross ium in a batch so you basically get a new vector R B dimensional vector for every single sample in your mini batch and then you can construct another feature which is your mini batch times this dimension and you what do you basically end up doing is you when you pass it to the next layer of the discriminator you're passing f of X along with ofx you concatenate f of X and all effects together and then you pass it to the next next layer so so this way your discriminator is now aware of a is aware of every single samples distance to other samples in its in its own mini batch and so it's it's getting some kind of higher R across that statistics as a feature vector added to your original feature vector all right so can we take a break now okay okay let's resume so we so among these five different techniques proposed for improving the Gantt training we've looked at feature matching we looked at mini-batch discrimination and next we're gonna look at historical averaging so historically averaging is basically inspired from Nash equilibria and so basically the ideas to make sure that when you're having a complicated two-player game you and and there is always the issue the saddle point optimization making things unstable right so for instance if you have a value function XY and like say one of the one of the two players is trying to maximize this with respect to X and the other players trying to minimize this with respect to Y then it is a good exercise to work out that like the game will basically be circling around in orbits around the center so so so basically that that's one of the main reasons that Gann training is really a magic or like a black magic and to figure out how to how to make it work so the opt is inspired from some techniques on fishies gameplay in Nash equilibria and try to make how penalty for the parameters in both the generator and the discriminator to be closer to their historical averages and you can keep backups of your own parameters historical averages like like in general it's basically the check finding system right so when you're running any neural network you're always checkpointing your latest versions of the parameters and there is a penalty for the parameters not a steer away too much from their previous versions so this way you ensure that the loss is not going to be to oscillating and it's a good regularizer among these three techniques seen so far these are like the three training techniques along these three techniques the most useful according to the authors is the mini batch discrimination and the second most useful is the feature matching and the third most useful is the historical average yeah yeah that's true so her question was it seems like historical averaging is a technique we should just use for any model you train it's not specific to Ganz that's true but in general any model that you train is always using an exponential is always keeping an exponential moving average of your parameters and an inference time you're always using the exponential moving average because that's better for a validation or generalization in Ganz very explicit like like at least in this paper they found it important to use even during training time whereas in other other cases the loss is not too complicated enough that you need to ensure this kind of a stability but these ideas like for instance making sure your parameters don't move away or making sure your output distributions don't move it too much has also been used in reinforcement learning for instance in trust region policy optimization and general trust region methods there is always the KL term that makes sure that the output distribution is not too far away from an older version of your policy so different aspects different instantiations of this high-level idea has been used in multiple places so the next trick that the author's purpose is called one-sided label smoothing but before that label smoothing in general is like a very good idea to use and it's been used in multiple places in supervised learning so the key the basically the idea and label smoothing is the following whenever you implement a classifier you're always giving it targets or labels and you're saying the target is some one heart vector right over your like number of labels so this is particularly like like like this is very clear like this you just want the output to be one particular class but but in general adding noise to this vector is useful because it does it's a good regularizer and it's it's not really gonna cost you anything it's not going to reduce your accuracy too much so let's say you change this to a 0.9 and like let's say there are 4 other classes you can just make the remaining ones is 0.1 by 4.1 by 4 1 1 by 4 and point one before so you're just adding extra noise so there's two marginalized two like 1 but your this is introducing this noise in that way you make sure that the classifier doesn't have to be super confident you can reduce the confidence and that way the mistakes are made you know you know without too much confidence and it's pretty good training something like again and specific the gans it's very useful if you want to prevent like large gradients your generator because your generator gets gradients from the output of the discriminator and so if your discriminator is not being so confident it could stabilize the gradients that you receive for the generator and but but but now there's a confusion right so you could you could basically do label smoothing for both the generator loss and the discriminator loss because both how this output of the discriminator has termed that the optimizing in different directions so in the artist is proposed to use label smoothing for the discriminator and so they don't use the label smoothing in the other version for the generator loss so the generated samples don't have any label smoothing but the discriminative samples had a label smoothing and the argument that the authors make is the following if you write down the optimal discriminator for given the label smoothing noise that you're adding since it's a binary classifiers it's just not not the way I wrote it for instance it's just gonna be like 0.9 and 0.1 because it's just a binary classifier so if you write down this particular optimal discriminator you end up with this term is one minus alpha P data X plus beta P model X in the numerator and alpha and beta are your noise probabilities for your real data and your generated data so the authors made this argument that they don't want any P model term in the numerator because if the optimal discriminator is having a P model and it's numerator then it reinforces the generator to produce more of such samples and so they remove the noise they don't use any label smoothing for the generator output for the generator samples but they only use label smoothing for the real data samples the the argument of the reasoning is very heuristic you could if you take it with a grain of salt but this label smoothing technique is very useful for training gams so finally we look at this thing called virtual batch normalization so previously I made the point that DC Gann uses batch Nam for sampling at test time and that's likely to produce correlated samples because you're you're basically something bunch of Z's and like you're averaging over that you know generator right so sometimes you could produce samples like these where this is all looking really orange or this is all looking really green and and that's basically because of this intra bad statistics dominating the generated quality and so the authors and improve training of ganz proposed to use a different version of batch normalization it's called the virtual batch nom and the way it works is as follows first we introduce something called a reference patch nom so you can create a set of reference samples that are decided beforehand call them r1 to RM and so for any mini batch X that you do a forward pass on you have x1 to X it could even be different sizes so you this is your mini batch and your mean and standard deviation are gonna come from this reference patch and you would use these medians and deviation when you do passional you do this right you you basically do this and you how does a fine term so this mu and Sigma are going to come from our but you would implement the same R for every single forward mini-batch that you the process and this way you're making sure that your bad statistics are not going to depend on your current batch at this time so that's the useful trick but there is a flaw with this trick the flaw is that if you fix one mini batch for your whole training process it means that your models could over fit to that's particular mini batches sample for as a reference batch so unless your reference batch is really really huge it's very likely that you're going to produce very non diverse set of samples so and having a very huge reference batch it's not really optimal because you need to forward pass all these reference samples every time you do the updates right because every single forward pass you're going to calculate the with respect to the current parameters you're going to calculate what these reference samples are producing at every layer a batch norm so the authors introduced this idea called virtual batch nom which is basically you take the reference batch nom and for every single sample you append your sample with the reference panel so you're basically your statistics are going to be collected from that particular sample as soon as all the reference images in your reference set of samples and you're just going to use all of them so your statistics are going to be tuned for your current sample and that way it's not going to be able to over fit to your reference batch but it's still a it it's still pretty dependent on your reference patch but it's probably the best fix you can come up with if you really want to use patch nom with reasonably small batch sizes and remember that this was during times when like big batch training or large distributed deep learning wasn't really Garriott right like people were still training with single GPUs and at the maximum like for two or three do it to two to four GPUs so yeah any questions on this so far okay so one final aspect of this improved training organs paper isn't it introduced the idea of semi-supervised learning with with generative models basically the idea was that if your discriminator is going to take some image and predict real or fake in addition you could basically predict the category of the image and you could and if you're if your data set has labels as well then you while training your discriminator every time you pass in a real data sample you could also produce a classification loss for that real data sample which means that when you do something like feature matching and the intermediate layer if you're a discriminator then the feature matching is now going to be much better objective for the generator to optimize red because that's like optimizing the feature layer of a classifier and you we know from experience in supervised learning that feature layers produced by classifiers are really semantic and so trying to optimize the statistics of those semantics could produce better generate like better quality in your generated samples and it could also met your unconditional generator converge faster and it could also let you do semi-supervised learning which is you don't need to rely on having a lot of labels you just need probably a few labels and the discriminator is able to also learn representations from the unsupervised objective as you've seen in DC Gann and so it's a nice way to unify semi-supervised learning with Jared of modeling and the authors are really good some I supervised results on C 410 which was considered really good at that time but not not not that great anymore but the fact that you could train a jerem model as well as not rely on too many labels and produce really good samples was really cool yeah yeah so the question was a Seaford and good enough data set to focus on far as the metrics are concerned like I said you should remember that the paper was not done at a time and a lot of compute was a common common aspect of deep learning so Seaford 10 is the best dataset to work with if you're trying lots of these different variants right because you want your the results to come really fast and it's still a good data set to try out your basic ideas like a lot of people still work would see far even though it's a salt like you know it's the top one it's close to hundred percent now and all that it's to the fifty thousand images is still a good enough data set to like quickly see some signs of life or a new idea yeah so we could take this offline actually because I mean writing papers is like you know there's a lot of other factors involved in it yeah all right so inception score was something Peter already covered so I'm gonna skip over that it correlates really well human judgment and it has it captures a lot of necessary for diversity because you want to produce make sure that you're generated samples are having as much diversity as your actual data set in terms of the class labels and let's see so these are the samples that were seen in the improved training and Gans paper you could see that the C for samples are really good but the imagenet samples they did an initial attempt and they were likes you can see some doglike images but it's still not getting any or it's it's like producing multiple heads of the dogs you know it's having a lot of these artifacts in the bottles and it's not really that good but it was really good considered good at the time cool so next we cover the more modern versions of Gans after that because these were all like a lot of nice tricks so remember that Peter focused on this aspect of ganz optimizing some kind of a distance metric between your real distribution in your generated distribution and Gans are implicitly basically trying to optimize this Jenson Shannon divergence between your real and generated but you could also think of trying to design Gann architectures of models that are trying to optimize some other distance measure and there is this distance measure called the earthmover distance which is which has connection inspirations are like foundations an optimal transport theory basically the idea is given a real distribution and a generated distribution you're the earthmover distance between them is basically this particular term looks complicated but basically the idea is so far as following given a Joint Distribution between PR and PG you can evaluate something called this which is every expectation over samples from coming from this Joint Distribution the distance between those samples right so you could take X comma Y from the joint and you could just evaluate the distance between x and y and this could be any distance you could define it with respect to any distance so here they use a l2 norm but and then if you if you take the expectation of this over this joint and you calculate this for various joint distributions and the infimum of all over all those joint possible joint distributions is here with no distance it's clearly intractable right but but we will see how the wall sustained Gann authors have come up with a very nice way to like optimize this particular distance so the way it works is they have this duality called the contra which rubenstein duality that has a dual version of this objective which is probably easier to work with and the dual version goes as follows it is the supremum over all one Lipschitz functions of this particular term which is nicely decoupling the expectation so you don't really have a joint anymore you have an expectation or independent distributions where this is an expectation over P Rio of and this is an expectation or this can be defined with any PR and PG I'm just using P real and generated so that you get the connection of the Gann so expectation over X from PR of f of X and expectation over P G of f of X where you're there's like various different have setup possible in the family of one Lipschitz functions and you want to take the supremum of over all those f and that's the DL version of this particular infimum term it's still intractable right because you're not going to be able to enumerate over all possible 1 Lipschitz functions firstly like I hope like your you're familiar with the definition of Lipschitz is basically that we go over the definition the next slide but this is the definition which is basically that for an F that maps from X to Y F is K Lipschitz if you can have distance functions DX and dy on x and y and make sure that dy of in the in the map projected space which is f of x1 and x2 is less than or equal to K times the DX of X 1 and X 2 for any pair of x1 and x2 so basically that you take a function you have an input input space and any pair of points you just map them to your output space you want to ensure that the distance and the output space is only going to be only at most as big as K times the distance in your input space so this is a very guarantee that your dare video or your smoothness of your mapping is ensured and it's a measure of how smooth your mapping is so so that's basically the definition of Lipschitz and the war sustained Gann paper done by Martin Martin or Joe Souki and collaborators is basically trying to figure out a way to optimize the duality of the earthmover distance and it's still intractable but then they're going to come up with some few tricks to ensure it works so the way it works is as false basically if you're if it's okay for you to search over caleb's its functions think about these functions being characterized by neural networks and let's say there is a family capital W and you're trying to optimize this particular distance over that and you can say for sure that you're if you have a particular family and you're calculating this particular measure it's going to be less than or equal to the supremum and this particular family is like family of all Lipchitz function so it's definitely going to be less than or equal to supremum and that's a premium if you have a Lipschitz constants of K it's just going to be K times the earth more distance so the key idea that they are trying to come up with this even if this family is not as constrained to be Lipschitz functions as long as it's somewhat constructs imitate Lipschitz your your your optimizing some kind of a lower bound on your actual earthmover distances you're trying to maximize so as long as you maximize as long as you're trying to work with this particular loss where you're basing basically having a max over some family that's approximately Lipschitz yet it's guaranteed to be less than or equal to the supremum which is the actual definition so so you you and you can think about this as like the distribution that comes for PG could be characterized by a neural net which is your generator and then just optimizing this distance for that generator could could let you work with the standard neural network packages and back prop so after all this building up I'm gonna disappoint you it's how they actually implemented so it if you look at the pseudocode of WG on and i've built up all the Lipschitz nice math and all that but but at the end of the day all they do is turn this is the Lipschitz regularization so they just clip the weights they do nothing else it's just basically clipping the weights to live within a particular value and they do the regular gain objective but the most important thing I want to highlight is they move away from this binary classifier version of Gans so they have the discriminator and the discriminator is just an network that's something like this is this fw which is taking in a real data or it could take in a generator data and it's just outputting a scalar value and you just ensure that this w is fw is approximately Lipschitz by clipping w so there is no classifier now so they have they introduced this notion called critique which is familiar to people who work on deep RL which is basically you can think about the generators trying to maximize the value of the critic where the discriminator is trying to minimize the values for the generated samples and maximize it for the real samples so that's a clear departure from the original way in which people thought ganz people thought about ganz was oh the generators a classifier and discriminate the generator discriminator is a classifier between the real and fake samples but the way they implement the Lipsius nests is very very trivially like way clipping and that's not the best or the most principled way to do it but they do how amazing results which is pretty surprising to everyone at the time so one thing about a classifier which peter talked about is the saturating problem which is that if you have a classifier and this is like yeah I guess the blue is real and this is fake auto-generated and these are the distributions it's all 1d now this is the discriminator and it's clearly classifying the real from the fake but the gradient for the fake samples or the generated samples so the discriminator is zero so it's the discriminated samples are not going to get any gradient from this kind of a discriminator but look at this water stain critic the blue line this one it's clearly giving you a constant gradient it's as close to a straight line so the slope is nonzero so the generated samples can actually get gradients whereas the previous version again has the saturating version has vanishing gradients this is a bit controversial right because if you compare it to the Nonsuch if you compared to the normal Gann nobody uses the normal Gann in the original loss they use the non saturating version which is where the generator just maximizes the log probability of the generated samples rather than trying to just exactly optimize for the negative of the discriminator loss but you know so that you should take this with a little caveat but in general is a cool fact that you can get a critique to work this way so they show that the Wasserstein estimate the was the same distance is pretty correlated with the sample quality and over training iterations as the generator keeps training the wall sustained distance keeps going down and the samples keep getting more and more real and this is what the DC can architecture whereas for the Jenson Shannon divergence they have their own way to estimated and it's not very it's not particularly correlating with the sample quality the main thing that they wanted to show was with exactly the same architecture just by changing the objective and the training mechanism the DC against the WC on samples are on par with DC on and that was really cool so it's second new Gann family but it's working as well but not just that they showed that WGN is somewhat robust to the architecture choices compared to a DC can you saw the DC gun involved a lot of hacks and I really stressed that this again wouldn't really work with on bash nom and you can see what happens you know whereas the WGN seems to be free pretty decent and it could it could work without a bad storm and they also showed that WGN can work with MLPs compare better than a normal again with the same architecture so the summary of WGN is that you in my opinion is the most useful summary is that you're you're basically moving away from this saturated a binary classifier version of Gans to something like a critique where Rd is a critique and the generators is optimizing this particular objective and this Max or Lipschitz families is basically implemented with my clipping so again the authors themselves say that it's terrible and if because there are reasons like the clipping parameter is large then it can take really long time for the weights because there's a lot of values they can take and if it's really small you're gonna get what you usually know as vanishing gradient because gradients are all scaled by activations and if your weights are low your activations also and therefore your gradient so low so but they encourage people to work on it make new new versions of the persistence which was really really useful and the next next version was called the double you can GP which is basically improved training or what sustained Gansbaai by a Sean GU Rajani and others so it's basically going to change the weight clipping version yeah so your lips justness is basically you can think of the discriminator as your whole neuron that you know discriminator right that's the function it's actually representing so just changing the output won't ensure that it's legit because that's I know that your what your argument is that sigmoid is a bounded function and so two sterile areas are like you can actually define a Lipschitz constant for that but the other other the computations so far till then they don't need to obey any kind of Lipschitz criterion right so unless you make sure that at every layer every single weight is capturing some aspect of the business you can't guarantee that because it's a composition of functions right yeah yeah so that's generally true but in gaen training it's pretty like because of this adversarial training your training dynamics are not going to be the same way as regular like say likelihood based models because you're basically trying to do saddle point optimization here you're not you're not trying to calculate some global or local minima like a regular function okay okay sounds good cool so the improved training of Wasserstein gans has a new way to ensure Lipsius true this idea called a gradient penalty I encourage you to look at the actual proofs of these propositions and corollary they're pretty involved and it's in the appendix of the paper but the key areas as follows you know this is the what sustained objective that you already saw and the variance was just using a clipping in this paper the addition is they add another term for the gradient penalty of the discriminator with respect to its input so what is Lipschitz you're trying to make sure that the derivative with respect your input of your function this is not going to be too big right so if that's what you care about if it's all about the magnitude of the gradients then take the if you have a discriminator and you feed in a particular input and it's going to feed in an output rate for the next layer so you can take the gradient with respect to the input of of your discriminator so so this particular term is grad D of X hat with respect to X hat so it's going to be the same dimension as your image dimension it's a pretty large tensor and you're gonna take the norm of that and try to make sure that it's close to one why one you should check out the paper though people have also tried like trying to not use one and just trying to this minimize that norm and basically you have a lambda term that figures out how much you want to give importance to this and the final thing is this thing so what are the samples or which this losses average is it the generated samples like to the discriminator you're feeling in generated samples as well as the real samples so you could take the gradient with respect to the input for both of them but the authors figure out that if you take a mixture which is you take a particular generated sample and you pick a particular real data sample and you do a convex combination of them and this is your sampling distribution just for the gradient penalty objective its heuristic it's not really principled or well motivated but it is the version that works the best you could think about it as smoothening the manifold or which the lips isness is enforced and therefore making the training work much better so the pseudocode is not going to change much from the WG on except for this term and no way clipping now any any questions on so they also use atom instead of Armas prop that's another change and the final point I want to make W can GP which became a standard and all the Gans later is no bash numb in your critique or your discriminator so the reason they went for it is not like oh let me see if I can get rid of bash dominant discriminator it's more because they are objective which is using a mix of your generated and discriminated samples for a sampling distribution for the penalty term there for implementing separate batch nom statistics for generated samples and discriminated samples so is not optimal for them because yeah a lot of the objective is looking at a mixture distribution now so they just got rid of the bathroom because they don't want to figure out a new batch number for the combined statistics and because the the architecture worked really well this became a standard in every single gun after that nobody uses batch norm in the discriminator anymore so WGN GP was more robust to the architecture choices so the little a lot of ablations so they tried various nonlinearities wherein generator discriminator different depths whether there's a batch norm or not and you know layer nom for the discriminator and and also number of filters and so on and it turns out that w can GP like succeeds on a lot of architectures compared to like regular ganz and so that was a cool result robustness is a lot more now and you can also see it visually about how WK + GP produces better samples than w and clipping which is the original WN across various different versions of the architecture and finally the bedrooms got better so that's what matters yeah all right so it was a state of the art on C far as well on inception score and it's a very highly adopted architectural model like a lot of people just take the code and just run it on a new new Dias and almost always works so the paper is very highly cited so this is a summary these are really cool but there are two negatives first negative is the wall clock time is gonna be much slower now for training because your gradient penalty terms in your objective so just to construct your loss you need to do another backward pass right so that's like one extra backward pass and then a backward pass after that so it's it's much slower than just training a normal again without the gradient penalty right so and the second thing is the heuristic distribution of your gradient penalty could be unstable because it could focus too much on your current generated sample distribution and so your atom is generally run on WGN GP with a very small step size so if you really want results very quickly double you can GP is not the model that you want to use because it needs to train much slowly much much more slowly and like there's much smaller step size so the next model we are going to look at is progressive growing against or pro ganas let's call this is a paper from nvidia and this is a cool paper that shows what happens if graphics people come and take over against so this is the first high quality samples ever produce in any generative model and you can look at the samples right these are the samples and yeah they're generated samples so like not even any compare you saw bedroom so far but like this quality of phases is much better and the key idea here is to progressively grow the size of your images both for the generator and the discriminator so you start with 4x4 image is very very down sample like you know these pixel art style very minimal 2 bit kind of models and then you're basically up sampling the spatial resolution by a factor of two every time and you're reusing layers that you already trained and just for adding the next next down sampling or up sampling more layer and your generator and and it got it doesn't generate just faces you can also generate like various categories in the Elson data set so very realistic like can generate like Android phones and all that it's pretty good cycles animals everything real real world objects and the bedrooms got better so so yeah these are also one megapixel images just unheard of than any any any jeredy model to them and this is you can see how it's you know it's able to slowly scale it a size up across across various various layers the exact details of the architectures it's not in the scope of lecture to go into it you're really encouraged to go and read the code because the paper is hard to read so the final version or the final set of versions we talk about is the essent gun and it's follow-up switch adopted it this was a very important paper and this is a lot more principled than just up sampling down sampling but and it also produce amazing amazing samples so so for that we need to understand like fundamentally like a few ideas on Lipschitz nests and as somebody asked before but how you know what if you just put a sigma at the end wouldn't it be Lipschitz and all that so so let's try to go over that now imagine this is some deep neural network and these are all like weights that are multiplying and this is the activation so the activation could be something like aurelio and you could think of your discriminator as some neural net that takes a stack of affine transforms and puts nonlinearities right and this is your org an objective like your usual you know expectation or log DX real plus log 1 minus D X Prime ex-prime generated so you could think of that as the objective right and now instead of maximizing over d you could just in the W Gann style you can think of maximizing over Caleb shits functions of this pretty clear objective so that is really the W and story so far now this paper talks about how to enforce Lipsius news without way clipping a gradient penalty in a more principled way so for that think of let's focus at every single layer what happens so G is a layer that takes an H in and outputs H are these are hidden layers and you could define the Lipchitz constant of a particular linear transform as this particular term which is the spectral this is a spectral law and the spanned this is your gradient with respect to your input this gradient with respect your input and G is some layer which takes in H in puts a weight matrix and a non-linearity and outputs a chart so firstly what is the spectral norm for those of you are already familiar optimization or linear algebra spectral norm is this written this way it's this particular optimization problem which crop which can in turn be written this way which is max over all vector is bounded by unit norm of a times H that's the spectral norm for this matrix a and this is basically the largest singular value of a if you do a single value decomposition away this is a largest single value you can you can read up on Wireless the case so given a particular linear layer with a resident on linearity the what is if you if you if you take an input H and multiply by W the gradient with respect to H is just the weight matrix right so this particular term is basically just going to be the largest singular value of your weight matrix and we will use another inequality from which is that if you compose two different functions and take the deliciousness of the compose function the constant is going to be less than or equal to the product of the individual functions so now with all these tools let's just take the layer and do the forward passes and figure out what the final Lipschitz function of your discriminator output should be because you can think of a neural net as composing functions on top so you just upper bound the ellipsis Nisour the whole function is the product of the individual arias Lipsius nests and every single non-linearity slips just is this one if you use really non-linearity because the slope of rally is one so so that way your lips justness of your entire discriminator ends up becoming the product of the singular values of every single weight matrix in its in the discriminator and the idea and spectral norm is to take every single weight matrix and scale it by spectra singular value or the spectral log so that's why it's called spectral normalization you take the weight matrix you calculate the singular value plus a singular value and you just divide the weight matrix by that value and you do this for every single weight matrix that's let's literally spectral along and if you do this and you train this train it with this you are basically ensuring that the output is very Lipschitz so so far so good right one one obvious question is is calculating the largest singular value tractable for from kind of the dimensions that we operate with neural networks so that's that's where they were very clever and so before that before I actually get into the details of how they do it I also want to point out one more key change in the gann loss that they did which was to adopt something called the hinge loss where the loss for the discriminator became something like an SVM classifier and for the loss of the generator just remain the same and it turns out this version works better if you care about optimizing the fresh at inception distance somebody asked previously like how do you try to optimize directly for that or like some device objectives for it turns out like this is a good this is very correlated optimizing for the hinge loss is better if you care about the evaluation metrics more so here's the architecture of SN GaN ice to tell you how they implemented the spec nombre efficiently but the architecture is very very similar to the WGN GP architecture it uses down sampling for the general discriminator and up sampling for the generator and the labels if you if you work with labels your labels are just concatenated at some point in between in your discriminator and in your generator they they use in something called the conditional batch normalization which we look at in the next slides so this time you sing like given pizzas right you just had pizzas so Gans can generate pizzas you can generate a lot of good-looking dogs so I think it's this Gans I just was just getting better and better and this paper improve it even further so let's now look at the conditional Vashem idea how spectral normalization is implemented at every layer and finally just like let's just summarize everything the S&T on paper then so before looking at conditional batch norm I want a lot of people over not aware of how bash now works for con layers to be aware of it bash na means you're averaging statistics across your batch but in a convolutional layer your effective batch size is multiplied by a spatial resolution you're taking your average across both your batch dimension your high dimension and your vid dimension so this is really like the standard con batch norm that everybody uses and these are like your gamma and beta are just some learning variables so this is bachelor now what happens in conditional bathroom is this particular term we're the gamma and B'Etor our functions of your are some functions of your class variable so your gamma is some function of your Y and your beta is another function of your way and these gammas and betas are what is used to do the a fun transform of your normalized input so this way you're fusing an information of your class variable at every single batch normal air that star in your generator and therefore this is much better than just feeding your class variable right in the beginning and not doing anything about it at all this way your generator is like really aware of what class it's trying to generate and it's averaging the statistics that are related to make sure it produces an image of that class now with spectral organization this particular variable that takes in the class variable and tries to produce the gamma and beta or this particular weight matrix that does that is nom by its spectral norm so that way even conditional random variables are not using spectral law so here's the spectral norm idea it basically is you like if you if you've done power iteration before you're very much aware of it but it's it's really the simplest version which is you take of it you take a particular pair of vectors U and V and you just take your matrix that you're trying to calculate the largest singular value for and you you just try to multiply it with W times U and normalize it and and and you can initialize that as your next B to D and then you take V Tildy it'll be a transpose because your matrix is rectangular and and and you multiply that with Vito DN e you figure out what is the next utility so if ur a square matrix is basically just saying that if you just keep figuring out you take a matrix a and you just keep calculating a times B multiple times with the latest week you converge to a fixed point and that's your a sync singular vector LA so for a rectangular matrix you need two different vectors of the row and column dimensions respectively and the most important thing is you're going to back drop through all this right so because you're going to back probe through all this you're not going to get gradients for me and you so you put a stop gradient on U and V and U and you're only gonna take gradients for your weight matrix W and you take the normed value and you're passing it to the next layer and you do all the operations with it so this is basically how spectral norm is done now here's the magic this is the magic you just need to do one iteration of that you would imagine that converging to the fixed point is going to take you a lot of time and therefore is super infeasible but the magic is you just do it one time so this one single follow-up in a single forward pass you can cut it calculated super efficiently and it's you don't even need to do the explicit spectral norm so so correctly and it still works everything works so that's literally it this is now the final final detail is spectral normalization it's very obvious if you use a dense layer because you just take the matrix and you multiply it and all the math works out now for a convolutional layer your weight matrix is not exactly a Maitre your output is not exactly a multi weight matrix multiplication of your kernel with your input right it's repeatedly applied at multiple layers but it turns out that if you just take your if you just take your weight kernel tensor and you just flatten it and you do the same thing that you do for a dense layer it still works in reality a convolution is actually a if you want to write a convolution as a dense layer it has the structure like a top Lutz matrix and so that's where you need to apply a spectral limitation on but in practice the authors just got it to work by just flattening the kernel and getting it to work it's kind of I thought it was pretty amazing so so this is the summary of Essen can it produce high quality class conditional samples at emission at scale and the first gaen model to work on full image net and it has a lot of computational benefits over WGN GP and uses way more principle versions of insuring Lipschitz news so you can see the computational benefits are very clear right so some began GP is really slow this is like a forward pass over 100 sample undred Z vectors and you can clearly see that s in ganas like pretty much more stable and comparable to wait an arm in terms of the inference speed so essen can also use this idea called the projection discriminator which also came from the same set of authors from preferred networks where the class variable can be inner product and with the intermediate feature layer of your generator and then can carry this is how you want to apply the class variables or feed in the class information for your discriminator and and so you the version that I showed you just concatenated it somewhere in between where you could also imagine introducing more multiplicative interactions by taking inner products and then concatenating the product inner product at the end so that that version also works it seems to work much better so essen can worked and authors from so hands and other people Han Zhang in Goodfellow and others they they took s in Gann and introduced this self attention architecture on top and we know from previous classes that self attention is really useful so basically that that's what they did you take with conditional feature map and these are your query key value and you you basically do Q times K transpose softmax is your logit and then another matrix multiplication output projection and suffered and that's it so they have a scaler instead of a actual output projection which is somewhat different from regular self attention but otherwise is pretty much the same and sorry so one other difference is it applied spectral norm to both the generator and the discriminator whereas the essent Gann theory was that you wanted to make the discriminator Lipschutz and you don't really care about doing it for the generator but it turned out that it was good to use it in a generator as well so it's a good good regularizer and it uses self attention in both the generator and the discriminator adopts the hinge loss that we saw and it actually was able to produce very good samples even with our class levels which is not the case for us in Ghana and I needed after the same mechanism for the class conditional version so you can really see the samples are getting much better much more diverse and it was able to work at imagenet scale and the inception score increased a lot like a significant increase and the fresh and inception distance significantly reduce so it was really surprising when it came out so now with all these tools you know that so what is left over right I think the leftover part is this is scale everything and that's what happened in began so these are the samples it's not even comparable to what you saw so far right like it's it's basically as good as a real real version unless you you know magnify it and look at what what's going wrong at the pixel level so so this was basically emission at 512 a fire in 12 class conditional and it was able to interpolate really really well between very different images and so one thing that the author's recommended in the began paper there's this trick called the orthogonal regularization which is complementary but not the same as doing speck nom you take your weight matrix and you try to make it orthogonal and to make it orthogonal you just need to take the transpose and multiple make sure its identity and this was something done earlier and in began what changed is in your identity matrix you don't want to penalize the diagonal terms you just want to penalize the non diagonal terms and it's stay show that that version works so that was the change only change mathematical change and began and architectures wise they are two different architectures began and began deep when first began came out they were not able to make it work or on deep architectures so they had to feed in the class information by splitting the Z vector into different parts and feed it at different levels but then they figure out how to train it with a deeper model and they got everything working so this is not really relevant now but the deeper model just feeds in the Z and a class at the beginning and the concatenated vector the same vector is fair at multiple levels for the conditional rationals so this is the architecture of began like it erupts a very similar mechanism to the WGN models there are like residual blocks and it could be either up sample or down sample depending on whether using the discriminator or the generator and this is the big and deep architecture which is the the residual blocks here are more similar to water using a classifier and the main trick this whenever you do down sampling or up sampling if you're trying to go in your channel of sampling if you're trying to go from C to 2 times C which is you're just gonna double the number of channels what they do is they don't do one by one cons that takes in a dimension D and multiplies it to D they just retain half of the channels they just retain the channels they have and just try to predict what they need to add and whenever they need to drop it they just drop it they don't do it one by one Khan it seems to work much better as far as metrics goes so that's some trick that they discovered and began so so basically began these are the salient bits increase your bat size as much as possible so if you have a lot of TP use you can like I think they use 502 a course which is basically like fire until voltage right and use cross replica of national your rational statistics are not just averaged over your mini-batch in your current device but in your local device but it's averaged over all the different course and you increase your model size you make it wider and make it deeper you fuse class information at various levels using conditional batch norm and projection discriminator use a hinge loss and use something called the orthogonal regularization and the truncation trick you can get amazing samples so we probably you can look at the oblations in the paper to see like how much each if it helps there really ablated it well and and and all the inception scores and fresher inception distance of a better than the Sagan metrics that you saw and they also use the self attention in the architectures so one main thing is that they do something called the truncation trick which is you sample Z from a normal distribution but you truncate the values you clip the Z that your sample so that it's within some constant range and they found that this orthogonal regularization was so useful to allow them to use this clip sampling because the sampling distribution at test time is changing from the sampling distribution that you train with because we're training time you don't clip it you only clip it for producing like more realistic samples and you don't want to produce like spurious samples and they found some of the regulations to be pretty useful to do this at this time and finally they make sure that at sampling time you unlike DC Gann which relied on bad sampling they actually ran various forward passes of different noise vectors at test time collected lot of statistics and used those statistics to sample for any single image now so that way it could work even with this if you just want a sample a single Z you just take a single Z and pass it forward you could use a lot of these test time statistics at for for the model and why you could produce a lot of cool images like a house of bird you could also produce some mixture of classes here so this is a dog and a ball so became a dog ball and let's see and like you saw the interpolations are really cool you could play around on the tents of the hub of their release models so finally we look at style gam it's it is the latest or latest and greatest of Gans basically the style gun is an idea there they move away from this concept of introducing a Z right at the beginning you already saw that in began they tried this idea of splitting Z into multiple parts and feeding it at different levels stankin took it to another level where they basically said there is no Z you feeling at all you just take the Z and you'd have an MLP that maps Z to another W and all you want to do is take that W adapt it in terms of what you feed in at various levels but don't feed anything directly you just start with a constant vector so this is a constant learner vector and there's no stochasticity here the only stochasticity comes from feeding w's at different levels and you could think of these as style vectors so these style vectors are based on what they do at every level yeah they're they're functioning as style variables of different nature at a more higher resolution they are behaving as more fine-grain Styles at a more lower cores resolution they're behaving us more high level aspects like posts and all that and we show you some examples and finally there's this noise variable which is the Gaussian noise it's very common in gang training to add both input and feature noise at various lows and so they also add a Gaussian noise at various lows so a crucial trick in style transfer not specific to against it's a style transfer trick it's called instance normalization in instance normalization if you have a tensor batch height with channel you only take the average statistics over your spatial dimensions so you calculate separate statistics for every single sample for every single feature map and that's very useful in style transfer you can actually look at this paper that proposed instance down for style transfer so the Saigon authors took instance Nam and made sure that these affine transforms on top of that just like how you saw in previous generated shoes now these are a function of your W vector that's coming in at various levels so this is some F of W and this is some G of W and this is done for every level so this is called the adaptive instance nomination mechanism let's try again so this is the stag and transfer now this is a source a and this is source B and this is more like middle and this is course this is fine so you can see that when it's doing the middle when it's doing the course things its retaining all the backgrounds from source a and taking the other aspects from source B whereas when it goes more middle and fine-grained and source a it's retaining more like more fine-grain aspects like the the eyes and the nose structure everything and but but it's taking things like the hairstyle and the background from source B so it's basically figuring out a bunch of latent variables which are like rendering variables some of it is going to be like high level like the background the lighting the hairstyle like they had color and all that and the low level details are like the eyes the nose the way the face is structured and so on whether there are glasses or not and whether they have like facial hair everything and it's gonna combine mishmash the mishmash these two things based on like what is the particular style vector that you're modifying which source you're taking which vector from and this is basically the dream right and generative model you should have these hierarchy of features which lets you learn the very different things some of them are very fine-grained some of them are more semantic and style game lets you do that in a completely unsupervised way and these are like really really realistic looking faces right like unless you really focus on the details it's really hard to say these are not generated from a John and the effect of adding the Gaussian noise at various levels is also clear the left is basically without Gaussian noise and the riders with Gaussian noise and you can see that the left is more like they you know some if a painter were to paint the same person it doesn't have the rich detail of the skin as the right has like it's it has more low-level fine grain lighting details and so on so that's it really for today's lecture like you've seen a lot of Gans you've seen how Ganso progress from very foundational mad that Peter did to like very architectural details that I covered so if you are like looking to do something new and gals both are really important that's the message you can't do one without the other thank you [Applause] 