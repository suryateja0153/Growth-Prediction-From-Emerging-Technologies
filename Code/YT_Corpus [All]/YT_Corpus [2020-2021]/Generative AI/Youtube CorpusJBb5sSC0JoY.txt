 - Great, all right, let's get started for today. A couple of logistical announcements. One, your homework one has been release last Friday, and is due this coming Tuesday. It's posted on Piazza. If you haven't seen it, take a look sooner rather than later, and you still have time, but don't start too late. Other things, I just posted the slides on to Piazza for today's lecture. It's still in draft, there might still be a few changes, at which point, we'll post the change slides later today or tomorrow. Any questions about logistics? Okay, let's get started on our technical part of Lecture Three then, Likelihood Models, still, but now, Flow Models. Oops. I didn't mean to (students laughing) wipe out our TAs. Still welcome. Okay, what's our goal today? We're gonna wanna fit a density model, P theta of x, with x continuous in some n-dimensional space. So a random variable, n-dimensional space. What do we want from this model? Wanna be a good fit to the training data, really, the underlying distribution that generated the training data. The p theta's will shift up there. For new x, we want the ability to evaluate p theta of x and want the ability to sample from p theta of x. And ideally, we also want a latent representation that we can work with, okay. What is different here from what we covered last lecture? Last lecture we covered also, likelihood models. We also wanted to sample. We also wanted to be able to evaluate the probability of a given sample. What's different is x is gonna be in a continuous space now which was not the case last time. And well more of this, I'm also finding a latent representation. Last time we found at the end, that maybe with Fischer scoring, we can find the latent representation, but it wasn't very directly build into the model. So here, we're gonna hopefully find one that's more directly built in. So the way we're gonna break it down is we're gonna build the foundations by working in one dimension, so one dimensional densities. Now, I understand there's many methods to solve for fitting a one dimensional distribution to one dimensional data. But the idea is that actually, a lot of things we'll see in 1-D will directly carry over to higher dimensional distributions. And so about, maybe close to half our of lecture, we'll look at the 1-D case and then in the second half we'll see how we can use the same ideas to go to higher dimensions. So quick refresher, what is a probability density model? Probability density model is something where horizontal axis here is a random variable x could take on, in this case, any value in the real line. Vertical axis is a probability density. The thing of course, with a variable that can take on any value in a continuous range is that you cannot assign a probability that's non-zero to any specific value, 'cause then it'll add up to infinity essentially. If you give every possible value a non-zero probability, it doesn't work out. So the way it's done it's essentially, the meaning of the densities now, the probability of x landing in a interval from a to b. For example maybe a is 0.6, b is 0.8. We'll call this a, call this b. Probability of lying in this interval is given by the integral, so it would be the probability mass under here measures the probability of landing in the interval a to b. If I asked you for a specific number with a probability of x being equal to 0.7 it'd be zero 'cause exactly 0.7 has zero probability but falling in this interval has a finite probability. So that's probability densities and those are the things we're gonna wanna somehow learn from data. So, how do we fit a density model? Let's say we get some continuous data shown in the left, so 0.22159854 and then another number and so forth. We can use the same principle hopefully as we saw last time, maximum likelihood estimation, so we're gonna look at maximizing, so find the theta that maximizes the sum of the log probabilities of each of the data points. Or equivalently, if you have a minimizer in your framework, which is often the case, we're gonna look at minimizing the negative log probabilities and sometimes we'll write it as a explicit summation over the data, sometimes this is just an expectation which is also a summation over the data if it's an empirical expectation or if it had a ground truth continuous distribution, you would have a integral there. So objective still looks a lot like what we had before. Question is, how we gonna do it? One thing you might have seen is that you can actually fit a mixture of Gaussians to data. So if p theta of x is a mixture of Gaussians, so here we have k Gaussians in the picture, k equals two, two Gaussian components, each Gaussian has a certain weight, pi i, is a number between zero and one and all the pi i's together sum to one. The probability of that, when you draw a sample, it would come from that specific Gaussian, i, and then each Gaussian has a mean and a variance. And then if you plot this out, the total distribution will be the weighted sum of these Gaussian distributions here. They're pretty much zero when the other one's nonzero, so would just be this thing over here would be the overall density and so theta then would be the weights, the means and the standard deviation. In practice often we do learning actually just so you're aware if you haven't seen this, instead of representing sigma as a thing we try to output from a network or something, which is kinda difficult 'cause sigma has to be positive. It's convenient when you don't have to worry about it having to be positive, so I think you would instead output log of sigma one would be output from your network and then you would just take the exp of this to get your sigma one. Okay, so this, actually a quick raise of hands, who has seen fitting a mixture of Gaussians before? Pretty much everyone, okay, great. Now, think about a mixture of Gaussians, high-dimensional data, it's actually not gonna work very well. Imagine you're trying to fit a distribution over images and let's assume our pixel values are continuous, let's say a number between zero and one for R, G and B. So we've a continuous space and there's an example of a picture, in fact dogs of one of our co-instructors, so that's a real picture on the left and then if you do a random perturbation around that, a Gaussian, mean zero, random perturbation to each pixel around that you get the picture on the right and you can still recognize the dogs, but it doesn't look like a natural image anymore, so we can already see that if you're gonna naively try to model your data as putting let's say a Gaussian centered at each image and then putting probability mass around that with a Gaussian, you're not gonna get anything that resembles a correct distribution over possible images. So even though I bring up the mixture of Gaussians here, something that is a way of fitting densities before, keep in mind this is actually not what we're gonna do in high dimensions 'cause it's not gonna be the right model to capture the complexity of the distribution you have in high dimensions. So how do we fit a general density model? So we have maybe go in x and then some neural net processing 'cause we wanna be very general so something parametrized there that we can learn and maybe outcome's p theta of x. That would not be unreasonable. In fact, that's kinda what we did last week. We would feed in an image. It would go through a bunch of neural net processing and out would come the probability of the image. It would be evaluated. There would be a softmax at the end that effectively evaluates the probability of the specific image that we have. But if we have continuous variables it turns out we cannot just put a softmax at the end to ensure that things normalize and be all set. We have a continuous distribution. We need to somehow show that things integrate to one and if you were to do the equivalent of a softmax that would mean something along the lines of, direct equivalent would be I'm gonna put something there with infinitely many possible outputs and somehow do something with that and actually we will do it with something close to that but kinda naively doing that, it's not clear how to do that. So you might say, well, how about we just don't care? How about we just make sure p theta of x is bigger than zero? The probability density should always have positive values, never negative, and we don't worry about this normalizing. Well the problem is if you don't worry about the normalizing you're gonna do max over theta, log p theta of x and this is gonna be on your training data, let's say, sum over i. Well if there's no notion of normalization you're just gonna drive up these numbers. You're gonna try to make the output of your neural net as high as possible. You're gonna drive it to plus infinity for everything you see and maybe even for other things. There's nothing holding you back from doing that. The normalizing, essentially ensures that you don't just try to put high weight on your training data, you also try to put low weight on other things so that you really understand the distribution rather than just try to output a high number at all times. So this is absolutely critical when we cannot ignore that we have to normalize. Otherwise we won't learn anything meaningful. So that's one challenge we're gonna have and different from the discrete case 'cause we can't just softmax it. Then the other challenge we need to think about is how to sample. Okay, here I drew it with x coming in and p theta of x coming out. Well that's not exactly a mechanism for sampling, so we need to think about that and then hopefully there'll be some latent representation 'cause it's also not really clear how to get out here. Let me pause here for a moment 'cause the next slide is gonna introduce the main concept and make sure that the challenges are clear. Okay, so how are we gonna resolve this? Instead of outputting p theta of x, we're gonna output often vector valued, but in our case, you see we're now in 1-D, a scalar variable, z. z is then some function f theta of x and generally z could come from some density, p z of z 'cause if x is continuous and then you process it for a while, out will come, again, a variable that's continuous and can take on pretty much any value unless you've a very special property to your neural net to bound it. So you'll have a continuous variable, z, coming out. It will be called and that whole process from x to z will be called a flow. So that's just what a flow, a flow just is the mapping from x to z assuming it satisfies a few properties and then if z is forced to come from a normal distribution then it's gonna be called a normalizing flow. So normal distribution, you say Gaussian with mean zero and standard deviation, one. So that's our main vehicle that we're gonna be using. We still have a lot of questions in how to train, how to evaluate the probability density, how to sample, but in some sense, what you see here is that we're getting around this normalization issue by saying we're not gonna output directly a density per se, we're gonna output from this transformation variable z which has a predefined density that we're gonna maybe have be a normal, maybe uniform and so forth. I don't know if there's some pictures or not showing here. Let me see what's going on. So. To do training, we still wanna do, oh question, Edward. - [Edward] Yeah, so why can't we just go off autoregressive method but instead of outputting a logit, just output a one-dimensional mixture of Gaussians? - [Pieter] Can we output a one-dimensional mixture of Gaussians? We could do that. That's a very special case we're gonna look at and we're gonna try to do something much more general but you can do that as a special case and we'll see that. So how do we train this? Well we wanna still maximize over theta, sum over i log p theta of xi, okay? But we don't have direct access to p theta of xi, we instead have this neural net here that turns x into z and we have some assumptions on z. So we need to somehow pair those things together and make it work. The thing that we're gonna need is essentially look at the change of variables formula, so let's say we have two variables, x and z. So let's put x over here. Let's put z over here. And we have a function, f theta of x that turns x into z. So this x over here will end up over here in z. And we're trying to figure out how do these two, if we have a distribution over x, what distribution doesn't it use over z? So imagine we have a distribution over x, that might be, I dunno, oops, where'd it go? Imagine you have a distribution over x that might be something, maybe something relatively simple like a Gaussian just like this. Then I might wonder, okay, this region over here of width dx, where does it get mapped to? Well it kinda gets mapped to this z over here and so if we wanna know how much, so we'll go up and this gets mapped over here and we might wonder how much probability mass should now land in this interval around that new point, z. Well, it's gonna have to be the same mass as lives over here. So whatever probability mass is there will have to live over here. So we'll look at this mass and for z we might have, I dunno, if this happened, let me draw it a little more explicit, so, well this one's kind of roughly sloped at slope of about one, so it's not the most interesting case. Let's find a different one. Let's say it's much more steeply sloped and we go from interval here to here and that's a very steeply sloped part of f. What happens is the little interval in x here goes very steeply sloped, gets spread out in z space and so if we had a density here, x, that's about this much, it's gonna get now spread out in z space over a wider range. And so if we have a steep slope, what we see is actually we have a lower density value 'cause that same probability mass now has to fit over a wider range of C. Well generally what we see that the derivative is what matters 'cause the probability mass is preserved, that's this equation over here, which means that this thing here is dz, dx, so if z goes up very quickly, then this number here is high and what that means is since these are equal, then then this is p z of z which means if this number is high here then this number has to be low to together equal what the original number was. And so the steeper the slope, the lower the density on z and the flatter the slope, the more density you build up on z, so special case would be if it ran, let's say you had a function f that somewhere ran near horizontal like this, then for this region of z, we'll effectively get this entire region of x mapped to just this narrow interval here for z because the slope is very flat. So this is something that gives us a way to transform between two variables and if we understand the density of one, we can know the density on the other one. For this to be true, requirement, f theta has to be invertible and differentiable. So if you actually had something for this to be able to go back and forth, you got something that actually runs flat, you can't go back and forth 'cause you don't know where it came from. It runs flat for a while, then we have many x's mapped to the same z and you can't invert it anymore. Similarly if it goes vertical, but function's not allowed to go vertical, but if you had a vertical mapping somehow, you'd have one x supposed to map to many different z's and you don't know how to map it, it's not clear how to do it. So invertible and differentiable is key. Invertible and differentiable actually if you draw it in 1-D tends to mean that you either always go up or always go down. You can't have a up and down pattern 'cause once you go up and back down, you cannot invert anymore. Just to make that a little more explicit, imagine you. Eraser. Imagine you had a function that maybe, f theta that went up and then back down and back up. What will happen is x here gets mapped to this point but then that point can also be reached from here, can be reached from over here and now it's not invertible. And so you can't use this kind of map because if you use this kind of map, you cannot invert and hence there's not a one to one correspondence between density on the x side and on the z side. Also, this equation would be incorrect even though you might be able to go in one direction, the direction you can go in actually will not support just using this equation 'cause you'll have to sum it over multiple spots you could be coming from rather than just coming from one spot. Okay, so our objective is still max over theta sum over i log p theta xi. We've now seen that z is a function, f theta of xi. We've seen that we can find a correspondence between the densities based on the derivative of that function at the point where you're looking at the correspondence between densities and so what it actually means is that you can use this thing over here, sub it into the objective and once you sub it in, you end up with this objective over here and that objective, we can actually optimize assuming we have some expression for p of z. So imagine, for example, you have a Gaussian density for z, then log p z of some whatever that transform value is, will be something you can optimize for and try to maximize and in fact what you'll try to do with f theta if p of z's a Gaussian, you'll try to see if your f theta can channel your x values into the center zone of the Gaussian 'cause that's where there'll be higher probability mass and so try to map things into there. And then the part accounts for the fact, well, as you map things, essentially, it comes down to normalizing in some sense ensuring that this all works out and is natural proper distribution over x and not something where you can put everything in the center of the Gaussian. That doesn't really work. Okay, so now that we have this, we can just feed this into an optimizer, run Gradient Descent and find a flow from x to z and if we train this, we know that we've now found the maximum likelihood estimate of a distribution represented in this implicit way, represented distribution by finding a mapping form x to z which allows us to optimize the likelihood. Question here. - [Student] How do you pick what the distribution for z should be? - [Pieter] Yeah, so good question. What we described so far is just a general principle. There's a lot of open questions. One is what is the distribution you pick for z? Another one is well, what's the neural net architecture that you pick for the f theta? And you have to think about the notion it should be invertible, it should be differentiable, differentiable's usually not a problem with neural nets. They're built to be differentiable 'cause you want a back prop to optimize, but invertible is a big consideration that you need to satisfy. And so we'll look at that soon. We'll look at it a lot more when we look at high dimensional situations where it's harder to think through this, but we'll see a few examples for 1-D also. Now, we've seen what we can do to train. How about sampling? Actually, sampling, since we have required f theta to be invertible, is fairly simple. z comes from a distribution that we know, for example, a Gaussian. We sample from the Gaussian and then we just work our way back because f theta's by definition invertible and we can find x and so any new sample just needs a new sampling of z and out comes an x. And in fact the original thing we were thinking about, finding a good latent space is happening here too. Z's our latent space, then we'll see later actually has pretty good meaning relative to the original pixel space. Going to your question, what do we need to keep in mind for f? Well recall that the change of variable formula requires f of theta to be invertible and differentiable, so we just need to keep that in mind. If we design any kind of architecture, make sure that that is satisfied, otherwise, the optimization objective we're working with is actually invalid, it's ignoring the fact that there might be multiple things mapping to the same spot and you're not optimizing correctly. So let's look at some examples before I dive into those specifics. Before training, top left, we have samples from a 3-D distribution of x, it's not exactly a 3-D distribution that we're seeing, but the data distribution's sampled from the true distribution. Then we have a flow, which is a mapping from x to z, which has to be invertible and differentiable, which it is. And then out comes the empirical distribution of z. But we wanted z to be uniform and clearly z is not uniform so then we do training of the flow which is just a mapping from x to z. We train it and after a bunch of training we see it has a different shape now that makes z close to uniform and currently actually is much higher likelihood than the original flow. One thing to pay attention to here is because we tried to map onto a target distribution that's uniform, we see that we design the flow to always be between zero and one because if you output anything outside of zero or one, that's wasted probability and you can predesign it and actually going back to Edward's question, this one is actually designed to be a mixture of Gaussians cumulative distribution function if you look under the hood and so it allows you to by a single cumulative distribution function of a Gaussian goes from zero to one for any distribution, cumulative distribution function goes from zero to one. A mixture would also go from zero to one, but with more degrees of freedom in how you go from zero to one and so this one has a few components. Do you remember how many mixture components here? Five, five, yeah, so five mixture components. It's enough to get this shape in the flow. And so what you learn effectively is the mean and the standard deviation of these mixtures components as well as the weighting of each mixture component in the mixture. You don't have to go to uniform. You can flow to a beta five, five, which is a peak distribution somewhat but with bounded support and so same thing here, top left distribution of the data samples, x. The original flow, x to z, original empirical distribution of z, it's gonna have a pretty bad score and as you optimize the flow, which is under the hood, the means, variance and weighting of the mixture of Gaussians, the cumulative distribution function of that mixture changes into something that maps it to something close to a beta five, five. You could also try to flow it to a Gaussian. So here then you actually don't have to worry about having your thing mapped between zero and one 'cause a Gaussian spans the whole real line and so again here, initially, we see something that is near the identity function, just outputs something very close to the original, but then after training, it outputs something close to a Gaussian, which will have much higher score. Any questions about these graphs before we move on? Yes. - So when you say you flow to the same, you mean that the same distribution for z is Gaussian? - [Pieter] So in these scenarios, here the assumption is that the distribution for z is Gaussian and the way that will be reflected in how things are trained is that when you look at this objective over here the p z, will be the density function of a Gaussian. And so that's essentially what forces it to try to optimize to get out the distribution that looks like a Gaussian distribution on the other side. - [Student] So you choose the parameter to the Gaussian? - [Pieter] Say it again? - [Student] Do you choose the parameter for the Gaussian, you don't vary it? - [Pieter] Oh, you're saying for p z? Yeah, for p z, we fix it. So we just fixed a few different things in the different examples. So here we fixed uniform, can't change it. Here we fixed beta five, five and here we fixed a unit Gaussian. We're not allowed to change it. In principle, there's nothing in that objective that prevents you from also allowing to change it. You could try it out and see what happens. You could still optimize that objective it's just a few more parameters allowing you to also optimize that part. Yes. - So we only do inversion at the time we want to sample something? The flow is not at training time? - [Pieter] So the question was do we ever need to invert at training time? Or do we only need to invert at sample time? So at training time, we don't need to invert. So at training time, we just need to evaluate this objective over here which is just f of theta and the log of the derivative of f of theta. And so there is no, well essentially absolute value of the derivative because just the way the probability masses map, negative or positive slope behave the same in terms of transferring weight over. There's no inverse function being considered here, but then in sampling, yes, we have to go in the opposite direction. And it's interesting question 'cause sometimes some functions are invertible but are a little expensive to invert, so you might have to do something like a bisection search to invert a function which might make it more expensive and you might not wanna, during training, have to run bisection searches all the time, but at sampling time, maybe you're okay with it because now you're just running it once per sample that you're generate, you only have to do it every epoch, for every sample that you have. Oh, question, yes. - [Student] Do you pick your distribution of z based on the problem domain or what are the-- - [Pieter] The target distribution z? Usually is just picked to be a Gaussian. Gaussian or uniform. The thinking there is that you're trying to in some sense map your original data in a space where, let's think about uniform. If you think about uniform, you take your original data, you try to spread it out in a way that's as even as possible and the thinking is that that might give a good representation of your data if you force it because then you would essentially warp your data in a way that things that are on whatever manifold it comes from close together's still close together in your latent space. That's typically how it would come out and you're also forced to fill it all so you can't just collapse things, but a Gaussian's essentially the same thing. It's again about filling a large space except that a Gaussian can be convenient because you don't have to worry about squeezing in between zero and one, yep. - [Student] Would taking the derivative of f be computationally stable? Do you get a really huge derivative that you can't fit in your. - [Pieter] So the question here is what shows up here is the derivative of function f and so is it possible that maybe that derivative will be very steep in some way or close to zero and have either a vanishing signal or exploding signals. It's a good question, I have not seen people write about that in their papers. That's something they perceive as one of the big challenges of working with these models even though I realize in a lot of, recurring in other models this notion of derivatives kind of blowing up together, I have to multiply together or going to zero is a bit of an issue and I have not seen anybody write about it here, yes. - [Student] So just to be clear, we're not in charge of the f sub theta, right? That's just sort of in nature, unknown. - [Pieter] So as we posit, no, not the sampling, for the training, as we posit a model for training, f theta is gonna be something that's open ended and we're gonna learn it and p of z is where we posit it ahead of time. We say p of z, unit Gaussian or something. Hey Edward. - [Edward] So you only really need the inverse to sample, so we don't care at all sampling, we only want likelihoods for it and we could use functions that aren't necessarily invertible, right? - [Pieter] So question is can you use functions that are not invertible in this process if you don't care about sampling because you don't need an inverse if you don't sample. It's a good question. My sense is that you're still gonna run into trouble in some ways because as you're training your f theta, so I drew a new f theta here that's the green one. z equals f theta x, so as you're trying to see, as you're trying to evaluate how much density lands in different places, you have a x here, goes over here, lands on this z part, but it's not enough to look locally all of a sudden because this x cannot claim all the probability density that's mapped to that z. It has to share that probability density and its likelihood score, essentially, it can only get in this case 1/3, like if there's certain amount of density here at that z, this particular x can only claim 1/3 of that. And so you'd somehow how to know that there is three others mapping to it and also know the derivatives at those other spots to understand how much the others essentially are contributing in the process and so I think in principle, you can do it. If all you care about is mapping it to z and having a density model in that regard, except that I think it'll be very impractical. You'll have a lot of work scanning for anything else 'cause you're saying if you can't invert, you still have to somehow actually invert in a multi way that you need to find all inversions of z to understand how much probability mass you can assign to this particular x and so I think by making it not easily invertible or not invertible, you essentially end up with a lot of challenges to make it work, but I do agree with you that if you can resolve those challenges, you could probably still run through this optimization. At this point we've seen the most important concept for flows. We'll have to generalize it of course and see how to apply it in higher dimensions, but we've seen how we can train, we've seen how we can sample, it's just 1-D but higher dimensions will not be that different. Let's look at some things we can do for choosing the functions under the hood. So if you want something invertible and differentiable, a natural choice would a cumulative density function 'cause a cumulative density function is a function that goes from zero to one monotonically which makes it differentiable, invertible, assuming there's no zero density regions and so a Gaussian mixture density could be an example and that's under a lot of the examples I've shown so far and mixture of logistics which we saw last lecture for autoregressive models can be used. Really anything that goes zero to one is a nice way to map to the uniform distribution. Can you use neural nets? Well one interesting thing is that if you have a multi layer neural net, if each layer is a flow, the composition of flows is also a flow because if each layer is invertible and differentiable, then composing those layers will still be invertible and differentiable. And so assuming you have core components, you can layer them as many times as you want. In each layer, what can you put in in terms of nonlinearity? How about ReLU? I see some no's, why not? Well ReLU is not invertible 'cause you map so many things to zero, you don't know where it came from, sigmoid? Yeah, that's essentially what I said above, mixture of logistics is a generalization of just a single sigmoid, definitely feasible, tanh? Well not if you want to map, probably not a good one to choose if you wanna map to the uniform distribution on zero, one, if you want to map to uniform on negative one, plus one, seems a great fit. Essentially just a sigmoid that's shifted and scaled. It's the same thing. Now the question you might have is how general are these flows? 'Cause it seems like the machinery we've looked at is somewhat intricate. We're saying, okay, we're gonna go from x to z with an invertible, differentiable mapping and somehow that's what we're gonna do and that might restrict what we can model and are there maybe some densities that cannot be captured this way? Well what do you think? How general is it? Who thinks it's very general? Who thinks it's really not general at all? Very general, a couple of people. Who thinks not very general? Nobody, it's a prior, 'cause it's covered in the lecture. It's likely somewhat general. Okay, what's the rationale behind why it's so general? We can go back to cumulative density functions. So let's say you have a density, p theta of x. The cumulative density function measures how much probability mass has accumulated up to a certain point, x. So integral from negative infinity to x, so the 0.2, this value over here is corresponding to this entire integral over there. So that's the cumulative density function. You can, for any density, you can draw this cumulative density function. It's uniquely defined once you have a density. Okay, so one think we can also do and that's interesting once we have a CDF, we can actually sample. What the CDF gives us is that you can sample from the uniform distribution, which is living on this axis effectively, and then find out if you, say, sample this number, find out what that maps to over here. And that's a way to sample from the original distribution. So what this shows is that what we have here is a cumulative density function of this whole distribution, which naturally maps the original variable x to a z that's uniform, zero, one. And so we have a very special kind of flow here. It's a flow that turns any density into uniform distribution. So what we have here is that if you ask how general can a flow be, well a flow to a uniform distribution is just a matter of finding the cumulative density function and we got it and then to sample from the variable, we just go the other way. Can we flow to anything or only to uniform? Well, if we can flow x to u and we can flow another random variable z to u, then all we need to do since the inverse of a flow is also a flow and a composition of flows is a flow, we can just use them to go from x to u, u to z and we have a flow from any density x to any other density z, and so what this shows is that with just two layers of such a cumulative density function and any inverse cumulative density function, we can go from any x to any z distribution. Not saying we never want more than two layers 'cause the cumulative density function, you might have to learn under the hood might be very complex and might be easier to learn with multiple layers than in a single layer just like with any kind of neural net learning. It's not always that you want to put it in as few layers as possible. Often more layers is easier to train than fewer layers. Same thing here but this shows a sometimes universality property of flows. So that's nice, so now we don't have to worry about being restricted. We can model any original density onto any target density. Any questions about 1-D flows, yes. - [Student] Is it convenient for one of the variables ends up zero and I suppose that somewhere then the CDF would not be rising, right? - [Pieter] So the question was if the density is let's say some stuff but then it's exactly zero here and then maybe it continues again, so when that happens and you build a CDF, the CDF value of all of these will be the same, which is, well let's think through it. I don't know if that's an issue or not actually because now, let's say I construct the CDF corresponding to that, the CDF would be something going up, up, up, up and then at this point it would stay constant and then it would start going up again, and so if this really has all zero probability mass, my sense is that when you sample from the uniform distribution over here, since the probability of sampling that exact point is zero, you actually wouldn't run into any trouble 'cause you should never sample that specific, there's zero probability of sampling that point and so if you do happen to sample it, you can say this was a zero probability event, maybe I'll just epsilon perturb it and use that center sample again. So I think it works, but maybe we have to give it a little more careful thought, but at least in terms of it still being invertible it does work. Question. - Do you have different variable constraints on your matrix layers, not the-- - [Pieter] So yeah, so far we've just done 1-D, so we don't have any matrices yet. We're gonna get to that. We're gonna go to 2-D, then N-D, then there will absolutely be considerations there, yes. Question. - Why is mixture of Gaussians a valid flow, since it has no inverse? - [Pieter] Why is mixture of Gaussians a valid flow? Imagine I have a mixture of two Gaussians, this one and maybe this one. Then the overall density would go above that, something like this and I can still construct a cumulative distribution function which will start going up here, then the slope will become a little less of how it goes up and the slope comes steeper again, and then, ah, doesn't go down, mistake, never goes down, but it gets less and less sloped over here till it reaches asymptotically one and so it would be something like this and another way to look at it is you could look at the individual CDFs that would be one Gaussian's CDF, another Gaussian's CDF. And you'd have the weighted sum of those. Maybe 1/2 plus 1/2 each one of them and so that together would again a function that roughly looks like this that goes from zero to one monotonically increasing. So just to be clear, the density is not a flow. It's the cumulative distribution function associated with the density that is the flow. Actually, I propose we take a couple more breaks this lecture than previous lectures, so I propose we take a two minute stretch break and then we restart and then we'll go for a while and we'll have our pizza break and then we'll go again, so let's take a two minute break and then we'll do 2-D flows. (students chatting) Okay, let me refresh then. The images that didn't load were a consequence of PowerPoint and Google Slides being incompatible not your images missing. That's why I had to draw on a few things, but yeah, it's kinda weird that a few things get typeset very funnily when downloading it as PowerPoint, but Google Slides doesn't have the annotation tools on here. Well maybe now it does. I didn't try, last year it did not. You could not annotate. Here, weird stuff. - [Alex] So I think this strategy should actually go before the architecture 'cause they don't know a checkerboard or channel. - [Pieter] Are these channels? - [Alex] Yeah. - [Pieter] In what sense? - [Alex] So it's like these are different channels and then you'll condition on half of them and then do that by transformation to the other half. - [Pieter] Why is there a five, six, seven, eight here? Why not one, two, three, four? - [Alex] It shows how they went from this and then did the channel squeeze. - [Pieter] Oh, it's a channel squeeze, got it. - [Alex] So the numbers are to show how the channels are worth and then the colors are like when you condition how they will be. - [Pieter] Okay. Sounds good. Okay, yeah, thanks Alex. I think it all flows really well now. - [Alex] Yeah, it shows. - [Pieter] Quite different from last year, let's see. Yeah? - Mr. Abbeel? I had a question about using neural nets to represent the flows. So you were saying that the composition of flows is still a flow, but would you have to do something like training the way through being only non-negative in order to prevent them from-- - [Pieter] Well we'll look at that, yeah. You're asking the right question. You're following along perfect. - [Student] Thank you. - [Student] Hey Professor Abbeel, quick question. I don't know if you have a second, but I was wondering, one of the, we love variational inference, we love to maximize evidence lower bound. Could this actually let us more easily use a non-Gaussian prior when we do such a task? - [Pieter] Yes, we'll see that when we look at latent variable model. Oh, I need to switch this back on. When we look at latent variable models next lecture, we can use these models under the hood to represent the prior over essentially the latent variables instead of just using a unit Gaussian. - [Student] Yeah, okay, cool, thanks. - [Pieter] All right, let's get restarted. So our goal today was to fit a density model to data. Rather than fitting to discrete data, we're now fitting to continuous data and we covered the foundations for flows in 1-D and we're now going to switch to 2-D and the N-D quantization. Dequantization is to allow us to apply the same ideas we've seen so far to discrete data. So even though I've said we'll focus on continuous data, turns out that with discrete data, we can play a little trick to make these ideas applicable also to discrete data. So since 1-D is really the foundation, let me check if there's any lingering questions from the first half before we transition over. Yes. - Can you briefly describe or abridge how the training process from, so you give continuous examples of x, how you would map them over to a continuous z input almost like the bots that you showed before. - [Pieter] So the question is what does the training look like. You set up this objective over here and you just let it run Stochastic Gradient Descent over your training examples. That's all you have to do. And so what does that mean in terms of what you set up? You choose a target distribution, z, maybe a Gaussian, maybe uniform, maybe something else, you choose that. You choose some kind of parametrization of your function, f theta that ensures it's invertible and differentiable and then you let your auditive software take care of the rest and optimize this for you. - [Student] So what will be happening is that you get a continuous example, x, you kind of map it to a continuous z. Take p of z basically and then that's the density of that-- - [Pieter] Correct, this here will be effectively your zi, what you map xi onto. Of course as you change your theta, the zi you find will be a different zi, so you can try to optimize your theta to make it map to a part of the z space where you have higher density in your target distribution z. - [Student] And then, it's like you wanna minimize the log likelihood of fz, or maximize, sorry. - [Pieter] Yeah, maximize the log likelihood of fz, correct. Actually, if you think about it, it's not all that different from having a softmax on the output. A softmax is essentially just forcing us into a discrete distribution for which we have some parameters, namely the softmax outputs and we look at what does our data map onto and which entry in that discrete distribution am I mapped onto. That's my log probability right there and here we're just mapping it onto another predefined distribution which is just a continuous density of which we measure the log probability under the densities. So it's essentially very, very similar. It's just different parametrization of a very similar idea. Any other questions about 1-D? Okay, let's go to 2-D. Simple way to go from 1-D to 2-D is to use the autoregressive idea which actually Edward alluded to at the very beginning of the lecture. Once we have 1-D, can't we automatically get 2-D, 3-D and so forth because if we do 1-D, x1 goes to z1 with a flow f theta of x1 and then to get x2 flow to z2, we can have an f theta that depends on both x1 and x2. x1 and x2? Yes, so what this means is that again for training, training will have a term for this one, a term for this one, so think about the training objective with something along the lines of sum over data points. We maximize over theta. Let's maybe call this one theta and this one phi to differentiate them and then you'll have a log probability under distribution that you have for z1 as your target of f theta, x1 plus then log, this is then gonna be dz1, dx1 and then plus log probability under z2 of f phi of x1 comma x2. Why also x1 here? Because you wanna be able to condition, you wanna have a conditional distribution. You don't want x1, x2 to be considered independent, so you want to be able to condition on x1 also when you do this flow, x1 and x2, plus log absolute value of dz2, dx2 over here. And so that would be your objective again and of course these things like dz1, dx1 is really df theta of x1 with respect to dx1. So very similar, it's just now for every variable we add, in this case you just added one variable, there'll be an additional term in the objective that looks just like what we had before and so going from 1-D, once you understand that, to 2-D is essentially trivial in this setup. So here's an example of a two moons dataset shown on the top right. That is the x1, x2 distribution space, so x1 here, x2 over here. Our target is gonna be uniform over zero, one, times zero, one and we see that initially, we're essentially, at box zero, we just have this, 'cause we're squeezing it into zero, one, zero, one, we get this over here. Then after some training it starts spreading out more, even more training, we cover the entire space, which we are required to do to maximize the likelihood under the uniform distribution. We got to cover that space and put everything in the same spot, we actually get a much lower probability. So we see this works out fine and people ask about what's under the hood. When it says x1 is a mixture of five Gaussians meaning that the flow used for x1 is a cumulative distribution function of a mixture of five Gaussians, same for x2, but then conditioned on x1. So what that means is that the pi, mu and sigma output by the neural network that tries to predict the correct pi, mu, sigma to make this work, that network will condition on x1 to output these for the current x2. Then you do this for a different dataset. I suspect you'll do this on your homework two. Here's a face and this seems like a distribution very different from a uniform distribution, but again, if you train your flow and it's expressive enough, in this case five mixture components for each coordinate, you can turn this not uniform distribution at all into what is very close to a uniform distribution. So this idea of course is not about 2-D. There's no reason you couldn't do the same thing for 3-D, 4-D, 5-D and maybe one million dimensions if your pixel's a megapixel image. So what it is for high-dimensional data will have a function f that goes from image to z and a inverse function that then when we sample from z space can turn that into an image. Now think about N-D flows, the first type we'll cover is autoregressive flows, which is what we already saw so far. Then we'll look at inverse autoregressive flows. Remember autoregressive models. Convenient for training, not convenient for sampling, very slow sampling. So inverse autoregressive models will be the opposite. It will be inconvenient for training, but very fast for sampling. So depending on the application you have, whether training is more your bottleneck or sampling is your bottleneck, you might want to use one or the other. Then we'll look at some completely different architectures. Very different from autoregressive flows that are actually much more popular. I would say the autoregressive flows are mostly popular for modeling the prior distribution inside variational autoencoders, which we'll cover next lecture whereas the other models are popular to model image distribution and so forth. And then we'll see in some sense next generation versions of RealNVP with Glow, Flow++ and then FFJORD. Okay, autoregressive flows, so kinda resembles the Bayes net thing again just like we saw in last lecture. Bayes net, you can think as applying the chain rule. You first have x1, then x2 given x1, x3 given x1 and x2. That's in some sense what these flow models do. You have x1 and x1 gets mapped to z1. So we also have just this little z1 equals f theta of x1. But if you're generating, you have a z1, which is some initial randomness, allows you to generate x1. Once you have x1, you can conditional x1 and some new randomness that comes in to generate your next part of the sample, x2. And then you can condition x1 and x2 and some randomness to get x3. So the right side is the sampling process the corresponding to that is a forward process that determines the training. And we've seen this training process for just two variables, x1 and x2, but there's no reason you can't extend this to any number of variables. So how do we fit it? We map again from x to z. It's fully parallelizable 'cause we saw in the previous slide all of this is in some sense happening independently. You don't need to wait to have mapped x1 to be able to map x2 and then map x3. They can all be mapped in parallel at the same time. They'll all contribute terms like the terms we saw over here. Each one will have their own contribution to that log likelihood. And that's all we need to do for training. There's a more complicated equation here which we'll see come back later but essentially it comes down to the same thing. In the autoregressive flow situation, it'll just be separate terms for each contributor and this will be clear later why this is equivalent. So x to z, same structure as log likelihood computation of an autoregressive model, very fast to compute log likelihoods, z to x also same structure as sampling of an autoregressive model. These are autoregressive models. There's nothing in some sense different about the original autoregressive models we saw last lecture. It's the exact same process happening under the hood. Just for continuous variables. Now discrete variables, last lecture. And so we do sampling, well after we have x1, you can insert in here to get out x2, you can insert in here to get out x3 and so forth so sampling's gonna be pretty slow. Question over there. - [Student] When we do this training, you're saying we can do it in parallel, but does that mean that we have to sample x1, when we're trying to train for x2 as well? How do we get that second term in the conditional? - [Pieter] Yeah, good question, so how does it work in a training. Let's go back to the two variable example where we have the equations on there. So this is the two variable example. We're gonna look at the terms for x2, which is these terms over here. Yes, they do rely on knowing both x1 and x2, but at training time, those are known 'cause your dataset has, and this will be sum over i, so really there's a i index at the top here for x1 i, x2 i and so forth, and so those are in your data, so you know what they are. You don't need to sample to find out what they are. - [Student] Thanks. - [Pieter] Okay. Now, maybe you're not happy with this slow sampling process. I mean, imagine you have a million variables for a megapixel image. You're gonna have to during sampling do a million iterations over your f theta. I mean, just like in Pixel CNN, could be very, very slow to regenerate your entire image this way if now it's a flow. Well here, 'cause things are invertible, we can actually think about them, we can set up the other way around. We can say instead of thinking of it as mapping from x to z, why don't we train a mapping that goes from z to x and make it work out that way? So what does that mean? z1 is gonna be f theta inverse of x1, okay? And x1 is gonna be f theta of z1, so we essentially, all we've done since f theta is invertible, we just kinda swapped f theta used to be on the left, now it's on the right and the other way around. So in that sense, it feels like we've done nothing really, we just relabeled it. One means the inverse and one means the forward. But actually if you look at the specifics of what's happening, if we compare here, we have x1, x2, x1, x3, x1 and x2, so it's all x's in the arguments and here we're having z's instead. And then similarly here, originally we had a mix of x's and z's and here we have all z. So this is all z that we have to have an input to do the calculation, here it's a mix and then here it was all x and a mix over here. Because we had all x over here, we could do training very efficiently because x is available to us and we can compute the log probability right away in forward pass for all of them in parallel. Here, we actually, as we do this calculations, we actually need to wait for z1, we want to be able to compute z2. You need to wait for z1 and z2 to get z3, so we get the effect that was asked about early, might it be difficult to train? In this formulation, that's what you get. Difficulty during training, you gotta wait for it to be generated to know what you can do next. On the flip side, what it means is that sampling time is that only depends on z, so let's say you have a Gaussian for pz, you sample from your multi variate Gaussian, in this case a three dimensional Gaussian. Once you sample that, you can in parallel generate x1, x2, x3. There's nothing that has to wait for each other in the sampling phase. You might say how's that even possible? What is the trick being played here? It's a bit mind-boggling that this kind of gets flipped around. The way I tend to think of it is to think of it as when I'm conditioning on z1 here, I kinda have a choice because I know this thing over here is invertible so either I condition on z1 or x1 is really, it's in many ways the same. They have the same information in them, so I can choose when I generate my z2, I can choose whether to put the x1 here or z1. Information-wise, that's the same thing. In the regular autoregressive flow, we choose x1. In the inverse autoregressive flow, we put z1. And so the way you can also think of it is that under the hood, the architectures you're gonna need are probably gonna be slightly different because if you generate z2 from x2 and z1, that's different from generated from x2 and x1. So there's some consequence on what you might do under the hood, but in terms of information in principle, you can choose here. Same thing over here and so forth. And so it's a choice that you can make that then results in either fast training or fast sampling, but unfortunately never both fast. So this is a bit of a summary. Autoregressive flow, fast evaluation of p of x in training, slow sampling. Inverse autoregressive flow, other way around, slow evaluation, so slow training, but fast sampling, so. Then there are models called Parallel WaveNet and Inverse autoregressive flow-VAEs that exploit the fast sampling you can do under the hood. So training will be slow still. So often what happens is that you might have a relatively low dimensional latent space in a VAE and then you can still train that low dimensional space IAF model and we'll see that next lecture. Now, think about it, naively, both the AF and the IAF end up being as deep as the number of variables you try to sample meaning the number of pixels in an image, if it's an image you're trying to generate. So call it layers or call it sampling steps. Just like in the autoregressive models we saw last week, you can do parameter sharing to make it statistically more efficient the way you generate x10 from x1 through x9 could be using a neural network that shares a lot of parameters with how we generated x9 from the previous ones, x8 from the previous ones. How do you do that? Well we saw how to do that. You can have an RNN, or you can do masking and that way you make sure you cannot look at yourself as you do the forward pass and you can do everything, especially during training, everything in one big forward pass, for every term in your training in parallel through one network. SO you can play the exact same tricks here as we played last week. Still, that wouldn't solve the issue of that sampling would still take a million layers or a million steps assuming you have the fast training you're stuck with this slow sampling process. Okay, let's move on then. So far we looked at is change of one variable only and in autoregressive models, yes it was vector x to vector z, but it was one variable at a time. Let me ask a question, can we do a change of many variables at the same time? We have a vector value, variable x, and somehow transforming to a vector value variable z, and can we think about how the density changes under that? So can we maybe turn a, I don't know, 10-dimensional x into a 10-dimensional z in just one stage and now if we think about things like sampling and so forth, we maybe have to go through so many stages. It's all happening at the same time. So let's look at the simplest, I mean the local scenario, what happens? We still have a volume preservation thing. If we go from a probability mass on the x space, which would be px times volume of dx, this thing would have to be equal to this thing over here, bring this to this side over here and we get p of x is equal p of z, volume of dz, volume of dx, and it turns out that this ratio over here, the volume that your small dz takes on over dx is the determinant of dz, dx or absolute value of that. This might seem a little bit coming out of nowhere and a little bit surprising and we're not fully deriving that in lecture today, but let me give you a little bit of intuition for this. So imagine I have a small volume in this is my x space, my small volume dx gets mapped to, by my function, f theta, to some volume, dz. And let's, often when you have anything with matrices, the simplest way to get some intuition is to think of it as what if it was just a diagonal matrix? All right, just a diagonal matrix. It's simpler to think about. Every coordinate's doing its own thing and effectively have a, what you'd have is along, you'd have a rescaling along the first axis of dz1, dx1. What's the ratio of those? And that would be one entry in your Jacobian matrix here. Then you'd have a dz2, dx2 and a dz3, dx3. This is of course the full Jacobian, has entries here, here, here, here, here, here, but imagine it's just a diagonal case, you just have those three numbers and those indeed say how your volume is rescaling. If along the first coordinate you maybe grow by a factor two, second coordinate you grow by a factor, well shrink by a factor 0.8, third on you grow by a factor, three. You can multiply those together, gives you the volume of the new thing that comes out. And so these local slopes on a one-dimensional basis tell you what happens even in a 3-D case as long as things happen to be happening axis-aligned. Now, in general, when you look at your dz, dx matrix, it will not be diagonal. Things will not be happening axis-aligned and so this will not be as simple to analyze, but let's stick with the axis-aligned case. Axis-aligned, well then the change in volume is the product of those three entries. The product of those three entries happens to also be the determinant of that matrix. It's the determinant of a diagonal matrix is just a product of the entries on the diagonal. So we know that at least in this special case, this equation makes sense. Now, one way you can think of this from there is say, well, imagine, who here has heard of singular value decomposition? Okay, pretty much everyone, okay. Imagine it's not diagonal. We can do this proof right on the slide here. It's a dz, dx is not diagonal. It's instead equal to U sigma v transpose, okay? You wonder how is the volume changing that I had. The v transpose is just rotating things. There's no volume change. Sigma is rescaling axis-aligned. U is a rotation again. So if we think about what changes in my volume, I just need to look at the product of the diagonal entries in sigma, it only has diagonal entries, and I'm good to go. So really, all I need is product of diagonal entries here which we could say, oh, what we really should put here is the product of diagonal entries of the sigma matrix in the SVD of this Jacobian. Yeah, that's one thing you could put there, but that happens to also be equal to the determinant of this thing and the determinant might have other ways of computing it that might be a little quicker than doing a full SVD. And so rather than saying we need the product of the singular values there, we can just say hey, we just need the determinant of that Jacobian and we'll figure out later how we can find that efficiently. Maybe we'll do SVD, actually, we'll not, but you could. We'll do something else. Okay, so we understand now change of many variables, so what happens is probability mass and again, the same thing happens if somehow the function grows the volume dx into a bigger volume on the z side, but the probability mass living inside dx now gets spread out as a bigger volume so as if you have a high dz, dx, so if this thing is high, this'll have to make this thing low to make it match up, okay? All right, so how do we train? High dimensional flow models, p theta of x equal p of f theta x times the determinant of the Jacobian matrix. Again, this is dz, dx, the Jacobian matrix and its absolute value because it doesn't matter whether you have a negative multiplier or a positive multiplier, so keep that in mind, this is absolute values. And you can train with maximum likelihood. So you can actually do the same thing as we've done in 1-D. It's a different generalization. First we saw 1-D, then we generalized by doing autoregressive and here we generalize by doing directly many dimensions in one go. Doing this has one additional requirement and in addition to the function, you need to be invertible and differentiable, we also need to be the case that that log determinant is easy to compute because if that's not easy to compute then your training objective shown over here will be hard to optimize. You want your training objective to be easy to compute So new key requirement, Jacobian determinant must be easy to calculate and differentiate. New in addition to of course differentiable and invertible f theta. Okay, so we have a new setup now to do higher dimensional flows directly, not autoregressive. Other thing to keep in mind is that even for this we can compose them. We don't need to do one layer of flow and be done, we can, once we have one layer, if it's a flow we can compose another layer, other layer, other layer, we can build deep networks, each layer being a higher dimensional flow in one go. Now, we of course the requirements, Jacobian determinant easy to calculate, differentiable, invertible, differentiable not too hard, we set up a neural network and we usually set it up to be differentiable, but the invertible and determinant thing is some things we need to think about. If we just put up an arbitrary neural net to go from x to z, it's probably not gonna satisfy those properties, question. - [Student] Is the f theta already derived function? - [Pieter] Say it again. - [Student] Is the f theta function already derived. - [Pieter] Good point, so when we worked in 1-D, we always stayed in 1-D. What we now work in x in Rd, z also in Rd. They live in the same dimensional space. Otherwise we cannot have a differentiable, invertible function I believe. So we can construct flows by composing these just like we did for autoregressive models. We did one variable at a time and you have a term for each variable. Now you have also a term for each stage. Sum of log determinants every step along the way. So very easy to increase expressiveness as long as somehow your core modules are reasonably expressive so you're not stuck in something that just linear, everything just linear. Probably not anything good coming out of it. Talking about linear, affine flows are flows that are just linear, so your f of x is just A inverse, Ax minus b, so for sampling, x equal Az plus b, z can be from a normal distribution. Then as a consequence, x will actually be from a normal distribution with mean b and covariance AA transpose, turns out. So that's an affine flow. It allows you to turn an arbitrary multivariate Gaussian into a univariate, well, unit variance multivariate Gaussian. Now, log likelihood can be expensive because you need to compute the Jacobian of f, in this case A inverse and so even though determinant is something that you cover early on in high school probably, it turns out that in general for a general matrix, a determinant is not very efficient to compute and so you need to be a little careful what kind of matrices you put in there 'cause computing those determinants can be quite expensive if you're not careful. It's a deterministic algorithm. It's easy to do it, there's a sequence of steps but it can be a very long sequence of steps that you might not want to back propagate through 'cause let's remember the distance, maybe do a Gaussian elimination and finally you can get the determinant. You don't wanna have to back propagate through that whole thing every stage of your flow. You want the Jacobian to be such that the determinant hopefully just falls out. So once way to get some of this done is to look at the elementwise flows. So if you look at things where each element is flown independently, this is a very special case of autoregressive model, where they don't condition on each other in a very direct way maybe. Elementwise flows are convenient computationally, but of course limits what you can do 'cause now everything's considered independent which is not great, but you might have it for some stages. So some stages might be elementwise flow. Some stages could be flows that are bringing them all together. The kind of big breakthrough in flows that kinda put it on the scene as something that can actually work pretty well is the NICE and RealNVP work. And so they said let's just do a split of our variables. I'll get to you later. So we're gonna say z for the first half, so have x, which is supposed to go and become z, the first half is just going right over, nothing happening, very simple. The second half has a affine calculation, so this is the second half variables. So affine calculation happening, in fact a elementwise affine calculation, so each entry in the second half, so I take one entry in the second half, let's say I take x, no, d over two plus one. I'm gonna multiply that with some function that I can learn of x1 of d over two. Can be a complicated neural net plus another neural net that computes a offset based on x1 through d over two. So what's happening here? This variable, x d plus one, and this'll become z d over two plus one. So this variable x d over two plus one is undergoing a very simple elementwise affine transformation. Just multiply it with some constant and something is added to it, but the constant multiplied with and the offset here come from the other variables that were just passed on. At first this might seem a little surprising that you can do this and that this remains invertible, but it actually is invertible 'cause what you can do is when you need to go the other way, so computing z of course from x is straightforward, we give the equation. What if you now have x, can you find z? That's the check for invertiblity here. Well, if I'm given x, sorry, if I'm given z, I can right away get these x's, they're the same. In fact, this is really also just z living in here. So what I see if I wanna solve for this guy from having the z's I can do that 'cause there's no complicated system of equation, nothing complicated, it's just a linear equation. Same thing here if I look at the one variable version. This is a linear equation in one variable 'cause this thing over here, this is really just z1 through d over two. This really is z1 through d over two, so the only thing that's unknown is this x over here which I can solve for very easily once I have the z's. So I can work my way back from z to x no problem for each of those second half entries. So invertible, check and differentiable, yes, assuming the neural net that you put here for s theta and t theta is something differentiable. Of course, you still need to think about the determinant, so can we easily compute the determinant of this transformation? Well let's think about it. Let's take a look at what this Jacobian looks like. The Jacobian looks like this. This identity block over here. There's a diagonal thing over here and then there's some pretty complicated stuff over there, but it turns out that you have a matrix that is all zeroes above the diagonal or all zeroes below the diagonal, the determinant is just the product of the entries on the diagonal. And so what we're exploiting here is that if we just make upper triangle or below the diagonal all zeroes, those Jacobians, we can get very efficient determinants for. Just a multiplication of the entries on the diagonal, that's it. And so that's gonna be under the hood here for RealNVP and NICE is that we ensure that we have these all zeroes and then it's tractable. The determinant of this thing is just a product of the entries on the diagonal. I mean, it depends on a lot of things. I mean, the parameters theta show up and the previous variables show up or the z's 'cause they're the same. They show up but it's tractable. You don't have to do some kind of Gaussian elimination scheme till finally you can find your determinant. It's right there with you. Okay, imagine we set this up. We're gonna have input image, let's say, I dunno, 28 by 28 image times three color channels. Do a bunch of processing. Out comes a z variable that is also dimension 28 times 28 times three in that case and that is supposed to come from a Gaussian distribution. Let's see what happens. So what we have here is, let's see if we can zoom in. Yes we can, at least a little bit. Is samples from, when we go the other way, you sample your z, you work your way back in the network to generate x's and we see actually you get pretty reasonable samples of faces, pretty reasonable samples of outdoor scenes, reasonable some more outdoor scenes and bottom left here I think is the bedroom dataset. This is from the Dinh et al. Paper that introduced the RealNVP and was the kind of big evidence for this might actually be quite feasible. Now, what I haven't told you about is what does it mean to take the first half of the variables and then the second half and once you do one of those layers do you do another layer of first half, second half, another layer. Seems like then the first half doesn't get to do much. It's just being passed on. There's nothing happening there. So you probably wanna mix this up. Maybe you wanna do first half stays the same, second half has this special processing. Then the next layer, you go the other way around and so forth. What does it mean to be first half and second half in an image? Top of the image? Bottom of the image? Does it mean something else? Once you're processing with multiple depth beyond let's say just initial depth of three, maybe you have 12 channels in your image. Maybe you wanna do your first half, second half across channels rather than across location in the image? A lot of choices to be made. So what's done under the hood is essentially a checkerboard pattern. So there are stages where it's spetially done, and so you say, okay, I'm gonna, the first half is gonna be one, five, four, eight and so forth. The second half is gonna be two, six, three, seven. Why, because you get to condition on the other half when you do your processing and so the closer those things are you get to condition on, probably the more information you can pull in that's relevant to you. So it seems pretty meaningful to do it checkerboard pattern. Another thing that happens in the RealNVP architecture like many neural nets, as you process from image to some conclusion, at some point you reduce the dimensionality, the spacial dimensionality and get more channels. So at a given pixel, you might initially have three channels RGB, but at some point 32, 64, 128 channels, but initial image might be 28 by 28, then you're only four by four or something. So same thing happens here. There will be the flow that I described with checkerboard, who comes first, who comes second in that flow ordering. After a few of those, you get a squeezing, so the image gets in some sense resized to have more channels, less spatial extent and then once you do that, after that, you can do a conditioning on first half of the channels versus second half of the channels instead of spatial partitioning. We'll look at it in more detail, I'll show you this 32 by 32 by number of channels image. Layer one does the checkerboard thing three times. Then there's a channel squeeze where the two by two becomes a one by one by four if it's only one channel or general times the number of channels that it had. Then split the result gets you this point 16 by 16 by two becomes your x, your new x and you actually drop, so in flows you have to go from Rd to Rd, but we're gonna say is that after we do a first set of stages, we have as many variables as we started from, but we wanna somehow split off half the variables and say these are already some z variables. Some of these z's only require a few stages to get to. These are the z's that compute, in some sense, local information, the last little bit that you need to fill in the details in the image. You drop those, you continue your flow with the other half of the variables and this process repeats. So at the end, you end up with a latent space, the high-level latent space z is a four by four by 16 times c, which is either three or one depending on if you had color input or grayscale input. So that's some specifics there but it gives you some idea of what's happening under the hood, yeah. - [Student] Is there a reason why we break it up in halves? - [Pieter] Is there a reason to break up in halves? That's a good question. So could we break it up in different ways? You can actually, I mean essentially, you just need to make sure that this thing is invertible and that you have a Jacobian that allows for fast calculation of the determinant and so I believe you could split it one versus everything else, which would be essentially autoregressive style model which might not require many, many layers before you get somewhere so my sense is that it's half, half just to maximize the amount of mixing that you get in every stage, more so than you need to be exactly half, half, yeah. You can play with partitionings. It's not always gonna work particularly well. So on the left are images with, well, the checkerboard channel squeeze, then channel partitioning, then channel unsqueeze, checkerboard again and so forth which are, that resolution might be a little hard to see, are faces and then on the right, these are much noisier faces with a lot weirder patterns in them. Here the partitioning was based on top half, bottom half rather than checkerboard and channels. So it does matter how you partition for the results you'll get. Okay, so that's in some sense the main idea for the N-D flows that's very different from 1-D. You can go beyond what I just described, so if you look at the Flow++ paper, you'll see that essentially as you do this transformation here, while affine is most commonly used, you don't need to be affine. Affine makes the inversion very simple but you can actually put some kind of nonlinearity around this. You can put some CDF around this whole thing. That will still be invertible. Now, depending on what you put there in terms of CDF, let's say you put a logistic there. If you put a logistic there, the forward pass still just as easy, the sampling backward pass will require a bisection search to be able to invert back to the x space. So the sampling might be a little more expensive, but you become more expressive and you might be able to generate much better samples than if you're not willing to do this and so Flow++ uses a mixture of Gaussians or a mixture of logistics. There's another paper on neural importance sampling which uses piecewise linear and quadratic functions. It's really just about invertible and differentiable. And of course the Jacobian determinant has to be easy to compute and so I suspect there's room for improvement there for if you wanna do research in this space, you think about, okay, what are other architectures that maybe are more expressive than what people have used so far yet satisfy those properties and hence are very practical to work with? So the Flow++ paper doesn't just do mixture of logistics, also adds self-attention in the neural network and so when you look at the bits per dim, which is again the metric for how well you fit the data, but of course, evaluated on the test data, not trained data, so what's your bits per dim on the test data? The Flow++ has better bits per dim, lower is better, than if you do the ablation without self-attention than if you compare with just using affine coupling rather than introducing a mixture of logistics in the coupling layers and this thing we'll cover in a moment why that matters. Then there's some other classes of flows, for example, Glow. Glow might be the one that many of you have seen because OpenAI made a really nice research result but also a really nice blog post on those results. Let me show you the demo they put up on their blog. So Glow, better reversible generative models and so let's first look at some pictures here. So what's going on here, well we see different faces being generated. What's being highlighted here is the quality of the latent space. So what it shows essentially is if you do interpolations in latent space, you can get meaningful faces in between. You don't just see the overlay of two images, you actually get meaningful faces most of the time along the way. These are the two main authors of the paper who are playing with their own faces here, but you can also, well, yeah. You can also generate many other faces. This is also some celebrities dataset and these are samples from the model. So very realistic faces being generated with Flow models. Again, often people think realistic samples gotta be GANs, actually doesn't have to be GANs at all. Here are flow models generating great realistic face images. Interpolation in the latent space. So here you see a back and forth between top left, Geoff Hinton and (laughs) yeah, one of the authors of the paper and so the thing to pay attention to is that the faces in between are not just a naive overlay, which would be just pixel level interpolation. It's actually meaningful faces in between the two and so you see this actually works out pretty well even between different genders, you get meaningful faces in between and so forth and different races, it all works out. Manipulation in latent space. So one thing you can then do is start manipulating images and I'll tell you in a moment how this works. Let's say, tap to choose a face, well, let's see. Oh, let's take Geoff. Okay, want Geoff to smile more. (laughs) (students laughing) Geoff is not the biggest smiler apparently according to this model. Let's see if other people are more, you can upload your own picture by the way. Let's see here. Well, she's, she smiles a little bit more. Age, let's make her younger, okay. Well I'll stick a not so extreme for right now. Make here older. (students laughing) Width of the eyes. This is less narrow I guess and more narrow. How blonde the hair is. It's already pretty blond. Let's see how much more blonder it can get. (students laughing) Definitely can. Oh, and it can get darker and can she get a beard? Le's see. It's difficult. Let's see what Geoff looks like with a beard. Oh, look at that. Not unrealistic at all. So what did Geoff used to look like? What if he were blond? So this is fun to play with. What's happening under the hood? So what's happening under the hood is the following. We now have a let me hook up my tablet again. What we have under the hood now is a model that can turn an image, x, into a latent variable z. So, oh that's not how we want it. So we can turn an image x into z. And now maybe I wanna, and I can also then use that z and it's invertible to get the exact x back, so I can x to z, z back to x. Let's say I now wanna give a face some different expression. Smile more or smile less. What can I do? I can say, okay, well I have my training data and I split it into smile, no smile and have a bunch of x's here, maybe x1 and so forth to xk for smile and maybe some xk plus one through xn for no smile. I can turn these into their corresponding z1 through zk and z k plus one through zn and I can take the average, so I can look at z average for smile and z average for no smile. We now have an image, say I have x of Geoff turned into z of Geoff, then I can add z average smile, subtract out the no smile and put a scaling here and this way I can in the latent space shift it towards something that looks more like somebody smiling. Once I have done this transformation here, I can use the inverse flow to go back to the x space and get out a picture of Geoff, that's depending on the alpha, smiling more or smiling less than the original. Of course, smiling's just one, but you can do this for other things too. Okay, the one thing, actually let me pause for questions here because what I'm gonna do now is very different from what we covered so far. Any questions, yes. - [Student] I've heard about smile vectors in autoencoders, is there a relationship between that concept and this one? - [Pieter] Usually when people talk about those things it's the same thing. Usually what happens is you have a latent space. You find two populations, one with the property on, the other one with the property off and in the latent space you average each population separately, look at the difference vector between the average and then you use that to add or subtract from a new thing that comes in that you wanna have more or less of that property in, yes. - [Student] Is it possible to force, say, z1 in responsible for a smile and z2 is responsible for hair color or something like that? - [Pieter] So the question is rather than us going in after the fact and finding populations and say okay, maybe I find 20 people who smile, 20 who don't and I average their z's and then do this, can I somehow learn a decoupled representation with independent factors coming out in the z's? In some sense that's happening with the hierarchical aspect of the flow in that the things that you separate out right away when you go from x1 stage out, you drop half the z variables. That's gonna be the details. And as you keep going, it's gonna be higher and higher level but what I showed to you here is not really decoupled. Some work that gets to decoupled is a paper called InfoGAN, which does it for GANs, but I suspect similar ideas presented in there could be applied to flows. Another thing that people have looked at is essentially to try to get things to be independent. What you can try to think about is if I go from x to z and back to x, right? But imagine now, I have x1 through z1, x2 through z2 and then I say I have z1 and z2 and I should be able to swap, let's say the fifth entry in z1, I should be able to swap with the fifth entry in z2. If I can do that and from that go back and get a realistic image, then it means that somehow this was an independent factor If found, so it would come down to something along the lines if I have x1 to z1, x2 to z2, let's expand this, let's imagine this is z1, one, z1, two, z1, three, of course, in principle there should be more and there should be then a z2, one, z2, two, z2, three and you could say well, what if I now consider z1, one, z2, two and z1, three? That's not present in my data, but if this thing, if I map it to x, gives something realistic, then it means that I somehow might have found some kind of independent factor and if we wanna essentially train for this, you gotta somehow find a way to evaluate whether this is realistic, that would be GAN-like training or you gotta somehow, another way to do it is if your prior here on z is a Gaussian, unit variate Gaussians, then it means that you're assuming every z is independent and somehow your prior is actually trying to force it. So your p, z, z, is in some sense already saying they're all independent. At the same time, often that's not enough and I would say it's a pretty big area of research to try to make this work better. There's no, I would say, cookie cutter solution you can just get independent factors out. More research has to happen. It's actually something a lot of people do research on 'cause intuitively it seems that if the factors can be maximally independent, there is something more interesting being learned about inside a representation about those images. Any other questions? Okay, so the last piece we need to cover is dequantization. Imagine you have some training data and it looks like this. You train your flow model, you get actually your, you train your flow model and then what's gonna happen is essentially, you're fitting a density model to a discrete distribution. When you fit a density model to a discrete distribution, a finite set of points, the global optimum is to put a very, very narrow super tall peak on the data points and you might put it on all data points, you can put just small peaks on most of them and a very tall peak on one of them. All of this will get very, very high score. As long as in one of them you have a very tall narrow, but very tall peak, you have a very high log probability density score 'cause the density can be essentially infinite as long as it's narrow enough or it's infinite and the way we score it by looking at the log probability density and so you run into trouble, and so what are you gonna do? Well one thing you could do is to somehow parametrize your flow in a way that it's not capable of generating those peaks. So that's one thing you could do and that might do the job. You could do something else also, which is call dequantization. And this is absolutely necessary if your original data is discrete. Imagine you have pixel values coming in, it's grayscale. Number from zero to 255, or RGB, three numbers, zero to 255, only discrete values coming in. If you're not careful, your density will have these crazy peaks. So what can we do? Really the way we think of the density is that if our values are really let's say one, two, three, in our data, and we're gonna fit a density, then we'd really want that density maybe to assign probability mass to this whole region roughly the same because if it's a density, it's assuming that every number's possible. You don't want it to go peak just exactly on that one. Same thing here, you don't want something that's just specific for that two, you want this to be in some sense somewhat uniform here. Same for the three. That's really what you think about. This is in some sense the model you have in mind when you think of your data. You say this is my underlying data model. It's not what the data is, but this is a more precise representation of what I actually care to model in my training. So how to do that? I mean, there's a way to think of it as well. We have a probability assigned to specific value should be essentially the density integrated around that value, so if you make the way you score based on the integral in a region rather than just the point value. Now, completing the integral in a region might be kind of annoying. You can fix it on the other side. You can just say instead of feeding in my one, two, three, when it's a one, I'm gonna change it into a one plus uniform negative .5, plus .5 and if I change my data every time before I feed it in by this perturbed value, I effectively get the same effect as trying to ensure that I measure the density over that region and this is very simple to do. It's as simple to feed in the perturbed values as it is to feed in the original values. So very, very simple trick to play. There is some math you can do that I don't wanna step through here 'cause it's really the math we'll mostly cover in the next lecture when we do variational lower bounds. It's the same kinda math, but essentially it's just some math you can do that shows that if you put uniform noise on your input data, effectively what you're doing is you're optimizing a lower bound on the actual objective which is the one where you were to integrate over that interval, which is the thing that we originally set out to do. So there's a nice correspondence there. Practice though, the key thing to know is don't feed in your data as is, uniformly perturb it within negative .5, plus .5. We run the training on that same data and well, slightly different data, but is this the right picture, Alex? It seems like the training data changed. - [Alex] Oh yeah, that's the right one. - [Pieter] Is it the right picture? - [Alex] Yeah. - [Pieter] Oh, 'cause you're showing the dequantized training data, got it. So here is the original training data, then if you just train on that, you get pretty poor performance meaning you get very good score, very good loss, but a model that peaks in one spot and doesn't do great. Once you dequantize your training data, this is what you get. The dequantized version of your training data. You train on that, you actually get a density here that is effectively, the kind of density that you want instead of this weird peak density. Any questions about this, yes? - [Student] So to some extent, having all flowing with the number of log likelihoods, why should there be a difference if you spread it out over smaller bins? - [Pieter] So the question is given on a digital computer, we don't have real numbers, it's all discrete, I guess, in some sense, how can we even uniform or dequantize because it's not even available, right? - [Student] What if the same phenomenon occurred? - [Pieter] So the way I would think about it is that when we do some of these experiments, the precision of the digital computer is pretty much infinite precision compared to the number of data points that we have and so think of it as a continuum is not a bad approximation that under the hood it's continuous, but our data is just a few data points in that space. So you should think about high dimensional spaces we're not even close to populating the space, so you get similar effects going on and it's just you're very sparsely populating the space and you want this smoothing effect to be in some sense built into how you learn. You don't wanna have a bias towards overfitting, rather you want a bias towards smoothing as you train. Yes. - One more question. So in the previous flow based models, we saw results on images. How did that work? Did you adjust the image and what data that would be? Or did you use dequantized data? - [Pieter] Yeah, the previous results actually dequantization was happening there. I didn't say that at the time, but all those image results used, well, the early results used uniform dequantization and the Flow++ results actually use a learned dequantization where you still look at the probability mass on that region, but you're not forced to use a uniform mass in that region. It's still the mass that matters, not the height of the peak, but you can choose the shape locally and that's one of the key things that's different about Flow++ versus the others and when we looked at the comparison, you can see here uniform dequantization meant that you use uniform dequantization and learned dequantization, so this is an ablation study, so whatever is listed here is the thing that was left out. So this here means the only thing left out is learned dequantization, instead it was uniform and if you do learn it, you get a better bits per dim score, lower is better. So some future directions, I'd say of all Chernev models lines of work, flow models probably have the least research papers so far, so possibly the most room for improvement. The ultimate goal is a likelihood-based model with fast sampling, fast inference, fast training, good samples and good compression, so low bits per dim score. Flows do seem to let us achieve some of these criteria, but there's definitely a lot of room for improvement and probably a lot of room for improvement is in neural net architecture design that is compatible with flows. I mean, just like in supervised learning, when we saw the history of ImageNet from hand designed to AlexNet to VGG and then later ResNets, now EfficientNet and so forth, all of these are neural net architecture innovations that made it performs a lot better and likely in flows there's gonna be interesting ideas you can play around with that make it better. Actually one of the innovations in Glow that I forgot to highlight, where was the Glow slide? One of the innovations in Glow was actually that they introduced one by one convolutions. So these are affine transforms. What do they do? Well if you do an affine transform, one by one, which is just in a channel dimension, and after that you have a partitioning, then effectively, you could learn your partitioning 'cause a permutation is a affine transformation, a special case of affine transformation, not necessarily a permutation that comes out but you could then learn which way you wanna partition across your channels before the partitioning happens and that's one of the things they show is something that helps quite a bit in getting the results they are getting. They also did very large-scale training. Another trend here I forgot to highlight is continuous time flows where essentially a differential equation continuous time differential equation governs the flow rather than discrete stages. And that's yet another direction that people are investigating and starting to get some interesting results with. All right, this is the bibliography for today and that's it. Next week we will look at latent variable models, thank you. 