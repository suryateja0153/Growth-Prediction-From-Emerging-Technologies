 One of the biggest stories in AI news this past week was the release of GPT-3 from Open AI, a company co-founded by Elon Musk. So why is this such a big deal? Because it's the largest NLP model to date, it can generate stories, articles, poetry, code, and even business memos. For example, you can give it a title, a subtitle, and say "article" and it can generate an article for you. Another good example was from someone at a fund who fed the algorithm half an essay on how to run effective meetings and the machine generated a three-step process, which he liked so much that he included it in his own work. And in these short few days this has garnered so much hype in the media and on twitter that one of the co-founders tweeted saying, "It's impressive, thanks for the nice compliments but it still has serious weaknesses and sometimes makes very silly mistakes. AI is going to change the world but GPT-3 is just a very early glimpse. We still have a lot to figure out. Hi everyone, welcome back, if you're new here, I'm Alentina, a PhD candidate at the University of Cambridge researching organizational behavior and artificial intelligence. If you're interested in the human side of AI and would like to hear more on the behavioral implications of artificial intelligence, please be sure to like and subscribe. In today's video, we're going to talk about GPT-3. So first we'll cover some of the technical features of it to understand why it's so important in the AI community. Then we'll talk about what it means for the future of written content creation, and then we'll look at some pros and cons, and end with some thoughts and questions. So what is GPT-3? GPT-3 stands for generative pre-training and it's the third version of a machine learning algorithm powered by language models. And what language models do is they predict the sequence of words. So it's based on probability. So say, for example, the probability of saying, "let's go grab coffee" is significantly higher than say, "let's go grab berries." So the machine has been fed 45 terabytes of datum, it has 175 billion parameters and it uses unsupervised learning to generate quite impressive and accurate content. So what makes this so special? Because it's not the first time that AI has generated, you know, parts of a book or articles or blog posts. I remember I was reading this book called, The Creativity Code by Marcus du Sautoy and towards the end of the book he mentions how a couple of paragraphs, I think, were generated by an AI system and I was so shocked because I wasn't expecting that. So yeah, this has been around but then we have to ask, what makes GPT-3 so special? What makes GPT-3 unique is its size and because of its size, it's able to engage in learning more quickly. So architecturally it's not very different from the earlier version but because it has more data and because it has more parameters, 175 billion which is 10 times more than what's available in the market today, then it's able to engage in meta learning. So what that is is essentially that it learns from itself. What are some limitations of this? So one of the limitations is that because it's a probability model and it can only do short passages so it'll be good for let's say 200 words or 500 words or even a thousand words but once the text gets longer it loses coherence because the machine isn't, doesn't actually understand what it's writing. So it's just generating based on probability. So from the beginning to to the end if it's too long, then there won't be a logical flow of sequence. Another one of the limitations is in its dataset so because the dataset is so large and it was used crawling different websites and google books and wikipedia then it's bound to be filled with biases. And because it uses unsupervised learning, it doesn't require labeled data and labeling data is a long and labor intensive process. So unsupervised learning is a machine learning algorithm that learns from its data by looking for patterns within the data. So a very basic example of this is if you give it a bunch of pictures of cats and dogs then it'll look for patterns and similarities within to categorize it into one group with the cats and another group into dogs. Whereas with supervised learning, one that requires label data but also requires human input then somebody would need to actually categorize you know the cat pictures as cat pictures and the dog pictures as dog pictures. So unsupervised learning makes it a lot faster but it also takes the human out of it. And so by doing so, then the data is not cleaned. And so, with billions of unverified data, there's bound to be more bias because there isn't a human there to verify and clean all of the data. So what does this mean for the future of written content creation? It can generate content. So we can ask the machine to write some blog posts for us or maybe a part of a book that we're thinking of writing or even a poem in the style of our favorite author. But because the machine doesn't have reasoning skills or common sense then these passages will have to remain short passages for the time being. And another thing to consider is how do we implement this correctly? Because the last thing we want is to auto-generate thousands of blog posts. As it is we're experiencing information overload, so the last thing we want is to fill up our cyberspace with more and more blog posts fighting for our attention. So having a realistic view of AI and of this model, of its strengths and limitations, means that we can use it with realistic expectations and use it to its full potential and help improve it. Okay so let's look at some of the negative implications of GPT-3. Because it's auto-generating content then it means that it can increase misleading content, it can increase spam, and it can also generate content in the style of somebody else so it can impersonate other people. And there needs to be some sort of transparency in knowing if the content was written by a machine or a human. Because as it is, there are a lot of misleading and fake articles, and so the last thing we want is more of this and not knowing if it was written by a real human or if it was generated by a machine. This reminds me of this mathematician I had read about who used his computer as a co-author in his publications. He even named it Salosh Ekad and in his articles. From the 90s until now, he still uses his computer as a co-author giving the computer credit for partially contributing to the publication. But if people themselves don't choose to be transparent then it would be nice to have some sort of a system in place that can detect if something was written by a machine or a human. This kind of reminds me of the plagiarism software that is used at universities and schools. And it might be nice to have something like this for you know GPT-3 or other auto-generating AI systems. Going back to last week's video where we discussed the psychological risks of living an illusion, GPT-3 could be a technology that potentially gives off this sort of illusion of what's real and what isn't, of what's written by a human and what's written by a machine. So this is something to think about for regular content creation as well as what it means for education and for the future of publishing. So now that we've covered some of the negative implications, let's look at what some of the positive implications of GPT-3 could be. I think it could be very good for inspiring different types of writing, so especially for you know writer's block. Say for example you want to tell a story, you could possibly feed it some story from your own life or from your past and then you can see what it does with that, where it takes that story, and then you can take that story and continue it in your way and then maybe feed it back to the system again. And so I think this will be a really nice collaborative relationship that you know writers can have with the machine where they give it a little bit and then the machine provides something and then they give it a little bit more. And not just for professional writers but I think this could also be good for us as a way to maybe reflect on our own writing abilities and our own writing style. So it could give us that little push that we need to be creative in our writing or to even just you know engage in more creative writing. I think another good thing that could come out of this is that if the machine is auto-generating more and more blog posts, then it will really push us and really encourage us to be more authentic and more creative in our writing. Because I think if, you know, if it gets filled up with so many new articles then we're going to search for the ones that have real meaning, that are very authentic, and stories that we can truly connect with. And so I think that could be a really good thing for all of us because it'll just inspire us to be more authentic and more real in what we're producing. So now that we've covered what GPT-3 is, why it's so important in the AI community, what some of the pros and cons are, and what it means for the future of written content creation, we have to ask, when this becomes publicly available who's responsible? So who will moderate this information? Do we want it to be left up to the public? And, how would we do that? Because there will be a significant amount of content that's generated. But then do we want to leave it up to tech companies to moderate content? Possibly not the best idea. But these are some things to think about now before it becomes publicly available. And another thing to think about is can we build systems with an internal safety and morality check? And can we build a culture where the psychological implications of technology is built into the foundation of the technology and not just an afterthought, after it's already been created and produced? I've signed up for access to GPT-3 but I know there's a very long wait list so I don't know if I will get access to it but if and when I do, I will play around with it and then i will make another video based on my own experiences. And GPT-3 has a lot of different use cases, I'm only focusing on content generation in this video because if I wanted to focus on all of them this would be a very very long video. But if you're interested, definitely check it out because it can also be used for music generation, chat, productivity, or even translation. Thank you for spending a part of your day with me. I hope you enjoyed this realistic coverage of GPT-3. It's definitely a major step forward but it's not going to be the future author of everything. But, you know, keeping a realistic view and having realistic expectations of its strengths and limitations means that we can be better prepared in using it. And while they're improving this technology, instead of saying "oh well, AI is going to take over our jobs," We can use this time to think about how we can use AI in our own creative writing and how we can bring more authenticity to what we create. Thank you again and if you enjoyed this please consider liking, subscribing, and sharing it with your friends. Take care and see you next Thursday! :)  