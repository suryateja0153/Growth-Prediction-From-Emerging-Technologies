 hello everyone thank you for joining I'm very excited to be part of dr. Kahn this year let me start with a quick introduction my name is Ashish Sharma I work for assistance he has is director of software engineering assistance II is is a boston-based frame tech company we provide products and services in the stock market domain to automate and streamline the entire investment lifecycle quick stats about the company we are in the business for over 25 years serving 1,900 asset managers worldwide as a company we invest heavily into R&D we have thousand 50 plus employees with offices located across the globe before jumping on the main topic I quickly want to talk about some of the core offerings to give everyone more context as to what we do as eclipse is our online investment management platform a typical buy site investment management firm like a hedge fund or ax or a mutual fund they are constantly running through this investment management lifecycle represented by the circle using using the capabilities of our platform and due to the nature of this business we place a very high emphasis on high high resiliency high throughput and low latency there are a lot of different parts in the investment lifecycle I will briefly touch upon the very very core ones starting with trading this is where the active active stock market trading happens and this is a very interesting engineering problem for two reasons first the stock market in general is a vast and complex domain and second its execution has to be very very performant our user can decide ok you know today I want to buy let's say a hundred thousand shares of Google and he can enter this information in our application we take that order we route it to the brokers and then in real time we show you the progress on your on your order it needs to be reliable it needs to be performant even a sub second delay in routing out orders to the market can cause a lot of money to our clients analytics this is real-time analytics like like a real-time view of your profit and loss and what users wants to know how much money they are making or losing as the market is progressing so to put it in the contest context think about this the price on the security kaanchi multiple times in a second multiply that with all the securities in the market and very soon this becomes kind of a very big data set to deal with very interesting problem to solve compliance we provide real-time compliance you can create rules like no matter what my exposure and technology sector should not go below 20% so we'll run these compliance rules for you to make sure you are kind of compliant with your investment strategies modeling this is useful for idea generation you can run large financial models or you can create hypothetical scenarios like hypothetically let's say I want to invest five million dollars in Varmus pharmaceutical sector so how this how this decision would shape my overall portfolio so some very interesting models and scenarios you can run operations start off the end of the operations we interact with hundreds of different third parties on a daily basis and each of these third parties have a variety of different formats in which they like to send and receive data the fun part about this problem space is to build like sophisticated ETL and data pipeline to handle all these variations and make it seamless accounting this is a bookkeeping of all the decisions you have ever made and in a time-series fashion coming on the main topic in this talk I will share some elements of the journey we took from monolithic to micro service architecture and how docker played a critical role in to making this journey a success micro services and docker is a powerful combination I will share with you how it impacted our tech stack developer experience testing strategies and delivery and as I present this to you from my home office this there is no question that we are living in this unprecedented times the spread of Kuwait 19 has affected the world in so many different ways and one of these has been the stock market during the very early days of pandemic trading volumes among our clients was higher than we have ever recorded as a company there's no doubt in my mind that had we not undertaken this journey the equation Eclipse platform would not have been able to seamlessly scale to meet this unprecedented load so let's see how adoption of micro services and docker has shaped our tech stack our journey started with simple idea we wanted to build an online cloud native investment platform and when you are in this very very early phase of the adoption curve the one on the right all you have is an idea and you're working to see if this idea has any kind of potential market so at that point it's it makes sense to keep your text example the very first version of the text tech looks something like this our front-end is a single page application written in angular we had around 10 to 12 back-end services all written in c-sharp and deployed as windows services these services communicated to each other over RabbitMQ and we use Microsoft sequel server for back-end storage so overall we kept it very simple and this is this of this well for a short time but as we added more and more features it became a monolithic system over time it got to a point where we were experiencing difficulties scaling it up it required coordination across the teams during deployments it impacted over time to recovery when things went wrong there was no proper fault isolation so in a way we we experienced our own share of traditional issues that we see with the monolithic architecture and we knew we had to do something about it and at the time micro service architecture was gaining attention in the development community and it had a lot of promises around the same time some folks from the from the organization also attend a docker con and they got excited about using docker of the day after they come come back so they applied both and today the tech stack looks something like this we have we have build polyglot stacks with loosely coupled services written in different languages preferred languages are no js' golang Python and dotnet core but we encourage our teams to explore and experiment with different text x to solve different problems reason being for example if you're building an ETL pipeline and using a language which doesn't have a rich ecosystem for data manipulation you would be burning a lot of development cycles trying to fit it in docker is a big part of our ecosystem all services and infrastructure components they run as docker containers we have 140 plus distinct services running in prod with more coming up every week every month the messaging infrastructure is now powered by RabbitMQ Engine X console in Amazon Kinesis we have extended we're back in storage to include my sequel and we heavily rely we heavily rely on Redis for caching this is the current state of text again and it's continuously evolving so wait now I have to worry about micro services and polyglot stack both an option of micro services architecture alone is a steep learning curve for everyone no question about that right you have to worry about decomposition patterns data sharing API boundaries service to service communication there are a lot of different things that you have to worry about polyglot stacks comes with their own own set of problems there are too many options to choose from in terms of languages and frameworks how do you even manage an infrastructure with so many heterogeneous elements without going insane a word of caution about polyglot stacks maintaining a true polyglot stack is difficult and it requires a strict discipline our own internal development culture and discipline has evolved over time to put us in a spot where we can better manage against potential pitfalls but it requests but it requests very strict discipline I will talk about how we manage Eclipse infrastructure with polyglot stacks uniform packaging all our services are packaged as docker containers heterogeneous services in a uniform package makes it much much easier to operate and reason about the system it's a huge it's a huge relief to our infrastructure team uniform packaging with uniform packaging it makes environment definition files simple so then it then it's it's much much easier to create consistent environments you can reliably spin up like a staging test performance and and those environments looks exactly identical to your product more mates on a click of a button dynamic scaling our our once once you have environment which is simple and consistent consistent it further makes it easy to dynamically scale it up and down currently in Eclipse you support scaling based on resource consumption so say if your CPU goes above certain threshold we Auto scale or based on on time by looking at the service usage patterns and this has been very useful in the recent times with the with the co 819 impact on the stock market our system was able to dynamically scale up just fine it also simplified our CI CD tool chain in the monolithic days in the absence of doctor we had like 10 different tools for building packaging deployments now it's all simplified with just using Jenkins as a docker CLI another elements that helps to manage the complexity is consistent adherence to standards standards are important it helps you keep checks and boundaries in place for teams to operate in a unified and safe way and it's easy to do in monolithic architecture and mono stack right you just have one language to worry about and all the components coexist together so you don't have to worry you don't have to worry too much about like API boundaries and things like that how do you make standards in polyglot stack we maintain our own set of internal standards which are agnostic to any framework on language and they are held and enforce at higher level here are a few categories resource access these are the things that happens within your service like accessing a database or a cache authentication authorization how do you make sure team cell teams are using consistent patterns across the board so to solve this we have written service templates in all four languages and they come with this predefined patterns to address all these category of concerns so for example if they are like let's say 30 services in node.js they all will be using the same template and they remain consistent like that API boundaries since there is no binary shading in micro services services are talking to each other or say rest or graph QL this becomes very important your API needs to look and feel consistent for both internal and external consumers we solve this by creating a set of API standards internal API standards that all services have to follow we use swaggin as API documentation tool to give that a nice look and feel and we also embrace contract driven development and this is a must-have discipline a workflow can spend multiple services owned by different teams and those different teams may use different languages so that makes contact contract driven development a must-have practice bootstrapping how his services get configurations and secrets when it starts well we solve this problem by extracting this responsibility completely out of the service when so when a service starts it can expect all the configurations and secrets to be available to it in terms of environment variables and and docker made this part very simple so all these mess all these measures come together to ensure consistency between polyglot services and allow allow developers to kind of focus more on the business logic let's take a look at how adoption of micro services architecture and docker has enabled better developer experience and to understand this better I will start with how other developer experience used to be back in the monolithic days then and then I will show you how the developer experience looks today and how this has been the life changing for us so in modern lytic days let's say I want to work on a story so my developer experience would be something like this come to office you know drink my coffee the most important thing for me in the morning download the code from the repo build it and we'll probably drink another coffee then restore database locally registered services configuration and this is a big step so I might end up drinking three more coffees when I'm doing this and then it would be lunchtime and hopefully after lunch maybe I can work on a story I personally have spent hours and hours setting up my development in Worman and I took it even further I mean not I would not even download the latest code I would work off the old version of the code for as long as I absolutely don't have to and and working with the nan ladies could actually came with its own set of problems it resulted in more issues when when they finally committed to the main line today it's way different like I said earlier we have 140 plus micro services and we didn't want developers to run 140 services locally on the system so this forces to kind of rethink developer experience and we came up with a hybrid approach now a developer only runs the service he wants to run he wants to work on locally on his laptop and everything else the UI the back-end services the infrastructure components and they all come from from a shared element in Worman so the Dell XPS today's looks something like this come to office start drinking my coffee download the code that I want to work on then I run docker compose up and that's it I have a working system and at that point I'm probably still drinking my first coffee and this is huge I have a working local environment it's just a few minutes earlier I would refuse to download the latest code but today the first thing I do is to get the latest code and getting a simple local environment is only takes a few minutes no more no more working with the old version of the code so let's let's take a look of how this hybrid service element approach works in practice in this demo I will show you how we how we setup the local service instance and use console to register register my service to a shared development environment ok so this is this is one of the micro services it's written in nodejs this service handles the life cycle of a stock that our users can borrow for short reading it's it's one of the trading strategies here's the docker compose this is the local council server I was talking about and here's the service that I want to work on locally I want to spin this up locally and this is the database that comes with the service ok so here I have docker for next up dashboard there are no continuous running at this time so now let's go ahead and run docker compose up [Music] so as you can see on the right it started my stack with all three containers to counsel the local service and the database so now what I can do is I can go to my web page which is hosted in a shared development environment I need to log in as myself so this is again this UI is coming from a share in moment and I need to I need to go to trading and locates ok so now what I'm trying to do is I'm making I'm gonna make some UI edits that should generate a request and my hope is that this request should reach to my service running locally on my laptop and we'll see that in the logs ok so let's see here I'm seeing some logs here already let me search here for Google okay so here ok here perfect here's here's a request that I made from the UI great so this is I have a local working system in just a few minutes now and this makes it very easy and straightforward to develop a service in isolation and also at the same time working with the full and latest context of the entire running application this has been life changing [Music] we also migrated from mono repo which is like a one gigantic single repo with all the code to micro repos what each repo represents a standalone service or a UI component so when a developer wants to work on a service is only downloading the code for that service and nothing else docker further makes it very easy to kind of write self-contained code meaning the code comes with its own explicit set of dependencies and it knows how to go and get them it comes with its own SDK to build and run tests it it knows how to package itself in terms of deployment unit which is a container and we get all these nice benefits without really relying on any or any of the system-wide packages all you need really is docker for desktop in any laptop and it's a mutable package so so it's not going to change between your dev and tests and you add in fraud like the days when we used to find issues because of a mismatch in the dependencies between your local and in any other employment it further makes unloading simple so when your code is small it has a well-defined responsibility it is self-contained and spinning up a local dev enrollment only takes a few minutes it makes onboarding a new developer much much easier another great benefit we see is a massive trend of cross team PRS so let's say my service consumes an API and and and this API is owned by a different team and if I need a like few more fields from this API so what I would do is I will download the repo and I will see what patterns they are using and I will quickly create a peer myself all they have to do is just review it here's a chart based on our internal data and this is a weekly pull request graph team a here is making somewhere between say 15 to 20 VPS in a week and then there's this sudden drop here it actually turned out that team a was busy making peers into team B's repos so we see a lot of patterns like this okay so how adoption of micro services in docker has impacted with testing strategies after we decided to move towards micro service architecture for our mainstream development practice the new testing was going to be challenging as we added more and more services we observed an interesting trend the teens were relying more and more on the integration test to test their services which was not great too many integration tests can slow down your delivery pipeline and they can be very flaky meaning they they may fail for very non obvious reasons so we had to shift left and think about how we want to test our services we enabled our teams to develop sort of services in isolation as you saw in the previous demo and we wanted to do the same for testing how can we test our services in isolation with more confidence so using some of the micro service architecture principles in docker enabled us to do exactly the same I'll show you how the diagram on the left you can you can think of it as a service this service exposes multiple resources as an API like resource want to end here and it's implementation is deep layer of stacks all the way to the database the small boxes here represents unit tests we take a small unit of code and we test it thoroughly but unit test alone doesn't give us enough confidence that when all these units coming together they are going to work as expected and that's why we need service tests to test the depth of the service the the long pink boxes here can be seen as service tests meaning if I call if I call the resource one API from outside it will exercise all the layers including the database and produce and produce an expected result we all love unit tests right they're fast easy that consistent they clean up after each run and they produce this nice core coverage reports which gives us a lot of confidence about our code wouldn't it be nice if would it be nice if service tests were as reliable as and as easy as unit tests so what are the challenges with service test setup dependent services and test data do I need to run do I need to run all the services that my service depends on maybe those services depend on more services so it makes a deep graph of services and do I need to run all the databases that these services use so building this environment with deep graph of services it's expensive right and it's it's unstable it's it's gonna be it's gonna be flaky and even if I manage to do all this I know I know it will it will slow it will be very slow to execute and I practically cannot trigger them on every check-in so let's see how micro service architecture and doctor helps us with these points starting with dependent services when you are operating in a micro service architecture as a service you really care about two things your immediate consumers and your immediate dependencies and nothing else no not no layers beyond that just your immediate dependencies using this to your advantage we created a simple mocking framework that was inspired by the web crawler technique so as a service I know all the APS I depend on up front like and all the api's I'm gonna call when when I execute any workflow so now what I can do is I can I can actually tell this mocking framework out all these api's and this marking framework and then further can go to these api's as it like a crawler and download download the data from those api is and save them in adjacent snapshot and later during the test run these snapshots can be served as mock so my service cannot really tell the difference if the if the DNA is coming from a real service or if it's coming from mock service so this solves our problem of deep graph of services the next step is to build the reliable test suite our service test suite looks something like this there are four containers the service container in the middle that the one we want to test the database container that comes to the series it has a mock container which is going to serve the JSON file so down on those the files that we downloaded with using the crawler and it has the test client container which is going to execute tests against the service by working API calls okay so let's see how this works in action in this demo I will show you the crawler based walking technique to download the snapshot of the immediate dependencies and then I will run a set of service tests ok so ok so here is the same service again I need to find crawler so test beauties ok so I want to show you API mappings ok so here they are there so these are the these are the aps that my service depends on I know them upfront so I can add them here what this framework will do is it will go over these api's and download the data in JSON files ok so let me run the download tool now so what is doing behind the scenes it's it's going to one of the share environments and looping or all those api's it's going to download these files then I'll show you after it's done okay so let's see here okay here's a whole list of files that are downloaded right the activity of downloading the snapshot is kind of independent of running the tests the frequency of download really depends on the stability of the dependent API like if if it is a brand-new integration then you might want to take frequent snapshots otherwise you can take it like maybe once a month okay so now let's spin up the test suit [Music] okay so it what it will do is it will create the three containers one the service itself the mock container thus this mock container will now serve the JSON files that we just downloaded and it will serve it as mark and the database container and the in the service container now let's run the test container the BDD test container okay so now you can see on the right that it started a test client container this client container is making requests to the service HTTP request to the service the service is then talking to the mock in order to process those requests or the database and then ifs returning result back to the to the test container the client test client container and this client containers further kind of validating the results on top of it okay so it's it's done a couple of things to notice here miss crawl up okay it ran hundred eight tests in twenty seconds super fast right and then it it creates it generates this nice code coverage report so with little innovation it is possible to get code coverage for all running services and and so what we have been able to do just now is to test the full depth of the service in isolation right and the characteristics of these Jets are in line with the unit tests they're easy to write the adoption of this style of service test has been impressive almost all the teams are doing it now they are fast since everything is local and as we saw it ran 108 tests in 20 seconds so not as fast as unit tests but relatively fast they are consistent that means they only break for real reasons and not because of some flakiness in the environment code coverage with some innovation it is possible to get some get code coverage from a running service as you saw in the demo the immutability aspect of doc docker container makes cleanup super easy just restart the containers after you're done and they trigger on every check-ins our jenkins CI pipelines are set up in such a way that it executes the service test upon average and the same service test suite can be further used for other style of tests here is an example of how we run chaos tests within the same setup so this is a chaos scenario where we want to calculate the profit and loss based on the start of the price if the real-time API fails to return the data this way the way to do it is kind of by hijacking one of the standard HTTP standard headers and tag it with some unique value in this case price which is 500 when the service receives this request it doesn't do anything with these headers all it has to do is to make sure it should pass all these headers to the downstream calls the mock in this case now the mock is a little bit more intelligent and when it sees this special tag it will return 500 status code instead of returning the real result and that that would make the service can fall back into this chaos moon in this case use the start of day price so in this way we are able to test the resiliency of the system in isolation and this same setup can be used for other different types of tests after the adoption of micro services in docker ecosystem it completely changed the way we deliver software and to appreciate this I will show how we used to deliver back in the monolithic days and compare it with how we deliver it today so back in monolithic days we had weekend only deployments and a dedicated team to do it it was a painful process a dev from each team would be on call in case something goes wrong it required extended down team down time and the roll backs were very very painful and since it was like we can only deployment there were a lot of changes that are going into it so if there was a problem we had to roll back all the changes today we have Dilli any time deployments we have empowered our development teams to push the code to production they are deploying multiple times in a day and all deployments are zero downtime and the roll backs are much easier now just only rolled back the problematic service he is a quick stat on the deployment history we deployed 554 times in last 60 days with moving average of 10 to 12 deployments a day and Mack switching to 2122 deployments and those long Peaks they're the gaps here you see are probably the weekends so we are not deploying on weekends anymore I will quickly touch upon patterns for safe delivery and I choose these to the dark launch in the feature toggle for two reasons first if you are in the journey of migrating from monolithic to micro services these two can be really real good assets and second they don't require any sophisticated setup you can just do it with simple setup dark launch is a practice where imagine if you have a view and code path and you want to migrate to v2 so the first thing is to clone the input that's going into v1 and then execute both v1 and v2 code paths in parallel and execute the v2 with the clone input and then you can compare the results of v2 against the v1 for the correctness and for correctness and performance so this will allow you to gain more confidence in your new code just make sure the v2 code doesn't mutate the state of the system otherwise you'll have issues feech it others so once you have gained enough confidence about your v2 version of your code you still need to be careful when rolling it out in the production so you can limit the blast radius by using feature toggles so in the workflow create a switch base of toggle and if the toggle is on run the v2 code path otherwise continue on v1 so in case something goes wrong you can immediately turn off the toggle it will improve your mean time to recovery significantly I want to wrap this up with key takeaways starting with tech stack microservices with polyglot stacks are truly magical it can further amplify your ability to react to changing markets but it requires strict engineering discipline developer experience invest into making your developer experience as quick and as smooth as possible your developers will love it testing testing and micro service world can be very hard shift left think about how to test your services in isolation delivery apply standard patterns were safe delivery your focus should be to improve your mean time to recovery once you have achieved this level level of maturity in the micro-service architecture where you have polyglot stacks services can be developed and tested in isolation teams are empowered to push code to production your software development lifecycle will absolutely run on steroids thank you 