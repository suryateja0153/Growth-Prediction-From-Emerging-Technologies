 my name is seth gleasman i am a solutions  architect at air verita and committee on   apache point and i'm really excited to be giving  this hands-on demonstration of building stable   serverless applications using apache flink  staple functions so uh if you're already   familiar with apache flink you likely know it as  a world-class stream processor it's very popular   in the data engineering space for continuous  etls real-time aggregations and reporting   and so there's this obvious question right  what does a stream processor have to say about   serverless and at the end of the session  i hope you walk away thinking quite a lot   so to begin i want to do a little bit of  table setting i think these terms serverless   and stateful serverless they've become a bit  loaded in recent years and that a lot of people   are using them to mean many different things to me  serverless is not simply these commercial function   as a service product although that certainly does  fall under the umbrella really it is a realization   of modern infrastructure capabilities allowing us  to iterate more quickly and with more confidence   so if our business is running a web app and  business is doing really well traffic spikes   we need to go from one instance to three  we're no longer requisitioning hardware   installing vms setting up networks instead  we simply increase our replica accounts   stateful serverless at its core is really just  about bringing these advances to the application   layer along with some key primitives that any  real world application needs so consistent   durable state your application needs to be able  to retain information it can act on in the future   cloud native fault tolerance so as we are  maintaining the state we want to do so in   a way that leverages what this underlying modern  infrastructure is really good at and to make our   lives as easy as possible in production and  simple messaging primitives between systems   your business is not built on a single  application but a whole host of systems   that need to communicate with each other in  arbitrary and complex ways we want to make   this as easy and intuitive as possible  so this diagram shows a very traditional   two-tiered application architecture something  i'm sure everyone here is very familiar with   where business logic is deployed via stateless  tier giving us those nice serverless benefits   state is then managed on separate tier via  database whether data store details here are   not so important but when an application receives  a request or other message something to trigger   computation it will likely communicate with that  data store and potentially update some values   and then do one of three things one it could  do nothing it could be that this message was   simply to update state we have done that and we  are finished two uh we may send results back to   an end user so this message or this trigger was  to read some information we're querying something   or three we're going to invoke another  service repeating the cycle you can think   of these different components interacting  with each other as a sort of data flow   and the obvious question arises what happens when  something fails well a fundamental problem with   this architecture is that for any failure  in any call across a service boundary it   becomes very hard to reason about which of  the desired outcomes were actually achieved   applications are forced to have a method of  determining whether or not to attempt to retry   or somehow make their state updates item post  but what if we rethought this product from the   beginning right what if we inverted it so  that messaging runs through the database   well it turns out this is exactly what stream  processors like apache flank have been doing for   years to provide what we call exactly one state  semantics business logic remains stateless and   is deployed as a separate service from data  storage but this time messaging is going to   flow through the database and the data store is  going to invoke functions in stateless containers   supplying state is the part of the payload of  each message these application functions will   then return back both state updates and messages  to be sent to other functions by moving messaging   from the compute player to the storage layer  state and messaging is easily made atomic   and if messaging were to fail for whatever  reason the state update is also rolled back so   retries are always itemposed this is exactly the  approach taken by apache flink staple functions   we are using a flink cluster for message routing  and state management while allowing the actual   functions containing application logic to be  deployed in a separate compute tier this gives   us a very powerful runtime where compute and  state are logically co-located for consistency   but at the same time physically separated all  state accesses and updates are integrated as part   of the function invocation request and response so  our business logic can be deployed however we so   choose it could be standard kubernetes service uh  using an orchestration tool like k native or even   a wholly managed service like aws lambda yet we  are able to retain consistent state and messaging so as we go through our checklist of a proper  stateful serverless framework the initial   requirements under pure serverless are easily  met by deploying our business logic in stateless   containers separate from everything else but what  about these stateful cons specific requirements   well to understand that let's talk about some  of the core concepts in stateful functions   when developing an application you're  going to implement several services or   what we call functions that are basically small  pieces of code or logic representing entities   within an application you could for example  define a function type representing a user   with a single instance of that function type  representing a single user within our application   think of this as uh in object oriented terms  as being the difference between a class in an   instance these function instances are invokable  through messages and do not consume resources   while i'm active or simply when they are not  getting your vote what this means is the runtime   can host a theoretically infinite number of  function instances within a fixed finite set   of resources and this whole thing is polygon  from the ground up so we are deploying these   functions in their own containers meaning you  can do so in any language so choose the only   requirements is that the language supports http  grpc or unix sockets which is to say we support   virtually every language communication  between the flink cluster and user code   happens to a very well defined and small protocol  certainly something you could develop against   yourself at the same time we realize that most  people don't want to do that day to day and so   the community does ship a number of predefined  sdks that wrap that protocol in higher level uh   idiomatic constructs for that language so there's  an sdk for python active development on golang   rust there's even a haskell sdk that recently  popped up in the community and hopefully your   favorite language coming soon uh adding new  sdks is very high on our prioritizations   but okay so we can have user code we can write  in different languages lots of people can do that   where things get interesting is that we can  run these functions with dynamic messaging and   consistent state so if you have used apache flink  in the past and you're familiar with this idea of   a dataflow dac that is completely gone instead we  support arbitrary communication between functions   using logical ids and so the only thing uh an  instance needs to know to message some other   function instance is its function type and id  so what sort of function do i want to message   in which particular instance if we were  maintaining user function to keep track of users   of our business we would have user as our function  type and there would be an instance for myself   so my id would be except someone else's might  be john or eagle or gordon or whoever else   and all of this can be done with exactly one  semantics so function instances are able to   maintain local state while the runtime ensures  that messaging and state updates are integrated   so users can have out of the box efficient  consistency this is true across event inputs   to the application application state itself  and outputs delivered from the application   and i think most importantly all of this is no  database required or better puts we are using   apache flink as our database so flink has long  provided a large-scale consistent state management   through these concepts of state back-ends and  distributed snapshotting state is stored locally   within the cluster for fast access season is  periodically backed up to simple blob storage   this could be amazon s3 google cloud storage  hdfs and nfs drive min io whatever you already   have available in the case of failure when a pod  that is part of the flint cluster itself restarts   for whatever reason it will simply download  its latest snapshot and continue on processing   this means we are not reliant on stateful sets or  persistent volumes for high availability of states   the only thing we need highly available  in the system is our blob storage   which is the easiest thing to achieve using those  model organizations have scaled managing hundreds   of terabytes of state within flink applications  themselves with the confidence they're delivering   consistent reliable results so that is enough uh  on concepts let's take a look at some specific   sdks uh actually build something today we're going  to be looking at the python sdk in particular but   all these concepts translate to all the different  sdks they all offer the same core premiums   so we need to begin by thinking about types  because remote functions can be implemented   in any language and a single application can  be composed of many functions written in many   different languages we need a uniform format  for communication and for that we've decided   to standardize on protocol if you're not familiar  it is a serialization uh standard out of google   that has very strong cross language support  and so all messages passed between functions   must be encoded as proto and in particular they  must be encoded as put above any which is very   convenient because it contains both the logical  type along with the serialized bytes and so within   a particular user function you can then quote  unquote unwrap that any message into a specific   concrete type using your language specific  protobuf library that you can then work against   uh same thing goes for state type so  anything we want is consistent durable state   must be player above any and this allows state  written arbitrary languages to be uniformly   maintained by the flint cluster uh flink state  back ends or simply going to store the serialized   any record at the same time we realize this  is kind of boilerplaty and so and it is if   you're working directly against the protocol  but for all of the language sdks we do offer   higher level constructs so that you only ever  have to develop against specific protocol types   using say the python sdk you will  rarely if ever actually see any record so uh as with any good introduction to a new bit  of software we're going to start with hello world   but make it state fund specific and so we're  building a greeter application that is going to   greet users of our service uh based on the number  of times that specific user has been seen so far   so every user is going to get a personalized  grade um first time i'm reading it might say   welcome stuff second time maybe welcome back  seth and third time third time's the charm seth   uh yet if someone else is created they are  going to get their own personalized reading   and this is going to show off some very important  primitives so we're going to talk about messages   right how to uh a greek request for myself  specifically how does that get to a function   and state so we need to maintain for every user  a count of how many times they have been seen so each function instance is associated with  function type and id as i said before which forms   its unique address this logical address is what  we do i use when messaging that function so when   i am to be greeted we are going to send a greek  request to the function type greater and the id   set we can see that as our input to the function  so this is the message that was being passed to us   again as i mentioned while the runtime is  using photobuff any by leveraging python3   types in this case uh we are able to have  the sdk automatically unwrap that for us   uh similarly we can send our  result to another function   uh we'll look at the middle uh bit of  creative and grading in just a moment   but we are going to both pack our results into  an any type so we can avoid that boilerplate and   send it to another function instance in this case  we're sending it to an email sender that is going   to ship out that greeting when messaging we're  using our address so we have our function type   which one is it well it is email center that's  the sort of function i want to message which   specific one while i want to message uh the  email center for this specific email address and then we can go into our  personalized grading itself   and this is showing off what i think is our  most powerful feature which is durable state uh   all this method is doing is keeping track of the  number of times this particular user has been seen   so far and then generating a message based on  that count so our state is being accessed via   our context and we are able to read out our  state based on some name and specify the type   so we're keeping track of scene count  which is the probable type i've predefined   we can both read that type out and write it back  and you know what that that's it the rest of this   is a standard python there's nothing state  fun specific about the rest of this method   the only thing that we have done  differently than say building this   in your cs 101 course is that our variables  are being managed via the context instead of   basic instance variables right so we're using  our context but otherwise it's just python right   and we get all these nice primitives  like durability out of the box so we've written our code right but we have to  make it available right it's it's running in   some remote container and so the first  thing we need is our function registry   this is going to map logical function  types to uh concrete bits of code   so in this example we have written both  our greet function and our send email   function they are both written in python  and they are both written in the same file   but neither of those requirements the send email  function for example could be a rust function or   it could be implemented in haskell and it could  be running halfway around the world smart reader   but we're going to bind these to a registry  and we're giving the function type so the when   we shoot off a message to that type we know how  to associate it with a specific concrete method   and then we need to expose it to the flink  cluster and we need to ensure that it actually   works against our protocol and for that we ship  a request reply handler which dispatches the   invocation requests to the bound functions and  then encodes their side effects uh both resulting   output messages along the state updates as an  http response to be sent back to the flint cluster   and then we simply expose this  handler using your favorite   http framework in this example and do later  examples i'm using flask but that is not a   hard requirements that is just something that i  chose to use plug in your favorite library here okay so greeters are interesting readers are fun  but um that's not what you're building your uh   business is not built on hello world applications  but it might be uh built on model certain so we're   gonna take a look at building a fraud detection  application uh specifically for credit card   transactions so as the transaction comes in we  want to build up feature vectors which require   looking at states right we need to remember  things about our users and our merchants   we need to query these functions in dynamic ways  and then we want to score that against something   that was likely provided by our data science  team giving us back a score on whether or not   we believe this transaction to be fraudulent  and at that point we can take some action   uh okay let's take a look at the  code for this model serving example   again we're going to be sticking with  the python sdk and for simplicity   all the functions are implemented in a  single file as a single flask application   but just reiterate make it very clear though that  is not a hard requirement simply for simplicity of   this demonstration these functions could all be  implemented in different languages they can be   packaged and deployed separately that is supported  and expected workflow of many state fund systems   so we're going to be building up feature vectors  whenever a transaction comes in we need to get   information um that we can use to send to our  model and one of those features is a fraud   so how many times over the last 30 days has  this particular account reported and confirmed   fraudulent activity the idea here being that the  more often that we see fraud for a particular   account the more likely we are to continue to see  it in the future it's a rolling 30-day sum because   people's behavior changes and so as things become  further in the past they become less relevant   so uh our function type is variverica counter  right this is the logical type we will use to   uh message this function and we take into  parameters our context it gives us access   to capabilities like state and messaging  and the actual message that was sent to us   leveraging python 3 type annotations we get to  avoid all of our any protobuf boilerplates and   i'm using a union type here because we support uh  working against multiple message types so let's   start with this confirm fraud message a record  is going to come in say from a kafka topic that tells us that a user has confirmed fraudulent  activity against a particular account uh this   function i forgot to mention is always  scoped to a particular account id so   fraud count is our function type account is  going to be our id for the logical address   when this comes in we need to increment our  accounts and so all we're going to do is go into   our context we're going to read out the current  accounts and then we're going to increment it   if it already exists or initialize it if there  has been no fraud over the last 30 days for this   particular account uh once we have done that we  will simply re-pack it we will set that value   and we're done so while we have switched to using  a context versus local variables we are otherwise   just writing very simple python code and getting  fault tolerance and durability from the runtime   but i said we also need to do a rolling 30-day  count so every time i increment this fraud counts   value in 30 days i need to decrement it well  we're able to send messages to other functions   but it turns out we're also able to send messages  to ourselves and more interestingly we can send   messages with delay so after we increment  our account we are going to pack and send   after and we are this means we are going to send  a message where are we sending it well we're going   to send it to ourselves we have the context we  can get the current address and so we're going   to send ourselves an expire fraud message that  tells us to decrement but we are going to give   it a delay of 30 days so this message will  not arrive until 30 days after we send it   uh and the runtime is able to ensure this  message is consistent and durable so that if we   have failure over the course of that 30-day period  for whatever reason this message will not be lost   and we do that and we're ready to go so we see  that expired fraud is also an accepted type and so   after 30 days it will arrive and what are we going  to do with it well we're simply going to decrement   our value so i will read out our fraud counts  we'll decrement it and then if it's zero we'll   go ahead and delete this state entirely just frees  up a bit of space and makes things more scalable   but this is really in optimization detail  otherwise we are going to go ahead and set   the new value so if it was five it's now four  we have decremented it and we are good to go but   storing state is fine we also need to act upon it  and so the third message type that this function   accepts is query fraud someone can message  a particular instance of this function right   they can query for a particular account and ask  how much fraud have you seen over the last 30 days   when we receive this we'll simply check our state  value if it's not already set if there is nothing   there we'll give it some default and then we will  reply so send this message back to the caller this   is everything we need for a distributed durable  consistent state and messaging of this function   let's now see how it's used so i have some of the  functions in here we're going to skip past but   uh the main function in this workflow is what i'm  going to call the transaction manager this is what   coordinates the whole workflow and builds up our  feature vector every time a transaction comes in   so uh again we have our contacts we have our  message types the main one being a transaction so   every time a user says swipes their  credit card or does something else   we will get a transaction event that contains the  account id it contains the merchant id of where   they were making this purchase and the amount of  the transaction and so we see this we're going to   cache it in state we want to hold on to this and  make it available later on and then we're going to   uh farm out to our different functions that we  are using to build up our feature vector so you   can see here we are querying that counter we just  defined above and we are going to the instance   for this particular account we are also getting  some merchant information and some other values when these functions respond right we saw that  our fraud count replies back with a reported   fraud well here it is when we get this what are we  going to do well there's a bit of business logic   to ensure that we have got all of our features if  we haven't we'll store that reported fraud count   and state until we get all different features back  but when we have them all we are going to build up   our feature vector and message our model this is  likely living somewhere else provided by the data   science team they're going to iterate and deploy  this separately of the rest of the application   and it will take in that future vector  compute a score and respond back   when it does so we are going to get this fraud  score so this is uh our confidence interval from 0   to 100 of how likely we think it is that something  is fraudulent so 0 being is absolutely not and   100 being this is absolutely fraud when we get  that score we will compare it to some predefined   interval and if it is above the threshold say  80 percent well we will send an alert to a kafka   topic called alerts that says hey we think this is  fraudulent and the user will see that and they can   act upon it they can maybe confirm it and the  bank will block that transaction or they can   say you know what this was really me uh please let  it go through we are also going to delete all of   our state values at the end because hey we have  scored and alerted on this transaction we don't   need to retain this information any longer as  we have built all of these up we are making them   available via the request reply handler and  we are packaging this as a flask application   i've defined an endpoint state form that accepts  a post and so whenever data arrives whenever we   receive that we will simply uh send the  whole payload to the handler and it will   manage dispatching to our functions encoding  our effects our state updates and our responses   and we will simply send that back to the caller  of this end point which is the flink cluster when we go to package this so let's take  a look at the docker file you'll see that   there is nothing state fund specific here this  is a plain and simple flask application there's   nothing about the flint runtime we're going to  see there's nothing special about this in any way   and if we look at our dependencies as well  uh we are including the state fund sdk   which is what wraps that high-level protocol  and then we are pulling in flask and whatever   other python dependencies we need so we're this  the model function we might be pulling in numpy   or scipy or any of those good data science  libraries we have full flexibility here   and when it comes time to deploy this well we are  going to deploy it as a standard uh kubernetes   deployment so i have written this deployment  specification um i've pushed my image i want   10 replicas of this because i want to be able  to scale out we are exposing it under port 8000   but this is all stock in standard kubernetes   additionally there is a service that is making  it reachable so that gives us our user code but   what about the flink cluster right how does  it know where to all right so this file is uh   our module.yaml file this is the configuration we  give to the flink cluster that tells it how to map   function types logical function types to  addresses under which our functions are reachable   so we can see here i have our counter function  i have said that this is the logical function   type so when you see a message that is targeting  ververica counter this is the metadata you should   use the function is exposed as an http endpoint  and this is the specific end point you should use   we also have at the bottom our e ingresses and  egresses this is how the functions communicate   with the outside world so you thought saw for  example that we were sending alerts to a kafka   topic we are also reading our data from kafka  let's look at the example of our confirm fraud   message well i have said that this is coming from  kafka i have given a name and i have my kafka   specific configuration so where the brokers  live on consumer group ids things like that   and then uh we give it a list of topics  to consume from so we are reading from   the confirmed topic we've specified our type url  so what sort of data are we reading and then we   give it a list of targets so what function  types do we want to send these messages to   we give it a list of types the id  is implicitly pulled from the header   and it will route our messages to the  appropriate function to begin that computation along with kafka we support  uh aws kinesis out of the box   and then if you're comfortable  writing a little bit of java code   we also support a whole host of other  systems including jdbc elastic pulsar proviga   rabbitmq and as we see demand we'll add more  first class gamble support for those other systems uh we we're going to take this file after we have  written it and build our doctor image so this base   image flink state fund contains the entire apache  flink runtime along with all the stateful function   specific runtime code and all we need to do  is copy our module.yaml file onto the image   there's no java code to write there is no flink  specific code right i am also uh including a flink   comp which is some point cluster configurations uh  but this is stock and standard if you have written   other apache flick applications in the past and  this is the image we're going to use to run our   cluster and so i'm in fact already doing that  uh let's go ahead and take a look at our pods   i have a kubernetes cluster that is running  uh three kafka brokers for our data i have a   data simulator that is simulating transactions  and confirm fraud accounts and all those good   things and then we are running our flint cluster  and our user code so i'm running three instances   three nodes in my flint cluster each of these  only have a single core so it's very small   and then we are running our user code and i  have a replica set of 10 so i want to really   scale out that compute if we go ahead and let's go  ahead and i believe i'm already port forwarded so   i can pull over the flink ui you can see that  everything is up and running this is the flink   ui if you're not familiar it tells us what our  application is doing and so we can see here we   have processed uh since i started this roughly 200  000 messages uh let's these are all calling out   to our user code it is being routed through this  application it is syncing the results into kafka   if we take a look at our checkpoints at our fault  tolerance we can see that things are going through   smoothly there was a failure but that's okay  we handled it gracefully uh and currently i'm   managing about seven gigabytes of state within  the flint cluster remember this is being stored   locally either in local memory or spilling to  local disk but it is always local we are never uh   using persistent volumes of stateful sets  min io is providing all of our faults   and when it is time to go make a change so maybe  i want to change my replica account or i want to   deploy a new version of my user code all i  need to do is apply those values that we have   for our function so i can  cube control apply dash f   state fun functions and this has our  deployment gamma and our service channel   and this will apply those changes in this  case i haven't actually changed anything   we can also apply a horizontal load  balancer so perhaps i did not want to   have a static set of functions but i want to scale  as my load goes up and down throughout the day   we can do that and we will be able to do so  gracefully uh and we can multiplex different   function modules together so i'm running this  code perhaps i'm more of a data engineering team   and so we're in charge of building the  feature vectors maintaining that state   the data science team has their python code  that is our model they're going to deploy   that separately and make their own updates and  we can all do that gracefully and consistent   so i really appreciate you taking the  time to listen my talk today i hope you   are excited about staple functions and the  future of stateful serverless applications if you have any questions i'm always on twitter   at sjwiestman also the apache flink  user mailingwest is the most active user   mailing list of any apache project and it's  a great place to get help thank you so much 