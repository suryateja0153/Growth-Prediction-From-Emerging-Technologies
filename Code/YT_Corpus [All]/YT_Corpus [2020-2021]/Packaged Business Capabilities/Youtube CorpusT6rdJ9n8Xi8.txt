 let's talk about how to accelerate data power innovation within your enterprise we will look at some of the key challenges that enterprises face and a five-step journey that we have seen lots of successful businesses take to get to data power innovation we look at what enterprises encounter along the way and discuss our Google cloud can help with examples hello and lack Lakshmanan I leave analytics and AI solutions at Google cloud what are some of the key challenges that enterprises face especially when it comes to data well firstly and probably the most important is the size of data the amount of data in most enterprises is growing dramatically 20 to 30 percent year-on-year such that an e-book back after five years you're looking at about a 5x increase in the amount of data and it's not just the size of the data by 2025 a quarter of that data that you have is going to be real-time nature and as you know real time data is harder to manage and maintain then historical data in addition much of the data tends to be unstructured and today less than 1% of unstructured data is analyzed or even used and so these are some of the key challenges when it comes to data and technology that's used to to work with the data and then put in place the environment that we're in today and you have unprecedented market ships and so you need to be able to analyze the data processed data increased velocity and scale if you want to power data power no innovation to your data and so lots of enterprises are driving their data-driven business transformations using Google Cloud so whether it's businesses in retail or gaming of Central Services and all over the world whether in the Americas or India or Japan and AIPAC we have experience helping leaders and all of these companies bring about data rate change and this consists of a five-step process and the first step is to stop the bleeding you need to reduce the cost of managing data especially as the data sizes grow dramatically and one of the ways that you do that is by migrating a data Lake and data warehouse to the clock once you have your data centralized the next bit is to break down silos within your organization build a data culture to democratize analytics and once that once you have everyone within your organization able to access the data process it the third stage of the journey is to be able to make decisions in context faster Rena beyond time I'll talk about what I mean by in context but once you're in this place where you're basically doing descriptive analytics as as well as possible it is then time to move on to predictive and prescriptive analytics and at that point the first step the journey is to look at end-to-end package AI solutions to leapfrog so that you're not building everything from scratch you're able to take advantage of what's already available in the marketplace and finally to be able to empower data power innovation that is truly differentiated by building an operationalizing AI so let's walk through each of these steps and talk about what enterprises encounter along the way and some of the things that we have to do the first step as I said is to find the budget to carry on data powered innovation and in order to do that you have to lower the cost of ownership you have to simplify operations and so that is the first stage on the journey to carrying out a Orbitz innovation the problem is that your traditional enterprise data warehouse implementations they hinder business transformation first of all they can't scale fast enough to keep up with us 5x growth in data that we talked about and secondly as 25% of the data when it comes streaming it becomes really important that all of your queries are on fresh data and you need your data to be fresh and that is a problem with a lot of enterprise data warehouse implementations thirdly we talk about breaking down silos and building a data culture it is extremely important to make sure that you do that in a secure way then and oftentimes the way that security is done is it provides data access restrictions that inhibit collaboration between different parts of your enterprise and of course the cost of maintaining and by the cost we don't just mean the software costs a license costs but also the cost of operational ISM so you're basically spending a lot of time tuning the data warehouse rather than getting insights from it and finally we want to make ourselves ready to carry out machine learning and AI and that is also something that traditionally DW implementations kind of hinder so that's the reason why a lot of industry leaders rely on Google Cloud when it comes to modernizing your data whether it's Home Depot who wanted to accelerate the time to value or Zulily who needed to be able to get instant insights for all of their offers that they're create dynamically on Hearst Newspapers wanted to use advanced predictive and analytics AI or AccuWeather which is looking at sharing data not just within their organization but with all of the partner organizations and their share and those kinds of insights with ease and doing all of this in a secure way which they know you want to be able to protect data operated trust as HSBC does the technological capabilities of Google cloud the fact that our data warehouse a server less is self-tuning that we offer you high performance streaming ingestion that every sequel query is automatically streaming in nature that you can operationalize machine learning without ever moving your data out of the data warehouse this is what allows you to modernize your data architecture and of course all of this is set up from the premise that you need to find budget to carry out your data innovation and that budget you're gonna get because by migrating to the cloud we can lower your total cost of ownership massively so oftentimes more than a 50% reduction and cost but it's not just about the data warehouse because data platforms and data rolls are converging and they need to interoperate so when we talk about a data warehouse and in a warehouse today functions often as a data Lake also so where we traditionally used to have extract transform and load these days you just extract and load directly into data warehouse and from there you transform as necessary and if you have Hadoop workloads in your data rake you want those hurdles to be able to read data from your data warehouse and vice versa you might have queries in your data warehouse doing a federated query on your data link so your data leak in your data warehouse we need to be interoperable and form the common data platform and this is true not just to the technology it is also true to the perspective of the people who are going to be using the data so whether it is data analysts using business intelligence tools or data scientists using a data science platform or machine learning scientists and data scientists using tensorflow and machine learning they all need to be able to access data whether it's the data lake or whether it's in a data warehouse that interoperability and this ability to build a data platform that combines both the data warehouse in the data layer or extremely important and that's the way that we've designed Google Cloud State about form so our idea is that you need to be able to derive a return on investment from your data link so that it's a business asset and not just an IT cost so all of the things that I've talked about in terms of modernizing a data warehouse they applied to a data Lake as well you want to get large-scale performance you want to do this is lower cost you want to be able to reduce your operational complexity you definitely want to make sure that your practical data that it is secure that you're breaking down data silos between different parts of your domains and using AI to extract value from all of your unstructured data so whether it's Pandora or our Institute or Metro all of these people are building data lakes for their unstructured data for their semi structured data and for their hadoop workloads on google cloud such that it is the business asset and for example done hundy had a blog where they talked about how they were able to move from their traditional monolithic Adu cluster to ephemeral clusters by ephemeral we mean a cluster that is job specific and get a cost savings of over 75 percent so again the idea of moving your data lay in the data where of the reason that it's your first stage in your journey is that it opens up the budget to do all of the best of the innovation that you want to do and in order to help you do that we have a lower-risk offer to migrate and modernize your enterprise data warehouses and your data leaks so if it's a data warehouse we have ways to virtualize and translate your queries automatically and optimize and reduce your cost if it's a data link we have ways to different shift and then move towards ephemeral clusters and then moving into a completely serverless system once you have moved your data warehouse in your database the cloud the next step is to look at how to democratize data access and build a data culture in your organization to talk about the kind of benefits it can take for example HSBC where they had to calculate the liquidity position of a country it used to take them 10 hours to do but they were able to do it in less than 30 minutes they were able to do financial crime analytics using bigquery 10 times faster but the reason they were able to do all this was because they were able to move their workloads to GCP such that all of the teams around the world has access to capabilities to help them innovate so we see the result of the innovation but the reason that that is possible is because of this democratization and of the breaking down data science and if if you're a company that need to move your to the cloud you want to make sure that you're moving it in such a way and breaking down your data silos in such a way that you have strong security and governance controls right especially when we talk about whether we want to basically provide access to data for any team who can access it who's allowed to access it you want to make sure that all of your data is encrypted in transit ingressive there's not a bucket out there that somehow left open that you're able to bring in your own encryption keys that you're able to use cloud DLP for classifying and impacting sensitive data are you able to discover data that all of your data that you're putting in the cloud is subject to third party audits and certifications if you need to be HIPAA compliant that you can be here for compliant and finally that if someone accesses the data there's access transparencies where you can go look at the audit logs and you know who has accessed it when they access it and by the accident so when you're moving data and you want to make sure that your data is available to a broad set of your organization one of the key problems is how do I know what data I have and that's where data catalog comes in we have a simplified data discovery mechanism such that you can go ahead and find what data sets you have and data catalog has built-in governance that you can for example say things you can tag pieces of data and on not necessarily entire tables not necessarily entire data sets but just columns and say these columns are sensitive data and you can cover and sensitive data across the organization in a simple straightforward consistent way but one of the things that happens is as you build your data warehouse on the cloud you start to centralize your data bring your data from sa P from terror data from Oracle from YouTube and Google Apps and you centralize them into the data warehouse the data warehouse on Google Cloud is called bigquery once it's centralized there what many organizations find is that they start to see analytics so rather than carry out the analysis in the data warehouse itself you see people taking extract of the data throwing it into spreadsheets throwing me to the dashboards and all of these things start to become slightly out of date so the next step is to look at how can we do analytics in a modern way and the way to do that is to apply governance and control to your KPIs and so we're looking at modern bi and analytics subsets you can have dashboards with tiles that are shared across your organization reports and and self-service definitely but all of their self-service reports and exploration take advantage of centrally built KPIs and this allows you to take insights and embed them into web sites and existing applications and also build data-driven workflow some customer applications with it so it's one thing to centralize your data but you also need to look at how do I centralize the analytics on the data how do I make sure that those analytics themselves are reusable instead of everybody writing their own excel formulas of everybody writing their own ciphers and this is again something that a lot of our customers have found great benefit and Credit Karma for example says there used to be a lot of Excel spreadsheets floating around and now it's audun looker program under common governance or Autotrader talking about democratization of how they will in a matter of weeks able to go from nothing to something that was usable but all of your sales squats so the third step then so the first step is to move your data to the cloud second step is to open up access and build a data culture and the third step now now that there is now everyone has access to the data the third step is to be able to make decisions in context faster and with real-time data so one of the key things when we talk about context is the location is incredibly important when we look at tabular data from most enterprises we see that about 15% of tables tend to have information under store locations customer information etc and you need to be able to do accurate spatial analysis within your data warehouse you don't want to have a separate your spatial database and that's one of the key things that you can do this that you can unless GIS data will fill your sequel part of your dashboard and be able to incorporate location information because location is incredibly important context the second big piece of context is stannum so real time data is very very very valuable so there is a big difference between being able to act on and on a three days later or being able to make a decision on and own within a matter of minutes and or within a matter of seconds real time is real value and but there's an e-commerce or in gaming or manufacturing or in retail you want to be able to make decisions as quickly as possible and in order to do that again we start with the data warehouse but the key thing to realize is that in the data warehouse and Google Cloud every sequel query is automatically streaming so all of the investments that you have done to get to the point of democratizing access and building dashboards and building common analytics all of these actually work in real time as well what we need to do is to make sure that they're ingesting transforming and enriching the data and streaming it into the data warehouse and that's something that we do naturally it flowered because both pops up and data flow just like requiring or all serverless they are very easy to operate very low cost and you can spend all of your time getting insights from the data rather than on operationalizing and on on the IT infrastructure of building a messaging system or building a data exists and once you have your data in the data warehouse you can do machine learning directly the data warehouse you don't have to move your data out of the data warehouse and this you to carry out machine learning as part of the streaming analytics pipelines for things like anomaly detection for example and this is learning credible benefit because normally when talk about machine learning we're talking about a data science team that has to eradicate all of this data and whenever you replicate data you run into a security issue right you have two copies of your data floating around so bickering machine learning baby allowing you to do machine learning directly in your data warehouse enhances your security posture enhances your access transparency and this is exactly what a VJ see for example it was able to do they have 25 user sessions per day they need to analyze 50 billion banks and they wanted to make better decisions and obtain increased revenue and in order to do that they needed they were able to basically fertilize access to large volumes of data within their company to basically carry out operations so at this point in the journey you have the ability to have a data culture to be able to make decisions in context spatial context temporal context and then the next step is to say okay how can we now do more prescriptive analytics and at that point you want to say you want to look around the marketplace it's not necessarily that you have to delete every piece of AI you need what can you buy because when you buy you can leapfrog and Google Cloud we have we have lots of readily embeddable AI marre so for example if you want to process video and you want to do video classification or action recognition or object tracking you don't need to go build this from scratch we have an API you can use and you can use it directly and in your applications for it or for example if you are into translation and you don't need to go build your own translation model Bloomberg for example and you use article comes in they need to send a global update to 40 in 40 languages to over 170 countries and they do that we go back to it and so the idea here is that Bloomberg is quite capable of building their own translation model but they did instead what they did was that they bought it and similarly right if you want to improve the performance of your contact center there are three things that you want to be able to do you want to be able to look at standard queries and automate them as much as possible and if a query is handled by a call center agent you want to make sure that that parade that's handled the first time correctly you want to improve their customer service of your support organization by providing the call center agent all of the information that they need as soon as they need it and finally you also want to use the support calls that are coming in to understand the business manner and this is the game something that you could build from scratch but why do that when you can leap from the head with the contacts centralized solution room to be plowed and that's what Ticketmaster does for example or that's what Marks & Spencer does where they basically take advantage of our contact center AI solution to improve their customer service similarly if you have forums and you want to extract information from the forms one form parsing you could build it from scratch or you could use a document AI solution and leapfrog ahead and allow us to basically carry out all of the hard work of doing natural language processing and building then for example if you wanted to claims processing here's an insurance company that was able to use a document AI to get an incredible reduction in the claims processing and settlement time by again leapfrogging so the key thing with AI is to think about what kinds of things do I need to build them what kinds of things can take time and when it comes to buying AI models you want to buy AI models that are high quality that are that have been no battle tested and that's something that you can get out of our platform the fifth step then at this point you you have prescriptive analytics but the prescriptive thing that you have is primarily things that vary by the last step of the journey to doing the ADA power innovation is to empower your data and Sheen learning teams and give them the tools for them to be successful so let's step back a bit and think about faith I talked about leapfrog in with AI and I were about to about a custom AI and the idea is that in in your organization you're gonna have a portfolio of business use cases not everything needs to be built from scratch and not everything in Canada bought so you need to look at your use cases and maybe make a determination of those use cases on 3criteria how faster you wandered how much effort do you want to put in and how much differentiation does this business offer and based on that it allows you then you choose whether you want to use a fully prepackaged AI model or B you want to take a bunch of building blocks and combine them in a bespoke way or do you want to build custom AI that is fully fully differentiated and regardless of which of these you want to do we have a platform that allows you to do it that gives you the ability to do it and we can partner with you to help you on this journey of others using the a out-of-the-box are building up on top of our building blocks like after Emma and the Curie ml and the vision API etc or whether you want to do custom ear but the idea here is that you will have lots of applications that are collecting and storing data into the data Lake and data warehouse that is a first part of your journey to doing data power inhibition and having done that then you basically arm your data science teams to be able to build and use AI when building use AI regardless of whether it is AI out of the box of building blocks or custom AI being able to take it experiment with it generate insights from it but once you start to see that this data science team is being successful in the development phase you need to have the to promote it and operationalize the machine learning and that's what we call Emma laughs to be able to do this routinely to be able to monitor and retrain the model when the model's start going out of sync because of changes in the environment of the data and that for example eBay for example uses custom AI third part of my model whether you want to do visual product search and at that point the training an image classifier on 55 million product images getting a 10% increase in image recognition accuracy in 10% is an incredible increase in accuracy been able to do that 10 times faster because of the amazing hardware called GPUs that are available on cloud again Google cloud incredibly differentiator in terms of both our data platform and in terms of our IT platform so when you are taking a journey to enable data power innovation within your organization we can help you derive value our every step of the journey whether it's to reduce TCO in the first stage whether it's to democratize analytics in the second stage whether it's to be able to make higher value decisions in the third stage whether it's being able to leapfrog your competition in the fourth stage or being able to really create a Frenchie ated transformative AI in the fifth stage we are happy to work with you at every step of the journey thank you 