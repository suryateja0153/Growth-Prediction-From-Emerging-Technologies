 (upbeat music) - Hi and welcome to my talk, the Tyranny of Data. It sounds like a dark depressing talk, and it's not supposed to be like that. It's gonna be about the good things that you can do with your data. But first, we're gonna have to talk a little bit about all the bad things that data maybe causes us in our daily working lives. My name is Sam Newman, I'm sort of here because I'm one of the people known for being an expert in the area of microservices. I actually wrote a couple of books on the topic. So my first book "Building Microservices" was out several years ago and I've just released a new book called "Monolith to Microservices", is all about how you take big existing systems and break them apart. And part of what you have to do when you break those systems apart is dealing with the data, and we'll come back to the aspects of databases and microservices a little bit later on. But to start off with, we should talk about data. Data is wonderful stuff. Data helps us get information about what our customers want and what they need, it gives us information about how we can help people, it's sort of the lifeblood of so much of what we do. And an awful lot of what our lives are as software professionals is working and managing and looking after data. Look as a computer programmer, 75% of your professional life is gonna be taking data out of a database and putting it on a screen. And if you're very lucky, the other 25% of your career is gonna be taking data off that screen and putting it back into a database of some sort, or is that right? Does our data always have to live in a database? The classic way we think of storing a database is the relational database, and the relational database is a wonderful thing. It gives us all kinds of powerful capabilities like transactions and you know, the ability to run ad-hoc queries and we can join across large volumes of data, but relational database is always kind of on to have all the data in it. We keep putting more and more data in it, it grows over time and we start realizing that we have to take our application to the database to make it work. We start doing horrible things like creating, putting database triggers and creating entire systems from stored procedures. The database grows because our data in it grows. And as a result, more of our time, energy and attention is all around this big, giant, critical entity inside our application. Our databases as they grow, become too big to fail for many of us. And the thing is, this is an issue, right? The database grows to such an extent that we're now scared of it. All of our critical information is in that one location. And as a result, any changes are really fraught. And so what do we do? We don't change the database structure, we find difficult to look at different platforms, any shift in making that data easier to work with are maybe sort of de prioritize, because the risk associated with any changes are so great. And this is a real issue. Data is valuable, we want to lock it up, we want to keep it safe. So we put it in something which we hope is safe. We put it in the database, that's where it is. We hide it from the outside world and we won't be tied as though if any of that information ever leaks outside. But again, let's think about that. Data is valuable. Does it make sense to put your most valuable, all of your most valuable stuff in one location? Is tantamount to saying, okay, we've got all of our valuables here and we put a big sign on it saying, "Steal Me". If anyone does breach that database and gets hold of your crown jewels, everything's gone. Large amount of data breaches that we see are not about one database being accessed. If everything, all your most critical, most sensitive information is in one location. And that one location is breached. All of your data walks out the front door, and that is a real issue. Now, of course, we don't want this. We don't want the bad people out there to gain access to all of our information. And so when we put it in that one location, we do everything we can to protect it, but we can't escape the fact that every now and then we might make a mistake. We may leave a back door open that allows someone to gain access to our information. And if all of the data is in that one place that magnifies the impact of one simple mistake that we might make. And this is quite aside from the fact that by having all of our data lumped together, we actually makes a bang, various different compliance out there, much more difficult, aspects related to complying to things that the GDPR or HIPAA compliance or PCI compliance. A lot of this is about how we treat and manage certain types of data. Now, if our most sensitive data is bundled in with our least sensitive data and all of our application works off that one big giant data source, that affects the larger parts of our ecosystem fall under the remit of these various different schemes. When you're looking at PCI level one compliance, for example, sure, you might only sporadically store full credit card information, but if that's in with all the other data you store, any process that accesses that database, reading or writing, even if it never touches that particular piece of data, it's gonna fall under the auspices of your quarterly audits. So a lot of time it can make sense for us to try and break that data apart to reduce the scope of us having to comply to various different industry regulations. And this has all got a little bit more complicated recently, we start thinking about data and where data lives. We've got a situation now where the EU courts have basically said that data for EU citizens can't now just be sent off to the US to work, to still be stored in the US and that's gonna cause a lot of tech companies to think differently about how they store data for different citizens. This idea therefore that we can store all of our data in one big place and sort of let everything be sucked towards it is kind of falling by the wayside a little bit. And this is a problem, right? Because relational databases work really well when everything is in them, but that causes us so many different difficulties. That big giant database has been around in your enterprise organization for many years, where it sucks all that development energy and effort towards it. The data should be working for us, but all too frequently, we find that we are working for the data. And, you know, I like to think that we can do better than that. Let's talk about microservices, because that's part of the reason I'm here, right? When we talk about microservices, one of the first things we focus on is this property of independent deployability. And this is the idea that I can make a change to microservice and deploy that microservice into a production environment without actually having to deploy any of the parts of my system. And this is the kind of the thing that we want from a microservice architecture. It would allows us to be much more effective and efficient in how we deliver our software. Now, if we zoom in and look at one microservice in isolation here, typically when we think about a microservice, we're thinking of all the code implements that microservice was being packaged together as a single process. So the deployment unit of a microservice is gonna be a process, maybe stuck inside a container. That process is gonna communicate with other processes via some kind of network protocol. That could be something like a synchronous REST-based API, or maybe if we're being a bit fancier, some kind of nice topic-based system maybe Kafka for example. And when we start thinking about data, we talk about data being hidden inside a microservice boundary, rather than using big shared databases, we say, well, look, the data that the shipping service needs is hidden away inside that shipping microservice boundary and this sort of a distinction of a microservice architecture is what causes us a lot of headaches because that's moving us away from the big giant share database. And that in turn causes challenges to us. Now, James Lewis, who was one of the sort of an old colleague of mine, and that's actually one of the first people talking a lot about microservice architectures. He talks about microservice architectures as being an architecture which optimizes for autonomy. Organizations all around the world are moving to microservice architectures because they see it as a way to make their organizations work in a more autonomous fashion. Autonomy is always a bit of a questionable word. It can mean so many different things to so many different people, but when I talk about autonomy, I think when James talks about autonomy, primarily what we're talking about here is the ability to distribute responsibility rather than having to have all decisions made centrally. Instead, what we'd like to do is push decision-making responsibility into teams, allow people to make decisions that are close enough to see the impact of those decisions. Allow people to react more quickly, allow people to work more efficiently in parallel, avoid the need to have to constantly coordinate between separate teams. And this one should allow you to do more stuff. If you can add more developers who can work more in isolation, reducing the amount of coordination they need to make with other teams, they can get more things done and ship more products and make their users and make their customers happier as a result. And there may be other reasons why you might adopt a microservice architecture. But in my experience, this is the most common reason people pick microservices because it allows larger development teams to still work efficiently without seeing the sort of huge downsides that can occur from just adding more people to the project. This is about getting the people we have working smarter. And if we get these boundaries right and we embrace this idea of independent deployability, this sort of promise of organizational autonomy can come true. So independent working is a key idea here. Typically the microservice architecture, we have this idea that a microservice is owned by a team. So Team A is a shipping microservice, Team B owns the customer service and the account service and Team C owns the return service. So if I'm working in Team B, and we may need to make a change in the functionality of the customer service, because there's property of independent deployability. I should be able to make that change and deploy that change into production environment, release that functionality out to the users of that software without actually anybody else having to know. As long as the way in which that microservice communicates with the rest of the world remains consistent. And the interfaces that they exposes remain consistent because here the customer service is being used by the shipping service. So I wanna make sure that when I make that change, I haven't broken the upstream consumer. Likewise, I don't wanna break the interaction with downstream services either. So when we wanna make independent deployability work, we need to kind of maintain backwards compatibility. And that's key to making independent deployability work. Now maintaining backwards compatibility of something like a service boundary can be a bit tricky, but it's okay because we've got a cutting edge idea that could help us here. It's a cutting edge idea that comes all the way from 1971. And that is a concept called information hiding. Information hiding is a simple idea. It comes out of the world looking at how to define effective module boundaries, work done by David Parnas. And the idea is kinda straightforward. Let's consider a prototypical service here we've got a microservice of some sort, and I've got an outside party that wants to make use of the functionality that this microservice exposes or the data that it has. Now we could decide to allow the upstream consumer to just reach in to all the internals of this microservice, reaching directly into the database, directly accessing kind of internal objects and implementation details. The problem of this of course, is when I'm working on that account service, it's very easy that as a developer, I might make a change to the internal implementation detail that ends up breaking compatibility with my upstream consumer. And if I don't have clear separation between what code and data can be changed freely and what code and data is actually part of that excellent contract. If those ideas in my head, as a developer are separated, it's very easy for me to make changes that break exit or parties. And the big giant shared database is the ultimate shared implementation detail. If you allow other people to reach into your database, it becomes very difficult as someone who's maintaining that microservice to be clear about what things can change without affecting other people and what things might have a nasty ripple effects. With information hiding, what we're aiming to do is hide as much information as possible inside the boundary of a module or in our case, our microservice. That's hiding implementation detail. In fact, the idea with information hiding is to hide as much stuff as you possibly can. You can always expose information or functionality later on that was hidden, but once you've exposed it, it then becomes part of the contracts that you expose to your external parties. So just be really careful about everything you share. If you're very clear and explicit about those interface points which are exposed and which those implementation details which are hidden, your world becomes much simpler and much easier to deal with. As a developer working on this account service, I'm much more in a safe space now, knowing that if I change anything in that hidden world, I'm not gonna affect anybody else. It's only the stuff in the shared world, the things that I've exposed to the outside party that I need to maintain compatibility with. So this is why what we want is a well-defined service interface. When you have a microservice that exposes functionality over some interface, be a REST-based API, a GRPC endpoint, or you know, a Kafka topic, (indistinct) events, as long as I know clearly this is what I expose to the outside world, this is what must remain consistent. As a developer I'm freed. I find it much easier now to work and make changes in an independent fashion. This is also good for the upstream consumers, because they've got an explicit understanding about what it is that you provide them. So information hiding is super, super important. It allows us to make changes in a safe way. Information hiding fundamentally makes it much easier for us to implement backwards compatibility and hiding data in this world is vital. And that's why we want independent deployability with our microservices, we need backwards compatibility, to make backwards compatibility our reality we need information hiding. We need to hide our data to get that independent deployability. This is a point it's often missed. So splitting apart data effectively can allow us to do more work in parallel. Now, teams to develop products in parallel, get more stuff done with the people we've got. And it also gives us a lot more options about how we build systems, how we organize ourselves, rather than having to bend our working practices to what the big giant shared database lets us do. If we can find ways to break that data apart, we open up a whole world of possibilities about how we solve these problems. So great, split data apart, it's all going to be fine, right? Well, there are some caveats here. There are some pain points that are caused by this whole approach. It's not to the issue of detangling your data, it's also the things we miss by not having that big giant database. But this of course is where companies all over the world are making use of Kafka. Kafka allows us, cause we've probably all heard before to kind of create this sort of central nervous system or backbone that runs throughout our organization. We can have processes that are emitting events onto this stream, consuming events from other locations. This becomes the way we communicate information across our system, and Kafka is gonna do that for us. In a, you know, a consistent throughput mechanism is gonna do that with a degree of ability to grow and scale. And it gives us a bit of sort of a shared place where this data can be shared and interchanged with applications written in different programming languages. And, you know, hopefully we can all just get along. Isn't that a wonderful idea? Like that's the promise, but is it that enough is just sending events around that we need to kind of solve this problem. When we think about sort of the simplest way you might use something like Kafka with a microservice, we think about this in terms of safety of venture of an architecture. I'm making a change to a microservice and as a result of some functionality that's been used inside that microservice, I emit some type of events. So in this case here I've got, I've emitted an order updated event. Events are useful things to us, they are an indication that something happened at a given point in time, right? So the time at which this event happened, that can be useful for all sorts of purposes. We have information about the change that occurred. I then basically threw that event out, you know, kind of metaphorically at this point I don't even know where that event is going. I publish it on a topic and it's up to whoever subscribes that topic to do things with that information. And of course there's a micro(indistinct) service I'm also free to consume events that are sent by other microservices. And so for sort of asynchronous event driven communication, if that's as far as you go, Kafka is a fantastic tool for that. But it can also be help us go further in terms of dealing with data, because it's not just a broker, you could use Kafka like that and it's great, but it's not really that different to some other types of sort of message brokers that you may have used. So Kafka has a whole lot of properties in that it really differentiates itself from other brokers that help us a lot when it comes to dealing with the tyranny of data. With traditional brokers, we typically think about the things that we send as being ephemeral. We find an event, we send a message, whatever. And once that message has been consumed by the downstream party, it's gone, it's no longer there. With Kafka of course we can configure how long an event that we send over at least for. This gives us the possibility of persisting that event forever. And that can be very useful. Think about that property of permanence can be useful because it allows new processes and applications to come on board, to make use of events that were previously broadcast. I may have been emitting order updated events for days, weeks, months, years, and now maybe on a rollout a new microservice that is gonna recommend things to me based on my past purchases. Well, if my stream of events is already available and it's already been, you know stored and made permanent within the kind of Kafka ecosystem, a new microservices spun up is able to ingest those messages, ingest those events and start recommending things to me. Whereas if these events were more ephemeral in nature, I would have to get those events rebroadcast in order for that recommendation engine to start doing its job. So that property of permanence is a really key idea in terms of shifting our thought processes away from event driven interactions, being more of a transient firmly in nature, and shifting more towards the thought processes where events are now maybe a bit more permanent and therefore could solve some of the issues of how we get data and move that data around. Now, event stream as an append-only set of records. This is again, just with nothing else is useful. There are lots of situations in which having an immutable append-only log is kind of fantastic. Think about financial ventures, for example. Having this, this is what has happened, this is what happened at this point. This is exactly what was done. If you go no further already for some processes and operations that you might need to implement, Kafka has given you a whole list of out of the box. We can go further though, right? But let's pause for a moment. Isn't sharing events, just like sharing a database? If I'm saying sharing events and I'm allowing lots of processes to combine and access those events. Isn't that the same thing as just saying everybody talked to my database? Well, it's different in a couple of ways. Firstly, with an event you've made a conscious decision to emit it. You said, this is something that I'm happy to expose to the outside world, and hopefully some conscious thought has gone into that. When we allow people on the other hand just access a database directly. We don't really think too much about it, we just say access the database. And once they access that database, they access the whole database, right? Every table, every relationship is being accessed potentially by an external consumer. When I emit an event on the other hand, it's just that one event that I'm having to reason about. It's much more smaller in scope, and therefore it's much easier to maintain compatibility of that event. But fundamentally the fact that you've decided to send that event out to the wider world, that conscious decision to send it out makes it part of your external interface, in the same way a REST API would. And this is significantly better than just sharing a big giant database in that point of view. But there are other problems. If we're sort of having data broken apart, we're sharing these events. There are other things that we get from a relational database, so we've kind of lost. What about joining data across lots of such of data or ad hoc queries? I might wanna pull some, do like a bit of ad hoc reporting I can't do this if we're firing events from microservices. And again, of course we can solve this problem with Kafka. If you've got your stream of events being phone across your enterprise, or you know, this lovely river of information to which you can dip in and grab whatever event you might need. Well, there's no reason why you can't just funnel that to a big old database. I think the issue is, and people say, well, isn't this back to what we had before, a big giant database? The issue here is that we often have built an application around a database and that database grows and grows and grows. And it becomes all things to all people. If what we want is a database dedicated for reporting, for doing things like ad hoc querying, we can design the schema. We can pick the technology that's fit for that purpose, picking the right tool for the job. We can then use Kafka to bring all of that information from all different systems and funnel it into the location we need. This pattern called the reporting database. So the idea here is that rather than sort of tying your reporting workflows to re reading data from inside an application, instead you have a dedicated location where that data is made available to you. And this is a really useful model. We decoupling how an application might store data for sort of online transactional processing reasons. And instead we're saying, no, no, this is the world where you do your ad hoc queries and you do your joints. You pick the right technology, you can attach Tableau or click or whatever else it is to those systems and the workflows, really for people saying data warehousing analytics don't necessarily need to change. We've made an architectural change by breaking our data apart, by breaking our workloads apart. But Kafka can still ensure we can carry on operating with workflows that are familiar to us and, you know, Kafka and you know, associated towards help. Now, of course we can do things like, I'm just gonna follow an event and then have something to read that event and writes it into a database, and sure I could do that, but you know, we could go a bit further. I am sure, you know, some of you have heard of Debezium, Debezium is fantastic. I don't know if I'm pronouncing it correctly, but that's how I say it, Debezium is a really useful opensource tool that makes this whole process much easier. What Debezium can do, it works like a change data capture tool. So change data capture processes work by looking at changes made in a database, specifically, look at the commit log. Whenever you make a change in state, in a database it's actually appended to a commit log tools like Debezium look at that commit log when a new entry appears to trigger some behavior. Debezium runs as a Kafka connector. What it does is when you write into your database, you can configure which part of that database it cares about it can pick up that data and send it somewhere else. So this becomes a very good way of getting data out of your microservice and putting into say an analytical system. This can also be incredibly useful if what you're trying to do is actually pull data in from maybe other legacy systems that can't be changed as easily. And there's no surprise that Debezium is so widely used for this purpose, 'cause it is fantastic at that. So we broken data apart, Debezium and Kafka together can help us bring that back together again. So that's cool, but it feels like we're still kind of just recreating (indistinct) from a reporting point of view, a bit of an old fashioned world. And there's something wrong with that, there's some fantastic things that we can do with kind of reporting databases and data warehousing. But you know, we were promised jet packs. So is there something else that we can do in a situation? And this is to me is what's gotten me really excited over the last few years has been seeing the development of Ksql, now what we call KsqlDB. So KsqlDB allows us to kind of create sequel-like statements that allow us to do stream processing either in Kafka clients or in a dedicated KsqlDB server. So what I can do is say, okay, I've got my order service, I'm emitting these events, these order updated events, right? So I'm gonna say the orders are being placed. And so I could create a push query that basically is like a constantly updating query. So I can define that query using sequel like syntax, I can then effectively have that dynamically updating, so I can subscribe to the results of that query. As these new events appear on that stream, this is gonna automatically update and give me a running total telling me how much money we've made today. And that's kind of fantastic, right? So rather than having to write a dedicated program that reads the stream and updates accountant and emits account, I can just take a query, run it on KsqlDB, subscribe to the results. Really, really simple, it's a great idea. We can no further, of course, this (indistinct) where things start getting interesting is when you start crossing the streams, right, rather than running a query on just one topic, right? What happens if I wanna start putting information together from multiple different locations? Maybe what I wanna do is say, well, I've got all these orders coming in, but I want to see how much money we're making on a genre by genre basis. You know, we're selling Pop CDs, Rock CDs, Hoover core CDs. What segment of our market are we making the most money in? Now, the order placed events don't contain the information we need. They just say, this is how much this order cost. We need information from another source. And the catalog service is where all the information about the albums that we sell, the CDs that we sell, we're a cutting edge organization over here in the UK. All of that information is coming in those events. So we can define a push query that pulls in information from the genre associated with a compact disc. And when that gets sold, we're able to associate the sale to the right genre. So we can now have a lovely updating table showing us that Rock is doing okay, Hip Hop is doing even better, but our biggest seller is obviously as it always should be Death Polka. We sold over two and a half thousand dollars worth of Death Polka today, which I think is a good day's business. So these push queries are fantastic. They're like kind of sequel statements and selects on steroids because they are dynamic, they continually update. And for sort of asynchronous workflows, they're really great and makes sort of, if you're already in that asynchronous event-driven world, they fit that mindset so well. But of course we often have different kinds of interactions we want. We wanna be able to ask the top questions, like, you know, what's the state of the given thing? How many orders did Sam place today or what was Sam's most recent order? And before that was a bit difficult when all we had were these sort of push queries, but now we've also got pull query. So I can ask questions, like, what was the most recent order by Sam? And this is moving me more to a world where it just looks like a traditional select statement to an extent. The point of view of the application, making that request it's like I'm talking to a database. It doesn't feel fancy in a way. So we've kind of started to make Kafka be real time. It can bring data together from different places. I've got these amazing sort of streaming updating queries. But if all I want is to use it kind of like a normal relational database, guess what? I can kind of do that too. So this for me is what's so exciting about the tuning in this space, right? We're moving from a world where we had to work for our data. And now we're kind of moving to a world where we've recognized that if we can break that data apart, it gives us all these great benefits in terms of independent working, but all these challenges too. And some of the stuff in this ecosystem and some of the advances that have been happening in Ksql and KsqlDB, they just make me feel like, okay, we've actually kind of come up with some solutions to that whole problem. We can have our cake and eat it. The data is now working for us more than us working for the data. So I'm not gonna lie to you and say this is gonna necessarily be an easy journey. I think going from a spanning start to we're all in on KsqlDB, that's a lot to bite off, but I think you can see your sort of a gradual progression of ideas here. Just using something like Kafka initially to just move data around, putting data into your data warehouse, for example, getting yourself familiar with that ecosystem. Then you start doing a little bit of push query stuff here and there. And then eventually you might find that you can push all of that work into the KsqlDB ecosystem. So there's a lot of ideas here and thinking about, well, how do I get my head around this? You know, are there other resources that I can look at to get to think about these ideas. You know, shifting from database thinking to thinking in terms of events, a great resource it would be is the book "Designing Event-Driven Systems" by Ben Stopford. It's a very short read, Ben of course has been at conference for a very long time and this book is freely available. And it's a really, really good read. I have to say that because I wrote the Forward. The nice thing about the book is that Ben sort of takes you through these ideas a bit at a time. And just like with kind of the Kafka, you can just use Kafka for the sort of simpler pieces first and these ideas grow with you and Ben takes you on that journey. So it's a great, great read, and I do recommend reading. As I said it's not a long read either. It's really a nice, concise overview of some of these ideas. The other thing that can be really challenging in this ecosystem is the process of breaking your database apart in the first place. I cover that in a lot of detail in my book, "Monolith to Microservices", specifically chapter four, talk about all horrible things, like what happens when you have to break a joint relationship and these sorts of things. But then for more information about the work that I do. I do training advice and consultancy services for companies all over the world. You can find information on my website, you can also find information there about what's happening with the second edition of "Building Microservices", which is a work in progress. I'm hoping it's going to be released towards the middle of next year. I really hope you've enjoyed the summit so far. And I hope you enjoy the rest of the time here as well. And thanks so much for watching. (upbeat music) 