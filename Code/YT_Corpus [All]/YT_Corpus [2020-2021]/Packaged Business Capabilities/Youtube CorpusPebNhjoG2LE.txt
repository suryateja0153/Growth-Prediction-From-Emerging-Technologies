 [Music] hi i'm steve behebe welcome you know thank you so much for joining us today in our first of a series of cloud platform summits which are scheduled throughout the year we'll be bringing you really relevant topics and use cases that are especially pertinent uh during this time today we'll be talking about how we build a modern data warehouse and we also have some amazing customer discussions ahead now you know first i wanted to start with the context of today you know to best serve our customers we're focused and committed to delivering the right outcomes for them for you in every way it's about supporting and enabling this today our customers are reimagining and reinventing their businesses to lead them into a successful future now when we look at areas that are top of mind they include addressing the economic impact how do we help save costs how do we make sure we have a dynamic environment where we can only pay for what we use and maybe shut it off we're not using it ensuring business continuity and business resiliency is obviously key engaging with employees and customers in new way with the rapidly changing dynamics of most marketplaces how do we stay on top of that and part of that is being able to rapidly develop deploy solutions to market whether that's customer engagement whether that's business practices uh whether that's moving from a a physical mode to a virtual mode like we're coming to you today really helping to accelerate digital transformation and finally and importantly and specific to the topic today it's about informing decisions through predictive insights and again this will be the core topic for today now next one of the foundational innovations in core to our data management and data warehouse offering is the oracle autonomous database it is the world's only self-driving self-securing and self-repairing database by self-driving we mean it automatically provisions configures tunes and scales from a self-securing perspective it automatically patches encrypts monitors and remediates uh so if it discovers a security action it can automatically help remediate a specific potential breach and then finally from itself for a repairing perspective we offer the same level of slas um and the lowest amount of downtime across any cloud provider we realize that as you move to cloud you cannot compromise and so we've done just that and more with the autonomous so next when you look at autonomous database it comes in really two key features or two services that are available the first is the autonomous data warehouse which provides the analytics in seconds it allows our customers to deploy net new or move existing data marks data lakes and data warehouses to the cloud and this will be our focus today the second is autonomous transaction processing which is designed for high performance transactional workload and that will be the topic for an upcoming event so now one of the other key areas of innovation that we've done is our focus on choice of deployment where we can offer unique access to cloud innovation wherever you choose so whether that's in our public cloud and that's autonomous database or our full suite of data warehouse solutions which we'll talk about we can also take our entire public cloud and bring that to customer prem so there are many reasons why customers can't move to cloud if it's data sovereignty data governance some sort of regulations uh latency issues or what have you highly sensitive data that can't leave the customer's premise premises we can offer a solution you can get exact copy of oracle's public cloud in your own data center and finally we also have a version autonomous database on exadata cloud customer where we can deliver just autonomous database capabilities to your data center and this level of choice um spans our entire data warehousing technology suite as well and that's what i wanted to cover next so when you look at modern data warehouse components there's sort of five key areas first is integration where i need to be able to collect data from multiple sources and those could be heterogeneous sources those could be multi-cloud sources that could be heterogeneous on primary cloud the important piece is how do i pull it all together in an easy mechanism and as we talk you need the power of the database that's self-driving self-sufficient and self-repairing you need to be able to support data lakes support advanced machine learning based analytics and finally data science uh that's also powered by machine learning and uh andy will go into much more detail in all of these areas but ultimately our different differentiators lie and in you know five key areas it's a complete integrated solution that meets all your data warehouse needs it's easy to start it's easy to operate and it's easy to analyze you we offer consistent high performance you do not need to compromise performance while moving to cloud and in fact we can give you more secure environment for your data warehousing and analytics with the comprehensive multi-layer security approach and finally as we discussed choice of deployment is key in terms of the benefits we provide to our customers now with respect to those customers um we have many companies around the world of every size representing every industry who are making better decisions and delivering insights faster leveraging our autonomous data warehouse technology whether that's spanning the likes of skye out front taylormade cern adventist health um vodafone unicomer and others and you're going to hear from three exciting customers today so with that the agenda for today we will have a keynote about how we can modernize without compromise building the modern data warehouse and that's going to be led by andy mendelsohn who's our executive vice president of database server technology for oracle that'll be followed by customer spotlights and finally a section on how we can get started and what those next steps would be so with that i would like to introduce uh andy mendelsohn andy thank you so much for taking the time to join us today it's great to be here steve okay so let's get started here so just to kick things off um i thought it'd be interesting to put up this quote from uh eric brynjolfsson of mit and it's pretty it's it's very true everybody has realized over the last few years that in almost every industry data is the the most important asset of the organization and customers are very focused on you know how can i leverage this data for business value and the problem is you know customers for many years have been trying to do this with some success but they've been running into a number of issues you know the biggest issue i hear from customers is that the data is all over the place they have data squirreled away in different lines of business locations eight central i.t in the cloud in object stores and for data analysts one of the biggest problems is just finding the data that's relevant for what the business tasks they're up to and and then being able to analyze it is is the next big step of course and because the data scatter all over the place data governance is really poor security is the weakest security of all these different data locations so security is a big problem um and because the data is so fragmented it's hard to analyze it you need to try to consolidate it into a place where you can analyze that data through complex etl mechanisms it's not something data analysts can do on their own they need a lot of help from lots of people in the organization to get to the data to integrate it to analyze it and finally you know there's been a lot of hype around machine learning and i think most enterprises i talk to in other organizations are very frustrated that it's they haven't really been successful in deploying machine learning algorithms to get lots of value from their data so at oracle we have been working with our customers for many years on premises to get value to their data via data marts and data warehouses we have tens of thousands of customers who are deploying our database technologies our data integration technologies as well and what we want to talk about here today is to help both our existing customers who are already using our data warehouse technologies to modernize those infrastructures very very easily using our cloud technologies and also we will welcome new customers who want to move from other platforms that are not been meeting their requirements very quickly to a much more modern much more effective data warehouse platform and to do that we're going to talk about four separate areas we're going to talk about number one our complete integrated solution for the modern data warehouse and then we'll go into a little more detail on how to implement that solution how you start how you operate how you make the data secure and finally how you you get value out of the data via analytics okay so we have a complete integrated solution let's move on um so this is a general architectural picture of what a modern data warehouse platform needs to look like of course on the left side you've got all these data sources for the data your analysts want to get to everything from package applications to third particle and third-party databases data can be on premises it can be in other clouds you have batch processing to get data into your data warehouse via etl you have events using kafka usually to move streaming data and you have tools like oracle goldengate to get change data capture from various data sources into your database and finally data can be in files also it could be in object stores and then you need a processing refinery that we call the data refinery to to take that data and transform it into a form that can be easily analyzed you move it into either object stores or persistent databases like oracle autonomous data warehouse you form data likes using data in object stores and and then finally you you apply the analytics and machine learning models to get value out of that data some of the key components of the oracle modern data warehouse are listed below here steve quickly went through them i'll go them through them a little more detail but they basically map to the picture we just saw you know the integration technologies are pulling the data out of the various data sources and oracle has very powerful data integration technologies that we've been using for with our customers for many years things like oracle data integrator golden gate for removing data from various sources into your data warehouse we also in our cloud of course have kafka service now as well for doing that um and then the the central point of our our data warehouse platform of course is the oracle autonomous data warehouse steve talked a little bit about that and we'll go into that more detail later in the presentation but this is not only the platform for doing high performance parallel querying of your data in the data warehouse it also can reach out transparently to data that's in your in the object stores that are forming the the data like if you're going in that direction as well and finally you know you get to the analytics side you can use traditional analytic tools like oracle analytic cloud or third-party tools like you know tableau click etc and then machine learning platforms for getting doing predictive analytics on your data the autonomous data warehouse is a very unique technology that oracle has brought to its cloud over the last couple years and the first bit of differentiation around autonomous data warehouse is that it is a converged database it's not just a simple uh traditional relational database they can look at structured data only you can look at all your data in a single data store you can have both traditional relational structure data you can have newer data types semi-structured data types like json xml you can have graph data you can have spatial data you can have multi-dimensional data and we have a powerful sql engine that lets you do queries across all all the different data all from one place okay no one else can do that this converged engine can do any kind of data management activities relevant for your data warehouse you can do powerful sql analytics you can do machine learning on the data right in the database itself we have algorithms we've been working on for 15 20 years that have all the popular modern machine learning algorithms we have in-memory columnar technology for doing very powerful very high performance analytics uh we can do in internet of things streaming uh blockchain right in the database itself you don't have to create go to another specialty database engine to do blockchain or any of these other technologies and finally this powerful sql engine not only works against querying data in the database itself but it can reach out to data that's in files in the object store so you can have single sql queries that join data in files and object stores in various formats like parfait um json etc with data that's in tables in in the autonomous data warehouse itself and finally the big advantage here is you know to get your job done in other clouds you have to use lots of different engines if you want to do things like graph and spatial blockchain json document analytics here everything is converged into one database so you can have a very simple method to query and analyze all your data and as steve mentioned earlier we have very unique deployment choices at oracle of course you can run autonomous data warehouse and our solution for the modern data warehouse in your in our public cloud data centers but you can also run the same technology in your data center either by having essentially a private cloud of autonomous data warehouse technologies in your data center itself or you can put our entire gen 2 public cloud infrastructure in a dedicated region dedicated just for your business okay so that's sort of a high level of our complete solution so let's get started now so what do you do it's to to provision a an autonomous data warehouse it's really easy you just go to your web browser you do a few clicks where you say how many what's your your best guess for how much compute and how much storage you're going to need you know give us a password and you press a button and in a couple minutes you get the world's most powerful database technology completely provisioned for you and then you go to the next screen and you can start loading data very simply via our online tools and then as you your needs get more complicated and you want to put together a more complex solution around autonomous data warehouse we offer you at the push of a button provisioning of much more complex infrastructures for dealing with departmental data warehouses or data marts enterprise data warehouses data warehouses that are integrated with data lakes on our object store doing machine learning and all these things can be provisioned at the push of a button and we provide you with collateral to help understand how to use these technologies as well so for those of you who are existing oracle data warehouse customers on premises and you're looking to modernize to a more modern cloud architecture going to oracle solution is the easiest by far tech way to go you don't have to make any changes to your existing etl scripts your bi tools all work the same security policies can work all you need to do is repoint the etl to our data warehouse on the cloud you take your tool existing analytic tools you re-point them to autonomous data warehouse and you're done that's it you know using any other kind of cloud technology to modernize your data warehouse is a much much more complex project and we have a quote here from a healthcare provider who who looked at the two it's just you know simple repoint versus a complete re-architecture of everything you've done rewrite all your sql queries all your etl scripts everything so for those of you who are existing oracle data work house customers it's sort of a no-brainer look to oracle's modern data warehouse solution those of you who are existing oracle fusion applications users using things like fusion erp fusion hcm we offer you a complete solution out of the box for your data warehouse no mess no fuss where we do you know complete etl for you from the data from your data sources in your fusion applications we do pre-packaged analytics for you and dashboards etc and so and then you can you know grow from there you can once you get what we give you you can say hey i need more data from other data sources and you can easily use autonomous data warehouse directly to add new data sources to add more queries etc so it's a fully extensible and customizable platform so again if you're a customer in this space it's the easiest way to go to create a modern data warehouse so let's look at the third area you know so once you've got your data warehouse provisioned what do you have to do well autonomous data warehouse is a very unique technology out there in the industry not only do we automate the provisioning and configuration of your data warehouse like all the other cloud vendors do we have unique tuning capabilities so that we will automatically do self-tuning so your data analysts effectively are dbas themselves they don't really need to know anything about the tuning they don't need another dba to come in and help tune the system for them we uniquely automate the security for them you know there's constant security patching needed in this world in this day and age and we do that all for you online with no downtime no one else can do that we uniquely do elastic scaling and i'll go into more detail what i mean by that as well and the the key goal here is to automate everything around managing the database so your analysts can basically do self-service they don't need you know the the dba anymore to provision and tune anything for them and it frees up the dbas and analysts and developers to do more innovation and create more value from the data which is of course what the business really really values um oracle autonomous data warehouse has very unique elastic scaling capabilities the other vendors talk about elasticity but it doesn't apply to their databases competing platforms like redshift and snowflake are have very traditional shape based provisioning technologies where you choose from one of a different various shapes so for example you might get a shape that has eight cpus and so much storage and memory and when that runs out then you go to their next biggest shape which might be 16 cpus and more storage in memory and when that runs out you have to go to even bigger shape that might be 32 cpus with oracle autonomous data warehouse if you were wrong with your initial provisioning and let's say eight cpus wasn't quite enough but 10 will do it no problem you just add a cpu you go back to the console all online you add a cpu and you're done no problem no mess no fuss no wasted cpus that you incur when you go to these shape based provisioning technologies the other thing we do that's really unique is sometimes people have spiky workloads you know normally 816 cpus is good enough but occasionally there's this batch workload that runs at the end of the day or the end of the quarter that needs maybe 2x or even 3x the compute workload to get the job done in a timely fashion with autonomous data warehouse auto scale you give us what you think is the baseline say eight cpu is what you need and then we will automatically scale up up to three times that baseline so if your baseline is eight we'll scale up to 24 cpus automatically for you on demand based on your workload no mess no fuss you don't have to do anything yourself and then we also let you know of course manually if you know your business is going to do some huge processing run at the end of the quarter that needs 40 cpus you can go in there on the console just rev things up from 8 cpus to 40 you know run temporarily with that and then go back down when you back when that that workload is over so the bottom line is elastic auto scaling drives down costs tremendously beyond what the other vendors are able to do um we you know we mentioned earlier we have automated a self-driving self-tuning database that's what autonomous database is all about uh we automatically do the the optimization for you by doing things like creating uh summary indexes for you uh we automatically take advantage of things like in memory kilometer technology for you we automatically paralyze all the queries select the best plans etc all done via artificial intelligence algorithms of various sorts including machine learning to give you the best performance and then the autonomous data warehouse for those of you already using oracle technology on-prem of course uses the exudated technology that you are probably using on-prem for your data warehouses we transparently are using it under the covers and taking advantage of all the great performance innovations that are in exadata all transparently to you so we have a customer here vodafone that i'm sure a lot of you have heard of they're a global telco provider and they were existing oracle data warehouse customer on premises and their example of a customer who was very able to very easily modernize their data warehouse by moving to our cloud in autonomous data warehouse and as part of that move they got six times better performance they're taking advantage of the auto scaling capability i mentioned earlier to automatically grow their warehouse and at very very low cost another unique capability of autonomous data warehouse of course is that it's based on the oracle database security technology this is by far the most mature powerful security for data data management in the industry um and we automate everything that's possible to automate automatically so for example um encryption of course all the data is automatically encryption encrypted you don't have to do anything take advantage of that we have a technology called database vault that makes sure that only you can see the data in your data warehouse people from oracle are completely you know locked out from that and then as i mentioned earlier you know if there are some security updates that we need to apply for you we do that all online with no downtime again a very unique capability and finally we have a tool called data save and data safe is a tool that lets you do things that we can't do automatically so for example you have to manage the users of your data warehouse and the privileges you give them and then we will do risk assessments so we'll actually look at what access in the real actual data where else your users are doing you know what queries are they performing what tables are they looking at and we make sure you haven't given them privileges to do more than they really need we do automatically auditing of all the data for you and put it into a central repository and let you run analytics against the audit trails and we also automatically will help you find out that you might have some sensitive data that you didn't know about and you might want to mask that data when it goes into a test environment and finally all this powerful security that's in the data warehouse database can be extended transparently out to data that's in files in your object store so you can apply all the same powerful oracle database security mechanisms to data in your object store okay finally you've got all this data now you want to analyze it just as a free part of the service we provide some visualization tools for you and we provide a very powerful tool called oracle application express or apex that lets your data analysts who don't want to write a lot of code build simple applications really fast at really high productivity so they again can can be self-sufficient you want your data analysts not to need anybody to provision the database for it for them to tune it for them and finally to write code for them they can do everything themselves with oracle's modern data warehouse platform oracle of course on our cloud provides a very powerful analytics tool called oracle analytics cloud for those of you who are using oracle bie on premise it there's a very simple path from bie to oracle analytics cloud which is basically a big superset of what you were using on premises and it's a modern tool that does everything from you know traditional governed analytics to self-service high powerful visualization of all your data using machine learning as well and then of course oracle knows that all of you used lots of different partners for your analytics for your etl and we work with all those partners to certify that their technologies work well with our modern autonomous data warehouse platform and finally there are the data scientists out there and for you we have two choices a lot of data scientists like taking data you know out of their databases or files and and working on them in a separate repository using open source machine learning algorithms to build their data models we have oracle cloud infrastructure data science for doing that sort of thing and for those of you who'd want more simplicity and you want to build machine learning algorithms on data that's right in the autonomous data warehouse itself or in the object stores that are accessible at from the data warehouse platform we have machine learning algorithms that we've been working on for years that are built right into the autonomous data warehouse itself so without any mess or fuss no etl you can build very powerful machine learning models that you can then deploy you know at your pleasure where where they're needed and finally both of these data science platforms support auto ml so this is a very sophisticated technology that we've built in our research labs at oracle that lets any data analysts become a sophisticated data scientist you just show us your example data set you show us the data and the various features we you want us to use to to do the predictive analytics you'd show us the result of of for each row essentially in a table that what you want the prediction to be and we will then go through our catalog of machine learning algorithms and figure out which is the best algorithm for you and if there's more than one possible we'll give you a couple of choices and then you can just pick out the machine learning how model you you want to use and go from there very very easy every data analyst in your organization can now do what before only sophisticated data scientists could do and of course as i mentioned earlier autonomous data warehouse has various unique technologies that deliver very high performance using very few uh compute cpus and therefore deliver very low cost because on the cloud you just pay for what you use and so when you compare autonomous data warehouse versus aws redshift or versus snowflake you will see that we use far less compute to get the work done and therefore save you money plus i mentioned earlier we have our fully elastic auto scaling capability aws and snowflake are first generation shape based provisioning technologies they they require you to pay for lots of unused cpu we have auto scaling so you pay for just the cpus you need and we'll automatically scale up just for a few hours or a few minutes to get your job done and no one else can do that they require you to move to these bigger and bigger shapes with lots of wasted compute so we okay um just as a final thing i mentioned i talked a lot about you know how we have lots of existing data warehouse customers to can easily modernize their infrastructure by moving to our autonomous data warehouse and other modern cloud data warehouse platform technologies but we also have customers moving from other places um so for example here we have ripley's a leading retailer credit card bank they were using aws redshift they moved to autonomous data warehouse they lowered their cost 25 by using the techniques we mentioned earlier they're getting better performance on fewer compute and having a great time using oracle's modern data warehouse platform and just to summarize you know what are the unique differentiators of oracle's modern data warehouse cloud platform versus what you get on aws and from snowflake number one we are using a converged database technology all your data types from structure to graph to you know json to xml can be analyzed with a powerful converged sql engine in one place you don't have to fragment your data across lots of different specialty databases to be able to analyze your data we have the world's most powerful data management security um that lets you secure your data both in the oracle database itself but also it can be extended out to data that's in files in your object store we have of course self-drive self-driving database very unique self-tuning platform no one else has that we talked a lot about our truly elastic auto scaling capability no one else has that and finally oracle uniquely lets you deploy our cloud technologies both in our public cloud and in your data center as well you know amazon snowflake can't do that either so just to summarize we talked about oracle's modern data warehouse platform gives you a complete integrated solution for modernizing your data warehouse technologies and we went through the different steps of how easy it is to start how easy it is to load data operate and secure your modern data warehouse and then we talked a lot about our powerful analytics and machine learning technologies both from oracle and from third parties that integrate with our our platform and with that let's move back to steve dahid great andy thank you so much you know what a excellent overview great insight and what a great way to kick off this series of virtual events thank you so much now i think we get to my favorite part you know i'm getting my popcorn ready it's time for our customer spotlight we will showcase uh different use cases spanning departmental enterprise and data lake use cases so please um welcome our customers hey thanks steve everybody scott wiesner senior manager product management so look throughout our history our purpose has always been to help our customers with better data management solutions because whether it's optimizing ad sales improving health care or unlocking nature's secrets your ability to collect analyze and secure data is your key competitive advantage so in this segment we're really honored and i'm actually really excited too to have three customers who are leveraging our platform to contain costs power innovation and then of course mitigate risk that's a big one right so this is a live event but our first guest was actually very successful so he was actually able to go on vacation this week we previously recorded my interview with derrick hayden a couple weeks ago let's play that video we are joined by derek hayden who's the senior vice president of data strategy analytics for out front media derek welcome to the summit thanks scott thank you for inviting me absolutely thanks for spending a little bit of time with us today so can you tell us a little bit about outfront media sure outfront media is one of the largest out of home media advertising companies in north america we have over 500 000 advertising displays in the us and canada and we deliver brand impressions on behalf of our advertisers to their customers through a combination of technology location and creative services so we think we have a really nice asset portfolio and hopefully when you guys are out and about whether it be driving around or on transit uh you'll see some of our assets in front of you i've definitely seen the billboards uh love to see the logo and again thanks for having thanks for coming on the event here today so let's go back in time a little bit um what was the situation about six years ago before you had your current system in place kind of give us go back in time a little bit tell us kind of what was going on at that time sure about six years ago we were a division of cbs corporation so um and we were certainly not one of the larger ones and so our charge was to make our revenue numbers and keep our expenses down and that was you know essentially what we focus on um we were spun off as a as our own company and all of a sudden um you know see we had a c-level suite that was asking real business questions um we had investors that we had to uh give answers to and a lot of our information even though we were uh fortunate to bring all that data with us i think that was very fortunate as a new company to have 15 20 years worth of data at our disposal um everything was very siloed it was uh it was really hard to get consistent answers um the it group at the time was constantly running you know running around trying to give answers um we weren't cross-functional in that at that time and then they would have kind of savvy users uh on sales and finance teams who would you know get data from different sources and aggregate it uh and it became it became gospel so to speak but people weren't speaking the same language necessarily they weren't looking at things the same way and it became problematic to say that you know the answer that people were giving was a governed answer that we all trusted so putting down the context a little bit so spin-off you know you i think you recognize at that point that the there was a bunch of siloed information people were kind of left to their own devices there was no governed access uh no consistency i would imagine um you know what what was what was the kind of the mindset there because i think you stepped in i don't want to cover that you realize right away that this was not sustainable right what happened next yeah we were we were in a meeting with the cfo and the sales strategy group and we were kind of kicking around you know how we could uh get information to people faster the sales strategy group had started creating an access database and they were spending a lot of their time wrangling data manipulating data and then distributing data so um you know the cfo kind of said you know we need to we need you guys to work on strategy you know not data and um we didn't have a date we didn't when we left cbs there was no analytics practice or platform for us so he kind of said we need something like a warehouse or something some sort of david warehouse uh that was his that was his uh technical term and so um you know i kind of raised my hand said you know i'd be willing to take that on um i was kind of on the finance and real estate application side um we were we were in oracle technologies my group um and i said you know let's i'll take that on because i just felt like as a new company our services as an ebs support group could be easily outsourced so i kind of saw it as an opportunity to maybe make us more attractive long term as as resources so the cfo recognized that okay i know the power of data i know the value of that data but we have the sales people trying to wrangle that data that's not what we hired them to do we needed them to be selling so you volunteered good for you you weighed right into this and said you know i i can actually set up a data warehouse i got some background here so so so that's kind of the context i think where you started and i would imagine our other customers have a similar situation even whether it's a spin-off or a line of business or we all face the same things where we want to be data providers to the business and let them do what they need to do versus having them to do everything so kind of what happened next after you ever after there was a realization from your cfo of all places the data warehouse well i mean ironically when we were spun off we actually didn't have a data center to go along with us so he said we want this warehouse and like to do something and you know analytics and we want to do it quickly so we evaluated a bunch of solutions and especially at the time that work will have had cloud database and vi cloud service um it seemed and we had oracle tech technology uh expertise as well so it seemed like the best course to success was was to adopt a cloud-first platform oracle technologies and you know kind of address the sales groups needs over a period of time so we kind of laid out a plan of about a three year plan to onboard 700 something users for the sales organization uh and everyone agreed on it and then we kind of got into it and we got the five or six key measurements from the sales group that they wanted to be distributed and not only within three months not only did we have those but we had another 10 and we had about 250 users on right away and then the finance and real estate teams were kind of hearing about this and started to you know peer over the shoulder like what are you guys doing over there hey we want in on that so uh we really we fast tracked a lot of this stuff in the first year we had everybody up you know that we had planned on for three years and we were covering three lines of business so to speak wow from my group's support standpoint wow nice job i'm glad you volunteered for that that role um so so so real quick um so i think there's a critical part and we'll come back to kind of what we lessons learned uh towards the end here but you started with the sales team which was good and you built that plan so it sounds like the first thing was let's build data warehouse um let's provide dashboards for the sales team um there's a little bit of destruction there right on looking at right around 2017 what kind of what happened there yeah we were the in that 2016 time period we were feeling really good about ourselves and delivering as quickly as we could possibly get requests in and you know we were adding value and i feel like we're streamlining you know the reporting side and really starting to introduce visualization and analytics as a as a concept in the organization as a technology team we kind of got you know caught up in in all the accolades and things kind of went haywire so we started running into issues with space in our for our data we had to cloud database we had to pick you know a economical cpu range to stay in and so we were you know we kind of got ourselves turned around so we just started prioritizing what do we really need to do we had to pump the brakes in 2017 and kind of completely rewrite our warehouse so you know that helped us address some space issues it helped us address performance but it took about six months out of our production of new content so to speak um and really you know kind of sidelined us for that period of time but then we kind of got right back into it you know we did an analytics assessment we wanted to make sure we were on the right track and we didn't want to get into that that space again and that's about when adw kind of materialized in the oracle portfolio so we started poking around with adw and it addressed a lot of the things that we were struggling with we did the universal credit conversion uh kickoff 2019 which kind of opened up the doors to all sorts of new tools it allowed us to address some third-party data that we've never really bought into our warehouse environment and uh we brought that in through adw and then that's a whole nother interesting story somewhere um that you know we were able to turn to turn around the media spend analytics in like three days literally um at least you know get it out back out into the field in like three days um so and then db came out on oac and odi and spatial studio so we've really kind of opened up our world now by by uh kind of sticking to the oracle cloud platform wow thank you so uh interesting so recognize the need for kind of centralized uh data management if you will and then you realize okay we probably better off in the cloud start doing things manually and then here's this autonomous thing which seems to be the right thing to do so so let's actually double click into your architecture a little bit um tell walk us through kind of what the data sources are i'm sure our audience wants to know kind of what does this look like maybe at high level what your data flow looks like and kind of go from a service level description here yeah so it's probably not that unfamiliar to people we have what we consider on-premise data sources which would be oracle ebs we have a db2 i mean they're both act they're actually in separate hosted data centers they're actually not anywhere near each other um we have our infamous access database that is still somewhat alive i'm trying to kill that off but um and of course excel always creeps in as a source or or probably more recently smartsheet my new favorite thing and then we have some cloud sources uh we've worked with cloud sources we've got boosters a is a media centric crm tool that we use we have some ibm cloud uh kicking out there and then this third-party media spin that we consume right now is is another huge source so like our main sources of data there on the left and it's kind of a hybrid you know i mean it's kind of coming from every direction yeah um so we as a again as a technology team we kind of wrangle it you know we use odi little pl sql tricks things like that but um you know we're really looking to to odi um as our i call it data orchestration method methodology and then our warehouse is you know adw and there's tools off of that which we actually use apex and spatial studio and um so that's been great and then the visualization side is oac perfect not uncommon i'm sure a lot of folks in our audience have a lot of different data sources and probably want to add even more data sources there's just a a hunger for all the different data from from various sources so tell us a little bit about uh why you chose oracle for this uh for this data warehouse you're basically implementing something new in the cloud and what what do we get for you yeah i mean it was it was we were an interesting position because we didn't have any preconceived notions of what a warehouse or an analytics delivery should be we went through a very long practical evaluation of technologies at the time and again we actually had a partner come and they recommended microsoft on our behalf and i i said if i'm going to own this i i don't believe that that's where we should be um you know we had oracle technologists we you know we've had a long relationship with oracle and oracle technologies um you know my team was uh ready to take the challenge you know they were ready to move into a different role in the company and you know we were able to kind of repurpose existing resources into the analytics space so it really didn't cost anything uh head count wise to take this on in the oracle platform um so it kind of was a no-brainer there you know the cloud was perfect for us it let us kind of get everything going quickly and i think the time to delivery from when we you know kind of spun things up to actually start delivering was was so quick um that you know there was no chance for second guessing um there was you know we saw immediate success and immediate value and really we never looked back since that time i mean it's been six years and you know the portfolio that oracle has put out in the in the cloud analytics space is really impressive and i've seen it grow i mean there's been changes to how you you know get to it but i mean right now i mean it's it's a really powerful um cloud offering um and we do everything through perfect yeah i think you think first of all thank you for uh sticking with us i do believe we have a lot to offer and you're proof of that and we'll hear later from some other folks as well tell us a little bit about um what your experience let's talk about just the data warehouse itself as you move from kind of doing things manually to leveraging the power of autonomous data warehouse tell us a little about your experience there yeah so i mean we were you know building the warehouse out in cloud database and that was great um obviously it's foundational to our uh our success story is is how quickly we can we created the warehouse yeah three years instead of three years fifteen months uh yeah that's the market that's pretty amazing but you know again my we didn't have a data we didn't have a data center we didn't bring any dva's with us when we when we were spun off so um know we had true technologists um and i really wanted them concentrating on data um and so in order for us to you know keep this going you know not only do we have the data warehouse relay but we had to worry about space issues um you know backups you know patching security we were limited or hampered i would say by we weren't able to scale up and back on the cpu side you know we had to commit if we wanted to move to that next level so you know we really i mean those first three years we were feeling our way around and we got we had such great success the the last part of it was trying to get my technology team to focus on data and partnering with the business um because that was the most powerful part of our offering um was being a partner and i didn't need them to spend you know 30 of their time dealing with the infrastructure even on cloud you know even on just the technology so that's really the with the push obviously for adw you know we get self-patching self-repair it scales we don't have any uh you don't have any issues with storage so it kind of just you know allows us to have that much more time back on the technology side and again just keep producing and and when we partner with our our customers and say this you know now we're not even an i.t group we're a technology services group so we really you know try and partner and i just tell them like don't think there's any limitations just tell me what you need and and we'll figure out you know they don't necessarily want to care about the back and and this infrastructure here allows us to say yeah we can do that yeah yeah that's perfect you've never had to say no to them for anything oh that's nice yeah you definitely you know as it professionals we always want to be partners with the business and delivering what what they want you know yesterday instead of them trying to figure out on their own which is what happened initially um one of the things for you know certainly a sales team is you know they need to get out sales quotes like immediately and some of these things can be really uh complex so how does the platform and particularly the data warehouse and how does that serve that need can give us an example of you know kind of you know like the query performance basically yeah so there's a couple things there's there's one finance team is really interested in the sales directors in the revenue forecasting back in the in the timeline slide part of that data warehouse rewrite that we took was to really facilitate some of these revenue forecasting queries they were complicated and so when we first got to abw we said let's see what this thing can do we just we took the data we moved it over we didn't didn't pull any levers or anything we just started writing queries and so the first time we ran our revenue forecast inquiry uh it took about six minutes and i sat there i was like oh boy this is this is not good but the second time we were in it it took two seconds wow and then the more we ran the better the performance and you know to to be fair the funny i'm not a technology i'm not a coder or not a technologist per se you know my team would say well we can make it faster than that you know sub two closer to one like we can make it faster and i said yeah but we made it faster in about you know two minutes like we took we had to stop and take six months to do it um this is probably the right way to go i mean most users aren't going to notice a difference between you know a highly tuned um query and one that was basically auto-tuned um sure yeah so you know that was great and then the you know the corollary to that is this media spend data that we were bringing in from the third party was something that we weren't really familiar with so not only was it a lot of data for us to ingest but we didn't want to spend a lot of time modeling and indexing and all that so we just let it run and we kind of let people hit against adw just as they normally would against the data and so we really haven't really touched that data model and just let adw do the work for us wow that's amazing so six minutes two seconds i'd imagine it's getting faster and you're right business doesn't care they just want as fast as they can whether it's auto-tuned or manual but you look time-to-market's everything and so to speak you can do more with the short amount of time that that's a great story uh appreciate that so uh let's kind of uh get out of the architecture a little bit and tell us you know a little bit more about you know some of the outcomes you have you know where you're going and and i'm sure the audience wants to hear a little about hey along this journey what did you learn and how do we how do we help our peers to go even faster um understanding the mistakes that you know or deviations from your path that you had before so let's start with a little bit of that about the outcomes yeah i mean we've touched on them when we talk but you know just to kind of reiterate that we have we have a data warehouse without a data center oracle is our data center for for lack of a better word so it really allows us to focus we've consolidated our cross-platform reporting on into one place it's governed from the standpoint of you know people go there and it's trusted if it doesn't come from there there's definitely some at this point some some questioning um c level has embraced you know the data and the visualization um yeah i think i mentioned to chatting at one point that you know i can tell when our cro is up early or if he happens to be over in london because you know something doesn't look right to him it's right to my phone nice bright and early so they've definitely taken it uh adopted that it's also allowed us to kind of create a common vocabulary one of the things about the silent reporting was you know we still had systems calling things a sales phase versus a panel for the real estate group and so we kind of all come together and and there are things that now are just common we brought those that vocabulary together i think always had measurements um kind of out in the company but uh this allowed everyone to you know really focus on what they want to measure and the the kpis were you know everyone knew what they were and they knew where they could find the measurements and i think that was that was powerful because sometimes i think we would just lose sight of what was expected um and i think that was really helpful um there's something that we that our sales team likes to call visual performance management um and of course the data drives that but the visualization tool is the obviously the visual part of that but you know drives it drives some competition um helps look for trends and outliers um that we they may not have seen in a spreadsheet before so i think that's been really powerful you know we've been able to introduce third-party sources into the warehouse to help you know do things in the past we're really manual and maybe you know quarterly or you know every once in a while people get it one of the things here that we're working toward on the media spend side is is understanding you know how we can identify people who are spending media dollars but not necessarily in that home industry and can we go look at peers uh of certain companies that are more favorable to us and create a story as a media partner and say hey you know we see you spending in these areas you know can we show you that you know newspapers and magazines the circulation is way down maybe you should move some of your dollars you're not going to change your marketing budget but your dollars can move into the at home and we feel like we're more more effective in that space so really just have those conversations as a media partner and we can do that with the with the adw now better to provide data to the business than trying to wrangle that data and yeah a common language so let's just spend a couple minutes on lessons learned if you would what how can we help the audience kind of as on their journey yeah so i mean i think one of the things you know the big league of faith that we took was you know let's go with oracle um let's utilize existing uh expertise so i think that was really that was a good decision on our part to say look you know we know we can deliver in this environment with this technology staff that we have so it allowed us to rather than focusing on the infrastructure focused on the data right so we can bring it get it in there um get it modeled and and then use visualization and that really helped us become a business partner so before we were kind of the standoffish i.t group that you know just waited for things to bad things to happen or some fire drill requests from from a sea level we need data and you know we would all run around for hours upon hours and spend weekends wrangling things so you know by partnering we we don't get surprised by that as often as we used to not say we don't ever get surprised by a request but i think what it has done is you know the things that they need to know they need to know um and we've put it out there in a way that they can consume it relatively easily and what it's sparking is other questions outside of probably the normal you know just how i need this or i need that we're able to partner and i think by having the quick wins up front not trying to try to deliver an entire platform at once we were able to kind of organically ensure adoption and you know the sales organization adopted very quickly um and then the finance and real estate organizations came on and they were like hey we want that you know we see what you're doing um and we were able to really come up with some creative ways to uh not only you know give them something some data that they weren't getting before but we actually reduced their reliance on some kind of third party or internal applications that they were using and and we've shifted all that to the warehouse because the cloud scalable and economical you know we can we can try stuff i mean we can my team might i tell them we don't have to be perfect you know you have to i want them to go try fail or succeed and um i think you know we've especially in the last two years within opening up of the tool sets we've done a lot a lot of trying failing something doesn't work we can you know we can abandon it in the cloud and we can start over and they don't feel like i've burned cpus or i like bought a machine i didn't need that well so uh you know look thank you for uh uh working with us uh sounds like the partnership is is really having an impact where you can focus on data and a data provider and partnering with the business um we'll help you helping you along with that journey um again thanks for being with us for a few minutes on on your story and lessons learned and we're looking forward to greater things with out front media in the future thank you scott look forward to talking to you again thanks all right thanks derek sorry to cut you off there but let's let's step it up a little bit um i'd like to introduce kim jackson who is the vice president of business analytics for adventist health kim welcome to the program thank you for having me absolutely so uh let's get right to it can you tell us a little bit about adventist health and what's going on there sure absolutely adventist health is a faith-based non-profit integrated health system we're in california oregon and hawaii with over 80 communities we practice in the seventh day adventist space we service rural and urban communities primarily rural our mission is living god's love by inspiring health wholeness and hope 21 hospitals medical groups and health plans with 28 000 employees and 1200 physicians thanks for that i imagine this is really important given the current environment um now this is about data so and being data driven so walk us through real quick your values on being data driven absolutely as a as a faith-based organization our mission and values are extremely important to us and we live by these through b statements two in particular that reach out to me uh within the data world is to be a mission owner you'll see through the rest of my organization that we outsource most of our data strategy and the work that we do but to be a mission owner we need to understand your strategy you need to drive your organization so i take that to heart and we wanted to have a new direction for our organization and also we needed to treat data as an intellectual property and to do so we needed to again own our data use it appropriately and help drive our organization forward perfect yeah outside of your people your intellectual property data as andy put down the beginning is probably your most important asset so we're about you know a little over halfway through 2020. so what are your can you take us like a minute through going through your priorities and i'd like you to touch on uh data literacy if you would um you wrote an article um recently in uh cio periodical about data literacy and how that how that's one of your priorities you walk us through that just for a minute absolutely so i love to share what we're doing i know when i uh attend these events i want to see what my peers are doing in particular in these areas so i talked about how we outsource so one of our first goal was to in-source our analytic team and our leadership i was one of those decisions so i've just come on my anniversary i've been with the organization for about a year to do that also we had to groom our talent and increase the work that we do so that we were less reactive and more proactive so that we could help our organization data literacy is a core fundamental to the fundamental strategy towards that um so instead of us just being technical people we really need to understand our data domains in the healthcare space understanding the difference between charges and billings and our payers and what clinical documentation in pharmacy and laboratory health care data is super super wide with all the traditional business functions as well as a really large um clinical longitudinal record from the outpatient the clinic it can go on and on and on so you can imagine how our clinicians at the bedside our financial uh fos don't have time to learn all of these data structures that they need us to be experts so that we can give them the insights instead of them learning a super complicated data model in data literacy and having our our talent and our employees understand that so that they can deliver that service is essential for us to be successful sensing a theme of partnering with the business not just being i.t people so uh rewind a little bit um you said you've been with the with advanced health probably at health for about a year and what was the situation going to the next slide and what was the situation similar to what kind of derek was talking about out front what was going on um when you uh when you dropped it absolutely so not only um had we outsourced our analytical leadership but we had outsourced our platforms um our platform was end of life and we were with our vendor our emr electronic medical record vendor it happens to be cerner they supported and we had our data warehouse in their environment so it was into life it hadn't had patches in two years we we weren't able to own our data model we had a lot of problems with stability so we really needed to have a modern environment our new strategy our 2030 strategy isn't dependent on us being able to have technology to support a data model support as hospitals go from just being hospital-based to community-based we needed more data sets and not just hospital-centric we needed to understand the community as well so we needed a broader data sets and tools and to help us do that we had multiple data platforms multiple edw's and we aren't able to connect all of that data together to actually make it um a differential in the work that we do rising costs the cost that we were paying for this end of life edw was very very large and we needed to reduce that in fact we were able to reduce it by 60 um by moving so i'm real proud of that and we were also dependent on our vendors roadmap they had competing databases they wanted us to use their tools and they made decisions that wouldn't allow us to do what we were interested in doing in their same environment so we were really locking key um to their roadmap their desires not our desires as an organization so multiple data sources rising costs you were kind of relying on a single vendor there that was kind of owning you wanted to own your own destiny let's go to the next slide you probably had some goals when you when you dropped in on on how to improve that get to those 2020 objectives uh you can walk us through that a little bit absolutely we wanted a single source of truth so we wanted our finance our clinical our operating operational data in one spot so that we can see them all together we needed faster more stable processing i told you our environment went down a lot it needed to be up we needed to be a tier one application so that our users could use their data to make business decisions we needed to move to advanced analytics we weren't able to because we didn't have real-time data you can't make clinical uh decisions three days later you really need to be at point of care to impact in the work that we do which comments on this retrospective to concurrent analysis and we need better security we um hipaa and the consequences for breaches are really strict i know they are in other businesses as well but that's for hormone in the work that we do and we need to increase our data quality of course the more data sources you have we have hundreds of people touching every record um there's so many clinicians and we need to make sure that we can wrangle that data and to use it appropriately to make businesses sure so got some great objectives lots to do lots of goals why oracle i think everybody so um in many ways we're an oracle shop we use our finance our our hr applications many of our databases are oracle so it seemed like a natural fit to make sure also our third-party tools they're also partners with y'all so we really felt like platform our data was already on using these tools in format so it would be a natural um combination for us to work with also data security talked about how that's so important and everybody knows about all the the hipaa roles that are required y'all were one of the very few who are willing even to share that um risk so y'all are partnered uh in this endeavor and that was really important to you if we're going to partner with you share our all data with you then we need to make sure that you take it as seriously as we do and talked about on the cost model uh it's more flexible um so you all have a catalog we can use what we want when we get charged for the use that we have and that's nice in case we decide we want to pivot and go in a different direction so putting you on the spot a little bit you know other vendors go i'm not ready for you yet other vendors claim data security so was there something unique about what oracle's doing or that the other vendors weren't providing that kind of put us in that leading position so uh for example uh amazon microsoft was were unwilling to sign rba bka which would share the risk so that what that means is in the healthcare space for example i don't know how many people who aren't in on this space if you have a breach or charge 500 a day for every patient that you breach so if it switches if a switch is changed and we have a breach of our entire population that could be millions and millions of records which is a really lengthy um time and hours isn't enough for that type of risk we wanted to make sure that our partner understand how important that was and we're going to share that legal risk with us perfect well well thank you for the partnership we're certainly happy to partner you got to lock that key right because i don't want to pay that bill i don't think that's right it's a shared risk as well as keeping our patients safe sorry go ahead number one thing yeah i just look we're happy to partner on the technology side uh more than happy to his partner on the security we really feel like we're a business partner uh at this stage so um got it thank you again for being a customer um let's talk a little bit about um implementation um absolutely and i i think this is in contrast to your your last um conversation which it was a while ago so people who might be wanting to switch how long did it really take us and what did we do and of course you can never just do one thing at a time these timelines it's important it took us about we had two months to lift and shift meaning create an environment in the new space and move it we also had new content so evan to stealth is on an acquisition journey so we're buying additional hospitals and medical groups and so we had a big acquisition that we wanted to bring on so we did that work we also did additional ap um integration we had a really large emr that's enterprise medical record upgrade in fact that they changed their id structure um so you can imagine if you change your id structure the patient id structure how that would really affect um we went from two different um id structures into one and we had to manage both of them at the same time as we transitioned so we have 21 hospitals as we move every hospital from this 2id structure to one iv structure i know that's super in the weeds but that's just not moving that i have to move and maintain both at the same time so that made it complicated and our prior our vendor didn't have audit reconciliation i want to make sure that my source and target they match and i reconcile for better data quality so we added that functionality that took about five months meanwhile i told you i wanted to in-source my department so we're in sourcing accountability and ensuring that we have the proper skill set um to support our program in the long term fantastic so uh i did this with the other guys so let's take a look at uh your architecture if you will so where where where do we stand today and and uh not uncommon to have other logos in there and in fact we embrace that so you can walk us walk us through your uh architecture substance today absolutely what you might notice here there's a lot of holes right and i think that's okay so we are in the beginning of our journey and i think that's important i told you that i wanted to own my own destiny and some of these are the results of the fact that we had outsourced that strategy and so we want to make sure that as we move forward and own our strategy we fill these holes but it's just as important to know what you what you have is what you don't have as well in my opinion so you see we have these operational systems we are looking to add more streaming data for that community data that i spoke about we have moved um to the autonomous data warehouse that we spoke about our electronic medical record happens to be cerner we use informatica and r in various different ways um to be agile and as as we've tried new things for data science we've used a little bit of python and we have a very uh young mdm reference data and we've used one of your tools apex for a little bit of that and salesforce and we're and we're working to grow that to improve our data quality we have elation is on our roadmap this year to implement it and we have some self-service tools tableau business objects i want to expand that and we use oac a little bit to do that and the goal is part of this move to expand utilization in that space as well we missed the oac logo in there that's okay so i've got good news for you kim i couldn't find it it looked just look like oracle so that's right if you have one i'll add it to my slide i've got good news for you in a minute we're going to talk to manuel and he actually has the story on streaming and data lakes so we're going to help you in a minute here um and and certainly happy to partner with everybody else i think you saw some value in hey look we've got the breadth and complete integrated story absolutely i think you would agree with that however the reality of this situation is sometimes you got to move and improve and lift and shift existing things to take quicker advantage i think we see that with some of these so uh let's move right along into some of the outcomes um i think everybody's curious to know what actually happened what'd you get out of this sure uh here's some examples i'm a real data driven person i am the data gal right so here's some examples in particular so we use tableau a lot so here's some examples we started in our prior environment of some of our larger sessions by the way these are large ones we started at 23 minutes and we were able to reduce it down to 10-minute jobs and again a multi-session environment we were able to cut it a third as well you can see uh that's a substantial amount one of the largest ones we went to from 40 um 41 minutes to four minutes so those are pretty strong reductions in in the time it took to run our jobs yeah look time in in your industry is critical and i i don't take that lightly so uh thank you for looking at our solution um let's take a look at you know queer performance is one thing let's look at the administration part of the equation and derrick mentioned a little this and again we're gonna hear from manuel a little bit what are you seeing from an administration perspective so we had about a quarter uh fte doing performance tuning we've been able to reduce that to about 10 of one person and doing that work we also used to create uh materialized views as you see that took about a 300 hours to create them and a ton of storage that's no longer needed um so we can we don't have to do that activity anymore and what i mean about is we used to spend quite a bit of time indexing and we don't have to do that anymore um but i always write tbd because i'm the proof in the pudding girl so until you can ask me again and see if i think the performance is the same i'm hoping from your last presentation they seem to think it's getting better and better so i'm looking forward to i'm continuing to see those results fantastic i i i don't think you'll be uh unhappy um uh based on what i've seen with other customers as well so it's a journey totally get that so um let's wrap up with kind of going back to your objectives from from a few slides ago um what sort of outcomes that you see that this platform and investment in oracle and our partnership our business partnership is getting you yeah so i would say uh just to make uh it clear that this sets the foundation for many of these things right so single source of truth we'll be able to add more data and we're already doing that in our projects as we more and more we have still have more than one source of truth of course that's a journey you will always have more data as you've seen we've already seen faster environment we want to make sure we're on a long term that we're stable and we've been able to do that set the stage for us to do uh advanced analytics in particular right now where we've been doing is forecasting volume over cases if you can imagine that's really important to an organization such as mine so we've been working through there moving we're doing golden gate as we speak now so that we can have more concurrent and data analysis and move our to have faster data acquisition uh i'm happy and we've had improved data strategy which is super important um and we've been doing all this auditing to improve the data quality from sometimes in our old platform up to 10 variants can you believe that to an sla of six sigma so we really want to improve your digesting quality as well yeah one thing we missed and then we'll wrap here is um one of your objectives i believe was acquisition so um you had mentioned previously about the speed of acquiring companies companies um there are companies too but hospitals right in medical groups so if you can imagine right if if we purchase a hospital we need to understand their operational clinical space so we need to be able to acquire data just like any other organization very quickly so we can manage them so for us to be able to do that quickly uh really helps our organization and ensure that we can um service the community as we'd like to yeah i think you had said that uh previous to moving to this platform took a long time to acquire uh those data assets um and now uh give us an idea of what it is it not only did take a a while but we had to with our with our vendor partner they had to move on to our platform so we're cerners so if for example they were meta tech we had to move them from meditek to cerner now an emr transition electronic medical record can take one to two sometimes three years next most famous one can take four years sometimes so that's a long time not to have the data that you need so here we can take the data we want move it in the data warehouse and maybe do parallels bring them side to side because we may never decide to bring them into our emr i told you rural uh hospitals um it might not be worth the money to do so it may be appropriate or maybe they're in a different part of the state and they wouldn't have cross population of patients so you need to do it purposefully not just because um and to do so this gives us flexibility to make those decisions perfect yeah i i imagine that we talked about this uh off camera um that the reason you acquire some of these is to move them from the red to the black and you need to do that as quickly as possible so absolutely outcome of of faster acquisition kim thanks a lot for your time um thank you for the partnership and we're looking forward to some great things and and improved healthcare in those rural areas awesome thank you so much thanks again okay uh again thanks for that next up is manuel martine marquez who is a senior project leader at cern manuel bienvenidos welcome thank you very much scott i was all the way from europe and i had to say that no i have to thank you as well because you know it's nowadays maybe this is the only way out we have to meet such a number of people and especially from the certain perspective as well we need to communicate what we do and reach new audience is something that we really appreciate it so thank you very much for this so yeah i mean coming back to your question tell us a little bit about cern and uh and what what's going on there okay thanks so much is an international organization cern is the european laboratory for particle physics so the international organization is funded by the europe mostly by the european member states so by the collaboration it's much wider today we have institute for more than 110 different nationalities which is kind of the same number of united nation countries yeah that work together to achieve the same goal so and the goal is pretty simple which is do fundamental research which link with our woman dates our mandate is very simple which is trying to understand how the universe works and what it's made of simple to say hard to achieve yeah so what we're trying to do is to do fundamental research to try to understand you know which is the main building blocks that make everything yeah which make yourself myself the connection that we are using today uh and the largest structures of the universe of course planets stars galaxy clusters and something even more fancy could be mata antimatter black energy black matter all this kind of fancy stuff that we need to understand what they're made what are they made of in order to understand the smallest things in the universe we have to build some of the most some of the biggest scientific instruments the human being have ever built and most probably the most complex one yes the large hadron collider which is our flagship experiment is 27 kilometers ring which in mild i think is 16.7 miles and it's built 100 meters underground which is 300 feet more or less underground yeah i mean this is really complex machine where we accelerate particles to almost speed up lights and then we make them collide it's on a special point where we take some pictures out of this collision please understand that this is a simplification and and you know we try to understand what i said before we're trying to achieve our needs so a larger instrument investigating the smallest things in our universe really and i imagine lots of data too so tell us a little bit about your requirements and you know it's all about scale i would imagine yeah sure finn so i mean you know there is a few things that is important to say here yeah i mean one of my let's say that our main mandate i mean from martin i work with myself is to make all the data that we are collecting from experiments all the data that you know is somehow important to understand all these mysteries of the universe and all the data that's what will come out from operations and make because this is quite challenging things we have to collect all this data we have to make sure that the data is collected properly we need to make sure that the data is stored and persisted sometimes forever this is something that is important to say what we do at cern is quite unique so i like to say that the data that we are collating sometimes is human heritage because there is no second chance to to get this data and you know somehow we never knows if we have to come back to the data that we are collecting today most probably we will hence our understanding of the universe change we have to rewrite the history of physics for instance yeah so yeah i mean you asked me about data volumes so yeah i mean when we are in operations we are collecting more than one petabyte per second so one petabyte two petabytes three petabytes and so on and so forth which is you know this is the physics data this is data that is interesting for for scientists to say that that got a nobel prize in 2012 with the i mean later on with the higgs boson discovery and all this kind of stuff so however there is something that is important to say here which is you know so even if you may think that you know this guy from cern one petabyte per second this is far away from what i'm doing in my daily base operations it's a small company or biggest one yeah maybe because i don't know that many i don't think that many organizations have the chance to run these large experiments or deal with such an amount of data yeah so but actually you have to understand that the background of the problem that you're dealing with is the same it's exactly the same i mean i've been doing any of us have been doing an investment on collecting data i mean now more than ever because the amount of data that we are collecting is increasing and increasing as well the complexity and so on and so forth so and we need to get profit out of this data it doesn't matter i mean for us it's understanding about the universe for you could be as independent and they said by vodafone that they wanted you to understand the behavior of some customers or whatever but actually it's profit that you want to get from the data so yeah i mean based on that i think i wanted to to get the face out to the reality to concrete in a concrete project that we are doing that i think is quite challenging quite interesting and i've been doing for the last few months with with part of my team so you know the nhc this large machine is composed of a large number of different sensors throwing millions of different signals and that's make one of the biggest iot systems in the world that i know so all this data is really essential for us a small problem in this data it's more probably happening in this in the system can take to learn or to some critical things for us i mean if we have to stop our operations because we have to cool down the theater or warm up that rate it will take months to recover so knowing what we are doing with this data is essentially critical so this scientist system just said million of different signals or whatever today is more than 1.1 petabyte of time of time series data and it's stored in an oracle database on premises one so what we wanted to do is to say hey guys i mean as we are speaking this data is increasing the complexity of our systems are increasing as well so we need to do something to understand better our operations to answer those questions that engineering teams and people an expert are asking us every day because they wanted to understand how the this you know we have to say that even if the biggest machine that they see this big machine is something that is our flagship experiment we have to understand this is still a prototype you know we are running a prototype of 27 kilometers in production yeah and that's bring a lot of challenges for us one of these is that you know the data is increasing and we need to understand better what is going on to reply to the demand of the people just asking new questions that were not there before yeah still saying but i don't think your budget was increasing and i wanted to go back to one thing you said earlier about the cost of downtime i want to make sure i heard that right if there's any sort of disruption it's months to recover yeah i mean we have to experience in the past when we had a little problem in one of our systems just take up two months to recover yeah i mean that was one of the biggest issues that we have in the past but i don't want you to focus on that sense you know for us this data is critical we wanted to do more with the same budget that we have it will be less because you know the situation with science is getting flat so we need to do something as well and this is very important to say to answer those questions that they were not able to give tools to be replied before so you know we are moving for an on-premises services to something better that can help us to increase i mean to cope with the amount of data but as well to have all these add-ons and things analytics much learning and all these kind of things that you can put on top of this yeah so but we have some some requirements that you asked me this is why i went through this lonely introduction so first one is what we wanted to keep the very simple architecture that we have before before we have an on-premise database so we then want to reinvent the wheel and go for something really complex because we don't have two resources to do that what we wanted to test is whether we can do the same with us the same simplicity of architecture we wanted to do the same i mean we wanted to improve do better without directing the things so this is where all these autonomous data warehouse with all the autonomous articles the notice came in i mean the that's com it's compatibility i mean what we were doing before is something the same thing that we can do it was pointed out by andy so in our case if we manage to move from if we manage to move from the privacy service to ew you know we can't improve but our clients or real times application and so on and so forth that query that data we just need to in the worst case change the connection stream so that somehow minimize the integration and iteration efforts so but there is something that is important to see here you know we wanted to because this was the main driven of moving to adw main requirement to move to dw which was unlocked data exploration you know previously with the current system that the system that we have it was pretty easy to get data i mean you write a sql query and then you get back some data as far as the data set that you are trying to find is reasonable as far as the time window that you're trying to achieve is fine so in short if you know what you're looking for that's a perfect system but if you wanted to explore the data to find out what is going on to understand better those prototype to get some values out of your data that you put there extra value i mean you need to offer this kind of flexible tool that allows your users your in our case our engineers team to do exploration to try to find these unknowns unknowns and then some day have today that they can just play with it to get somebody doing much learning artificial intelligence and all these kind of new techniques that they are really willing to do but they need the perfect environment to do that yeah and then we found edw so of course i can't talk about all the requirements like lowers operations and development costs but you know this is nothing new if you if you know a little bit w or autonomously so we have more data don't have as many you know the number of people to support that is not growing more data uh you know we want to provide uh all that capability to your your users and also keeping a control on cost so let's figure out what what was the architecture that you landed on for the cloud i mean you know we have to admit it so the first thing that we think is that you know a part of this simple a part of all the things that already mentioned so we need to be you know cost effective so we need to to you know maintain the cost that was really critical in our requirements from the beginning so for in that sense we we think like you know the first thing is that we need to use this order called autonomous knowledge but we need to be really smart so in that sense you know we have 1.1 petabyte of time series data that we need to move to the cloud and use cdw2 to query the data why don't we want it to or why don't we just be a little bit smarter and then instead of putting this data into the extra data use synopsis store i do the storage to that so and that was pretty i mean that was pretty new approach that all this flexibility all these new technologies or how it was called during the this event modern data warehouse allows you is to have all these flexibility as new things and then fit with the one that works for you so what we did is take all this data coming from the control system use kafka this is another thing you know it's an open source technology that integrates really well with your infrastructures yeah so use kafka as a buffer collect the data data stay in kafka we do some transformations with so we must ask the data to create parker files then later on we deliver to the object storage and then we use cdw as acquiring engine to this so with that what we got it's a very very very how to say simple system actually you know we don't have to deal with all the you know all the complexity of having data into database or whatever if it even if the autonomous is doing all the tricks for you so this is very cost-effective because we got the same performance that we were getting before but i would say even better and there's nice study that i can tell you later we have the time to see how we improve the performance using this this attack yeah but actually the cause of having the data in the object stored instead of an extra data it was a fraction of the cost yeah because you are paying less for storage which is objective storage is really relatively cheap in terms of cost yeah and then the performance that we're getting to access to these spark files as an external tables was more than enough than what we need so but then you know i mean the system that you are providing guys it's pretty flexible we wanted to do something better you know so we were just collaborating with your development into in the meantime they really really was evolving because we've been working with you for quite a long time and then we came out with this idea of saying why don't we use these hybrid partition tables so we know that 90 percent of the of the athletes that we have is to hold data data that's been produced over the last few days why don't we move this data to extra data and the rest of the data is used is kept in the object storage with that what we got is a kind of very nice transparency layer that allows you to work with the data sql and then the data is being collected for the way or from the web from the place it is transparent into the user and then the performance was relatively very very good for us so this thank you for that so this solution um very simplified which is one of your objectives cost effective but i think the other the third leg of that stool is your performance is actually better than you thought it was going to be with this implementation so i think you've achieved kind of the the objectives there would you agree yeah sure finn and thing i forgot to mention about this sonic 1880 cloud that we put on top you know do you remember that aspiration that i told you before so that was key as well to allow data exploration too because you know even if you think that cern is plenty of smart people that sometimes it is sometimes it's not it's just any other place you know we need to offer tools to do the things in very simple manner because even if i'm the most knowledgeable data scientist sometimes i just need to explore my data so we need to a tool that allows you to sport it in a simple manner but as well to scale in complexity and in volume as well another collaborative that was kind of a good fit for that fantastic well again i i feel like this is uh once again more of a partnership and you're taking advantage of some of the services in the manner you see fit so let's actually go to that um the next part here which is what did you see as gains implementing the solution um for you yeah i mean you know i would like to say that stress from the point that you mentioned before i mean which is partnership and this is very important for us before getting into this because you know i don't see this is my personal opinion i guess the opinion for everyone working at cern with you guys is that we are not customers we are partners so we've been working together for many years so i have to thank you because that this project is is actually the result of this partnership i mean you get some hints i mean we are working very closely to the development teams to make that that works and i think that was part of the success as well yeah so if we talk about our gains so it does it was already mentioned so we got a very simple architecture it was compatible with what we had before so all these real-time applications that we have hundreds of application affects into this data they don't have to change i mean because there are legacy applications maybe maintaining this application is an admin but they just change the connection string in the worst case yeah and then they are working i mean the move from there is transferring the majority of the cases it's very cost effective so you know by for the cost of 1.1 petabyte of it's much less because we compress the data in parque as well but let's say for the cost of one petabyte in online storage which is almost neglectable we have a very good performance system yeah so we got automatic assets to advance optimizations you know so we don't want to optimize the system what we want is to profit from the system to do whatever we wanted to do which is analyze the thing so and then you know when you have this extra data and all this performance turning that is done automatically by you for you this is perfectly fine i mean that's perfect scenario for us yeah so transferring back out of patching so we don't care about this all the data is in object storage these three copies of the data is done so another important point is to scale up and down you know sometimes we have bits in demand so you have this auto auto scale up which even sometimes we saw that the peak would happen afterwards and everything was fine so we don't have to worry anymore about this which i think is pretty good so after i mentioned already the benefits of salina x8 is in the background so this is a very safe point from our perspective so something that i wanted to focus so to say here is that i think it was mentioned before you know no dbas no dbas you know the team in this project you know and many things you know even i'm a father of two years old girl which is sometimes challenging yeah and i've been doing many many different things in my work but you know i'm not a dba so and i didn't i didn't have the need to be a dba to this huge project you know and this was a very important asset and that take me to the other point because you know what we saw already is that you know people doing boring dba is tough because you know some people get more about doing the same thing and once again you know performance studying you dealing with users and all these kind of things you don't need to do that anymore i mean you can just swap the work of these people to get the most out of the data that you have what we saw is that there is already a swap of people doing db stuff to start analyzing the data which at the end of the day is what we need to do yeah yeah let's not worry about the infrastructure we'll take care of that it's part of our partnership um let's wrap up on this topic and if you spend a couple minutes on uh what this platform allows you to do with predictive analytics we haven't really touched upon that yet but but can you discuss i'm putting you on the spot a little bit here manuel but what does the platform allow you to do with predicted because we talked about downtime that's kind of a problem is are there things we could do out of that platform to provide that and this is a good question because you know as i said that was one of the main motivations to change this is the system that we have for that system is tell the people to do analysis of this so you know we're running a prototype in production and therefore we need to understand what is going on in order to scale our operations or even into stubble or operations right or even to driven decision to drive decision towards the futures of the organization so and that understanding came out of with this machine learning artificial intelligence thing that we can do but as well we can be more how to say optimal on how we are dealing with our infrastructures yeah so and this is where breakthrough analytics comes from i mean you know we have different projects where we are using breakthrough analytics based on the system that we've been discussing of this presentation over this talk that somehow are capable to predict whether some certain devices are about to fail so we have a schedule for interventions and then we can go back to the tunnel we can go down to the tunnel and then replace these devices before they fail because if they fail later on while we are in operations in the best case we will have to send someone stop operation and then send someone to fix we'll be a downtown in our operations so if we use impressive analytics which is already the case can avoid this kind of thing this is a very very important game for us yeah staying with the theme of being cost effective if you know when and where to deploy somebody fix it you can get on with it because downtime is can be very expensive yeah uh manuel muchos gracias thank you for joining with us uh thank you for your partnership we're looking for more interesting discoveries of the universe from cern thank you very much thank you very much scott it was my pleasure absolutely okay i believe now if i'm not mistaken we have a poll yes so um if you would please take a little bit of time to answer the following polling question let's pull that up so you've heard some use cases which of these use cases most resonates with your requirements we talked about departmental data warehouses departmental data warehouses with enterprise application data data lakes and so on if you would take a moment to click one of the radio buttons and we'll see the results here in a minute all right we got some votes so we got departmental data warehouse using enterprise application data oh that was quick so that's what i saw uh not uncommon and of course we're happy to work with you partner with you on setting setting that up which actually leads me to let's spotlight on you how do we help you get started and what those next steps might be so let's go to the next slide um the next the next step really is instead of you know leaving everything to let's try to figure out which services are appropriate for my environment um we've come up with a few things to help you deploy quickly and with confidence and they are you've developed architectural patterns which is why the question before was so key based on those use cases the second thing we've done is we've taken those patterns and we're starting to make those available as automated deployments within oracle cloud infrastructure the console itself we're going to walk through these by the way and then last i'm sure there's folks in our audience who are saying that's all great but i just want to learn an experiment you have something brand new called oracle live labs so i'm going to give you a quick tour of each of these let's go to the next one so the data warehouse architecture patterns um follow the polling questions actually which was which was nice it was very uh not coincidental uncoincidental um so departmental data warehouse enterprise data warehouse these are all patterns and i'll walk you through the website and i'll give you the link at the in a little bit here that have architectural diagrams descriptions white papers um technical papers i should say and terraform scripts where you actually go through and choose and you can if you're a little more technical uh apply the terraform script so let's actually go to the next and it's a video just so we don't have any uh errors here here are the different use cases um there are pictures let's go down to the departmental mark for e-business suite similar to what andy showed earlier here's the use case pattern for that this is an example of an enterprise application in departmental spreadsheets collecting that into the data warehouse with analytics cloud then of course you want to say okay let's take a look at that let's go into more information here in our architecture center and there's a ton of these in here by the way i'm just selecting one not totally at random if you scroll down here here's the architecture again here's the description of each of the services in the different categories here are some recommendations on services we think are appropriate here some consideration and what's most interesting is we've put together a deployment for this we we've scripted this into a terraform script and it's actually housed in git github here's the link here's the architecture here all the components not trying to hide anything there's no magic here we've we've uh put these together for you to really speed deployment um prerequisites and so on uh in order to deploy if you don't like it we can destroy it and again back to the reference architecture oh that's a misspelling there all right so uh let's go into the next so that those are those are your architecture use cases let's also look at the oci console so we've taken those patterns we packaged them in and we've we're starting to automate them in the console itself so if you're in the console there's a uh kind of in the upper quick actions there oh let's go back there we go i think this should be this should be playing that's okay there we go um it is a lie it is this is recorded but this is live so there's a button there says create a stack and when you select this button what you will then see is a list of solutions and in here i'm kind of cheating there's the departmental data warehouse similar to what we saw in the pattern and in this this automated deployment will deploy autonomous data warehouse and oracle analytics cloud it's as simple as selecting that there are elements in here that you can change so let's take a look at that let's just deploy the sample solution we're going to click next there are some parameters we can change in here to our heart's content whether it's i'd want to change the name a few parameters there the whole point is automating this process will take care of that for you and it's pretty quick imagine if you had to do this yourself you absolutely could again trying to improve time to market at these solutions um you can apply all the changes and move on let's actually go to the next slide this is just a screen of actually deploying and it's and it goes pretty quickly the third piece is i get automated deployment i get use cases i just want to get my hands on what's new is something called oracle live labs it's a series of workshops you reserve the date and time you run the workshop in the oracle cloud and i'll show you a little bit about that and then hopefully through the workshop you learn the skill so let's take a look at this so this is the website again you get the url here in a minute um a variety of featured workshops um let's go ahead and take a look at um all the available workshops if i click there i think it goes in a minute there it goes i just described that go back to viewing this featured workshop yes so let's take a look so and this will continue to grow so this is what we have today uh autonomous database in depth so on i can actually filter so let's take you know hey i want to actually learn a little i want to be more productive what do we have in the area of ai and ml oh machine learning and autonomous database uh we heard about that a little bit earlier so what does that workshop entail um here's the about screen there's some pre requisites there's the outline and then there are ways to run this workshop you can run it on our free trial the always free environment and also we allow you to run in your tenancy as well using your credit so you actually get to own uh the resulting infrastructure so these are just some of the ways in this course we can reserve the workshop pretty simple here let's go to the next slide all of these are available through our our new a link here oracle.com modern dash data dash warehouse seems like a mouthful hopefully it's memorable so if we go to the site should be live today you'll be able to access the things i just mentioned obviously you're going to have to go to the oci console to access that but the architecture patterns and live labs you'll be able to access here and much much more but we didn't stop there next slide we're going to have more events there was a lot to cover here so if we look at uh our road map our our calendar uh out here if you go to oracle.com transform data all one word there's a variety of events virtual events this is the kind of the i don't want to call it the new normal but this is really easy to consume on getting started the changing role of the dba and i'd like to point out our partnership with accenture on august 27th we're partnering with partners as a business partner our customers we're partnering with our partners to deliver these solutions in a timely manner as well with that um steve i'll turn it back to you great thank you and first a big thank you to all of our customers for sharing their stories um and as andy showed we have a complete integrated solution that has all the elements you need already working together and it's easy to get started it's easy to secure and operate and it's easy to analyze and of course with autonomous operations you're going to spend a lot less time operating your modern data warehouses and much more time discovering the valuable insights from that data that can help impact your business your customers or whoever you might serve and i think we heard great examples from the customers there and again i'd encourage you to look at the labs and the other resources that were covered uh they're there to support you and we're here to help as you move forward in your journey um so you know we do hope you will attend our next virtual summit um the focus will be on the efficiency and transformation that can be achieved through database consolidation so we're really excited about that have a great lineup uh set for that and again we hope that you will join us you know with that again i would like to thank everybody for attending this event and hopefully joining us for future events and once again thanks our customers for joining us thank you andy um you know it was it was a very knowledgeable um you know couple hours together so really appreciate it thank you [Music] you 