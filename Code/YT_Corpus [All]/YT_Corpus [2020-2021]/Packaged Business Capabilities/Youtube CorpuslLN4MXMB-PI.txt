 hi my name is borja munoz and i'm a product manager at carto first of all i would like to thanks databrick for organizing organizing this summit and today i want to show you how you can unlock your special analytics use cases by combining carto and data breaks first a brief presentation about carton and carto is a location intelligence platform our main goal is to unlock the power of special analysis currently we have more than 500 000 users and we have a team of more than 150 and we are one of the leaders in the field of special data science why does special analysis matter more now than ever because we have new end users traditionally a special analysis has been a field dominated by dis specialists or gis analysts but now there are more uses that more users that want to take advantage of a special analysis for instance we have been business analysts or product managers we also have data scientists trying to use special information and decision makers that are using a special information as one of the key fields for their decision we also have new data streams that weren't available before now for instance we have human mobility information coming from telco companies or we have iot hardware information or transactions information coming from credit card companies also with cloud computing and the new distributed computing platforms new analysis are possible so we can implement new optimization algorithms algorithms for a spatial prediction and we have a whole new field called spatial data science with the combination of character and data breaks we can solve some of the most frequent challenges that you can find you can find when you are trying to use special information and do analysis with it the first challenge is to be able to do a spatial analytics at a scale so usually a special analysis have been down by gis analysts using powerful workstations but you are limited by the capacity or the resources of the workstation and now using a platform like data rigs you can scale computation in a cluster of machines so you can work on very large data sets the second challenge is related to information that is in silo database system and you want to combine and to enrich the information this is something that is quite easy using cartoon data breaks because you can use the carto deer sql connection feature to access the information in your carto database and then combine it with the information in your databricks delta lake and finally the third challenge is related to data exploration usually one of the first tasks that data scientist does is to explore the data set and if the data set has special information you need to be able to explore it in an interactive map and this is something that you can do with carto and the carto frames package with within a database notebook what does the architecture look like if you are using cartoon databricks so starting from the left we see that carto has a relational spatial database with the special information then you can read this information from the spark cluster in databricks using the dire sql connection you have two options you can storage the information in the delta lake for later processing or you can directly use character frames for doing visualization and reach manual analysis with both options at the right at the end you can write the results back to the character spatial database so we have mentioned two of the main character products carto features these are the dire sql connection and cartoon frames direct sql connection lets you connect to the character database from the database cluster and read write information very easily using spark data frames cartoframes is a python package that you can use for enriching analyzing or visualizing your spatial data within your databricks notebooks in more detail with the direct sql connection we can easily create a spark data frame by just using the spark.read.format method as you will use with any other jdbc regular connection and with cartoframes you will install in your databricks notebook as you will do with any other python library then you need to set up your default credentials that is your character username and your cartoon api key and finally you can read the information easily from a character data set into a geo pandas geodata frame using red carton so when should you use direct sql connection or when should you use character frames you should use the direct sql connection when you need to perform data engineering tasks that are not suitable for relational databases or that are difficult when using relational databases for instance if you want to do a white to loan transformation you should also use the direct sql connection if a scalability is an issue if you need to to perform special analysis over very large data sets you need to use the direct recognition to be able to use spark data frames and then use the full power the full resource and capacity of your database cluster and then you should use scar to frame if you want to do data exploration interactive map do you want to visualize your data if you want to enrich your data sets using cartodate observatory you should also use carto frames and finally if you want to do some special analysis like generating iso crowns geocodings cartoframes is also the way to go now we are going to talk about use cases we will start with generic use cases and then we will move to industry use cases starting with generic use cases the first is data engineering we are going to use data breaks for collecting and preparing the data sets that we will later visualize or analyze using carto if we are talking about the data visualization use case what we want to do is visualize in a carton map the information that we have and we want to do it within a databricks notebook because it's the environment that we are using and finally we have the we have the data analysis use case where we can take advantage of the carto features for a spatial data science without leaving your database notebook we are now going to focus into some industries use cases these use cases are available in our website at carto.com slash industries where you can find these use cases and also use cases for additional industries starting with logistics logistics are using these companies are using special analysis for several use cases one of them is route optimization where we want to identify the optimal routes and optimize them by using special information it is also used for pickup and drop-off site planning so for instance if you want to to install or to have self-serve loggers and you need to decide where is the best location you need to be able to predict demand using a special modeling and you can also use it for fleet management so for instance if you want to visualize where your fleet assets are or how you can optimize their activity you can also use location data streams like traffic data or weather data another industry where spatial analysis is being used a lot is telecommunication companies they are using a special information for creating data products and for doing data monetization and sold selling these products to b2b companies they are also using a special analysis and special information for planning their network deployments so they need to be able to to look at market requirements to look at potential customers and they are going to use special information for that another use case is a smart cities and iot where they are going to to currently they usually provide the communication platform for the iot hardware the iot sensors but they can offer more value if they also provide analytical tools using special information financial services is also one of the industries where spatial information and special analysis is being used in key use cases one of them is bunch consolidation for consumer banks where they need to decide which are the which is the branch they are going to close and for that they need to be able to monitor site performance using a special modeling or special analysis another typical use case is atm location optimization where they need to decide where to locate the atms in order to maximize the transactions and the revenues by looking at demographics or socioeconomic profiles and for instance credit card companies are also using special analysis for doing transactional analysis so they know not only the where but also the why behind consumer spending habits and this information is really essential for the marketing and retention departments we also see a lot of use cases within the consumer package goods or fast-moving consumer goods sector they are using a special analysis for for key functions like distribution management or supply chain networks they need to ensure that they stay in a stock and in order to do that they need to use special information to ensure that they have a very really robust distribution management system they are also using a special information for other use cases like territory management where they are designing territories that are better balanced in order to optimize or to improve the field sales performance and finally the last industry we are going to talk about is healthcare and pharma in healthcare and pharma the companies are using for instance healthcare factor analysis especially the insurers and the public health authorities to ensure that they allocate their their resources in a very smart way looking at information like data streams and also they are using special analysis for doing site planning when they want to open a new hospital a new clinic or a new vaccination center they need to decide what is the which is the best location so they can maximize their return on on investment and for that they will use health data catchment areas population data that all of them has half allocation components now we are going to see a real world example we will see how we can use carto frames with the data observatory and also with special analysis features to deliver some very interesting insights the first thing you have to do is to install the cartoframes library we are going to do it using install pipi from the dbutils library next we need to setup the credentials for accessing our carto account you can use cartoframes without having a cartoon account but some of the functionalities like data enrichment through cartodate observatory won't be available are going to look at the target store locations in the united states and we'll try to get some insights using visualization and a spatial analysis first we create a pandas data frame by reading the csv file and then we convert it to a geopandas geodata frame creating the geometric column that will store the location from the longitude and latitude columns after we have our data geodata frame ready we can quickly visualize the information on an interactive map by creating a layer object now you can easily write the information from databricks to your cartoon account calling the to carto function with the data frame and the name for the data set we want to create then we read the data again but this time from our carto account to create a geodata frame and now we are going to create a map taking advantage of cartridge advanced visualization capabilities first we are using the color category style helper function to create a curved left map where each target store will be assigned a color depending on the subtype attribute our visualization will also include a pop-up showing the store name and the address and we will add two different widgets to show the number of stores in the current view and the distribution of stores depending on the state attribute finally we are adding a map legend and we are selecting the dark matter base map as we zoom in we can see the different stars with a color base and the subtype attribute and we can see how the values on the widgets are updated we can also publish our visualizations to our cartoon account so they become available by just using a url we can protect this url with a password if we want to let's see how we can generate some interesting spatial data science in sites let's start by creating isochrones we will generate areas within a 10-minute drive for each store location in order to do that we just need to use the cartridge data services api through cartoframes to create an object to call the isolines service and then call the isocron's method passing by the geodata frame with the store locations the drive time in seconds and the mode there are also additional location data services like geocoding service to get locations from addresses or place names the result for this operation is a new data frame with a polygon for each store representing the drive tank areas when then we can write a results to cartoon or we can create a visualization showing the polygons if we zoom in we can see the polygon here probably the target store is around here we can take advantage of spark distribution features to scale the computation and be able to process simultaneously several isochrones or geocoding if we have a very large data set for instance we could write a new df function that will process one store at a time probably the most useful feature is being able to do data enrichment using the data sets from cartel data observatory first we take a look at what are the data set categories available if we want to look at demographic data we can pull out all the demographic data sets available then we select a socio-demographic data set from the american community survey and we take a look at the area covered by this data set in this case all the us to do the enrichment we need to select one of the variables in the data set the first thing we need we do is to retrieve the available variables we can also filter the list here we are pulling out all the variables that include the word income in the description column now we select two variables median household income and total population and use them to enrich our data set that contains the drive time polygons it is very easy we just need to call the enrich polygons method and we will get a new geodata frame with two new columns at the end representing the median household income and the total population for each drive training area we currently have more than seven thousand data sets available in the data observatory some of them are premium data sets and you need to get a subscription first but we also have public data sets like the ones from the american community survey or population data sets with worldwide coverage like workpop now we are going to create our final map we are going to join the enrich drive times polygons with our original data sets with the store locations after that we are going to create a visualization that will assign a different color to each polygon depending on the median income attribute we will define a pop-up that will show the store name and address and also our new two new columns representing the median income and the total population and finally we will add four widgets to show the number of stores in the current view the store distribution based on the state attribute and two histogram widgets showing the distribution of the two new variable variables so this is the end of the demo you can generate pretty interesting insights from your databricks notebooks by using cartoframes with the cartridge data services api and the data sets from carto data observatory so thank you thank you very much for attending this session i hope you find the content interesting please don't forget to rate and review the sessions thank you very much 