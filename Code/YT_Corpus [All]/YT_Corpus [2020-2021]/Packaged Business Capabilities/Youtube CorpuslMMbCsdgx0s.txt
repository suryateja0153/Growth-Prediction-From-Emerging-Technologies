 [Music] hello and welcome to this session covering best practices for building cloud-native applications on azure my name is sean mckenna and i'm the lead product manager for the azure kubernetes service when we talk about best practices we mean tools and techniques that have been demonstrated at scale to deliver the best results for customers in azure we've worked with thousands of customers running cloud native applications on kubernetes in conjunction with services like cosmos db today we'll specifically be looking at best practices in two domains cost optimization and security a core tenant of the cloud native philosophy is that workloads and the associated infrastructure should be dynamically allocated to match the needs of the business that dynamic nature provides significant opportunity when it comes to optimizing costs in the cloud today we're going to talk about two specific use cases for optimizing cost dev test environments and production environments with fluctuating demand devtest environments usually have relatively limited load that is largely constrained to weekdays during business hours while you don't want to be paying for resources that aren't in use overnight or on the weekend it can also be time consuming to completely tear down and reprovision resources on a daily basis aks and cosmos db now offer great solutions to this problem with start stop clusters and serverless databases respectively the new start stop feature in aks allows you to completely shut down your cluster including all agent nodes while persisting the azure resource and its associated metadata this means that when you restart the cluster in the morning or after the weekend it will rehydrate exactly as before with all of your node pool configurations intact and with all of your applications redeployed let's take a look at the aks start stop feature in action for this demo we've got a cluster running the azure voting app from the aks tutorial the app includes a number of services and deployments as we begin the cluster is running normally let's set a number of watches to observe the effect of stopping the cluster we can issue the stop command from the azure cli within a few seconds we can see it begin to take effect much like a cluster upgrade stopping a cluster will cordon and drain the nodes to ensure graceful termination for any running applications but within a few moments the agent nodes have been shut down meaning that no further vm charges are being incurred shortly thereafter the control plane itself shuts down causing any watches on the kubernetes api server to timeout as the cluster has been fully shut down the azure voting app is no longer reachable now let's reverse the process and rehydrate the cluster within a few moments we can see the status in the portal change to starting as aks begins to redeploy the control plane and our node pool as the cluster comes back to life kubernetes natural control loops begin running and seeking the goal state that was preserved during cluster shutdown resulting in the azure voting app coming back online ready to serve traffic again you can easily pair this capability with cosmos db serverless option which ensures no minimum charge for your database and allows you to share throughput across multiple database containers keep in mind that the serverless option is more expensive on a per request unit basis than a standard or auto scaled database so it should only be used in environments that have light intermittent traffic which is why it's perfect for dev test environments of course moving into a production environment doesn't mean that you no longer care about costs so aks and cosmos db have you covered there as well with rich support for auto scaling let's take a look at a few of the auto scaling techniques available to match your application's capacity to its demand kubernetes supports horizontal scaling on two dimensions pods and nodes the horizontal pod autoscaler or hpa allows you to scale the number of pods in a deployment to match a desired metric such as requests per second when the current value of the metric exceeds the threshold pods are added when it drops below it pods are removed in many cases the most relevant metric for pod scaling will be one that is external to the cluster such as files being created in a blob storage container or items being added to a service bus queue kubernetes event event-driven auto-scaling or cada allows you to easily integrate these events with the hpa growing and shrinking the number of pods in your cluster in response to those external events as the hpa increases the number of pods needed to service the application's traffic it will eventually hit the limits of the cluster's compute capacity at which point additional nodes will be required to host those pods to address this aks includes built-in support for the kubernetes cluster auto scaler which can add or remove nodes based on the needs of your applications you can configure the autoscaler on a per node pool basis to ensure that you only scale the type of compute needed for your workloads and you can now scale node pools all the way down to zero for periods of low traffic the cluster auto scaler is well suited for use with spot node pools which enable you to take advantage of unused capacity in azure's data centers at a massive discount as spot nodes are evicted to reclaim capacity in one part of the azure fabric the cluster auto scaler will attempt to acquire capacity elsewhere the additional churn and non-guaranteed nature of spot node pools make them best suited for low priority workloads like long running batch jobs but when used effectively they can deliver massive savings azure cosmos db complements the auto scale behavior of aks with its own auto scaling capability which will automatically adjust the provision throughput of your database containers based on demand up to a maximum that you define you are then charged based on the hourly maximum of provision throughput let's take a look at how we can use aks and cosmos db's auto scaling behavior together to match our application's capacity with its demand in this demo we'll be using the cosmos db to-do list sample which has been packaged for deployment for two kubernetes to demonstrate how aks and cosmos db can scale to meet demand we'll generate some load on the application now initially we're running just one pod so within a few moments we'll max out at around 24 hours per second with poor response times let's deploy a horizontal pod auto scaler to scale out the number of odds to match demand we've configured the hpa to seek an equilibrium of 10 requests per second per pod any more and it will scale out any less and it will scale in as the hpa continues to add more instances it eventually hits the limits of the infrastructure that's deployed resulting in pending pods which cannot be scheduled this is the trigger for the cluster auto scaler to begin adding additional nodes to the cluster we can monitor the status of the cluster auto scaler using the config map in the cube system namespace from that status output we can already see that the cluster autoscaler is in the process of scaling up adding additional nodes to the cluster in order to schedule those pending pods and within just a couple of minutes we can see those nodes come online the addition of compute capacity to deal with higher load in turn results in higher demand on the cosmos database that backs our application eventually we hit the limit to the provision throughput for our database container resulting in throttling by default the cosmos db sdk responds to throttling with retries resulting in gradual upticks in our response times as shown in the p50 and p95 response time graphs switching over to the azure portal we can confirm that we have maxed out our provision throughput let's remove this bottleneck by switching our database container to auto scale within just a few seconds cosmos db begins adjusting our provision throughput to match demand resulting in even higher overall throughput for the application the higher throughput also eliminates the throttling that the sdk was performing bring our response times back in line with an acceptable range when the load eventually stops the process repeats in reverse with cosmos db the cluster autoscaler and the hpa all lowering capacity to match the lower demand while saving costs on infrastructure is great there's ultimately nothing more costly to a business than a security breach that's why azure is committed to a multi-layered security approach to kubernetes deeply embedding security and compliance controls throughout the application lifecycle let's take a look at a few of the latest innovations you should consider adopting in your environment identity management is a critical element of any security strategy we recently announced two new capabilities to make linking your existing identity assets and workflows to kubernetes easier aks's new managed aed capability enables automatic integration with your existing azure ad directory without having to manage client server applications or request tenant admin permissions on top of that aks now supports direct integration with azure rbac this allows you to use azure rbac roles to control access to your aks clusters as with other azure resources you can leverage the built-in roles like reader writer or admin or define your own custom roles let's take a look at how azure rbac and aks work together here we've got a cluster that's configured for aks's new managed aed capability when we try to query the list of pods itcast requests that we sign in using the standard aad authentication flow defined for our organization including corporate policies like two factor off in this case despite signing in the request to get pods is denied because we are not currently authorized to perform that action now you could manage the addition of that authorization by defining kubernetes roles and role bindings but in this case we'll look at how you can do it natively with azure rbac in the portal we'll navigate to the access control blade for our cluster and add a role assignment note the selection of built-in roles for reader writer and admin in this case we'll choose reader and assign the role to user account now when we attempt any read action including getting pods or nodes those actions succeed however should we try to perform a write operation such as deploying an application that still fails as we were assigned to read a role let's go ahead and do a new role assignment to an admin role in this case we'll do it from the azure cli note that we're going to scope the creation of this role assignment to just the default namespace with that role assignment complete deploying the application succeeds however when we try to create a new namespace that fails as the scope of our role assignment was limited to the default namespace one of the truly powerful things about the integration of azure are back with aks is that you can apply role assignments at a resource group level and have them cascade down to all clusters within that resource group in this case we'll create a role assignment assigning a reader role again assign that to a user and we're performing this at the resource group level which means that all clusters within that resource group will automatically have that role assignment applied and indeed when we go and query pods and complete authentication to aad we'll see that the getpods request succeeds in addition to managing identity and access control most enterprises also maintain a rich suite of corporate policies with azure policy and aks you can use the same declarative approach used with infrastructure and application deployments but for policy statements all built on an open standard known as rego common policies include blocking the creation of externally exposed kubernetes services and limiting the containers running in the cluster to only trusted registries azure policy for aks is now generally available finally we'll close with a technique that is fairly nascent but which is destined to become a best practice for highly sensitive workloads confidential computing nodes confidential nodes provide hardware-backed integrity guarantees covering code hardware firmware and microcode components as well as an execution environment that blocks access to your data from the guest and host os hypervisor and even azure administrators ensuring the highest possible confidentiality best of all confidential computing integrates through cleanly into your existing workflows and even your existing clusters with the ability to create confidential node pools alongside standard ones okay so this has been a very quick tour of some best practices for building cloud native applications on azure covering cost optimization and security there's obviously a whole bunch more to learn so i encourage you to check out some of the resources that are listed on this slide thank you very much 