 alright hello everyone this is Camille CTO and co-founder at starburst and I will be talking about powering interactive VI analytics with Bristol and that data like today all right so in this presentation I first want to introduce presto for those who don't know what that is and a little bit of starburst and what we do here to help enterprise adoption of Westar you know the main topic of my presentation is the Delta Lake integration that we done for Forrester and and then sort of show how we can combine Forrester and data breaks spark and Delta together in one data path from architecture and how to efficiently use the best capabilities of both technologies and then finally show real use cases where that combination actually delivers best results for for your team so with that it's going so presto and starburst first itself open-source community driven project and its core it's a high-performance NPP sequel engine it was designed specifically to be geared towards interactive analytics and perform on cc-cold analytics over large scale data ranging from gigabytes on the low end to to petabytes on high end and the design goal for a Presta was also to to help with high concurrency queries and fast performance overall one of the unique capabilities of faster is that it provides full separation of computational and storage layers so that allows you to scale those two independently add more storage without adding compute nodes and then boosting compute if you need to perform lots of analytics and then scaling back when when you don't need them at that capacity so allows you to manage cost and performance elastically and and cost effectively also crystal itself is just a computer layer right it accepts a sequel statement does all the eyebrow execution of the query and then it's done right there's nothing in fest about storing the data so you have to kind of bring your own storage and storage this most efficient effective for your use case and most appropriate so in a sense Bristow allows you to run cycle on anything and with the powerful extension if it is you can actually connect to a variety of different sources and also write your own connectors to two sources that are not yet available in the open source or commercially the side effect of this is that you can also run federated queries meaning that in a single sequel statement you can actually refer to tables coming from different sources and and thus allow you to do correlations of data coming from different sources and perform this interesting analytical queries that will otherwise be waiting for you to bring all your data into one place which obviously is a long process and I mean you may hit a lot of barriers especially in the enterprise settings and finally you can deploy presto basically anywhere we've seen successful deployments of presto initially on bare metal on-premises systems then in virtual virtual environments and cloud now kubernetes is really the way to go for deploy questo in essentially any of those varmints so gives you a lot of flexibility and you have to be tied any specific platform and you can move pressed analytical system on one place to the other angle were word your center of gravity for de Guise so first has been around for some time it's been about seven years this was originally developed by the team at Facebook and got initial adoption is a large Silicon Valley Internet companies and then progressively expanding more and more companies globally different verticals and news cases so you can see some bloggers here on the screen that just gives you a sense of you know presence applied pretty much anywhere for psychological purposes and it's been the fastest-growing sequel engine among any anywhere in the open source for sure you can run at a massive scale um so some of the larger deployments especially its Facebook Links and lived talking about hundred machines whirring petabytes of data and running thousands hundreds of thousands of queries a day so it's really really impressive from what those companies are doing and pushing the limits and scale and performance so now starburst we are the enterprise Presta company we are the commercial arm behind the open-source project much like databases for spark what we value we are bringing to to enterprise customers specifically is it's around simplifying the usage of Christ provides you lots of security enhancements with integrations data encryption masking permissions education with without that talked about you Ranger and other security related protocols you know in the enterprises you typically have lots of different additional connectivity needs beyond just open source formats you'll be talking about querying Oracle generate db2 snowflake etc etc and those are all packaged in the enterprise distribution of presto that starburst offers on-demand route side as I mentioned you know ease abuses super 4 into enterprise settings so we manage configuration of a scaling H a monitoring and just deployment in all those different environments simplified orchestrated effectively and all this is obviously wrapped into as providing support for presto and because we have the largest team of experts in the world we have first the creatures behind company right now and provide hotfixes security patches 24 by 7 support and I know the thing that's also very important and it's also true for beta breaks like we are the leading contributor and committee compressed itself the open source project all right so so we are driving the roadmap and enhancing quest to get you more performance more scale more functionality in the open source and then if you choose to go with our platform and managed enterprise offering and that's where you get additional benefits as well ok so so you've I've talked about how priced I can connect to many different sources already so why why did data like we obviously got interested in in the Delta like when when it was first announced by little bricks a couple years ago then subsequently when it was open sourced last year and built alike is really really exciting technology for many reasons so I just mentioned several of those here which I think are fundamental first of all it gives you the acid properties over the data Lake and this is huge right it's massive because you can now go and and delete individual rows and update individual rows and and incidental rows effectively over your data like which no in the past was quite cumbersome and and complex frameworks to do that that's now simplified everyone can treat object storage from as essentially a database table which is which is amazing that's how we want to work and that's how presto here's the world so that's great it's opens open on open source table formats multiple different tools can use it spark obviously was the initial implementation now presto there's connectivity for hive and I suspect it will be much more broadly adopted in the future as well the actual data and under underneath is todos porque file 14 files which is great because ok is very very efficient format to store the data is columnar it's compressed is built in new max indices and other performance optimizations which would which is very great for performance the other supports object storage natively on on as free adls and an address storage which is where the world is going with when storing large amounts of bacon these days that's that's amazing on top of all those basic fundamental benefits there are really great features around scheme evolution and even time travel allowing you to see you what would be the answer to this query if you are in fact a day ago and I think that's that's very interesting for for in many workloads and especially in the analytic space and then you know just to further show the benefits of Delta having the dedicated metadata information that's outside of highlighter store having statistics on the data [Music] that helps out with performance because you can skip the data and you know if you have data arranged in a special order but also helps to speed up many queries so I think we all are very very thankful to data breaks for inventing Delta and making that popular file format is getting lots of adoption many of our joint customers are also users of Delta so it really made make a lot of sense for for us at starburst to enhance presto and allow it to to also equate Delta effectively as I mentioned stuber's developed a native purse Delta Lake connector and I would like to acknowledge great help from engineers at data bricks that assisted us with that integration and provided further details in addition to the official specification of how to implement this effectively so we decided to build a native connector written from computer from scratch right so there used to be a legacy solution that I was manifest files and hive might restore integration and simulate access from presto to Delta I was very very inefficient because it wasn't taking advantage of any of the inherent data Delta like properties so in this implementation we actually natively read the Delta transaction log we perform data skipping based on the metadata read from from from Delta and we are able to fetch the Mystics about the data and basically leverage those as an input to the optimizer and that allows us to effectively perform joins among both the tables as low as those other tables and other other team was coming from different sources so I was really important to have this native integration built for stay here ok so once we build the very first version of that we accuse obviously how that performs to this legacy solution which basically treated Delta as a collection Alfredo perky files so we run on standard age benchmark and across 22 queries in this benchmark we so on average except for for all the queries and obviously those queries are doing much more than just real data so this in the fastest query and do we observe in this metric which was a single table scan doing some aggregation was like she's showing six acts performance boost in there and with the native reader so that's that's pretty substantial and and once we get the pollination done efficient implementation of the Delta reader we also share this with our preview with our early customers and the feedback we got was actually even more enthusiastic because they were reporting speed ups you know over 10 X 4 for the native reader versus the previous solution so this is dry seductions helps house for the performance overall and it makes users more happy to come back and run more queries and do more analytics so I definitely encourage everyone to try out this native integration of Delta with starburst Presta and the link and documentation link here on the screen so you know having that is great obviously engineering edition yet another connector for the presto but how you soft.i all those things together right to work together effectively in one environment and I thought I would spend a few slides on this topic how to effectively leverage data reefs and starbursts one architect so this terrorist platform itself obviously at the center of that is Presta as this fast sequel here that's talked to many sources we've done in addition to just leveraging open source press or we build you know additional connectors to more sources I mentioned some of those are traditional commercial DBMS sources a very column in prices being in the primary examples we enhance support for additional SP compatible storage engines for premises as well as the clouds all the clouds obviously and today and press it can also talk to your modern no sequel stores such as manga elastic Cassandra etc I can and so having a tall one to reach to all those different sources both legacy and and modern places where you may store your data is really really powerful however there's always always challenge well with all the diversity of sources how to do it effectively how to do it securely and how to manage the whole experience for the end users right so this called as part of our platforms we call it data consumption layer and big piece of that was building a global security mechanisms that will govern secure access to all the sources from presto and and also from the users that are occurring this data by oppressed and of managing permissions masking has sensitive information ensuring data encryption both for reading the and data encrypted at rest as well as encrypting data and flight when it's moving from the source to Preston and no we Winterfest a cluster and then between the client tool and the past right you can all these all the queries you can have very fine-grained access control down to the table and column and actually you cannot survive role or all filters and further restrict access southern things so all of those things are part of starburst platform edition and on top of that we found integrations and certifications with a number of your favorite BI tools there's no look at RBI click cetera and and so enabling that class all the modern tools and that can issue sequel starting from Jupiter notebook superset read ash which I think right now is part of data breaks all those tools can talk and speak to restore natively and gives all the users power of the the broad set of connectors and security features of starbursts so that's on the starboard side and now this question aloud like you know I have spark I beta breaks I have first and starbursts like how do these things work together was the best way to effectively leverage both right and what we see we all live deployments with our customers is that pretty much everyone who ever just arbors and impressed they also leverage spark and often doubts data bricks spark right and the reason is that I think those technologies excel at different things and complement each other very well so if we're talking about swimming ingestion of data talking about you know being machine learning jobs artificial intelligence obviously managing data like doing really really heavy long ETL jobs all those things will you do it through just native spark syntax or sequel all of these things are best positioned to happen inside data bricks and inside spark and spark astir and the way to do this effectively on the other hand presto really was designed and excels in high concurrency sequel so if you're running tens and hundreds of course at the same time if you if you're doing some bi reporting analytics interactive data discovery using sequel and and you want to also further write different sources right so you have Delta Lake and in porque files on this we and you know Avro and also relational database with no sequel engines and this is where presto I can provide a lot of value and we feel that Interactive faster performance is enough of distinguishing factor that drives adoption of question starburst and because we have now so many joint customers feel really compelled to really advocate for its joint architecture yeah so if you look look sort of holistically at the data ingestion analytics ecosystem this is all coming together by having you know your raw data sources all being ingested by a data breaks and spark into detail like you know these days like specifically right but you can also just put your your data on you know either Amazon s3 or as your Els and and you can run all the machine learning guy and all flow or say to make her over this data for for those use cases while if if you want sequel over this data plus you know correlate this information with with lots of different data sources but there are DBMS new sequel and all over the place they're like Star Wars and presto are is the perfect answer right and provides so higher more responsive having currency more responsive sequel access and allows you to leverage or the di tools and secure editors and reader super say Jupiter notebook and your favorite tools for analytical purposes for the more classical unknowingly purposes there's of the promise this is very interesting as well so these days we highly recommend deploying pressed on the kubernetes cluster and in fact I will be showing present deployed ETS service later in the demo but we see this pattern of press to being deployed by kubernetes on Azure I guess also in practice so we work closely with an open platform for the Aqua his deployment and we see people playing event is you know pretty much anywhere but as I mentioned you can obviously do it in many different ways as well at your company and and so if we simplify all that configuration deployment management sufficiently so it's really very very [Music] deployment and management and simplified drastically to what what you've been used to in the past okay so now bringing all this together right it's all great architectural technology was the real use cases that we can show here right so they're the one use key that we want to advocate for and we've seen this already being leverages you know in a joint architecture data breaks and starbursts we have obviously some IOT data streaming from from the IO devices swimming into the Delta like we may have some data coming from us yep your piece system you know you know already batches we are moving the statistic from the application layer into again Delta like specifically and we can go through all those different layers from you know ingestion bronze refined silver layer of Delta Lake and then to the aggregate store or the gold layer right which is the best position to find for fast analytics and starbursts can come in all that everything before happened in data breaks now starburst comes in and allows you to run your fast concurrent sequel and fetching data from your eyelid store as well as reaching out to more or less refined source versions of those tables if needed right if your analyze data scientist refers to look at the very raw data that's also possible from compressed and now while we have also made that being ingested into this there's data like it's not going to be everything you have potentially in your ecosystem we've seen a lot of companies coming with data like started this and having the center of gravity there however your our DBMS traditional Oracle carry to a db2 etc are always there and there is sometimes we're interesting data there that you better correlate with with the information coming from other sources and for other data sets like no more textual data so say web logs and user comments instructor less doctor data you know things like elastic or MongoDB rr-really properties ways and places to store them right and now since your data is spread across so many places you know you need the ability to query all those at the same time quite often due to arrive at the best insights that a contraction of business forward and this is what what we can provide here with starburst and all the connectors we have to enforce bathrooms so Adam as I mentioned will be doing so the real-time ingestion of event data and I'll be showing this in a demo in a moment will be doing also the the hourly jobs of hoping that the more traditional enterprise data and then reaching out your fukui data sources as well and we modify this and refine this and and we prepare this data for further outdoor means as appropriate in in our overall architecture so we're you know obviously the thermosphere provides this this single access later right it's no longer true that you have already been in one place so there's no single source of truth for day care this single access point the queried is date right but so visualize and query environment is where the power of questa is right so you can leverage the power of the appropriate data stores for your data and in query data right now rather than wait for maybe if your job to bring this Oracle legacy data or X data into one face because you it's hard to imagine you know how you might need to modify and prepare this data sometimes for your purposes if you give if you're given access to this elasticsearch you can actually push down some process from there and find appropriate data by by doing that and external it takes essentially and bringing that back to the relational world and unpressed to service can provide so far a global access control for sources so analysts are appropriately privileged to the data that's that's meant for them to be arised so now in the in this demo know and users will be leveraging API tools sequel edit first look at a blonde power bi leveraging connectors like JDBC drivers ODBC and wagons for other languages and wizard of mouch want to show all of this you know in a demo and briefly comment how you put all those things together ok so we are in a database notebook here and we have obviously as we bracket mounted and we are setting up a structured streaming ingestion from from a Kafka topic into into a stream that will then save into a Delta table so sitting up here making sure that connectivity is is live stream checking the schema making sure it's matching what to expect and now basically issue a command to receive this data and save into a Delta like calm and very fine that the stream is actually live okay so we have ingestion going on now we are switching to a DB view which is a secret or tool in which we have mounted all different data sources such as Delta Lake and we'll be trying out the query Delta from Malaya Prestone from from DB per client that came up very quickly as you can see now showing the same for elasticsearch so this is vastly set up in I was in cloud we can query that very quickly as well individually so that's perfect and then our zone RDS we have an Oracle system with some customer information and and that's together now bringing all this together into one sequel statement we can sort of correlate this information and do the join between orders customers parts and execute the sequel and here in the first web UI we can see how query is progressing we can know in toward a state of the cluster and see the progress of this join pretty calm based on query with 40 rows then in the results the beaver screen here now putting back to a more bi classy bi tool below we have a dashboard we are so according all the same information and displaying the orders on a Jeju graph your dashboard visualize I know which which orders become from which countries so it's like pretty quickly we have another statement being Ram and and all of this is now reflected on on a dashboard again and the difference is here that we have a virtual view by directing all this sources but still the same Delta a state rhetorical as free and now in addition to that body bounce you around a classic report here we are showing customer country for region and for visualization and all of this is interactive life course against the real data coming from all those different sources during COI time and so we've I hope that the unsession was helpful and you see the power of both tools together and we thank you all for attending and really happy to answer any questions you 