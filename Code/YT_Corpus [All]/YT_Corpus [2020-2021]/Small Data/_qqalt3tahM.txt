 welcome to the virtual presentation fast linear programming through transposition computing in the following 15 minutes we investigate how to accelerate the mathematical foundations of polyhedral compilation let me start with a concrete example that motivates the need for constraint solving in compilers on the slide we see a piece of code i now invite you to think for a moment about the computational complexity of that code most trained computer scientists understand computational complexity intuitively but what about the memory access cost of this code or the benefits of optimization such as tiling the answer is less clear memory access behavior is today critical for the performance of a program consequently memory aware programming is central for programmer productivity while classical cache simulators contribute to this awareness we've seen over the last years that analytical models based on constraint programming can give results a lot faster meaning programmers can get results in seconds instead of hours while seconds are great many use cases for constraint programming would benefit immensely from even faster constrained libraries hence understanding and accelerating the way foundations of such libraries is critical let me now take you from the world of imperative programs directly into the world of mathematical representations over 30 years ago paul fortrier introduced the so-called polyhedral model brilliant model uses fine constraints to reason about imperative programs and explains precisely how integer sets relate to programs i invite you to learn more about this relationship in our paper in this talk we will directly focus on understanding integer sets an integer set lives in a multi-dimensional vector space in our example in a two-dimensional space an integer set can be unconstrained and consequently contains all integer points in this vector space however typically we use integer sets to model constraint spaces where the precise boundaries are specified through a set of affine constraints on our slide we introduce for example two constraints to limit the range of x and two constraints to limit the range of y together they clearly mark the set of nine integer points that are an element of the set s when analyzing a program we perform different operations on integer sets typical operations are intersection and union calculating the difference of two sets or the complement of a single set other operations are emptiness checks finding a single sample value out of all the points in an integer set we can normally compute these operations rather quickly but any given program analysis will run a lot of these operations on many different integer sets hence being able to perform these operations fast is important before we can accelerate arithmetic over integer sets we first need to understand how a modern integer set library is structured at the highest level of such a library we compute complements subtractions intersections and various other user facing operations these operations are typically implemented in terms of more abstract mathematical algorithms such as parametric integer programming general basis reduction coalescing or just the detection of redundant constraints all of these core algorithms use their very foundation algorithm called simplex in this work we aim to accelerate the implementation of such a simplex solver at a very abstract level the simplex algorithm solves an optimization problem where a linear objective function is minimized while ensuring that a set of linear constraints are satisfied simplex solvers typically start by identifying a valid solution that lies on a vertex of the polytope that bounds the valid solutions the solver then jumps in so-called pivot steps from vertex to vertex always improving the objective function until it reaches a vertex where no further improvement is possible this vertex corresponds to the solution vector that minimizes the objective function while there are many ways to implement a simplex solver the most classical implementation uses a simplex tableau a two-dimensional matrix where columns correspond to variables and rows correspond to constraints each pivot is implemented as a scan over this matrix where all the coefficients are updated according to the rules that you see on the slide in the most simple case this scan is just a two-dimensional loop where the outer loop iterates over all rows and the inner loop iterates over all coefficients if we can get this and a couple of other loops first we can accelerate our simplex solver simplex servers are today already heavily tuned so our objective is not tuning a general proposed simplex solver instead we look at the very specific characteristics of simplex problems in compilers and exploit these properties to create a solver that is fast for the use case we care about for this we identified three state-of-the-art compilers that use political modelling the first is poly a polio loopnest optimizer for lbm the second is ppcg a political compiler that translates sequential c programs into fast cuda code finally there's haystack the analytical cache model that we already used to motivate this work we run all three compilers and obtained almost half a million sublexables let's now analyze these sublex step loads we first look at the number of bits across the individual coefficients in program analysis coefficients can become almost arbitrarily large we can see coefficients with up to 127 bits in our analysis however we also observe that more than 99 of the coefficients require less than 9 bits hence we typically have small values now let's look at how many dimensions a simplex star blue typically has we see that all our problems use less than 28 dimensions and we observe a median dimensionality of only 18 columns hence the templates have typically lower dimensionality finally let's look at sparsity again across all tableaus we see that our median sparsity is 84 percent so our tableaus have high sparsity now can we exploit these properties to tailor a simplex solver for compilation our solution is to design a fast simplex server around a technique called transposition computing transmission computing enables us to exploit the typical small values of coefficients while still allowing us to reason about constraint system with very large coefficients the key idea is that the matrix data type representing the tableau and the sublex algorithm itself are implemented using c plus plus templates so that such that the data types of the code can become template parameters as a result we can instantiate multiple versions of the same code a number of fast variants that use small integer types as well as variants that use big integer libraries to support integer values that require more bits than supported natively each transposition matrix keeps track of the data type that is used to represent the coefficient in the current tableau whenever we perform a computation on the tableau a transposition dispatcher checks the current precision and calls a specialized simplex instance for this position for example for 16-bit all operations are performed on 16-bit as long as no integer overflow erases in case an overflow arises we detect this overflow expand the data types in the tableau and rerun the algorithm this setup allows us to use fast native integers for the common case by falling back to arbitrary position integers as needed we also use transposition computing to exploit the low dimensionality of the simplex tableaus in particular we do not just specialize for the size of the integers but also for the number of dimensions a given matrix has as the number of dimensions is typically small we can use modern vector instruction sets to compute each row-wise operation with a couple of vector instructions instead of a sequential loop overall coefficients to use vector instructions we automatically generate vectorized overflow checks as visible in this figure this works best when we use an element type with 16-bit as abx-512 provides instructions to compute both the hybrids of a multiplication as well as a saturated add for larger integer types the calculation of overflow checks is possible but becomes increasingly closely hence transmission computing is crucial to run as often as possible in 16 bit we also introduce algorithmic optimizations to keep values small each pivot step may increase the values of coefficients but often these new coefficients may be scaled by the same constant factor we identify the shared scaling factor by computing the gcd of all coefficients and remove it by dividing all coefficients by our gcd this helps to keep the coefficients small and allows us to use more of the faster low bit width variant of oils and black solvent unfortunately computing a gcd is slow the classical approach would sequentially compute the gcd using expensive modular and division operations we now introduce what we call the prime gcd algorithm an algorithm that is specialized for small coefficients and allows us to efficiently use fast parallel vector instructions instead of storing coefficients in two's complement representation each number is represented by a bit vector where each bit represents a prime factor for example the number six is represented as bit vector indicating a prime factorization of two times three computing a gcd in the prime factor base is fast it requires a simple bitwise boolean end over all operands to run it on a vector of coefficients we translate the vector into prime base using a lookup into a precomputed table perform a fast horizontal vector and and divide using a simple xor as a result we replace a costly sequence of divisions with a couple of fully parallel vector operations we also exploit the third property of observed sparsity and i invite you to read about our new small sparsity format in our paper now let's see if our work actually improves performance for this we look into two parts first we check the actual core simplex pivot loop if we compare 16 32 and 64-bit native operations with gmp's arbitrary precision arithmetic we observe a huge benefit only by using fixed size computations we also observe that the use of 16 or 32-bit vector instructions scales well if we have more columns we get very notable speedups for 64-bit that's not the case because again overflow checking is expensive our prime factor gcd is also fast if you compare the runtime of the unoptimized code at the top and the optimize code at the bottom we see that we move from around 60 cycles down to around 15 cycles by normalizing a constraint in combination we see an order of magnitude speed up both when running hot and cold caches while these speedups are impressive we only optimized a very small part of the algorithm the question is now does this translate to higher level operations in this work we only picked a single high level operation koilesk and ported it to our fast simplex solver we then extracted 25 000 test cases from poly ppcg and haystack the cordless operation with all optimizations spotted in dark green shows from 0.5 x to 10 x b type combined this is a 3.2 x mean speedup over gmp we also observe for the sum of all the execution times a 6.7x difference between our approach and gmp while the sum of execution times metric is atypical it is an interesting metric for compilation tasks where the longest running task typically dominates compile time let's conclude we analyzed the characteristics of simplex doubles typical to prolial compilation and found matrices to have small coefficients low dimensionality and high sparsity we then exploited these properties by implementing a transposition simplex server where we vectorize the central people loop our evaluation shows that this gives us both a fast simplex as well as a fast koalas operation we believe that our work is a big step forward we take only a very first step it's an open question if other operations would benefit and it's also unclear if the data structure transposition arithmetic will both scales to full political compilers would be excited to see future work towards high performance integer set arithmetic for compilers being built on top of our work you 