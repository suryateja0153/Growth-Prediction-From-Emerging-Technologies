  JOHN KUBIATOWICZ: Say-- [WHISTLES]---- can somebody prop open that door? I don't think you can actually get-- oh, maybe you can. Can you get in? STUDENT: Yes. JOHN KUBIATOWICZ: All right. It wasn't working before. Maybe you can get it now. Come on in. [WHISTLES] Berkeley time has arrived. Hello? Sit down. Now, welcome back, everybody. Welcome to CS162 again. Looks like we're doing okay from a seating standpoint. So if you remember last time, we got started on trying to decide what an operating system is, and we came up with at least three things-- the fact that it's refereeing resources, it's providing clean virtual abstractions, and it's giving a bunch of common services. And what we said was, essentially, that operating systems can mix and match these. So not every operating system provides them all, but most of the ones you might have for your laptops definitely do. Shh. Settle down, folks. I know this is so exciting for you that you got to talk, but-- the other thing that we talked about was the fact that hardware is, by nature, very complicated. And the interesting stuff is not a processor, which is here, but things that are in the South Bridge area down below there, and that's all of the I/O. So if you are an operating system designer for some new piece of hardware, it's going to probably tie in down here, and you're going to have to write device drivers. And device drivers are where all the bugs are. And we'll talk more about that as we go forward. But the complexity is something that's got to be tamed. We've got a lot of things-- memory channels, high-speed I/O, direct media interfaces, which give us all of these things-- disks, USB, Ethernet, audio, PCIe, et cetera. Those are all the really interesting things. When you think about your laptop, what do you think about? Well, maybe if you're a computer architect, you think about how cool it is with the number of cores and out-of-order execution, but probably you think about the bottom things. And those are the things that the operating system really has to help provide abstractions for. Along those lines, in terms of complexity, we also briefly looked at this graph of growing complexity. And you notice here that the Mouse Base Pairs is out here at 120 million items, and gee, a modern car is up there. So if you wonder why the NTSB gets called every time a Tesla crashes, it probably has something to do with that because of the number of lines of code. The other thing is it always advances forward. So if you look at one version of Linux and you look at the next version of Linux, there's a lot more lines. So code-- it grows. I've very rarely seen code shrink. And of course, cars are very complicated. So how do we tame this complexity? And really, this is where we ended up last Time it's every piece of computer hardware is different. Different CPUs, there's many different instruction sets. I didn't put RISC-V here. That was probably what you talked about in 61C, mostly. Different amounts of memory, disks, different types of devices-- mice, keyboards, sensors, fingerprint readers-- different networking environments. And you start asking these questions, which is this is literally where we ended up last time. Does the programmer have to write a program that does a bunch of different things simultaneously? Well, yes, but it's got to do so in an environment that deals with all these differences. Does every program have to be altered for every piece of hardware? I hope not. Every one of you who have the same laptop probably have slightly different hardware configurations, and that would be really painful if we had to change our program for every laptop it ran on. And of course, the other questions are things like, do faulty programs crash everything? Well, that would be bad because we have a lot of faulty programs. How many people have ever written a faulty program? Those of you that didn't raise your hand are probably in the wrong class. [LAUGHTER] And also, does every program have access to every piece of hardware? Hopefully not. There are some pieces of hardware that are really key. Like if every program could alter the timer and stop interrupts, we're going to have a big problem because suddenly, the scheduling doesn't work. And we're going to start talking about things like this today. So really, what we talked about was the need for what we're going to call a virtual machine abstraction, and that virtual machine abstraction takes your 61C knowledge of processors, memories, disks, networks, machines and turns it into virtual versions. So we're going to talk about threads. We're going to talk about address spaces. Not today, but we'll also talk about files and sockets. We'll touch on processes today. So really, the operating system takes this messy hardware down here and turns it into something that's hopefully programmable. And the goals really are things like remove software and hardware quirks. We're fighting complexity. Optimize for convenience, utilization, reliability, basically helping the programmer. And for any OS area-- file systems, virtual memory, networking, scheduling, you name it-- you start asking these questions like, what's the hardware interface? What software interface should we provide? And as you become more skilled at your OS skills, as your OS skills develop, you're going to start knowing to ask those questions. What is the hardware interface? How should I virtualize it? What API am I going to give to the programmer? So OS really has this goal of protecting processes in the kernel. So we want to run multiple applications and keep them from interfering with each other or crashing the operating system, keeping them from interfering with or crashing each other. This is your favorite blue screen of death. Now, these used to happen a lot. They happen a lot less frequently. But I will tell you that I had a brand new machine that I got a blue screen of death on six months ago, and that was because of some weird interaction with device drivers. But we basically don't want this happening, especially if we have, say, that Martian Rover we talked about. If the Martian Rover is busy just discovering life on Mars and it gets a blue screen of death, that's probably bad, and the Martians laugh all the way over the next hillside. So really, part of this interacts with virtual machines. And you're going to get to know a lot about virtual machines of many varieties this term. But the idea of a virtual machine in its simplest form is virtualizing every detail of a hardware configuration so perfectly that you can run any operating system and many applications on top of it. If somebody were to come up to you in a party and they'd say, hey, let's talk about virtual machines-- that happens all the time, right? They're probably meaning this. This is like the basic, we're going to virtualize everything, and I can load any operating system on top. And there's many different ones. VMware Fusion, VirtualBox, et cetera, Xen, Vagrant. You name it, they're there. The basic idea of this has certainly been taken on by a lot of Apple machines. How many people here have a Windows configuration running on their Apple machine? A few of you. That's convenient, right? You got two different operating systems going. It provides isolation, gives you complete insulation from change. So you could, for instance, if you wanted to, keep running your old operating system, even when the newer, more secure version comes out. It's the norm in the cloud. So basically, whenever you interact with the cloud-- let's say EC2, whatever-- you're probably interacting with a virtual machine instance. It's got a long history. A lot of people tend to forget that virtual machines were, in the '60s, first came up and they got revitalized in the '90s. And all our work in this class and your projects are going to take place inside a virtual machine. How many people have started homework 0? Okay. How many people got their Vagrant running? How many of you are vagrants running Vagrant? No. [LAUGHTER] So great, so you're going to get a lot of experience on virtual machines in this class. Another way to look at this is this idea that I might have a host operating system-- or in some cases, something that's thin. It's called a hypervisor-- and then a bunch of different guest operating systems running on top, complete with all of the code for those operating systems-- all the binaries, et cetera. So one view of this in one instance of virtual machines, and we'll talk more about this in later in the term, is that this operating system might have no idea that it's in a virtual machine. It thinks it's actually running on the raw hardware. And that's one way to fool the operating system into thinking it's got full access to the hardware. Other types of virtual machines, you might do what's called paravirtualizing-- we'll say a little bit about this later in the term-- where the guest operating system has some clue that it's running on top of something else. The other term that comes up a lot are containers that are virtualizing the OS. Typically, Docker is something that oftentimes you run into. Anybody do cloud-level development with Docker? We got a few of those in here. So there are many different types of containerization. This big one in the middle is the simplest one that's gotten very popular, where you package up a bunch of apps and they're still all running on the same operating system, but each app is given its own libraries, and environment, and so on, and so it thinks it's got a unique version of the operating system just for it. But in reality, often, they're running on the same host operating system. And I won't say that this is the only way that people use the word container because there are native Docker kind things, and then there are hypervisors, which sometimes the virtual machine running on a narrow hypervisor is also called a container. Basic tool, which we'll talk more about as we get forward today, is really what we call dual mode operation, where we're going to make some distinctions in the hardware between a user and kernel mode that's essentially going to allow us to start protecting things, so that maybe we have the container environment, or the virtual machine environment, or even the operating system is given full control and a lot of other things are not. And today, we're going to talk about, what is it that the hardware can protect against so that we get more secure environment? And I'll say more about dual mode later. So that's okay. The basic Unix structure, which you've all maybe seen, looks something like this. We didn't get to talk about this last time. I just wanted to flash this up now. I'm also going to flash it up at the end of the lecture. But basically, what we typically do is we divide something we call user mode from something we call kernel mode, and the kernel mode is that code that gets 100% access to everything, hardware-wise, and the user mode is that protected environment that's virtualized in some way. And every processor that supports a protected mode has these two options. Just out of curiosity, how many actual levels of protection does the x86 have? Does anybody remember, or do they talk about that? STUDENT: [INAUDIBLE] zero through three [INAUDIBLE].. [AUDIO OUT] JOHN KUBIATOWICZ: Very good. So I was actually only looking for the four, but yes. We'll get to the negatives, as well. So there are four basic ones. And basically what happens is 0 is kernel mode, and 3 is user mode, and there's a couple others that are in there for military spec. And then when we start getting into hypervisors later in the term, we'll talk about the negative ones, as well. So even though most pieces of hardware have two, a lot of other hardware has many more, and we'll talk about why. Our goal for today is to talk about four fundamental OS concepts to get ourselves going this term, and one of them is the notion of what a thread is, which is really an execution context. And I'm going to show you that this is a virtual version of what you learned about in 61C. So this lecture is going to partially have hope that we're starting to refire those neurons from 61C, as well. We're also going to talk about an address space, either with or without translation, which is really the set of memory addresses that a given piece of code is allowed to access. And then we're also going to talk about how we can throw translation in there and make it secure. We're going to then talk about what a process is, which is really an instance of a running program, protected address space in one or more threads. And then we're going to revisit this dual mode toward the end of the lecture. So this is where we're going. You ready? STUDENT: Yeah. JOHN KUBIATOWICZ: All right. Yay. So the bottom line in all of this is really running programs. And so I wanted to put up a little graphic, so here's George. He's been working late. And he's trying to get his program working, and so he's written some code. And that code, because it's C, is going to have a main() function. It's probably in foo.c. And it gets compiled into an executable, a.out, for instance. There may be a linking process where you pull in your libraries, and you get to do a little bit of that with homework 0 to get going. And now this executable is just laying there somewhere on the file system. This is a program, but it's kind of useless because it's not doing anything. It's a bunch of bits. And just to really re-emphasize that for a moment, this program is in C. This is in some machine code related to the processor you're trying to load this on. So the processor you guys are dealing mostly with this term is x86, and so these would be x86 instructions. And one of our sections-- in fact, the section next Friday-- not tomorrow-- is going to talk about x86 in a little more detail. And then what we have to do to actually make this thing do something interesting is we load it into memory, and we toss the processor at it, and we say, go. So until it's actually loaded into memory, it's not running. It's just sitting there. And once it's in memory, there's a certain layout that the operating system is going to go with for that program. And I'm giving you an old layout for the moment here, where actually, all of the OS data structures are mapped at the top, and then the stack grows downward and the heap grows upward, and then this data and instructions, which are the thing that came out of the compiler and linker, are actually at the bottom. And so when you do malloc() to allocate new things, the heap is growing upward. And as you do recursive calls-- because we all know that we want to compute Fibonacci of 5,498 because that's something that they start you off right away-- the stack is going to grow downward. And the way we run out of memory, of course, is if the two of them run into each other. So just out of curiosity, does anybody know why we don't want to have the OS mapped in this way, although they did it for years, and years, and years? Yes? STUDENT: [INAUDIBLE] [AUDIO OUT] JOHN KUBIATOWICZ: Yeah. There was something even worse than stack vulnerabilities. There's something called Spectre, which came in 2017, '18, where just by having the OS mapped here, you have a secure hole in your operating system. But forget that for now. There is no problem. Okay. So what we're really talking about here-- that is a good enough story that I will definitely tell it to you because Spectre, Meltdown are things that happened in 2017 and got revealed in '18, and basically blew away computer architects because nobody was quite expecting them to be that bad. And we'll have a good story there I'll tell you later. Forget it for now. There are no problems. So what we do is we're going to load the instruction and data segments of the executable file into memory to actually run things, and then we're going to create the stack and the heap, and we're going to then transfer control to the program, which means this happy little register here called the Program Counter, or PC, gets pointed at the starting instruction to your program and then you tell the CPU, go. Hopefully, there's some things here that are bothering you about this. Like for instance, if there's only one processor and we're pointing the program counter at where the user code is, then how is the operating system running? Because the operating system is code, as well. So if that was bothering you, it's okay, because I got to explain it, right? And obviously, the other thing that's going to happen is once we get this thing running, we also got to provide services to the program. So what that means is there are ways to go into the operating system and ask for opening a file, or closing a file, or reading data, et cetera. Those are services the operating system's going to provide. And so without those operating system services, this program of yours is probably even more useless, even though you got it running. Because you got it running, but it can't talk to the world because there's no I/O yet. Well, that's what the operating system provides. So these are all things to look forward to. But for now, the bottom line in what we're trying to do is run programs, because that's what a programmer cares about. And our first project is really going to be looking at user-level aspects. So what does the user do when they deal with an operating system? Questions? Everybody with me? Good? Questions are okay, you know? So and we got to protect the OS and the program itself as we're doing these executions. So 61C may have been a long time ago for you guys, but remember that there was a discussion of the processor, and we had this instruction fetch, decode, execute cycle. And I want to remind you of it. So there is a special register in the CPU processor called the program counter. And what does it do? It points at an instruction in memory. And so your code that you got from the compilation is all here. And what happens is the processor fetches the next instruction to execute. Now, what does that mean? That means it reaches in and it grabs some binary 1's and 0's that represent the next instruction. I didn't show you somehow writing 1's and 0's, I showed you writing main() and the C code. So what happens is the compiler, and assembler, and linker, and all that stuff turns what you asked it to do into these instructions, such that when they're loaded in here, the processor knows what to do. Those bits are now decoded, so if you said you were going to do an add, it figures that out. And then the actual execution part of the processor takes over, and there are some registers in there, which might actually have the two arguments you're trying to add together. The decoder tells this guy to basically take two registers, put them in the ALU, tell the ALU to add, let's say. There might be some results that get written back to memory, or results that are pulled from memory, and then we go back and we do the next one. And so that fetch, decode, execute happens over and over again, and when we do that, now we got a program that runs. Does that sound familiar? Are there any questions on that? Mostly, I think, you were dealing with RISC-V probably, which is a nice, clean pipeline. And so that was a good instruction fetch, decode, execute cycle to have been exposed to first. Now that we start talking about x86, things get more complicated, but this is essentially what you should think about in your mental model. And of course, the next PC is basically I have a pointer to memory, and this next figures out how to move it down to the next full instruction, which is probably more than one byte. So the first OS concept that we want to talk about is a thread, and it's really a single unique execution context, which is what? Well, let's think of it as like a miniature virtual 61C processor. It's got a program counter. It's got registers. It's got flags, and a stack, and memory state. So if you think about your little 61C processor you heard about, and you virtualize it, now you got a thread. And a thread is executing on a particular processor or core when it's resident in the registers. What does that mean? Resident means basically that the registers have the root state of the thread. So for instance, the processor now has a program counter and maybe even the currently executing instruction for that thread. So the PC points at the instructions in memory. The instructions are all stored in memory, as I said. It includes intermediate values in the registers, so they could be like actual values-- integers, whatever-- or pointers to values in memory in the registers. There's a stack pointer-- everybody remembers the stack, I hope-- which is in memory, as well. And the rest of everything's in memory. So basically, the registers have a special bit of state that points at the right parts of memory in order to make this thing work. And we're resident and executing when we've loaded those registers. And when they're not loaded, then it's not executing. So it's suspended when it's not loaded. And so what does that mean? It means the processor is pointing at some other thread. So thread A is suspended because thread B is running. Now, I realize that may sound like I'm being overly pedantic there, but I wanted to point that out. So you got a thread? Thread A is no longer running because the program counter is pointing at thread B's instructions in memory. Everybody clear on that? And now you can start to see what we mean by virtualizing, because there is the thread that's currently loaded, but there's potentially this space of other threads. And by deciding where to put point the processor at, suddenly, we could have many threads that look like they're running simultaneously, even though they're not. If you don't quite have that yet, it's okay, because I'm going to look at that in another way. But let's now remind ourselves yet again what happens during program execution. So here's a loaded thread from 61C. You've got a bunch of registers. There may be some integer ones. There may be some floating point ones. There's a program counter. And the execution sequence are fetch instruction at some PC, wherever it's pointed to, decode it, execute it, write the results, get the next one, and loop over and over again. And here we go. PC points at first instruction, second instruction, third instruction, fourth instruction. And if I happen to stop all this and point the PC at some instructions in a different thread, all of a sudden, I'm doing something different. So maybe on this slide, I'm computing pi to the last digit. I go over. I'm computing e, also to the last digit. Takes a long time. You dealt with RISC-V in basically 61C, so there was a bunch of registers. And notice they all had particularly important things that they were doing. So like x1, the return address register, is used during procedure calls and returns. There's a stack pointer. There's a pointer to the frame. There might be temporary registers used for linking, and saving, and executing. Does this ring a bell to everybody? A little bit? And notice off to the side here, I say whether these are caller or callee saves registers. So when you go to call a procedure, you have to decide, do I save the registers now or does the guy I'm calling have to worry about saving them? Ringing a bell? Ding. So we get to deal with a much more, I would say, esoterically terribly complicated state with the x86. So what's nice about RISC processors is they were re-did from scratch here. By the way, RISC is Reduced Instruction Set Computing, which Berkeley is famous for. But this x86 processor, which is the most common processor in the world, has more complexity to it, but it also has register state that you can think of in terms of program counter, stack pointer, et cetera-- registers that you can add and so on. And so when you approach a brand new processor, part of it is figuring out, okay, what are the registers that represent that live state that has to be loaded in order for the thread to execute? It could be simple, like the one on the left. It could be more complicated, like the one on the right. But fundamentally, in order to run a new thread, I've got to save out old registers, load new ones, and now I'm running a new thread. And it's that multiplexing that's going to get us what we need. Questions? Okay. And in section next week, actually, we're going to talk a bit about x86. So here's what I just said, kind of. I just said, look, we can have many threads that appear to be running at the same time, and they've all got their place in memory with a stack that grows down, and a heap that grows up, and so on. At the same time, just by doing this switching trick, we run it for a little while, and then we run the next one, then we run the next one. And that switching trick is really an illusion. And one of the things I said last time is that one of the parts of what the operating system does is it's an illusionist. And how does this particular illusion work? Well, we want to make it look like we got a bunch of these virtual CPUs all running. And in fact, how do we do this? We multiplex in time. So this is just what I said. We vCPU1 for a little while, then 2, then 3, then 1, then 2, then 3. And we do this with incredibly bright magenta, cyan, and yellow, except they look better on my screen than they do up there. Now, threads are virtual course here. I'm going to stop for a second. This is a concept I want to make sure that everybody's got before I go a little further. Any questions? Yeah? STUDENT: So when the state of one thread is-- JOHN KUBIATOWICZ: That's a great question. Is the state of one thread preserved in registers while the other one runs? That's your question. Usually, what happens is the state of the registers are actually stored in something called the thread control block, which I'll say in a couple of slides, in memory. So what happens typically is you unload the registers into memory of the first thread and you load other ones from the second thread. So it's often in memory. That much being said, by the way, everything I say has an exception in many cases. So there are special embedded processors where there are a set of registers that are kind of off to the side that make threads switching much faster than storing to memory. But in general, for this next several lectures, think of this as being stored out to memory and pulled back in from memory. Question. Yes? STUDENT: Do all the threads get an equal amount of time? And [INAUDIBLE] decide that? JOHN KUBIATOWICZ: Good question. Do all threads get an equal amount of time? Yes, no, maybe. So that's called scheduling. So the question of who gets how much time is another thing that we said that operating systems do, which is they decide on resource control. And so this question of who gets what amount of the processor actually depends a lot on what you're trying to do. So for instance, this cyan one up there, the blue one, may represent something that is so high priority-- like say, your brakes in your car-- that it's got to always be able to run when it needs to. But mostly, it's sleeping. So in that instance, it's mostly getting zero CPU, but boy, when it needs it, it goes. Maybe the magenta and the yellow one are getting equal amounts, or maybe magenta is really higher priority than yellow, so magenta gets twice as much as yellow. These are all scheduling questions. And in fact, we're going to have a lecture or two on scheduling, and we'll even talk a little bit about real-time scheduling on how to meet those kind of constraints, like don't crash my car because I put the brakes on. Good question. Yes? STUDENT: So to be clear, [INAUDIBLE] [AUDIO OUT] JOHN KUBIATOWICZ: Yes. So the operating system is the thing both doing the switching and making the higher-level policy decisions about who gets what. Yes, question? STUDENT: When you say [INAUDIBLE] does that mean that [AUDIO OUT] JOHN KUBIATOWICZ: So that's a good question. So does virtual core mean that the thing running can't tell that it's virtualized? So that depends a lot. And so I'm glad you asked that question early in the course here. So the notion of virtualization doesn't necessarily mean you don't know you're being virtualized, but rather, you don't have to act on the fact that you're being virtualized. So I can go ahead and happily run along thinking I've got the whole CPU, and I don't have to do anything different than if I did have the whole CPU. That's typically where we talk about virtualization. Where it gets really tricky is if you set a timer, and you run the thing, and you're getting swapped out a bunch, and then you look at the timer, what you'll find is it takes a lot longer maybe than you expected for something to run if you thought you had the full core. So there's always some way to know. And the question gets interesting of, do we have as a feature of the operating system that we make it really easy to understand what level of virtualization we got? And that's, again, a scheduling question. Good question. Great. I like this. Are there any other questions, or should we go forward? Yes? STUDENT: [INAUDIBLE] JOHN KUBIATOWICZ: So typical limitations for how many threads are often either you run out of memory, because every thread needs a little thread control block plus some stack and whatever, or so many are running that you're essentially thrashing and spending all of your time switching and not any of it computing. So good. Yes? STUDENT: Can you clarify what a core is? JOHN KUBIATOWICZ: So I am intentionally confusing processor and core because think of a core as a processor or processor as a core, because today, what happens is you've got what's called a multicore chip, and it's got many of these things, each of which can do what I'm talking about. So if you have four cores, each of them can multiplex threads on all four cores. So a core is exactly like a 61C processor. Processor, core, core, processor-- for now, you can think of them as the same thing. Yes? STUDENT: So [AUDIO OUT] JOHN KUBIATOWICZ: [LAUGHS] I will talk about hyperthreading next lecture, I promise. I don't want to get down the hyperthreading issue right now, it's one too many levels of complexity. But I promise I'll put a slide up next time, all right? It basically means that parts of the pipeline are actually being multiplexed on the fly in the middle of instructions, rather than just between instructions. So the contents of a virtual core-- now hopefully, we're starting to get to what that means. It's a program counter, a stack pointer, some registers. And you might say, where is the thread? And we already answered that. It could be either on the real physical core, if it's running, or it could be saved in a chunk of memory typically called the thread control block. Consider this scenario. We've got virtual CPU1 is running at T1 and at T2, virtual CPU2 is running. You might say, well, what happened to transition from CPU1 to 2? Well, the OS took over. Somehow, it saved out all of the stuff for CPU1, it loaded in the stuff for CPU2 from the thread control blocks, and it said, go. The interesting question there becomes, what triggered this switch? And we'll talk more about that in a couple of lectures. But it could be a timer went off. It could be that CPU1 said, okay, I'm done for a while. It could have yielded. It could be I/O. It could be many things we talk about. So that's something you don't have to worry about quite yet. So what's the OS object representing a thread? We already said that it's typically called a thread control block, which holds the contents of the registers in memory-- think of it as a structure-- when you swap out. In PINTOS? Hopefully, those are you are really getting excited and maybe even have started looking at some of the source code. You can actually look at thread.h and thread.c to start seeing what represents a thread in PINTOS, which is our operating system. So there's actually code that you can start looking at. Some administrivia. Oops. I didn't have this setup to go forward, so this is fine. Start homework 0 immediately. It's due next Friday, which incidentally, not accidentally, is also drop day for this, because it's an early drop day class. You have to make a decision. You get to do all sorts of stuff with homework 0. In addition to reminding you what C is like to program, you get to get your accounts set up, you get your virtual machine set up, you get familiar with some of the 61C tools-- 162 tools, that's what I meant-- you get to submit to the autograder, and et cetera. So I would recommend getting going really soon because that can help you figure out, for instance, when you go to section on Friday, what to ask. And just to say something about that, if you look, we've got 15 sections in this class on Friday. They're all up on the website. For the first two weeks, you're welcome to go to any section you like, but definitely go to section. Just because you're not assigned one doesn't mean you should skip this Friday and next Friday because this is an important way to get going. These first two weeks are the, how do you get going in this class by remembering the stuff that you forgot? So don't miss that. And also, Monday is an optional review day for C. Right now, it's scheduled for 306 Soda from 6:00 to 8:00, and because the TAs there will be writing on the board and going through some examples, we can't really webcast or handout slides for it. But how many people would be likely to go to this? It's going to talk about basic C things and get you up-to-speed. Okay. That's probably about a quarter. Everybody who wants to or feels like they're not sure about their skillsket in C should go because you can get your questions answered. We're going to try to remind you of some things that you probably forgot, like what's a pointer, and what's big-endian and little-endian, and some of these things. Midterm conflicts. So we're still getting room assignments and stuff, and so actually, we'll leave that for later. But if you look on the schedule, we've already shown you when the midterms are. And right now, they're tentatively 7:00 to 9:00. Let me know if you think you've got a conflict going forward that doesn't involve an overlapping class and we'll figure out what we can do. The other thing that we've got here is office hours, 1:00 to 2:00 PM Monday and Thursday. So the reason I do it that way is because some people have Mondays and some people have Monday, Wednesday, Friday classes, whatever. Some people have Tuesday, Thursday classes. So I try to make sure that I've got something in either domain. Let me know if that's not working for you. Send me an email and I could certainly come up with alternatives under some circumstances. And then the webcast you get by logging into CalCentral, and it's a bit delayed, just you know, so it's not posted immediately after the lecture. Questions? The most important thing? Homework 0. Second most important thing? Go to section Fridays. Third most important thing? Don't forget the review session if you feel like you want to learn a little bit more about C. I told you about this last time. Just be careful, okay? Don't share code or test cases. Don't copy or read other people's code. Don't get them off the internet, et cetera. That will just cause problems. So the second concept-- so we've got a thread. The next thing I want to talk about is an address space. I gave you this figure here, which kind of shows the bottom of the address space is all 0's and the top is all Fs. This is the simplest address space. And the stack goes down and the heap grows up. And why do I do with that way? Why don't I have the heap and the stack grow in the same direction? Yes? STUDENT: [INAUDIBLE] [AUDIO OUT] JOHN KUBIATOWICZ: There you go. So what we're really trying to do is we have no idea how much stack versus heap, and so this gives us the most flexibility by growing down and up. It depends a lot on the processor and the compiler calling conventions as to whether the stack grows down and the heap up, or vice-versa. But anyway, address space is really the set of accessible addresses. So if you've got a 32-bit processor, typically, there are 2 to the 32nd, or four billion, addresses that can be accessed. And those are byte addresses these days. And that's the address space. So if you think about the program that you started running, it's got 32 bits worth of addresses it can access. And that's kind of what's assumed by certainly your 61C processor. The simplest version of it is, yeah, anything I go after that's 32 bits is a valid address. And what happens when you read or write that address? Well, in 61C, you've got memory there. You read and write a byte, or an integer, or whatever. How many bytes am I reading when I read an integer, typically, in a 32-bit processor? STUDENT: [INAUDIBLE] JOHN KUBIATOWICZ: 4. Good, good. Just checking. So but now that we're becoming sophisticated about our operating systems, other options that could happen is it might allow me to read but ignore my writes. Or some addresses might actually cause I/O to happen. That's called memory-mapped I/O. It's possible that there's part of this address space that when you read and write, it actually causes a byte to go out to some network or causes something to happen. That's called memory-mapped I/O. Maybe it causes an exception because for instance, your stack has gone this far, and your heap has gone this far, and you have tried to read and write in the middle of this address space. You get an exception at that point because it's an invalid address. It's possible, also, that there's a chunk of memory called shared memory where, when you read and write from that, two processes-- we'll talk about what that is in a moment-- communicate with each other as a result. So there's a lot of interesting things you can do with an address space other than just, there's a memory value there. But if you don't know anything more, your first assumption probably is this first one. And as we get further in our operating systems discussion, we'll show you why other ones are more interesting. So an address space in pictures. Here we see that, yeah, we've got this layout. And the program counter points at an instruction somewhere in the code segment, and the stack pointer probably points to the bottom of the stack. Why is that? Well, because that's the point at which, if I make another call recursively, the stack grows down, and if I return, I move the stack pointer up. And so all of the action in the stack, so to speak, is happening right there. So what's in the code segment? What's in the code segment? STUDENT: [INAUDIBLE] JOHN KUBIATOWICZ: Code. Yeah, good. What's in the static data segment? STUDENT: Static data. JOHN KUBIATOWICZ: Static data. What is static data? Yes? STUDENT: Strings? JOHN KUBIATOWICZ: Yes, globals that are declared at the top, and they can be strings. What's in the stack segment? Ooh, that one maybe you have to reach back in your memory. STUDENT: [INAUDIBLE] JOHN KUBIATOWICZ: Well, the stack is on the stack segment, but that one's a little harder to just say the stack. What's on the stack? STUDENT: [INAUDIBLE] JOHN KUBIATOWICZ: Yeah. So if I call Fibonacci, your favorite thing of 61A, and B-- and D, if it existed, would have a Fibonacci written in some new language-- is that I call Fibonacci of something and that's the value that the function gets at the beginning. So if I call Fibonacci of 12, maybe the input variable is 12. And then when I recursively call Fibonacci of 11, what happens is I move the stack pointer and 11 goes on the stack, then 10 goes on the stack, and then 9, and so on. So the stack frame is actually keeping track of those local variables that are arguments to your procedure. And so you should remind yourself of how that works in your 61C notes, because we have to worry about saving and restoring pointers to the stack when we start virtualizing threads. What's in the heap segment, other than heap? That one gets less interesting. STUDENT: Dynamically-allocated heap. JOHN KUBIATOWICZ: Great. So the heap represents things that I execute malloc() on, where malloc() is a way to get a new chunk of memory for something like an object. So all of the things that I allocate with malloc() go on the heap. Good. The other thing that's on the stack, by the way, is any variables that are local variables in a procedure. So if you have a procedure and inside, you say int x that you're going to use for a loop, x typically has a spot on the stack. The previous discussion of threads that we've gone up to now-- what happens here? Well, all of the virtual CPUs share non-CPU resources. Like they all share the same memory and the same I/O devices. Every thread can read or write the memory of every other thread. In fact, what you've seen so far is perhaps wherever the operating system is, a thread can overwrite that, too. Now you might say, is this usable? And the answer is, well, it depends on what you mean by usable. So-- [LAUGHTER] --very early days of computing, there was no protection of the address space. Everybody shared the same address space. I grew up in the days of gee, I could modify an MS-DOS driver by just writing some code somewhere. And suddenly the operating system, whenever you asked it to print to the printer, would go compute pi to the last digit instead. So that's a way to do something. That's one structure for a processor. It's also something that you might use if you didn't want to spend a lot of hardware for protection. So a lot of embedded apps, if they're really small processors, like something that might be in a little IoT sensor, might actually operate this way, where the operating system and the thing that's reading the sensor share an address space. And boy, if there's a bug, you could overwrite the operating system. Some of the original versions of Mac and Windows did that. However, there's a little bit of a risk element here. Notwithstanding the notion of a malicious entity coming in and trying to screw everything up, just bugs. A simple bug can actually screw everything up. So Windows 3.1-- what was interesting about that is it had windows, and a mouse, and all that stuff that you're used to, but you could write a program that you'd launch that would go into an infinite loop. And all of a sudden, every window froze. Why is that? Well, because there basically was no memory protection and there was no timer, so we didn't get the multiplexing that we've been talking about, either. So what can we do that's different than that? Well, we need to somehow protect the address space so that the address space of thread 1 and the address space of thread 2-- maybe they don't overlap or they don't allow cross-writing so that thread 1 can't screw up thread 2. Make sense? Somehow. I haven't told you how we do that yet, although you did get a little of that in 61C. But bottom line here is that the simple multiplexing we're talking about doesn't have any protection in it. The operating system, if we're really worried about reliability in the face of a bunch of issues like bugs, really must protect itself from user programs. So it's got to be possible for the operating system to prevent a program from overwriting it. And then, of course, we got to limit the scope of what a thread can do. If you've got two threads, one of which is a secure thread that knows a bunch of cryptographic keys, it better not be the case that some other thread can reach in and grab those keys out of that protected environment. Privacy is another issue. There might be very private data. Keys are that way, as well. And then fairness. If we don't have the right protection, then perhaps a thread can screw up the scheduling algorithm and get much more time than it was supposed to. And so basically, user programs protected from each other and the OS protected from user programs, et cetera, is an issue. So user programs can impact each other. So we got to figure out something here, and it really boils down to we've got to start with the address space because it's a little bit too permissive right now. So what can the hardware do to help the OS protect itself from programs? Anybody have any ideas? Go for it. I see. So you'd tag memory to know whether it's acceptable or not acceptable for what? A particular thread to access? Okay. I might buy that. What else? What you just said there is pretty generic. That actually covers a wide variety of mechanisms, so yeah, go ahead. STUDENT: [INAUDIBLE] isolated storage unit. JOHN KUBIATOWICZ: Mhm, some sort of isolation. Okay. What else? Anybody? Yeah? STUDENT: [INAUDIBLE] JOHN KUBIATOWICZ: Yeah. So make sure that when the access is happening, is it a user program or the operating system? Yes. Great. Okay? STUDENT: [INAUDIBLE] JOHN KUBIATOWICZ: So each thread has a unique space. Yes. Yeah, go ahead. STUDENT: Have multiple [INAUDIBLE] the hardware [AUDIO OUT] JOHN KUBIATOWICZ: Yeah, so exactly. You could actually, if you wanted and had the extra hardware, you could devote a whole core to the operating system and make sure that nothing else ran on it. Now the issue with that, of course, is not only are you devoting CPU resources, but you've still got to put a protected container around it or something. So if you had them completely separate, then maybe you could do that. Good. So let's talk about a very simple thing we can do, which is base and bound protection. And this actually is related to some of the early processor protections that were available. So what we've got here is the address space-- and by the way, I flipped it around, with 0 at the top and the higher addresses at the bottom here, just to see if you were paying attention. But if you notice, what we want to do is we're going to load this program-- the yellow thing-- into memory. And here's the operating system in gray. And what we want to do is somehow prevent the program in yellow from messing with the operating system. That's going to be our first goal. And if you look, what we're going to do is we're going to have a new register. Remember the registers we talked about already were the program counter, the stack pointer, and we talked about some integer and floating point registers? This is a different one. This is called the base. And the base has got, loaded into it, the address of the beginning of the code, the yellow stuff, and the bound says something about, where's the top of this? So these are actually just two addresses. And what we can do is we can just make sure that when the program is busy running, what we're going to do is before we allow an address to go to memory, we first say, is it greater than or equal to the base and is it less than the bound? And if the answer's yes, go for it. So this is extraordinarily simple. And what I've done is I've done this in hardware and so I basically set this up so every address coming out of the processor is checked against the active base and bound. And If it's violated, I do something like I prevent it, or I cause a trap that kills off the yellow program. Everybody with me? Extremely simple. And actually, as a computer architect, if you were designing processors, what you might like to know is, how expensive is it to do this? And notice that I can do the base and bound checking in parallel, independent of the lookup, as long as I squash things out afterwards. And so this is really not going to impact my cycle time in any way. So any of those you that have taken 152 might think about that. But I want you to also notice something else I've done here. This yellow code that I've set up has been compiled and linked as if it starts at address 0. But notice I've loaded it in here to address 1000-- whatever, 10000. So what is an issue there that I might have to worry about? Yeah? STUDENT: The program itself tries to access something legitimately which is outside this base and bounds, specifically like less than the base. JOHN KUBIATOWICZ: Yeah. So notice if this code makes a jump to itself, and it thinks that it's jumping to something near 0, and it's doing it here and it tries to jump near 0, that's going to take me into the operating system. And that's not even a malicious thing, that's just broken. So there's got to be some process with base and bound where, when I load, I actually have to relocate-- is there a word that sounds familiar-- the code so it will properly run starting at 10000. Okay? So notice that here are the programs running at 1000. The addresses are translated at loading time by something called relocation, and we're protecting the OS and the program, because we can do this-- multiple programs can't get touched, either. But it needs a relocating loader. And there's no addition on the address pass, so it's fast. So I want to remind you of what relocation is. So 61C, you have, say, something like a jump in length. This is a call in assembly language. So we've already compiled from C into assembly. What you look at is when that gets generated into an object file before it's linked and loaded is it actually has the jump and link address, but then it doesn't really know what the address of the printf has to be. Instead, what it does is it puts in some special code so that the original thing that's on the file system. Is sort of generically set up so that when I actually load it, the process of loading can fix everything up so that it jumps only within itself at the proper address. So that's called relocation. And it's part of this compilation and loading process where we're relocating so that, say, these instructions run at the right place in memory. For instance, here, the code thinks that it was linked to run at 10000 instead of 0000. That's called relocation. And typically, the un-relocated version is what's on the .exe, let's say, that's on file system, but the process of the operating system loading it to be ready relocates it. This is a little complex, but workable. This will work. Once we've relocated the code, now I can trust that this code can't touch the operating system. So the operating system is now safe. And if I have other versions of code with different colors, the yellow can't touch the blue, or the pink, or the green, or whatever other colors I've got. Yeah? STUDENT: And is there a difference between how [AUDIO OUT] JOHN KUBIATOWICZ: Say that again? STUDENT: Is the difference between the base and bounds [AUDIO OUT] JOHN KUBIATOWICZ: Well, so when it's loaded in, you're asking, how do we choose base and bound? That's a great question. So when we start talking more about options in a couple of lectures, the problem with this-- this is very simplistic. This is for lecture two-- is I have to decide in advance how much memory I need. And when I need more, the best I can do is copy it all somewhere else to make a bigger chunk. So this is not really very desirable from a management standpoint, as you've already started to figure out, because I've got to pre-choose how much I've got. Yeah, go ahead. STUDENT: Given that the OS is just [INAUDIBLE] how does the hardware tell the difference between [INAUDIBLE] which has the authority to change base and bound and-- JOHN KUBIATOWICZ: Well, I mentioned very briefly, and will mention again at the end of the lecture, dual mode. So what'll happen-- I don't know if I'll get to my full example. We'll see. But what happens is when you're running in the operating system, there's a little bit in a register of the hardware that says, hi, I'm kernel, and the kernel is allowed to have the full physical address space, whereas when I'm running in user mode, that bit's off, and I only get to run restricted by base and bound. Yes? STUDENT: How secure is this? Can a program write to this base and bound register somewhere? JOHN KUBIATOWICZ: That's a great question, as well. So that basically is the same answer I just gave him-- basically, no pun intended. The same answer says that only if I'm in kernel mode can I change base and bound. So obviously, if I'm going to do this for protection, I've got to make sure that only authorized things, like the operating system, can modify it. Great question. Good. So here's a different option. So remember this problem that I had to relocate with the loader at the time I loaded? Notice how this is a little bit different. I actually put up an adder in my program address stream. So now, the processor is running with the addresses that think that it's starting at 0. And what happens is I add the base to the program address before I go to the physical address. So what I've just done here by that little sleight of hand of putting an adder in there, is suddenly I've got virtual addresses on one side and physical addresses on the other. And so what happened there is the program says 0010. I add the base, and now I'm 1010 da, da, da. And so this data here looks exactly the same, no matter where I put it in memory, because I'm translating stuff. And so the loader doesn't actually have to relocate to load this. So this is hardware relocation. Don't have to do it in software. And can the program touch the operating system? No, it's still protected. But in this case, notice that I'm checking the bound a little differently, but the bound is now a size, rather than an address. Questions? [INAUDIBLE] Can't touch other programs, either. So by the way, if you look at the x86 registers, which you will, there are a bunch of segments, and stacks, and so on. The segments, like the code segment or the stack segment-- that's what CS and SS are-- are literally base and bound with offsets like the PC or the extended instruction pointer starts at 0, but that 0 is wherever the code segment points at. So really, the one way to think of segments in the x86 is literally like this version of base and bound with hardware relocation and with a bunch of segments. So there's not just one base and bound, it turns out there's a lot of them okay, so there's at least six segments, depending on which version of the x86 you have. Yes? STUDENT: Are we using segments in this class, or are we just using [AUDIO OUT] JOHN KUBIATOWICZ: We are basically going to be ignoring segments pretty much. So if you set all the segments to 0, you kind of get the same thing. And then there's paging on top of that, but we'll worry about that later. Okay. Everybody with me on segments so far? Good. So another idea is we could do address space translation. And address space translation is a general version where the processor has general virtual addresses and we translate to physical addresses, but that translation can be pretty much anything. And basically, we can break the virtual address space into a bunch of pages-- 4 K is a good size-- treat memory as page size frames, put data in into any frame. And this is another thing that I think you talked about in 61C, but we might be getting to the limits of your memory. So the idea here is if you notice, a processor has a virtual address that's really some number of bits are the page number and some are the offset. And what we do is we take the page number to figure out which page we're interested in. That points to a part in physical memory. And then the offset points within the page, and that's how we get the final part of DRAM where things are. Now, don't worry if you don't quite catch this because we'll go into it in detail in a couple of lectures. But just suffice it to say that this gets rid of that problem that people were worried about-- well, how do I pick the right size-- because what we've done is we took the processor's virtual address and we divide it up into bunch of equal-sized pages that make management really easy. And we don't have the external fragmentation issue that we had otherwise. Okay? And they talked about virtual addresses, I think, like this probably in 61C. Right? Anybody remember these? Let's take a very brief break because I think everybody needs to stand up for a moment. But let's keep a short one. Sorry, I'm going a little slower today than I thought. Let's take like a few minutes and then come back. If guys can bear with me for another 13 minutes or so. Now, our illustrious head TA wants me to announce that there is a new skeleton for homework 0 that may fix a few problems people have said. A new skeleton, a new virtual machine. So they need to just pull it, right? We apologize. There were a couple of bugs in there. So if you just stop for a little bit and pull it, hopefully, they'll go away. Good. So what do we have so far? We have threads and we have address spaces, possibly with translation. And by the way, the "with translation" is the important part that we will use as we go forward. But for now, we can put them together and we're going to get a process. So processes are things that people sometimes, who see this for the first time, have a little trouble catching, but I'm going to try to walk you through it. A process now is an execution environment with restricted rights. And so really, it's an address space-- and I'm going to say protected address space-- with one or more threads in it. It owns some chunk of memory, which is its address space. It owns, by the way, file descriptors, file system contexts, a bunch of other kind of I/O things. And it encapsulates one or more threads sharing these resources, as I kind of said. That's a little bit duplicate there. So when you write a program, and you compile it, and you link it, and you start it running, that thing runs as a process, is the simplest thing. Now, more complex applications can fork or exec child processes. We'll talk about how that works in a little while. So there could be more than one process, but at least one. So when you run something, to say that again, in a modern operating system that's not a little embedded thing, you get a protected environment for that to run in. And that protection starts with the address translation we talked about. Why have processes? Well, I've kind of been alluding to this the whole lecture. They're protected from each other. So if you have two processes running, they don't have the ability to mess with each other's address space. That's our goal. And that translation-- base and bound being the simplest one I talked about but the general translation being more interesting-- prevents process A's threads and process B's threads from even knowing how to address the physical memory underneath because I translate it away in a way that, even though this process goes to address 0 and this one goes to address 0, they're both different address 0's because they point somewhere else because of the translation. And that's our fundamental idea here of how to provide memory protection. So threads are more efficient, however, than processes for some parallelism reasons, and we'll talk more about that as we go forward. But from a protected standpoint, the process is important. And really, and we'll talk more about this as we start further programming the environments, but there's a fundamental trade-off here between protection and efficiency. So if you want to have parallelism, like you learned about in 61C and some of the others, you could have a bunch of processes that you start up and they're all doing different parts of a given task, and now, woo, you got parallelism. Each of them has one thread. The problem with that is going to be communicating between them is harder by nature. We don't want them to communicate. So we make sure you have to punch holes between processes to do communication. That's kind of the whole point of this. So if you are doing, say, a 61C-style parallelism and it's all for the same program and user, you're probably going to have one process, many threads, because the threads could talk to each other really easily. They're sharing an address space. So that's going to be the beginnings of what we want to tease out of this. So the process is a protected environment with multiple threads. And so this is a picture from the dinosaur book, which is one of your optional books that I keep for years and years because I love it. But here is an example of a single-threaded process where there's one thread. It's got code, data, registers, stack, et cetera. Here's multithreaded process where there are many threads, each of which has its own stack. But both of these types of processes have a protected environment. I probably should have made the boxes here like highlighted in red or something. So in this guy, the address space is protected like this. Many threads, each of which are all sharing an address space, and so they can easily overwrite each other. Oops. That's not protected. But they're presumably from the same user, so if they screw up their own program, it's too bad. It's a bug. So threads encapsulate the concurrency, the execution context, the basic 61C processor you remember. The address spaces encapsulate the protection element. And you can think of them almost like the passive part, whereas the threads are the active part. The threads are computing. The address spaces are protecting. Address spaces are the bodyguard. And I already told you why you have multiple threads per address space. Perhaps you're actually collaborating together to do something. All right. Any questions on this? Yes? STUDENT: Does the [INAUDIBLE] instructor then you have the process at its own address space. It's got its own 0 to FFFF whatever it is. But then within the process, there are multiple threads. Does each thread have its own address space? Because the thread has its 61C. JOHN KUBIATOWICZ: That's a great question. So for folks on the webcast here, what's going on-- the question was, if you have a process with multiple threads, the process clearly has its own 0 and full address space, but what about each of the threads? The answer is that each of the threads all share the same address space. So if we were to lay out the code register stack for each thread, they would be in different parts of memory. And so they would all have a different notion of-- or they'd all have the same notion of who got 0. So the threads are all operating in the same address space. But each process has its own 0, and that's why threads can collaborate with each other so easily because they all share the same address space. They could screw each other up, but then that's a bug. And it's like, too bad. It's a bug. Good. Other questions? Yeah? STUDENT: Any thought about the difference between [AUDIO OUT] JOHN KUBIATOWICZ: So it's very hard to make two processes share all their memory. In fact, that's likely to be very tricky because you have to actually explicitly come up with a shared memory segment, and you have to do some work with that. So yes, so basically, threads really do share all the memory inside the process. But having two processes share all of their memory is pretty much not to a thing to do. There's pretty much no point because you don't have any protection between them anyway. Yeah? STUDENT: This might be just [AUDIO OUT] --have multiple threads, I can assume they will, if possible, run in parallel? My Python has a whole-- JOHN KUBIATOWICZ: Yeah. So the answer is probably, but there were versions of Java in the old days where threads were single, one at a time, although they were in the same address space. So by the way, one other thing, since you bring up Python and you guys are used to Python, one advantage of having multiple threads in the same process is they can pass structures between each other and each thread can use the same structure as the other threads because all of the addresses are shared. And so the object itself, which you guys are going to learn about structures in C, if you don't already remember them, every one of the threads can address the same structure by the same name. But you're asking a scheduling question, which depends a lot on the Python environment you're running. So the short answer is probably yes to your question, but it depends. So by the way, what's the kernel code and data in the virtual address space look like? So typical Linux has kernel mapped up top, for good or for bad. And then the stack grows down. The heap grows up. And then there's a bunch of stuff in the middle. And when you have multiple threads, then there are multiple segments like this that are all in the same address space. So the fourth concept, which I'm just returning to because we started with it, is dual mode. At least two modes in the hardware-- kernel and user mode. And the key there is what do we need to support this? Well, we need one bit of state, the user system mode bit. Certain operations, like setting the base and bound registers, are only available if you're in the kernel. And the interesting question is, how do you get from user to kernel mode? Well, that can be in a variety of ways. But when you transition in, then that bit gets set. And then when you transition out, the bit is restored. And the key is going to be that you can only transition into the kernel at well-defined entry points. It's not possible for a user to randomly go into the kernel in some space and suddenly get the bit on and cause problems, or it's not supposed to be. And the kernel to user mode clears the bit. So it's really the question that was earlier. How do you know the difference between the operating system and the user? It's that one bit, typically. So here, we can imagine a bunch of code that's in user mode, a bunch of code that's in kernel mode. The kernel mode runs exec to user mode, and exit takes you back. And then all sorts of stuff can happen, like system calls can go into the kernel and come out again. A system call might do something like open a file, close a file, might be write data to the network. So that transition-- a system call is actually a type of synchronous trap. But that trap goes to a well-defined entry point, like the file open handler, and so in the transition from blue to red, we set the bit, and on the return, we clear the bit. And we could have interrupts. So for instance, a packet comes in off the network. It'll cause an interrupt, which could bring us from user mode into the kernel. Or as we're going to talk about next time, a very important interrupt is the timer interrupt, which is how we make sure that we always multiplex things properly, because the timer interrupt says, oh, virtual CPU 1, you're done. Let's do number 2. Oh, number 3. And it doesn't matter what virtual CPU 1 is doing. It could be in an infinite loop. But the interrupt from the time will cause us to switch over to the next virtual CPU. And that's kind of how we make sure things work. Interrupts typically go into the hardware, figure out what's going on. We handle it in the device driver. We might do multiple things. And then we eventually return, and that return is often without the code having to know that there was an interrupt or a device handled. And we'll talk more about how that happens. And then we could have an exception, like you divide by 0, and that'll go in the kernel and usually kill off your process. And again, we showed you this last time. There's code that runs in kernel mode and code that runs in user mode. And this particular arrangement of an operating system is what we call a monolithic kernel because all of the kernel stuff is linked together and is a lot of stuff. If you look there, there's I/O, and file systems, and so on. It's all a big monolithic structure, all running in kernel mode. And unfortunately, for instance, if there's a bug in kernel mode, it screws everything up. So there are other operating system structures. We'll talk a little bit as we go. So let me very quickly do this, and I'm going to show you tying it all together. So here we have base and bound. We're running in the operating system here. So that means the program counter is pointing into the gray area. The stack is the gray area. And all of a sudden, we want to do something to start some code running. Well, notice that while we're in the operating system, we got the full access to all of the memory space. But we set up a user PC to where we're going to want to go and a stack is set up to where we want to go. And we do a return to user, and all of a sudden, we're running that code in a restricted mode where the yellow code, running in user mode, can't do anything because the kernel mode bit's off. When the kernel mode bit's on, I can access everything. When it's off, I can't. And when I want to switch, then what happens is how do we return to the system? I'll talk more about this next time since we ran out. System call, interrupt, trap. But we can go back here and switch over to the green. And I'll actually show you this example next time. But in conclusion-- [WHISTLES]---- hold on for a second. In conclusion, we told you about four fundamental ideas today, and I hope that they made a lot of sense. One is the thread, which is a virtual CPU. The second is an address space, which is all the addresses that a particular thread can see. But we could put protection in there, as well, with translation. A process is an instance of a running program with a protected address space in one or more threads. And dual mode is really the hardware support to make all of this secure. And we'll talk a lot more about this as we go. So you guys, don't forget to go to section tomorrow, any one you want. You can go to a different one tomorrow and the following Friday. It'll be great, great fun. 