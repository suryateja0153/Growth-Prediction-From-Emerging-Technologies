 hello welcome everyone I wanted to start by thanking you for coming to the first seminar of the semester of the MIT robotics series and I wanted to thank Matthew for donating your time and effort to come here and visit and give us a talk and I especially wanted to welcome I don't know if there's anyone in the audience this is your first semester at MIT if you are here I wanted to especially welcome you right this is seminar series and MIT robotics and many of the things we do this is for you right so welcome to what hopefully will be your home for for a long time anyway yeah hopefully will be a home that you will get to enjoy where you will get to sort of chase your dreams Matthew as I was saying thank you for coming Matthew he's an associate professor at Michigan University in many departments naval marine EEZs so a lot of interdisciplinary work as it's usually the case in robotics which is cool Matthew is a very well-traveled person it is PhD in Sydney I don't know if you might be able to get it from Tintin your accent then postdoc at kth in Sweden and then now faculty in the US so very very broad spectrum of experiences and he will talk about robotic perception in the field and one of the anecdotes that I that I like is that he sort of he was driven into this field into this research because of the the first DARPA Grand Challenge trying to make cars drive at high speeds in the desert and where if you think about it this sort of gives you an idea of how why he sort of inspired to continue working on a robust perception to get robots to do real things in the real world anyway thank you for coming Anna please Thanks well thank you so much for having me I'm really excited to be here and thanks for inviting me you guys um so to that point I'm gonna hopefully talk about some things that get to this idea of how we would translate a lot of the things that are happening right now in perception more broadly into the field and sort of where that interface is between robots particularly robots that we want to have out in the field so field robots big sort of outdoor robots and sort of where perception is going so nowadays if you think about perception so much of it has become dominated by this idea around deep learning and so I guess some of the work I'm gonna talk about today is sort of how we're trying to think about where the gaps are in-between where deep learning is and sort of where computer vision is as a field or deep learning outside vision and where we are in robotics and so you know and this is sort of unthinkable even sort of when I was an undergrad and in grad school this idea that we would get good enough at identifying things in still images that you could have sort of image retrieval work at sort of robust levels and so this is sort of the canonical example now of where deep learning is that I can show you a static picture and you know with some level of accuracy say what things are right and I think it's something that we take for granted now but I do want to kind of give this Oracle perspective that if you talk to people that have been around for a long time this is a huge step forward in what is possible and vision and impasse the perception in general and so that is really really cool so everything i'ma say today is not that none of this works but that perhaps some of it maybe needs to be rethought or we need to think about where that sort of approach kind of intersects with robotics and so you know the kind of again the canonical example of how this works is slice told from and video rate is you have a bunch of training labels for a supervised problem cars buses trucks motorcycles whatever you train them in your deep net and then perform inference with it right and this has become something that we're all really familiar with because it is blasted at us through many different channels and the popularity of those fields have become really really immense I think it's important to acknowledge that sort of VAT is sort of where a lot of the research has been going and so you know really specifically I'm going to talk about two kind of field applications one's gonna be driving the other one's gonna be underwater robots but you can imagine that the first thought you would have is to how to apply this right is you're gonna say great I'm gonna run that thing that worked on static images I'm gonna run it on images I take from a robot and I'm gonna say things that are be robot relevant so in this case people cars helmets motorcycles so not just you know birds and ducks and trucks et cetera but things that are irrelevant for the robot but I guess if we get to the the crux of that right is that the the right answer and is that always going to be the right approach and I think it was the first approach people took and the question is how do we kind of move beyond that or what are the limitations of that so you know people have been developing more and I guess I would argue more sophisticated problems around that moving beyond just bounding boxes or static image classification to per-pixel classification and again you can imagine why that would be useful for a robot but if you get down to the core of this right it's not immediately apparent how a per pixel segmentation is immediately actionable right it's actionable and for certain tasks that you would have for like let's say your Facebook's or your googles or whatever but it's not immediately clear how you would use this to take action in the world how do I move through this space particularly because this is in the image plane and so all those decisions I'm making are in two dimensions and if you look at the accuracies or even the performance of these things in kind of full 3d space you see this big drop-off and so again classical vision has been or not classical vision but modern computer vision has been focused on these problems and and how do we make that work probably most importantly I think to think about where the weakness is or where the limitations of this are are around the way that we get to those things so if you look at this the way that is actually produced is through really really a laborious hand labeling and so there are a bunch of data sets that have been produced that we now use and we think about in robotics as sort of ways of solving these problems but they come on the on the sort of backs of very intense human labeling efforts and one of the most important things to think about this is not that necessarily well there's some to go in moral moral questions around whether or not this is the right things that humans should be doing but most importantly is just that the scalability of these approaches is quite limited right even if you're willing to dedicate large amounts of human resources to labeling you are still quite limited in the number of scenarios types of data you can label but most importantly if I get new sensors if I get new environment so they get new whatever if I have to repeat this process where I have to put hundred people in a room where they spend 10 20 even an up to an hour labeling an image the question is how scalable is that in the long term and is that the right thing for robotics particularly we operate in a much wider range of sort of scenarios and environments than would be sort of applicable for your Facebook label your friends Apple images etc and just to give a sense of what it actually means to do this task this is what a human has to do so they have to go through and I've actually done this with my graduate students for a project it was incredibly unpleasant it took over an hour per image for me and I don't know if I'm sort of the median or whether or not they're people that are faster or slower but it was really really boring I did like 10 images and I was like this is for the birds and so what this really gets to is this sort of division that I see so I like to work on these type of platforms down here there's a bunch of vehicles I'll talk about a few of them a little bit in the talk today but this division between what has kind of risen up and imagenet is again I would argue sort of one of the jumping off points for a lot of this where we had a large label and image dataset and then we were thinking about okay well what can we do with that people have done really amazing stuff with that data but what is the division between that and the type of robots that would wanted to point in the field so these are some robots that we've used I'm also gonna talk a bit about self-driving cars and applications than that but we also have fetches and other kind of humanoid robots doing grass manipulation and the question is sort of where is this gap between what the computer vision community is doing up here and what we're interested in doing down here in robotics from the perception standpoint obviously so you know I talked about sort of what it takes to think about perception these contacts but I spent a lot of my time and in fact probably an inordinate amount of my time just working on deploying robots and so luckily that you know for at least underwater robots all the big companies haven't dumped a bunch of money in to compete with me and there's not a bunch of googles and Facebooks of the world trying to deploy a visa and so I've a lock on that market for a little while but these are the typical vehicles that we use in my lab to take image data and so we have downward facing cameras on them you can see them maybe they're a little hard to see but we're using traditional machine vision cameras they're very some of the cameras that be in your cell phone and we have them downwardly facing and there's someone around taking pictures we're also using the fetches we put cameras on bigger robots this is one run by the inverse of Rhode Island so that we have a downward facing camera set that we have some data on from there and this is what I spent my thesis working on so this is an AUV developed in Australia watched story developed it hoo-ee which we then modified in Australia and built some downward facing cameras on and took a bunch of camera data so the long insured I guess of sort of the arc of what I want to tell you about is there's lots of ways of gathering really rich camera information from robots prickley underwater robots now and also obviously cars and so we've been luckily enough to have a research lab with four they've lent us several of these vehicles we've kind of outfitted them with a bunch of additional cameras and so we're able to now gather huge volumes of data and so this sort of notion of big data or whatever you want to call it we're sort of there with robotics now right we can gather huge huge volumes of really really rich camera imagery at whatever rate you want that's sort of where we are now and the real challenge or the real thing we don't have access to is massive amounts of human labels and so how do we bridge that gap how do we use the data that we have more intelligently or how do we get away from having the label data itself to give a little bit of history of the stuff that I like to think about when it comes to self-driving cars and and why maybe they don't work right now if you think about from like the early nineteen sixties they've been sort of self-driving car projects that have been around that have done sort of lots of interesting things it was an RCA project where they had metal cables installed in the road and were able to drive only lateral control but fairly credibly even as far back as then there are a number of projects at universities and other places I would argue probably the kind of pinnacle of it being the nav lab 5 and in the 1990s and that vehicle drove all the way from Pittsburgh to California and again arguably doing the same thing that sort of a Tesla autopilot is capable of doing and so one of the reasons why I think we see this big gap in where we are now and sort of what the challenges are still facing self-driving cars is that a lot of the things that we think of as being credible systems that are you know really out there doing a lot are doing things that we knew how to do a long time ago and then kind of stopped because we hit a wall we realized ok well this isn't gonna be deployable more widely there's been a huge injection of money but that doesn't seem to have gotten it's over that hump and so we'll see what happens I got in much later into the early 2000s on this vehicle but there was sort of this arc in increasing sensing modalities better sensors certainly better more sensors and higher volumes of data with kind of greater levels of fidelity but I think one of the challenges we still don't have a way of taking that data interpreting it and turning into something that is actionable in a way that is always safe and so it is not the volume of data that is limiting again it is thinking about the way that we interpret or use that and so again it comes back to this idea of is it just hand labeling more of this or other other ways of getting around that the last thing I'll say I guess I'm self-driving cars before we kind of jump into some of the more technical material is just that we may be somewhere on this hype curve that I stole from some firm that probably makes a lot of money doing this but the idea is that you know one of the things I hope that that certainly younger students take away from this right is that there may be some backlash that we are approaching on certainly level four or autonomous cars that don't have people in that and I hope that some of you stick around in this and keep working on it because I do think we've come quite a long way from where we were at least in the early 2000s when I got started on that and it would be ashamed to see a lot of that effort not being continued just because it hasn't worked out as quickly or as efficiently from a capital perspective as people wanted and again you know a lot of what is involved in self-driving cars I use this image this is all lidar data that's being labeled I believe this is in China from a New York Times article that was talking about sort of some of the human costs of doing a lot of this machine learning work okay so that's a lot of may saying so what are my suggestions as to how we would get out of this so I'm gonna talk about a couple different ones but some of the ways that I think we can avoid this trap of continually labeling additional human data is thinking about physics and simulation there's a lot of people doing really great work on that axis we'll talk about motion which is an inherently robotic thing that doesn't exist in your image retrieval tasks talk about multiple sensors cross or self supervision and then finally a bit about domaine adaptation using some of the data that we have in a more smart way okay so physics and simulation so I was a PhD student and then I was a assistant professor or post document professor so I had no time to play video games and in the intervening time video games got great and so one of the things that really amazed me when we started looking into this is just sort of how far graphics or photorealistic rendering engines had come so this is grand theft auto which a couple years ago we spent a bunch of time trying to get data out of just because it was sort of the most photo realistic simulator that we had access to we were quite afraid that we were gonna get sued by Rockstar Games it was us and another outfit that we're trying to basically grab data from this we called Rockstar Games were like hey do you guys want to work with us they were disinterested we did not get sued and now enough time the past I don't care anymore but one of the things that having a photorealistic stimulator really enables right is the ability to do this idea of domain randomization so if I take the same scene and I permute many of the photorealistic properties of it you can use the same labels that you're automatically able to get from the video game but think about having a network learning variances across these different photo properties right and so this is the same scene lit in a variety of different ways and so what we ended up doing is actually hacking into the game engine running one step of simulation of the game time then cycling through all of the weather can slate parameters everything else we could do in that game to kind of vary the visual properties and then feed that all directly into a neural net and so you can see that they have some degree of volumetric rendering there's some weather conditions you can vary those kind of things I'll save you the details it was incredibly difficult to get any data out of this it was a huge nightmare it breaks every single time they update the game I would not recommend anybody pursue this people have started produce better simulators now while other simulators now tragically I don't think anybody has really max the photorealism of Grand Theft Auto so there are some new ones now built around the Unreal Engine there's a bunch of commercial companies but long and short of this is that if you take this data you take a label from it and you pump a bunch of it into a simulator really the only one you get is by using a lot more data and so we looked at cityscapes which is about three thousand images of the common data set used for autonomous driving Kitty is eight thousand images or at least it did at the time of this and we generated a million simulated images and pumped those into a network at great grad student and cloud time costs but the long and short of that is that you can actually end up doing reasonably well particularly if you think about cross dataset classifications so what you're seeing is classification it's very object detection of cars on the kitty dataset if it was trained on cityscapes versus trained on simulated data down here and basically kitty divides it up into three different types of cars it's not super important but the long ensure the story is the more simulated data use the better you can do there's an asymptote there and what we I think probably the more interesting outcome of this work was just that you hit some limit basically hit the limit where you've gotten all the information on the simulator we went from 500,000 to a million to two million to five million images and you don't see any greater performance right so whatever information was contained in there the network is keyed into whatever is useful in that and then everything else you're adding is really just adding noise it's not adding anything useful so I have a graduate student Alexa who's been looking at trying to use that simulated data or think about sim to real in some ways and try to adapt that data to be better for network learning and so one of the real goals of that work was thinking about what do we know about how cameras work what we know about the physics of cameras and can apply the physics of those cameras to data itself and so we know things about chromatic aberration those are effects is that the camera lenses we know that lenses often apply blur certainly in the extremes of the lens we know some things about how the camera sensor actually takes photons in and turns them into pixel values so you can play with the limited exposure range that cameras have some degree of sensor noise and then to be honest with you a hack on top of that where you kind of just shift the color around to kind of fill out the domain randomization problem so what that looks like in the end is that you have real images from let's say kitty and cityscapes and they have a tone to them or have that they have a look to them that if I showed them to you you could even immediately recognize it and say oh that's from that data set there's been lots of work great work done here on sort of the data set bias that exists in these things but if you look at simulated images like virtual kitty which is a data set that matches Kitty exactly but just puts it in this sort of artificially rendered realm or even Grand Theft Auto they don't look like real data and so this was sort of early work in in thinking about how do we take the physically-based model and apply that so what you'll see is sort of the domain randomization of that simulated data and it still doesn't look real but the performance gains you begin to see by even performing this domain domain randomization where you're kind of flipping up the color is changing things making it look less like a sort of artificial setup and much more I guess less artificial to some degree begins to sort of buy you something when it comes to training these networks and so it helps you least get over some of this asymptote that happens when you're training on lots of simulated data the thing I will point out here and the other thing we learned from this is that there's a limit to this as well so we still are unable to match the performance of extensive amounts of real data and it's not clear whether this approach will ever get to that point but I do think that sort of the early things that we have learned is that it's really important to have a physical model of what your camera is doing and think about how that physical model of the camera is applicable to whatever synthetic rendering process you have so here's an example the red squares you're gonna see here is going kind of fast the red squares you're gonna see here my mouse is really tiny the red squares you're gonna see here are examples of missed detections when you just trained on the simulated data or the simulated data combined with some real data and then if you apply that augmentation it's able to recover some of those misses but this really gets to some of the important points when you deploy these things you're still missing significant amounts of of cars I mean obviously not always as you get closer you tend to get better but you can see for a single shot detector that's just trying to make individual decisions about is just a car or not we're still kind of nowhere near where you would need to be to have this be sort of the only system you had in a safety critical system okay so sort of extending on that work and thinking about that and the underwater domain I two other graduate students both of which have recently graduated that we're thinking about okay can we use a physics based model of what we understand about how light propagates so in that same way that we were thinking about cameras but thinking about this in an underwater context so why would that be an important thing why do we care about that well we deployed camera systems like this on big underwater vehicles so this is the sentry vehicle we build that camera system with a postdoc advisor this is that URI vehicle we built the camera system again in Australia and put it on there but what we tried to do with that is build big 3d reconstructions and this is sort of lots of my earlier work again with my graduate advisors on building big 3d models of the undersea environment by taking those cameras around and taking pictures and the first thing you'll notice is that you'll see these well may not the first thing you'll notice but you'll see these sort of light and dark patches that are on there that actually aren't real so those lines you're seeing there are effects of camera attenuation not really effects that are on the bottom the other thing I notice is like stuff underwater looks crazy this is all these are bacterial mats at almost a thousand meters off the west coast in the United States but again you can see these patterns these sort of stripes and lines and it's a little hard to see but you can see these stripes and lines here those are not real effects those are not actual there you can see it better so those are effects of the vehicle and effects of attenuation of light and water we also be a big reconstructions of shipwrecks this is just one I think it's cool these are a bunch of amphoras and a wooden ship that sank off the southern coast of Turkey like a million years ago which is not an accurate figure but long story short we know what the physical process of light and water is there's been really great research done on that we understand how that works and so can we use that to take data and then modify it to be better or more accurate and more accurate actually reflect what we're really seeing and so if you're thinking about image formation I will kind of run through this really quickly because it's not super critical but we know some component of is direct transmission so that is how far away is that object so it's the distance along the line of sight what wavelength is the light so that means that things in the red spectrum attenuate faster than green or blue was attenuation coefficient the water that's the thing that you need to estimate so that's one of the challenges here what's the radiance of the source and then what's the viewing angle and what's the ray direction another component of that is scattered light so the rest of that is energy that is thrown back into the camera so it leaves your strobes or the Sun it hits particulates in the water and it comes back to the camera before it hits the thing you're interested in so there's a big component of that this is a really well understood formulation the challenge is not do we know how light propagates or do we know how to measure these things it is how do you estimate the parameters of this function online or how do you estimate them for some body of water or how do you estimate them in the long term and so we did lots of work sort of maybe five ten years ago trying to figure out if we could do this in a more traditional way using sort of nonlinear estimation techniques it end up being very very hard to do this robustly and so now sort of in the era of having big nonlinear function estimators that everybody is excited about aka deep learning we've been able to do a fair bit to advance where that was using those exact same models and injecting the physics understanding we have of how light propagates in water back into the way that we actually estimate these things online and in this case we use sort of the de jour technique generative adversarial networks but let me walk you through just in general sort of these students work so the kind of key here is we want to make sure that you could leverage something that we had easy access to so the thing we have easy access to is air data with range maps and so there's tons of n air data take a connect you wave it around that gives you an in air RGB image and an inner depth map right connect does not work underwater it's the first thing people ask me why would you just use that underwater doesn't work I can get into that why perhaps offline but long story short we have tons of n air data it's what we would do is we would take that in air data with the connect map associated with it we would use the propagation of light in water which we understand to render a version of that image given the depths of it as if it were taken underwater now that's not gonna be super accurate but doesn't really matter because what you're trying to do is actually learn the function of that for your water data and so what the gand piece of this allows you to do is allows you to take this RGB data set or two BD data set and your real underwater images so I just gather as many real underwater images as I can I don't need to label them I don't need anything from them and then what I'm attempting to do through this adversarial process is produce the function that would give me images that look similar to my real underwater data by which I am learning the parameters of the data that I'm seeing now and so as opposed to have to having labels or depth data that is accurate for the underwater data I have I can use the aerial data along with huge volumes of underwater data which is obviously almost free to capture once you have your robot you can ask all the grad students they've built the robot if it's free to build the robot it is not but that process then gives me the ability to take that produce underwater images and then I can train a set work a separate separate network to then invert that process to give me images as if they were taken in there um ok that was the slides of this I don't know if everybody's familiar with this but like a two second explanation of how generative adversarial networks work so how am i comparing the two pieces of data right so I have a generator network to take some input data and produces fake data in this case it was the rendered underwater images using the physics mom I then have a separate network that it's trying to decide whether the data is real or fake and it has examples of real data it has examples of fake data and I obviously have the labels for that because I know what I produce and I know it's real through that process I can iterate and get better at producing fake data and get better discriminating between fake and real data okay so how does that work with the actual physics of this what we do is we actually take in the same noise vector you taken a traditional Gann we're taking depth maps in the same way that we would have from the aerial data in this case the RGB image and then I have physical models of all these things I have physical models of backscattering our physical models of attenuation I have physical camera models and I can use all of those learn the parameter of parameters of those online and then produce my simulated RGB image from air compare that image using that same discriminator to real RGB data taken underwater and so the attenuation model looks like this and you can refer the paper if you are interested and the backscratcher model again can be composed of several convolutional filters and some noise but i put all that together and then I end up again with a synthetic RGB image and a real RGB image that I'm comparing and at the end of that process what I really want to do is take that back it out to a depth map back it out to an RGB image and then produce an image as if we were taken in air and so again the kind of end process of this is I can take underwater data that I have no labels for I can learn the relationship between the attenuation parameters the backscattering etc and I can then invert all of those things to produce data that is more photo consistent that looks as if I were taking an air in some new environment in which I have never been before but I only have data that I'm gathering online and so again so we did some experiments with this we set up our camera system we had some artificial rock platforms and tested it and at the end of the day this is what almost all your underwater images look like typically this is what you can do sort of the generative Network and there nothing inherently like this looks ok you can achieve similar results if you sort of hack around it and basically do histogram Equalization the one benefit of this is it ends up being consistent over depth so that means that can be shallow I can be deep we're history I'm equalization were produced different results purchased like this or more photo consistent so they aid with 3d reconstruction slam whatever their task you want if you guys have any questions as I'm going please don't hesitate to let me know ok great ok so switching gears again talking about ways of getting out of having to actually label any data we've been thinking about what other things do motion affords you that allow you to kind of save time and be able to learn things without having to do anything so this is a graduate student that's probably a year away from graduation one of his recent papers buddy up my robot driving around and my robots driving around and I took that image earlier where we were getting single shot detectives we're detecting cars or detecting buses whatever but we weren't always getting it right and the first thing if you talk to somebody that deploys vehicles they say well you know we'll put a tracker on top of that and that's the easiest way of kind of eliminating this problem this noise that you see in single shot detectors and that actually works super well so when your modern deployed AV you have a single shot detector and you typically have a tracker on top of that that used additional information to figure out where things are and one of the things that came to one of my graduate students was that ok well I have this really rich information of where this vehicle is over a long period of time and I know I have a single shot detector that I can run on that and so I now have the ability to detect when my single shot detector failed but my tracker said there is an object there and if you look and you run that forward and you run that backward in time you can end up with all of the places where your single shot detector failed but you know that there's still a car there or you least you know there's a partially occluded car or something else happened they made your single shot detector fail so what he ended up doing is then he would run his detector he was on this tracker he would pose this as a binary classification problem is this a real false positive or a real false negative and you do the same thing with stereo right so if I the left image and a radium image from a stereo pair if I see a thing in the left image and I have a depth map I know precisely where that thing should be in the right image right and for a car or something as big as a cars or a person really just the sort of even if you're coarsely aware of their depth that's enough to project to the other image correctly and so if I run my single-shot detector on my left image and my right image and I don't get the same thing or I don't get the ship to that in the right way that tells me again that I probably am missing a thing now when he posed this to me I said look you know I don't think this is gonna work he's like you're only get a few examples like it's mostly gonna be correct and you're gonna find a few corner cases and I ended up being proven wrong and then one of the other cool things is they were able to implement this so we work with Ford and they were able to implement this on a much larger fleet or we're working to implement this on a much larger fleet level so you could have this running on any car that was out there taking data because again it comes for free because all it really requires is a detector and a tracker or stereo estimation what you're doing anyway so what he started pulling out was with sort of the detector to shore of the day all the examples of things that were missed with free labels that came from projecting the bounding box forward in time through either using the stereo or temporally and you end up with all of the basically every example of why use the only passive sensing is going to be terrifying because tons of things cause it to fail so we ended up with thousands of examples of oversaturation causing failure of shadowing causing failure of occlusion causing failure and even some examples were it's just beginning it right not clear why it's getting that one wrong or that one wrong or that one wrong yeah so there's some more these we ran this on Kitty and Oxford robocar and a bunch of others these are the stereo cues we ran on the simulated data which was a much larger data set but let me see this should hopefully show you so if you're watching a single-shot detector which is your SSD please the owner a diner tracker have any failures and then having a mislabeled in your yeah no great question so I mean one of the challenges here is you have to make that binary classification problem you say should I accept this label very aggressive right because it's better to throw away you can run this as long as you want and so there are examples in which exactly move along the precision recall curve until you're taking only data that you're pretty confident in um but even this alone was able to produce like on a bunch of data sets trained on sort of you know kitty which is arguably fairly state-of-the-art for well it used to be kind of say the art for what we were doing so what you'll see here are actual detections all the green boxes are temporal errors and the blue boxes are gonna be stereo errors and that's a little hard to see maybe this won't be easier so there's a ton that you miss as you're driving around so if you just run on this you'll see that kind of small cars cars that are partially occluded and you end up getting tens of labels per every sort of minute of frame which to me was more scary that our object detectors are missing tons of stuff rather than you know this being incredibly anyways question please yeah so it ended up being kind of everything that we know to be problematic so it is changes in illumination loss of texture rapid shifts in color really all the things that make sort of the corner cases of object detection in imagery hard so sort of where he's going with this or sort of what the next steps are are incorporating this back into a learning framework to iteratively get better but using the labels that you know to be noisier than the labels that you would get from a human and so there's lots of work happening in sort of the ml community to deal with nausea labels and complete labels all of that so the hope is that as that it ends up being a bats also a really hard independent research problem and as those that gets better hopefully approaches like this I think the most useful version of this is really at a fleet level as opposed to on any individual vehicle because you can be gathering all of the corner cases across many vehicles and then you're talking about tens of thousands of labels as opposed to sort of order of you know thousands even over the datasets that we have okay um so sort of one of the other kind of angles here in terms of thinking about physics is trying to apply physical constraints to problems like tracking human beings or figuring out where they are in the world so this is a data set that we gathered intersections at Ann Arbor so there's a four-way stop and we were particularly interested from a autonomous vehicle perspective about figuring out sort of what human behavior is at a four-way stop how do I know that someone's indicating they're gonna cross with their body language is all the kind of things that make it easy for a human being to do that but hard for a autonomous vehicle to do it so sort of in in strict contrast everything I've been saying we ended up having to human label a lot of this which ended up being really really really annoying and so we started out with that premise that we're gonna human label lots of it and then in fact what we did is we moved away from that because it was costing like tens of thousands of dollars to get you know hundreds of fat or maybe maybe a thousand 2000 frames labeled with the kind of fidelity we wanted which was human skeletons pixel wise segmentations etc and so then what when I got just seems very develop was take lots of the work that's been done in body pose fitting and body model fitting and using the additional data that we have from a navy so in this case lidar data and fusing the lidar data the stereo data and the image data together to extract 3d body pose models of human beings and so really what this was was a bootstrapping of a labeling task for a data set where if I started with some course information let's say like bounding boxes where the people are and I take the lidar data and I put it in and it's when these body fitting models I can end up with both skeletal information pixelize information but ultimately we're interested in 3d body pose information and so the goal of sort of both that dataset and then sort of what I'll show you in terms of what we're doing in prediction is predicting full up 3d body pose models from human beings and not in sort of the canonical examples which are in a background list room or with Kinect data in doors or even up close you know you can see sort of where modern computer vision is they can tell my skeleton for images that are near but at sort of ranges that would be useful for autonomous vehicles so 30 or 40 meters out and so what we began to do is capture lots of variability in what humans were doing they were carrying things holding hands carrying bags talking on cell phones moving through intersections and then we got even more complicated ones so we got into trying fitting about in Mo's body pop body pose models on cycling wheelchairs dismounting bikes running etc and so what you're seeing here these are the end results of us using the lighter date of the camera data the stereo data to fit body pose models across a variety of pedestrians in different positions and under fairly heavy occlusion where this is heading or what our goal was was not just to figure out where they are now but obviously to figure out where they're gonna be and so thinking about the prediction of body pose models over a number of steps and so our initial attempts of that we're sort of in the skeletal phase where we're just trying to figure out sort of where we think the key points of the person are gonna be and it turns out doing that in 3d works very very poorly so you can see these really great demos of sort of where the state of the art is in true 'nobody post-detection if I take that out even sort of 510 meters away from where I am the 3d positions of them become wildly inaccurate so it's a little hard to see here but they can be up to like a meter off and Z sometimes in human in X&Y so we started to move to was not skeletal positions but again these full 3d meshes we wanted to predict and again it gives you a lot more rich information because we have the whole disparity map of what we're seeing from the stereo and you can get away with sort of things that so you can eliminate things that would be inherently problematic with just the key point estimation by thinking about this entire one that is kinematically rigged in such a way that your body can only move in the way that your body can move and so what we got really focused on it's thinking about fitting these kinematic body models to where people were in 3d space and then thinking about making predictions by thinking about how that's going to move through time and sort of how that body model obviously can't include itself it can't run into itself and thinking about how that full 3d motion of the person is gonna move over a period of time we want to include things we knew from physics so things like support and so we looked at sort of where the foot placement would be and all the things that we do and legged robots to figure out stability and how the person and how a legged robot isn't gonna fall over we're also useful for predicting where a body pose is going to be and so we would calculate the envelope of support we would also look at some ideas of symmetry of the body from where that comes sort of how your limbs are typically oriented as you're walking what those periods look like and all the things that people in kinematics or sorry in sort of whatever body pose estimation for human beings have figured out and then if you throw all of that sort of parameterization at sort of the same tools that are now really really popular like LS TMS other things like that you end up with results that look reasonably credible and work typically better than what you would end up with if you're just using the raw perception data to estimate the pose so hopefully this will show you that and so again we were focused on thinking about people at far ends of intersections and like sort of where this would be so the car is positioned just out of frame and the second thing we're interested in for large groups of pedestrians so not just for one person but thinking about everybody and their global positioning with respect to where the vehicle is so what you'll see here is the vehicles here there's a spinning mirror lidar a set of stereo cameras and for this the vehicle is stationary but we're trying to estimate the full 3d pose of all the pedestrians in the scene so we had four Vella dines two stereo cameras and again looking at the period of the human gait and so thinking about what we can predict in terms of step after step thinking about all the symmetry that would happen in the limbs in both in plane and side plane think about the ground contact between the foot and the ground and so this is on that pedak dataset our prediction results and so these were the initial training labels and then you'll see these four groups of pedestrians moving and safe so they look a little crazy but I promise you if you look at the raw data from just what you'd get out of sort of your kind of standard prediction algorithm it looks much worse and so this is sort of what some baseline approaches look like just doing single shot versus where the person actually went so the red is the ground truth the blue is sort of a much simpler baseline and as you get further out you get worse and worse but like you end up with a motion that to some degree would be enough to act on I would argue at least for a couple of seconds in the future from your self-driving car and so we would try and real intersections to look at sort of what this would actually look like you end up for the most part getting the things that I think would be critical to understand like holding a phone holding coffee those kind of things um that is one of our engineers I don't know why we blurred his face but him moving around and then someone walking a bike again okay how much more of that I don't know why this is here okay no these are supposed to be skipped okay I'll go back to them so one of the kind of last things I want to tell you about it is the idea of thinking about self supervision just from one set of data so this is not across data this is leveraging the relationship between semantics and sort of estimation of disparity to figure out better disparity maps and also better semantic segmentation Maps so if I have a left image and a right image and a sparse depth map and that can come from lidar that can come from other sources what we're doing is passing that all newer networks it's probably the most traditional computer visioning thing that we'll show you and then ending up over here with a dense depth map that up samples whatever this input depth map is with the stereo information and look there's a huge range of this being done in the computer vision community what I think we and robotics probably have an edge on is lots and lots of it as being done with a single modality and almost exclusively with monocular imagery and so I think there are huge gains to be had looking at stereo data which obviously in robotics we have a lot of a lot really great familiarity with but just isn't being used really really heavily and so these are up sampling of sparse point clouds to get full and complete depth maps and it's a little hard to see at this resolution but you end up getting complete cars full people telephone poles all that kind of fun stuff and if you look at the air maps you end up with much much lower air than you would with either just the lighter data or the stereo data and particularly at long ranges that end up being a big win from that perspective I think the other important thing that we took away from sort these ideas was that you can have really really sparse amounts of lidar data and still end up with really really high-quality results and so there's this big push in self-driving cars to have more expensive lasers at every kind of step and so we have 16 beam later and then 64 beam lasers and now you can buy 128 beam laser for $70,000 and with just one camera and particularly with two cameras we were sampling down to basically the equivalent of an eight beam Vella dying and seeing performances that were very very similar to what you would see with 128 beam so again you get these wins if you kind of avoid thinking about using all of that okay the last thing I will tell you about is me and every other Joker in the world now as a start-up and so because I got really frustrated with everything not working in full-size self-driving cars we started building really small and lightweight vehicles and so this is about a hundred pounds it travels about 15 miles an hour and so what we're trying to do is push a lot of the same and Thomas technology that we think is gonna work at some point eventually for full-size cars but maybe not you know anytime soon and just make it a lot easier by going a lot a lot slower and so the stopping Issa's vehicle is about five feet and so you can end up messing up a lot more often and it's really late I've been hit by it several times it's totally fine and so if we're targeting food delivery as well the early applications but I was having a conversation with John lund earlier and we're just talking about how it's not when autonomy full-time he's gonna get here but it's sort of in what forms and I think the forms that make it easier like quickly going slower and being lighter and all of that fun stuff make that a lot easier so we got this video produced very expensively so I will zoom through it but the at the end of the day I think we've come so far in self-driving and in a lot of other kind of ways in robotics and I think to get to market we just need to make the problems we're solving simpler these are paid actors I don't know these people they were very nice they came at high costs they look like they're very excited to receive their food yeah okay that's it thanks guys [Applause] thank you very much any questions thank you thank fantastic talk thank you so much one question I have is in regards to the Heidi's idea of his aesthetic data how can be so prevalent why is kind of your insight on what kind of features are very hard to estimate from it and which one it does really well estimating do you have any Z yeah so I think the things that we found worked really well particularly from the photorealistic stimulators cars were much better than people people work pretty poorly cars are certainly the best things that are big and metal which we know how to render really accurately and so like that's probably the the first thing so artificial man-made things are probably the starting place the thing that is I guess most difficult that we found and sort of where most of our efforts are going now is that networks are really good at keying in on things that you don't care about or not visually selling it to you as a person but the network you say hey here's much data figure out what matters in this and that's great and that's like taking us a long way from where we were when your hand designing features but that also means that you can key in on features that are completely unimportant and so probably the most distracting thing is that there are rendering artifacts or things that are just are produced in artificial data they just don't exist in real data and the sort of distributions of error do not look like they do in real data and so what often will happen if you don't regularize whatever your training process is you know with real data or thinking about ways of other ways of regulating it you end up with it King in on rendering artifacts to figure out what is a car which is not useful when you go in the real world and there are no rendering artifacts so that is probably what I would argue is one of the biggest challenges for outdoor sort of self-driving rendering things if that helps noise I can hear you I'm sure oh wait guess it's for the video or something right thanks I really enjoyed the talk so I guess my question is about what can you learn sell supervise first what do you need human labels for so you know geometry information all the stereo stuff makes a lot of sense to learn self supervise but if you just don't know what a car is and that's a human thing yep so are there ways to sort of still self supervise that like learn classes and then I'll label them you know on the back end or do we have to use sim to reel so sort of for those more semantic things is there any way to avoid at least some amount of human labeling you know I mean that's the thing that we try to think a lot about we wanted to see if we could bootstrap off either self supervise or something else to get to the point so I'm of two minds of this right I think there's some really interesting research questions that could be asked about how you would fundamentally go about with no knowledge of the world beginning to categorize things and there's actually a lot of work happening in machine learning to do exactly that right so if I just have really unlabeled data and I want to label it into categories but it goes back to that problem I was just highlighting there are many things in there that may be selling in the data they're irrelevant for the semantic meaning that we would apply to it I would say at least the pragmatist in me is we're in a world now where image world exists I mean the image that exists right and so the notion that you can bootstrap to some degree to get you to a place where man-made objects are recognizable at a reasonable enough rate that you can begin to play games with getting to that kind of long tail so we have not spent a bunch of research time thinking about the thing where you know nothing about the world simply because it there's such abundant such abundant sources of small amounts of labelled data I think there's probably lots to be done there we've sort of just shifted to focus on this idea of domain adaptation so if I have data from some some label data from some world that somebody has already spent the time to gather and I have a new camera system different cameras set a different environment lighting how do I adapt to that but I think to some degree somebody should be looking at whether or not we even need any labels right because it doesn't feel like that is okay so you mentioned about the performance reaching an asymptote when you're training in the simulators right I mean how much do you think that is because the diversity issue yeah is it I mean the fundamental algorithm is the diversity move if you have more diversity you know simulators would kind of match up yeah so I think it is diversity in the simulator almost certainly I think there's an asymptote probably to some of the approaches but I'm not sure that we had hit I mean if you think about your modern know that is you know 100 million to million parameters there's a lot of room in there to learn stuff and if you talk to so I mean we are poor and lowly academic researchers if you talk to people within some of the larger self-driving outfits they will tell you the sizes of some of their label datasets and they are orders of magnitude bigger than what working with right so we're putzing around with kitty you know 7,000 images inside like uber and others you know you're talking datasets of a couple million - even more for some problems right and so and they say that there's obviously value they like I didn't spend you know whatever a million dollars to label 10 million images for no reason so there's value in continuing so what we hit was the asymptote just in the variety the simulator to your point the the simulators have hit a limit and there isn't enough sophistication or enough variability or enough photo realism to capture the Aero distributions that exist in the real world and so that is still the gap and but you talk to them they spent you know a million dollars labeling 10 million images and one of the real bombers is that if you change out that camera system that 10 million images great gets you 98% on on that dataset you change the camera system out you even change the environment you're running in and you and it doesn't drop to nothing right it still works right that's why image networks when our data but it drops to 90 or something that is well below the asymptote like the the long tail you need to get to so better simulators but probably better ways of adapting that domain to the real domain would be the two things I would say like if it always comes orderly better simulators or more content creators who can contain which are content right well that's the other piece right so the reason that GTA works so much better than I think some of the other ones is that the assets in like the rendering engine they're all you know Unreal Engine they're all sort of comparable to some degree the assets of GTA are much better they just they they spent I don't know 100 million dollars making that game I think it's the highest selling media property of all time if I didn't want to talk to us they're made like a billion dollars I think they sold like I don't know I think like they made a billion like literally a billion dollars or like 10 billion dollars or something so like it's like hey come jacking around with our little academic research factors I don't care about that right there so they're still in conference the game but the assets are so much richer but then are we just trading human labelers for under paid 3d artists that sit in some room and make 3d assets all day that doesn't seem like a future I there right so I don't know any questions on the slide you quickly showed that you had something like 2 million synthetic images are you ever worried about overfitting to your synthetic data set versus your real data set and also what is the ratio synthetic images to real so all that data was completely all the results there were a complete sin so there's been a lot of work with balance him and real datasets and so the goal of that paper was to show with no real data what you could get to yes overfitting is a huge problem that's where that asymptote came in in fact you begin to see performance decrease right so to that earlier point like there isn't enough variety in the simulator to really facilitate continued gains after some point but all of those results were two million simulated images 0 real images if you put religion the performance goes up but that's only if you put in real images from the domain you're looking at so if you're putting real kitty images it helps if you put in real cityscapes images no please yeah yeah yep how can humans learn from so few examples well that's a great question John we can stop there and I don't know have a good answer to that I think ultimately this highlights I think the problem we have in lots of these things right we don't I don't think we have a great model for why we're so much better at these tasks at with so much fewer examples right like it gets to the generalization our ability to generalize they just don't exist in the system and the brittleness of these systems is their inability to generalize and if I change cameras and it doesn't work it's just how ultimately fragile supervised learning is for solving some of these things so thing is the perfect question to transition after the reception and the other side of this room there's food and drinks and maybe we can find the answer thank you for anything thanks everybody [Applause] 