 It's really my privilege and my owner as well as my pleasure to introduce our speaker today. Professor George Canada kiss from Brown University, now in a way George doesn't need an introduction. He one can safely say that he's one of the world's Preeminent Applied Mathematicians and computational scientists with many, many pioneering contributions over the years. So but very quickly, George received his PhD from MIT in 1987. He was a faculty member at Princeton University. Before he moved to Brown University in 1993, where he is shown, Palmer, Bairstow, Professor and the Charles Pitts Professor. He's also affiliated with MIT, where his research scientists and with the Pacific Northwestern National Labs where he has a big group and he has many different groups. As you can see from his slides now here. As I said, he has many similar contributions in various areas of computational science and discontinuous collection methods. Spectral element methods. Generalized polynomial chaos fractional beedis microfluidics, nanofluidics and more recently in machine learning and really pioneering very important part of machine learning that he's going to talk about pins and its applications in scientific computing. George has his member of all fellow sorry of all professional societies that you can think of. Siam is me. Many awards and honors. The list is too long, but let me mention the Ralph Kliment rise from Siam in 2015 among others. So George, it's really our our pleasure to host you and maybe one just to testify. Testifying your impact is the fact that you have action index which is more than 100 now, which is really remarkable. So your impact applied mathematics is massive and we are truly delighted to have you here and we're looking forward to your talk. The floor is yours. Thank you very much. I said that very generous of you and can you hear me very well, very well that's great. I just invested on this nublu technology for for a microphone that's good. So I want to talk about. Computational solutions of partial differential equations. An using neural networks kind of different take on what you may have heard so far, because I want to talk about not just functional approximation but functional approximation and operators approximation and just so that. I give you the main theme from the very beginning of what you saw in this movie that I play before you play it again. I can play this anyway. What you see here is. Some data which some of my friends from Germany La Vision gave me and drink coffee every morning and just have my coffee. I was very curious to see what is the maximum velocity over my espresso Cup and can I quantify? The pressure field and the velocity field everywhere, given that they give me a temperature. So that's like something that curious and and have some data, but obviously it's an ill posed problem. So the main theme of my today's talk is can we solve problems using piddies which we cannot solve easily with conventional methods. OK, so that's why I'll get back to the coffee later on. But let me, by the way of introduction and outline, give you some Glossary that I will be using today. Pin is a physics informed neural network that about I will talk about this in the context of function approximation. DF net is functional approximation. I will give you some application and how what kind of network should approximate the functional? Very useful for autonomy. Another big applications important applications then my favorite one is Depot net. It's paper that will appear in nature machine intelligence soon and it has 16 different applications. Ann is basically neural network at the level of of operator of system identification and nonlinear operator and then how you can use if I have time if I don't run out of time I will talk about deep Eminem, this sweet new framework we introduce how you can use Depot Nets as building blocks to solve multiphysics and multiscale problems. Very very fast, maybe 10 thousand 100,000 times faster than the state of the art and that's our current. Project with DARPA and we have to produce results every week with this project that so I'm very excited about it. So just to get started. Universal function approximation first was developed by your side Pangle when he was Professor Assistant professor here at not far from where I live in Boston at Tufts University. He proposed if the function is continuous function, then single hidden layer with a activation function Sigma, which is non polynomial, would be a sig model at the time. What they said. Of course we know now this activation function could be some other. Nonlinear non polynomial function, but a single layer is enough to approximate this at the arbitrary dysfunction at arbitrary accuracy. This is sort of the basis of almost everything. The subsequent work by massacre. One of my collaborators, currently my new project on. Newly that we're kicking off tomorrow. Actually this this new Murray if you want to watch out. It's a multi University collaboration between myself Brown at Brown and Caltech and Stanford University of Utah on machine learning for Piddies, which is the topic of my talk today. So about discontinuous functions. This theory also covers this continuous functions, but shallow networks do not work very well, but deep networks work very expressive and work really well for for discontinuous functions, and I want to demonstrate that with something new that we have one of my collaborators, doctor young, young, John Shin. Develop the following. Thierry will not go through the theory on to tell you about the sort of a neural network. If you have functions which are discontinuous or solutions of Piddies which are discontinuous like in hyperbolic conservation laws, then the weights explode one of the observations you can make is that the way to explode. So the question is can you can you come up with a universal approximation theorem when you have a bound on the weights. For example, here we have a bound of two we don't want. Our weights, if you look at the distribution of the weights and just for those of you here are the weights. What we're after is these weights, which are basically these lines here and these biases. But these weights we don't really want these weights and bias to be the big greater than two, so we want to contain the growth of the weights given some initialization. So there's some details here which I will skip, but you can see here in this box. What I mean is that we can dial in using the theory, and I want to make the point that the theory is very practical tool in this case, and I'm I'm after very practical tools so. Puritan leaders to an architecture. And so I just want to demonstrate it is here. Given some inputs for, for example, in this theorem, like the input dimension, how many discontinuities you have an A controlling the accuracy to numbers that control the accuracy that you want, and so on. The how you represented some other constants you can come up with an architecture. Like for example, I want to approximate not just this function, but actually the advection. So linear advection of these functions du DT plus du DX equals 0. This is the initial condition and I want to approximate this. And I will tell you how later, but you can see I have an architecture that has two inputs and then the first letters, 20 second layer, 26 euros, 314. Sort of a crazy neuron numbering of neurons per layer that this theory gives us. But notice that results for long term time integration. There's this adaptive way of approximating functions in this. In this case, the solution of a linear vector equation, which is of course is the deceptively simple equation be cause. We see no dispersal node node diffusion in this with this simple architecture, if of course if you dial in any architecture, you get Wiggles, you get all the GIF stuff and so on. But this architecture gives you something good, so so there is some value. There's a lot of value actually in controlling the way. It's not just computing the weights, but controlling the weights and connect that to the functions that you are targeting. As I said here, sort of dial in some of the properties that targeting like the number of discontinuity. But the main thing I want to talk about today is. Data how will combined data and PD is in a Seamless Way and I work a lot with with the Department of Defense and somebody from the Army told me gave me this five deal or that the data he said their data is not smooth and nice like the academic people like their Dinkie which means small, dirty, noisy, dynamic streaming and deceptive. They said because the Russians everywhere. Sorry about that. I didn't mean to offend anybody but this is I wanted to quote. This I'm not English army person so so if you have this data how can you use them in the together with PD's? Of course there's lots of data simulation methods and so on, but we know how expensive are current data simulation methods. So from the three scenarios where we're used to for long time I work on lots of different methods myself and I always look at this left scenario where I know the physics. In other words, I know the PD and I know that. Parameters all the parameter Piddies and I love the smoothness and then they have small data. Presumably this will be my initial and boundary conditions fully determined, but. More often than not, I don't know my initial conditions and boundary conditions. I don't know this small parameters in front of the important terms. Even the Rangers number in Navier Stokes. I may not know. I don't there are there many properties in multiphysics problems, especially constitutive laws, dirty plasmas and so on. I partially announced plasmas. There's lots of uncertainty. How do you take that into account? How you use data to improvise? So I'm looking at this day. I will be looking at this middle scenario where I know some of the physics. And I don't know some other part of the physics or I may not simply I may not be able to resolve it, for example in turbulence even in isothermal turbulence, I May, I know exactly the equations. We know they're valid. However, if it's high Reynolds number turbulence, there's no way, even with exaflop computers, I can resolve your physic, your physical turbulence at the scale of 1,000,000 or hundreds of millions of in terms of radius number. OK, so there's no physics scenario. I will skip it today, but there's also possible to do something. On that, using lots of data and discovering new equations or new operators. In fact, I'll talk about that later, so. This brings me to pins physics in form Neural Networks, which I already define. As any new idea, this idea was resisted too close to our two years. Actually 2 to get this paper published because nobody wanted to use it to publish the paper, and now this paper has already. I think it has more than $600 or 700 + 700 citations, and it's a very simple concept. Let me introduce the concept of a pin. They said. This is a neural networks like we said before. Here we have two layers. Sigma is the activation function, X is the input. Use the output. Of course, if I have lots of data, can learn this very, very well. I can learn this function no problem. Neural networks are extremely well approximating. This function, continuous functions and discontinuous functions as I said. However, I don't have data in science. We never have enough data and Biomedicine would never have. Enough data, but we know that you satisfy some conservation law here. I pretend that conservation laws is parameterized. Nonlinear Schrodinger equation could be in the Navier Stokes, it could be a multi physics system. It could be some equations in some constraints, anything one of the difficulties of course is how you are you going to go back to standard numerical schemes, use finite elements and so on. Finite difference spectral elements that I like. But if you do that then you will end up having a grid. A grid is fine if you academic and you do homework problems, but if you're in an industry. Generating management takes six months real match for an airplane or or a gearbox or or digital twin of a complex system. So here instead of using. This. Conventional methods. Instead, we use automatic differentiation. Automatic differentiation is the same method that allows you to back propagate the error, and in the Standard neural network in terms of law in Pytorch. In all these frameworks now is standard, so we use automatic differentiation. Works really well and maybe not so well for very high, very high order derivatives, but certainly works well we have. I'm basically take the derivative of the neural network itself. Now I don't have a network here because if I take the derivative, this becomes a very complicated network. So what you see here in the box is actually another graph, another network which becomes very complicated if you use tensor board. For example, you can visualize it, but it's very complicated. The Nested graph. And of course if you take secondary, but if every time you take a derivative, the depth of this original network fits 10, it doubles to 20. If I take a second derivative, I will have a really deep network of with 40 hidden layers. So in the process what you have is is 2 loss functions. One will be with a mismatch of the data. The data doesn't have to be at. The boundary, doesn't have to be boundary conditions. Our initial conditions. It could be anything for initial value boundary value problem and then you have this extra term which will help you. It's like supervised learning. That's like the residual that try to compute at random points inside the domain. I will explain this, but let me just say that. Despite the the complex way of describing this. Sorry bout. This describes despite the complex way of describing. It is a very simple concept that very, very simple implementation and the industry loves it. Becausw because of its simplicity, so so companies like NVIDIA, arm seasons, Emmons now already started using this. In fact, that NVIDIA has already called parallel code that they give for free. It's pretty good code. With almost parallel perfect parallel efficiency on their GPU's and so on South, so the industry likes it and also the academic people like like it now and the reason the academic people like it is because they say it's not good enough so they want to improve it. In fact, recently someone student of a friend of mine gave a talk to my group and said that they improve it by 11,000 times and I wasn't. I wasn't offended. I was very happy to see all these people having a job. And writing good papers. So anyway, let me just explain the concept here using this burger equation. This various equation with Viscous Burgers Equation. So just implement first set up the domain is at a time domain and the domain the space domain. We never discretize this variables. These are continuous variable 60 and we take automatic differentiation. Here's the star points at the data points. The initial conditions. Here the boundary conditions and you can see that on purpose I don't have a lot of. Boundary data. I have some boundary data and that will be my first term in my loss function. Now the second term is the residual F. The residual of this of this burgers equation and I need to evaluate it at random points inside the domain using like let's say a space filling curve or Latin hypercube or some important sampling for high dimensional PDS. But random points inside your your domain. The extra domain. You can evaluate this NF points. So in terms of. Implementation and encode it tensorflow. You define the Neural Network, which I had here on the left. One here is just one line to define it, and then you need basically to work on the right hand side and the right hand side is only three 4 lines transferred. The neural network. Take the derivative first of the neural network in spacetime to eyes. Someday map put it in the loss function, then call your favorite Adam or otherwise minimizer. So by doing so that you can see that within an hour a graduate student, even a professor, I joke these days that even professors now can implement their own ideas. And you can get solutions. Pretty good solutions. Pretty good looking solutions of burgers equation in an hour starting from scratch, not knowing anything about numerical methods, so that's the convenient part I talked about. Of course there's a lot of danger with that because the solutions the optimizer is will not give you a unique solution. There's lots of dangers. That's why there's thing there's a lot of room for improvement. To show you this, I don't know, let me ask. My chair seed. Can you hear me please? Yes I can hear you very well and you have not seen this picture before right? I don't think so. So this is another another extension of pin, we call it the generalized domain decomposition. It says space time domain decomposition. So here we have two neural networks. One is the blue, the Blue Sea where the dolphin swims and the other one is the dolphin itself. It's an arbitrary, it's a gimmick to show you that we can. We can decompose the domain into subdomains. The Blue Domain is 1 neural network. We called Neural Network won the dolphin domain is another neural network. So we can decompose FaceTime. We can simultaneously work on both domains, so I will speed up the operation by a factor of two. If I do it in 100 hundred pieces, I speed up the operation. The operation by almost 100 because I can get almost parallel speedup and the reason I get parents without is unlike these continuous Gallery methods where I had to compute fluxes and continuous galerkin's dealing with stiffness matrices and so on. Here the extension is very, very simple basically. We want to simply make the residual of the conservation law continue, so that's very easy. So you can have convex non convex surfaces and so on. And you stitch these solutions together very simply by penalizing the jumps of the residual, which of course is 0. How you have to make it continues in the process we created a parallelism of course as I said, but also we create great flexibility. For example here I can have a different neural network on on the Blue Sea and I can have a different neural network. On their dolphin I can use different activation functions and so on and down. Here you can see that because I control the number of points on the boundary, I can have a really seamless integration in space time. That's what I wanted to make. The point that I have parallelism time, which as you can appreciate for initial value problems, is not trivial at all. You can use Arial, but there are other problems where that's not so easy, especially hyperbolic problems to do that. OK, so that's. After one extension I will talk about that later. This summarizes generalized domain decomposition. Again, if you have an arbitrary domain like this, you can have multiple subdomains and then it's sub domain. This is especially good for multiphysics, multiscale problems, where you have bonded layer. You have multi rate for example if you have chemistry. Later on I will show you what happens if I have chemistry with hypersonic problems. Very difficult problems, but this gives you an extra flexibility for it. Let me show you a couple of applications. This is something we published in science last February. We call it hidden fluid mechanics because we pretend that we have data, not only their noisy data, but their data on an auxiliary field. I don't have a day any data on the velocity in the pressure? I mean, there's never lost the pressure in fluid mechanics. Of course I have Daytona passive scalar, for example if I have. If you follow clouds, if you have if you have smoke. I have data on the smoke, had data on the dye, something that visualize what's going on behind an object. Like here I visualize a Patch of my domain of interest. That is this Patch and I have a video of that. OK can use this video to extract the velocity and pressure there, so as long as you have some gradients of the die like I have here and some time snapshots, then you can have. You can compose a neural network which is multi physics problem, namely. The passive scalar and the Navier Stokes equations with incompressibility constraints very easy to in this framework with LaGrange Multipliers too to include any type of constraints. But basically using the data, the auxiliary data, then you go to primary data. That's why we call this hidden fluid mechanics because it's sort of an analog. It's not exactly, but there's sort of an analog of a hidden Markov process. You know data is trying to infer the primary state, and that's what we do here, down here for one snapshot for the velocity and the temperature. They infer dynamics is within 1 to 2%. What you would get if you knew everything, and if you were solving the follower problem. Of course, here we solve the inverse problem together with the with the physics. If you have this die going around the object, you can see I can even compute the forces on the object, which is something useful if you do experiments in a wind tunnel or a water tunnel and do that now. This is another rail post problem. At timely we got this from a YouTube video. We downloaded that's how we became friends with low vision and German company. Big company on lasers and visualization. So we said OK, can you estimate the can we actually infer the velocity field around the mouth and the nose of this person? This is actually our collaborators Thomas Bergen La Vision and when he wears a mask or without the mask and here I just show you that we can quantify totally in time space the pressure field. This is a pressure puff that comes out of his mouth. We can quantify it. If he wears a mask, there's very little pressure field. You can see highly reduces a little bit from the side because he didn't use his mask very accurately, but basically velocity field, which we also computed 10 times small, smaller with. With the mask, this is a another I'm not going to show you a lot of applications today. I like applications but I just want to make the point. Working with real data. This is ultrasound testing of materials from Wright Patterson Air Force Base. They use that technique for the wings of airplanes and composites. This is an aluminum alloy. There is a surface crack you can see that you put in ultrasound module here and you send them ultrasound to the surface cracking or it will reflect the ultrasound and from that you can. See where the crack is, how you quantify exactly the crack is we use physics informed neural network here by having the wave the wave equation in this simple problem you can use the elasticity full elastic equations. If you are, it's an immersive crack, but in this case this was a hacker phone by a dozen teams at the last year in DARPA there was twelve teams. As I said, none of the teams were able to do it because they said we didn't have. They didn't have enough data. We found we had too much data. We only use 10% of the data of the ultrasound data and we nail this problem because we were able to actually quantify the wave speed. As you can see up here on the right. And if you quantify the wave speed, then you can exactly determine where the crack is. The size of the crack, the orientation and everything. OK, now the key to this was of course to include physics, and by including the physics you don't need so much data, you also have noise and you regularize the problem instead of using the arbitrary weight regularization using neural networks. So but another another important development was that we use what we call adaptive activation functions, and I want to show you what this is. We had done a lot of works in the last three years on this adaptive activation function. So as you know, in neural networks we have one nonlinear. Activation function website called Sigma before and that is the same for all neural networks. However, there's no reason for all neurons to fire at the same rate. There's no reason why all layers fire at the same rate, so we can make we can make the activation functions layer specific, or we can make it neuron specific and we can introduce a parameter for each one. Here A and here I can use a parameterized different activation functions to show you. That every neuron could have its own mind went to fire. Now this a becomes also hyperparameters. So now my trainable parameters using stochastic gradient descent will include awaits the bias and new parameters. These are not that many parameters, so even if every neuron has its own firing rate, it's still it's a small problem compared to the weights. For example, where you have all these combinations. But but you can do a layer wise, so that's actually adds maybe 1020 or 50 parameters at most. So in fact we found from practice at that layer wise activation adaptive activation is even better. This factor in here is the factor that the user can use to be more aggressive or less aggressive on this. Just an example here to see how this. Let's say we try to learn this function and if you use adaptive activation function after a few iterations you can nail the function. In fact, you can recover it. It's fully a spectrum here on the right totally for this low high frequency even discontinuity it generates the long long tail spectrum. You can totally recover it with adaptive activation function with a fixed activation function. You can see what happens you get stucked into a minimum. And you're there. But this is a loss, but you can see that eventually with adaptive activation function you can get the loss tourists more value. We have used that for benchmarks like Missy for and so on, and we beat everyone. So is there a theory behind it? There's actually theory. One of my postdocs jacked up and Maya jacked up, and Kenzie Kawaguchi from MIT sale. They prove theorem. That shows that adaptive activation functions depending on the specific case eliminate, but minimum, which is big deal. Of course, for training and so so so so it's very useful, especially if you have noise. If you have rough landscapes and so on. This is very important. There is another part of the theorem that don't have it here that shows that effectively what you do with your own wise activation function is like you have a second order STD. Without paying the price for occasion. So as you know, people like Jordan and so on have done great job in high order methods except nobody is using them in practice 'cause they're very expensive if added to compute occasions and so on. So you can use it. That sort of later stages, but this one allows you from the very beginning to get hired. OK, I want to kind of summarize what I said here. We develop a lot of different methods. By far by now, but I have show you anything, just standard pins and X pins their pins for high speed flows where we can bring conservativity variational pins, stochastic pins and blah blah blah blah. But here here it says stochastic pins. We use guns if you don't know guns. Guns is a really great concept. Is this adversarial training of a generator that uses arbitrary noise as an input? It generates fake data that discriminatory, which is the target network that you want to. Train. Both generated data in real data we have shown in this paper in Cisc which again take it took three years to publish this paper 'cause people don't like to see new things we we realize that we can actually dial in any stochastic process if you can dial in stochastic processes, you can definitely do stochastic piddies. This is an example of a PD that we solve the stochastic PD. It's the problem is that app is such that I know something about the right hand side which is stochastic excitation. I know a little bit about the solution, some some measurements somewhere, and we know something about the hydraulic conductivity we apply this to. Real problems like this Hanford side, which is the largest nuclear waste site in Hanford close to in Seattle, northwest of the United States, where apinan alias where directing the center. We had an application, so this is kind of introducing uncertainty in stochastic PD. Now we modify the standard gun to have also. A generator for all the stochastic feels there right hand side and maybe the boundary conditions generator for the conductivity and you so then the discriminator. See this concatenated vector of realizations from the stochastic fields plus some real data. This would be real measurements. For example if you were looking at polls media, there will be like 1000 wells which their data actually from this huge nuclear waste site. And so on. South. What was the benefit? From this? I will not go through the this paper, but we could. We were able to go 220 dimensions and eventually 10,000 dimensions using a big computer, but within quite bitter curse of dimensionality a lot of people talk about that, but we made the cost almost linear, quadratic at most with the number of dimensions. Variational pincers, nice mathematical underpinning, tweet it say. Let's say we're trying to solve this. Passion or passion equation you can integrate by parts once or twice and and you can mix spaces. One of the interesting things with if you use their petrol galerkin approach, you can have the neural network being the approximation space and there's a good reason for for that I show you this adaptive adaptivity built in in that nonlinear space, and then you can have the best functions being smooth functions. There's an elegant way of doing this. In fact with simple Ray lewenstein functions you can find yes, yes, questions that are coming in. But my suggestion is that we take all the questions at the end, is that OK? Yes, yes. That 'cause I want to. I want to show you the Depot net and I cannot afford to just exactly right, so let's see. This is to show you hear that thank you, Steve. This is to show you that actually you can write Analytically and do some theory. You can actually with a revenue function. You can write Analytically also for a couple of more layers and cut up you appreciate if actually how this. Big spaces how these residuals look and how these are different methods and so on. So variational pins is interesting. Let me briefly talk about the math of pins. This is a big frontier. This is, I think this audience would be really interested in that there is this approximation. Especially I will not talk about here, but approximation when you have this. What I 5D data I said earlier, the multiple data, not all data are the same level, not. And how you bring that in the nonlinear approximation theory? But the main issue here is generalization, and I want to mention the two papers too. Seminole papers by the Chair Submiss Rahu. Motivated, he was motivated by some of these applications of pins and produced perhaps the best two papers on generalization already of pins. Generalization error refers to the fact that although you're in this neural network, this HM space and you can do a good job from the approximation in the universal theorems of approximation, you can minimize this by making a really big and expressive neural network, making this box bigger and bigger cloud going closer to the. So the true solution, then there's always this approximate this generalization error becausw you replace your Journal. You can never exhaust the training space you can ever. You have to use a. You go from a integral to Quadratures. That's very important error, of course, as always, the optimization error. The fact that you, which is another beast, but you can see now how this analysis of this. Approximations for PDS is much more even for function approximation, but certainly for pieces is much more complicated. The complication is that I'm interested in solving ill posed problems so you don't have a standard well posedness you have to find conditions or conditional well well posedness and that's the paper of of Professor Mr that they exploit that type of knowledge. Another paper that that some of my collaborators worked on is. Is based on a different approach to this, again tackling the issue of convergence generalization, but for linear piddies and elliptic and parabolic's, first of all we wanted to show that even without the boundary conditions pins converge as you make the space. HM, bigger it converge in the L2 space if you include also the boundary conditions, you can find the H1 converges another paper which. We haven't put on the archive yet, but it's complete. Just about to submit it. Is using equivalence of norms and compactness arguments to derive a priority in a posterior estimates in terms of the solution or actually in terms of the functional which you compute the posteriori and their two formulations, that discrete formulation, the continuous formulation and this down? Here is a sketch just standard when you have continuous form lesson. If you have the expressivity of the Neural Network you can show convergence generalization. We refer to generalization as compatibility compatible networks. Or other conditions? I don't have time to go through this very technical paper, but it will appear in the archive version. See I don't know if you can see this movie. It's a huge movie so I don't know if you can see I can see it perfectly and destroyer real destroyer of the US Navy going through North Atlantic is supposed to be autonomous. No crew, no captain. Can we do it? The answer, we cannot do it because if I were to use CFD to predict how this the dynamic motion of this destroyer takes about one week for one simulation for one state, this is state 8 where. 20 meters waves are in front of you. This aerial spectrum of the North Atlantic. So standard conventional methods will never meet what you need for autonomy. For cars or airplanes, and so on, or the ship. So we want to use neural networks to approximate to as an input will have a stochastic input as as the stochastic. See elevation as an output. I will have the sekeping motion dynamic degrees of freedom, so that gives me the opportunity to talk about the second theorem. That second theorem is by Shannon Shannon. It just so happened that Junping Chen, who developed this at the same time at your side Pango, he is now 8687 years old and he sent me an email This morning. What is going? Yes, an an he he proved his theorem. I mean is is such and not many people know about this theorem. He proved this theorem and he published in IEEE Journal in 1993. So we started. Yes there is a network that can approximate that. How can you do it? It turns out with today's technology we can use analysis TM. We can use Google's sequence to sequence and so on to other stochastic input to different realizations to get like 3 degrees of freedom. And you can see here. This is now the first column is for the first excitation, second column, second excitation. This is all unseen data for the train day for the train neural network that we can predict the function of. It completed whatever you want. You can predict the time series here. Underneath you can see the CFD in red, but clearly we can do a very good job in in how fast you can do this. You can do this in .01 second on the laptop, so now we're meeting real time requirements. But this is more interesting and again, going back to Chen and Chen at the same time a couple of years later they prove something so profound, which I these days I spend all my time working on this topic, and that is. If you really want to talk about generalization, you have to go to higher levels of abstraction. Overparameterized neural networks may do a good job, and there's a lot of controversy and lots of interest in that, but if you go to a higher level of abstraction if you learn that operator, you know you can do much better. You don't want this robot here to go through all the calculus that we did, because then they have to calculate the way we try to calculate using hexa flop computers. So this robot will have a huge head like an excellent computer. With a thermal cooling system which is bigger than my house, so so that's not the way to go about intelligence and Collective Intelligence. And so on. Robots, the way to do it is by system identification by offline training and so on. So this theorem of challenge and basically extends the Cypress CYP angle theorem in their own theorem to nonlinear operators. So now the input is functions from this space, compact Space V. The output are functions and G is the mapping of the input of. The output is a nonlinear continuous operator, so the theorem of Chen and Chen says that any. Mapping like that in a non linear operator user input can be approximated by this double summation on a neural network which is the output here which will call trunk and an input which will call the branch so you can see the input is here which you sample from the space V and then the output is there because because that's also the output itself is because you have Geo view of Y is also a neural network. This is for a signal layer, but recently and this is in the paper that is coming up in nature machine learning machine intelligence, we prove that this could be also extended to an arbitrary deep neural network. It's not very different from there's some technical arguments, the compact space. Of course in application of the compact spaces is a big constraint, but but as you will see, most of my in my examples I take compact space, which is a gauss random field which is not non compact. So it works with non compact, but it's impossible to prove it. OK, So what does this set up the top is here. Let's let's look at panel BI have panel be. I observe my function UA second function you another 100 different functions you and then I observe the output at a few points. Usually observed the input at many more points than the Outputs. If I go back here to panel a, then you give me a new function of from inside or outside the space V and I want to give you the output in terms of functions. OK, now one of the main things when you have the approximation theory theorem is as I said, Generalization. We talk about Generalization. The question is can you? Design your networks with small generalization error. And today, with there's so many papers out there, one can control everything. The space V, by the way, is very important as you can appreciate, appreciate. So that's also an area of where research can go. I just want to give you a teaser. If we use a Gaussian random field, here is a Gaussian, for example with a kernel with a correlation length L. And if I do a sampling with uniform sampling, you can define this Copa here in terms of M, the number of points we're sampling in this space V. And then you can find that Copa is of the order of em square 1 / M ^2 number of Points Square is sampling correlation Link Square and then you can prove something like that that this neural network that approximates this operator G is certainly bounded by this error. OK, now how materialize this? How do you actually implement this? In practice, this is what this is. Our contribution that we propose this neural network. Let's consider this unstuck case, which turns out to be the best in terms of generalization error, so there's the branch network that implements this big parentheses that will relate to the input, and then there's a trunk with the output, and then here you can see we cross. We just do this math here by crossing it on P points PPR. The points on the output K defines the output. M is the number whereas. I'm sampling OK so Depot net this deponents based on rigorous approximation theory. I want to give you a very simple example how we actually do this. I'll be done in about. 8 minutes. Is that OK? Absolutely, George. Take your time. OK, so this is an integral you can see that. I used 10,000 functions. I only observe at one point and I find that I can learn this integral operator and hear the testing error and the training error is basically about the same. If you use a another full feedforward network or something else the you can see this difference is the generalization error so I can learn these functions. I can learn exponentially fast. This is a pendulum, the input is U, the output is 2 functions. You can see that I can learn exponentially fast for this network. But then eventually I saturate, was a break and very slow convergence. I guess somebody out there maybe in the audience today, is smart enough. The smarter than us to design the neural network that will maintain this exponential learning rate. You can do that for PDS. I want to show you that I can do it also for differential for fractional differential operators. And here I use a puto derivative Caputo Derivative. Let me just show you what it is. A couple of derivatives. This side is defined up here. The singular kernel used in initial value problems for fractional piddies. So the idea is, is again I use numeric standard numerical methods to derive this outputs. I use lots of functions as a nipple input and then I can. It's expensive to evaluate everything in fractional is expensive, so can I amortize all that cost? Can I learn Depot net that does that for me? It turns out yes and this middle plot here is to tell me to tell to tell you that the mean square error can be very small, but it depends so much how you actually. Approximate the input space. For example, if I use a GRFI get this green line. However, if I use Poly Fracton Amiel's, which is a spectral methods that I extended to like legend polynomials, or that they extended to fractional singular value problems. Then this eigenfunctions of those, if I present my function VI can get. I can increase the accuracy tremendously, and then I can. I can compute this fractional operators. I can infer them very quickly. This becomes very important for the Laplacian bounded domains. Here, Myspace V is represented by their nikkah polynomials. I don't need a lot, but you can see again if I use. In fact, one can use other Neural Networks CNN. This is one of the cases we found in a nice domain like this at the CNN. I can map this to a nice image Square. It could outperform actually deponent sodipo net is very powerful, but it's not necessarily the best and I'm sure the concept is very strong. Other people can find I'll skip the stochastic differential equations because it's very similar. For example with color noise, you can see what we're doing here. The input is represented by. Michael Cohen live expansion, so the input is bigger and the output is of course space is. The is becomes bigger I using this truncation I get very accurate solutions for samples I want to show you this. This is deponent now learning Euler equations with non equilibrium chemistry. OK this is not a trivial even for double you know this is a recent paper that my colleague shoe. Producing in JCP. Give us the code to generate data. Give us the code you can see. We have three species who generate this data. We only took hundred trajectories for training, and we use 50 trajectories for testing. Now here's what we did. We use the conservation laws we started from initial conditions from nitrogen, oxygen, and velocities, and so on. We can basically predict at anytime and in the next XT. We can predict all this and we can predict it very, very quickly what the input is is this parameterized form of the jump in the pressure and the smoothness of the of the jump. Of course, from that you can generate a lot of initial conditions, so at ETA is very is very continuously between point auto and .2 and basically you controlling the gradient of this and then jumps in the left and right in this Riemann solver for initial condition. So that can give you anything. But here's an example. After sometime I can predict, for example, at Target time this red line starting from initial conditions from black you can see for all species I can predict again without solving the problem. The Depot net can predict this with 10 to the minus 5 accuracy. We have some theory for it. Let me give you a teaser for Conservation laws. This is the theorem that shows error estimates on the right hand side. Here I have the number of points where we sample the input. Here P is the output, 1 / P. Then here's the parameters of the branch network and these are empty. Is the number of the trunk which is the output network. This looks good, except this middle term. Cause here I have square root of the number of points and this number of points is the is a large number, so we're trying to get rid of this. Something in log or something like that, but to prove convergence. Oh well, this is an open problem. I invite you to actually do the theory better than us. We're still working on it ourselves, but if you have any ideas this is a good place to tackle. I. Let's see how much time do I have. Let me take 2 minutes to show you this hypersonic problem because I want to talk about Depot net dependent. M sodipo Nets are building blocks for very complicated problems. So this is a current problem of interests around the world. From the Chinese to Russians and especially the United States so. I'm looking here at the specific example of five speeches now non equilibrium chemistry mark #8 to 10. So I use standard solvers to learn Depot Nets for all these fields separate fields OK, I use 240 trajectories. I use a DNS to produce the fields in this Mac #8 to 10. You can see I can go from chemistry to velocity from and temperature from chemistry. Rusty, but these are. These are my datasets in this market number I want to show you that it's not trivial. Look at the velocity, there's a very very steep structure both in the species and almost eight orders of magnitude. So here we did. Math #8 and 10. Then we can predict from temperature, velocity and predict all the chemistry in .1 second and vice versa. But the concept of Eminem is the following is multi physics and. Basically, in hypersonic problems another electric convection and so on. You have some measurements somewhere, for example for the slow scales. For chemistry you may not have anything except the initial conditions. But here on the fly literally you may. You may have some measurements for velocity and temperature and pressure for example. So you use this. You have this prefabricated pre train networks offline in a split of a second. You predict the Newfields. Then you pipe that through this. Other Deponents which are pre trained and then with this loop is a Standard neural network. You now have a loss function which you test your operators. This is the operator here. Basically, why are the fields you trying to predict measures the the outputs of the operators and then these are the real data and we have shown that we just very few data to three data. You can really solve multiphysics problems for unseen data and this is a paradigm we have not published this yet. This is the first time I actually second time I showed this show this on Friday. But but this shows you the possibilities that, so in terms of accuracy, the accuracy is pretty good in terms of speed. We do 50,000 times faster than DNS. In predicting this, let me finish here. I want to treat you with my espresso coffee. Now you can see the mug has the same thing as my background that Sodipo net deponent. That's it. That's a pin I want to tell you a story about this, because La Vision, they give us this data. And we predict that the maximum velocity in my Espresso Cup is .4 meters per second. I said, You know, I'm in engineering. I know about these things. There's no way that this is .4 meters per second. Such a slow flow. And then they said yes, they only find .08 meters per second from there. From there you know naked, naked eye type of measurement. Then they set up an experiment with PV particulare modulo symmetry, and they found that indeed the velocity that the neural network predict was point 4.4. The .45, so it was a blind comparison we did. They were were very gratified to see this, and there's a library for those of you who want to start. Playing with this for any deep XD work for any at the level of Education ordered the level of of real problems, but it's a good way to get started. This library I was just told by my collaborator has 5056 thousand downloads and it's about a year old so I guess students are using it a lot. I would like to think spring and I like the thing seed for inviting me here today and I don't know if there's time I can take some questions. Yes absolutely. First of all thank you George for this superb talk. India Tour de force if I may say so with lots of different concepts as as we expect from you so. Delighted that you gave this talk and it has a lot of responsible already. A lot of questions. Of course we all are busy people, so I tried to ask you some relevant ones. I have the questions in front of me. Someone is saying, for instance, this one is a relevant question from Nick. Nick does the work on STD SDS with Pi guns work for Ito Stochastic Piddies as well? Ah, really, good question. I was asked this before. We don't. Have that yet. There is something we published. In fact, yesterday every every Saturday at 11 we have an ad hoc group working on something like that, but we have something that you may be interested in. It's called generative. Uh. And symbol regression. So we're looking at from unpaired data looking stochastic, ODS and so on. But we haven't. We are considering this, but we don't have anything that we can answer this meaningfully for you, so, so most of the stochastic stuff we did so far is more like color noise with truncated and so not not what you have in mind. Someone is asking about Caputo fractional derivative, but maybe I just built up a little bit on this question and ask you yeah, this work on fractional. You had this enormous body of work and fractional derivatives before and now with this F pins. So what's the message here? Means is that there are certain models, refraction, derivatives are good or yeah. No, no, you're right, you're right. So so so the main idea is to cut the cost with fractional pins and that would you know that's a good question because I need to clarify. There's no automatic integration in any of these frameworks. Are automatic differentiation, automatic differentiation? Automatic integration is basically what I show you that one day we were trying to use Depot net, so I was thinking how we can build basically automatic integration using this Depot net. And of course, if you do that then you can deal with integral operators computer. Otherwise, in particular, computing the fractional Laplacian on a finite domain is really very expensive, and I have written papers on H metrics, another pay and other people have done that and so on. But still we want to. We want to compute and this comes from a real need. For example, I was trying to do terrible and Simulation Subgrid and I wanted to use. Nonlocal models for that, and it was almost impossible, although was more accurate. It was very expensive to compute the nonlocal subgrid models for closure, so so we thought that we need to do something like that to compute. You know fractional appliance ingredients and so on to compute it, very precomputed, and then just spit it out very quickly. That was sort of the motivation, but maybe I I ask you a question, just a quick follow up on that because you have this very nice machinery of people net which I have to understand each time you give me at give a talk and I feel that I have to understand this more now. But so now you had this compactness assumption somewhere in the approximation, but you have been made available space the input space. Here the input space, but we have been using sort of Gaussian random fields and fractal normals and so on. Bad this assumption. Is feeling right and at the same excellent results. So yes, so I think you will see also in the challenge and if you go through their theorem you will see you need that to do that. Now if if anybody wants if you are able to like with some weighted. Space too. To remove that and go to Infinity, but waited. I think that that's probably just, but we haven't been able to do it so, but in in your right, for example. You know you want to do to learn Laplace transform, right? Do quickly laplace transform so arbitrary functions and so on? That that's that's very important, and so that's why we're pushing this for the applications. But the theory itself doesn't cover us for that Case No, but of course this is theory is always late to the party, but another. It's very important. I mean, if we can even not removed, but like you some waiting, waiting spaces too. That I think that will be. A huge step forward also and and what did you use for the input space? For instance, for this compressible euler chemistry problems, what was it? Was it a finite dimensional input space that yeah, finite dimensional space and one of the questions that we ask that kind of really interesting question is what happens if you extrapolate outside the map #8 to 10? Let's say you go to model number 13. It turns out actually, you know that's what Darpa's interest, of course, and it turns out it works really well, for example. In the worst case scenario, we get a deponent to give you 20% error. OK, without nothing now if you increase the number, if you can reach that space V like I use only 100 Projectors. If I use 300 trajectories the error the extrapolation error. That's the interesting thing. The Extrapolation regards down to 10%. So you refine more the space. The input space. Then you do a better extrapolation which makes some sense, but it's good to see that maybe depends on this on the on the application. Also. For example if I go on the other side on Mark #7 or 5 or 4. Because there's no chemistry, I cannot predict it very accurately, of course, so it depends also on how smooth solution extends to those. So it is really interesting from the point of view of the of the application of you, as you can imagine absolutely yes. So any further questions. If not, I think we stop here. We all have to go and we thank you. Know there is one question. Yeah, so it's for next week. Yeah, thanks a lot. George again for this very sort of as I said it was Real Tour de force with so many different components and one really has to think a lot in order to just get things absorbed. So thank you very much and I think it's highly appreciated what you're doing. Charm. Thank you. Thank you. Thank you very much. Thanks everybody. Have a good day. Bye bye child thanks adults. Thanks for joining everybody. 