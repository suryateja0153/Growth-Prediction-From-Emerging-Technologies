 hi my name is adrian hernandez and today i will be presenting my am project named smart first let's start with a rather oversimplified explanation of what our neural network is we can consider a neural network being a mathematical function that takes an input and produces an output on ai we want the output that the function produces to match other outputs that we have for the certain input that it's taking so we take an input we calculate the output then we calculate a loss from the output that the network actually calculated to what was the true output from the input that was given and we back propagated that mass in order to update the function we repeated this process many times in order to obtain better and better results each time that is what training a neural network is for this specific project what i wanted to do was to generate pictures of paintings and for this purpose i elected one specific type of network again a generative and virtual network that's what gas stands for you need generative because well it generates something in this case it's going to be pictures of of paintings adversely because it's a system of two networks trying to outsmart each other the generator and the discriminator for this example we can consider the discriminator victim being an art critic and the generator an artist and on this network the art critic is going to tell or is going to try to tell if a picture is going to be generated from the artist or generated by an actual artist from the real life not like the artist network and then through a training process they are going to become better and better at this game in order to perform better and to produce best results okay let's explain in more detail how we train against or how i train them for this specific purpose first i have two sources for the pictures i have a dataset that is just a bunch of real pictures real images real paintings generated by real people and then i have my second source of pictures my generated generative network my artists i'm going to take a picture from the data set to first train the discriminator and i'm going to tell him this is what a real picture of a painting is that we're going to take a picture generated by the generator that at the beginning of the training is going to look like nice pretty much as you can see in the image and i'm going to tell the discriminator the critique the art critic this is a picture a painting generated by the network so it's fake then by this process the discriminatory dart critique is going to tell okay now i can tell pretty much how it is and for the next step we train the artist because at this point the critique has become better and doing what it does but the artist doesn't to train the artist we basically make it fit a painting to the discriminator and we mark that painting as if it was real then the loss calculated by the discriminator is going to be their loss of the art critique no the loss of the artist to actually match a real painting because if it was marked like a real painting and the this art critique was already trained for one step then what it was missing was actually the failure or the loss of the generator the artist to actually create a real painting okay so at this point we know that we're marking things like real or fake but we don't know exactly what is happening on detail what happens here is that the discriminator the art critique it's actually providing more information to the artists from like just if his painting was real or fake it actually provides information about little details and it tells him it is it looked fake because of this and it looked real because of this so now when the artist is submitting optimizing itself there is actually going to be some things on itself that he's going to increase because that was what the art critique told him that was good about his work for this example and there are other parameters that he's going to decrease and this repeatedly quickly it's going to create the artist to generate better paintings okay so let's talk about the first approach i took about my project by a problem that was generating art itself now my thinking at this point was okay i know what a gun is so i'm going to fit it pictures of thousands and i mean thousands ten thousand three more precise images of art that i got from a data set of a museum and i'm going to tell the generator to actually train to replicate this so that was what i did and these were the results that i got okay here are my first results as you can see they like if you look them from pretty far away far away you could say that there are real paintings but there are not figurative paintings and you cannot spot people on them or special objects they're just like paintings abstract paintings that's what they look like i mean they're sharp and definitely the network got to capture many of the results many details but they didn't have like specific objects with them like here are more examples and as you can see they still look like abstract paintings although they look sharp and like if you look them from very far away you could say that they're real paintings but at this point i would like something when looking at my data set because i was like why wasn't this network generating the results that i wanted and what i realized is that while i wanted results that looked like this people pretty much was what i wanted i had a lot and i mean a lot of images of this of images that could be considered art but weren't exactly what i was looking for on my data set and what i mean a lot of images i'm meaning that like maybe like twenty percent two thousand out of the ten thousand percent images of my data set or this and i wasn't going to divide this all by hand so i needed to rethink what i what my goal was and what was my method to get to it here's my second approach now i'm taking the first the first architecture of network that i had before the same network that i used to generate the images of paintings before i used it to generate something else now i wasn't able to get a pure data set of the images that i wanted portraits because after revising the results i realized that that was like a pretty specific thing that i wanted i wanted my aia to generate portraits rt portraits so what i did was okay i'm going to generate pictures of faces and then somehow i'm going to turn them into portraits i'm going to put an art style on them i at this point i did a new and i'm going to get to that later so i trained my gang to generate faces of people and these were the results that i got as you can see they look pretty much like faces than people and compared to the results from before you can actually tell there's an object here and at this point i already had the pictures of people now the only thing that i needed was to actually convert them or to put an art style on them and for this i needed to implement something else something called neural style transfer neural style transfer to be what i'm going to be using normal style transfer here is to transform the images that i already have of faces into portraits that actually have an art style okay so i will start transfer network it's different from a network that i already had the gun so for this network i take a pre-trained bgg 19 network that is a network for object detection and i just just use certain layers of it to extract features of images for this process i'm going to need three images a content image that is going to be my face image the face generated by the gan and the second image it's going to be an art style image this one is going to be from an artist of my preference in this example i'll be using vanga now for this process first i pass the content image through the network and out of certain layers there are going to be features extraction that are going to be the outputs of certain layers i'm going to actually save and record those features extracted by the network the features that it extracted from the content image and then i'm going to do the same for the painting that i want to extract the style from now i have my features set for those two paintings for the layers that i want of the future extraction of the pre-trained network and what i'm actually going to be optimizing is a third image that is going to be at the beginning the same as the content image now i'm going to optimize my image of input in this case everything on the network is going to be pretty much fixed what we actually are going to be optimizing is the input image and we do this by passing the image through the network that i already mentioned and for each layer i'm going to compare the features that were extracted from the input image to the features that were extracted from both the content image and the style image and i'm going to calculate losses for those and depending on the losses and the losses for those features i'm going to be optimizing the image and repeating this process over and over is going to eventually make the image to become a balance over the content image and the style image here's some examples of this process over the same image and with different styles of artists here's a quick demo about the final program you can see that it generates portraits you 