 FEMALE: So, let me start by saying that when I was putting this talk together, I was trying to decide that’s first, that’s first. I’ve never been to a roomful of epidemiologists. So, I’ve decided to make it sort of breath [sic] first. So, I might skip some slides and then hope that we have kind of a direct discussion afterwards based on what everybody’s interest is. So, what I’m going to try to talk to you about today is kind of challenges in school situations versus static data. So, as Sarah mentioned, we are working on the same broader collaboration together and this is kind of a complementary work to the one that she’s talked about. But I should first acknowledge my colleagues, Andrea Gonzales, [unintelligible] Ray and Brandon Soffer [phonetic], also Lara Silvermore [phonetic] and our cancer experts, collaborators at IMS and at NCI. So, I guess maybe it’s obvious to all of you, but one thing that I like to start saying is what is the value of synthetic data? Right. The primary purpose is not to drive your knowledge like you have from real data. But there are a lot of secondary uses that are really valuable. I list a few of them here. But from our point of view, we are really interested in synthetic data for accelerating research whether it be because the data is not available or it’s going to take a year for legal arguments to come through or because the data is heavily biased and you want to augment it. So, there are a number of ways that synthetic data can be used to accelerate research. And specifically, coming from the machine learning perspective, developing complex models, you generally need very large datasets, synthetic data can be really helpful for that. And so, the first consideration is what is good data? So, in machine learning, we know when you have a task, it’s really easy to define whether it’s good or bad. If it’s a predictive model, you want to know did you predict it right or not? If it’s a clustering task, you can see like how homogeneous are my clusters? Are they making sense from an expert point of view? Synthetic data, the answer is a lot greater than any of these things. And I like to list kind of the aspects of goodness of synthetic data in three ways. The first and foremost is privacy preserving, especially when you’re talking about patient data. One of the reasons data is not available is because we’re concerned about protecting identities and privacy. And so, that’s the main sort of necessary condition but not sufficient for good synthetic data. And then I put it in quotes because I think the term is clumsy, but I talk about individual realism and population realism. You want each one of your individual samples in your synthetic data to be realistic. You don’t prostate cancer in a female patient. But you also want your general synthetic population to reflect the same type of overall statistics of the general population. And for each one of these kinds of broad criteria, there are many ways that you could go about it. And here’s the [unintelligible] of my talk. There isn’t a single best way to do it. It’s one of these three criteria. You could have tens of hundreds of different match rates. And how do you go about selecting it? And how do you go about saying, “Okay, when good is good enough”? So, I don’t have close [unintelligible] answers, but I’m happy to discuss ad nauseum. And one more consideration for specifically medical and patient data is that it is primarily categorical. And that’s for anybody that works with physical [sic] modeling, it will be very obvious that it’s a specific challenge that kind of makes a broad variety of methods that are available to us during the data, kind of not even an option. Because modeling correlations in a high-dimensional categorical space is tricky. And here’s my kind of --. I imagine for most of you this is obvious, but if it’s not, imagine having two features and now you’re trying to model their pairwise correlations. Like age and gender, one of them has two levels. The other one has, if you bend them, a bunch of levels -- 100 or 10 levels. Okay, but now we have another feature. Okay, now you have to model all the possible three-wise correlations. Now you have four. And now when you get to [unintelligible] large, you get to a common [unintelligible] explosion. And what that means, in fact, is that you need a lot of data. And it’s worth to think about it’s not a matrix, it’s going to be like a hyper Q [sic], it’s really sparse. A lot of cells in there are empty. You don’t have examples for that. So, how do you go about estimating this? And this is a problem not just in microdata, but high-dimensional, multi-varied, categorical data. And so that’s something that we have to have in mind when thinking about synthetic data for these types of problems that we’re talking about. And so, the way that I see it and this is not a standard division of approaches, but it’s kind of how I see the landscape is that there are two broad categories of approaches for synthetic data. One is very much in lines of what Sarah talked about. It’s what I’m calling profit-driven methods. So, you basically combine some population statistics with some kind of domain knowledge that in the case of Sarah, she’s talking to these cancer experts and talking about, okay, what states can you transition to? And what’s the probability of going from no cancer to cancer in one year and so on and so forth? And one of kind of the main things about this type of method is that you don’t use patient data or real data, which is really good from the standpoint of protecting people’s privacy. You’re not using actual people’s data, so that problem is not a problem. On the other hand, you have what I’m calling data-driven methods and those are the methods that I’m going to talk about today. And those are methods where you have a model with all of your assumptions in it. And you use real data to learn parameters in the model and then you can [unintelligible] from that model and generate data from it. The obvious drawback relies entirely on real data. And so, reidentification is a possibility and it has to be taken seriously. But on kind of the other side of the coin, is that the reliance on the main knowledge expertise. So, in the case of this process-driven method, depending on your specific application, you might want to have to talk to a case expert about the probability of transitioning from one state to the other or what’s the probability of getting cancer if you have heart disease? And I couldn’t even start to think about all the different things you have to consider and that’s a pretty daunting task. And so, it’s very application specific. You can’t just get a method and throw it at -- go from cancer to obesity and be able to just leverage that. The data-driven method, because we don’t leverage the main specific knowledge, the application is agnostic [sic]. So, you have data, you learn a model, you generate data, you see how good that data is. So, there are a lot of parallels with an earlier conversation about how do you go about science? Is it hypothesis-driven or data-driven? It’s the same thing here. And, of course, it’s never black and white. There is like a whole spectrum of hybrid methods that you could see in between. And so, what I’m going to do is tell you a little bit about a comparative study that we’re doing and I’ll see if I get to the results. But the main point is just what I think will be most useful, you know, for this workshop is talking about the thought process and what are the considerations. And then kind of discuss that with all of you. But for this particular case, yes, you’re going to need peer data because of the collaboration. And we are looking at different data-driven methods. And, obviously, within data-driven methods, there are a whole variety of broad classes or sub-classes, if you will. So, we tried to categorize them to these three groups and use some examples of each one of those. And then you have to say, “Okay, how am I going to compare them? I have synthetic data 1, synthetic data 2. Which one is better?” And other punchlines [sic] it depends on your application. If you are using this, for example, our collaborators at IMS, they want to be able to have synthetic data that they use to help people train on their systems. You don’t need the very intricate correlations across the different variables. So, the bar is a lot lower. They just don’t want to disclose and have people have to go through the whole -- I don’t know, what is the training process to be able to access that data. But you might want to just eliminate all of that. But if you are looking at using this data for predictive models, for machine learning models, for statistical models, then you really care about these correlations in addition to the privacy protection. And so, there are a lot of different metrics. I'll show you results from a couple of them. But one thing that I said earlier in another conversation is you want to look at all different aspects. If you shift your way of looking at it and go at the problem just a little bit, something different may [unintelligible]. And a lot of these matches will tell you redundant conclusions for this. But, I guess, instead of thinking of that as kind of like a work of effort, it’s more validation of whatever it is that you’re saying. So, for the results here today, we just look at Cancer Diagnosis 2010 and we look at a breast cancer cohort. There are about 26,000 cases. And then respiratory tract cancer cohorts, which has about 18,000 cases. And I couldn’t tell you what each one of these features are but this is from the tier [sic] data and all of you are familiar with that. And so, what you’re seeing here is each row is a feature. And then each column -- oh, here, I will show you results from three datasets, but it’s actually just two because they’re redundant. But what you’re seeing is basically the number of levels for each one of those features. So, in the breast case, that has two levels in all of those. But, for example, in the histology for breast you have 102 codes, for respiratory you have 169 codes. And so, you can kind of get a sense of just how high-dimensional and kind of -- yeah, that’s how this data is. And so, I have three minutes, so what I’m going to do is just tell you broadly about a type of method and then just before I [unintelligible], we can go into details. But in the synthetic data, data imputation, disclosure, 56 areas of research. There are several kinds of broad classes of methods. One of them is this kind of imputation method. So, you start by saying, “Okay, I have real data. Some other features need to be protected, not all of them. So, I’m going to use the ones that don’t need to be protected, throw away the ones that need to be protected. And I’m going to treat that as a missing data problem and throw that out. And so, here’s our -- this thing doesn’t point -- but this is what that one in the middle is. And then you have like a whole class of kind of probabilistic graphical model approaches, a lot of [unintelligible] models that are really good for [unintelligible] classification, which is really important in this case. And in fact, somebody here -- I think you, Vicara [sic] -- was asking Sarah about, okay, what do you do when the data is noisy, right? Well, depending on the type of model that you have, you can get at that using the uncertain classification if you have methods that are able to get at that. Because if the signal from that one feature is very noisy, you’re going to get a broader error band, if you will, on whatever estimate you have. And so, we look at some full joint posterior probability models that try to get at the correlation, estimating the correlation a different way. And then, for those of you that are familiar with kind of machine learning and just learning in general, there is this class of methods called Generative Adversarial Network that comes from kind of the viewpoint of like gang theory, where you basically have two networks. One is trying to generate synthetic data; the other one is trying to discriminate between the synthetic data and the real data. And you put them against each other and try to generate really good, quality, synthetic data. And so, those are the kinds of classes of models that we are looking at. And metrics. I have one minute and a half. So, there are a lot of metrics that get at trying to identify whether the synthetic data captures the same level of relationships across the many features. And there are a lot of metrics that get at are you disclosing real patient data? And I listed a few here. And these are all metrics for which we have results. I’m not going to go into the details, but in the discussion if it is of interest, I'll go into that. And what I’m going to do -- oh, wow, this is bad planning. But I'll go to this one. What we did was compare a bunch of different methods and what you’re seeing here is basically one of kind of modeling the correlation level across the different features. And what you’re seeing is that different methods are doing better than others. And then when you put that same metric against patient disclosure, when you are using the 40 features that I talked about and when you’re using a much-reduced number of features. And what you see -- I guess some of the take home messages here are that on different criteria -- you would have to stare at this for a little bit, but different criteria, different methods will perform better than others. And depending on even like specific -- relatively minor differences in your dataset, the amount of data that you disclose varies a lot. Whereas when you have a lot of features, all the methods are doing pretty well. As soon as you reduce that, it’s a lot easier to identify patients and so, you start to get different methods kind of surfacing to the top. But what I'll tell you is that in general we tested these on several different cohorts with different conditions. What we see again and again is that these full posterior probability [unintelligible] methods tend to be at the top for most methods, but not always. And I’m going to skip this. What I'll just say and I think I’ve said most of these things at some point. But one of the kind of thoughts is there is always a tradeoff between the level, like the quality of the synthetic data in terms of measuring the correlations across the different features and how much disclosure you have. It’s not a linear tradeoff, but it is certainly there. And this is something that we care a lot about when we’re developing our methods. There are multiple methods that could be competitors and no single one is best. Our interest is we have lots of results from different metrics and, again, these kind of [unintelligible] models tend to perform better within this class of kind of data-driven methods. Again, which I didn’t even include in the results because it’s performing so poorly, it’s the types of methods that do really well in a lot of other domains like Google and all of these companies that use this very large dataset with images and lots of text, again, do really well. But their potential hasn’t been realized yet in this domain. And one thing that we have noticed and we are investing is because in a lot of the data of our study has been used to the number of levels that you have in each feature is very low. But in our case here, you have histology that has hundreds of different levels. Primary site, if you use subsite also has lots of different levels. And if we remove those methods, actually these types of methods are doing better. So, it’s just a matter of time until we find the correct architecture for it to do better. And the other thing that I didn’t even show the results, but even within the same method -- so this is the most points addition [sic] method. If you notice, there were three columns for the same method, which were the same method but using slightly different optimization algorithms leading to completely different results in terms of disclosure and kind of prediction performance. And so, I guess, the unsatisfied conclusion is that there isn’t a recipe for what’s best, but there are guidelines or hope that these were sort of the guideline to help people think about what is important when you are thinking about this type of data and what should be considered. So, I'll stop here. END OF FILE 