 welcome to lesson seven the last lesson of part one this will be a pretty intense lesson and so don't let that bother you because partly what I want to do is to kind of give you enough things to think about to keep you busy until platt - and so in fact some of the things we cover today I'm not going to tell you about some of the details I'll just point out a few things where I'll say like ok that we're not talking about yet that we're not talking about that and so then come back in part two to get the details on some of these extra extra pieces right so well you know today will be a lot of material pretty quickly you might require a few viewings to fully understand at all or a few experiments and so forth and that's kind of intentional I'm trying to give you stuff to to keep you amused for a couple of months I wanted to start by showing some core work done by a couple of students Reshma and in patters Eero one who have developed an Android and an iOS app and so check out Reshma's post on the forum about that because they have a demonstration of how to create a both Android and iOS apps that are actually on the Play Store and on the Apple App Store so that's pretty cool first first ones I know of that are on the App Store's that are using first AI and let me also say a huge thank you to her for all of the work she does both for the first AI community and the machine learning community or generally and also the women in machine learning community in particular she does a lot of fantastic work including providing lots of fantastic documentation and tutorials and community organizing and so many other things so thank you rush me and congrats on getting this app out there we have lots of less than seven notebooks today as you see and we're going to start with the one first notebook we're gonna look at is lesson seven ResNet amnesty and what I want to do is look at some of the stuff we started talking about last week around convolutions and convolutional neural networks and start building on top of them to create a fairly modern deep learning architecture largely from scratch when I say from scratch I'm not going to re-implement things we already know how to implement that kind of use the pre-existing PI torch bits of those so we're going to use the amnesty data set which so URLs that amnesty has the whole M&S data set often we've done stuff with a subset of it so in there there's a training folder and a testing folder and as I read this in I'm gonna show some more details about pieces of the data blocks API so that you see how to kind of see what's going on normally with the date data blocks API we've kind of said bla bla bla bla and done it all in one cell but let's do them one cell at a time so first thing you say is what kind of item list do you have so in this case it's an item list of images and then where are you getting the list of file names from in this case by looking in a folder recursively and that's where it's coming from you can pass in arguments that end up going to pillow because pillow or PIL is the thing that actually opens that for us and in this case these are black and white rather than RGB so you have to use pillows convert mode equals L for more details refer to the Python imaging library documentation to see what they're convert modes are but this one is going to be a grayscale which is what M list is so inside an item list is an items attribute and the items attribute is kind of the thing that you gave it it's the thing that it's going to use to create your items in this case the thing you gave it really is a list of file names that's what it got from the folder okay when you show images normally it shows them in RGB and so in this case we want to use a binary color map so in first AI you can set a default color map for more information about C map and color maps refer to the matplotlib docume tation and so this world set the default color map for faster I okay so our image item list contains 70,000 items and it's a bunch of images that are 1 by 28 by 28 remember that play torch puts channel first so there one channel 28 for 28 you might think we'll why aren't there just 28 by 28 matrices rather than a 1 by 28 by 28 rank 3 tensor it's just easier that way all the coms 2d stuff and so forth works on rank 3 tenses so you want to you want to include that unit axis at the start and so faster I will do that for you even when it's reading one channel images so the dot items attribute contains the things that's kind of read to build the image which in this case is the file name but if you just index into an item list directly you'll get the actual image object okay and so the actual image object has a show method and so there's there's the image so once you've got an image item list you then split it into training versus validation you nearly always want validation if you don't you can actually use the dot no split method to create a kind of empty validation set you can't skip it entirely you have to say how to split and one of the options is no split right and so remember that's always the order first create your item list then decide how to split in this case we're gonna do it based on folders in this case the the validation folder for reminisced is called testing so in kind of fast AI parlance we use the same kind of parlance that cackled does which is the training set is what you train on the validation set has labels and you do it for testing that your models working the test set doesn't have labels and you use it for doing inference or submitting to a competition or sending it off to somebody who's held out those labels for you know vend or testing or whatever okay so just because a folder in your data set is called testing doesn't mean it's a test set right this one has labels so it's a validation set ok so if you want to do inference on you know lots of things at a time rather than one thing at a time you want to use the test equals in in fast AI to say this is stuff which has no labels and I'm just using for inference okay so my split data is a training set and a validation set as you can see so inside the training set there's a folder for each image for each class so now we can take that split data and say label from folder so first to create the Animus then you spit it then you label it and so you can see now we have an X and a Y and the Y are category objects category object is just a class basically so if you index into a label list such as la reine as a label list you will get back an independent variable independent variable x and y so in this case the X will be an image object which I can show and the Y will be a category object which I can print that's the number it's the number 8 category and there's the 8 next thing we can do is to add transforms in this case we're not going to use the normal get transforms function because we're doing digit recognition and digit recognition like he wouldn't want to flip it left right that would change the meaning of it you wouldn't want to rotate it too much that would change the meaning of it also because these images are so small kind of doing zooms and stuff is going to make them so fuzzy as to be unreadable so normally for small images of digits like this you just add a bit of random padding so I'll use the random padding function which actually returns two transforms so a bit that does the padding and the dip that does the random crop so you have to use star to say put both these transforms in this list so now we can call transform this empty array here is referring to the validation set transforms so no transforms for the validation set now we've got a transformed labelled list we can pick a batch size and choose data bunch we can choose normalize in this case we're not using a pre-trained model so there's no reason to use imagenet stats here and so if you call normalize like this without passing in stats it will all grab a batch of data at random and use that to decide what normalization stats to use that's a good idea if you're not using a pre-trained model okay so we've got a dead data bunch and so in that data bunch is a data set which we've seen already but what is interesting is that the training data set now has data augmentation because you've got transforms so plot multi is a faster Oh function that we're all plot the result of calling some function for each of this row row by column grid so in this case my function is just grab and grab the first image from the training set and because each time you grab something from the training set it's going to load it from disk and it's going to transform it on the fly right so people sometimes asked like how many transformed versions of the image do you create and the answer is kind of infinite each time we grab one thing from the data set we do a random transform on the fly okay so potentially you everyone will look a little bit different so you can see here if we plot the result of that lots of times we get eighths in slightly different positions because we did random padding you can always grab a batch of data then from the data bunch because remember a data bunch has data loaders and data loaders things that you grab a batch at a time and so you can then grab our X patch and a Y batch look at their shape batch size by channel by row by column all fast a a data bunches have a show batch which will show you what's in it in some sensible way okay so that's a quick walk through with the data block API stuff to grab our data so let's start out creating a simple CNN so confident so the input is 28 by 28 so let's define I like to define when I'm creating architectures a function which kind of does the things that I do again and again and again I don't want to call it with the same arguments because I'll forget I'll make a mistake so in this case all of my convolution is going to be kernel size 3 stride 2 padding 1 so let's just create a simple function to do a cons with those parameters so you try to have a convolution it's skipping over one pixel so it's doing jumping jumping two steps each time so that means that each time we have a convolution it's going to have the grid size so I've put a comment here showing what the new grid size is after each one so after the first convolution we have one channel coming in because it's remember it's a grayscale image with one channel and then how many channels coming out whatever you like right so remember you always get to pick how many filters you create regardless of whether it's a fully connected layer in which case it's just the the width of the matrix you're multiplying by or in this case with a 2d con it's just how many how many filters do you want so I picked 8 and so after this it's drag to so the 28 by 28 image is now a 14 by 14 feature map with 8 channels so specifically therefore it's an 8 by 14 by 14 tensor of activations then we'll do batch norm then we'll do value so the number of input filters to the next con has to equal the number of output filters from the previous conf and we can just keep increasing the number of channels because we're doing strive to it's going to keep decreasing the grid size notice here it goes from 7 to 4 because if you're doing a stride to convey over 7 it's going to be kind of math dot ceiling of 7 divided by 2 that's normally lu confer now down to two by two professional recon we're now down to one by one right so after this we have a batch side of the picture map of let's say ten by one by one does that make sense we've got a grid size of one now so it's not a vector of length 10 its a rank 3 tensor of 10 by 1 by 1 so our loss functions expect generally a vector not a rank 3 tensor so you can check flatten at the end and flatten just means remove any unit axes so that will make it now just a vector of length 10 which is what we always expect so that's how we can create a CNN so then we can return that into a learner by passing in the data and the model and the loss function and if optionally some metrics so we're going to use cross-entropy as usual so we can then call own dot summary and confirm after that first cond we're down to 14 by 14 and after the second column 7 by 7 and 4 by 4 2 by 2 1 by 1 the flatten comes out calling it a lambda but that as you can see it gets rid of the 1 by 1 and it's now just a length 10 vector for each item in the bench so 128 by 10 matrix a little mini so just to confirm that this is working okay we can grab that mini batch of X that we created earlier there's our mini batch of X pop it onto the GPU and call the model directly remember any PI torch module we can pretend as a function and that gives us back as we hoped a 128 by 10 resolved okay so that's how you can directly get some predictions out now find fit one cycle and bang we already have a 98.6% accurate confident and this is trained from scratch of course it's not pre-trained we literally our own architecture about the simplest possible architecture you can imagine 18 seconds to train so that's how easy it is to create a pretty accurate digit detector so let's refactor that a little rather than saying clowns metronome really all the time first day I already has something called conf underscore lair which lets you create cons batch norma lu accommodations and it has various other options to do rather tweaks to it but the basic version is just exactly what i just showed you so we can refactor that like so that's exactly the same euronet and so you know let's just try it a little bit longer and it's actually 99.1 percent accurate if we train it for all over minute so that's cool so how can we improve this well what we really want to do is create a deeper network and it's a very easy way to create a deeper network would be after every stride to cons add a stride one cons because the straight one comes doesn't change the feature map size at all so you can add as many as you like right but there's a problem there's a problem and the problem was pointed out in this paper very very very influential paper called deep learning deep residual learning for image recognition by coming her and colleagues at then at Microsoft Research and they did something interesting they said let's look at the training error so forget generalization even let's just look at the training error of a network trained on so far 10 and let's try one network of 20 layers just basic 3x3 cons it's just basically the same network I just showed you but with that batch norm let's try train to 20 layer 1 and a 56 layer 1 on the training set so the 56 layer 1 has a lot more parameters it's got a lot more of these tried 1 comes in the middle so the one with more parameters should seriously over fit right so you would expect the 56 layer one to zip down to zero ish training error pretty quickly and that is not what happens it is worse than the shallower network so when you see something weird happen really good researchers don't go oh no it's not working they go that's interesting so kiting who said that's interesting what's going on and he said I don't know but what I do know is this I could take this 56 layer Network and make a new version of it which is identical but has to be at least as good as the 20 layer Network and here's how every two convolutions I'm going to add together the input to those two convolutions add it together with the result of those two convolutions so in other words he's saying instead of saying output equals con two of con 1 of X instead he's saying output equals x plus conf 2 of conf 1 of X so that fifty six layers worth of convolutions in in that his theory was has to be at least as good as the 20 layer version because it could always just set com2 and cons 1/2 a bunch of 0 weights for everything except for the first 20 layers because because their X the input could just go straight through so this thing here is as you see called an identity connection it's the identity function nothing happens at all it's also known as a skip connection so that was a theory right that's what the paper describes as the intuition behind this is what would happen if we created something which has to train at least as well as a 20 layer neural network because it kind of contains that 28 layer neural network there's literally a path you can just skip over all the convolutions and so what happens and what happened was he won imagenet that year he easily won imagenet that year and in fact you know even today you know we had that record-breaking result on image net speed training ourselves you know in the last year we used this to you know res net has been revolutionary and anytime here's a trick if you're interested in doing some research in the whole research anytime you find some model for anything with her slight medical image segmentation or you know some kind of gain or whatever you know and it was written a couple of years ago they might have forgotten to put rest nets in res block res blocks this is what we normally call a res block they might have forgotten what res blocks in so replace their convolutional paths with a bunch of res blocks and you'll almost always get better results faster it's a good trick so at Europe's which Rachel and I and David orders came back from and Sylvia we saw a new presentation where they actually figured out how to visualize the loss surface of a neural net which is really cool this is a fantastic paper and anybody who's watching this lesson 7 is that a point where they will understand most of the most important concepts in this paper you could read this now you won't necessarily get all of it but I'm sure you'll find it again enough to find an interesting and so the the big picture was this one here's what happens if you if you draw a picture we're kind of X&Y here are two projections of the of the white space and Zed is the loss and so as you move through the white space wrists at a 56 layer neural network without skip connections is very very bumpy and that's why this got nowhere because it just got stuck in all these hills and bellies the exact same network with identity connections with skip connections has this lost landscape right so that's it's kind of interesting how how how her recognized back in 2015 you know this shouldn't happen here's a way that must fix it and it took three years before people were able to say oh this is kind of why it fixed it it kind of reminds me of the batch normal discussion we had a couple of weeks ago people realizing a little bit after the fact sometimes what's what's going on and why it helps so in our code we can create a res block in just the way I described we create an end up module we create two con flares remember a con flare is kind of 2d that's normal you so it comes to derail you betcha norm so create two of those and then in forward we go conf one of ex-cons two of that and then add X there's a res bak function already in fast AI so you can just call res block instead and you just pass in something saying how many filters do you want so yeah so there's the rest block that I defined in a notebook and so with that look with that res block we can now take every one of those I've just copied the previous CNN and after every con to except the last one I added a res block so this is grant now got three times as many layers so it should be able to do more compute right but it shouldn't be any harder to optimize so what happens well let's just refactor it one more time since they go come to res block so many times let's just pop that into a little mini sequential model here and so I can refat to that like so keep refactoring your architectures if you're trying novel architectures because you'll make less mistakes very few people do this most research code you look at is is clunky as all hell and people often make mistakes in that way so don't don't do that be you know you're all Coda's so use your coding skills to make life easier okay so there's my ResNet ish architecture and ela find as usual fit for a while and I get ninety nine point five four so that's interesting because we've trained this literally from scratch with an architecture we built from scratch I didn't look at this architecture anywhere it's just the first thing that came to mind but in terms of where that puts us point four five percent error is around about the state of the art for this data set as of three or four years ago now you noted am nest is considered a kind of trivially easy data set so I'm not saying like well we've broken some records here people have got beyond 0.45% error but what I'm saying is that you know we can't you know this kind of resonate is a genuinely extremely useful network still today and this is this is really all we use in our first imagenet training still now one of the reasons as well is that it's so popular so the the vendors of the library spend a lot of time optimizing it so things tend to work fast there are some more modern style architectures using things like separable or group convolutions tend not to actually train very quickly in factors if you look at the definition of res block in the first AI code you'll see it looks a little bit different to this and that's because I've created something called a merge layer and a merge layer is something which in the forward just keeps dense for a moment the forward says X plus X dot orig so you can see that some thing ResNet ish going on here what is X dot orig well if you create a special kind of sequential model called a sequential e^x so this is like the fastes sequential extended it's just like a normal sequential model but we store the input in X dot orig right and so this this year is the quencher IX conv layer conf layer merge layer will do exactly the same as this okay so you can create your own variations of ResNet blocks very easily with just sequential e^x and merge layer so there's something else here which is when you create your merge layer you can optionally set dense equals true what happens if you do well if you do it doesn't go X plus X dot orig that goes cat X comma X dot orig in other words rather than putting a plus in this connection it does a concatenate so that's pretty interesting because what happens is that you have your your input coming into your res block and once you use concatenate instead of plus it's not called a res block anymore it's called a dense block and it's not quite a ResNet any what more is called a dense net so the dense net was invented about a year after the res net and if you read the dense net paper it can sound incredibly complex and different but actually it's literally identical but plus here is replaced with with cat so you have your input coming into your dense block right and you've got a kind of a few convolutions in here and then you've got some output coming out and then you've got your identity connection and remember it doesn't plus it con cats so this is the channel access it gets a little bit bigger all right and then so we do another dense block right and at the end of that we have you know all of this coming in Rob sorry we have okay so at the end of that we have you know the result of the convolution as per usual but this time the identity block is that big right so you can see that what happens is that with dense blocks it's getting bigger and bigger and bigger and kind of interestingly the exact input is still here right so it's actually no matter how deep you get the original input pixels are still there and the original layer one features are still there in the original layer two features are still there so as you can imagine dense Nets are very memory intensive there are ways to manage this the best from time to time you can have a regular convolution that squishes your channels back down but they are memory intensive but they have very few parameters so for dealing with small datasets you should definitely experiment with dense blocks and dense Nets they tend to work really well on small datasets also because it's possible to kind of keep those original input pixels all the way down the path they work really well for segmentation right because for segmentation you know you kind of want to be able to reconstruct the original resolution of your picture so having all of those original pixels still there is super helpful so so that's that's res debts and the main one of the main reasons other than fact that res Nets are awesome to tell you about them is that these script connections are useful in other places as well and they're particularly useful in other places and other ways of designing architectures for segmentation so in building this lesson I always kind of I keep trying to take old papers and saying like imagining like what would that person have done if they had access to all the modern techniques we have now and I try to kind of rebuild them in a more modern style so I've been really rebuilding this next architecture going to look at called a unit in a more modern style recently and got to the point now I keep showing you this semantic segmentation paper with the state of for camford which was 91.5 this week I got it up to 94.1 using the architecture I'm about to show you so we just we keep pushing this further and further and further and it's really was all about you know adding all of the modern tricks many of which I'll show you today some of which we will see in Platt too so what we're going to do to get there is we're going to use this unit so refused a unit before I've improved it a bit since then so we've used a unit before we used it when we did the camp its egg mentation but we didn't understand what it was doing so we're now in a position where we can understand what it was doing and so the first thing we need to do is kind of understand the basic idea of how you can do segmentation so if we go back to our canvas notebook in our camping notebook you'll remember that basically what we were doing is we were taking these photos and adding a class to every single pixel and somebody go data dot show batch for something which is a segmentation item list it will automatically show you these color coded pixels so here's the thing like in order to color code this as a pedestrian you know but this as a bicyclist it needs to know what it is then these two actually know that's what a pedestrian looks like and it needs to know that's exactly where the pedestrian is and this is the arm of the pedestrian and not part of their shopping basket it needs to really understand a lot about this picture to do this task and it really does do this task like when you looked at the results of our top model it's it's you know I can't see a single pixel by looking at it by eye I know there's a few wrong but I can't see the ones that are wrong it's that accurate so how does it do that so the way that we're doing to get these really really good results is not surprisingly using pre-training so we start with a resinate 34 and you can see that here unit learner data comma model start ResNet 34 and if you don't say pre-trained equals false by default you get pre-trained equals true because why not so we start with a resonate 34 which starts with a big image so in this case this is from the unit paper now they're images they started with one channel by 572 by 572 this is for medical imaging segmentation so after your strive to cons you they're doubling the number of channels to 128 and they're having the size so they're now down to 280 by 280 in this original unit paper they didn't add any padding so they lost the pixel on each side each time they did a con that's why you're losing these two but so basically half the size and then half the size and then half the size and then half the size until they're down to 28 by 28 with 1024 channels right so that's that's what the unit's down the sampling path this is called the down sampling path look like ours is just a ResNet 34 so you can see it here learn dot summary right this is literally a ResNet 34 so you can see that the size keeps having channels keep going up and socials okay so eventually you've got down to a point where if you use unit architecture it's 28 by 28 with 1024 channels with the resonant architecture with a 224 pixel input it would be 512 Kennels by 7 by 7 so it's a pretty small grid size on this feature map somehow we've got to end up with something which is the same size as our original picture so how do we do that how do you do computation which increases the grid size well we don't we don't have a way to do that in our current bag of tricks we can use a stride one conf to do computation and keeps grid size or a stride to con to do computation and half the grid size so how do we double the grid size we do a stride half conf also known as a deconvolution also known as a transposed convolution there is a fantastic paper called a guide to convolution arithmetic fatigue learning that shows a great picture of exactly what does a 3x3 kernel stride half conf look like and it's literally this if you have a 2x2 input so the blue squares are the 2x2 input you add not only two pixels of padding all around the outside but you also add a pixel of padding between every pixel and so now if we put this 3x3 kernel here and then here and then here usually other 3x3 kernels just moving across it in the usual way you will end up going from a 2x2 output to a 5x5 output so if you only added one pixel of padding around the outside you would add up end up with a 3x3 output right so that's sorry 4x4 so this is how you can increase the resolution this was the way people did it until maybe a year or two ago that's another trick for improving things you find online because this is actually a dumb way to do it and it's kind of obvious it's a dumb way to do it for a couple of reasons one is that like hello look at this nearly all of those pixels are white they're nearly all zeros so like what a waste what a waste of time what a waste of computation there's just nothing going on I'm also this one when you get down to like that 3x3 area two out of the nine pixels are non-white but this one what out of the nine of non-white so they're kind of like there's different amounts of information going into different parts of your convolution so like this just doesn't make any sense to kind of throw away information like this and to kind of do all this unnecessary computation and have different parts of the convolution having access to different amounts of information so what people generally do nowadays is something really simple which is if you have a let's say a two by two input with these are your pixel values a a B C and D and you want to create a four by four why not just do this a a a a b b b b CC CC D D D D so I've now upscaled from two by two to four by four I haven't done any interesting computation but now on top of that I could just do a straight one convolution and now I have done some computation right so an up sample this is called nearest neighbor interpolation nearest neighbor interpolation so you can just do and that's super fast which is nice take into a nearest neighbor interpolation and then astride one con and now you've got some computation which is actually kind of using you know there's no zeros here this is kind of nice because it gets a mixture of A's and B's which is kind of what you would want and so forth another approach is instead of using nearest neighbor interpolation you can use bilinear interpolation which basically means instead of copying a to all those different cells you take a kind of a weighted average if the cells around it so for example if you were you know looking at what should go here you would kind of go like oh it's about three as to cease 1d and to bees and you kind of take the average not exactly but roughly just a weighted average by linear interpolation you'll find in any you know all over the place it's is pretty standard technique anytime you look at a picture on your computer screen and change its size it's doing bilinear interpolation so you can do that and then a strike one conf so that was what people were using less what people still tend to use that's as much as I going to teach you this part in part two will actually learn what the first day I library is actually doing behind the scenes which is something called a pixel shuffle also known as sub pixel convolutions it's got not dramatically more complex but complex enough that I won't cover it today there's a same basic idea all of these things is something which is basically letting us do a convolution that ends up with something that's twice the size and so that gives us our up sampling path right so that lets us go from 28 by 28 to 54 by 54 and keep on doubling the size so that's good and that was that was it until unit came along that's what people did and it didn't work real well which is not surprising because like in this 28 by 28 feature map how the hell is it going to have enough information to reconstruct a 572 by 572 output space you know that's a really tough ask so you tended to end up with these things that lack fine detail so what Olaf Rona Berger and a towel did was they said hey let's add a skip connection an identity connection and amazingly enough this was before res Nets existed so this was like a really big leap really impressive and so but rather than adding a skip connection that skipped every two convolutions they added sections where these gray lines are in other words they added a skip connection from the same part of the down sampling path to the same-sized bit in the up sampling path and they didn't add that's why you can see the white and the blue next to each other they didn't add they concatenated so basically these are like dense blocks right but the Skip connections are skipping over larger and larger amounts of the architecture so that over here you've literally got nearly the input pixels themselves coming into the computation of these last couple of layers and so that's going to make it super handy for resolving the fine details in these segmentation tasks because you've literally got all of the phone details on the downside you don't have very many layers of computation going on here just for right so you better hope that by that stage you've done all the computation necessary to figure out this is the bicyclist or is this a pedestrian but you can then add on top of that something saying like is this you know is this exact pixel where their nose finishes or is at the start of the tree so that works out really well and that's a unit so this is the unit code from fast AI and the key thing that comes in is the encoder the encoder refers to that part in other words in our case a ResNet 34 in most cases they have this specific older style architecture but like I said replace any older style architecture bits where the ResNet bits and life improves particularly if they're pre trained so that certainly happened for us so we start with our encoder so our layers of our unit is an encoder then batch norm then rally and then middle con which is just con flare comma con flare remember con flare is a con rail you betcha norm in first ago and so mid or con is these two extra steps here at the bottom okay just doing a little bit of computation you know it's kind of nice to add more layers of computation where you can so encode a batch or Lu and then to convolutions and then we enumerate through these indexes what are these indexes I haven't included the code but these are basically we figure out what is the layer number where each of these stripe two comes occurs and we just store it in an array of indexes so then we can loop through that and we can basically say for each one of those points create a unit block telling us how many upsampling channels that are and how many cross connection these these things here are called cross connections at least that's what I call them so that's really the main works going on in the in the unit block as I said there's quite a few tweaks we do as well as the fact we use a much better encoder we also use some tweaks in all of our apps sampling using this pixel shuffle we use another tweak called ICN are and then another tweak which I just did in the last week is to not just take the result of the convolutions and pass it across but we actually grab the input pixels and make them another cross connection that's what this last cross is here you can see we're literally appending a res block with the original inputs so you can see I merge layer so really all the works going on a new net block and unit block is it has to store the the activations at each of these down sampling points and the way to do that as we learn in the last lesson is with hooks so we we put hooks into the resinate 34 to store the activations each time there's a strata to con and so that's you can see here we we grab the hook okay and we grab the result of the stored value in that hook and we literally just go torch doc hat so we concatenate the upsampled convolution with the result of the hook which we Chuck through batch norm and then we do two convolutions to it and actually you know something you could play with at home is pretty obvious here anytime you see two convolutions like this there's an obvious question is what if we used a resident block instead so you could try replacing those two comes with a resinate block you might find you get even better results they're the kind of things I look for when I look at an architecture is like Oh true columns in a row probably should be a resonant look okay so that's that's unit and you know it's amazing to think you know it preceded ResNet to preceded dense net it's been it wasn't even published in a major machine learning venue it was actually published in Mekhi which is a specialized medical image computing conference for years actually you know it was largely unknown outside of the medical imaging community and actually what happened was cackled competitions for segmentation kept on being easily won by people using units and that was the first time I saw it getting noticed outside the medical imaging community and then gradually a few people in the academic machine learning community started noticing and now everybody loves unit which I'm glad because it's just it's just awesome so yeah so identity connections regardless of whether they're a plus style or a concat style we're incredibly useful they can basically get us close to the state of the art on lots of important tasks so I want to use them on another task now and so the next task I want to look at is image restoration so image restoration refers to starting with an image at this time we're not going to create a segmentation mask but we're going to try and create a a better image and there's lots of kind of versions of better there could be different image so the kind of things we can do with this kind of generation would be take a low res image make it high-res take a black-and-white image make a color take an image where something's being cut out of it and trying to place the cutout thing take a photo and try and turn it into what looks like a line drawing take a photo and try and make it look like a Monet painting these are all examples of kind of image to image generation tasks which you'll know how to do after this padlet class so in our case we're going to try to do image restoration which is going to start with low resolution poor quality JPEGs with writing written over the top of them and get them to replace them with high resolution good quality pictures in which the the text has been removed two questions okay let's go why do you compat before calling com2 com1 not after because if you did kind of one Conte you know if you did your combs before you concat then there's no way for the channels of the two parts to interact with each other you don't get any you know so remember in a 2d conf it's really 3d right it's moving across two dimensions but in each case it's doing a dot product of all three dimensions of a rank three tensor row by column by channel so generally speaking we want as much interaction as possible we want to say you know this part of the down sampling path and this part of the up sampling path if you look at the combination of them you find these interesting things so generally you know you you want to have as many interactions going on as possible in each computation that you do how does concatenating every layer together in a dense network when the size of the image feature Maps is changing through the layers that's a great question so if you have a stride to cons you can't keep dense knitting right so that's what actually happens in a dense net is you kind of go like dense block growing dense block growing dense blocker rings are getting more and more channels and then you do a stride to cons without a dense block and so now it's kind of gone and then you just do a few more dense blocks and then it's gone so in practice a dense block doesn't actually keep all the information all the way through but just every up into every one of these stride two calms and there's kind of various ways of doing these bottlenecking layers where you're basically saying hey let's let's reset it also helps us keep memory under control cuz at that point we can decide how many channels we actually want good questions thank you but so in order to create something which can turn crappy images into nice images we need a data set containing nice versions of images and crappy versions of the same images so the easiest way to do that is to start with some nice images and krappa fie them and so the way to crap by them is to create a function called krappa Phi which contains your krappa fication logic so mike ramification logic you can pick your own is that I open up my nice image I resize it to be really small 96 by 96 pixels with bilinear interpolation I then pick a random number between 10 and 70 I draw that number into my image at some random location and then I save that image with a JPEG quality of that random number and a JPEG quality of 10 is like absolute rubbish a JPEG quality of 70 is not bad at all okay so I end up with high quality images low quality images that look something like these and so you can see this one you know there's the image and this is after transformation that's why it's been flipped and you won't always see the image because we're zooming into them so a lot of the time the image is cropped out so yeah it's trying to figure out how to take this incredibly JPEG artifactory thing with with text written over the top and turn it into into this so I'm using the Oxford pets data set again the same one we used in lesson 1 so there's nothing more high qualities and pictures of dogs and cats I think we can all agree with that the krappa fication process can take a while but fast AI has a function called parallel but if you pass parallel a function name and a list of things to run that function on it will run that function on them all in parallel so this actually can run pretty quickly the way you write this function is where you get to do all the interesting stuff in this assignment try and think of an interesting krappa fication which does something that you want to do right so if you want to you know colorize black-and-white images you would replace it with black and white if you want something which can you know take like large cut out blocks of image and replace them with kind of hallucinatin image you know add a big black box to these if you want something which can kind of take old families photos scans that have been like folded up and have crinkles in try and find a way of like adding dust prints and crinkles and so forth right and anything that you don't include in crap fi your model won't learn to fix because every time it sees that in your photos the input and output will be the same so it won't consider that to be something worthy of fixing okay so so we now want to create a model which can take an input photo that looks like that and outputs something that looks like that so obviously what we want to do is use a unit because we already know that units can do exactly that kind of thing and we just need to pass the unit that data okay so our data is just literally the file names of a of those from each of those two folders do some transforms data bunch normalize or use imagenet stats because we're going to use a pre-trained model why are we using a pre-trained model well because like if you're going to get rid of this 46 you need to know what probably was there and to know what probably was there you need to know what this is a picture of bags otherwise how can you possibly know what it ought to look like so you know let's use a pre train model that knows about these kinds of things so we create our unit with that data the architecture is ResNet 34 these three things are important and interesting and useful but I'm going to leave them to part two okay for now you should always include them when you use a unit for this kind of problem and so now we're going to and this whole thing I'm calling a generator okay it's going to generate this is piano ready of modeling they're kind of there's not a really formal definition but it's basically something where the thing we're outputting is like a real object in this case an image it's not just a number so we're going to create a generator learner which is this unit learner and then we can fit we're using MSC loss right so in other words what's the mean squared error between the actual pixel value that it should be in the pixel value that we predicted MFC lost normally expects two vectors in our case we have two images so we have a version called MSC loss flat which simply flattens out those images into a big long vector there's there's never really reason not to use this even if you do have a vector it works fine if you don't have a weight vector it'll also work fine so we're already you know down to 0.05 mean squared error on the pixel values which is not bad after one minute 35 like all things in fast day I pretty much because we're doing transfer learning by default when you create this it'll freeze the the pre-trained part and the pre-trained part of a unit is this part the downsampling part that's where the resin it is so let's unfreeze that and train a little more and look at that so with you know three minutes of four minutes of training we've got something which is basically doing a perfect job of removing numbers it's certainly not doing a good job of up sampling but it's definitely doing a nice you know sometimes it removes a number it maybe leaves a little bit of JPEG artifacts but you're certainly doing something pretty useful and so if all we wanted to do was kind of watermark remove all would be finished we're not finished because we actually want this thing to look more like this thing so how we going to do that the problem the reason that we're not making as much progress with that as we'd like is that our loss function doesn't really describe what we want because actually the the mean squared error between the pixels of this and this is actually very small right if you actually think about it most of the pixels are very nearly the right color but we're missing the texture of the pillow and we're missing the eyeballs entirely pretty much right and we're missing the texture of the fur right so we want we want some loss function that does a better job than pixel means great error loss of saying like is this a good quality picture of this thing so there's a fairly general way of answering that question and it's something called a generative adversarial Network again and again tries to solve this problem by using a loss function which actually calls another model and let me describe it to you so we've got our crappy image right and we've already created a generator it's not a great one but it's not terrible right and that's creating predictions like like this we have a high-res image like that and we can compare the high-res image to the prediction with with pixel MSE okay we could also train another model which we would variably call variously call either the discriminator or The Critic they both mean the same thing I'll call it a critic we could try and build a binary classification model that takes all the pairs of the generated image and the real high-res image and tries to classify learn to classify which is which you know so look at some picture and say like hey what do you think is that a high-res cat or is that a generated cat how about this one is that a high-res cat or a generated cat so just a regular standard binary cross-entropy e classified so we know how to do that already so if we had one of those we could now train we could fine tune the generator and rather than using pixel MSE is the loss the loss could be how good are we at fooling the critic so can we create generated images that the critic thinks are real so that would be a very good plan right because if it can do that if the loss function is am I fooling the critic that then it's going to learn to create images which the critic can't tell whether they're real or fake so we could do that for a while train a few batches but the critic isn't that great the reason the critic is that isn't that great is because it wasn't that hard like these images are really shitty so it's really easy to tell the difference right so after we train the generator a little bit more using that critic as the loss function the generators going to get really good at falling the critic so now we're going to stop training the generator and we'll drain the critic some more these newly generated images so now that that generators better it's now a tougher task for the critic to the side which is real and which is fake so again so we're trained that a little bit more and then once we've done that and the critics now pretty good at recognizing the difference between the better generated images and the originals real caught we'll go back and we'll fine-tune the generator some more using the better discriminator the better critic as the loss function and so we'll just go ping pong ping pong backwards and forwards that's again well that's our version of again I don't know if anybody's written this before we've we've created a new version of again which is kind of a lot like the original Gans but we have this this neat trick where we pre train the generator and we pre train the critic I mean games have been kind of in the news a lot they're pretty fashionable tall and if you've seen them you may have heard that they're a real pain to Train but it turns out we realized that really most of the pain of training them was at the start if you don't have a pre train generator and you don't have a pre train critic then it's basically the blind leading the blind right you're basically like the critics well the generators trying to generate something which falls a critic but the critic doesn't know anything at all so it's basically got nothing to do and then the critics kind of try to decide whether the generated images are real or not and like that's really obvious so that this does it and so they kind of like don't go anywhere for ages and then once they finally start picking up steam they go along pretty quickly so if you can find a way to generate things without using again like means quit there are pixel loss and discriminate things without using a can like predict on that first generator you can make a lot of progress so let's create the critic so to create just a totally standard fast AI binary classification model we need two folders one folders containing high-res images one folder containing generated images we already have the folder with high-res images so we just have to save our generated images so here's a teeny tiny bit of code that does that we're going to create a directory called image gen pop it into a variable called path gen we've got a little function called save Preds it takes a data loader and we're going to grab all of the file names because remember that in an item list the dot items contains the file names if it's an image item list so here's the file file names in that data loaders data set and so now let's go through each batch of the data loader and let's grab a batch of predictions for that batch and then reconstruct akil's true means it's actually going to create fast AI image objects for each of those each thing in the in the batch and so then we'll go through each of those predictions and save them and the name will save it with is the name of the original file but we're going to pop it into our new directory so that's it that's how you save predictions and so you can see I'm kind of increasingly not just using stuff that's already in the FASTA a library to try to show you how to write the stuff yourself right and generally it doesn't require heaps of code to do that and so if you come back to part two this is what you know platon months apart - were kind of like here's how you use things inside the library and of course here's how we wrote the library so increasingly writing our own code okay so save those predictions and lend this just do a pil dot image to open on the first one and yep there it is okay so there's an example of a generated image so now I can train a critic in the usual way it's really annoying to have to restart to put a notebook to refresh your reclaim GPU memory so one easy way to handle this is if you just set something that you knew was using a lot of GPU to none like this learner and then just go GC collect that tells Python to do a memory garbage collection and after that you'll generally be fine you'll be able to use all of your GPU memory again if you're using nvidia SMI to actually look at your GPU memory you won't see it clear because plate watch still has a kind of allocated cache but it makes it available so you should find this is how you can avoid restarting your notebook okay so we're going to create our critic it's just an image item list from folder in the totally usual way and the classes will be the image gen and images will do a random split because we want to know how well we're doing with a critic to have a validation set we just label it from folder in the usual way add some transforms data bunch normalize so it's a totally standard object classifier okay so we've got a totally standard classifier so here's what some of it looks like so here's one from the real images real images generated images generated images okay so that's it's going to try and figure out which class is which okay so we're going to use Brian air across HB as usual however we're not going to use a resident here and the reason we'll get into in more detail in part two but basically when you're doing again you need to be particularly careful that the the generator and the critic can't kind of both push in the same direction and like increase the weights out of control so we have to use something called spectral normalization to make Gans work nowadays we'll learn about that in part two so anyway if you say gain critic that will give you fast AO will give you a binary classifier suitable against I strongly suspect we probably can use a resonant here we just have to create a pre trained ResNet with spectral norm hope to do that pretty soon we'll see how we go but as of now this is kind of the best approach there's this thing called game critic again critic uses a slightly different way of averaging the the different parts of the image when it does the loss so anytime you're doing again at the moment you have to wrap your loss function with adaptive loss again we'll look at the details in part two for now just know this is what you have to do and it'll work so other than that slightly odd loss function and that slightly odd architecture everything else is the same we can call that to create our critic because we have this slightly different architecture and slightly different loss function we did a slightly different metric this is the equivalent gain version of accuracy the critics and then we can train it and you can see it's 98% accurate at recognizing that kind of crappy thing from that kind of nice thing but of course we don't see the numbers here anymore right because these are the generated images the generator already knows how to get rid of those numbers that are written on top okay so let's finish up this game now that we have pre trained the generator and pre-trained the critic we now need to get it to kind of ping pong between training a little bit of each and the amount of time you spend on each of those things and the learning rates you use is still a little bit on the fuzzy side so we've created a again learner for you which you just pass in your generator and your critic which we've just just simply loaded here from the ones we just trained and it will go ahead and when you go learned outfit it will do that for you it'll figure out how much time to train the generator and then when to switch to training the discriminator the critic and I'll go back on forward these weights here is that what we actually do is we don't only use the critic as the loss function if we only use the critic as the loss function the Gann could get very good at creating pictures that look like real pictures but they actually have nothing to do with the original picture the original photo at all so we actually add together the pixel loss and the crew los and so those two losses are kind of on different scales so we multiply the pixel loss by something between about 50 and about 300 again something in that range generally works pretty well something else with cans cans hate momentum when you're trading them it kind of doesn't make sense to train them with who mentioned because you keep switching between generator and critic so it's kind of tough maybe there are ways to use momentum but I'm not sure anybody's figured it out so this number here when you create an atom optimizer is where the momentum goes so you should set that to zero so do if you're doing gains use these hyper parameters it should work okay so so that's what can learner does and so then you can go fit and it trains for a while and one of the tough things about ganz is that these loss numbers they're meaningless you can't expect them to go down right because as the generator gets better it gets harder for the discriminator the critic and then as the credit gets better it's harder for the generator so the numbers should stay about the same okay so that's one of the tough things about trading ganz is it's kind of hard to know how are they doing so the only way to know how are they doing is to actually take a look at the results from time to time I haven't and so if you put show image equals true here it'll actually print out a sample after every epoch I haven't put that in the notebook because it makes it too big for further repo but you can try that so I've just put the results at the bottom and here it is so pretty beautiful I would say we already knew how to get rid of the numbers but we're now don't really have that kind of artifact of where it used to be and it's it's definitely sharpening up this little kitty cat quite nicely it's not great always like there's some weird kind of noise going on here certainly a lot better than the horrible original like this is a tough job to turn that into that but there are some really obvious problems like here these things ought to be eyeballs and they're not so why aren't they well how critic doesn't know anything about eyeballs and even if it did it wouldn't know that eyeballs are particularly important you know we care about eyes like when we see a cat without eyes it's a lot less cute I mean I'm more of a dog person but you know it's it just doesn't know that this is a feature that that matters particularly because the critic remember is not a pre trained network so I kind of suspect that if we replace the critic with a pre trained network that's been pre trained on imagenet but is also compatible with Ganz it might do a better job here but it's definitely a shortcoming of this approach so I'm going to have a break Oh question first and then we'll have a break and then after the break I will show you how to find the cat's eyeballs again for what kind of problems do you not want to use units well unit so for when the the size of your output you know is is similar to the size of your input and kind of aligned with it there's no point kind of having cross connections if that level of spatial resolution in the output isn't necessary or useful so yeah any kind of generative modeling and you know segmentation is kind of generative modeling right it's it's generating a picture which is a mask of the original objects yeah so probably anything where you want that kind of that kind of resolution of the output to be of the same kind of fidelity as a resolution of the input obviously something like a classifier makes no sense right you you in a classifier you just want the down sampling pass because at the end you just want a single number which is like is it a dog or a cat or what kind of pet is it or whatever great okay so let's get back together at five past eight just before we leave Dan's I just mentioned there's another notebook you might be interested in looking at which is less than 7w again when games started a few years ago people generally use them to kind of create images out of thin air which I personally don't think is a particularly useful or interesting thing to do but it's kind of a good I don't know it's a good research exercise I guess so we implemented this this w again paper which is kind of really the first one to do a somewhat adequate job somewhat easily and so you can see how to do that with the first AI library it's kind of interesting because the data set we use is this else on bedrooms data set which we've provided in our URLs which just as you can see has bedrooms lots and lots and lots of bedrooms and the approach you'll see in the pros here that sylvia wrote the the the approach that we use in this case is to just say can we create a bedroom and so what we actually do is that the the input to the generator isn't an image that we clean up we actually feed to the generator random noise and so then the generators task is can you turn random noise into something which the critic can't tell the difference between that output and a real bedroom and so we're not doing any pre training here or any of the stuff that makes this kind of fast and easy so this is a very traditional approach but you can still see you still just go you know again learner and there's actually a double you can version which is you know this kind of older style approach but you just passed in the data or in the generator and the critic in the usual way and you call fit and you'll see in this case we have a show image on you know after epoch one it's not creating great bedrooms or two or three and you can really see that in the early days of these kinds of cans it doesn't do a great job of anything but eventually after you know a couple of hours of training producing somewhat like bedroom ish things you know so anyway it's a notebook you can never play with and it's a bit of fun so I was very excited when we got fast AI to the point in the last week or so that we had Gans working in a way where kind of API wise they're far more concise and more flexible than any other library that exists but also kind of disappointed with them they take a long time to train and the outputs are still like so-so and so the next step was like well can we get rid of cans entirely so the first step with with that I mean obviously the thing we really want to do is come up with a better loss function we want a loss function that there's a good job of saying this is a high-quality image without having to go all the over game trouble and preferably it also doesn't just say it's a high-quality image but it's an image which actually looks like the thing is meant to so the real trick here comes back to this paper from a couple of years ago perceptual losses for real-time style transfer and super resolution Justin Johnson at our curve this thing they call perceptual losses it's a most paper but I I hate this term because they're nothing particularly perceptual about them I would call them feature losses so in the first AI library you'll see this referred to as future losses and it shares something with Ganz which is that after we go through our generator which they call the image transform net and you can see it's got this kind of unit shaped thing they didn't actually use new nets because at the time this came out nobody in the machine-learning world knew about units nowadays of course we use units about anyway something unit ish i should mention like in these kind of these architectures where you have a down sampling path from about up sampling path the down sampling path is very often called the encoder as you saw in our code actually we call that the encoder and the up sampling path is very often called the decoder in generative models more you know generally including generative text models neural translation stuff like that they tend to be called the encoder and the decoder two pieces there right so we have this um this generator and we want a loss function that says you know is the thing that it's created like the thing that we want and so the way they do that is they take the prediction remember Y hat is what we normally use for a prediction from a model we take the prediction and we put it through a pre trained image net network so at the time that this came out the pre-training network network they were using was vgg people still take that's a kind of old now but people still tend to use it because it works fine for this process so they take the prediction and they put it through PGG the pre trained image net network it doesn't matter too much which one it is and so normally the output of that would tell you hey is this generated thing you know a dog or a cat or an airplane or a or a fire engine or whatever right but in the process of getting to that final classification it goes through lots of different layers and in this case they've color-coded all the layers with the same grid size in the future map with the same color so every time we switch colors we're switching grid size so there's a stripe to convey or in VG's case they still used to use some max pooling layers which kind of similar idea and so what we could do is say hey let's not take the final output of the vgg model on this generated image that take care of something in the middle let's take the activations of some layer in the middle so those activations you know it might be a feature map of like 256 channels by 28 by 28 say and so those kind of 28 by 28 grid cells will kind of roughly semantically say things like hey in this in this part of that 28 by 28 grid is there something that looks kind of furry or is there something that looks kind of shiny or is there something that was kind of circular is there something that kind of looks like an eyeball or whatever so what we do is that we then take the target so that the actual Y value and we put it through the same pre-trained vgg Network now we can pull out the activations of the same layer and then we do a mean square error comparison so it'll say like okay in the real image grid cell 1 1 of that 28 by 28 feature map you know is is furry and blue and round shaped and in the generated image its furry and blue and not round shaped so it's kind of like an okay match so that ought to go a long way towards fixing our eyeball problem because in this case the feature Maps going to say this eyeballs here it's right here but there isn't here so do a better job of that please make better eyeballs so that's the idea okay and so that's what we call feature losses or Johnson and alcohol perceptual losses so so to do that we're going to use the since seven super res notebook and this time the task we're going to do is kind of the same as the previous task but I wrote this notebook a little bit before the game notebook before I came up with the idea of like putting text on it and having a random JPEG quality so the JPEG quality is always 60 there's no text written on top and it's 96 by 96 so and it's before I realized what a great word crap Fi is so it's called resize so here's our crappy images and our original images kind of a similar task to what we had before so I'm going to try and create a loss function which does this so the first thing i do is i define a base loss function which is basically an account i got to compare the pixels and the features you know and the choices mainly like MSC or l1 doesn't matter too much which you choose I tend to like l1 better than MSC actually so I picked l1 right so anytime you see base loss we mean l1 loss that you could use MSA loss as well so let's create a vgg model right so just using the pre-trained model in vgg there's a attribute called dot features which contains the convolutional part of the model so here's the convolutional part of the vgg model because we don't need the head because we only want the intermediate activations so then we'll check that on the GPU we'll put it into eval mode because we're not training it and we'll turn off requires grad because we don't want to update the weights of this model we're just using it for inference right for the loss so then let's enumerate through all the children of that model and find all of the max pooling layers because in this in the vgg model that's where the grid size changes and as you can see from this picture we kind of want to grab features from every time just before the grid size changes so we grab layer I minus 1 that's the layer before it changes so there's our list of layer numbers just before the max Pauling lives and so all of those are values not surprisingly so those are where we want to grab some features from and so we put that in blocks it's just a list of ID's so here's our feature loss Plus which is going to implement this idea so basically we when we call the feature loss class we're going to pass it some pre trained model and so that's going to be called m feet that's the model which contains the features which we want to generate for it one our feature loss on so we can go ahead and grab all of the layers from that network that we want the losses for that we would throw that we want the features for to create the losses so we're going to need to hook all of those outputs because remember that's how we grab intermediate layers in play torch is by hooking them so this is going to contain our our hooked outputs so now in the forward of feature loss we're going to call make features passing in the target so this is our actual Y which is just going to call that V GG model and go through all of the stored activations and just grab a copy of them and so we're going to do that both for the target call that out feet and for the input so that's the output of our generator in feet and so now let's calculate the l1 loss between the pixels because we still want the pixel loss a little bit and then let's also go through all of those layers features and get the l1 loss on them right so we're basically going through every one of these end of each block and grabbing the activations and getting the l1 on each one so that's going to end up in this list called feature losses which are then all up okay and you know by the way the reason to do it is a list is because we've got this nice little callback that if you put them into sync or metrics in your loss function it'll print out all of the separate layer loss amounts for you which is super handy so that's it that's our perceptual loss or feature loss plus and so now we can just go ahead and train a unit in the usual way with our data and pre-trained architecture which is a resonant 34 passing in our loss function which is using our pre-trained vgg model and this is that quad back I mentioned lost metrics which is going to print out all the different layers losses for us these are two things that we'll learn about in part two of the course but you should use them ll find I just created a little function called do fit that does fit one cycle and then saves the model and then shows the results so as per usual because we're using a pre trained network in our unit we start with frozen layers for the down sampling path train for awhile and as you can see we get not only the loss but also the pixel loss and the loss at each of our feature layers and then also something we'll learn about in part two called gram loss which I don't think anybody's used for super ears before as far as I know but as you'll see it turns out great so that's eight minutes so much faster than it can and already as you can see this is our output modeled out pretty good so then we unfreeze and train some more and it's a little bit better and then let's switch up to double the size and so we need to also have the batch size to avoid writing a GPU memory and freeze again and train some more so it's now taking half an hour even better and then unfreeze and train some more so all in all we've done about an hour and 20 minutes of training and look at that it's it's it's done it like I mean those it's it knows that eyes are important so it's really made an effort it knows that firs it's really made an effort so it started with something with like JPG artifacts around the ears and all this mess and like eyes that are just kind of vague light blue things and it just it really created a lot of texture this cat is clearly kind of like looking over the top of one of those little chlorine frames covered in fuzz so it actually recognized that this thing is probably kind of a capita materials that's created a cavity material for us so I mean that's just remarkable so talking of remarkable we can now so I I've never seen outputs like this before without again so I was just so excited when we were able to generate this and so quickly one GPU air and a half so like if you create your own crap of occasions and train this model your build stuff that nobody's built before because like nobody else's that I know of is doing it this way so there are huge opportunities I think so check this out what we can now do is we can now instead of starting with our low res I actually stored another set at size 256 which are called medium res so let's see what happens if we up size a medium res so we're going to grab our medium res data and here is here is our medium res stored photo and so can we improve this so you can see there's still a lot of room for improvement like you see the the lashes here a very pixelated place where there should be hair here is just kind of fuzzy so watch this area as I hit down on my keyboard bump look at that it's done it you know it's taken a medium res image and it's made a totally clear thing here you know the thirds reappeared but look at the eyeball let's go back the eyeball here is just kind of a general blue thing here it's added all the right texture you know so I just think this is super exciting you know here's a model in an hour and a half using standard stuff that you've all learnt about a you NetApp retrained model feature loss function and we've got something which can turn that into that or you know this absolute mess into this and like it's really exciting to think what what could you do with that right so one of the inspirations here has been a guy called Jason integer and Jason was a student in the course last year and what he did very sensibly was decided to focus basically nearly quit his job and work four days a week or really six days a week on studying deep learning and as you should do he created a kind of capstone project and his project was to combine Ganz and feature losses together and his krappa fication approach was to take color pictures and make the black and white so he took the whole of image net created a black and white image net and then trained a model to recolor eyes it and he's put this up his do defy and now he's got these actual old photos from the 19th century that he's turning into color and like what this is doing is incredible like like look at this the model thought oh that's probably some kind of copper kettle so I'll make it like copper colored and oh these pictures are on the wall they're probably like different colors to the wall and maybe that looks a bit like a mirror maybe it would be reflecting stuff outside you know these things might be vegetables vegetables are often red you know let's make them red it's it's extraordinary what it's done and you could totally do this too like you can take our feature loss and our gain loss and combine them so I'm very grateful to Jason because he's helped us build this this lesson has been really nice because we've been able to help him to because he hadn't realized that he can use this pre-training and stuff and so hopefully you'll see two older phi in the next couple of weeks be even better at the olaf occation but hopefully you all can now add other claims of D Kappa fication methods as well so I'm you know I like every course if possible to show something totally new because then every student has the chance to basically build things that haven't been built before so this is this is kind of that thing you know but between the much better segmentation results and these much simpler and fast d krappa fication results i think you can build some really cool stuff do you have a question is it possible to use similar ideas to unit and ganz for NLP for example if I want to tag the verbs and nouns in a sentence or create a really good Shakespeare generator yeah pretty much we don't fully know yet it's a pretty new area that there's a lot of opportunities there and we'll be looking at some in in a moment actually so I actually tried training this actually tried testing this on this remember this picture I showed you with a slide last lesson and it's a really rubbish looking picture and I thought what would happen if we tried running this just through the exact same model and it changed it from that to that so I thought that was a really good example you can see something it didn't do which is this weird discoloration it didn't fix it because I didn't crap a fire things with weird discoloration right so if you want to create really good image restoration like I say you need really good gratification okay so um here's what we've learned so far right in in the course and some of the main things so we've learnt that neural nets consist of Sandwich layers of affine functions which are basically matrix multiplications slightly more general version and nonlinearities like r lu and we learnt that the results of those calculations are called activations and the things that go into those calculations that we learn are called parameters and that the parameters are initially randomly initialized or we copy them over from a pre-trained model and then we train them with SGD or cluster versions and we learned that convolutions or a particular affine function that work great for auto correlated data so things like images and stuff we learn about batch norm drop out data augmentation and weight decay as ways of regularizing models and also batch norm helps train models more quickly and then today we've learned about res slash dense blocks we've all of us you learn a lot about image classification regression embeddings categorical and continuous variables collaborative filtering language models and NLP classification and then kind of segmentation unit and gains so go over these things and make sure that you feel comfortable with each of them if you've only watched this series once you definitely won't people normally watch it you know three times or so to really understand the detail so one thing that doesn't that doesn't get here is our it ends so that's the last thing we're gonna do fairness okay so marinades I'm going to introduce a little kind of diagrammatic method here to explain our ends and the diagrammatic method I'll start by showing you a basic neural net with a single hidden layer square means an input so that'll be batch size by number of inputs right so kind of you know batch size by number of inputs an arrow means a layer broadly defined such as matrix products followed by value a circle is activations okay so in this case we have one set of hidden activations and so given that the import was number of inputs this here is a matrix of number of inputs by number of activations so the output will be sighs by a number of activations it's really important you know how to calculate these shapes right so go learn dot summary lots to see all the shapes so then here's another arrow so that means it's another layer matrix product followed by non-linearity in this case we're going to the output so we use soft Max and then triangle means an output okay and so this matrix product will be number of activations by a number of classes so our output is batch size by number classes okay so let's reuse the that key remember triangle output circle is activations hidden state we also call that and rectangle is important so let's now imagine that we wanted to create a get a big document split it in two sets of three words at a time and grab each set of three words and then try to predict the third word using the first two words so if we had the data set in place we could grab word one as an input Chuck it through an embedding right creates motivations pass that through a matrix product and a non-linearity grab the second word put it through an embedding and then we could either add those two things together or concatenate them generally speaking when you see kind of two sets of activations coming together in a diagram you normally have a choice of concatenate or or add and that's going to create the second bunch of activations and then you could put it through one more fully connected layer and softmax to create an output so that would be a totally standard fully connected neural net with one very minor tweak which is concatenating or adding at this point which we could use to try to predict the third word of every from pairs of two words okay so remember arrows represent layer operations and I removed in this one the specifics of what they are because they're always a fine function followed by a non-linearity okay let's go further what if we wanted to predict word for using words one and two and three it's basically the same picture as last time except with one extra input than one extra circle but I want to point something out which is each time we go from rectangle to circle we're doing the same thing we're doing an embedding which is just a particular kind of matrix model play where you have a one-pot encoded input each time we go from circle to circle we're basically taking one piece of hidden state month of activations and turning it into another set of activations by saying we're now at the next word and then when we go from circle to triangle we're doing something else again which is we're saying let's convert the hidden state these activations into an output so it would make sense so you can see I've colored each of those errors differently so each of those errors should probably use the same weight matrix because it's doing the same thing so why would you have a different set of embeddings for each word or a different set of a different matrix to multiply by to go from this hidden state to this hidden state versus this one okay so this is what we're going to build so we're now going to jump into in numbers which is less than seven human numbers now this is a data set that I created which literally just contains all the numbers from one to nine thousand hundred 99 written out in English okay and we're going to try and create a language model that can predict the next word in this document it's just a toy example for this purpose so in this case we only have one document and that one document is the list of numbers so we can use a text list to create an item list with text in for the training and the validation in this case the validation set is the numbers from 8,000 onwards and the training set is 1 to 8,000 we can combine them together turn that into a data bunch so we only have one document so train 0 is the document grab its dot text that's how you grab the contents of a text list and here are the first 80 characters it starts with a special token xxxx BOS anything starting with xxxx is a special first day token the OS is the beginning of stream token it basically says this is the start of a document and it's very helpful in NLP to know when documents start so that your models can learn to recognize them the validation set contains 13,000 tokens so 13 thousand words or punctuation marks because everything between spaces is a separate token the batch size that we asked for was 64 and then by default it uses something called B PTT of 70 B PTT as we briefly mentioned stands for back prop through time that's the sequence links so for H of our so with each of our kind of 64 document segments we split it up into lists of 70 words that we look at at one time so what we do is we grab this for the validation set entire string of 13,000 tokens and then we split it into 64 roughly equal sized sections okay people very very very often think I'm saying something different I did not say they are of length sixty-four they're not there 64 equally-sized roughly segments so we take the first 1/64 of the document piece one second sixty-four space - okay and then for each of those 1/64 of the document we then split those into pieces of length 70 so each batch right so let's now say okay for those 13,000 tokens how many batches are there well divide by batch size and divide by 70 so there's about 2.9 batches so 3 there's going to be 3 batches so let's grab an iterator for a data loader grab 1 2 3 batches the X and the y and let's add up the number of elements and we get back slightly less than this because there's a little bit left over at the end that doesn't quite make up a full batch okay so this is the kind of stuff you should play around with a lot lots of shapes and sizes and stuff and iterators as you can see it's 95 by 64 I claimed it was going to be 70 by 64 that's because our data loader for language models slightly randomizers be PTT just to give you a bit more kind of shuffling get a bit more randomization it helps the model and so here you can see the first batch of X yeah remember we've numeric alized all these and here's the first batch of Y and you'll see here this is 2 18 10 11 8 this is 18 10 11 8 so this one is offset by 1 from here because that's what we want to do with a language model we want to predict the next word so after two should come up 18 and after 18 should come 10 right you can grab the vocab for this data set and a vocab has a text defy so if we call it exactly the same look at the same thing but with text fi that'll just look it up in the vocab so here you can see xx POS 8001 whereas in the Y there's no xx POS it's just eight thousand one so after xx POS is eight after eight these thousand after thousand is one okay and so then after we get 8023 comes x2 and look at this we're always looking at column zero so this is the first batch the first mini batch comes 8024 and then X 3 all the way up to eight thousand forty right and so then we can go right back to the start but look at batch one right so index 1 which is batch number two and now we can continue a slight skip from 8042 eight thousand forty six that's because the last mini batch wasn't quite complete so what this means is that every mini batch so every yeah every mini batch joins up with the previous mini batch you know so you can go straight from x1 0 to X to 0 it continues 8023 8024 right and so he took the same thing for colon comma 1 you'll also see they join up so all the mini batches join up so that's the data we can do show better to see it and here is our model which is doing this right so here is this is just the code copied over right so it content contains one embedding ie the green error one hidden to hidden brown arrow layer and one hidden to output right so each colored arrow has a single matrix okay and so that in the forward paths we take our first input X 0 and put it through input to hidden the green arrow okay create our first set of activations which we call H assuming that there is the second word because like sometimes we might be at the end of a batch where there isn't a second word assume there is a second word then we would eh the result of x1 put through the green arrow remember that's KH and then we would say okay our new H is the result of those two add it together put through our hidden to hidden orange arrow and then rel you then batch them and then for the second word do exactly the same thing and then finally blue arrow put it through H oh right so that's how we convert our diagram to code so nothing new here at all so now let's - okay and and just you know so we can check that in the learner and we can train it 46 percent okay let's take this code and recognize it's pretty awful there's a lot of duplicate code and as coders when we see duplicate code what do we do we refactor so we should refactor this into a loop so here we are we've refactored it into a loop so now we're going for each X I and X and doing it and loop guess what that's an errand in an hour it n is just a refactoring it's not anything new this is now a narrative okay and let's refactor our diagram from this to this this is the same diagram okay but I've just replaced it with my loop does the same thing so here it is it's got exactly the same unit literally exactly the same just popped a loop here before I start I just have to make sure that I've got some you know a bunch of zeros to add to and of course I get exactly the same result when I train it okay so next thing that you might think then and one nice thing about the loop though is now this will work even if I'm not predicting the fourth word from the previous three but the ninth word from the previous eight it'll work for any arbitrarily length long sequence just nice so let's up the BP TT 220 since we can now and let's say I'll say okay instead instead of just predicting the enth word from the previous n minus one let's try to predict the second word from the first in the third from the second and the fourth from the third and so forth right because previously like look at our loss function previously we were comparing the result of our model to just the last word of the sequence this is very wasteful because there's a lot of words in the sequence so let's compare every word in X to every word in Y so to do that we need to change this so it's not just one triangle at the end of the loop but the triangle is inside this right so that in other words after every loop predict loop predict loop predict so here's this code it's the same as the previous code but now I've created an array and every time I go through the loop I append HOH to the array so now for n inputs I create n outputs so I'm predicting after every word previously I had 46% now I have 40% why is it worse well it's worse because now like when I'm trying to predict the second word I only have one word of state to use right so like and when I'm looking at the third word I only have two words of state to use so it's a much harder problem for it to solve so the obvious way to fix this then would you know the key problem is here I go H equals torch zeros like I reset my state to zero every time I start another B PTT sequence well let's not do that let's keep H but and we can because remember each batch connects to the previous batch it's not shuffled like happens in you know image classification so let's take this exact model and replicate it again but let's move the creation of H into the constructor okay there it is so it's now self dot H okay and so this is now exactly the same code but at the end let's put the new H back into self dot H okay so it's now doing the same thing but it's not throwing away that state so therefore now we actually get above the original we get all the way up to 54 percent accuracy so this is what a real iron tin looks like they you know you you always want to keep that's date right but just keep remembering there's nothing different about an iron and it's a totally normal fully connected neural net okay it's just that you've got a loop you refactored what you could do though is at the end of your every loop you could not just spit out an output but you could spit it out into another errand in so you have an errand in going into an errand and that's nice because we've now got more layers of computation you would expect that to work better well to get there let's do some more refactoring so let's take this code and replace it with the equivalent built in pipe torch code which is you just say that so n n dot R and n basically says do the loop for me okay we've still got the same embedding we've still got the same output still got the same batch norm we still got the same initialization of H but we just got rid of the loop so one of the nice things about our a-10 is that you can now say how many layers you want so this is the same accuracy of course so here I'm going to do it with two layers but here's the thing when you think about this right think about it without the loop it looks like this right it's like keeps on going and we've got a BP GT of 20 so there's 20 layers of this and we know from that visualizing the lost landscapes paper that deep networks have awful bumpy lost surfaces so when you start creating long timescales and multiple layers these things get impossible to train so there's a few tricks you can do one thing is you can AB skip connections of course about what people normally do is instead they put inside instead of just adding these together they actually use a little mini neural net to decide how much of the green arrow to keep and how much of the orange arrow to keep and when you do that you get something that's either called giu or an LS TM depending on the details of that little neural net and we'll learn about the details of those neural nets in part two they really don't matter though frankly so we can now say let's create a GI u instead that's just like what we had before but it'll handle longer sequences and deeper networks let's use two layers bump and we're up to 75 percent okay so that's our intends and the main reason what it's show it to you was to remove the the last remaining piece of magic and this is one of like the least metrical things we have in deep learning it's just a refactored fully connected Network so don't let our own ends ever ever put you off and with this approach where you basically have a sequence of n inputs and a sequence of n outputs which we've been using for language modeling you can use that for other tasks right for example the sequence of outputs could be for every word there could be something saying is there something that I is sensitive and I want to anonymize or not you know so like is this private private data or not or it could be a part of speech tag for that word or it could be something saying you know how should that word be formatted or whatever and so these are called sequence labeling tasks and so you can use this same approach for pretty much any sequence labeling task or you can do what I did in the earlier lesson which is once you finish building your language model you can throw away their kind of this h0 bit and instead pop their a standard classification hit and then you can now do NLP classification which as you saw earlier will give you state at the out results even on long documents so this is a super valuable technique and not remotely metrical okay so that's it right that's that's deep learning or at least you know the kind of the practical pieces from my point of view having watched this one time you won't get it all and I cannot recommend that you do watch this so slowly that you get it all the first time but you go back and look at it again take your time and there'll be bits that you go like oh now I see what he's saying and then you'll be able to like implement things you couldn't implement before and you'll be able to dig in more than you thought so I like definitely go back and do it again and as you do write write code not just for yourself but put it on github right it doesn't matter if you think it's great code or not you know the fact that you're writing code and sharing it is impressive and the feedback you'll get if you tell people on the forum you know hey I wrote this code it's not great but you know it's my first effort anything you see jump out at you people will say like oh that bit was done well hey but you know for this bit you could have used this library and safeties in time you'll learn a lot by interacting with your peers as you've noticed I've started introducing more and more papers now part two will be a lot of papers and so it's a good time to start reading some of the papers that have been introduced in this in this section all the bits that say like derivation and theorems and lemmas you can skip them I do they add almost nothing to your understanding of practical tech learning right but the bits that say like you know why are we solving this problem and what are the results and so forth really interesting and then you know try and write English prose not know English prose that you want to be read by Geoff Hinton and Yann LeCun but English prose that you want to be written read by you as of six months ago right because there's a lot more people in the audience of you as well six months ago then there is of Geoffrey Hinton Danielle Loughran right that's that's the person you best understand you know what they need right go and get help and help others tell us about your success stories but perhaps the most important one is get together with others about people's learning works much better if you've got that social experience so start a book club get involved in meetups create study groups and build things right and again it doesn't have to be amazing but just build something that you think the world would be a little bit better if that what existed or you think it would be kind of slightly delightful to your two-year-old to see that thing or you just want to show it to your brother the next time they come around to see what you're doing whatever right I just finish something you know finish something and then try make it a bit better so for example something I just saw this afternoon is the Elan Elan musk twitch ener okay so looking at lots of older tweets creating a language model from from Elon Musk and then creating new tweets such as humanity will also have an option to publish on its own journey as an alien civilization it will always like all human being Mars is no longer possible hey I will definitely be the Central Intelligence Agency okay so this is great I love this and I love that Dave Smith wrote and said these are my first-ever commits thanks for teaching a finance guy how to build an app in 8 weeks right so you know I think this is awesome and I think like clearly a lot of care and passion is being put into this project you know will it systematically change the future direction of society as a whole maybe not you know but maybe a nun will look at this and think like oh you know like I maybe I need to rethink my method of prose I don't know I think it's I think it's great and so yeah create something put it out there put a bit of yourself into it or get involved in fast AI the first AI project there's a lot going on you know you can help with documentation and tests which might sound boring but you'd be surprised how incredibly not boring it is to like take a piece of code that hasn't been properly documented and research it and understand it and ask Sylvia and I on the forum what's going on why did you write it this way we'll send you off to the papers that we were implementing you know writing a test requires deeply understanding that part of the machine learning world to really understand how it's meant to work so that's what was interesting stairs Backman has created this nice dev projects index which you can like go onto the forum in the faster I dev section and find that you the dev project section and find like here's some stuff going on that you might want to get involved in or maybe there's stuff you want to exist you could add your own create a study group you know Deena's already created a study group for San Francisco starting in January this is how easy it is to create a study group right go on the forum find your little time zone sub category and add a post thing let's create a study group okay but make sure you you know give people like a little Google sheet to sign up some way to actually do something you know the great example is Pierre who's been doing a fantastic job in Brazil of running study groups for the last couple of classes of the course and you know he keeps posting these pictures of people having a good time and learning deep learning together creating wiki's together creating projects together great experience and then come back for part two right where we'll be looking at all of this interesting stuff in particular going deep into the FASTA a code base to understand how did we build it exactly will actually go through as we were building it we created notebooks of like here where here's where we were at each stage so we're actually going to see the software development process itself we'll talk about the process of doing research how to read academic papers how to turn math into code and then a whole bunch of additional types of models that we haven't seen yet so it'll be kind of like going beyond practical deep learning into actually cutting edge research so we've got five minutes to take some questions we had an AMA going on online and so we're going to have to for a couple of the highest-ranked ama questions from the community and the first one is by Jeremy's request although it's not the highest ranked what's your typical day like how do you manage your time across so many things that you do yeah I thought that I hear that all the time so I thought I should answer it and I think got a few votes because I think people who come to our study group are always shocked at how disorganized and incompetent I am and so I often hear people saying like oh wow I thought you were like this deep learning role model and I'll get to see how to be like you and now I'm not sure what it'd be like you at all so yeah it's for me it's all about just having a good time with it I never really have many plans I just try to finish what I start if you're not having fun with it it's really really hard to continue because there's a lot of frustration in deep learning because it's not like writing a web app where it's like you know authentication check you know back-end service watchdog check ok user credentials check you know like you just you're making progress where else for stuff like this Dan stuff that we've been doing the last couple of weeks it's just like it's not working it's not working it's not working no it also didn't work it also didn't work until oh my god it's amazing it's a cat that's kind of what it is right so you don't get that regular feedback so yeah you know you got to have fun with it and so so Mike yeah my day is kind of you know I mean the other thing I do I'll say I don't I don't do any meetings I don't do phone calls I don't do coffees I don't watch TV you know play computer games I spend a lot of time with my family a lot of time exercising and a lot of time reading and coding and doing things I like so you know I think the you know the main thing is just finish finish something like properly finish it so when you to that point where you think 80% of the way through but you haven't quite ready to read me yet and the install process is still a bit clunky and you know this is what 99% of github projects look like you'll see the readme says to do you know complete baseline experiments document blah blah blah it's like don't be that person like just do something properly and finish it and maybe get some other people around you to work with you so that you're all doing it together and you know get it done what are the up-and-coming deep learning machine learning things that you're most excited about also you've mentioned last year that you are not a believer in reinforcement learning do you still feel the same way yeah I still feel exactly the same way as I did three years ago when we started this which is it's all about transfer learning it's underappreciated it's under-researched every time we put transfer learning into anything we make it better better you know our academic paper on transfer learning for NLP has you know helped be one piece of kind of changing the direction of NLP this year it made it all the way to the New York Times just a stupid obvious little thing that we threw together so I remain excited about that I remain unexcited about reinforcement learning for most things I don't see it used by normal people for normal things for nearly anything it's an incredibly inefficient way to solve problems which often solved more simply and more quickly in other ways it probably has a maybe a role in the world but a limited one and not in most people's day-to-day work for someone planning to take part - and 2019 what would you recommend doing learning practicing until the part two course starts just code yeah just code all the time I know it's perfectly possible I hear from people who get to this point of the course and they haven't actually written any code yet and if that's you it's okay you know you've just go through and do it again and this time do code and and look at the input the shapes of your inputs and look at your outputs to make sure you know how to grab a mini batch and look at that's mean and standard deviation and plot it and you know there's there's so much material that we've covered if you can get to a point where you can you know rebuild those notebooks from scratch without too much cheating but I say from scratch I mean using the first day library not from scratch from scratch you know you're you're being in the top echelon of practitioners because you'll be able to do all of these things yourself and that's really really rare and that'll put you in a great position for part two nine o'clock be honest do one more and where do you see the fast day Iowa library going in the future say in five years well like I said I don't make plans I just I just piss around so I mean our only plan for fast AI you know as an you know organization is to make deep learning accessible as a tool for normal people to use for normal stuff so as long as we need to code we failed at that so the big goal you know cuz 99.8% of the world can't code so the main goal would be to get to a point where it's not a library but it's a piece of software that doesn't require a code it certainly shouldn't require a goddamn lengthy hard working course like this one you know so I want to get rid of the course I want to get rid of the code I want to make it so you can just do useful stuff quickly and and easily so that's that's maybe five years yeah maybe longer all right I hope to see you all back here for part two thank you 