 you [Music] we're very happy to have Benson visiting us today I think many people here already know when from his time interning in the Redmond lab but if you don't when is when when has done some very interesting work both in theory and empirical in the areas of imitation learning and reinforcement learning and today he's going to talk to us about some of his results on generalization efficiency in her Elle thanks like everyone thanks for coming I'm one from CMU today I'm going to tell you a little bit about reinforcement learning with the focus on the generalization ability and sample efficiency all right so there are my PhD career I've been thinking about this problem how can we design algorithms that I have the ability to generalize and is also sample efficient in terms of learning to make complex decisions right to answer this question I'm gonna talk about two approach today so the first proach is to leverage extra help in this talk it will be expert demonstrations right so I will tell you how can we use expert demonstrations to design an algorithm that can efficiently solve sequential decision making problem problems that are otherwise are probably very hard for any reinforcement in the algorithm for the second part we're gonna focus on the settings where unfortunately we do not have any any extra help so we are going to look at the problem instance and trying to explore the structures of those problems in order to design sample efficiently algorithm right and what's more I'm going to tell you unify the story that can capture the complexities of the problem instance from a very large family now before diving into the details I just wanna quickly pointed out the difference between supervised learning and sequential decision making so supervised learning is probably the first thing that we learned in machine learning 101 we have a set of training data where each point is ID sampled from some unknown distribution and we fit at some function approximator right and afterwards given the New Testament we just make a prediction so the key here is that this whole process is kind of passive in a sense that you make a prediction on this test the image it does not gonna affect the future image that you're gonna see right because after after all we assuming everything every data point is IBC from some animal distribution right in other words your prediction is not going to affect the data distribution right but this is a fundamentally different in sequential decision making everything is active here the the sequence of the image that you're going to see in this video game is determined by the action you apply it right if you apply the bad actions this is the state the future states that you're gonna see forever if you want to go to state space where you have where you want to collect a large reward you have to learn to make a decisions to get to those state space right so you should everything is active your decision affects the future data distribution that you're going to see right so learning sequential decision-making can often be capture arised by this reinforcement learning framework where we have two components and learning agent and an environment where in this agents mind he has something called policy which is denoted by PI in this talk that map's a state that describes the current environment and the outputs interaction right and then they send the action to the environment and the environment evolves accordingly it sent back it one step immediate reward and it also generates the next state conditioning on the current state and the action it received we can assume that the transition dynamics here yes Markov Markovian transition okay and we assuming that the dynamics is unknown so we repeat this process for each many steps and the goal is to find a policy that maximizes the expected a total reward so in this set up the environment could be you know a self-driving car system where the state is the state of your car in the state of states of other cars right it could be a human like us where the agent is some math tutoring system for instance that tries to pick a math problem for us and the goal is to maximize our math performance across the entire semester for instance okay so more formally we want to find a policy pi that maximize the expected total reward across this edge many steps right so in this talk we're going to focus on finding horizon but the work that I'm going to describe later go can be easily extended to infinite horizon with discount vector and I'm gonna constantly interchange between reward and a cost because they're roughly the same you can think about the reward as a negative of the cost okay so reinforcement was very popular even 20 years ago right at that time we use reinforcement techniques to solve backgammon that was a big news at that time recently became popular again two years ago we sort of sort of the ancient board game go and we'd be human champion in this game and last year open the I tried to beat human champion in terms of playing this video game thought that which is very challenged it looks like you know IO is easy right because we sort of solving all these problems but let's look at into details of these problems for instance the opening i-5 so open here basically says that they played this game between this agent on hundreds of GPUs of course but also on 128 K CPUs right that seems a lot and if you zoom in each CPU years for instance 100 bucks then you can do the quick math wait this is the bill that you're going to receive maybe this kind of money isn't is not that much for Microsoft but I can't imagine what expression my divider would have if one day he see something like this right so I think if you have to spend that much money to just set up the computers to train this agent for just playing this video game I don't think the problem is easy right I don't think IO is solved and if you're looking into the details of these algorithms one major component that they're using is some random exploration strategy right which you basically do random trying and arrow by a massive simulation on massive number of CPUs for instance right well it doesn't really mean that the technique on this left hand side can be directly transferable to real-world applications for instance can we duplicate a patient into one billions of herself to do random try an arrow can we destroy one millions of cars before we learn how to drive it seems we can't right so what we really need for this real-world application is the sample efficiency now speaking of sample efficiency we have to look at the progress that we have made so far in terms of the interview in the theoretical reinforcement in community right so we understand how to solve discrete MDPs very well we have beautiful algorithms for this discrete MVPs and we know that in order to achieve absolute near optimal policy we just need to make polynomial in number of interactions with this word with this discrete word right polynomial with respect to the unique number of states and unique number of actions in the horizon and these algorithms are very nice they are mathematically beautiful but the major issue is that this polynomial dependency on the number of states forbids us to transfer what we understood here to real-world application right for instance for playing go we know that the total number of states in this game is larger than the total number of atoms in this universe so this poly dependency on the number of unique States is just going to kill us right not even mentioning real-world application we have continuous state space and the feature vector is usually high dimension and very complicated ok so what we could do so again let's look about supervised learning after all this is something that we understood pretty well in terms of both theory and in practice right so we have a large set of training data we fit a model on the training data and then we do prediction and there is no such thing called polynomial dependency on the number of unique image in the world right so that would be crazy because it means that if we want to make a prediction about this dog we pretty much have to see all the images of the dog in this world right and we actually can achieve this type of generalization via rich function approximation right so we would like to do the same thing for large skew MDPs specifically we want to bridge the gap between the left and the right by using rich function approximation technique right so that we can generalize across the states that we never seen before right so this is nice but in worst case we cannot achieve the generalization that I was just hoping for so the problem is that for problems like this needle in the haystack where you have only one reward in one of the particularly and if the agent has no prior knowledge of this problem the structure of this MDP the structure of the reward if it has to start from random it pretty much has to look at all the passes in order to find the needle in the leaf right so this means in the worst case the number of interactions you can make with respect with the loop with the real word is going to scale linearly with respect to the number of states right there's just not that much you could do okay so this leads to the two things that we're going to talk about so the first we're going to talk about how can we leverage expert Dennis trations imitation learning to efficiently solve problems including the needle in the haystack problem that is really hard for reinforcement any algorithm right and then we are going to look at the settings where we do not have expert demonstrations but how can we exploit the structures of the problem that we are facing and develop simple efficient algorithm for a large set of problems ok so let's talk about expert demonstrations what I'm going to show you here is first why we want to do imitation learning why imitation learning is better than reinforcement learning - how can we reduce the sequential decision making problem into a sequence of supervised learning by leveraging expert demonstrations 3 I'm going to talk about how can we generalize from a set of local experts okay so imitation learning when we talk about mutation learning we usually have three components an expert that provides some data some demonstrations and you have some machine learning algorithm that takes the data as input and compute some policy the maps from state to action and you can deploy this policy during test time okay so in this part of the talk we're going to focus on the setting which is interactive imitation learning with access to reward signals so the setting is basically we have a global optimal expert that is available during training and we can quarry for feedback from this expert when the example is a human sitting behind the wheel it can take over the car whenever the system asked him to write so this is the interactive expert that we're talking about more formally let's see we have some learning algorithm currently learning some policy and we drive the car using the color and learn the policy to some point and then we ask this interactive expert to take over from this point and the interactive expert just going to drive this car to the end of this episode and then we're going to record the expert trajectories total cost right so you can imagine this total cost is sort of indicating how easy it is to recover a potential mistake that just made by the learner okay so you may wonder what kind of examples are you know interactive expert well the first example is human sitting behind the wheel right that's how now we collect data for training autonomous driving and the second example that I want to talk about here is that an expert does not have to be human right it could be some algorithm or some software for instance a planner will control in some robotics applications for instance I want to train a policy that maps from image from cheap front cameras of a self-driving car to control signals right I can train this policy and tree and fashion but they're probably going to take me forever to train it what I could do is that in training time I probably will be able to buy very expensive sensors right and I can use them to build very accurate state estimator which allow me to build very complicated but accurate motion planner or optimal control during the training time which I'm not going to use it as a global expert during training right of course that kind of an expert is only available during training time because when we ship the cars to the market we just want to use the cheap sensors right so that the customers can afford it and the second example is that in some natural language certain tasks for instance dependency parsing by using the Koran truth labels in the training data and the objective function that we care to optimize we cancel the bill used search algorithm as the interactive expert so we will get to this point later all right so this is setup so now at this point you may wonder you have access to the objective function you have access to what signal why bother using imitation why don't you just use your favorite reinforcement learning algorithm right so intuitively imitation learning is just much more simple efficient than reinforcement only and a good example is high jump it took top athletes a couple of decades to figure out the correct way to do high jump but once dick Fosbury figured out this fast preflop right after one or two year pretty much everyone converge to this possibly flop right so it basically means that imitating is much faster than reinforcement Li you know trying to figure out the correct way from scratch so now let's formalize the advantage so the first advantage is global optimality if we assume that the expert that is available during training time is nearly global optimal then there are algorithm such as aggravate stands for aggregate with values they can learn a policy such that the the performance of the learn the policy PI hat is close to the performance of the expert so we avoid a local minimum in this case and the second benefit is that we can just learn much faster right so what we can show is that there exists a set of MVPs such that with global optimal expert during training we can learn Europe optimal solution using imitation learning algorithm buy time skills logarithmically with respect to the number of total states in this mvps while for any reinforcement in the algorithm you have to visit pretty much every state now this log will take a time here is something that we want right we want to learn near optimal behavior by just a visiting tiny part of the state space right so let me quickly explain how we get this exponential separation so the idea is that we're just gonna look at this needle in the haystack problem again right so it's very simple deterministic Markov decision process you have rewarded only at the leaf node and the needle is on the left hand side all right now without any prior knowledge of the structures of the reward of the dynamics we're just given the learner an optimal expert an optimal planner yes function right so we have an optimal planner that during training time at any state it will look at the subtree beneath this state and find the path that leads to the leaf that contains the highest reward okay now with the help of this expert we can pretty much reduce the sequential decision making problem into a bunch of sequence of supervised learning problem right so for instance at the very beginning the learner because the learner knows nothing it probably picks the worst pass right at this point we're just gonna ask the expert at the loot so the expert tells us that you should go left because the needle is on the left sub-tree right so now we just convert the learning problem at the first layer as a supervised learning problem we have state s 0 X 0 and we have labeled left right and with a supervised learning and we can safely eliminate the entire right subtree including the state that we never visited right now let's see we update our policy but we make a mistake again at X 1 and we do we ask the expert for help again and the expert tells us that you should go left again right and we do supervised alertly learning again and we eliminate the entire right subtree pinnies the state so basically every round we are eliminating half of the remaining nodes right yes they're smarter I mean why can't I just there's one and for the states and the exception tells me this is the correct one to have a gap from s21 sorry can you repeat the question why does the depth of the tree matter like why can't all the state be at there's one all the state adopts one so there's no transition from state to state yeah just I mean you stop you start at zero and then you know you just don't know where to go thank you is it because you want to actions on I mean you can do clean action visits of a number oh yeah I mean if we have many actions like here okay I guess it's just trivial reason separation could be from 1 to s but I think they'll be able to pretends on the number of actions that we have access to so here basically the arrow and I am one both of the same set of actions yes the number of actions is not growing it's being held constant yes but the aisle solution is honest yes the other case we need we need a lot of the number of actions to grow with the number of students for that construction yes why is that the problem you have to be careful your multi class classifier might have a flow dependence a number of actions it can have up to limit the number of actions specifically but here there is nothing statistical such a thing I don't ask them so deterministic there is no this yeah so this example is issuance but you can transfer the stochastic sorry but you you won't get this home but you get just gonna walk twice this exponential with a majority average well you may have a logarithmic dependency on the number states again but it's not just going to be so this is my hobby really average this is domestic but you can extend it just on se setting by running the in the X forever that's the number of actions it's fun it is here it's Superman yeah so we're thinking that the polycomb tendency on the action is fine but the party fans on the stage is not yes so why is YouTube down just for the purpose of constructing this this this example for proving the know about what your but all of your states could be like you could like you could only have one yeah so so if if it's not balanced if you have like just a pass then both words are going to perform the Simmons seem similar right but we're just trying to build a concrete example where you can separate the RL and intentionally just for the purpose of ability the right so as I mentioned one of the algorithms that can achieve this level of efficiency is the aggravate stands for aggregate aggregate with values so again the idea is very simple we want to convert this problem interest rate provides reading right we're going to use two procedures flowing and row out to generate a supervised learning sample so let's see at any time during training I'm just gonna execute my cullen donor in the real world and stop at some randomly pick time step generate a state right and then I'm gonna ask the expert to take over I will try first action a1 and then I ask the expert take over and the for instance in this time the expert occurred cost 100 it's completed out of a control a 1 is just a bad action at State at state ax right and I go back try a chip and the actually ask the expert to take over again and this time we drive very smooth stay mainly in the track so cost is zero and then I go back again try a three and ask the expert take over and this time for instance the total cost is five so basically by doing this procedure I generate a supervisor only sample where the feature vector is state X and I have three labels here a1 a2 a3 and each label has its own Associated cost and if I do this sorry so in this example for instance if I classify it to action a1 it's really bad because it's cost is 100 right but if I classify it to a3 it may be okay because 5 and it's not a big deal so if I do this procedure multiple times I can get a data set that consists of pairs of state and cost vectors of which dimension is the number of actions which is 3 in this toy example right so this is nothing but a cost-sensitive classification data set a supervised learning data set right now with this supervised learning data set we can actually train our policies using state of art functionally nonlinear function approximation techniques right for instance we could just start directly parameterizing our policy using very deep neural network differentiable neural network and we do this role in law procedure much of all times get a constant cost-sensitive classification dataset and we form a cost and save loss write a classification loss and then you just differentiate this last function to compute the policy gradient and once you have the policy gradient you just close the loop by performing gradient descent SGD or stochastic nature of gradient descent the key here is that by converting this problem into supervised learning by generating supervised learning datasets we can use which function approximated to take care of potentially very complicated features so wonderful example that we did is dependency passing on handwritten algebra equations and solutions so the input is a algebra equation and its solutions provided by some student right so it's a low pixel image and we want to output a dependency tree a path tree okay now dependency parsing has been studied for a while and people convert this to a sequential decision making problem in fact researchers in MSR actually did some work on this for dependency parsing for natural language right so we're just going to use the same framework that converts dependency parse into sequential decision making but the key challenge here is that we have to deal with this low pixel image the handwritten algebras written by a student right and what we used here is some OST m to represent an encoder that scans the character one by one and the end outputs a feature vector that sort of serves as the summary of the handwritten algebra is provided by a student right and afterwards we just going to use another decoder to compute the sequence of transitions which will be used to build this parse tree right so at this point everything is basically similar to dependency processing on natural language so we compared this approach to the reinforcement approach which uses the reward signal but it ignores the interactive global optimal expert right and we also compared to another imitation learning algorithm which uses the interactive global optimal expert but ignores the reward and we can show that by leveraging both interactive global optimal experts and the reward signal we can do much better compared to either of the two baselines all right cool so if we have a global optimal expert we can use it to achieve Sampo efficiency in both theory and in practice but what if we do not have a global optimal expert right so what we're going to do in this part of talk is to show how can we generalize from local experts and we explain what it is so the motivating example is alphago zero so for alphago zero because we are dealing with the game go we have 40 access to the known and deterministic transition dynamics right and at any point during training we have two policies we have a faster reactive policy often represented by some deep neural network that can be executed in test time in real time right very fast and we have some very slow policy at the same time which in this case is a search tree right at every state visited by the reactive policy I'm just going to grow this tree and then I'm going to do searching this tree and provide a supervision for training the reactive policy right so in other words I will never ever train the reactive policy using reinforcement learning techniques I'm actually training this reactive policy by treating this slow policy as the expert and the tree it used to provide learning right this is actually the key that different different difference between this alphago zero and the older version of alphago all right and I won't point out here is that by no means that this search tree is a global optimal expert right as I mentioned the total number of states in this game is larger than total number of atoms in the universe so this tree just explore tiny part of the state space around the states that previously exploited by this faster reactive policy the global policy all right but nevertheless this local expert provides useful information to supervise training this fast reactive policy now this alpha zero leverage the transition dynamics to Bo the local experts but when we talk about reinforcement early in general we do not have any prior knowledge about the transition dynamics right so how can we do this yes right right so we can learn the model on the flight so what I'm gonna show later is that we're gonna spend minimal effort learning the model that just for the purpose of building local experts right all right so this leads us to the framework called do power situation where the idea is simple every time during training I'm gonna maintain two policies a faster reactive global policy and maybe some complicated slow local experts right so let's see at some iteration n I have my calendar and policy a global policy voice is represented by some deep neural network I'm gonna execute in the real word january some trajectories and I'm gonna extract a bunch of state action next state triples from the generate trajectories and I'm going to use supervised learning techniques to fit a estimation that estimate the transition model right that takes state action as input for dates the next day and I want to pointed out that this learned model here's just a local model right because we are fitting this model on the data that generated from the Cullen global policy and with this model available now we can apply local planning or local control what we do is that we use this learned local model to expand the search space around this red trajectory the state that was previously explored by the Cullen global policy right so once we explore the space a little bit using the learn model we can do search or planning on for instance on this tree okay and once we have this local control we're going to use blackbox imitation learning algorithm to update the parameters of our global policy so that the behavior we're getting closer to the local experts right and then we close the loop hopefully we improve the performance we can in Milan assumptions this process guarantees monotonic convergence monotonic improvement and gets to convergence you have in mind here like yeah I think about locality in the policy space in terms of positions and use yeah so so what we did is in terms of distribution used so what you do yeah so what do you do is that you take this learning model as input and the reward function is input you're trying to ask the optimal control to compute a policy but you're gonna subject your trust region constraint no it's actually it's elastic so when you were saying you you're predicting the next state how can you tribution oh you just predicted distribution places using absolute contradiction yeah so as I mentioned this is a very general framework you can plug in any supervised learning techniques to fit the model you can use any black box optimal control you can use any black box imitation on the algorithm right so the first instance that I'm showing here is that we use a bunch of patient linear regressors to fit time-dependent dynamics and we use iterative linear quadratic regulator as the black box optimal control and we use aggravated with natural gradient update as the black box a mutation owner and we tested it on helicopter funnel so I'm just demonstrating the behavior of the learning policy on this helicopter simulator right so learns the policy how to do this helicopter funnel which is flying this helicopter in a circle and in second video I'm showing you the trace of the center of mass so after five iterations doesn't know what what's going on but roughly after ten iterations it kind of learned what's going on and after 15 iterations it pretty much can do this motion and comparing two popular deep-pile methods we can show that in log scale we can actually learn much faster than for instance this baseline right and then what about comparing to more classic but probably efficient algorithms such as conservative policy division so we did experiment on synthetic discrete MVPs which I generated randomly and we used another special instance derived from the do policy iteration framework where we use maximum likelihood to estimate the model we use value iteration to as the optimal control and again we use aggregate very aggravated as the blackbox imitation donor and again we can show that in log scale it can significantly outperform conservative policy division right okay so we have talked yes I'm trying to place this in the landscape where there's also a bunch of these papers looking at self imitation that's another approach where the the most basic one I know office where you don't try to learn these dynamics models you just based on all the rulers you've collected just try to limit the best-performing ones so you mean sign bunch of random actions and basically if you have your current global policy just generate a bunch of robots under that yeah look at the subset of the robots that seem to have got your good robots and try to immediate just the subset so this is very similar to your voice this is the tree search right yeah it's just a new just using shooting hours and should generate a bunch of actions and basically I'm trying to like understand what are the trade-offs between those classes of approaches which are also invoking an imitation learning style or occur when you are not having to learn our forward dynamics model I feel like if you doing if you're dealing with an authorized on problem if you don't learn the model right if you're just using shooting algorithm to generate a sequence of actions and repeat that process you know 500 or 1000 times then pick the best one it is asymptotic but the chance that you get a good sequence of actions is is very small right it's exponentially lower when your horizon getting bigger right but if you can learn a model you are basically leaving the search procedure to a black box optimal control subjective how good the learn model is yeah we'd also be by the rubric stochastic dynamics is so long it's like learning the dynamics model becomes much much harder because now and it will done you get the distribution over the next States right what are you get America tech please all right so for the first part we talked about how we leverage exploit them instructions to quickly solve problems that are potentially hard for any reinforcement in the algorithm alright so now let's talk about the second part where we do not have any extra help so what we're going to do is look at the problem instance and trying to exploit the structures of these problems in order to design standpoint fish in the algorithm right what I'm going to tell you later is a unified story that can captures a large number of problem instance and specifically first I want to tell you why we want to exploit the structures in other words why we want to do model-based rl2 I'm going to just kind of introduce you this unify the measure that captures the complexities of a lot of special problems previously studied in the literature all right so first of all what is model-based RL versus model-free our L so this boils down to the effort of modeling dynamics the first example is the example where we know that we we know the dynamics perfectly right we have a previous simulation simulates the world yes question yeah yes oh yes yes it is reinforced metal setting but you still need the expert to law out right yes it is a controlled yes oh you mean the second part yes so we're just trying to right so we just basically treating this control as the expert so in some times this expert is not it's not like a for instance a human expert yeah but the planning step in the control step is probably very computation expensive I was wondering so in the very early part of the talk we were talking about expertise logical access and then the second part locally optimal experts right so where does the what do you use the distinction in the in your algorithm but why basically doesn't matter for the expert it's close the first I think the guarantee of the second part is just much much weaker than the first one okay so it's basically hidden even in the proofs somewhere yeah yes I was actually wondering could you comment a little bit on what exactly can you guarantee in the second the guarantee that you can get is pretty much the whatever the approximate power situation type of a person can get you so you use you sort of guarantee just the local clarity and if you're lucky if you have good initial state distribution then you may be fined result basically the bounteous skills by a ratio that's between the state distribution you introduced and the optimal parse is the distribution times the like the Delta they all tell us the policy improvement from the imitation learning and the optimal control yep okay so the first example we know perfect model and we can do planning and control just in the simulation right there's no we don't need any real-world samples the second example is that in unstructured environments we probably don't know the exact model right it's really hard for us to write down the differential equation for this car you know drifting and jumping when driving in high speed on this rough terrain but we could do this using data driven approach to learn the model that predicts the real word transition model right and then we can do planning and control so this is model based RL the third approach is that I don't know the model but I don't even bother to learn it I'm just gonna directly predict the behavior by doing random trial and error this is a model of free so in this part of the talk we're going to focus on model-based RL right so in terms of setup we have the real word transition to notice P star here for instance this captures the real word transition of my car right now because it's unknown so I need to use function approximated approximately so we assuming that we are given a set of function proximities where each function proximity trying to approximate this peach dot that takes state action as input and outputs a conditional distribution over the next state right and we assuming that this function class is rich enough to capture the true real-world transition okay and we assuming that we have an optimal planner that takes the an arbitrary model transition model as input and the reward function as input I'll put the policy there optimize the reward function subject to the given model right so this is nothing but just a planning or control step and plan is very hard but as I mentioned before we often in practice have very efficient and accurate motion planners and I emphasize that this step does not require real-world samples right everything is done in simulation because the model is given now why we want to do model-based Aria there is a long debate in terms of model based versus model free right raging from very old methods such as iterative learning control from 1980s to more recent works such as guided policies search or the do policy iteration framework that I just mentioned before practical wisdom seems indicating that model-based algorithm is often more efficient sometimes probably exponential exponentially more efficient than model free right now this is cool but what about in theory can we do any claim can we claim anything in theory so this is will be shown in this summer is that there exists a set of Markov decision process including common ones such as a factor a MVPs such that in order to learn your optimal behavior model based algorithm gonna spend time polynomially with respect to all the relevant parameters but any model free algorithm has to spend time skills exponentially with respect to the horizon of the problem right so to the best of our knowledge this is the first exponential separation in sample complexity for model based versus model free right to me this theorem basically motivates me to further study model-based I or algorithm right and in fact in the literature people actually spend a lot of time studying efficient model based algorithm for different problems but the effort were kind of independent in the sense that we design specific algorithm for a specific problem we design algorithm for fracture the MDP for instance which is often used to model something like data center cooling system but the algorithm that we designed for factor the MVP does not directly transferable to for instance a linear quadratic regulation system and a vice-versa so can we build a unified story can we come up a unified algorithm that can simultaneously check that can achieve sample efficiency for all these problems including the problems that not shown in this picture so this is what I'm gonna tell you now so before we dive into the details I would just want to quickly introduce one statistical two that we're going to use which is a tool used to distinguish two distributions right imagine I have two distributions P and Q P is a distribution for instance models the real-world bedroom image and Q is a distribution for instance you come up in order to approximate that there were real word distribution right the imaginary samples from your model to imaginary bathroom samples and now there's some technique called integral property metric that basically takes these two sets of samples as the input and outputs a number that tells us how far these two distribution is basically a uses set of discriminates a function that maps from image to real number and look at these two samples and they tell us the number that captures the divergence between these two distributions right so you can you can sort of intuitively think about this discriminator that looks in an image and tells us how real it is right and this is a very general divergence in a sense that if it's if we design specific dish criminals we recover very common divergence for instance if if all the discriminant functions with bounded values then we recover total variation distance if all the discriminates are functions with bounded Lipschitz constant then we recover what's a certain distance right so you should this is a very general divergence that looks at two sets of samples from two distributions and output a number telling you how far they are with each other okay now with this to you mind we can actually do these cool things how can we distinguish a candidate transition dynamics from the real-world transitions right now let's assuming that we have a set of samples represented by the green dots a set of states I will tell you how we generate them in the dark side but let's assuming we have these sets of states available now for each state I'm going to apply a random action and then I'm going to generate an imaginary sample from my candidate transition and at the same time I'm gonna generate a real word sample from the real-world transition okay so now I'm going to do the same thing for all these green dots now I get two sets of samples the first set of sample is the imagined samples generally from my transition dynamics and the second set of samples the second set of samples are the real-world samples January from the real-world transition now you probably can imagine what I'm gonna do I'm just gonna do the integral part bit metric right I'm going to assign a set of discrete discrim it is to look at the set of two sets of samples and it computes a number telling me how real the generated transitions from my candidate transition yes right so here you can think about the discrimination you can think about this criminal here as a classifier that looks a transition state action next state ripple and tell you how real it is right and we call this number model misfit in a sense that if this number is small it means that from the discriminators perspective the transition january from your canada model looks very real right so in other words it means that your canada model is likely going to be the real world model so with this in mind now i'm ready to introduce the concept the unified measure that captures the complexity of a lot of special problems so let's look at this matrix which we call the misfit matrix of which the size is the number of models times number of models right it could be really big but let's just look at a one particular entry that is indexed by this law model PR and the column model PC right so we're going to use a low model to generate a bunch of state actions the green dots that I showed in the previous sites right and now we're going to condition on the green dots and we're going to distinguish the candida model from the real-world model again using the set of discriminates right and we call this number at this number is basically The Misfits of the column model and the model rank is defined as the rank of this misfit matrix so intuitively if we have low rank it means that the models in my model class kind of look similar all right so as I said this is a giant mages right if your model classes has an infinite number of models these matrices it has infinite size how could it be that the rank is small right okay so in fact for a lot of interesting cases the rank is actually very small for instance for Lipschitz continuous Markov decision process where the transition dynamics is literatures continues we can show that the rank is no larger than the covering number of the underlying state space and for factor the MDP which is often used to model the data center cooling system we can show the rank is no larger than the exponential with respect to the in degree of the underlying network right in this example the in degree or our degree is two because this particular server is only going to affect the two servers the two nearest servers around it and for Palm DPS the rank is no larger than the number of hidden states which means that we can apply the to setting where we have very rich observation space right such as raw pixel image and for continuous control such as the linear quadratic regulator we can show that the rank is no larger than quadratic with respect to the state vector dimension so in all these cases yes your separation that you mentioned between model feeders yeah yes yes it's specific specifically the lower bound is constructed from this factor MVP right there are also other examples where we can show that the rank is small all right so with this rank we can actually design a very unified algorithm that works for all these settings achieve Sampo efficiency for all these problems and I'm not going to dive into the details of the algorithm but we can show that we can achieve sample complexity that looks like this where everything is polynomial with respect to the relevant parameters including quadratic dependency on the rank right and the key here is that there is no such polynomial dependency on the unique number of states in this problem this is exactly the supervisor a new type of generalization that we want we want to achieve near optimal behavior without brute force state visiting every possible States in the world right cool right so this summarize this part of the talk so so far we talked about how we leverage expert Eman straightens yes so did you have a constructor algorithm that let's use this yeah so does that one of them need to know are yes or no no means that you can just run the doubling trick to guest all right and just pay a little bit more in terms of log but you're never gonna come you're never gonna build that giant matrix in a computer react right but you just guess like the using the W schedule to guess the RAM all right so in the second part we talk about how we leverage the structures of the problem to design standpoint fish in the autism and specifically we build a unified measure to capture the complexities of the problems from a large family so now for future work I'm interested in extending reinforced learning to applying reinforced learning algorithms to real-world applications such as medical treatment designing personalized education system or robotics tasks such as autonomous driving or robotics assistance in disaster recovery now these kind of tasks are quite different from the video games or the simulation that we currently using in the deep reinforce learning community right for this kind of application we have a sense of urgency in a sense that we do not have that much time and loom to the random trine and arrow before we can propose something useful right so again for these tasks we need to care about sample efficiency so as I mentioned before leveraging expert demonstrations is a good way to to get us in significant improvement in sample efficiency in both theory and in practice so I would like to continue work on this in future especially to reduce the assumptions on the expert so we mentioned Amit interactive limitation learning but as you probably noticed we have to put a lot of burdens on the interactive expert imagine the expert is a human he pretty much have to pay attention during the training time because the learner gonna ask for feedback at the end time during training so what I want to do here is that to reduce the burden on the expert probably all the way to the study where we can just learn from watching right the dream of Lobo learning I wanted this Alice humanoid low bar to the backflip by just watching how Bruce Lee did it right but this means there will be no interaction because unfortunately Bruce Lee is not here anymore - there is no expert action I don't think Bruce Lee can figure out how much talk he applied on he's he's joints right to the back flip there will be no reward signal because human we can just do it very naturally and we just cannot call this setting as imitation learning from observation because we are just asking the learner to observe the expert perform some tasks now we recently looked into this setting from a theoretical perspective yes the setting is informations agent is going to be getting not the same sensory inputs different action space and probably different spaces do you know yeah so we recently looked into this but from much simplified study where the learner and the acts were operating in the same word in the same MVP for instance and we designed somehow isn't called average servile imitation observer for whatever server imitation learning in short unfortunate stands for fail but the general idea is that we want to learn a sequence of policies forward in time right and here I'm demonstrating a set of export trajectories where the ellipsoid represents the state distribution of the expert at a particular time step and start from the initial distribution I'm just gonna learn the first policy such that the resulting state distribution match to the expert state distribution in terms of minimizing the integral probability metric right then I'm gonna freeze the first one during the second then freeze the first tool and the third and all the way to the end and we can show that we can apply this to large-scale MVP and achieve the supervised learning type of generalization that we want meaning achieve near optimal behavior without proof all state visiting all the possible states in the world even though it's called a fail but it kind of worked well in practice so we apply them on some robotic simulation tasks where without any prior knowledge of the task without any prior knowledge of the robots and without any prior knowledge of the physics of the world by just learning from watching the expert we can learn policies such as moving this manipulator around to chasing the changing goal or move the manipulator around to push the object to diverse goes but really motivates me here is the potential applications of this method or this setting right wouldn't be really cool if we could put a robot in the kitchen and let it watch the chief do amazing stuff and learn how to do cook right well of course there are a lot of challenge because as you can see from the left to right there's a big gap right so the first thing that interests me is that how can we maybe boost the imitation learning algorithm by first initialize it using videos after all we have so many videos uploaded to YouTube pretty much every second right videos are probably recorded from different people from different house from different angles how can we figure out a common pattern that we can use can be used for the downstream imitation learning algorithm and the second challenge is that how can we interact with the expert when we imitating a human sometimes it's really hard especially for today's robot it would be really cool if we could figure out a way to interact with the expert ask in for instance can you teach more gently or can you teach more slowly and the second direction that I would like to explore is how can we do extremely fast learning from prior experience for tasks like medical treatment or robotics assistance in disaster recovery there is just not a much time and loom for us to do try random enero right we have to figure out something we have to propose something very fast and maybe I'm super biased but I don't think there exists a single reinforcement on the autism without any prior knowledge could achieve that level of sample efficiency for the task at that at this level of complexity but for us human we sometimes can do very well on a relatively new task like without that much random training error it's mainly because we have seen a lot of prior knowledge or prior experience about solving similar tasks so I would like to do the same thing for reinforcement li for instance given a new problem I would like to build a memory store that stores all the prior experience about solving maybe similar tasks so that I can quickly quarry the relevant prior experience for solving the day's tasks and the same time I want to do consistently refine my memory store using the latest experience that I have in terms of solving this new task I wanted this data structure this memory store to be efficient in a sense that I one of the search and refinement to be very fast one example toy example is that maybe one day I taught this humanoid robot to do you know right like jump forward couple months ago and learned how to do backward or maybe ten years ago and learn how to do forward already but today I saw this robot I asked it to stand up with no training no fears our training for instance right now if we have such renal system the low power can quickly look at his memory store and figured out relevant prior experience experience that relevant doing similar local motion tasks right and maybe we can do completely offline learning from the prior relevant experience so that we do not need any fresh samples to solve this standing up task so in summary we talk about how we leverage a mutation learning to quickly solve reinforcement learning problems and we also talk about how we can exploit the structures of the problems to design sample fish in their algorithms if we do not have any extra help right we also briefly mentioned how can we design a memory store so that we can do extremely fast learning from prior knowledge in an offline learning fashion in the early years of my PhD I also worked on something that could be useful for solving reinforcement in tasks for instance I worked on policy evaluation where the goal is to sort of figure out the performance of policy before even you deployed in the real world system this is very important for safety critical application because you need a good get a good sense of the quality of the policy before you deploy it I also worked on system identification where the goal is to figure on model such that you can do state estimation for instance we use it to estimate the height of Joan or estimating the weight of an object that is currently being transported by a robotics manipulator we use to estimate the velocity and the positions of a micro particles but just to using the sensor readings from microscopes for instance system ID is extremely important because pretty much for every problem is this big circle we have to get a good state estimation so that we can use it for the downstream in reinforcements right so in summary I'm excited about combining all these pieces together to bring reinforcements closer to real-world application that's it yeah so you can choose this F as long as you satisfy some realizability assumption that we define the paper in a sense that you're your discriminate or say should be rich enough it should be rich enough to capture the optimal value function of your model class so you have a model class you have a bunch of models and for each model it has its own optimal value function right so we assuming that the this criminal class is richer enough to contain the optimal value function of each model in your model class you have some relation between these discriminators are the value functions yeah so in some sense the very function is a very good test function to witness the difference between your model but you don't really need something that is more richer than that I think yeah I think this kind of thing is interesting because as you said if your function class is too small that you do not yeah much power but if you functions classes to not die it will kill your sample complexity okay yeah [Music] yeah I think a critical need will be offline because missiles can not not not in our case remember we have the ability to use the model right in some sense we are comparing two distributions when distribution is a real word distribution we do not have access to the likelihood but we actually have the likelihood to we have actually have access to the likelihood of the distribution that we picked right this is different from again yes it seems very important that you could guarantee that the policy would behave in certain situations so I think that for that product I think a policy evaluation is extremely important so in some times before I'd apply this policy for instead tested on the real word I should have a good sense of the performance of this posit so in none of this stuff that I talked about we we didn't consider safety constraint but I think it will be really cool if you could somehow integrate the safety constraint into this reinforced millennialism especially when you do exploration because exploration is kind of you know contradict to stop safety right because you want to explore you want to make a mistake in order to learn yeah yes if you try to do imitation learning consolations by having the age and quit feeling the video screen yeah but it does it sooner or later you'll run into the situation where there's something that's crucial for accomplishing the cusp it's missing from the video and for instance you know watch the chef cook something right while they set the temperature in the oven to some value but they'll be blocking the view of the opening in the page and there is nothing the agent can do it but I was wondering is there anything about your algorithms that will at least allow recognizing the situations that the agent is missing critical critical features not not in the current setup I think if you have some information may think without any other prior knowledge it's just hard right so an imitation learning there is in general I mean I'm assuming this this problem the local certainly have to occur in settings where you are just trying to teach station by having the first video but even a more standard mutation where in settings you can run into this issue because sensors of the agent a very different from the sensors of the human so in imitation during there is in general no we were in detecting some situations where I'm not aware of this situation at least the the the tasks that I looked into pretty much the expert in the learner they are operating in the same environment yeah and yeah so the learning from watching stuff is it's definitely very it's it's still like I still just starting to look into it this I have no idea so far I don't have a good answer to how to deal with partial so missing information necessary my my question this packs on to that one I know this is going back to the first part of the talk where you establish this gap between a magician having an RL and I ignition can give you so far is learning like yes I was actually very stunned when you claimed that the failure algorithm can also get those super worst kinds of carriages because I was thinking of exactly this failure case like like if there's even a little bit of partial observability yeah do we even know that imitation learning is a good algorithm to use or is something better so then you probably have to somehow the average or Vasya right or the others description of the task to do something to achieve something at least otherwise you can just you cannot just to learning by just watching but without with that critical information being missing but it could be you could combine whatever the partial information or partial observation you have and the reverse signal if you have access to the raw signal together to learn something this is basically the issue of unobserved confounder yeah so I can just flip a bit in the unobserved part that the demonstrator is like sort of giving the optimal policy assuming that flipped fit is one hmm but the agent has to act in a world where the flip it is zero so there's an observe confounding like all the demonstrations are collected can't give me any information about the actual world that I'm in right now it's almost like I can construct these kinds of degenerate examples where limitation learning cannot give me supervised learning the guarantees I will need to act in this world yeah yeah the supervisor is out that I showed is a is a much simpler policy yeah yeah it's a much it's an MDP and you just you have a set of demonstrations that consists of the states that visited by the expert there's no like patterning information or whatever is is there yes in distraction of using memory to leverage higher experience in social existing work that you see is promising so I think people start looking into leverage memory to solve a for instance supervised learning problem including some work that I did with John but I haven't see the result on the reinforcement setting all oh there are some memories memory related memory augmented reinforcement learning algorithm but they are not like sort of lifelong instance that they try for instance they trying to solve this maze problem they just build the memory for this particular maze and when you go to the next maze you're just gonna erase the whole memory and do it again in something the memory is the only live for this particular problem but we want to sort off the memory continuously grow but I won't store the memory for this maze the memory for that maze and for every maze that I visited if I see I'm amazed that it looks similar to the previous maze that I visited I can quickly extract the relevant experience so I sort of want to build a lifelong memory rather than a memory a short-term memory that is only gonna use for for this particular episode yeah what do those architectures like PNC and those Ural Turing machines yeah so they basically it's they basically maintains a matrix right it's a fixed size and every time you pretty much have to scan every row of the matrix to extract useful information and then we insert something you have to scan all the laws as well yeah and I also want to do search and refinement as fast as possible I don't want to spend lean your time on this right because when your memory getting bigger and the bigger linear time is just gonna kill you and also I don't want to pre assume the metric right so those those those structures you mentioned they assume in the Euclidean distance as the underlying metric for instance some of them [Music] [Applause] 