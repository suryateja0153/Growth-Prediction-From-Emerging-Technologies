 - I next have to invite myself up to give a brief chat about visual culture computation in the Digital Humanities lab, and what we've been doing over the last couple years. Almost everything I'm gonna show today in my talk is actually a team effort. It involves user experience design from Monica Ong Reed, it involves some incredibly sophisticated programming from Doug Duhaime from whom you'll hear later. It involves a lot of planning organization advice feedback from Cathy DeRose from whom you'll also hear later. So I just wanna preface that by saying what I'm gonna show you today is the result of a laboratory coming together of trying to build reusable for folks in the cultural heritage space. And so that's what I'm gonna focus on today, focus on the problem of visual culture computation and I think, if I'm not misremembering, I think this is actually a term that Catherine Haskins, who's in the audience today asked us to think about. My own background as well as that of Doug, and Cathy and others in the lab is actually really textural, so we have degrees in literature of some kind and coming to Yale for all of us was this really amazing experience because we started to think about the world-class physical holdings that included oil paintings and medieval manuscripts and sculptures and things which would never be reducible down to Unicode text. So all the things I had done on my post-doc of text mining Google Books, suddenly I'm in a world where I've got RGB pixel values, I've got 3D data from sculptures, it was really exciting challenge that Catherine and others, including our Dean of Humanities laid out for us in the lab, to be responsive to visual culture and so what I'm gonna show you today is gonna be some steps towards that. What I'm really gonna try to focus on is the problem of scale. So the notion of collection-scale analysis, collection-scale experience is really important to us. And that's because I think none of us want to interfere with the traditional art historical praxis of sitting in front of an amazing oil painting at YCBA or looking at archival photographs, that's how I was trained as an undergraduate in our history, that will always go on. What we're trying to enable with visual culture computation is methods and approaches which are impossible once you reach the scale of 10s or 100s of 1000s of objects. And in order to start this conversation I wanna give my very basic attempt at talking about some of the principles of machine vision. There's really no better person in this room than me, the PhD in Swedish literature to talk about machine vision and deep convolutional networks, I can't tell you how much I'm looking forward to this. But what I wanna start is with an interactive thing, is if you have a cell phone, usually people say, put away the cell phone, take out your Android, take out your iPhone and just go to your photo library. And doesn't matter if you use Android or you use Google Photos, if you have photos on your phone, doesn't matter, you'll either find a search box at the top or bottom left, sometimes you have to pull down. I'm just gonna recommend that you type the word cat. Cats are my favorite example for this and in my example I have 254 pictures of cats. These are generally my sister's cat, sometimes they're librarians cats here at Yale. And if you don't have a cat you could try dog, you could try kid, you could try car, bicycle, whatever you think you might take pictures of. No matter if you use Google Photos, if you use IOS, you're gonna find what you're looking for I hope. And you can ask then what's going on? And why am I seeing pictures that I have never labeled? This isn't Flickr, I haven't described these things, I haven't hired a processing archivist from the Beinecke to expertly describe this. So who decided these were cats and how did that happen? And the answer of course, is that all of us on our phones or in the cloud depending on how your architecture is, Google does it in the cloud, Apple does it on the phone. There's a convolutional neural network, which is captioning or describing everything you take a picture of. And this is really powerful, it means I don't have to go and label my pictures, in fact, I don't even think there is a way to label my pictures on my cell phone, right? You take a picture and you can then search for it. So this is very powerful, very exciting and all of this, although theorized many years or even decades previously really only hit the real world around 2012 when a convolutional neural network was able to solve a image description challenge in a way that was significantly better than other image captioning algorithms in 2011. And so you can see around 2012 that something changed and it meant that we started getting these systems, which trained on categories like cat and dog and chair and bagel and banana would be able to describe those things. And so this is what's running now in all of our cell phones whether we want it to or not. So this is really exciting, why don't we take these systems which can describe cat so effectively, which can find bagels and skateboards, why don't we take these machine vision systems from companies like Google or Amazon or Facebook and let them loose on our cultural heritage? There's truly nothing that could go wrong here, right? And so I just got back from a brief talk in Sydney in Australia and so I was interested in this idea of let's use machine vision to catalog Australia's visual heritage, so I went to an Australian history website and I took an image, the first one I saw, and I put it through probably the best machine vision API, the Google Cloud Vision API, this is absolutely, 100% gonna give me the right answer. So I gave it the image, and I said, could you describe what this is? And it told me that it was a fashion accessory, probably jewelry, with little less certainty possibly earrings, but maybe body jewelry or silver. So let's actually go to the source and figure out what these are. Well, these are leg manacles of course from Australia's convict history and actually the fact that Google would recognize them as such is not something that I invented, it's something that a curator in the Smithsonian told me based on some experience. You can only imagine the valence of this in an American context with African American slavery. So what happened, what went wrong? Well, really, this isn't Google's fault, and this is not Amazon's fault, it has to do with the kinds of usage and captions that off-the-shelf captioning systems optimize for. They generally optimize for the cats that I'm taking pictures of or the cars or the bicycles, contemporary images from 2019. Images which are captured on cell phones, so we tend to take single pictures of cute cats on our cell phones, that's what we use 'em for. And crucially I think monetizable use-cases. It wouldn't surprise me if Facebook has a recognizer if I upload a picture of myself drinking a Coke, they can probably sell me an ad of Pepsi next time I log in based on some brand recognition, that wouldn't surprise me at all. The training and the categories make all the difference and if there is bias, it's in the training categories and the captions and the data, not necessarily inherently in the linear algebra. This horrific justification of a piece of metal that is connected with a convict or with a slave history depending on which country you're in is not the only unexpected juxtaposition of visual culture and in fact artists like Fred Wilson working many years ago in the early '90s at the Baltimore Country Museum put together a terrific exposition that was called, Mining the Museum. And Mining the Museum is exactly what Yale professor in American studies and women's, gender, and sexuality studies Laura Wexler thought of when she saw this juxtaposition of convict shackles from Australia being described as jewelry. She thought of Fred Wilson's installation within Mining the Museum called, "Metalwork." And so I wanna acknowledge Laura's point and I was able to find an image of slave shackles here we're in the American context, the Southern context at the bottom center of this image juxtaposed by artist Fred Wilson with some of the other elements and material culture from Baltimore and Maryland broadly, the production of which was of course enabled by slave labor. So as we encounter these unexpected or horrific miscaptions in our convolutional neural networks, it is I think as we try to fix that, worthwhile thinking about what such unexpected juxtapositions might trigger in our own minds and I think that that may be what Fred Wilson was doing in an analog way back in the early '90s. So as we turn away from some brief notions of convolutional neural networks in how they see and how they describe the world, I wanted to then move on to three experiments with visual culture computation that try to get around this problem, that try to use the power of what industry can give us whether from Amazon or Facebook or Google. Take these pre-trained networks and in some way shift them and distort them to serve the needs in cultural heritage institutions, perhaps natural science institutions, what are the ways that we can take the advancements that are allowing me to find all the cats I've ever taken pictures of, but not miscategorize the 19th century or describe something in a way that's very inappropriate. So we're gonna start with the first one, which is specifically can these pre-trained captioning networks be useful without their categories? And in order to answer that question we took a collection which is actually split between the Yale Beinecke Rare Book and Manuscript Library and the National Portrait Gallery in the Smithsonian. The majority of the collection physically is in the Beinecke and then there's some very important Mathew Brady negatives that are at the National Portrait Gallery. It's one of the largest private collections of 19th century photography from the 1860s to the 1890s, the Civil War to the Gilded Age. And it's an incredibly rich and powerful collection to look through, it's famous for its depictions of Lincoln. There's a problem though, it doesn't include any cats. And as we know these convolutional neural networks are only good at finding cats. So we have this mismatch between our source data and the networks which are really good at describing the world. And so the impetus behind trying to fix that problem, behind trying to find a way that these networks could still be useful, but in a world where they weren't trying to describe things too explicitly has actually resulted in this demonstration here, so what I have here on the screen are just a couple images from the Meserve-Kunhardt Collection. There actually isn't too much import in which ones are on the screen right now, this is a random sample from our Civil War collection. What maybe is a little bit more interesting is the ability to take this website and do something with my mouse, so I'm gonna move my mouse over this dude here on the left. And when I move my mouse over him, I'm gonna get images which are visually similar to him and when I move my mouse over this guy I'm gonna get images which are visually similar to that one, move my mouse over a guy in an oval, I'm gonna get images also in that same oval vignette. Let's go to another image, I dunno if anybody likes boats, we got some boats, like houses, we got houses. Now in some cases these are actual literal reprints and that's okay, it's a large collection, reprints were made for a long period of time. In some cases we know what these things are, we have captions and this happens to be Lincoln's house in Springfield. Sometimes we know who this person was, we know it in one image on the bottom right, but not on the top left. So those are reprints, but what I think is more interesting than the reprints is the notion of the pictorial conventions of depiction here. So what I can do by surfing through this website is look at how people were depicted in various ways based on visual similarity. And the visual similarity actually has a lot to do with the convolutional neural networks that we were talking about before and this project is based on pre-trained convolutional neural networks which are designed to find skateboards, bagels, and cats. But in this artificial vision system which starts with very simple discoveries about maybe lines or small shapes and evolves all the way into this is a cat or this is a dog, we step back from that final layer, and we take the penultimate, the semi-final layer in the network, which gives us about 2,048 abstract dimensions of sight. There's really no way to describe these 2,048 ways of seeing except that they are crucial in distinguishing cats from skateboards. In that sense it kind of solves the futurization problem for us, we get a pre-trained model, we shift it to do something else, we have this relatively high dimensional space, 2,048 ways of seeing, so think about one dimension of x, y, z, and then 2,045 more dimensions that I don't have letters for. We can do math in that high dimensional space. We can use a proximus nearest neighbors algorithm to find the images which are the closest to a given image that I'm moving my mouse over in these other dimensions, and that's what results in all these other pages of writing showing up, perhaps because there's one dimension which was responding to a white background and another dimension which was responding to squiggles, put that together you get basically all the archival handwritten documents in the collection. So we are basically taking pre-trained networks, but intercepting them and using them to surface other images of women in sort of interesting ways. What I think this tool really gives you is the kind of way to surf through, by starting with one image and discovering conventions of portraiture in a time when exposure times were long, people took a good deal of time getting dressed up, sometimes their bodies were even stabilized by metal stands, so a lot of thought went into these conventions of depiction. All right, so that is neural neighbors and my favorite example from this particular data set which George Miles helped bring to the Beinecke, I call it the pugilist example, so I've moved my mouse over a boxer in the bottom and I get all the other boxers, and what you'll notice here is that one intuition is that the network is more responsive to line than it is to tone, and that's one theory. And this might explain why it doesn't matter if the boxer is facing left or right, nor does it matter if the boxer is Caucasian or African American, now there may or may not be an explicit category in the Beinecke's own expert cataloging of the Meserve-Kunhardt Collection of pugilist, but it doesn't really matter because we've built a virtual subject category by relying on the conventions or portraiture in this particular collection. So that's Neural Neighbors and that's open-source software that people can use, in fact we're pushing this tool forward to help solve some problems that might be familiar for those who know about the Farrows Collection of photography and we're also pushing that in a couple different directions. We wanna make this available for anybody to use, it's all open science, we didn't invent these algorithms we just sort of put it together. And we also wanted to go further, and so the second technique I'm gonna talk to you about actually builds on the idea of I have one image and I wanna see everything which is similar to it in my collection. Using that intuition of using pre-trained networks, but stepping back from the final category and going to a semi-final layer which is more abstract, can we use that more abstract space as a way of organizing the entire collection en mas. So this gets to what I was talking about earlier, the collection-scale analysis, or collection-scale visualization. And so we have another software product out there called PixPlot, and it actually uses some of the same math that we just talked about, pre-trained networks, discard the layers which are optimized for 2019, take the penultimate layer, and then we have the same dimensionality reduction problem. We need to somehow visualize 2,048 dimensions in our web browser and that's really tough to do with two-dimensional screens, so dimensionality reduction is basically like the idea of taking a flower and pressing it between the leaves of a book. You as a human are able to figure out, how best can I crush this flower and still preserve its distinctive shape. It's pretty easy for a human to do that from three dimensions to two dimensions. To do it from 2,048 dimensions down to two dimensions takes more math, and there are a whole bunch of techniques, something as simple as principle component analysis, something more advanced like t Stochastic Neighbor Embedding or tSNE and then a really cool, new algorithm called UMAP, Uniform Manifold Approximation and Projection, they're always a minimizing loss, minimizing what you're destroying by crushing that flower from three dimensions to two dimensions. And now we have a two-dimensional space if we're successful in this reduction and we somehow now have to show you a lot of images all at the same time. I'll show you how this works here. And for this demo I thought maybe with the permission of Manuel to show some images from the Yale Center for British Art. So we've taken about 31,000 images as we know the Yale Center for British Art was a pioneer in putting non-copyright encumbered images online for anybody to work with and so we benefit from that generosity and I'm able to take all of those images they gave me and throw them on the floor. But they haven't landed randomly. They've landed in such a way that images that share the same visual characteristics have landed on the floor at the same place. So I'm sure there's a technical word for these, I call them the caricatures of dandies. These are all the well-dressed men in very nice clothing down here at the bottom left of my visualization. This visualization is in fact the result of a UMAP production of 2,048 dimensions into two dimensions. And what I'm doing here is showing this reduction and hopefully UMAP has been effective at keeping images that look like each other near each other in this much reduced dimensional space. It isn't just these guys, we also have, anybody here like animals, let's go to the zoo, here's the zoo, and it's everybody from horses to camels and things like that on my screen. There are some other really interesting things which show characteristics of YCBA data. All the vignetted sketches. My favorite one may be since we have some natural scientists in the room, is that there's this really cool cluster of botanical illustration. I'm sure there's a category for this in the explicit metadata at YCBA, I just think it's kinda cool that this thing just all landed in the same place. And then finally, there's some cool stuff down here. The Yale Center for British Art of course justifiably considers frames to be artwork in and of themself, so we have some really terrific frames here. What I love about his is these images of frames are close to this other cluster of images where the frame is too big, really in my opinion, so it's Frameville, right, we're in Frameville. But we have this enormous visualization. People sometimes use metaphors of this looks like a constellation, it looks like an archipelago of islands and things like that, but this is what anybody can do if you load your pictures into PixPlot. We recommend at least 5 to 10,000 images. We are pushing PixPlot forward in some interesting ways. It is primarily authored by Doug Duhaime and so I do want to acknowledge his work in programming this in WebGL which is a way of programming websites that's more akin to writing a 3D video than using HTML tags. That's the reason we're able to get really high performance out of this. But what I wanna do is to talk about another example from my own research, I'm gonna give you about 30,000 pictures from the Southern Swedish city of Lund in Scandinavia and so we're gonna have these images which were taken by two photographers just to show you how this works on photography. So we have these unexpected clusters. These are all sort of from the early 20th century. Turns out people liked taking pictures of kids in chairs in the early 20th century in Sweden. These are all these kids in chairs. I had no idea this was a genre, and it's nearby the cluster of standing children, so I guess as a parent you could decide do I want my kid standing up for their portrait or do I want them in a chair? There's other examples here, turns out marriages cohere pretty well, these are wedding pictures, right? There's women in white gowns. And there's also something kind of cool I wanted to show you which is we've got pictures of buildings here in the center of my screen, pictures of parks down here, and what's between buildings and parks, turns out to be buildings in parks. So there was originally probably one dimension which was responding to the grass and trees in parks and there was another dimension that was responding to the linearity, the roof lines, the walls of houses or buildings and UMAP, a dimensionality reduction algorithm was able to somehow collapse these two lines into a much smaller space that still preserves the spectrum. I wanna show you a really early alpha version of PixPlot which accomplishes something even more exciting. In this demo is going to be showing you the interrelationship between human metadata and what the computer sees. So of course when I was dealing with the Yale Center for British Art images, I was not taking into account who painted that or who drew that. And what we're doing with the new version of PixPlot is being able to pull these explicit metadata edges like who drew this, who wrote this, who painted it, what year was it, and put that into conversation with some of our visualization, so the first thing I'll show you is that we do have the ability to think about these images in a variety of dimensionality reductions, this is just the most banal, this is just all the images one by one by one, that's pretty cool. But before we were showing you a couple different dimensionality reductions, now I can show you the animation between them. So we might have for example these images, which are about 30,000 orienting themselves into a t-SNE reduction, which is just one way of organizing them. We can do that with another dimension added in, and you're seeing about 30,000 objects move all at the same time on your screen in WebGL. T-SNE has managed to orient these down in a way that here's the neighborhood of those kids in chairs, so we still have that clustering, right? Each algorithm will put the stuff in different parts of your screen and there is a stochastic or random aspect to each visualization, so we don't want people to take these as deterministic, we just want them to take them as suggestive, and in fact that's one of the reasons we've moved towards this animation engine. So that people can see that all of these arrangements are really contingent, and there are different ways of thinking about the data that are all legitimate. This is UMAP, one of our most exciting visualizations 'cause it certainly clusters things in a certain way, but I think what I wanted to show you here was this interrelationship between the metadata, who took this picture, and the algorithm, which sees the notion of children in chairs. So what I can do with this version of PixPlot is I can say turn off a particular photographer. Only show me pictures by Per Bagge or only show me pictures by Ida Ekelund. Now when I do that most of the images of children in chairs disappear. There's still this cluster of children in chairs, but if I turn off Per Bagge, and only show you Ida Ekelund then what I get is a much reduced cluster whereas if I switched to Per Bagge, here indeed are all the kids in chairs. And so what this tells you is there's a substantial difference between the output of these two photographers who worked in the same building and trained with the same woman in Lund, and they were active at the same time, but this woman did not shoot babies in chairs, that wasn't her bag. She actually shot a lot of images of women in their 20s and 30s with very short hair and in the American context you would think of these people as flappers, so there's a separate cluster of flappers that only Ida Ekelund took pictures of. That's one of the things that we can discover when we put machine vision in conversation with the explicit metadata edges of who took this picture, and when did they take it. Another exciting thing about the future of PixPlot is that we want to turn this into a tool for curators. Pretend we were going to that zoo area we were looking at before, and you say, hey, I'm really interested in horses, so I'm gonna trust the algorithm to cluster all the animals together 'cause it seems to do a pretty good job of that, but I don't want any camels in my collection 'cause I'm preparing an exhibit on horses. And so what we can do is let you have a kind of lasso, remember you're in Photoshop or Paint and you're trying draw a complex shape around something, so what we'll allow you to do is draw your own circle or your own lasso shape around just the horses that you like, you'll be able to Shift + select, deselect. And then what do you do with that stuff, well you might download it as a CSV file, you might bounce it into a PowerPoint, you might upload it to Omeka, you might just have a list of things you need to take a closer look at. So being able to put the human intuition into a conversation with a way that the machine vision is organizing these images we think is really exciting. We wanna work with human expertise not against it. I think that's my main takeaway. I have five minutes left and now I wanna show you something kind of edgy. I like to think of this as if, have you ever been working on a book manuscript or an article or a student evaluation? And you've been working on this for like every day this week, your book or your article or whatever, maybe you're reorganizing your closet. And it's all you can do all day, and when you go to bed there's this horrible thing when you start dreaming about the chapter you're trying to write during the day, it will not leave you. And we were thinking that we're making some of this Nvidia hardware and these convolutional neural network software work really hard all day. We run these jobs for seven or eight days in a row, multiple GPUS in the cube over there. And we thought well, what happens when these neural networks go off the shift when it's five p.m., what do they dream about? Do they start hallucinating about all these portraits they've been seeing during the day? And so what we wanted to do was explore the notion of generative models. And generative models are of course a form of unsupervised machine learning and the notion is that given enough ground truth, given enough training data of things as they are in the world, algorithms might be able to come to an understanding of a probability distribution of the characters in the words of a book or the pixel values in a 19th century portrait. And we might be able to sample from these networks and produce new objects which have never existed in the world before. If you read in the Times or the Washington Post articles about deepfakes, that's actually what's going on here, they're generative models. And so what I'm gonna show you today, a notion of generative adversarial networks. Now the cool thing about generative adversarial networks is actually two networks working in tandem or more accurately working adversarial to one another. Generative adversarial networks consist of two networks, an art forger who is shown images of 10s of 1000s of objects and does its best to invent or hallucinate new things. There's a joke here about then selling the forged object to the Getty here, which I won't make 'cause we have somebody in the Getty here. The detective though is working for the art museum and he or she is looking at all these objects and trying to discern which one actually existed in the real world and which ones were dreamt up by the forger. And what's interesting about this interrelation is that each of these two neural networks pitted against each other in an adversarial relationship learns from its mistakes. If the forger does not get an image through the detective then it says, well I screwed up and I gotta do better. Let me go back to the ground truth and try to learn a better prior, a better probability distribution of RGB pixels or characters. If the detective accidentally lets a fake Greek sculpture through it then says I should never do that again, let me revisit my ideas about what is real, and what is fake, and all this happens 10s of 1000s times a second and we run these for days and days and days. So this is what happens when we ran a generative adversarial network on the Meserve-Kunhardt Collection of faces from this 19th century, Civil War, Gilded Age collection. What you're seeing is the output of the generator fighting against the discriminator trying to arrive at some kind of consensus on what a 19th century face looks like. And now some people when they see this they think it is uncanny, some people think it's like a horror movie or something, but you're seeing in real time, the result of two neural networks fighting each other over the definition of a face. It doesn't understand what a face is. But that's how we interpret as humans. You'll sometimes see faces that are split in half. I have another example here I'll show you, this is the same algorithm running those Swedish pictures I showed you earlier. And these are about 30,000 faces. And so you'll see these two networks fighting each other and what you'll find there on the left if you look on the right side of the screen I have some samples drawn from the results. Early on in the process, what we call an early epoch, you'll see static, I mean this is no better than just random pixels. And that's because the generator at the very beginning doesn't know what it's doing. It's like, well, have this page of static and see if that works. Of course the discriminator, although not expert yet is good enough to determine this is no good. The middle picture is actually I think an example of a good example in the sense that this woman never lived, but if you looked at this image, this face, you might think ah, it's just a lossy JPEG face of a Swedish woman, not a problem. And then on the far right we have an example of maybe overcompensation or some kind of mistake the generator made which said, I got a face through with two eyes and one nose, let me try three eyes and two noses and see if that works. I hope to God the discriminator realized that was not a real human, but you never know. So what can we learn from these, well not much. These are kind of in the realm of art, but I think that it is interesting to compare ho algorithms come to understand probability distributions of RGB pixel values when we ourselves spend so much time thinking and looking at these same images. Looking at the uncanny or horrific is sometimes a way of casting new light on the work we do with the authentic. And I do think there is possibly some role here for curators and other professionals to work with these types of things at least as ways of understanding some of the aspects of the possibilities of machine vision when it goes off work and it starts to dream. Promising areas for the future, I think that we do need to appropriate algorithms from private industry, I think we need to build up expertise within GLAM for machine vision. I think private partnerships are also really crucial but I'd love for if Google came knocking I'd love for people on Yale's campus to bring not only their existing expertise, but also some basic knowledge of what these algorithms can do. I think combining collections across institutions for example reassembling Meserve-Kunhardt between NPG and Beinecke. Placing human curatorial and disciplinary knowledge as equal partners to machine vision. We're starting to do some baby steps towards that with being able to show metadata facets in our visualizations, rearrange to suggest multitude and complexity rather than deterministic simplicity and let you go in with that lasso and select things that are meaningful to you. And finally and most crucially, if you listen to one thing I've said today, it's that institutions such as Yale and Smithsonian are actually uniquely suited to produce captioning models, descriptive models that do match the world of bumblebees, that do match the 1860s because we have the excellent metadata and we have the training data and all the metadata of the raster pixels of our own collections, rather than getting upset that something is mischaracterized in an image, we can retrain the last layer in these networks and produce our own 19th century models of American visual culture, I think that's very promising. So with that let me end. We'll save questions for the lunch. 