 you [Music] is that I'm here from QUT Queensland University of Technology down in Australia under the supervision of dr. Frederick Mayer and my work mostly looks on 3d inference from monocular vision or as I like to say I solve hard problems badly all right lots of ambiguities in there lots of things you can't do perfectly just with geometry but my main philosophy is if you're not using computer vision so it computer graphics techniques in computer vision then you're trying to learn too much that's it there are ambiguities you I do feel in computer vision you do need to learn some things right but that doesn't mean you have to learn everything they were talking about a couple of papers that look explain most of the theory in terms of human pose estimation because that's a bit easier to get the head around it's a bit of a toy problem as we'll see at least the part that I focus on it's a bit of a toe problem and towards the end we'll look at some single view reconstruction stuff as well I would prefer you guys if you've got questions to stick up your hands and ask them throughout rather than waiting until the very end I'm happy to address them and I promise if I'm just about to get to the answer you know just tell you to wait for a slide or two okay so probably human pose estimation I'm hoping we all look familiar does anyone not familiar with human pose estimation well roughly the idea is given an image right we're gonna try and locate where someone's joints are in either two dimensions or three dimensions so we'd be given this image without the green overlay and the challenge would be to find the green overlay all right this comes in a couple of variations right number one is the 2d version right so given image try and infer the joints in two-dimensional space in terms of datasets there are a fair few out there they're normally human annotated all right so we set up a whole bunch of images and generally Amazon Mechanical Turk or or some other service will get people who will work for very little money to do fairly dull tasks of annotating this data per example it's kind of expensive depending on your definition of expensive it might take sort of 20 cents per frame all right normally we have very very images so not many cases the same person the images themselves generally come from a source like like Flickr or something where you get a large large variety of different images all right normally all you have is the image so you don't have the camera parameters all right what sometimes good sometimes bad well generally bad you can always ignore the other version is the 3d version so give an image try and infer it in three dimensions right these are normally done with motion capture systems all right setting those systems up is quite expensive once you have them set up though you can capture frames it very very quickly you can just recall it 60 frames per second and record 10 minutes and you've got a very large number of frames those frames don't have very much variety between them there all right and that can be trouble this image comes from a data set called human 3.6 million all right there's three and a half million frames in there but they're all taken in the same room and there's only I think 10 or 12 different subjects right they're all wearing the same clothes when they do it they've all got these little motion capture dots all over them so it's very easy to train a very good Network under those situations that just relies on finding those dots my own obvious that's not very helpful outside of the motion capture environment and if you already have the motion capture environment you don't need a network to learn these things all right so pros and cons to both approaches the other thing some people look at and what I focus on is this like 2d to 3d lifting module so if you have a really good 2d human pose inference system all right and you can take that to the input and try and infer what it looks like in three dimensions all right we call that a 2d to three lifting module the idea is if we train a network on datasets that have been collected under a very varied environment then combine it without lifting module then we'll have a 3d pose estimation system which is good in the wild I'm not going to focus so much on that on the end-to-end problem I just look at that matte middle section which again is a bit of a toy problem it's not incredible it does depend on having a good to Depot's inference at the front but for the purposes of research that's what we've I've decided to focus on here okay so how do we do this well there are two methods the one I'll compare against right is this paper again in 2017 my mutton is at Oh at the time a lot of people were doing some fancy stuff with this lifting and these guys pretty much came out and said hey guys if we just take approaches that work really well in other areas of deep learning and just put them all into one network then we get pretty good results no I've said oak linear layers they took that's normally to well you either drop out all right they took skip connections and they just like plonk them all into one network trained it so to depose in three depots out and they beat most of the state-of-the-art results at the time right now again that's for this particular dataset right it doesn't generalize well to different camera poses and in the data set itself there's only four different camera poses so it does very much over tuned to those camera positions and those specific cameras all right but they just say that this should be our baseline all right so so one thing is like depth like overall scale is ambiguous but if you know the camera is always pointing at a certain angle down and you can see their feet then you get quite good at inferring scale right and that's a great baseline I think it's a fantastic paper I think it's people are over complicating things and doing fancy things and they don't get better than that then they need to sort of reevaluate why they're doing this all right but that was me it's perfect I have a few issues in principle with this approach if you like all right the question is do we need millions of parameters for this 2d to 3d lifting thing essentially we're just trying to infer depth Oviatt with ambiguities and n self occlusions right and I'm very happy learning a little bit but do we need millions of parameters right if we just disappear back to back structure this linear to linear mapping right in the middle that's got something like a million params right and there's this whole blocks repeated twice so we end up with about four million parameters over this whole thing right do we need four million parameters just to infer depth and you can take the number of parameters down and under this architecture you see the results do actually decrease all right so I'd say under this is architecture yes you kind of do need that many parameters all right but in general can we design the architecture better so that we don't need those four million parameters all right next question is can we use simple computer graphics techniques to reduce how much we need to learn and to encourage consistency we have really good models of how light works right assuming you're not passing a supermassive black hole all right light travels in straight lines right and so we we would like to use the fact that we know how that works rather than trying to learn from scratch for each computer vision problem all right and lastly can we devise a model which takes advantage of camera parameters if we know all right I work in a robotics lab no it's fairly easy for me to convince the roboticists that you'll be able to know the camera parameters in advance not all circumstances but a lot of the time you will know those camera parameters all right if we train a model and we want to switch cameras do we have to train that model from scratch all right or can we just switch some camera rameters in our model and have it work so what's going to ideas well question yes yes we just dealt with with that that's it presumably I never tried that but yeah I mean it never actually used the images themselves it just used the 2d and 3d coordinates so you can even like virtually seem like that if you have 3d in one pose you can always simulate a new pose sorry new camera pose because you know the camera parameters that there are in the data set the occluded joints are labeled because there's a system of cameras all around so it doesn't just come from that image so that the two D joints came from a variety of sources one was just the ground truth projected actually that the real truth 2d points the other one was a network that they trained on a different data set all right so there it's doing its best to infer the occluded joints but obviously it's hard if you can't see them and then a third version sort of took that model and fine-tuned it on this data set so again still has that occlusion issue right but it gets very good at detecting little motion capture dots so to discuss a few ideas about how we we can sort of do this right idea zero definitely not a paper in this we'll just discuss it very quickly because it will help the other ideas the idea as well if we know where they are the points are in 2d let's just hypothesize that's as fine a 3d pose which is consistent with that to knee pose alright and let's just optimize however we want right thing you can do with it at least squares one step bam--it's it's convex all right the idea is we just start anywhere and we move our 3d pose until it lines up exactly with our 2d posts all right so ever on the Left we actually have both both the ground truth and the I pulled our solution right into D from the cameras perspective all right but if we look at it from a different angle over on the right we see that they're really not the same books right this is never really going to work it just take a bunch of points and move them until they line up with array and rating from the camera we're not taking into account feasibility what human droids can do limb length consistencies angles or lethal effects right and the slightly more complicated version might try and hard code all of those constraints give yourself a skeleton model know how long the limbs are know that elbows can't bend backwards and these sorts of things but that gets fairly tricky to enumerate all possible things and you'd find that even once you've done that you'll probably have some possible poses which just don't look good it's very difficult to define that idea what makes a pose feasible and that's where I think we should start learning things in this current model Dez there's no burning all right a feather to depose you just plug it in and you'll get a 3d pose all right but that projection stuff is all very very well understood very simple not doing any crazy projection we're just you know dividing by the depths with some camera parameters in there all right so idea one just says well theoretically let's try and search only the feasible poses for the moment I'm gonna learn how we get that definition of feasible poses but pretend we have all of these all possible a 3d poses in that sort of slightly darker area all right subsets of that a manifold inside that we're going to define us as feasible poses so points on that blue line are feasible then for a given image or a given set of 2d observations we'll end up with another set of points or another set of poses which are consistent with our observation they're the ones that lie up and I know obviously not every one of those consistent poses will be feasible just like not every feasible pose will be consistent with the given observation all right but hopefully there'll be some intersection given where those little orange dots and we'll try and find those orange dots now there's still going to be a few there are still going to be ambiguities from a single image we can't do away with those entirely all right for the my word it's going to say let's let's hope that getting to one of those orange dots is good enough it's a feasible pose it agrees entirely with our observation it's about as good as we can do with a single image right if we get a different observation right that'll can correspond to a different set of consistent poses hence the intersection will be different right so the idea is we train up or we learn this feasible set of poses once alright and then for each different observation and we look for a different intersection alright at inference time we start with some hypothesis like this this black dot right and then we just roll our ball down the hill right roll a ball towards the red line and we do that with any optimization scheme we want right if it turns out that we can make it convex then we use some convex optimizer as it is we just use normal gradient descent methods right definition position will get us to the different point right this can be handy if you run some sorta like particle filter you can get an idea of all possible consistent and feasible points and if you pump down a thousand points you'll get a bunch at one point a bunch of the other right that can sort of represent some measure of uncertainty or at least the distribution of the intersection of feasible and consistent choices so I left that big question the beginning how do we define that feasible set well we trained a generative adversarial Network again anyone not know what again is you know there's lots and lots and lots and lots of literature on Ganz every conference you go through there's 17 you gain architectures we use the really simple was a stone again nothing too crazy for training we use just normal as 3d poses so we wrote had all three new poses for the hips with pointing forwards they were all the same height and the vertical axis was the vertical axis all right so we trained and the idea is that once you've trained that again if you trained it well the generator will always generate feasible poses so long as it inputs not too far away from the normal distribution under which you train we also have a discriminator at the end of that which measures or give some measure of how feasible a given pose is we experimented with augmenting our loss with that discriminated loss as well and we found it didn't really need to be there after we trained that up we used to stand a gradient based optimize this to minimize consumer loss I think we used yes yes but just off the shelf didn't do any tweaking just plug it that's a quick video here which quite work out how to do nicely but boom look at that integration so that's the training again right we've got a bunch of knobs there like AG an input as we move those around will get a different generated pose and the idea is that pose in 3ds should be look fairly feasible it's a weird skeleton it looks like it's got a very high bellybutton or something but that's that's just how I chose to visualize these there's no sort of center joint alright seating was hard ganz have this thing called sort of mode collapse and you can get to see the poses but they're not amazing at runtime alright let's pause there for a second we have some initial observation that's this guy up here and we just started with like a random Gann input so we have some pose which has nothing to do with the image right and then we tweak those knobs that we had in the previous section until we match up with what we see over on the right you see a different angle in 3d no you just go and kill you sort of align and in this case we took up enough iterations until it just said it was converged but we didn't really worry about optimizing that too much uh so when this one pushing the limits of a memory but I think we had 16 inputs for our best model so sitting again and then like a hundred hidden layers and then number of joints out I'm sorry yes you operator the sixteen variables plus a rotation about the z axis plus and global offset plus a scale parameter I think yes how does this compare to parametric like a hard-coded didn't actually compare I would suggest probably not amazingly better because we know lots about humans and there's lots of work that's been going into those handcrafted models I guess the idea here is let's see if we want to transfer this to a different domain all we had was a data set then that's all we need ganz for the crease is my first year of a PhD and they sounded pretty cool I mean it fits fitted exactly what I wanted it's something that generates only feasible things from my understanding VA is tend to sort of smush things together they sort of get averages of things whereas scans are better at producing sort of distinctly separate yes no I mean like I'm not saying this is the only way of doing it I like the idea of EA's a lot but Gant's a cool I had to jump on that bandwagon before I realized that everyone's jumping on an end and no one pays any attention to them when conferences gans are good but before seeing them confidence much falling into local minimis we did see sort of mode collapse which you'll see in a second for things which are less common in the oh wait we've already skipped it I bet where do we do the seating one yeah that's the head of the ceiling ah no sorry he sits down at some point he'll come so for seeding stuff that there's a few in the data set but there's much more standing up walking around so again just as decided that was hard and decided not to try and play very nicely we did just go for sort of simplicity over anything else weeks Wednesday taking optimized off-the-shelf plug it in see what happens I'm not trying to suggest this is an exhaustive study of all the possible things you can do with this yes that's mostly the depth is you and again if it's turning right to the side then sometimes he flicks around backwards and forwards because there's just a high degree of variability that again if you took higher level 2d features you could probably sort of learn which way they're facing but when you extract just the points you lose that orientation information when they're exactly sidon here's the mode collapse this is terrible right sitting down there it doesn't happen very much in the training set so we see a really struggling over on the right there - no not here so here we just do them I think I might have used like initialization thing but the optimizer jumps around a fair bit so there's no temporal smoothing here we have some temporal smoothing right where we just sort of apply an additional loss which says things can't move very much between firms all right you see if you if you wait that too heavily free tends to run in a circle that's just a smaller radius which is a bit unsatisfying if you do it just a moral amount then you get sort of smooth that but it's still not very satisfying here's the glide over the ground right not on the yum episode that could work as well all right I really love this idea right as a sort of first there stab we came up and all those things are correct I love the fact that your hard-coding how much smoothing there should be right we didn't play around with trying to generate sequences and that got hard getting a training again to generate a feasible pose we never sort of saw anything pop out that really looked feasible they just sort of moved from different feasible poses to other feasible poses but not as a feasible sequence but the thing that really kept me awake at night yeah the thing that absolutely terrified me was what happens if our blue thing looks like this right that could be a perfectly generated generator output space right and that every single pose that's feasible could be in that blue a thing and every which one of those blue points could be feasible but when you try and use this any optimization thing unless you're very lucky you're going to fail all right if you start off on either of those positions right you're never gonna get to a consistent pose all right and now we didn't really see a huge amount of evidence of this but it still terrified me all right and I want to move away from Ganz so we decided idea number two right idea number two was let's try to learn a loss function well obviously we can't have the the reconstruction loss we had us minimize the reconstruction loss we can't move our 3d points towards the real 3d points at inference time because we don't have the real 3d points but we had a function that approximated that reconstruction loss right then we could just minimize that function and if that function is sufficiently close to the original function all right then we'll be all good so that idea would be his his our loss function with our our desired point right here we want to get to here's our original loss function know or a true loss function which which cheats which uses information we don't have all right and there's maybe out learned loss function all right well let's take those two and let's look at the difference add a bunch of points and let's try and pull those guys together and trace this blue curve would be a parameterised loss function all right which is which can know the 2d joints but it won't know the 3d droids other times that's a problem that's a little bit harder than we need alright that's overly constrain the end of the day we don't want a function which is exactly the same as the true loss function all right what we want is a function which behaves similarly to the true loss function under optimization alright so if we have that function and that global minima we look at where our ball rolls all right we end up over there all right we want it to be over here so let's look at the difference between where we end up versus where we wanted to get to and then we also add in a bunch of other ones to encourage us to get there fast all right and then let's tweak our loss function so that next time we run it with this input we'll get closer or the optimization process will get us closer to where we wanted it all right that's the difference between learning a loss function and learning a loss function fit for purpose all right so long as that our minima are co-located and our surface is such that under gradient descent or whatever optimize we use we end up in the same place all right then where happened it doesn't matter if we're off by an offset or you know it's slightly different gradients here in their question we don't enforce it so much but we penalize ones that don't we just do gradient descent we literally sort of yes but then we'll take I mean we do that that's one example in our training and we iterate over note that's what we're gonna do right and yes that that specifically is not my work but the choice of energy functions is just to summarize the ideas idea zero was let's just try and minimize the reprojection loss oh it's some loss function I'll get to the specifics but yes it's something that combines feasibility with consistency or really it's just a function that we hope has co-located minima with the true assumption not even that one that behaves similarly under optimization so if you have momentum and there's a few you know things that momentum get passed we don't really care this is summarized I love this slide but I figured I should have one slide with mass unit option zero says let's just minimize the reproduction loss doesn't really work doesn't take into account any sort of feasibility idea one said well let's only optimize the feasible poses yes so tilde means an inferred something b3 is the three-dimensional one picture was that the two-dimensional one and a star is an optimal so idea one said let's train again in the first stage and then in a completely separate stage let's just optimize that surface you might pull them being that that surface hasn't been optimized for optimization it's been optimized for feasibility right so it might be a hideous surface to optimize I do too says let's do them at the same time all right let's define our p3 as some minimum over the possible poses all right where our energy function is from calling e right he's just trained such that that works well I think the pictures do a better job than that slide but if you want to ride that formally you can between the 2d and the loss function is is the reconstruction once to the different LT loss of the three-dimensional poisons oh sorry yeah probably should be some squares in there okay sure energy function if I call energy is that yeah it's kind of trying to learn something similar to the loss function the truth with the label truth yes so there's a picture that kind of summarizes the idea this is specifically unrolled optimization so I said earlier you can um you can construct your energy function to be convex and then solve it the optimization in one step or something we chose to go with unrolled optimization to this F here represents ah that was definitely wrong sorry where are we oh yeah this F just represents a step of gradient descent right you can throw in momentum if you want if you really want to code up a an atom version you could do that it's just some iterative update step which is itself differentiable alright we optimize our energy with respect to the input whether the 3d input Y which also knows about the 2d input X this is like our to depose is our 3d pose we plug them both into this this big red box it's the energy function right we get some scalar output we use some gradient sent business to optimize our Y with respect to that loss function right and we end up with a whole bunch or sequence of inferred poses yeah and then we take each one of those we compared to our label will los right and we use that to tweak the parameters of our energy and we can also tweak the parameters of our optimizer I things like learning rate or momentum of the inner optimizer can be optimized so I'm not just optimizing the surface or optimizing the surface in conjunction with the optimizer and that hopefully will get rid of these these hideous surfaces right because we're what learning surfaces that are good to optimize I said this is been done before right this day you better believe is Berlanga I've done a few of them he wasn't the first but he sort of formalized he did some stuff with convex stuff he did some stuff with unenroll optimization and there's some really good paper since that one and a subsequent one I would encourage you look up if you're interested all right here's the one slider code which is a bit scary but just it's it sounds a bit see everybody in terms of code it's not too bad all right the idea is we have some we set up some initial in a learning rate we get extract some visual features right and if your labels are 2d poses that can just be the 2d poses it can just be the identity function alright we have some initial estimation Network all right which could just be random doesn't really matter I mean it might affect results but it could be very simple all right and then we also compute the ad loss just to get us done right and that will encourage us to get that that sort of it trains the initial estimate Network a little bit if it exists right and then our inner optimization loop we don't try and solve to convergence right the theory says you can do it to convergence we just do it for a fixed number of steps and that number of steps doesn't have to be large for the human pose stuff we had think up to 16 for the reconstruction stuff got bigger Network so I think there's four steps right so even less than having co-located minima we just want a surface that will tend to roll ball downhill towards the true global manner or a our updates that was very simple here we just had quite innocent I'm just familiar gradient descent we also experimented with momentum as it turns out I didn't really see a huge difference between the two alright if you train it with momentum the surface just ends up being slightly different at the end of day we just plug that into a standard optimizer all right I think what did I say ah come on I didn't specify not optimize I think we used atom and we just optimize with respect to the learned premise all right and they're not just the parameters of the networks that also the parameters of the optimizer as at the very beginning you do want to make sure this in a loss network is differentiable with respect to network parameters and that that derivative is differentiable with respect to the network parameters I'm not convinced values definitely won't work I'm happy to be convinced that really use won't work but in practice they didn't work very well at all so we just used softer I'll use like nothing to do crazy stuff Raley's worked pretty well all right so there are some talks pretty much what those red boxes were as I said this frameworks means hell up before I'm gonna start speeding up a little bit cuz we're running a little bit short on time all right buff your human pose stuff we just said well let's take our 3d pose here all right let's put a pin inside the red box we're going to project him down to 2d so we have what we would see under our hypothesis we have what we actually see we plug that into some energy and that's supposed to represent the consistency at the same time we plug our three-dimensional pose into our a separate network all right which is supposed to represent our feasibility we could think of that as learning a prior over three-dimensional poses all right and we take the output we optimize with respect to our Y and hopefully if we do that we'll end up with a pose which is both feasible and consistent we could have just used a reconstruction as a reprojection loss for this energy up here so I'm going to compare the two as it turned out a lot of the 2d datasets have different joint annotations to the 3d datasets just subtly they have like a different number of joints 17 in the face sometimes so instead of that to make it a bit more generic we just said let's take our two versions our in third to depose and now grant observes to depose alright let's concatenate them together and then take the paired difference the prepared distance between those joints fits a pairwise distance between those strengths that has a number of nice property that's rotationally invariant all these other things more important to the three-d pose you have rotational invariance and and some nice things that just come along which I generally good mean you have to learn less things these networks I call them deep networks but they're not really deep they're one hidden layer followed by one output layer so very very small you've got roughly 100 inputs hidden layer if I think 64 and then maybe eight outputs which we summed together so all up very very few Network parameters in terms of formats it's a little hard to see but the solid lines are the ground truth or observed observations right and the dotted lines are the inferences after a number of steps so we see if we're in if we're in the camera view we start off fairly decent but there's still a bit of a discrepancy after even one iteration we've synced up very very nicely alright and on other views right we don't go terribly away from feasible places it does generally do fairly well I do need to speed up just a little bit quantitatively here we see our baseline methods in the red and blue that's whether you have one block or two block and then each dot represents a different number of hidden layers so in general you've got more computation more learned parameters as you go to the right on our red and blue dots the size of the dots is a number of parameters in that model all right down the bottom we have the number of operations and vertically we have the the error so down is good left is good the black dots were all exactly the same set of models the only difference was the number of iteration steps we took in that optimizer a number of iterations well right so we the iterations it's just a exactly the same as the baseline methods right even one iteration gets us a long way towards better take some more a few more steps but decreases the accuracy a fair bit all right I think we had 0 1 2 4 6 8 12 Oh gets better it does eventually Plateau it doesn't really get much better than that last number in terms of accuracy we were just like a millimeter or two on average worse at the most I think this is our most most favorable plot so we put in the paper but on average we're typically you know a millimeter or two on average worse in 3d over the course of meters I was okay with it I will just say just cuz there's less multiply ads doesn't necessarily mean it's faster at least on GPUs these big models tend to just use up more cause if you these tiny models just don't use up all the cause so they don't actually take full advantage of that if you're on a small embedded system or something without that you know mm cause I think you would see a very decent speed each one of these was trained separately theoretically you could adjust that evaluation time we didn't really investigate that is that answer you question yeah see what you are sort of fine-tuning the surface to the optimizer and vice versa and yeah we we just said let's take a fixed number all right I have very much time but I've got to try and get through this maybe a little bit faster reconstruction is sort of still 3d inference from monocular vision but the idea is given an image let's try and get some sort of 3d CAD model representation we decide to use voxels for this work voxels just a 3d pixel terminology before you add let's do a really quick aside in 2d convolutions in general d convolutional networks of news for semantic segmentation stuff and they pretty much take an encoder write down some latent variables where and then a decoder which almost always just like turns this around in terms the number of filters size of things as it turns out if you do that the number of operations doubles per resolution and then sorry the number of operations stays the same and I think the number of the memory requirement doubles for images in 3d the number of operations per resolution doubles and the memory requirement quadruples well that means is you generally end up with models that do something like this all right which take maybe a 128 squared image and output a 3d CAD voxel grid in like 32 cubed resolution which is just a little bit dissatisfying for me so given that work that radically reduced the number of parameters right and number of operations we decide rather than trying to reduce that number let's keep roughly the same number and try and increase the resolution whereas really infants right so everyone's I'm sure is pretty comfortable with the idea of coordinate transformations right that's that's not too hard we need some concept of projection right in the first step to that is normally doing some coordinate transformation for this work rather than taking our output in camera coordinates and transforming it back to world coordinates and comparing it against some world coordinate inference thing which most people do we transformed our label data set to frustum or clip space coordinates turns out it's a little bit faster but we slide this question like how do we project probabilistic occupancy grids and that's a rather you can come up with lots of ways in your head that might be great the most obvious one might be what I call sort of max projection all right so if we take out our 3d voxel grid reduce to 2d for visualization purposes in clip space that's in first time space so max projection just says well let's go along the Ray alright let's just take the maximum value of the probabilistic voxel grid along the way if we take that and we compare it to some image features that we got out of our image for illustrative purposes I'm just saying let's pretend we've got a silhouette so some some pixels of black pixels of white alright then we can plug these guys into one CNN right and get some learned energy function alright so in this case you'll see like there was nothing here but there were lots of guys along here so really we want to be able to space carve that out in like one shot you want to say we saw absolutely nothing we saw background all of these guys should be off alright problems going through this if we've got this max projection all right when you do that update step and they're in a loop you end up with very sparse gradients so you'll still get an update right those guys will get changed like the green ones will go up a little bit and the red ones will go down but you won't touch the rest of them you'll only ever touch one and you can try and do some sort of softened version of a maximum all right but you don't really solve the problem there you just smear it out right so we played with a different idea right we said let's just use concatenation you can not call this projection if you want if you want to call this concatenation and not concatenation projection feel free the idea is we take all our voxel values and we sort of concatenate them into an image feature so we take a 128 cubed voxel grid and we convert it into 128 squared image with 128 features now we take that we can catenate that with our image features plug into a CNN and then we can update all of our values right it's not quite as straightforward as the 2d image version like you can it's like that the to knee pose stuff right again because projecting these guys is a bit tricky all right but we found this worked quite well in practice so now we're not going to try and split up into an explicit projection function reconstruction loss and prior loss all right we just have these CNN's which would say how consistent is this pixel feature with this array of voxel values we still want to take advantage of some of the hierarchical features our characteristics of sort of unit style architecture or convolutional architectures so we just took a couple of different image networks here we see model net version two no mobile net version true but we also experimented with the bigger inception the four ResNet as we have our our beautifully illustrated voxel probabilistic voxel model over on the right alright we just take each one of those or pick that multiple times and we're going to pull it and isotropically maps off the top here in our image we've got high resolution low number of features all right we're going to combine that with something that's been pulled such that we have lots of fine-grained resolution in the image space and not that many features so not many boxes along the dips and we just do average pulling now we do that for a bunch of different resolutions there's a one-to-one mapping here alright so the architecture ends up looking something like this now we only extract the image features once so we pass it through mobile net and then doing do a unit style up sampling as well yeah we take those features alright we take our voxel grid we pull this in at different rates right interpret it as an image just by reshaping alright concatenate with the image features and get a loss function yet again go through and we optimize this white up I'm gonna skip over these results because they're really running out of time in short the we found at 32 cubed resolution at least the changing of the data set from world coordinates to frustum coordinates didn't make the problem any easier made a slightly harder in fact we propose that's because for things like planes right you can pretty much pump down the average of the data set for plane and you'll get roughly right they've all got two wings they've all got a fuselage as soon as you throw in different camera rotations it's harder to learn those those price and only that our models did slightly better than just like a baseline that's being said we didn't really target the 32 queued resolution but we want to compare against the baseline and they don't work at higher resolutions at higher resolutions compared to other people we did slightly better in some categories slightly worse than others cars we did slightly worse because they're kind of like blobs this mattress a Russian doll Network sort of layered things on kind of like a Russian doll as they they're quite good at cars and blobby things whereas things like tables we did significantly better all right because we were able to take advantage of the sort of space comping aspect in really leverage there's computer graphics techniques and those results really only other time here but enjoyed depending on what you deal with your lost function you can either make a conservative loss function which does better on those metrics if you make your loss function look more like an in section over Union then you're in section of Union with all results get better over on the right and they actually look worse I feel right but I actually score slightly better on the metrics that we chose to to target all right that those metrics don't have a feasibility component they're just intersection over Union you do get a bit of depth shadowing this example here highlights the depths alone so if you look in the same perspective as the camera right you get no false positives like it sorts space carves pretty much perfectly alright but if you look at a slightly different angle it doesn't really know where along those rays it is so you get this very slight gap show but it's not massive I hope you're good all right so it has learned to pry out of those tapes quiet I think I'll skip okay so this really quick summary Wow networks essentially the generic decoders that are parametrized mostly by an energy networks we don't really need to write this explicitly code right we don't need to write a deconvolution nowhere now now every construction thing do we have a 3d deconvolution network alright we just had a convolution Network and even that we did some pooling hacks to make it not too computationally intensive alright thank all right explicit computer graphics techniques rather than learning them alright if you really want to you could just say they're just specific customizable recurrent players all right I think that's it takes away but if you want to do some theoretical analysis you can you can take all the stuff from our own ends and sort of apply the generic Arnon I here we're gonna skip that any questions [Applause] you if for some penalties on the gradient field off this encoding that you're not in the middle so my question would be is it possible that those things I think watch me use those things over for example spectrum organization to file the Lipchitz constant or you know enforcing some properties is the Hessian or the SVD because you never saw so small that you could use those techniques right you can compute the stable rank of a single layer Network you don't need to do gradient steps to ensure necessarily you don't need to enforce it I I don't think I agree with your assertion but it's just a regularization [Music] why I'm not saying you're wrong I just don't talk I don't understand your assertion that oh sorry it seemed to me and maybe I'm wrong these are not familiar with this particular bit of the literature but it's it seems to me that if you're doing gradient descent steps inside this and you're enforcing some properties on the mapping right it should be smooth for example in the local vicinity of where you are currently in this energy function so to me it seems that this is a regularizer on the curvature of this mapping and so you could think of it in terms of touching the Lipschitz constant or the has seen at that specific point and there's been this all this work in ganz of spectral normal stable right now and so on that seems very familiar but just a different way of doing it so I'm guessing the question is more like do you have you looked at any of these things or do you see this as a valid point first questions easy no I have not looked at these things and I am not familiar with that literature so I would defer sounds like you are so if there's a connection there fantastic I will save those things try to recognize the curvature and don't and you may have observed I like your observation that this is somehow regular in the curvature but it may be a difficult or effective way to do it with something relevant yeah exactly best approximation this weapon gets so interesting this is also quite chilling just do a couple of senses rather than computing say like trying to compute the Hessian or something yes so I mean essentially you just want a function that behaves well you know you're not trying to enforce that I mean in in the network itself you I did try with rally using too much right well and I'm willing to be convinced that that was never gonna work I've read people that say definitely never will work but they don't really justify any link because the first week when you should have a nice - yeah like I can't remember exactly what my generator architecture was like it was soft well using one and hard yep yep so I mean although they were purely illustrative me dragging and dropping things that they did they want anything to do with what actually came out of the can does that answer your question makes and see how to do doing optimization over the money for food compared to what you're doing yeah do you mean my GP LVM or something was it possible so they are absolutely worried you know / when they are forcing things to be smooth and to be connected through your learning with the volumetric work to do repeat all that rather than an occupancy grid which is quite coarse did you experimentations like for example truncated signed distance functions ah I'm glad you asked this whole work started looking at signed distance functions and I want to do I want to evaluate signed distance functions to like point clouds so I needed to sort of do the isosurface extraction there and I did that by rolling balls down hills in the learned scientist ins function and then I had an optimization step like as part of my evaluation and it sort of went from there and we didn't have too much success there in terms of future work out I want start applying this to point clouds and as soon as you decide to move points around to a global minima in a point cloud that's getting very close to isosurface extraction problem is you need a continuous function with respect to the inputs and you would like some sort of hierarchical thing point net isn't hierarchical if you know anything about putting that point I plus plus isn't continuous anything that uses a fixed ball search isn't continuous I guess haven't tried maybe it just works well even though you've got discontinuity see but I feel that optimizing a discontinuous function with gradient descent is not going to be pretty so yeah well I'll overcome you know recently it's more than just general point cloud convolutions how can we make those things efficient so that you can be doing inside you know loop and also continuous so that it makes sense I would love at some point to get back to the signed distance functions because I think they're really elegant so long as you don't have to evaluate them on a grid yep okay thanks what do you think evolutionary methods might be good for this didn't our haven't tried on their cool I'd like to go explore so we normally had a small D convolutional thing so in the human post stuff we just use exactly the same model architecture as that the baseline just with ten times fewer hidden layers but hid hidden units and only one repeated block so it was just just much much more for the 3d stuff we just used a 3d ID con thing that went I could take your cube resolution and then average pulled out I think this did some dirty hack to make it yes yes so you can think about this as a refinement model yeah what each step is just a small refinement yeah we didn't really we took basic input like 3d stuff we took a fairly simplistic approach but that's because the focus was on 