 so let's let's get started welcome back to lecture this seven or eight robot yes week week seven of deep unsupervised learning so what we would do today is we will spend probably two-thirds of the time talking about more gains and then we will spend some time talking about non generative based unsupervised learning methods questions on like logistics and high level stuff yeah do that do you get what so right so if you were careful in using that you see if you have some spare that's left I don't think we have planned for like dedicated project credits yet oh you'll see cool so so far we have learned about Auto regressive models flow models and latent variable models and all of them really fall under this likelihood based framework like either you have explicit tractable likelihood or in the case of latent variable models you don't have the explicit likelihood but you can still use variational influenced to get a pound of the likelihood but again it's really a very different animals in a sense that in the sense that you don't really get any likelihood evaluations and really what you're trained is you train a some deterministic function that map's certain noise variable into some data space and you don't know the prescribed likelihood and you don't really have other access to to the models and really the only thing you can do with the model is to basically sample from it and now you we essentially require a training regime where you can actually train this thing just by looking at it samples without doing anything else so that's the and that's within this implicit genital models framework again is the most popular one the one that most people would know so that's what we would talk about and well more like recap so I will skip through a lot of them so again is I would say like probably most people know generative models by Ken's like rather than knowing it by some other discipline so it has really popularized called this domain and the this is again just recap and the cool idea of gaen is you have sort of a two-player game instead of like training one journeyer model like you train one junior model and then you also train a discriminator and the job of the discriminator is to try to say what data points are more likely to be real and what data points are more likely to be fake and then you can essentially derive a gradient direction from how probable your discriminative things the data point is and this is again just to walk through the schematic of what we are doing here essentially on the on the left is the discriminator what it tries to do is basically take in each data point and then try to predict the score that says whether this is likely to be fake or likely to be real and in the original game framework like we formulate this classification problem but it is really doesn't need to be a classification problem just think of it a certain score that you want to have higher score for real images and lower scores for fake images and then the job of the generator would be taking in some noise you may be through some differentiable function and and that becomes your data point and then that would get fed through the discriminator and then you will try to fool the discriminator you will try to generate data points that achieve a high score under your discriminators call function question so far and what I would like to do today you have seen all the math the derivation what is the optimal discriminator and things like that what I would like to do today is instead of going through the derivation or the maps I would like to like take a more intuitive look at what is actually happening in this two-player game and someone make a really nice in browser training game training framework so like this again you can play with on your own laptop as well and so what is being shown here is that the green dots are all the real samples and so you can see it's like kind of an elongated Gaussian plus a more standard Gaussian so it's a mixture of these two distributions and all the purple points are basically pawns they are being generated by your generator and what it's also plot here is a gradient direction that would increase the like the score for a particular data point so let's say if we look at this one then when the trouble vector is pointing into this direction that means that discriminator is telling it that if you move into that direction you would increase your discriminative score the most so that is the core of the training dynamics of off again so you basically generate a bunch of random guesses and then based on those random guesses you hope that your discriminator can push you into the direction of being closer to a real distribution and the discriminators job would essentially be continuously trying to spot like in what region it's more likely to be fake and in what region is more likely to be a real images and what we also see here is like kind of typical of Ken trainings where you see a lot of oscillations and it really took a while to converge to something that looks like a true distribution and even when it after it has converged if you train it for long enough it could still collapse and then we have been training this for a while and then you can see that some of the great interactions I've really totally off let's say like some of the purple dots that here is trying to pawn to the top-right corner but this is not where the right thing to do because like we know real data doesn't really distribute some significant probability mass into that direction but why does it do that it does that because the discriminator is telling it to do that but you don't have a perfect discriminator so and that is why this kind of model is always very delicate in a sense that it your generator is learning from a code a score function that is not perfect and at the same time the score function cannot be too perfect because if the score function is perfect and let's say we are looking at some kind of truncated Gaussian distribution there has no support in certain regions of your data space then your if your charity samples land on there then it would never get any meaningful training signal so in all kind of gain architectures design you are trying to balance these two things now you want to have a discriminator that is as correct as possible otherwise it would just point you into a wrong direction yet at the same time you want to recognize it enough that even when you're far away from correct data distribution it still gives you meaningful signal questions gonna see more here you really see the oscillation of it like originally it was too concentrated in the middle by the way when we look at this kind of low dimensional data is really not doing again it's just this like if for this kind of data if you're just train let's say an autoregressive model or a flow or a VA yi you can learn this in under a couple minutes and it would make like stable progress to us there and it would never diverge and unlike this kind of chaotic dance that's being performed by again so but but really we're again really shines is in high dimensional data especially in images when you couple that with a convolutional generator and discriminator it does magic and I don't think anyone really know why that is the case but just want to say that like seeing that performance of can here it's sort of expected in low dimensional space cool so these are some of the original game samples from the initial game papers you can even argue that they are not that different from some other alternative models in that in that kind of years but one thing that's make this model pretty unique it's is fairly difficult to evaluate this model let's say I propose this scan model but and I look at these images like what can I tell of this images like I it's really hard to say that like these are much better than some other models because you don't really have a good way to evaluate it so part of what we will spend this lecture on is talking about how to evaluate implicit generative models like games and we will cover some of the methods but this is still pretty much an open problem like there's no known good way to measure gain quality or any implicit generative models quality that has no obvious drawbacks so we would look at some prevailing methods that people use to evaluate gans but they all have their own drawbacks that we will point out and this is also to just give you the knowledge to kind of read new game papers like how can I say that this new game laws this new game architecture actually gives you something substantially different so in the initial game paper the method or the model was evaluated by this method called pause and window density estimator or it's also known as kernel density estimator and the the premise here is I have some known data points that come from this distribution and I can build a density estimator condition on those known data points and then I can treat that as a estimation of what let's say I take n data points for my game generators and then I can feed those n data points into a kernel density estimator and then that would in turn give me a nonparametric density function then I would evaluate that nonparametric density function on test data and then I can report certain test generalization density metric and the typical way that this is done is you would have so let's say this is my guess this is my guess of the density function and then I will take n data points to create this density function and what it does is it really just looked to n data points and then it creates a probability mass around each data pod and then k the kernel function is usually just the density function of a standard normal distribution so if you think about what it's implicitly doing is that for each point that is close to the end data points that I have I would give it high probability mass for anything that is far away from it the probability mass decays rapidly and you essentially imagine this to be a high dimensional space where you have small gaussian bumps at a lot of places that your journey is your generator has generated samples for so that's the kind of the the oldest method to evaluate and whistles [Applause] also if we look at this equation there's another hybrid parameter that we did not talk about that is H so H is usually called bandwidth in this kind of kernel density estimator parlance and the bandwidth H really matters because if we think about what it is doing which sharpens or dampens your the probability mass that you put around each gnome data point so if I choose a very small edge what it means is that I only put probability mass exactly at the data point that I know that is generated from the model and if I choose a higher edge and it has then it will have this largest smoothing effect and we can see that here so what is being shown here is the grave curve is the density of a normal distribution so this is just like what the ground truth is and then what we can do is we can generate a bunch of samples from this standard normal distribution and after I generate a bunch of samples I can use those samples to create a kernel density estimator and then now what we vary is we try to use different bandwidth parameter to create different kernel density estimator so what we can see is let's let's start from the last one from the orange one if we choose a very small bandwidth we call what that would do is it would put very picky for with a mass around gnome data pod and then you get a density curve that looks like this so it should have a it it definitely is a sample here it has a sample here it has a sample here it has a sample here but instead of like interpolating these points into a smooth curve what you would do is you would put very high probability mass close to a point that you see and I have shoppe decayed from there as you can see this orange curve is really a large deviation from the ground truth grey curve and unless you increase the bandwidth you make the curve smoother and smoother so if we look at the green one it's a lot smoother a lot less high frequency fluctuation than the orange one and then if you look at the blue one it's actually pretty smooth and tracks the gwangju of density pretty well and the way that positive window density estimator or kernel density estimator is used in an evaluation is that you essentially have a validation set and then you would create you would basically choose your bandwidth hyper parameter based on validation score so you try to find a bandwidth parameter that maximizes the density results on the test set data questions on the mechanics of this method so the question is is there a I guess the question is is there a principled way to choose X by saying I want this to be this spiky I want this to be this spread out so unfortunately there's no good way to do that without making assumptions on your like underlying data so if you know what my underlying data is then you can probably choose the X in an optimal way but then that kind of defeats the purpose but if you already know the underlying distribution then there's no point in estimating that from finite data from the first place cool so these are just some of the results that are polled from the initial gain papers and what they were shown here was essentially gans achieve competitive density modeling results if you assume this estimators are accurate and we take a look at why the some of these are like how to interpret like this is not to say that the initial gains were not as good as some of the others they probably were much better than some of the others but it's just very difficult to read that result from just a kernel density estimator so this kind of estimator can be really unreliable so we are showing two cases of where it can be problematic on the Left we can show that it's incredibly difficult to use this kind of nonparametric estimator to get to - to get a gwangsu of log likelihood or density so what is being shown here is that for a distribution what you try to do is you try to increase the number of samples that you used for this kernel density estimator so and this is unlock scale so this increases quite drastically and this black line here is the ground truth density and when you would hope to see is that as you increase the number of samples you approach ground truth density very rapidly but this is not the case especially in high dimensional space where even if you put a bunch of calcium pumps on it there was still plenty of spaces that you don't cover and you really need a lot of a lot of samples to fully cover the space so for one thing like it could be highly grossly under estimating the underlying probability under the second dimension is this table is pulled from this paper a note on evaluation of generative models and they essentially constructed a pathological case where kernel density estimators would actually give you higher score than gwang-soo so what this means is that this kind of estimator can not just underestimate but they can also grossly overestimate the performance of the model and some of this come from the fact that it assumes a certain kind of continuous distribution and if the data that you're modeling is really not continuous then it could have rather arbitrary things happening so people in the field also understood the limitations of this kind of kernel density estimators and that's why in the past couple years people have been moving away from it if you look at the new again papers they typically do longer reported this score so any more questions on kernel density estimation cool so after this kind of attempt to estimate density in high dimensional space people have looked elsewhere to find a good way to measure how good is your generative model so one of the idea that is proposed in the improved gain paper is a good generator should generate samples that are semantically diverse so let's unpack what does that mean so that means if we have a good generators then it should generate a lot of distinct images but how do you measure this thickness like like you can just say this images are distinct in RGB step space so we say that this images are distinct in semantics like they correspond to different classes of objects they correspond to different type of scenes so in this particular construction what people did is they talked pre-training inception Network v3 that is trained on imagenet classification problem and essentially what you get is you get a new network that takes in an image and then gives you a probability distribution over classes basically what what class does this object does this image belong to and here Y given X Y is one of the 1,000 image net classes by the way it's like really random in case you have not looked at image net classes before these are the classes like I mean honestly I think I would do extremely poorly like I have no idea what is a English Setter sealion anyways so basically this is what the semantic predictor is trained on so basically the the inception network should take in an image and then say how likely it is to belong to you one of this 1000 categories and a couple considerations so if we take this inspiration and then make it more concrete then there are two criteria one criteria is each image should have a distinct object because most of our models are trained on images that only contain one object so if our generator is good then it should generate images that you can see distinctly there are objects in it so if you see an image you should be able to say that this belongs to a certain class with very high confidence and probabilistically what does that mean is this conditional distribution Y given X should have low entropy that means after I see an image X I'm very certain what class does it belong to and then the second dimension is there should be as many classes generated as possible so if I have a generator that's trained on or imagenet but it can only generate cats for me then like you can say it's a great cat generator but it's not a very good general generator so you also wanted to be able to generate as many classes as possible and we encapsulate that by saying the marginal distribution of predicted label should have high entropy so mathematically this is what it means what it means is that we are given an inception model and again this is just a model that is trained on imagenet like we don't really train this part this part is given to us and then after I have this conditional distribution I would use that to define a marginal label distribution and then the way that you do that is you send ho a bunch of points from your generator which we know how to do is just simple a bunch of points from your noise distribution and then you map it through your generator now you get a bunch of generated samples and then you would evaluate this and then you will evaluate this conditional distribution and then you will have urged that so that gives you the marginal label distributions and then what the inception score is is simply we can ignore the exponential for now is simply you generate from your generator and then you evaluate a KL divergence and the KO divergence is simply the KO divergence from your conditional label distribution to the marginal label distribution and then you sample a bunch of times and then you take average of that KO dive whoops and then you take average of that KO divergence and after you take the average you take an exponential of that for easier to compare and and that's the so called inception score which you find a lot in kind of modern game architectures and methods and they use this to compare but you might say like this this way of writing the inception score is rather cryptic like it's not very clear what is this actually doing like you trying to generate a bunch of samples and then for each sample you measure the Kyodai versions but then like what is this actually doing so what it is actually doing is actually has a really nice intuitive interpretation so if we go from this line to the second line where we simply expand the definition of Keio divergence now we get something like this all right so what we can notice here is that the second term is simply the marginal entropy of Y all right because we are generating x and y comes from the conditional distribution of X so you can see this as generating from basically the definition of above so what this corresponds to is it this corresponds to the entropy of Y and then the first part corresponds to the conditional entropy of Y given X and what this boils down to is an equation like this and it actually measures so basically what it measure is the entropy of Y subtracted by the conditional entropy of Y given X which is exactly what we talked about in the slide before like you want entropy of Y to be high so that your model generally a lot of different classes and you want the conditional entropy of Y given X to be very low and that means after I know after I see the X my label predictor is very certain and that is basically all of this is measuring it's trying to measure that how semantically diverse my generator is and also within one image how distinct it is and it has another interpretation that is the mutual information between x and y but we're not going to that with the mechanics of it clear yep yeah yeah so the question is for to evaluate this correctly you need P of Y given X to be very accurate like otherwise if I give you a cat but if your creditor cannot tell like it actually predicts something that is high entropy then you'll be wrong so the answer is yes so the kind of implicit assumption here is that a model that is trained like in an inception model that is trained on imagenet this good enough but then like it's really hard to evaluate whether B is actually good enough or not yeah yeah yeah so the question is since we use an image net classifier then that means like the kind of measure of semantic diversity is is implicitly defined within the scope of image net classification so if you have a very different problems like medical images or some other images then the inception score no longer applies so the comment is correct like you it's no longer meaningful to do anything like that and in fact like the most common ways to use this score I would say like it's not totally valid either like most of this most of the times this score is used when you train a model on say C for ten and then you vary on image net classes like you can say that image tener Casas is a is a richer definition of the semantics but there's through a certain shift that is not like totally just modeling how well you can generate C for ten classes like it's measuring something that is somewhat different cool so these are some of the this is a table taken from the improve can paper and what they're showing is different variants of Gans and the inception score that they achieve and what we can see is that like for certain games that I was in terms of like there's no there's no distinct objects in it and there's not very semantically diverse then it gets a lowest score and then for again that is better in terms of being able to fit being able to fit images being able generate images that are fairly distinctly look like some kind of objects like this horse look like a horse this looks like a ship and for this kind of generator it gets a higher inception score and now of course gwangsu would once you have does better than this yes yeah so the question is the inception score is not a component of the laws and we only use it to evaluate so that is correct and it's actually very crucial because if we optimize on inception score actually anyone would guess what will happen yes yeah yeah yeah yeah yeah yeah so that's that's that's that's that was exactly what I was looking for so if you actually optimize on the inception score you would not give you the correct thing like most likely it would find some other serial patterns okay can you can we right right so so the question is a great one so the question is let's say my ground truth data does contain a thousand classes and let's say I'm trying to fit again to those 1,000 classes but for whatever reason my discriminator cannot distinguish among some classes and that is possible let's say there's some classes that rely on like maybe it needs to have a certain specific architecture to detect it but my discriminator doesn't have that architecture then essentially your can training mechanism would have no supervision or training signal for your model to capture those classes and as such like your model would get penalized in this inception score but that in a sense is to be expected because if your discriminator is not good enough that causes your generator to not be good enough and then it gets penalized by this score more questions on inception school so the next one is sort of the latest addition to to the mix so assessment score is it's okay but one example of the weakness is that like it's doesn't really sufficiently measure diversity due to the fact that you are just measuring mutual information between X and label Y is essentially bounded by whatever number of bits that your classes have and a concrete example of like how you attack that is say if I have a list of images let's say I take one image from each of the image net classes and I just treat that list as my generator and now that list could obtain perfect Inception score right because for each of my image my classifier is going to be confidently say this code belongs to cause something and then I have a thousand different classes then that means I maximizes the diversity measure on the inception school and of course like it's not likely if you actually get a generator that looks exactly like that but that also implicitly say that you could actually have a generative models that are not very good but still achieve very high inception school and the the essence of this is because it's only trying to capture the label distribution and then use the label distribution as a way to compare the generators and this I will not attempt to pronounce this and F ID is proposed to capture more nuances than that and what it is trying to do is it's trying to say instead of comparing distributions based on only the conditional label distributions what I would do is I will try to compare different distributions based on certain semantic features and mechanically how they do it is you say for each image that's generated by your generator I would feed it through some network and then transform it into some feature space and hopefully this feature space has good semantic meaning and the the choice that's used in the community is you again take the inception v3 Network and then you take the post we the pool three layer so which is a two thousand dimensional activations you treat that as your kind of feature embedding space of the image and then what you would do is you would sample a bunch of points from your generators and then for each point you maybe through this embedding and then you would compute the mean and covariance of those embeddings so you can also think of it as your fitting and empirical Gaussian to all those embedding data points so you now you have one Gaussian that is for the generative samples then you also generate a bunch of points from the validation set of your training data so now you can actually construct another multi-dimensional Gaussian from data that your model has never seen and then what you would compute this you would basically compute a distance metric between these two gaussians and but I mean since they are not Gaussian in the first place is kind of arbitrary but really just think of it as how difference how different are your are the means and covariance from your generator how when are they from the means and covariance of of the crunch of data yes yeah so well you do it on validation set data so you basically you do you do this procedure for some guidance at data and you also do this procedure for a bunch of generated data and then you compare how similar or dissimilar are they from each other well I mean in in this case there's there is an even a class definition so you just take some Treasury network and then you take some of the features and I hope the features are good but in a sense it's also richer than just taking the classes labels because you can imagine that the two images that map to the same class will still have very different intermediary new net activations so this is trying to capture those additional details cool so in the paper that proposed this distance this evaluation metric so again they also did some pretty interesting evaluation they basically compare they take a good image and then they compare what will happen I basically proposed a certain noise model on a good image and then what they do is they increase the noise and so if you have noise that is increasing then you should say that the distributions are becoming more and more far apart and what they are comparing this to is they're comparing this to inception score so let's look at inception score first so this is what they call inverse inception score something which is just really just negative of inception score and what you can see is that as you start to add more Gaussian noise to the image the inception score stays flat even decrease a little bit which is not very good and you can say that this is because our modern classifiers due to the fact that we train it with so much randomizations it's actually very robust to different noise so if I give you an image and I add some noise to it your classifier is probably going to tell me the same thing as long as it's not ever so real nice and this is reflected here so you we can securely we can clearly say that this is a much worse image than the first one like much less faithful to the original one but inception score doesn't really capture that whereas if you use fi d tube to measure this what you can see is that as you increase your Gaussian noise you actually get larger distances so what that means is that this metric is able to capture this kind of noise and then they do a similar exercise for a couple of different noise models so in the second role what they try is they try to add Gaussian ization below to it and then as you can see that as the distributions become more blurry the distance increase which also correspond to our intuitive notion of like the distributions are further apart and similarly for inception score just because of the fact that it's a little bit below it can still tell that this is a human face or some other things it doesn't really change the inception score a whole lot so these are all fairly er cases of Inception scores that can be captured by F ID yes also we so so f ID is a distance so we will know to minimize the distance between two distributions which is why we are looking at negative inception score here so both of them we want as long as the deviation becomes larger we warn the distance to be high and we want the negative inception score to be high so basically ideally we should always be looking at an increasing curve when we look at as we increase the noise so there's some other things like things like drop out some parts of the images and this is even a more interesting case so in in the last in the second row here what they did is they try to basically compute the distance metric on a face data set and then they would start to inject some random images from other data sets so you see that initially it's all faces and then later you try to inject images that are from sea-fowl image net so this is funny right because like if I have a generator that is trained on faces I don't expect it to generate cars for me but if you actually if your generators somehow is doing that then in Inception score inception score would give you a higher score because now you're generating more semantically diverse objects so inception scope would give you a higher score and as such like lower- inception score but this is kind of wrong because like I won my game to model faces like I don't want it to model other things but that part is really not captured by inception score just because it's kind of fixated on the predefined label classes but for F ID on the other hand because you're comparing the empirical feature space distributions between your generator to a held out data set you can actually compare these two data sets and if my hello data set only has faces then it would say as you start to introduce more classes like it would create larger distances because like you you start to deviate from your validation data so what this tells us is that F ID would very likely capture more neurons as more details than Inception school can and it's being used by more people's but at the same time is still not a perfect metric but it's only a comparing like takin order yeah yeah so the common is around like fi the similar to style transfer in a sense that is looking at up to first and second degree of water of Statistics so I think that's a fair comment and I think that's probably the I would say the biggest attack you can create for F ID that is like you can create high order distributions that have high order of interactions that are not captured by just first and second order of statistics like you could construct distributions that have the same mean and covariance and yes do these two completely different distributions so so that is true so I think like likely if you train you can buy directly optimizing F ID you would get something like that instead of like actually learning the shoe distribution questions on evaluations of ganz okay I will turn it over to oven to finish the rest of the events can methods yeah we can post it now [Music] cool yeah let me like quickly recap DC gang or W can and continue to us arrest of against so yeah DC Ganz kind of was a big engineering advance and making sure that ganz can output like really good samples samples of large resolution bedrooms over large datasets millions of images faces and smooth interpolations and also emission samples even though it's a small resolution 32 cross 32 the fact that I worked on a large data set was really cool and you also saw these vector arithmetic s-- across different attributes of faces and all another key result was that you could do representation learning with a discriminator where you could take the features trained on imagenet and then use it on C far and achieve like really good results for classification so in the discriminator the discriminator is a continent so you take that trained on image net and put it on c far so some issues that remain after DC gang like given up for DC can was done was that the training was still pretty unstable a hockey and there was a lot of brittle architectural choices like you had to use leaky rail you in in the in one of the discriminator generator i forget which but you have to use leaky rail you and one only one of them and not in the other these kinds of things and also the batch normalization you have to be very careful not to use it in the output layer of the generator and input layer of the discriminator these these were very specific choices and even though a bunch of tricks to improve it the training methods cause in this paper it wasn't really super stable so the after while people thought about connecting some ideas from optimal transport to training ganz there so far the objective function in as far as far as the optimal discriminator is concerned that the generator is trying to optimize the Jensens Shannon divergence between the true data distribution and the generated distribution so inspired from optimal transport people thought what if we could optimize the earth mover distance between the data distribution and the generated distribution so the earth mover distance is defined in this way where you're taking an infimum over all possible charge distribution such that it's marginals are equal to the data distribution and the generator distribution and expectation over this Joint Distribution distance between the Paris samples so it's called earth mover because X minus y is some kind of a distance measure and the probability mass on the point XY z-- like the mass that you're trying to move from X to Y and by this corresponding distance so you can think about this as the amount of effort needed to transform a probability mass from one distribution to another distribution so the goal was to design a Gantt objective such that the generator would minimize this between the data and generator distributions the problem with this particular measure is that it's intractable to estimate and fortunately fortunately they called upon this mathematical duality principle for this particular objective function such that this infimum over the strong distributions can be converted to a search or soup a supreme search over one Lipschitz functions where the objective function gets into the form where there's one expectation or four over samples from the data distribution and other expectation over samples from the generated distribution so the this kind of an objective function is very similar to how you have it in normal Gantz where you have a term for the log probability of the true data samples and the log probability of P fake samples and you have separate expectations so this is really cool so but still the we we need to figure out how to make sure that the search is constrained over these one Lipschitz functions that's pretty constraining so that's what the WHA sustained can paper try to do so Lipschitz so the the definition of a Lipschitz function mapping from x to y if it's a K Lipschitz function is that the distance function in in in the Y space between f of X 1 and X 2 should be upper bounded by this K Times distance in the X space DX of X 1 and X 2 so the thing is since we have a objective function here which is linear in terms of the expectation we can search over Caleb shits functions instead of 1 Lipschitz functions and then so the way the authors constrain the search to be over Lipschitz functions is through this argument where they say that assuming that you have a parametrized family of functions f w where w comes from capital w if i can ensure that i search over these functions and one of one of these functions is guaranteed to be a K Lipschitz function then I'm whatever it max I get over this particular search domain is guaranteed to be less than K times the water stain distance so why is this important this is needed because in practice the F that we use for discriminators are going to be neural networks and you can't exactly make them k Lipschitz but through some kind of hacks or constraints on what kind of weights they can take or what kind of gradients they can take you want to use over those weights we can constrain them to be approximately K Lipschitz so so that's that's what the wasis thing can does this particular line over here is the part that ensures approximate Lipschitz Ness basically they are clamping the weights to lie between a constant the magnitude to be like magnitude to be not more than a constant C which is fixed to be 0.01 the reason for this is you can think about if a discriminator is a neural network and has a bunch of weights you can think about the weight as the gradient for every input and so if you can constrain the gradient to be between a particular range then that behaves approximately Lipschitz because all you want to do in Lipschitz is constrain the slope so you can think about this as the generalized definition of a slope dy by DX so as long as you can constrain it by upper bounding it with a constant you can ensure some time of approximate Lipschitz behavior yeah right so that's that's gonna be a follow-up yeah yeah right yeah they use rmsprop in this paper you have I guess if you want to reproduce the results yeah it's it's a hyper parameter for the specific data set and architecture yeah the rest of the algorithm is very much like the regular gann right you're you're drawing samples from the generator and real data samples and this is the objective for the discriminator and then you take a gradient step on the generator with a negative of this term so the key difference from the regular Gann is that we've gotten rid of all the sigmoids in log log of the log logits and probability of binary classification so we saw that those led to all the discriminator saturation problem those things no longer exist these are general doesn't know output non-linearity in FW you're just constraining it to be the weights to be lying between a particular range it's not that small for like general ganz in yeah in general people I mean Andre recommends three minus four and this is what people generally use but Phi minus Phi is okay it's not that small for ganz it's pretty good okay and yeah precisely the problem that we saw the previous week was that the great the discriminated gradient would saturate for the sigmoid because gradient of a sigmoid would have product before Sigma times 1 minus Sigma and water is already 0 with the gradient would still be 0 so that's what is being shown here this is the optimal Gann discriminator in the regular Gann on the other hand the W gang critique has Lee a linear gradient so there's no longer the saturation problem so this is a classifier this cyan curve is not a classifier but it only learns to assign high values for one distribution low values for another distribution so the gradient is well more well behaved so what does this mean you can actually train the critical convergence here there's no problem with having a converged critic for for the generator objective so you can actually do multiple iterations for n critic and they use five iterations for every generator update if you do this for the regular if you if you remember the previous the in the regular Gann pseudo-code even though the serial code mentioned K K iterations for the discriminator there was also the k equal to one was used so here it doesn't you can use more than one cool and it also seems that the wasps sustain distance correlates with sample quality so over the training of the generator the wall sustain estimate keeps reducing over time and you can see how the samples also keep getting better initially this is random noise then slowly gets the scene of the bedroom and then it starts getting the details so it's you can think about the wall sustain estimators some kind of perceptual metric and that apparently turns out not to be true with the gentle Shannon divergence estimate so the here is the the samples produced by WGN are in power or on par with the samples produced by the DC can as long as you can optimize this again perfectly if you use all the hacks recommended by the DC gain authors and run it on the same data set you can you will definitely get good samples it turns out W gain can match this against sample quality so what's the progress in W again if it's just able to do what DC can did the key thing is W again is more robust to architecture choices so if you remember DC can use a lot of batch normalizations so if you use the same architecture of DC gain but you don't use batch normalization W gain can still produce reasonably good samples where DC gain ends up reducing noise and also if you change the architecture so if you instead of using a con net if you use a regular MLP for W gain so you can see that the standard gain or DC gain would actually you shouldn't call it DC gain because no conclusions so standard again we would end up having more collapse you just see a lot of same samples repeated here whereas W can can still output some variety of samples yeah for no choice so he's done like like being a different architecture or like is there a bus to Tremont that your head then I prefer off dividing the one thing that again yeah I think they use the same set of hyper parameters yeah those so the summary is that you departed from this objective earlier you had this minimax loss log probability of real samples law property of fake samples she went from this to this from was a stain formulation where this max is over Lipschitz functions and you're trying to minimize this difference between two expectations and like finder us you can it's likely to be the case that clamping I mean it's pretty obviously clamping rate new way to do this enforce this Lipsius constraint and that was obviously followed up but there was still are like in terms of robustness this was a big step let's see so maybe we can take the break after that we can GP so WG on GP address this knave way of enforcing the Lipsius Ness where it uses a more something that works better rather that's the best way to put it so there is a there is a mathematical proposition that's proved in the paper but we we're not going to go through it here but the the conclusion from that is that this was the wall sustain critic objective that you already saw and you're trying to maximize over the discriminator now you have an additional term that enforces the norm of the gradient of the discriminator with respect to the input so these are not gradients with respect to the weights of the discriminator screening through respect to the input sample that's being what's going in and you want to make sure that the norm of this is close to one where the expectation is taken over points that are obtained by linearly interpolating between real and fake samples so X and X two they are from two different distributions X 2 these from the generator distribution axis from the real distribution and this is just a linear linear come a all straight lines that pass between these two points for different values of epsilon and you take an expectation over that and you enforce this constraint on those points and you can read the paper for knowing why this could be a better better way to like enforce Lipschitz as compared to weight clipping but they have a lot of experimental comparisons from simple toy tasks where for this there's a data set of eight gaussians and 25 gaussian and you can see that regular weight clumping wouldn't really capture the data distribution that well just getting all these squares whereas the one with the gradient penalty captures the structure of much better and for the Swiss roll dataset as well and as also this weight clipping and histogram of the weights under a clipping and gradient penalty where weight clippings is too sparse and concentrated around specific values whereas the histogram under weight gradient penalty is much more smoother little more well-behaved yeah it's somewhat connected to the proposition actually how they actually come up with the approximation to that for the implementation but it's not super principled yeah so this is the pseudocode for w ng p where the things are changing from w again is that you no longer have the clamping but the objective function is change so the objective function is this there's this extra term and yeah but similarly similar to W again you're going to have multiple trading steps on the discriminator for every gradient step on the generator and okay yeah the cool thing is now you can do it seems so so they're not using Armas probably using Adams of learning rate is 10x bigger probably can converge faster and then there's no momentum used in the atom other things are so and the coefficient for the korean penalty is 10 so these are the default values and if you actually take this and try to run it on new data set it's very likely to work these are very engineered type of parameters any questions is this any critic the same as yeah just a number of times you take updates I think whatever works so the way so this paper took robustness to another level from w gan so they try the bench market on different nonlinearities in the generator discriminator they also try this soft glass non-linearity one important thing to make sure it's like the nonlinearities you're trying should obey the fact that you want the function that's output by the discriminator to be approximately Lipschitz so you want to make sure that the slopes of these activation functions are not super high they're well-behaved so r lu has the slope one so that that's the reason for this weird non-linearity and they also try different depths different um layers without and with batch norm for the generator discriminator and the number of filters so all possible combinations of these variations they tried these different Gans rate so and it turns out that W can GP succeeds a lot of times compared to the normal Gann so that's that's another robustness argument similar to the W can W can GP is a very robust architecture robot robust gain models is that is robust to architectural choices so in terms of visible results you can see how Ellis can which is another type of Gann dcen they kind of collapse when you use larger models or different nonlinearities press W can and W and GPS will continue to work and uh began GP works even better than that we can be using make bigger models like 101 layer resonance for G and D the samples are really good on the bedrooms note that these samples were achieved with larger models compared to DC gang or w gan so it's not the case that everything all everything's good now because of the new gradient penalty trick it's just that it's easier to train these larger models with the new tricks and therefore you get even better results and Peter introduced these inception score metric and the that began GP on C 410 for unsupervised see for 10 it works the best for supervised conditional C for 10 it works as almost as for this the another game costs tagging which you're not going to cover here but it's it was pretty close the state of the art at the time so main summary of WK on GP is robustness to hyper parameters architectures and it became a super popular yan model like it has like more than thousand citations and only negative of this is the gradient penalty is expensive to calculate backward pass another backward pass basically for every time you want to compute the objective so it could be having a slow walk what time even though you could use faster learning rates and stuff the MA you might just suffer from just slow training because of the objective function cool we can take a break here now so yeah like we saw this is a robust architecture so how did it become like very popular this particular paper is the reason so this is a paper from Nvidia on progress of growing of Gans I think you've seen this idea in pixel CNN's I mean in Auto regressive models to us en when they talked about this idea of upscaling in in in the color channels and spatial dimensions so that that's kind of what's happening here where let's say that you have your latent and then you have your discriminator and generator operating at different resolutions so so imagine that instead of just having a 1 megapixel image and the generator and discriminator trying to do operate on that resolution you have the generator and discriminator operating on all the different divisions of powers of 2 of that resolution 4 cross 4 8 16 32 so on and the architecture is constructed that way and you have this multi risk multi scale or multi-resolution loss for both both the generator and the discriminator and so this way the training progress is in a way that you get a lot of signal early on from smaller resolutions and that helps you to train on the larger resolutions or else so it's super hard for you to output these kind of samples without having that diverse enough that it makes sense so these are some these are not real people they are samples output by the scan so that's the level of impressive results that was possible with this WK NGP architecture and i should definitely say that is not because of the wall sustain that all these things happen there are plenty of upscaling and downscaling tricks that the authors adopted inspired from graphics of computer vision such as nearest-neighbor upscaling and so on so the best way to like actually understand this paper is to go through the code because you probably can understand the paper you don't have to read the archived version of paper you better read the code if you really care about it yeah so and in it's not just faces so I remember when paper came out people thought okay faces are super easy but actually that it turns out like as long as you can condition on the categories you can generate these different day to day objects TV monitors mobile phone screens cycles bus horses and so on and the bedroom samples are even larger resolution now and even more clear and sharp compared to the samples from w GaN GP yes yes and and we'll see how to like condition the generator I under discriminated on the class label in different ways pretty soon and these are thousand 24 across thousand 24 million pixels in this in these images and these are the different resolutions at which the generator and discriminator are working small and so on size of scaling so that's all about the wall sustain Gann now we're going to look at another important Gann architecture again again algorithm called the spectral an organization for ganz but the commonality between both what you've seen it was sustained and this is Lipschitz Ness so let's say that your discriminator is some kind of neural net that this matrix multiplication activation function so on up to the layer L from the first layer and you're ignoring the bias term for now but this you can have the same math operating on for bias so the discriminator is some function of this neural net and the generator objective is you're trying to do I mean the can objective is basically how this cost function and play this minimax game this was changed in was a stained Gantt to be searching over these kale ellipsis functions for the discriminator and the key idea in spectral normalization is that you can look at the Lipschitz constant for every single layer in the discriminator so every layer in the discriminator neural network would be some function that takes an H in and outputs H out and the definition for Lipschutz would be the supremum over the singular value of this particular gradient so you're taking a gray in respect to the vector of the output with respect to the input so the singular value definition if you from linear algebra is this over unit norm vectors you're just trying to maximize the matrix vector product and so that becomes a max over h l2 norm less than or equal to one of l2 norm of a H and all this is not taking torque on the activation function because if you use if you're going to use standard activation function like really the the Lipschitz constant for that is one so you don't have to worry about that so another mathematical trick is the fact that if you take composition of two functions the delipcious constant of that is less than or equal to the product of the Lipschitz of the individual functions so a neural net is basically a composition of different layers operating on top of each other so if you keep applying this at every layer you're going to get the you're going to get a upper bound on the Lipschitz of the actual function as a product of all the singular values at every layer of every single layer and that's just the weight matrix at that layer so product of all the weight matrices accumulating at every layer would be the approximate it would be the thing that you are worried about if you want to constrain a Lipschitz constant of the output function so so what what the spectrum the organization idea does is it takes the weight matrix computes this Sigma of that matrix and divides the weight matrix by that value before passing it to the next layer so this way you can ensure that at every layer you're making sure that the single norm is this the spectral norm is one for that layer and therefore the output well behaved and Lipschitz approximately let's this make sense yeah yeah III actually show you how the serial code is implemented see so the question was that how do you how do you calculate Sigma how do you do this every forward pass so they use something called the power iteration to get an approximate value of the Sigma and luckily it turns out that just one iteration of that is sufficient to implement last cave models so just one iteration of the just one power iteration on the weight matrix is good enough that you can implement with standard tensor flow of Pi torch another other important difference from other other Gans in this paper spectral normalization is that they move away from the wall sustain critic loss or the standard gaen loss and used something called the hinge loss which you all know from SVM's but this was first used in this paper geometry again but to use it in the modern architectures as in Gann was the first to do that it's just using that hinge law and there's a reason for use in the hinge loss Sims so there's some math that optimizing the hinge loss is equivalent to optimizing the reverse care but that's only under certain conditions and not super relevant but the hinge loss doesn't come out very randomly and so that's another important change from the Wazza staying pretty close cool let's see so the architecture for the generator and discriminator are here you're going to be using residual blocks all the way through and the output is at an edge because you constrain you you pre-process your input to be minus one to one in general so and the discriminator is basically gonna be another big rest net this is for the unconditional case and the quest one way of doing it for the conditional case is somewhere in the middle you fuse the embedding of the class and run the rest of the discriminator so the but these two architectures are very standard like rest net blocks that you do in general for classification where you keep up scaling the number of channels and reducing the spatial dimension so why was this paper important this paper is important because first time you could get a conditional Gann working on the full image net data set till then like there were some results with some partial subsets of image net but no even even for a conditional case knowing Gann at work on full image net so this was the first can that work done by preferred networks so you can see how the the dogs and pizzas are generated not the pieces you had now yeah I don't know if that operation was done it might have been done so you could look at it paper not but I don't remember that specifically being done which one are you oh yeah yeah so it's still not that great thing there's more regularity probably the data set saw centered the phases data set was constructed by a lot of pre-processing so that's another thing I forgot to mention so the progress again result was not just possible because of the architecture but the the author has constructed their own data set called the celibate HQ data set where they did a lot of pre-processing on top of the original set of a data set and made a smaller version of it so it's possible that in general faces work because there's more structural the data set has been more structured and regular whereas imagenet is not but this is not a problem now with the modern Gans like or at least it seems like it's not a problem they are able to generate even dogs very well I just want to go through some details so if you're not a verified batch numbers implemented for con layers this is an opportunity to know how the one if you've not read the original batch nom you might think that the average is and the variances are taken only over the the batch axes of the but it's actually taken over the pixels as well to ensure that the the normalized filter is fired the same way at every location so the effective batch size in batch norm for con layers is number of pixels times the actual number of elements in your batch and so I wanted to point this out so these are your scale and offset and you just multiply this by whatever standardized version you get from your bench so why are we talking about batch now so so there was a question asked on how to use class I mean how to train class conditional models and one important thing one important architectural component and all the conditional gans is the conditional batch normalization trick and what is that basically you saw that in the previous original batch norm you're just getting you're just calling a bunch of free variables for scale and the offset these are not conditioned on anything so in conditional batch norm you're going to condition that on the class label so the class label you you would you would get an embedding of the class and then you would get a linear version of that you take a dense layer of that and output the scale and offset parameters and use those to adjust your Bashan organization so this way or if you're having a residual block in your generator and discriminator like in the architecture you've seen here all these risk blocks will have batch normalization in them and that batch normalization is now going to take in the class label if it's conditional gang and the scale and offset parameters are going to be conditioned on the class level so you can think about this as ensuring that you let the discriminator and generator and know that it's operating on that particular class now and all these specific pixel specific normalizations are adjusted for the class it's a very important trick and every can including the latest began uses it and this is how it's actually implemented so there is a small detail so one thing is these dense layers could be shared at different levels like you could you have multiple residual blocks and all of them are using batch now so you can think about sharing the dense layer for the conditional batch norm for on a class label at different levels and that's a trick that big an users which we are going to see now but I'm sure you all have heard about it okay so another so we saw spectral normalization so what spectral organization does is you can take it a step further where not only do you make sure that you fuse in the class label for the bachelor mutation but you also make sure that this layer uses spectral organization on its weights and I'll show you how to use this use this in flag like house actually implemented but basically it's there everywhere now so this is the power iteration that I mentioned earlier it's very simple so you just want the eigenvalue to be equal to one so you just keep doing so you basically so you have this particular operation where you're searching over this unit knob so every time you can multiply this matrix by a vector and take the result and normalize it and keep repeating it multiple times and that's basically what power iteration is so you have this original weight matrix that corresponds to that layer you're flattening it such that and then you take a variable keep multiplying that with debate matrix normalize it but again multiply and normalize it and so on so one thing they insure in their paper is that they don't propagate gradients through these two vectors so I don't know why if that's I mean that that's basically a choice for a computational simplicity but in principle you could do it in principle you could you you don't need how these top gradients and then you finally after a while after a bunch of iterations you are normalizing your weight matrix by this normal you that you get which is your singular value and you reshape it to the original shape of a so this is this is basically how this particular equation is implemented in the discriminators layers yeah yeah yeah yeah once you build a graph it's fixed there's no dynamic thing going on here we are talking to you in a dot product become zero let me okay yeah I could I guess in practice it doesn't happen but are you you so you're saying that this matmo could become zero but if it becomes zero then you just get out as long as you can make sure that you don't divide by zero but if some epsilon here well it doesn't need to happen as long as you're initializing it properly the space for you to have the space for the zero eigenvalue is n minus one so it's in the dimension space for your zero I can manage n minus 1 so that mobility should be there it makes perfect sense to me that when you initialize randomly yeah what I'm doing yeah it could happen actually so I've never trained this so I don't know in fact like there are some cases in the began where they show the divergence in in the training so it could happen and how you implement it it as part of every dense layer is basically you just have a flag if you want to use the spectral arm you just call it number of iterations and turns out not to eat that you think you'll need one iteration it works and the they implemented in the same way for convolution and weights as well but that's in principle not correct because convolution operating on an input tensor is not the same as flattening the kernel and doing a dot product may are doing a matrix vector product there's a different matrix structure for convolution the toe plate structure so that's not taken into account here so this is a you this is not very principled but this is how it's actually implemented now cool any questions on this yeah you just you just don't want to like to learn these you and Vees you just you're okay with starting yeah you only care about this alright so this paper got very high quality samples for class conditional Gans at imagenet scale and first time work something on full imagenet and has computational benefits over that we can GP because you don't have to take normal gradients and your objective and take gradients of that so now so I just want to mention this projection discriminator trick but also through that talk about how conditional against different kinds of conditional Gans can be implemented the first original conditional gain paper basically implemented it this way where you just had the discriminator and you you fed in the class label in addition to the sample concatenated them and learned a neural net that output the adversarial loss and then there was a dis paper where that realized that instead of feeding the class table very early on you confuse it at somewhere in the middle so you can you can you can process the input which is an image through some bunch of con layers and then feed in the class label embedding and another paper try to not have this classification loss on the discriminator in in addition to the adversarial loss so you basically let's say yeah yeah it's kind of a trick in the improved gain paper so it's similar so there is a classification loss that just regular classifier in the discriminator in addition to saying that the generated sample whether it's a generated sample or from the data distribution and finally does this trick or the projection discriminator is also from prefer networks where there is a principle reason for doing this but that's better understood if you read the paper but the architecture works this way where you take the input image you get a embedding from a residual convolutional neural network you take the class label and embed that and get another vector for that and you take an inner product between these two embeddings and add that to the output of this particular small MLP and that becomes your adversarial loss in practice this turns out to work much better than any of the other three cases and this is now a very standard trick being used in the modern gang architectures it's not attention but yeah some kind of similarity between the embeddings are captured by the inner product yeah cool so now it a fought at the gun after Essen Gann was which is important to unknown is self attention gang or Sagan so the abstract says why this architecture is important so it improved the state-of-the-art on 128 cross 128 image net from the inception score improved from 36 1 8 252 and the F ID reduced from 27 to 18 so these are very big reductions and increases so and you've seen self attention being used in auto regressive models flow models so obviously somebody had to do it for ganas well and so that's what these guys did they took a so so this is how the self attention is implemented so let's say that this is how it's actually implemented is using a 64 64 resolution they figure out these three feature maps using one cross one cons and this is the query key dot product part of the a self attention and once you figure out the attention values you add it to the other part with the value part and get this new self attention feature Maps one trick to do is pay rate for one part of the query key value in this case let's say you had 64 64 is the input it wouldn't be scalable to have self attention map over like 4096 cross 4096 so what they do is they reduce the height and width dimension and increase the channel dimension for the F and G maps we're using strides and are pooling you can do either and for the other one they increase the number of channels so that when you find take the certain in feature maps you end up with the same dimension again best understood if you go and look at the code for Sagan which is open-source yeah it's part of the architecture I can show you how it's part of it yeah so mathematically this is what is happening so one difference from just regular self tension is that the scale and offset doesn't that they're not learning a new parameter for that so this is just a single scalar value I don't know why the this was a conscious design choice or not but this is AI was implemented so that's a single value of gamma for all the dimensions of I and O I X I Y I so one thing that I felt was pretty counterintuitive in this paper is that even though the argument for using spectral on this session was to the discriminator or make the discriminator weaker by making it more Lipschutz they actually apply this federalization here to both the generator and the discriminator weights not just the discriminator and it seems to help a lot another thing is they use yeah the use of attention in both degenerate and the discriminator the hinge loss which you saw which which was being used in SN ganas also be used here and this paper is important because just like how SN can produced the first full dimension and conditional samples this was the first kind of produced was good unconditional image net samples again how it's possible that if you use self attention and hinge loss on other earlier gaps it might have still work and the conditional models are implemented by using conditional batch norm on the generator at the protection discriminator for the projection discriminator in the discriminator so that's the way in which you use the class label differently in the generator and the discriminator well the projection discriminator is this thing that okay cool so these are the visualizations of the self attention so I don't know if you can see the difference these are red green and blue these are three query pixels and you can see that for the red pixel these are the closest in terms of attention values these are the closest pixels and the other parts of the image for the green these are the closest and for the blue these are the closest so you can see that even though these pixels are neighboring in the original image the self attention can attend to like other pixels there are much farther apart and in very different ways for each of them so that's the global notion of self attention operating here and in fact for this dog you can see that this red focus is on the body and the green focuses on the background and the blue focus is on the other leg so there's a blue pixel query here on one of the legs and it's attending to another leg here for the dog so that shows how self attention is actually helping began to capture a global structure of the image and these are the different samples so that conditional samples for different categories of imagenet they're all looking really good like pointed out in the abstract the inception score and the F ID a state of the art and by a big jump from the previous state of the art which was Essen Gann with projection discriminator so this was the big I mean uses of self attention and other latest tricks basically made it possible finally the big an architecture you like we saw this in the first lecture of the class all the interpolations in introduction so there's absolutely like no connection to the kind of samples you saw so far and these samples these are just too good and these are high-resolution 512 cross 512 samples on on the actual raw version of image net1 one caveat is began as a conditional generator model it's it's conditioned on the class label so all these were obtained by feeding in like a particular class label for these categories and the coolest thing about began is interpolations that's why it became very popular so you start off with this latent and you move more cross and get to this when you get these smooth interpolations across different classes so you have these a dog and a bird over here and a dog and cat over here so so one important tricking began which is worth knowing there are so many tricks so the best way to distill all of them into your head is read the paper but but this trick is what made a big difference for samples the first line is this is the auto normal regularization on the weight matrix this isn't another parallel paper published it up along with the SN Gann this is a different kind of regularization on the weights is that you basically for every weight matrix you enforce this constraint and make the weight orthonormal so instead of doing this they have a mass so that the only the off diagonal elements of this matrix are constrained to be equal to one and the diagonal elements are not penalized don't ask me why but it does the reason that this trick is important so if you use this trick it helps you to do something called the truncation sampling where in again let's say you feed in the Leighton's and let's say the latencies are from a gaussian and you get the samples you can constrain the standard deviation to be small so let's say instead of sampling from a standard not normal Gaussian you can perform a truncated Gaussian now if you keep reducing this truncation so that your Gaussian is narrower you it it reflects what you call it some more collapse where you just keep getting the same sample but if you increase the width of the Gaussian you get more variety so this has like a width of two and you can see different kind of dogs here and as you keep reducing the width to 1.5 and less than 0.5 you keep getting the same same dog again so and they found out that this truncation trick was only possible because of that particular regularization and the reason was that if you don't condition if you don't use that regularization and you try the truncation trick these are the kind of samples you see cool so one more thing I want to point out is the when the big and paper is originally published they did not they weren't able to Train deep models the models were very wide they had a lot of parameters a lot of numbers like channel dimension in the cons but then the depth of the Gans were not much the architecture they adopted for the shallower version was to take the latent variable split it into four parts three parts here and feed them at different levels of the generator architecture so different residual blocks you can just think about think about it as in addition to getting the scale and offset parameters in the in in the condition of batch nom based on based on the class label you could also get them conditioned on the latent noise you can fuse them in multiple different ways so so basically they are feeding in the Z's at different levels of the generator and this non-local block is the self attention there so someone asked where where you actually feed it in we feed it in in the middle of when you get 64 cross 64 resolution and the class label is also fed in at different levels so this was the architecture they adopted for the original began but then they figured out that you can you can train a deep began or began deep as they call it by actually concatenated the zenith right at the beginning not seeing it but feeding the same thing everywhere and that seems to work modulo some other tricks like how you how you do the up sampling and down sampling so usually when you do the when you increase the number of channels or reduce the number of channels you use one cross one convolutions but in big and deep actually do it in a different way so if you want to reduce the number of channels they just pick the first half of the channels drop off the second half and there's no learning so and if you want to increase the number of channels they just take the existing channels and just predict how the additional number of channels from this using a 1 Cross 1 and concatenate them so they have to do these kind of tricks to make it work in addition to a different kind of conditioning and this is the architecture of began deep this is the generator this is how a generator block looks like so when you say rest block here this is how the rest block looks like in the generator and this is how the rest block looks like in the discriminator so this is what I meant by drop channels and this particular step here is the projection discriminator where you embed the class label take a you know product and add it to the linear version of the previous layer cool the key the key bits from this paper is that you have to keep increasing a bad-size how much of how much how much ever you can whatever it can fit on your processing unit and then increase your model size how wide and deep models fuse class information at different levels use hinge loss and the auto nominal regularization and truncation trick so let's see the numbers so this was Seguin the previous state of the art this was the F ID for Seguin this was the inception score and now you can see that the fitn inception score and began of a better so from 18.65 you dropped a 7.5 actually for the same resolution you drop to so if you if you just want to compare the same resolution the state of the art is seven point seven and the inception score increases to up to 120 166 or 200 depends on what you want and look at like the combination of both I think it's basically around 2.26 166 so so that's a big big difference between the previous architecture in this and it also works well at high resolutions one 128 256 512 and yeah so these are the bad size this is what I meant by training on a larger bad size so if you keep increasing the bad size you know you can think about it as having a large batch letting the discriminator having a large batch helps you to be aware of more collapse but it's not I mean the real reason is hard to say this is a very empirical progress because of the batch norm class condition of batch norm yeah a batch doesn't need to have all the samples from the same class so we're having embedding so these class for different samples and they're all fused in different ways that's some I sense averaging across all of them it's being aware of what's being fitted so these are the different samples you can get dog balls okay so finally I just want to mention the latest advancing gang called the style gang again there's a lot of tricks in this gang but the main thing the main shift from the previous can was that just like how you beat you saw that in the previous gang architectures the noises soon began the noise is being fed in at different levels everywhere so that's what this style can is also trying to do but instead of feeding in a single linear embedding of that everywhere they are basically having a neural net to run a run over the Z for a while and get a vector get a get a vector W for the Z and they feed this in two different levels along with some perturbation noise and they have something called adaptive instance normalization which you should check the paper out so this basically changes the gang architecture because the way you think about gano yours you have one Z and then there's a bunch of conclusions up convolutions up sampling and you get a sample but now it's different you have a neural net that's just operating overseas and whatever it processes is being fed at different levels along with some perturbative noises which is really adaptively added based on the batch so this is this is what they call as a style based generator because it turns out that these different levels can learn different styles top levels can learn some core styles and bottoms can learn some fine grain styles yeah at the top of the constant protocol oh that's just a some constant that could be learned or not learned it depends on the specifics of us implemented so the samples are just so good it's so good that people made websites like this person does not exist and every time you load it it will it'll have a sample from style gone so so yeah that's the state of Gann now you saw how like it's no longer like there's a classifier discriminator and all these so different architectural changes have made possible like huge advances a lot of tricks like spectral organization self attention and so on so if you want to do like some work on top of these things the best way to start is github you have to read the code for all these papers but note that all these require a lot of resources as well so big an was done on a deep mine on GPUs yeah that's I think at some point we like you can be negative about it but there's other positive ways to think about it too like way young laocoon says it is like even deep learning was like that for a while like you know like really nonlinearities will renew at some point and you'll consider a trick and residual connections Bashan organization and all these are all these were tricks and we're all adopting it as standard practices now so it could be that after a while we'd reduce to a small set of tricks that are becoming standard tools that everybody uses in terms of perceptual quality the advances are you cannot be you cannot ignore these advances like it it may turn out that density models are more principled but the Gann is way ahead in terms of sample quality yeah you can try to do more tricks I don't know usually the way these are written is like the people try hundreds of things and write what work dot but the nice thing about the began paper is the author also mentions what didn't work so it's a very good it's a very good people to read what's the discriminator was oh actually I'm not sure maybe you should check out the paper but I think it's a standard ok let me don't say anything I'm not super sure about it but as far as I know that generator is the one that had the architecture update you can you've seen the video right for sigh again yeah yeah I'm not aware of the ne ne can advance after Saigon so that's why we stopped here but it's likely that it happens in a few weeks or ones it's a trade-off between that and the computational computation aspect so you can't do it at large resolutions because you won't have enough memory to store that tension matrix so the it's only implement I don't know if you you can probably try to do some experiments on that but so far it seems like all the gang models are doing it at 64 cross 64 right in the middle I don't know if people have tried to do some modifications yeah it would make sense as long as you have enough computation you should do it but yeah you can try it you can try that experiment cool so I don't know like maybe like make began work without labels like that might be something like as in began level results but without relying on labels or like the thing that Peter Chen mentioned like having scenes with a lot of different objects can you can all these style Gann like architectures generate that it seems like phases bedrooms are doable but what if like there are four or five you moans in the scene or a human and a cat these kind of scenes can you do it without any inductive bias or do you need such object kind of like some discrete object latent spaces though those kind of directions are future also proud definitely like people will keep working on more tricks more stable guns cool that we can and generate a model part of the course and move on to like representation learning let's see alright are you tired of gentle models or sorry oh this course is deep unsupervised learning we do things other than generation as well so okay so now we look at something very different from what you've seen so far we so the title is non generated representation learning so what is the motivation to study this so you know that the image net success happened in 2012 where there was a big reduction from classical vision based features on the object detection benchmark or object classification benchmark so and then over successive years better architectures you know vgg ResNet and so on kept reducing it and like the other point it went lower than the human baseline human baseline is one on record party actually so he just memorized image net in a weekend and benchmark himself so now there is super small so however these relied on carefully curated million labels points a data set of such size that scale collected from humans so collected from the internet actually so that's not feasible if you want to like learn from raw samples and so the the motivation is that you want to be able to learn such features even without you even without good labels so if you have a large data set with good quality labels and and if you can train a large deep neural network like like success is more or less guaranteed that that's something that you can take for granted now but the point of this part is to make sure that you can do the same you can generate such high quality features without labels and how do we come up with these kind of object functions that allow you to do that is what we're going to look at I want to take you back to the cake again because you saw this in the first lecture and so supervised learning is the icing and R is the cherry but we're going to figure out how to do the sub supervised or unsupervised part and the way young Laocoon talks about it is predict one part of the input from its observed odd so imagine you have like say an image and you're trying to predict the color from the grayscale or the future from the past so basically you have some inputs you mask out some of them you're conditioned on the other and you try to predict it so that the slides are gained from yeah Nicole we're so there's so many different ways that you can do this predict the future from the past or recent past past from the future top from the bottom any anything you occlude and predict from the visible portion so if you can do this successfully then whatever features this helps you to learn are going to be general or something's representative of common sense so that's his hypothesis so today's lecture we are going to look at different cognitive principles that allow us to do this the first thing is reconstruct from a corrupted version which for which we see specific instances denoising auto-encoder then reconstruct missing portions image in painting predict one view from another we see how colorization is implemented and there are other visual common-sense tasks like for predicting relative patches jigsaw solving jigsaw puzzles in patches or predicting the orientation of a patch or doing some tracking from colorization these kind of ideas and next week we'll actually look at more like ideas which involve like using related entities and in neighboring portions like word avec CPC birthday etc so first denoising auto-encoders a basic idea is you corrupt your input you make it noisy to add different kinds of noise encode it and try to decode the actual non noisy version of it so this way you hope that the encoder figures out what's the actual signal present in your input and there's a figure taken from the original paper imagine if X is an M this digit so you can have different kinds of noise so you can just have a drop out on the input make it zero and have the encoder F theta and decoder G theta Prime and you just hope that the predicted Z matches the actual X B and the loss you adopt depends on the input nature so if it's M nest you use Sigma across entropy if it's some other continuous value data set you can use mean square error or Gaussian log likelihood and you can explore with different kinds of noises so you can have additive noise Gaussian additive noise or masking noise very fact a fraction of them are turned off for zero they also explored with this salt-and-pepper noise where a fraction of the elements is set to the minimum or maximum value of that particular input so this is how it looks like if you want to think about it as a manifold the two data points are lying on this manifold on this tangent plane the X tilde or the noise that you add has some particular probability distribution around X or some distortion and your decoder tries to bring back these distorted samples back to your original data manifold so this way you hope that this G theta Prime and F theta understand the structure of the manifold based on your distance for based on the distance of X tilde from the actual tangent to this plane it has to figure out how much to push it back so geometrically it has to understand the underlying structure of your data and one thing you could do in the loss function is you could prioritize the reconstruction of the corrupted dimensions so you saw that you could turn off some fraction of the elements to be to be nice out or to be set to zero over here so in the decoder part and you could make sure that you're having a higher coefficient for the terms that are actually noise and lower coefficient for the terms that are originally percent and and this way you emphasize to you emphasize the denoising part more than reconstructing what's already percent because reconstructing was already presented leads to the auto encoder objective and that could collapse to an identity function as long as you have enough capacity in your encoder and decoder so you want to avoid that so the paper was actually called stag denoising auto-encoder the reason is that it was inspired from that earlier work those days in layer wise free training so you would take an amnesty JIT figure out an encoder with the denoising auto-encoder objective now you take the first the encoder encoder representation of the input and you you don't noise the input anymore but you noise the encoder now so you run a you mask out some entities in your encoder or adds some different kind of noise to your encoder and then you learn a denoising auto-encoder on the encoder representation and that gives you the second level of the encoder and if you keep doing this successively you get the next levels of the encoder based on the denoising auto-encoder run on the previous level so this is what they call a stack by noticing or encoder and this is quite representative of work in those days of deep learning where people who trilayer why's free training of rpms so you can think about this as layer by spray training of the auto encoder so why would you do all this the hope is that if you do it for submission levels at some point you can just stop doing it and I had a linear classifier on top and use the just freeze this use the representation you got from the stag denoising auto-encoder and be able to learn a classifier so hope is that the stag denoising auto-encoder learn these latent variables that are representative of the class so these are experiments done in the original paper I don't yeah if you I'm sure I don't know if you are able to see these digit strokes here yeah so if you don't have any noise if you're not if you're running a regular auto encoder your filters are being very random but if you add the noise like say 25% of masking or 50% masking you start seeing some reasonable filters and the more noise you add the more global the filters are as in the more rigid strokes you can see compared to jizz random perturbations here and there so this is just zoom zoomed in for these different neurons and in terms of the classifier so you just after you train this tag denoising auto-encoder there are a bunch of different tasks that people constructed those days with emilis where they have the basic Emnes they had a rotated version of the M&S digits and one where they use a different background but the amnesty it's something that you saw on the first assignment and then other I mean combinations of all these things and it turns out that this tag denoising auto-encoder can perform really well compared to the models of those days like deep Boltzmann deep belief networks and RBF kernel SPMS and so on cool so the next cognitive principle is predict the missing pieces so this is a work done at Berkeley by a debug Pathak and other people and Ali O'Shea froze so the idea is you take an image let's say you mask out one portion of it and you're asked to predict the mask portion raster output the pixels corresponding to this white mask and this requires some kind of common sense because you want to make sure that whatever pixels you fill in here correspond to the window you see here and so on so and and so you just need to basically look at the background around these pixels and try to fill them fill the rest of the portion inside according to water it makes coherent sense so the ways implemented is you take the image there's a mask you can have a zero one it doesn't matter and then you encode it and the decoder outputs whatever pixels correspond to that particular patch that's hidden you have the true version of it and you have some loss function that can compare what your output and what is pre true and you back propagate that loss into the encoder and the decoder so that hopefully by the time you're done this encoder learned some good representations and you don't have to take the mask at the center portion of the image alone you can take masks at random locations and output all the random masks that operate all the locations that you masked out and if you have some supervision some other data set which has boundaries of actual humans or objects in its scene you could use them and use use use those masks instead of just taking rectangular masks and different locations the way you can implement it very easily if you want to implement these two versions is that just pick your mask output the entire image instead of just like one small version of it so you take whatever input resolution you take that would be the output of your decoder as well but you only must take the reconstruction loss or any other loss that you want you only have back propagate the losses corresponding the pixels that were masked out so the encoder takes in the unmasked pixels and the decoder masks out the pixels that you want a back propagate and in this paper day experiment with two different losses one is using a mean square error reconstruction loss and the other is a adversarial loss or the passive it's a genital adversarial network and the key engineering detail is that the discriminator takes in the entire image but only the generator takes in the masked version of the image that that was because if it turned out that for them nothing worked when they fed in the mass and the mass version to the discriminator as well and and and now they have a joint objective where there's a gain objective and the reconstruction of the likelihood objective and they adjust the hyper parameters such that it works and this is the architecture where the output of the decoder is fed into the discriminator as well in addition to having a reconstruction loss so if this is the input and this is the mascot version just using the l2 loss will give you a very blurry output here just using the adversarial loss introduces some random artifacts but using both of them and shows you get some reasonable in painting results here note that using some pixel level classification kind of objective as in a pixel CNN Woodworth clear much better than these things but I think at that time those were not very popular so in terms of representation learning they trained a bunch of on a bunch of images after training this context encoder objective they they tried to use the encoder and fine-tune it on image net on Pascal walk the text set as a test set and they are classification and detection results on Pascal walk and segmentation results on the paskevich 2012 so and and it seems like the the fine tuning works we welcome other baselines at the time like auto encoder and some other cells provide objectives which which we see and it also works in reasonable amount of pre-training time so end up learning reasonable representations cool so next is the cognitive principle of predicting one view from another actually denoising auto-encoder fits that principle too because you take the corrupted version you encode it then you predict the actual version so another way of looking at it is you take the raw data have different views of the raw data and try to predict one view from the other and so for example let's say you have the input image instead of the RGB color you let's say you could use the LAV color space so that's another color parameterization and it's very useful because the distance in the la be parameterization pixel wise distance is much more meaningful compared ad as compared to the RGB color space so let's say that you take the l channel separately and a b channel separately this is how the image would look like and you're gonna try to predict the AV channels from the l channel and hope hope that the encoder that does this and the encoder and decoder that they're doing this are going to learn good representations this is make sense so the actual image is looking like this and you're gonna try to predict the the a B from the L and again L to regression wouldn't really work here because they're going to be multiple possible a bi outputs for one input so you're just going to get a degenerate output here which is averaging across all possible outputs and but it turns out that if you use a classification objective where you if you take the a/b space take all the possible ranges in your data set bin bin them correspondingly and have categories for each output then if you just instead of using a mean square error loss if you use a la crosse entropy objective much better and this is in general true in deep learning as long as you can bend something you should been ving and run across and to be lost because the ground truth is like that right see a question is when using a factorize decoder just that the prediction for each pixel is independent of the other how does it still learn if you make sure that actually they have another trick in this paper to which I haven't mentioned here which is to make sure that you only you prioritize the predictions of infrequent pixels as in after you have been the a B values to certain specific number of Bin's you have priorities for the loss of the a B value based on how frequent is in your data set so maybe that that's actually helping them cool I think I'll stop here and continue from the rest next week 