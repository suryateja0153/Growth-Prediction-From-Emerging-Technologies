 Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In the last few years, the pace of progress in machine learning research has been staggering. Neural network-based learning algorithms are now able to look at an image and describe what’s seen in this image, or even better, the other way around, generating images from a written description. You see here a set of results from BigGAN, a state of the art image generation technique and marvel at the fact that all of these images are indeed synthetic. The GAN part of this technique abbreviates the term Generative Adversarial Network - this means a pair of neural networks that battle each over time to master a task, for instance, to generate realistic looking images when given a theme. These detailed images are great, but, what about generating video? With the Dual Video Discriminator GAN, DVD-GAN in short, DeepMind’s naming game is still as strong as ever, it is now possible to create longer and higher-resolution videos than was previously possible, the exact numbers are 256x256 in terms of resolution and 48 frames, which is about 2 seconds. It also learned the concept of changes in the camera view, zooming in on an object, and understands that if someone draws something with a pen, the ink has to remain on the paper unchanged. The Dual Discriminator part of the name reveals one of the key ideas of the paper. In a classical GAN, we have a discriminator network that looks at the images of the generator network and critiques them. As a result, this discriminator learns to tell fake and real images apart better, but at the same time, provides ample feedback for the generator neural network so it can come up with better images. In this work, we have not one, but two discriminators, one is called the spatial discriminator that looks at just one image and assesses how good it is structurally, while the second, temporal discriminator critiques the quality of the movement in these videos. This additional information provides better teaching for the generator, which will in return, be able to generate better videos for us. The paper contains all the details that you could possibly want to learn about this algorithm, in fact, let me give you two that I found to be particularly interesting: one, it does not get any additional information about where the foreground and the background is, and is able to leverage the learning capacity of these neural networks to learn these concepts by itself. And two, it does not generate the video frame by frame sequentially, but it creates the entire video in one go. That’s wild. Now, 256x256 is not a particularly high video resolution, but if you have been watching this series for a while, you are probably already saying that two more papers down the line, and we may be watching HD videos that are also longer than we have the patience to watch. And all this through the power of machine learning research. For now, let’s applaud DeepMind for this amazing paper, and I can’t wait to have a look at more results and see some followup works on it. What a time to be alive! Thanks for watching and for your generous support, and I'll see you next time! 