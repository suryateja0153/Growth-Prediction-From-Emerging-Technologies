 okay hey everybody can everyone hear me well okay so we'll be going through a whirlwind to a bit of a hands-on four channels of adversarial networks otherwise known as Ganz how we can make some pretty pictures from them so just to guide you through what we're going to be doing we'll have a little bit of an intuition of how cans actually work diving a little bit into how they really work a little bit on the math side but we'll cut it a bit short because everyone really wants to get into code we'll be doing a hands-on of how do we use Ganz to make new pictures and especially because Ganz are really really hard and often very tricky to train what are some tips and tricks to actually make them work for you if this is your first foray into Ganz and then finally if this is your first foray into Ganz where do you go from here because this is going to be the high-level introduction to this very big area so what again so ganzel introduced in 2014 by Ian good fellow who's currently now at Apple and in the last four years we've seen huge explosion in the number of application areas of Ganz but also the quality of things they're able to produce just in face generation alone we can see such hyper realistic images of faces being generated and that's just in four years granted that's fairly high computational cost for those who are interested in this I'd recommend going into a site that this face does not exist comm or for those cat lovers out there this cat does not exist comm other examples let's say we want to go from one domain of an image to another domain of an image we want to take a black-and-white image and we want to recolor eyes it we want to actually take an image that was taken in the day and make it look like it was taken at night oh we just want to draw an outline of a handbag and maybe produce an output that's something that we can now do maybe our really internal self really feels like we could make Monet paintings so we can take some photographs and make them look like a Monet or we can make a horse look like a zebra this is the horn I find most fascinating is given a textual description of an image we can now start generating actual images just from the textual description and you think of the possibilities of that and finally if the stock wouldn't be here without deep fakes maybe you just love putting Nicolas Cage on everything that's just your thing but we have to acknowledge what the fakes the obvious things like political implications that we can have from this and the possible harmful usage of this technology however we must note that cans aren't only used for images they can be used for all sorts of data whether it be generating new voices whether it be images where they be music any sorts of data you can decide but we'll get into that so what are germs of adversarial networks and what are these things that make all of these cool things super possible and the goal of what can is fundamentally how do we generate new data that was never ever seen before and to break it down what does Dan actually mean by generative we mean we're gonna be learning some sort of generative model adversarial Methos model was trained in an adversarial setting and finally we will use deep neural networks to actually do this so the stalk will make some sort of assumptions that you vaguely know bit about deep learning if you've heard of words like backpropagation convolutions loss functions maybe batch norm you'll be ok but if you haven't that's also ok I've provided a notebook on that link that I've showed up where you can actually go after the stalk and really run through it and you'll get a good feel of how it actually works I'll just show you some tips and tricks to what to look out for when you run through that notebook so what is this word generative it sounds a bit weird so in most deep learning what we deal with our discriminative models let's say we've got labels many sets of images let's say like cats and dogs and that's out data X and we have labels which are obviously cat and dog and we want to learn a model to predict whether something is in fact a cat or dog and that's a discriminative model we learn a conditional probability which is given our label how do we given X how do we predict Y so probability of Y given X so in a generative model we don't actually care about our labels at all and what does that mean by this we actually want to learn how the data was generated in the first place so you might be saying okay that sounds cool why does it even matter why we care about where the data comes from because if we know where the data comes from we can generate new examples of the data and for those slightly mathematically inclined that'll be you if we have a probability distribution we can sample from that distribution to make new samples of data I cover it a bit more in the notebook that is linked on github but just because we don't have labels doesn't mean that all unsupervised learning is generative that's something that's nice to point out and everyone should be aware of because something like k-means clustering which is what everyone thinks about for unsupervised learning is definitely not a generative model so what does adversarial mean so at a high level a chance of adversarial Network has two sets of neural networks one called the generator and one called the discriminator so the goal of the generator is to produce new data for this new data that's never been seen before and the goal of the discriminator is to decide whether this data was in fact real was it faking made up and so before I got into this whole area of computer vision and data science maybe I thought I could actually make a bit of money and why would I need to work if I can just fake a Mona Lisa so I decided let me give painting ago I make a painting and I show it to an art evaluator and you get obviously in the beginning tell me that this is pretty fake but it gives me some feedback of what a real Mona Lisa really should look like and by the next time I get slightly better but it's still pretty fake and the evaluator who's the discriminator network can actually give me feedback on what am I doing wrong and after I do this many many many times I kind of get pretty good that he can't actually tell the difference between the real thing that I've just made and the fake and the real image of the Mona Lisa itself and that's a high level intuition of what a charity of adversarial Network is doing it's these two networks which are dueling against each other one trying the generator trying to fool the discriminator in the discriminator trying not to be fooled itself and that's how the network gets better and better at making such hyper realistic images so how do they actually work like what are in these little black boxes of generator and discriminator so we'll be using something called the deep convolutional again or DC again you can go there I've linked it it's to a nice little post about it so the first thing is the generator like how do we actually go about producing these images we basically almost start from nothing well not absolutely nothing just a little bit of noise and what we do is we pass it through a neural network to produce an image for those who do know convolutional neural networks you might say this is the wrong way around everything is just opposite to each other well not entirely because we is something called transpose convolutions for those who have dabbled in imaging it's if you can think of up sampling it's something very similar to that but instead of just pure up sampling we use a kernel which we learn through training to actually do that up sampling to generate a new image and at the output that's where we generate that cool pretty picture the discriminator on the other hand is just a normal convolutional neural network it takes in our image with a goal of deciding whether something is real and we give it a label 1 or if it's fake and we give it a label 0 so this will be the only slide that is a bit mathy and ugly looking but we'll go through it very quickly and I'll leave it here just for more people if you want to dig into it a bit later so the fundamental thing in all deep learning and neural network training is defining a loss function and this loss function is what are we doing to actually allow the network to train so in a generative adversarial Network we something called a many max loss function and that's defined by that equation and what do I mean by that the generator tries to minimize the reward that our discriminator gets by fooling it and our discriminator tries to maximize its own rewards by never actually being fooled ever by correctly predicting if something is real and something is fake that's a very ugly equation that is there I won't dive too much into it and I've shown on the slide things that people can dig into afterwards because I don't think this is the ultimate purpose of the talk but for those interested they can go in and look in what this loss function actually means and how we use it and what are some fundamental problems that we have with this loss function for more advanced applications but I think this is what everyone actually really wants to do is how do we actually build these things how do we actually train something and you can follow along on github on that link and there's a self-contained tutorial notebook which links to Google Holub you can run a free GPU there and it's very cool what are we actually going to be generating and it's those pictures which come from fashion amnesty so you might be saying when you showed me all of these hyper-realistic faces in the beginning of this presentation why aren't we just doing that and well those are pretty cool they do take a lot of GPU power and it might take you a really long time to train maybe a couple of hours so I thought it's probably this a bit better have something that you can actually run and do give you outputs in five minutes just to give you a head start so the first thing let's assume we've loaded in our data we've loaded in fashion in list this is a total necessary evil of any deep learning system is we need a preprocessor theta I get it in a nice format whether it be appending another axis because we've got two dimensions but we'll put three dimensions on it in case we want to ever use color images we want to make sure we have floats and we want to sent out data between minus 1 and 1 and what does that mean is we just 0 sent to the data with a unit variance because it makes our network train a bit better at a high level so the next couple of slides are more just set up of the actual network structures and for those who are interested they can go in and look at the code what I want to point out is what are the tips and tricks that really make this work and I'll be high like that so you know that it is a two-pin trick that really will help you make new images so the discriminator is basically just a four layer convolutional neural network and what are the things that actually make this run really well so the first thing is instead of radio activation functions we use leaky radio activation functions what that means is that we just never send weights to 0 outputs to zero because the Nikkei value isn't at zero the second being is that this is from the actual authors of the DC gain paper is that we wouldn't do average pooling we'd rather do something like a strided convolution and what is the strided convolution it just we skip pixels every time we do a convolution we do it on every second pixel in the third being batch normalization just makes us train a lot better and at a high level batch normalization just means through every single convolutional layer we just renormalize our data so that we train a lot faster so the generator also has a pretty nice structure it's fairly simple two layers of convolutions but what I wanted to point out is what is transpose convolution and that's represented by those two as I talked about it's literally just up sampling and doing convolution so no difference since the flow does actually have a function for pure transpose convolution but I thought this is a better way to represent it especially for your first time to know exactly what's going on and the second being you know just you know generator network we want to use a radio activation function but the last thing is we want to use hyperbolic tangent so that our data of our image that pops out actually has arranged between minus 1 and 1 because that's what our other network expects it to have once again this is the last bit of setup before we actually get into the real meat of things and this is loss functions so as I showed you that big loss function there's a very ugly looking equation but you can see in code it's not too bad looking so the first thing we want to define is that in binary classifier where we have in this case it's real and fake will use the normal loss function which is binary cross-entropy but for the discriminator because it takes in real images and it takes fake images we need to classify a loss on our real images and what is the loss for fake images and add these two together the generator loss is way easier it's literally just binary cross-entropy but I think this is the nice off but of the slide it really gives you a nice intuition of what is going on so there's actually about seven steps that we just repeat over and over and over to make these new images so the first thing is we do we create something called the latent sample which is just noise and how do we do that in code we just draw from a normal distribution and make a latent sample and that's our step number one now step number two is we take that noise and we stick it through this generator network and we pop out an image that doesn't seem too bad it's another line of code where we take that we pop it through that generator that we've just defined can we get our generated images it's step number three we then take these generated images and we take real images and we pass it through the discriminator to say hey are they which are the real ones and which are the fake ones once again this is like two lines of code we take our images and we get outputs for real and we get outputs for the fake then like any neural network we then want to calculate our loss which is basically at a high level what mistakes have we kind of made and how should we update our network to not make those same mistakes ever again so that there means we just compute our loss and that's passing it through those functions that we general with that I showed you a bit before once we've done all once we've calculated our loss remember we're doing this for many batches so for small amounts of data and we then calculate a mean for every set of data that we've generated once we do that something called back prop happens so what do we do is we update the weights on our discriminator and then you might say okay wait a minute the output of the discriminator is just going straight into the generator so what that means is that the discriminator is effectively the loss for the generator and i have linked it again in a notebook it's something pretty cool and for people to explore later let's approximate something called the Jensen Shannon divergence it's something pretty cool you can go read about it more in the notebook and then we use that to update our generator so both are slightly getting better and better and better at every iteration the generator is getting better at making images and the discriminator is getting better at not being fooled over and over after we do that we know how to back we perform the back prop using literally one function doing ten with the tensorflow gradient which is just doing automatic differentiation and finally we then apply our optimizer which in this case was Adam you'll note it in a notebook you can run through that so if we run this many many many times what is actually happening so we sort of like that we're just feeling a random noise so obviously we're gonna probably get a bit of random noise out anyway at the end but let's say we go through the next step clearly something's happening but this is the bit of garbage to be honest but let it go on for a bit more mmm we start seeing actually some sort of structure to our data we start seeing maybe there's an outline of a shoe maybe there's an outline of a shirt let it go even just for twinning epochs which is about four to five minutes the network knew nothing but noise and after five minutes it knows to generate those pictures and if we let that go even longer our pictures would become much more crisp and in the notebook you can run it probably around fifty epochs does they're very nice then you get some pretty nice crisp looking images so this is probably the most high level of manila introduction to Ganz they don't train such good things because there are some specific very deep problems that we'd need to address if we really want to make this work but if you say okay it's fine I know vaguely how they work I just want to use these things that's where we'll go to some fun examples so what are the deeper problems that this basic gang is not addressing and why can't we apply it just for everything off the bat so the first thing is sometimes these things just don't converge because our discriminator might be pretty good it might be able to figure out if something's real or fake and the solution to that is you just add a bit of noise to it confuse it a bit I've linked to an article there which explains it nicely to you the second might be something called a vanishing gradient what this means is that our discriminator is also really really good which means when we perform that back propagation step to update the weights we're not actually updating anything because it is actually really really good already and so we have no gradient flow into our generator so it's basically stuck you can say in a local optima so we have an issue with this so we can use something called the watch the screen loss and it sounds really hectic but if you go look at that link you'll see the equation is way way nicer than the one I showed on on that slide of loss functions so it's something worth definitely checking out finally it's something called mode collapse so we would want to create many different instances of new data if we just created pictures of shoes nobody would be really happy by that but that's the problem with mode collapse the network can start learning to produce only certain types of examples over and over and over no matter what we do and this happens because we get stuck in a local optima and the network decides this is a good way to exploit it because it's being really successful at not being detected as being real or fake and that's something called mode collapse and come here once again I love you alleviated by something called the wast esteem loss which is probably the easiest way or something called unreal can and you can go check out the links for it over there and finally let's say you're like okay this sounds cool but I don't have GPU power I don't want to sign up for AWS and pay for GPU power and I don't want to spend 4 hours or a day training this hyper-realistic can because to be honest nobody got time for that but that's why the open source community is really really nice because many other people have spent their Amazon credits and both many many games for you for many many different applications and you can go check out this git repository and all that it will do is you'll be able to download that generator Network and have it by itself and pop out new examples from the generator and pretend we didn't even care that this discriminator network even existed in the first place so it's nice that you know how they work but we rather let somebody else do a lot of the hard work and heavy lifting for us if we really want to do face aging which many of these applications are doing right now which has been very popular whether we want to generate text to image or whether we want to do image to image translation please I highly recommend going to check out this git repository it's really good I didn't want to leave some time for questions so please feel free so yes so they both actually very different so so the so the first one when I mean that the discriminator gets too good its determining whether something is real and something's fake is something just too easy especially at the beginning of training is that our network isn't really good because it's a really hard problem to generate data and the second one unfinished ingredients that typically happens when halfway through training we might go halfway through training and we don't have any incremental change to our generator like we're not getting belt better on our generator fast enough and our discriminator gets really good at saying ok we have the middle point of training so I don't need to keep updating my weights anymore so those are two separate but it's a different stages of the training yeah yeah that's one is it that one okay so in this specific example because the data itself is quite simplistic we won't be noticing it as much but for example in the face example what the network is then learning when it's doing this whole noise to the step it's learning that let's say eyes should be located near foreheads and noses should be below eyes so if we learn all the features of say different noses and different eyes and we knowing how to locate them we then able to have this probability distribution which is what we've learnt of the data so we can cycle through it to change structures so we can have the same person's nose but with like say a different set of eyes it's just the Sun I guess on a very simplistic example we're not going to be able to notice it very specifically because we don't have like very high-rez features to it so in terms of like actually activating it so ultimately that generator it's inside is almost once it's done is a probability distribution so depending on what noise we stick into it it will activate different things in the network and that's why we get random outputs of different looking pictures okay so I'm the ground truth so if you look was I think it was actually here so it's over here so when we say these are actual real images we say we create a vector of ones and we say all of these images that are coming in now are the real images and we put a vector of zeroes and would say all of these images that are coming in now a fake and then that gives it the ground truth so we first feed in real and then we feed in the fake okay so there has actually been I probably didn't link it but there has been originally even encode fellow someone even asked and read it and he replied in 2014 that canopy used to generate new text and he said no but in the last year so there has been research work but it's still ongoing research it's not like as good as the image sort of stuff sorry not that I can think of so typically cans would be if I were to break it down so I've shown a lot of examples I guess of imagery but especially in terms of the data that's typically used it's very high dimensional data where it's image mu like audio or even I guess text but additionally I guess in the finance industry the issue would be is that you almost because again is hallucinating data that actually doesn't exist out wonder the peak ability of hallucinating data in finance okay so if it was more high-dimensional how you tweak it is actually by changing how we generate this latent sample here so currently it's actually just drawn from a Gaussian distribution but if we tweak those parameters of what noise we feed in we can then start learning of what set of the noise activates what in the network to generate what set of features so it's basically you just tweak what you feed into the generator the case of any last questions in the last minute but if not please feel free to go find this it's a full notebook linked to Google code up here as well as the slides at this link here it takes you to github and it's way more detailed and you can actually run through it it shows you what sort of parameters to tweak and you can see how outputs change based on that so I highly recommend it because with anything in code seeing it on a screen and playing with it in real life make a big difference [Applause] 