 [Music] hello and welcome back in the next series of lectures we will look at generative models especially deep generative models and in this video we'll just take a look at what they mean and what they are used for so it's a brief outline of this lecture so we have some background to cover and see what a generative model is and why we need them and the types of generative models we'll be covering as part of this course okay so so so far you have looked at problems where in we have this pairs available x and y where x is the data it could be multi-dimensional data images whatever a form of data that you are having that you have with you and the label associated with it so typically task is a classification task yes or no or like we saw an image net data you have thousand classes so you have a input X and the corresponding label is given so these are label data sets and what we use the deep neural networks for is to learn the mapping from X to Y ok so X is the input and we have a deep neural network which processes that input and outputs a probability score which we in threshold and make that into a class label right so this is in the context of a supervised learning this is what we have seen so another way of looking at it is that the the deep neural network defines a classification boundary like shown here in this graph area we have two features feature 1 and feature two this could be some arbitrary numbers after scaling and the red and the blue dots or the two are the data sets or the data points corresponding to the two different classes and what the neural network does is to determine this boundary if you can look at it that way and for which we have access to both the data as well as the label we have also seen that you know unsupervised learning wherein there is no Associated label so Y is not known right why is not given and what we try to do here in this situation is to learn some underlying structure in the data all right so that falls under unsupervised learning some of the techniques we can also call them clustering techniques were typically used in that context so for instance k-means clustering where in which is just specify the number of clusters that you hope to find in the data and we end up with labeling accordingly right so in this case we have again two features and what k-means algorithm does is - if you if you think that there are two clusters present in this data and the k-means algorithm determines the center of those clusters right and is able to label each of the data points based on the distance from the cluster centers as being - as belonging to either either one class or the other okay so here we are just trying to determine some underlying structure to the data all right and we don't have access to the labels of the data we just have the raw data available and we try to find someone laying structure so in this context the clustering is whatever you've seen feature learning or density estimation all these problems fall under unsupervised learning so so part of this unsupervised learning is the density estimation problem okay so it's a it's it's false and unsupervised learning and the idea is to determine probability density model for a data from which you can draw new samples okay and you determine that probability density your model density from the given data density so you have given data X so this is in this case let's say that you have n data points 1 to n data points and so that is basically X I are given which is an approximation to your from which you can get an empirical density right that is P date of X okay so we don't know the true P data of x for which we require infinite number of points are all possible XS that you can get your hands on or or that exist which is not possible so you with the data available you can give an empirical density estimate of the given data but we what we want is to determine a model from which we can sample the data points okay so we just basically generate new samples so they're handsome word generator is the terminology generative models right so given probability so given this data point so to rephrase so given these data points okay so again we would not have access to the labels per se so most of the time we just have the raw data and we have the index data and what we want to do is to figure out or have a model for the underlying probability distribution which is that's what we want so that we can draw new samples from it which are similar to our training data okay so we assume that our training data is representative of the problem that we are trying to solve okay so in what console is this useful and what do we do with it so before you proceed there we can also there's another way to look at it which is to say that the generative models learn the drawing joint probability distribution P of X given Y X comma Y whereas the U our classifier for instance right you are if you even if you are using a deep neural network to Gen the classified data into one of a K classes then the output of the neural network the scores that are output by neural network can be interpreted as the posterior probability of the class given the data so that's P of Y given X okay so that's say so that's not the generative model right so what we actually want is this of course these are the these are related through Bayes rule here right so if you have a good model for our prior P of X and we can actually figure this so this is one way of looking at it so what we are this so this is what is called a discriminative models right these are called discriminative models now in the context of classification so given what is the probability of the label given the data that's what a typical machine learning algorithm outputs so do most of your deep neural networks that you use for classification but we do seek something of this sort where we have a where we actually want to model the underlying data distribution that is P of X typically is what we want to get so why do we need generative models okay so let's say basically we are now trying to look at what kind of applications can they possibly serve so the most common application is they can generate new nice pictures right you can also use them for image to image translations are shown in these examples or for instance figure them labels or outlines can you generate new data so for instance blackton to transform black and white to color so you give the aerial map of a city can give you this you know this kind of an output like you see in Google Maps or difference between night and day here day and it's the same in night time and if you give labels once again if you train the model so that if you give it an outline you can give a output that you seek since it's a new design for a handbag if you can think of it that way so there is aerial to map you know labels to street scenes labels of facades and black and white the color so these are some neat cool applications that you can think of and if you say in the generative models right so but if you but the more deeper applications for instance generating speech from text right so if you have say programs like an automated a system that responds to your phone call that's the kind of program that it would acquire so it has some takes distort in the digital format and should be converted to a speech signal in order to respond to a caller right so for raw audio okay so you would like to have a generative model if you think of it generating sequences of text so here is one example but you can if you think of it a simple application is you know the autocomplete in your your cellphone's or when you're writing mail mail programs that you're using might have that so if you type a few words you should be able to complete the word or you may be you in the sentence that is also a generative model right so we can so those are some of the more obvious practical applications of generative models okay another application would be super resolution since you are given so it this can think of it two different ways like so here you are given a low resolution data and ideally you want the code to generate a high resolution and data of the same you can also think of image in painting so wherein if there are some holes in the images or some damaged areas in the image you would like to remove that damage so if you have a generative model should be able to figure out what the missing data is so can think of it for as a data imputation problem also alright so these are some of the you know very high impact or also some first-order application for generative and resilient networks okay so in the context of medical imaging this also has it's very powerful uses so some some immediate one set has also been reported in literature is so far for medical image replication so you not only need patient data that has to be obtained to it patient consent and there's a lot of you know work that has to be done in order to make sure that you are sampling the correct way of patient population and all that so given that it's hard to obtain labeled patient data right so in fact just to obtain equation date and then of course you also have to label them that cigarettes and it's even more difficult problem so that he can train a classifier but if it would be nice if we can actually generate data that is generate images of anatomy so you have let's say brain tumors right so you want to analyze brain tumors or liver liver tumors it'd be nice if you can have a generative algorithm for generating images of the liver or the brain even regular Anatomy you have to be nice so that we can actually do at least anatomical level segmentation okay so that will be a very immediate application for generative models there's another neat application would be image to image translation here it's it might this is a little bit complicated from the you are trying to understand this so in many K in many situations you know you will have a lot of data for a particular modality so let's say CT images of the liver okay they were quite a few scans would be easily available so large number of even unlabeled data is easily available right so then you can train a model to what our diagnostic tasks that you wish to complete right let's say CT images of the liver you have used CT images of the liver to let's say segment the liver right just to mark the anatomy of the liver well then but then the new MRI scanner comes in and you're starting to take a more images of the liver which are generally maybe not so easily available right so do not a trained a supervised classifier for liver segmentation from M are images of the liver you need a lot more data which might not be available then it'd be nice so a generative model in this context can be used to convert or translate the MRI image to a CT image use the CD image segmentation model and then transfer the segmentation mask onto the MRI image so that's that's a possibility so you have you have spent it so you your train an expensive model with a lot of data and a very similar problem comes along it'll be nice if we can use that train model where you know spend a lot of time and resources doing that all right of course this doesn't work for everything so just to for those who are not in the medical imaging field so you cannot train you know their CT images of the head let's say you have a network that does some Diagnostics diagnostic task on CT images of the head and then you cannot take em are images of the liver and use that network without of course you can do fine tuning and then change the Nets different times for learning problem we cannot directly use that network there's no image to image translation here you have to use use it in the correct context so here I'm looking at city images of the liver and MRI images of the liver then it's more meaningful to do that of course these are slightly more research-oriented applications but very high impact if they are solved so it's not only in this context but there are various other contexts in which again we're in generative models are used especially in medical imaging tasks where they're very useful and especially in scenarios maybe where you have I don't have too much date down this might be this might come in handy so okay so what are the types of generative models that we will cover right so we will go into detail some a couple of these techniques are not all of them and we'll show how they can be used to generate data so most of the applications we'll be looking at would involve generating images okay that's that see most common application so far we won't look at text or speech but just generating images so that context there are different types of generative models so this is one of the more highly cited model states the pixel RNN or pixel scene and they're called they are based on what are called auto regressive models okay so they are called explicit so they explicitly determine the probabilistic the probability density of the data okay so they call Auto regressive models because they if you think of an image you try to generate a particular pixel based on the pixels you have looked at so far so if you raster the image row by row or column by column try to predict a pixel which is in the second column by considering the pixels in the first column of the image that's a Steve basic principle so this comes under auto regressive models and it gives you an explicit density which is basically the neural network itself the deep neural network is we the density is modeled using the deep neural network alright so just to give you an idea so for instance if you are working with some simple data you can model that data using a let's say a Gaussian right so so your density would be something of this sort okay right they of this sort so instead of this we will we'll just have some f of X which is basically nothing but a deep neural network okay so in most of the applications most of the generative models using deep neural networks the idea is to model P of X with the Dib neural network itself so the network is the model okay so another class of generative models which also explicitly determine density is the latent variable models one of them the more highly cited work is the variation lata encoder so we will cover this in the next few videos so where is not to encode us again is there they are it's called variational auto encoder because there is a it's it's an approximate way of determining the density function right so that's one and we'll see why it's called latent variable model when we actually look at the algorithm in detail the of course the most studied model in the recent past is the generative adversarial Network okay here we do not explicitly model the density the probability density instead we just sample from it directly okay so the neural network outputs the sample all right so directly outputs the sample so that's the idea behind counts and again the DISA deep neural network and primarily has been used for generating images so in fact I haven't seen any application otherwise but it's the more popular applications are for generating images and there have been very recent successes for generating images of human faces and they look very realistic so you can look them up if you do a search so we will primarily focus on VA ease and Gant's okay so variation auto encoder and generative address cell networks will be the two generative models that we will focus on so to someone so generative models try to model the underlying data distribution so data is basically your training data so which is we want to figure out this we want this where X say training data right so that we can draw samples from the probability distribution which looks like your training data so that's how you should interpret generative models so once once that is that Cup once once you have that model and there are a lot of applications that are possible that's the ones I showed you and they can also be used as classifier so they can also be used in the context of classifying of course it's much more straightforward to if you have label data it's much more straightforward to train the classifier directly using a deep neural network so but this is it's a possibility to do that using the model that you have figured out okay so we will continue with this in the next few lectures we look at variation autoencoders and generative adversarial networks thank you 