 hi I'm Sam I'm a software engineer on a our core Google's platform for building augmented reality experiences I'll be showing you how to implement face effects using the Augmented faces API in AR core for iOS links to Android and unity versions of this video can be found in the description augmented faces works without a depth sensor so it can run on any iOS device 6s and above it uses the phone's camera and machine learning to provide three pieces of information synchronized with the camera feed first a sinner pose for the face which helps you do things like a render a hat second region poses which are useful for placing assets on honor near the forehead and nose and third a 468 point 3d face mesh which allows you to paint detailed textures that accurately follow facial movements in this video I will show you how to install the Augmented faces cocoapod I will then show you how to initialize an Augmented face session that consumes avfoundation camera images and I will show you how to attach 2d textures and 3d objects to the face the AR core SDK is distributed using cocoapods to integrate augmented faces in your project simply include the Augmented faces sub spec in your pod file to integrate with our API you need to initialize an Augmented faces session this object is responsible for taking in camera images at 60fps and will a synchronously return face updates to a delegate method when initializing simply pass the capture devices field of view and make sure you set the delegate now that your session is initialized and configured properly we can start sending camera images to the session in this application we have chosen to get camera images by creating an a/v capture session with video frames from the front camera here you can see our implementation of Av foundation's delegate method which passes the image a timestamp and a recognition rotation to our face session after the image is processed our API sends a delegate callback that gives you a GA our Augmented face frame it contains an Augmented face object which helps you attach effects to the face it also contains the image buffer in the time stamp that you passed into the update method this is useful for synchronizing the face effects to the images this object also gives you a display transform and a projection matrix to make sure you can set up the 3d world and 2d views in a way that makes it easy to render your face effects so now that everything is hooked up let's talk about the effects you can create first I will show you how to apply a 2d texture to the face using scene kit let's take a look at some of the sample code our sample app provides a class to convert our augmented face to an FCN geometry object you can use this geometry to create a scene kit node which you will place at the Augmented faces center transform to add a texture simply create an SEM material like the one shown and add it to the node finally let's add some 3d objects to the face the GA our Augmented face provides three different bones or transforms you can use for attaching content to the face these transforms allow you to get the nose left to the forehead and right of the forehead relative to the devices camera you can see here some sample code where we use the nose transform to a touch of sphere to the nose we do this by getting the nose transform every frame and updating the node with that transform now you have everything you need to get started with air cours augmented faces API you'll be able to use your own assets to create custom face effects for more information about this tutorial and AR core in general you can check out our sample app source code and documentation in the description below we can't wait to see what you build and hope you share it using the air core hashtag thanks for watching you 