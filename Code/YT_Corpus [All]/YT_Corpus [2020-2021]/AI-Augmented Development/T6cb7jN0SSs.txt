 [Music] okay so let's start hi everyone thanks for joining us today and I'm so excited to be here with you today we're from DNA which is one of the largest game publisher in Japan and today we are going to share our implementation on gaming so far we have released to AI applications and we're going to talk about the motivation and the implementation and the outcomes of these projects my name is Jim Ernesto and I'm leading this project also let me introduce each Tanaka he's a data scientist and built our models in this part both of us were present so to be you is this work has been accomplished by for machine learning engineers and a lot of collaborators from gaming team each of us our own area of expertise and I really love this team so they start I think most of you are not so familiar with DNA our company is one of the largest Internet company in Japan although we have various business domains such as game automotive and life science on sports most of the rain comes from games actually we're providing dozens of in-house game titles and other publishers titles and maybe some of you may know that we have an alliance with Nintendo for example our company is in charge of the parking system of Super Mario run so this is who you are so intercessions we will talk we focus on the house game title octane o Soniya so first I'd like to talk about the program we had and how we challenge to solve these issues so this game jock day o Soniya is a strategy up game the players action is based on the board game called author or reverse e and the player put black on white pieces alternatively and what is unique about this game here is that each piece has its own skills which had strategic components to the bottle actually when you start a play in the game there are parties of play stars and the entire endless exploration is required to master the game so this title was released three years ago and the fortunately it's still growing we have more than 24 million dollars so far and the service region is expanding gradually of course I hope in the future we can release it in the US as well so in awe sironia there is the concept of text and characters so before starting the bottle player a construct deck which is composed of 16 characters among more than 3,000 options and each characters had its own skills and the players have to choose consistent chapters according to their play styles as you can imagine the number of the combination is extreme and it's it it's very hard for to learn how to build optimal decks another issue here is that building an optimal deck requires two long effective strategies through the process of trial and error so let me explain why this is a problem so generally speaking if the game is complex game has complex features which can distract players from continuing the game the churn rate increases and impacts the future revenue right and for example there's a nice to mention that just a few points decrease in the 30-day generate can increase the revenue by 20 to 30 percent in the future so back to Sonya we actually have a high Chum rate so we have a trouble here and this is partly because the game is complex and lacks sufficient Oh Bernie support and therefore it's hard to foreigners to construct the optical deck all right so also there is a problem that they don't have a good place for practice in order to improve their play skills for instance we only have two bottle options for wine NPC bottle which they battle against very very weak opponent and the other is a PvP battle which they are frequently matched against a very skill or experience players so we needed in opponent that whose play level is similar to the winners so here's our approach to support we have provided two functions the first one is the deck recommendation system which selects appropriate characters from players piece books and we are using Association analysis to provide a better recommendation the second one is the bottle AI which we call AUSA Lunia dojo and in these functions we have developed AI that can play at any levels a deep neural network is used for DCI later he will explain how these techniques are used in our cases okay so by the way from the machine learning perspective we need to provide prepared data to build such a eyes we are creating various kinds of in-game data since the release of the game such as tech logs and battle logs so we had a good amount of data when we started this project and in our cases we used data for the last nine masses which contains billions of log entries then we filter the data by players skills and augmented the original data in the battle area cases the image on the right side is an example of battle logs which is formatted in JSON and describes interstate and action information of the bottle and the bottom diagram shows rough data pipelines and bigquery did a very good job at this pre-processing phase for instance it can handle even terabyte order of data very fast and we could reduce the processing time drastically big you could handle our data in a few minutes and well compared with Ione Prem how to Hadoop clusters that took like an hour so yeah we are very very happy here and we also love that BQ is easy to collaborate with an other GCT services and here I want to stress that the array has been successfully built by sufficient amount of data and the reliable pipeline so let's see how this how each of these use cases works this is a rough overview of the accomodation system so we first exploit purposes deck logs to extract information about how each characters are associated which we call Association rules with this analysis we can score the strengths of the relationship across the old Apparel's chart chapters if there is a request from the player the recommendation server responds I mean the set of receive data set of characters in the players base box and it fills all the 16 characters according to the score of Association rules the function this function is very flexible and the player can specify that archetype and the cost and also the favorite chapters to be included in the deck regarding KPIs that we're monitoring several metrics for instance we're watching how many players have used this recommendation system and whether the recommended deck has been accepted or whether the deck contributed to the players in terms of win rate at battles asked us for the dojo player can battle against AI with spurious archetype Sunday strength so here we trained a deep neural network from pre-processed but logs the clients and current the game status and the bottle API server returns the inference of new networks at every turn of the bubble and the apps as for the matrix we're monitoring how frequently that dojo is used for and whether the strength of AI was sufficient internals win rate so these two guys have already been released and the we observing plotted the KPIs and before showing the results I want to pass the talk to Ikki and he'll talk about technical details and some tips for building AI hi everyone I'm Miki and the data scientist at DNA so I'm one of the developers that created our gaming system by using machine learning and data science so today I will talk about the technical details of our use cases with TCP so now let's look at the story of the technical part first I'd like to start by the tech recommendation this is ultra of the document the algorithm faster is the algorithm receives some input from a prayer such as a tech archetype that cost the possessed characters and characters the player wants to use next algorithm select a leader by using the result of Association analysis and statistical data in this algorithm the leader character is chosen first and the next character is next characters are secretary based on the leader the associated characters are selected simply simply selected based on the result of Association analysis if the deck is not have 16 characters the another character is circular to base the character as a base character after repeating this procedure several times the recommend deck is proposed to the prayer if the game is different the way of fine tuning is also different in a game cocktail sironia for example we made additional rules to decide a leader and we ignore the characters who the skill will be never activated in the deck a directive gives a brief explanation about how to extract the relationships between characters in our recommendation we are using the Association and nurses the Association Isis is is is a rule-based machine learning method for discovering meaningful relations between contents in large dataset this method can perform high-speed calculation to extract some many rules and extract relations are easy to understand and useful in various settings the relations are evaluated with some metrics exact like the strengths of the connections and frequency of the rules and patterns our recommendation with the deck by selecting the character sequentially with strong relationships one after another this method can handle the various kind of data easily so this technique is applicable to many games and other fields next I will talk about the system so we with our applications like the clock the commendation and divining a bot with TCP so why did we choose this EP for machine learning systems there are two main reasons the first one that the DCP has powerful and flexible tools for machine learning and data science in the data analysis we often try to investigate the data visualize it and create the prototype models the disapear services for the big data analysis make it possible for us such as bigquery equal data lab and Google compute engine and the crud of storage provides a scalable and the fastest service for large data and it helps and it helps us to manage l managed cervical data and a lot on machine learning models about Krauthammer engine this we we can replace air models to create a machine learning engine easily and quickly and it also provides a user-friendly api's additionally disappear by the many services for to perform the machine learning efficiently for example we are using we are trying to use gk control service and quad range in training with custom functions for scarab and distributed table trainings ii wise that the DCP has scalable and robust infrastructure services we can run applications easily by using google app engine with several programming languages like Python and code language and we can check many metrics truck driver logging and others a direct emphasis that the auto scaling of the Google App Engine it's very well serviced because new instances are increased was stopped in response to traffic and request are distributed to the instances automatically these disappear services are easy to integrate into the same system moreover also I don't touch on the product details Google Cloud brings many advantages to our business like the security equation and openness for these many reasons we are choosing DCP for Emeril systems next I will show the system shows us I will show the system architecture of the documentation with DCP that the clocks are saved in bigquery which are used to extract the rules we set across scheduled to perform about processing of the association analysis in Google compute engine and in order to catch up the trendex with latest characters we are performing the processing everyday the include functions processes result Association analysis and appraise the data for production environment in GCS call the function checks if the data is a plated correctly for example is the dead sizes too small or large then we are using Google App Engine Python 3.7 for Luca MIDI API server the server receives a request from the client then build a deck using analyzer data in the final it returns a recommended deck to the prayer one of the notables point is that GE Python 3.7 was generally available at the end of last year so we could use it just at the right time as a result of a played to Python 2.7 we didn't need to change our code that we had already developed in Python 3 so I've explained the algorithm and the system with a recommendation let's move on to the next topic departing Airport it's called Australia dojo dojo means our training room to improve playing skills this is overview the training in differently but the planning was suitable for a case too many complicated features such as state of the export hands status and the combination of the features the tip planning have both receives a receives a state of battle from the client the informational battle is converted to the numerical features which is which which are used as the inputs of the d-brane here we created more than 5000 features the network learns the actions of top tier players as binary classification finance the different model outputs the values of each action that will be used to decide an extraction we used more than 10 million butter records in the training in our training we are using the battle record of highly skilled players but we needed to increase the AI with appropriate skills for beginners in order to devise the strength of AI into multiple levels we are processing the inference results specifically we insert the highest iteration value when we want to make a strong AI and we wanna weaken the strength of AI we select the second worst our values with a probability technical is ability is calculated by softmax function which transforms the values into the gravity distribution after tuning of probabilities we made five levels of AI in the latter part we will show the remarkable results of the winning rate against the actual players next I will show the system of a about first of all I talk about of training phase a large number button records in philippi is saved in cold storage through bigquery the training process consists of the pre-processing of the features and module training which are running in Google compute engine as the pre-processing the features are generated on demand by emitting the game in our case we created the custom function of data generator that creates the features during the training after the training the trainer model is saved in crud of switch to deploy on to the Quadrangle engine regarding the contrast integration and delivery of machine learning it's so-called clcd we are testing and training modules to check the dependencies and features automatically by using the unit tests will regression test also it is important to deploy a model automatically in short cycles with high reliability we're releasing new we are currently trying to simplify a complicated process from training to deployment by using CSC detours for executing it more quickly and easily I think we can leverage a crowd build the contrasted every service of DCP to accomplish better CCD I move on to the inference phase please take a look at the communication between the client and the inference a peer server on the left of the slide the bottle records are posted to the inference API server from the client who is praying against the airport - serious server he produces a bottle from the record they extract the features and it sends the features to the only prediction API of the cloud every engine on a prediction of credible engine is very useful because it is optimized to run the air model with low latency in your time Kodama engine uses the trainer model store the included storage and the return the values of each action the values are sent back to the client and the client moves for the butter step by selecting the best action according to the values that's the overall communication between the client and server appengine and crud Amira engine can auto scale in response to the traffic not a very good function by taking the benefit of the auto scaling we can manage a large amount of requests last we are touching on the technical topic the testing of the systems so why should we test the systems I mentioned in the last write the code ml engine can auto scale that's good is that true but we didn't have enough knowledge about how much load the system can withstand in the mobile game generally the Lord tends to increase at the moment a new event opens the thing I was right shows the example system that estimated requests per second in a realistic event this is a trial calculation the requests per second will increase to several hundreds in ten minutes we have to check in advance if the system can withstand like this load as a point to be noticed we cannot predict the action of AI as a batch processing at regular intervals our system has to respond to the request in real time that is where the AI should push the character in the next stone to achieve this we chose only prediction of the query engine we've checked in Hoosick whatever engine can satisfy our requirements by performing a lot of testings in advance of the release in our case the testing was performed in the setting of the request increased linearly and reach a maximal 1,000 weakest per second we found that 'scrote-meal engine can scale quickly responds to the request and the average latency of the prediction is less than one second however there were huge answer took a longer time than usual since this might not be possible to risk return return all the requests to the player quickly so we needed we needed to resolve this condition our team and credible engine team cooperated to investigate that bottlenecks the dam we found found that time what had happened when the new instances warmed up in the process of the auto scaling due to heavy roll it was challenging to handle large number requests which consists of many features at the same time with only prediction but if we can resolve this problem by building an efficient back-end system we don't have to amass the system beforehand and system will be more robust so our team took attempted to deal with this problem let's take a look at how we tackle to remove the errors when the error returns within the time we can simply throw it back again to the API but in the case of time out the system cannot send it back again because the time error is not returned therefore when the inference a player server does not receive the response within five seconds the request is sent back to the API again because something bad has happened in the inference also it was a destination was a request is the same endpoint it will further increase the load on the server as a result we set another backup endpoint to receive the request that fails to be operated in the main point that it's a second request is sent to the backup endpoint which is prepared independently from the main and the point this is a result of the lot of testings with backup endpoint the figure shows the total error rates of several settings no backup backup is Temenos and the back of 20 million loss in the case with a backup and a point on the left about 1.3 percent of errors remained in the middle setting of the back up with Timmy Munoz also small error still remained the backup can deal with most of the main point errors we found that reasonably small error was because a number backup instance was small so the tie-rod had still happened was warm up of the auto scaling in the backup environment the rightmost case is fine-tuned setting where as a number of okapi instances was set to 24 covering the main and 0.0 errors sufficiently in this case we could handle all the requests without errors in this very DNA and crud arrange engine team got over the challenge high under spiky traffic and the latency limit visitors after deployment in this setting the system has been stable in handling many requests let me show you one more thing it's a useful service to monitor the system performances we can check the server performances through stackdriver which provides error reporting are at notification logging threads of latency and many others structural dashboard especially provides a visualization tool that is easy to add a chart and customize it we are monitoring the many metrics according to our needs in the one place for example the number of requests a PR agencies CPR the memory usage and the number of learning instances in your time additionally you can change the time window to focus on and check the percentile values we are using 99 percentile values for anomaly detection in the system so so far I have explained the technical details were AI with DCP the system with DCP can be constructed flexibly to cope with various problems in gaming and other fields now I'd like to hand over to June for the KPI result of use cases in the future thanks okay so this is a result part so as he mentions we needed a scalable and robust system to successfully reduce our a eyes so in this final part we would like to share what happened after the release so first for the deck recommendation here is a chart which shows the number of pinners who use the key recommendations there were constant usage after the release but this function was not well known at this time but the station has been changed after we started an in-game event to promote players to build new decks so the deck recommendation system has been recognized more and even after the event has finished many players and the mostly beginners are still using this function frequently so second we are very glad that the recommended tech push the wall rates of the by about 5% compared to the deck created without the recommendation so since our focus was to support beginners we are very satisfied with these results also I'd like to mention that this function has good adoption even form top T appears that this was sort of by product but we are observing that heavy players usually update their deck frequently and this recommendation system mitigates the cost of building deck from scratch they use recommended recommendation as a first suggestion and then customize the deck as they want so let's see the system matrix as well after the release we have a stable latency of 20 min take which is vecina for our cases and we're surprised by the result because recommended shown resumes is very complex and we haven't expected such a low latency so the auto scaling does a good job we haven't had any citizen trolls so far so briefly we will explain about the evaluation of the planning area but after the release we are observing that the our a boat has good enough win rate as parties partner foreigners so this is chart shows that the the win rates of AI against players the horizontal axis denotes the players skill levels and the vertical axis is the win rate of AI so in the dojo we provide five different AI levels and the strongest one can even be top tier players with win rate of 50% so we think that our a is is strong enough for winners support so handy from perspective we invest of AI ranges from our 20% to 80% which means that the player can choose any I mean appropriate levels of AI according to their skills or preferences so therefore this AI works as a sort of good mentor although this was a released last month and that we don't have enough data to evaluate the business impact daughter is used by being very well and we hope that this can lead good kept here in the future and yes for Dmitry the system matrix the inference of machine learning engines is about 200 to 300 the music and very stable even for the inter system the response time is about a second which is enough fast for our case and become by a backup end point is working well and we don't have system trouble so far so at the end of my talk I will show items which worked in our cases so first to train the AI that play the game like a human we need the battle logs that completely reproduce battle status and action so if you have some random effects in the game make sure that the random seeds are sore in the logs so secondly for for the evaluation of our AI we needed to run thousands of battles to get performance statistics so a headless numerator helps us a lot in this context and well if you want to try to use brain enforcement running techniques you also need to very I mean the very very fast simulator because the number of the battle there are origin goes through very really matters and is heard we also want to stress that scalable and robust system is critical for I'm implementing AI in VR services we see that GCP in particular called machine learning engine and GAE who are a good choice for us and as for the CI CD we have to think about how frequently or by which trigger the am older should be updated because the game environment changes frequently we need to keep I mean adding features and the retraining the models after the release so the this operation is very heavy and we have to think about the CIC be made before LEDs in particularly in the amo leading phase the final item is not technical but a critical generally speaking we don't know how well it will perform and how big the business impact will be before starting a project so in such an asset a case situation the communication controlling the expectation is very very important although we are machine learning engineers we try to understand the game and the business issues correctly and I think that this was a key factor of the success of these projects so that's all from me and I like to pass the summary to Henry thanks so hi everybody I'm Henry Tappan I'm a product manager on the AI platform team working on what was formerly known as a cloud machine learning engine but we've just rebranded to be the AI platform this was a really exciting thing so we've been working with the DNA folks for a while for this one mostly being inspired by the very elegant way that they took a number of machine learning techniques and built a awesome experience for their beginner gamers now when we saw this you know we we notice there's a lot of different things that they had to use a lot of different pieces of technology between App Engine compute engine the cloud machine learning engine and so on and so forth so the challenge to us at Google is how could we make make it even easier for teams like DNA or other gaming companies to get started even faster producing these kinds of innovative but also very accessible gaming experiences for it using things like machine learning and AI so we've been working on a number of features to make the process of getting started that much easier and that much simpler so one of the ones that we noticed was starting to think more about machine learning beyond just tensorflow Sai kit and XG boost so cloud machine learning engine was one of the first cloud-based platforms that enabled you to train a tensorflow model in the cloud since then in the last few years we've added support for other libraries like scikit-learn and XG boost but especially seeing what was done with association rules and other data mining techniques we realized that the world of saw and the world and the problems are much bigger than that which is easy to do inside of these frameworks and so we've been working on introducing support for custom containers into our training component this is now available in a public beta we expect to go to full release probably in this quarter but certainly inside of 2019 we're with custom containers you can take any code any framework that you can stick inside of a docker container and then push it to the cloud machine learning engine in the same way that you would use tensorflow models or Sai kit or XG boost or any other kinds of training scripts so this gives you access to not only traditional machine learning framework like tensorflow pi torch cafe etc but also to run any other software that you might need so if you've written some C++ I'm Java codes some different Python routines even our code for solving certain parts of your analysis or production of models you can now go it go ahead and easily train them on the cloud machine learning engine additionally it's not shown on here but we've also been thinking more about how we get easier pipelines for solving some of these things so earlier late last year we released into the open source a system called cube flow pipelines which is a composition and an orchestration engine for running machine learning focused workloads on cube flow pipelines you can start up a kubernetes cluster on gke or on your own premises etc specify a simple workflow that says all the different steps you'd like to do on the way including training jobs on the cloud machine learning engine and then the the cube flow pipeline system will take care of doing all the steps necessary making sure that it runs runs effectively so this could be a great way for tackling bigger parts of your machine learning workflow besides just the actual training part at the very end of it so the second thing that that we really inspired by and I've been working on is I think June's on pointed out very very well that when a nikkie as well that when predictions come back from predictions come back from the machine learning framework or as you're putting data into the system's you often need to do other transformations to make it compatible for the game so coming back to the example of the AI it wasn't enough to just get a particular score back from the model but they also needed to select based on the players skill level a random choice of a different different option in it now this was could be really hard and that you know you might have to set up different servers for post-processing or pre processing the data before you send it to the machine learning engine so to make this simpler we have available as of today in beta the ability to run any Python code on the cloud machine learning engine prediction service before or after you make any predictions so if we need to reshape someday maybe take a different result convert from a score back into label so on and so forth it's now just as simple as giving us a Python class and you've got whatever you need to make happen along the way this incidentally also gives you some more flexibility as well we allow you to install any Python packages on the inference server as well and then do whatever you'd like with inside those packages so you have much more many more options for running jobs on side of our machine learning systems and then finally the other big advancement that we've seen that was I think especially relevant for gaming and one thing that was inspired by DNA is giving better flexibility and better capacity for it so you know it's really great to see to see heavy usage of our systems and we want to make sure that we're giving you exactly the capabilities that you need that you are able to run your jobs bulletproof even for when you have high demand bursty instances for it so today in the machine learning engine on the prediction side you're able to choose either a four core or a dual core machine with a choice of RAM in there we're expanding this range of options to be virtually any machine type that's supported on cloud machine learning engine or we're sorry that's sort of support on the Google compute engine and including into the support for accelerators like GPUs so that way if you've got a job that is very intense or very bursty or you want to maintain a certain level of latency you're now able to select many more machine types and much more power to those ones especially if those who are looking for really complex models like reinforcement learning or you're trying to beat the next Starcraft AI this might be a great option for you so let's kind of sum up with these things this was a chance for us to see that AI can be used for much more than you know recognizing images or speech or something but we can actually use machine learning AI to create compelling gaming experiences both to challenge our players for you know when we want to give them a smarter AI but also to assist them so that beginners have an even playing field with those who have been on it for a long time and the result of these kinds of experiences can result in better engagement and revenue for your for your game and then finally to know that Google cloud is here to help we're very interested in the gaming segment we're very interested in machine learning so anything that we can do to help you achieve success in your game and we're happy to do it [Music] 