 hi everyone welcome to our talk anomaly detection at the edge this is arun kejiwal with my colleague ayra kohan we have known each other for several years we have worked in the space of anomaly detection uh in different capacities i've in my previous life i've worked on anomaly detection in the context of marketing in the context of infrastructure on the other hand ira co-founded a company called anna dot uh which has built a great product around this topic he will talk more about uh the company and the product the customers later in their talk if you have any questions uh please feel to reach out to us via twitter our twitter handles are there on the slide deck uh in general uh feel free to ask questions after the talk via the uh via chat uh we'll be more than happy to answer any questions uh without any further ado let's uh uh straight in cloud computing has enabled scale innovation connection over the last decade or so going forward uh edge computing uh is supposed to be the next big thing where it will complement cloud computing by providing more real-time value more immersive experiences via more data production and intelligence at the front end you know where the people are where the things are in fact idc forecasts that they'll be around 46 billion devices generating around 80 zettabytes of data in the next five years now you may wonder that what will make edge computing successful so there are new technologies like 5g which are coming up very soon on the application side internet of things which comprises broadly speaking of sensors and actuators is ramping up pretty fast now as we all know that during the covet pandemic we were all stuck in our homes so and what if there is a network outage or there is a machine downtime in such scenarios is critical to be able to still provide value to the customer so hence in such scenarios edge computing plays a very vital role now uh spark summit is a uh industry centered uh forum so one may wonder what is the business opportunity behind edge computing so by certain estimates from gartner and mackenzie it is expected to have a market share of around nine billion in the next four years and from a use case perspective there are plenty of them like real-time automated decision making and the criticality of edge computing stems from the fact that we need to be able to extract insights at the edge uh owing to a increasing data volume and i'm going to talk talk more about it in the subsequent slides and and of course uh there is an element of efficiency because as much as insights may be helpful but cost is also a very practical matter so on the uh so broadly speaking there are four driving factors uh guiding the growth of edge computing one is how do we minimize the latency behind inside extraction then how do we address the data growth and the bandwidth limitations and as i already mentioned that the system should work you know even in the presence of um connection issues what if there's a machine failure and last but not the least then how do we guarantee or provide a high level of privacy and security to the end customer so broadly speaking uh there are several aspects associated with edge computing one is on the compute side as the data volume grows uh it's not tenable to keep growing the data centers because they're very power hungry and it's very expensive so over the last few years federated learning has immersed as a very promising paradigm where the competition is done on the edge itself now to make it possible we need high density ultra low read write latency on the devices itself because the data volume keeps increasing and now as you can imagine that video consumption is on a steep rise now if if one were to provide video and analytics on the edge this will definitely require high density storage moving on uh as i mentioned as i briefly talked about earlier data security is also a critical challenge now in the context of ace devices given their limited compute capability the challenge is that how do we provide advanced authentication and encryption support uh given the limited compute capability and last but not least on the dependability side of things ensuring continuous operation even in the wake of network outage is going to be a big challenge and it's not only about network outage in general you can imagine that users may be on 5g they may be on 4g on 3g the network connectivity is not uniform across the globe so how do we ensure that we provide good experience to the end customer now unlike most of the talks where the code idea is presented without any insight into into the application here uh both ireland and i we will walk the audience through what are the different use cases so that the audience can walk away with the core techniques and explore how they can apply in their respective domains so in the next few slides i'll walk through some of the use cases um in the context of edge computing so as illustrated on the slide there are several domains where edge computing can play a vital role for example autonomous vehicles it can increase visual awareness which essentially corresponds to sharing data around weather about route conditions uh about uh peer traffic so this can improve uh the driving experience and also um help mitigate um the number of accidents so similarly on the manufacturing side uh it can help with providing insights around predictive maintenance of turbines of motors and other heavy equipment ah likewise there are plenty of applications in the realm of telecommunications retail how do you do hyper targeting and more in the real-time context let's say you are in downtown and there is a concert going on you know uh can you be recommended any discounted tickets given your location in real time and of course uh the uh on a more personal basis healthcare and life sciences presents uh a tremendous opportunity for edge computing and given the world we live in today where we have all been logged in in our homes due to kobe 19. this is a great example of where edge computing can help you know how do we do remote l sensing to identify clusters of people who may be asymptomatic or they may be exhibiting certain symptoms likewise how do we monitor the onset of a disease how do we detect anomalies in any lab test reports so here on the bottom right i show one of the example anomaly reports from anode where they applied their product to detect a sharp increase in the number of new cases for one of the countries continuing on the different use cases in different industries um we have energy and utilities agriculture is a tremendous opportunity today around the globe uh a large percentage of the production uh gets wasted uh because of lack of insights around crop quality about insects about water uh availability and so and so forth uh over the last decade and a half data centers have grown by leaps and bounds uh they constitute a significant percentage of the energy consumption around the globe so how can we make these data centers more efficient can we monitor uh metrics like humidity air flow and other data center metrics to improve their efficiency in the realm of finance we all know that most of the payments are done on it uh digitally today especially uh in the uh kobe 19 world so how do we make ourselves more robust against any potential fraud we all may be making payments through our phones so can we have some sort of a payment bot on our phones where it can detect any potential fraudulent activity broadly speaking the use cases can be categorized into three pockets more on the business side on the people's side and that what we know call it as internet of things now either you can have uh interaction between self interaction for example a business can have a self interaction in the form of distributed business processing or different domains can talk to each other or interact with each other for example businesses can have a more immersive experience with people via client content delivery so how do we personalize that content uh on the edge itself uh likewise you know people interact with things how do we provide more immersive experience and in in that context one of the key aspects is around ar vr it's still a way out before it gets to the masses but how do we personalize that experience [Music] on the device itself uh moving on there is a plethora of of use cases from ranging from video analytics from security productivity i just talked about virtual reality like in a multiplayer game how do we uh tailor the game based on how the different players are playing the game uh so it's it's more dynamic instead of being static in the augmented augmented reality realm um one can customize the shopping experience on the device itself and this will ensure that we are not limited to any connectivity issues which the end user may experience just to quickly wrap up uh the suite of use cases uh more closer to home smart homes provide a great opportunity um how can we build intelligence on smart meters how do we increase the energy efficiency of our homes uh smarter transportation data reporting environmental monitoring presence another few use cases and again this is more on the personal side like how do we track air quality water quality and for for instance um during the kobe 19 pandemic there were certain um anxieties around hey is the water drinkable you know as has that got contaminated so conceivably you know we can sprinkle uh sensors in the water reservoir and we can monitor the water quality in real time at the edge itself again more closer to to us privacy is a big deal so can we support ideas like differential privacy on the device itself so that while providing customized experience to the end user the privacy is not compromised automation in the realm of of um industries how do we monitor the performance of robots of drones we talked about health and safety earlier and then last but not least conversational interfaces many of us have gotten used to using cd or ok google or cortana how do we tailor these digital assistants by the respective use cases so my use case can be different from ira's use case can we optimize the digital assistant on a per user basis on the device itself so that there is no information leakage from one device to another uh more at a broader scale we have plenty of opportunities in the realm of smart cities around logistics uh insurance is another big opportunity my driving style and and the number of miles i try may significantly vary from how much uh ira drives um out there in israel so uh can we tailor the insurance policy based on our different driving styles uh so that we pay uh accordingly and even in a infrastructure context like railways you know can we monitor the condition of the different uh railway cars and then provide insights around predictive maintenance uh so one main one may so so far we have talked about on more on the software side so there are plenty of opportunities on the hardware side as well for the use cases as per mckenzie it's around a 200 billion dollar market now what is interesting here is that you know i would have expected that public sector or health care were would have been at the top two of the uh potential market uh but it turns out based on the research travel and transport uh uh is at the top now this is sort of cons understandable because in the context of healthcare we have regulations such as hipaa compliance in the public sector and utilities context and there are challenges around regulation so this will take a while to to grow but uh but we do expect these two segments to provide major opportunities for edge computing so moving switching gears to more artificial intelligence that how where does ai at the edge come in so broadly speaking there are three flavors either you can have uh visual response you can have an auditory response or you can have a tactile response so here uh in the um one of the uh use cases is around yeah so um as i mentioned like one of the use cases is around facial recognition which can be used for authentication and similarly there are other applications around vision our arvr language translation is big so talking about language translation there are different flavors you can have text to speech uh which is commonly used um especially like uh for uh reading emails when you're driving there can be other flavors where it textures converted into acoustic features then it's converted into into waveforms to analyze the text itself there are several challenges in this regard like the the model you develop for text-to-speech for instance has to be really small because the memory you have on your devices uh is pretty limited also um as i mentioned uh early in the talk one of the key aspects is that how do you uh provide insights in in real time so so in in that context uh one has to make a space-time uh trade-off uh speed accuracy trade-offs uh and there are other challenges as mentioned on this slide like you need to have a small like devices have a small warm uh form factor they have to withstand a regular environment so these does uh have a implications um from a algo design perspective so jumping right into the uh broadly speaking you have either training or workloads or you have inference workloads uh training workflows are typically very compute heavy so one has to design uh robust algorithms which i do not which are not compute and memory uh intensive uh on the inference side uh there can be many metrics uh for prediction like velocity orientation trajectory activity um there are many uh signals which which one can use either from the accelerometer gyroscope and so on and so forth so the the main premise here is that the availability of data is huge the number of metrics you want to predict um is also pretty uh huge so um how do we facilitate this so federated learning as i mentioned earlier is is one paradigm where we federate uh the infinite training across different devices to make this happen so uh on the federated learning side we have a uh we have a wide suite of applications like mobile keyboard vocal classifiers and so on so forth um the challenges uh um as i mentioned earlier is going to limited compute and memory on the devices uh convergence time uh can be a potential issue and then if you are leveraging federated learning then communication between devices can be a challenge so given the challenges to quickly brush through we have one way to design algorithms is that they need to be one pass so that they are fast um the the other flavor is that the algorithms have to be incremental in nature so that we can meet the real-time constraints so so in the incremental context there may be challenges around numerical stability how do you build incremental algorithms when you have a wide set of signals so then essentially you have data in a high dimensional space so that may be a big challenge in itself so so far we have talked about the use cases about the applications now everything rests on the data the incoming data being of high fidelity you know otherwise that's uh most of you in the audience may have heard you know it may be a case of garbage and garbage out now now data fidelity is especially a big challenge in the context of edge devices because as i mentioned these devices have to withstand a rugged environment they may be connectivity issues so you you may be you may bump into issues like missing data or you can have anomalies in the data and that's where anomaly detection comes into play in the context of edge computing now you may wonder that hey you know what's new in this this anomaly detection has been studied for over 125 years so what is and here this slide essentially highlights some of the techniques which uh of flavors of techniques which have been uh used in wide variety of context um outside of edge computing now these techniques are typically not viable um because of concept drift you know the underlying distribution may change you know which because of you know you're connecting your connectivity dropping in real time due to a variety of data you can have communication bottleneck between devices and of course most importantly the algorithms which which have been proposed over the last 100 years didn't have any real-time constraints uh in mind so one of the common ways to develop these techniques is called sketching so um there are different flavors of sketching depending on the application some of these are listed here on the left on the slide so ira will uh will walk you through some of these uh techniques uh in a couple of minutes so uh in the context of privacy and security there are techniques which have been proposed more recently to provide tamper proof resistance so tampering can be think can be thought of as a anomaly in the data so that's another use case for anomaly detection at the edge so these are some of the algorithm algorithms which have been proposed specifically for anomaly and detection of theirs these are brought essentially variants of techniques which have been in use for a while but they have been sort of slice and dice to make it fit on the devices because of low memory and how to make them run faster on the devices and due to low compute so now i will hand over to iras to walk us through some concrete use cases um based on his interaction uh with the customers of anode so thank you arun uh to introduce myself again i'm eric cohen i'm the chief data scientist of anadot and we're a company that developed a product that does anomaly detection both as a service and also potentially at the edge so i'm going to talk about a few of these use cases right now so first i'm very happy to be at this uh at this virtual conference uh first i didn't have to travel far i didn't have to shave but i have to wear my reading glasses in order to see my laptop's screen because if i look at the big screen it looks like i'm looking somewhere else so i hope it's not weird for you listening to this talk so let me talk about anadot's anomaly detection and how do we do anomaly detection and we do it basically in a sequential manner not just because of educat edge anomaly detection cases but even when you want to scale non-edge case to millions and billions of time series doing it using non-sequential algorithms is very expensive computationally and then the benefit of the anomalies is smaller than the cost of running that platform so you have to weigh those in as well the way we do it is we collect the data continuously as a streaming service and analyze all the data on the stream itself to detect to learn the normal patterns and then based on those normal patterns we can detect anomalies whenever they arrive these models of learning neural behaviors have to keep getting updated sequentially so any algorithm that you take you have to adapt it to be sequential if it's not designed that way from from the beginning otherwise it's very hard to scale this up now another important point that is both important at the edge but and but even more critical of the non-edge cases you want to correlate the anomaly so suppose you have a host of sensors and you can detect their anomalies on the edge that can be done very fast you still want to be able to correlate them into a concise story to tell you all the anomalies that occur that are related to each other because those those correlations often lead to an actionable insight and then the algorithms can can provi can can get feedback if it's available and improve themselves so this is how it looks like sequential updating of the models and learning uh one particular time series as time goes on in this case you see on the left when the time series you have very few samples your normal pattern is very very wide or the baseline is very wide which means the pattern is not learned well yet but as time goes on the algorithm should adapt themselves and detect more and more uh get more and more information about the time series and in this case it first detects that there is a daily pattern and then applies it to the model and then it detects that there is a weekly pattern and then applies it to the model and at the end you get a very tight baseline which is the output of that machine learning model that learns the normal behavior it's a visual output of it and the thing is and especially when we're talking about sensors from multiple sources and you can get a lot of different behaviors of time series so there isn't really a single model that fits them all and a roon listed in several different models and i can tell you some of them work on some data some of them work better on other data we have found that the right approach is an ensembling of models fitting the right model to the right behavior of the data or ensembling multiple models together in some cases and these are real examples of both time series and anomalies that our system detected which actually have very different behaviors and require different algorithms to learn their model now the correlation helps helps in two regards first it helps understand uh what happened and then where it happened because you might have a lot of sensors firing various anomalies and the correlation themselves assuming that some of them are not just symptoms but are also indicative of root causes they can help understand the root cause much faster and that's really another aspect that is very important which is typically more expensive computationally which means if you do it at the edge you actually do need to make your algorithms much faster now anomaly detection by itself is meaningless unless you inform somebody that that normally happened and once you start informing the alerts the notion of false positives becomes very very critical to minimize and a lot of the minimization is done first of all by applying the right algorithm to the right data to the to the data so it can capture as accurately as possible in its normal patterns uh but that we have found that that that by itself is not enough because not every anomaly is created equal not every anomaly is interesting as another so we baked into our product and we believe that these kind of things have to be baked into any type of product that does alerting various filtering mechanisms that allow the user or the system to automatically discard some anomalies that's not interesting and send only the right alerts so it starts from scoring anomalies giving various parameters and optimize the duration of the anomaly their delta uh allowing the user to see simulations of past anomalies so they can do things with it filter filter based on that add influencing factors that help you filter anomalies or decide whether they're important or not and that lets you add context and correlation and at the end of the day if you can if the users can provide also feedback for alerts in the form of this was good or bad it can loop back and in some supervised fashion improve uh the the naturally unsupervised algorithms that are used in our detection so with all of these basically [Music] we've we have a product that has been widely used by a lot of different companies in various different industries and use cases and the reason they can use it is because we really baked into the product a lot of different algorithms for capturing many types of behaviors and the focus on false positives so all of this is nice these are the main use cases that people use us today for revenue cost monitoring partner monitoring customer experience monitoring but what does this have to do edge you must be asking yourself so let's talk about uh edge use cases as well and i think in the context of covet 19 it actually is pushing it quite significantly quite fast along uh in in a variety of way um the first way is uh monitoring health monitoring so remote monitoring of health is not new it's been talked about for a long time in in this day and age kobe 19 it's actually pushing it quite significantly and here you have watches that can measure a lot of things about your health and detect whether something becomes anomalous either sleep pattern or blood oxygen or anything like that whether you're sick or not and especially if you are a carrier of covet 19 you might not want to go out to anywhere and you might want your device to be to inform that your doctor that they should call you to see whether you need to go to the hospital or not in the hospitals themselves uh uh cov19 actually presented a new type of issue especially in icus where it's dangerous for the doctors and nurses to be present in close proximity to the patients that they have to take care of normal icu you have you have a lot of staff for each patient uh it depends on the geography but typically at least one staff member for a patient one or two patients in kova 19 first uh some hospitals in some regions saw an overload of patients in icu and the doctors and nurses should their time with inside the icu next to the patient should be minimized to only the cases where they're absolutely needed so that lends itself quite nicely to edge cases and i'm going to talk about monitoring patients actually get volume which is the case of pandemic so you want to monitor patients at home you want to monitor them in a hospital when they're ventilated and where they're not and the problem is the the existing techniques in healthcare for alerting on deterioration of patients is actually quite lacking it's well known it was known even before but in but here it actually exploded a lot more for example blood oxygen if you're ventilated if it goes below 90 all alarm bells will go off but when it reaches that you might have missed the deterioration that happened for a few hours or days from the uh um from the from the patient and at that point it's already might be already be too late now the other side of it is that a lot of these static thresholds built into the devices are generating many many alerts we see it in the i.t space anybody working in ite systems knows they get a lot of false positives it turns out in icus and in healthcare it's the same because while specific thresholds and specific parameters might be correct they they oftentimes lead to too many alerts and large fatigue patients the requirement is remote monitoring early warning score to identify determination conditions and minimizing false positives so just like the business use case so let's look at some of these examples so this is an example of a real of data from uh from a watch uh it's looking at respiratory rate which is important in the colbit case been shown that deviations and respiratory rates actually uh indicate deterioration of disease sometimes uh so if you the problem is each person has their own respiratory rate that is normal and might even change throughout the day and at night so you have to adjust the model to the data of that particular person you cannot assume some static thresholds here so this is an example of an anomaly of a respiratory rate that went down for one person being monitored this is an example from an icu patient looking at and here the correlation comes into play looking at multiple parameters being monitored uh through these uh through the monitoring of an icu patient sending it looking at the device itself finding the anomalies and sending that information to the mock or the patient a lot of hospitals just created an outside room or they can observe these vitals but also get alerted and again when there is a lot of patience this has to be uh the alerts are really critical because there isn't an eyeball looking at every single patient and their health and their health situation right now and a lot of times they miss deterioration as we see in this anomaly here now showing what we learned is that showing graphs to doctors and nurses uh like that oftentimes they don't know how to interpret them looking at graphs over time the the right way to show is to give some score and there's been a lot of work especially in the last few months trying to provide an early warning score for the generation of icu patient and these scores that you see here is actually for a patient over 20 36 hour period um where the score goes up to uh you know anything about seven above seven is already critical and what the doctor can see at any given time is what is the score of the patient uh from zero to any number above seven based on all the anomalies that were detected in the vital signs of that patients so what are the benefits early detection so you improve outcomes the system is constantly monitoring you you can scale it so you reduce the load on the medical staff by using this type of autonomous monitoring or machine learning based approach and you reduce the risk to to the staff and you actually when you monitor people at home you also reduce the risk of them going out and infecting other people so this is quite quite beneficial and uh so yeah so uh with this uh we've come to the end of the talk and uh we've gone a long way as a slide here writes from you know the the engines to edge computing and being able to actually run these machine machine learning models on edge computing and we see the benefits of these edge computing use cases and as i demonstrated here in the notion of a pandemic these benefits become even more important for keeping everybody safe so thank you very much and i think now we can take questions 