 Hey welcome back, So I am really excited to start this new series which will be to try and reproduce the tech from Spiderman Far From Home, the EDITH glasses Not sure if you guys have seen the movie, but spoiler alert Essentially Tony Stark hands down his Super Smart AI Glasses called EDITH which stands for Even Dead, Im The Hero. Classic Tony So these glasses give Peter Parker access to all of stark tech and also has smart Augmented Reality capability So this series will try and push the boundaries of existing technology and see how many features we can fit in this project just some acknowledgements to some really cool people who have worked on EDITH Glasses are The Hacksmith and JLaservideo. Links to their channels below So before we dive into our approach to this technology lets check what has already been done The Hacksmith state that we can get close to EDITH glasses by using a product called Focals by North Focals are very stylish smart glasses that let you manage your digital life without taking out your phone While they do not provide Augmented reality functionality, they do provide a holographic display that shines directly into your retina On top of that they also provide support for Alexa and third-party apps My opinion is that they are like a smart watch but in a spectacle form factor. Jlaservideo, has already two iterations of his EDITH Smart Glasses The first iteration he uses a transparent oled display that’s mounted onto the glasses. With a shoulder mounted webcam he is able to detect and recognize text from street signs. In iteration 2, he made some significant upgrades He used a Raspberry Pi 0 with the pi camera along with a mini HDMI display that can be hacked to any glasses frame The display is reflected off a plastic screen that allows the wearer to view the HDMI display To add intelligence, the EDITH prototype takes pictures from the pi-camera and then uploads it onto a drop box server where the google image search is used for image classification The issue that JLaser mentioned was that the process was really slow and with a lot of lag But still really great progress in short amount of time Now I know I said he only had 2 itterations but as I'm creating this video he just released a third iteration of the EDITH Glasses which makes use of the AR glasses by Vuzix The Vuzix Blade run an open source androind so anyone can develop apps for the glasses You can this glasses to view what a DJI Drone is seen by the app or use Google Translate to do a live text translation from the camera So what can we bring to the table in creating an EDITH glasses prototype Well with our powers and special set of skills in AI Computer Vision and Augmented Reality we can focus on the AI and UI Im going to divide this project into several phases or video tutorials to cover certain aspects of the prototyping and development so subscribe and click that bell icon to keep up to date with the project as it gets released. In this iteration we are going to focus on the AI side of things so we are going to start with face detection or face recognition In this video we are able to get face detection working So lets pretend for now that it is face recognition if we detected a face, we would want to attached some information about the person that we have detected and recognized just like how in Spiderman Where they were able to identify various individuals on the bus. So just be aware in this tutorial Im detecting a single person and assuming that the person is me so that I can bring up my bio in the AR view. Okay so without further lets get started! Okay so this free Youtube series is just small part from the full course Some parts of the project development will only be available on my course called Ultimate AI CV Practitioners Pro which is the complete training that teaches you everything you need to know to become a PRO AI developer in computer vision So this includes, object detection, object segmentation, pose estimation as well Android AI app development There will be a link in the description where you can enrol directly into the course or if you want to learn more about AI_CV then there will also be a link to a FREE webinar explaining my 10 Tips to becoming a PRO AI practitioner Okay so let’s get started with Project EDITH! First up there are a couple of paid assets that you are going to need for this project For the UI I used a unity asset called Customizable SciFi Holo Interface by Excelsior Assets For that futuristic look, there is optional asset called Hologram Effect, which is self-explanatory And for the AI you will require OpenCV for Unity from ENOX Software which contains the face detection and object detection APIs There are direct links in the description below to obtain these assets. So once you have the assets, create a new project, we will call this Phase 1 – face detection and you can Import and Download all the assets Face Detection To get face detection working, we need to get the Resnet Face detection example up and running In explorer we need to copy this Streaming Assets folder under the assets folder I ran into errors because I didn’t read the instructions So make sure you do this step properly. You will find the instructions over here in the setup_dnn_module pdf Next we need to download the Resnet face model called res10_300x300ssd_iter140000.caffemodel and store it in the streaming assets/dnn directory Do the same for deploy.prototxt and store it in the streaming assets/dnn directory Do the same for deploy.prototxt but when you save it, make sure to rename it from deploy.prototxt.txt to just .prototxt. Open up the example scene and ensure that you have a webcam connected to your PC We can run it and if all goes well you should be able to see a nice green box around my face But green boxes are so last decade Lets spice it up by using Excelsior GUI that we also imported Go to our github repository, Project EDITH, click download open up, unzip and lets import it into our project. Open up the this scene and copy over the Welcome Screen and line it up our canvas space You may take you a while to get this right so try scaling and rotating this 3d Object until it aligns in the frame Also don’t worry too much about it as we will control it from a script You will find this script on my Github Repo Called Project EDITH In the repo you will also find the GUI game object that contains the prefab holoGUI Copy and paste this script So essentially what this script does is that it takes in our game object and places it at the detected locations of the detected bounding boxes You can also hide the boring green boxes for a much more immersive experience and commenting out these 2 lines Now chances are that you may not see your pre-fab holo gui when you enter unity Play mode You will have to empirically calibrate your prefab location using these variables over here Getting the location just right depends on the camera that you are using and the resolution that you set right over here. Mine is set to 720 x 480 So if all works well you should be able to press play, and be able to detect your face in real time and have a futurist UI pop up with some information on you and that can be set right over here There is an entrance and exit animation that you can set based on a time delay and that can be set right over here Really cool but simple however this is just phase 1 of the EDITH prototype lets quickly go through the planned phases of the project If you would like to add on your ideas to the project, you can comment down below so we can add features to make these glasses a reality. Phase 1 Which we have just completed is to get face detection up and running Phase 2 is to take phase 1 a step further and to get multiple Face recognition working in Unity and to be able to identify various faces. Phase 3 We want to scan for objects and for the AI to tell us what we are looking at and this can be achieved with either a MobileNet Object Detection or a Tensorflow Image Classifier Phase 4 where things become interesting We can try implement Voice Commands similar to Google Assistant or Alexa to execute a drone strike Just kidding to scan for objects or scan for faces Phase 5 which is the IoT Solution this entails switching on/off our IoT devices and giving us visual feedback on our device if possible Phase 6 quite exciting which is that our Android or Raspberry Pi solution basically having all this power on a gaming PC is really nice but eventually we need to make it mobile this may be a bit of a challenge especially since we're dealing with low-power devices however we'll investigate if we can put all this intelligence that we've posed into the previous phases and see what features work and what features may require a compromise and then we have phase 7 which is a VR Solution or nReal solution Once we have it running on mobile devices, can we turn it in to wearable tech This may be in the form of google cardboard with a phone attached to it or using a much more elegant formfactor using the nReal AR glasses which may be released anytime now So yeah we have a lot of work to do but let’s make it happen Comment down below if you’d like share your ideas and we’ll implement them in the upcoming phases Phase 1-4 will be free on Youtube and Phase 5-7 will be on my full course Ultimate AI-CV PRO which is the complete training that will teache you everything you need to know on becoming a PRO AI developer in computer vision Sp this is where we teach, object detection with Yolo V3, object segmentation using Mask RCNN and pose estimation using Open Pose as well Android AI app development There will be a link in the description below where you can enrol directly in the course or if you want to learn more about AI_CV then there will also be a link to a FREE webinar explaining the 10 Tips to becoming a PRO AI practitioner Alright thank you for watching and we’ll see you in the next lecture 