   [MUSIC]   [SHARMILA] Thank you all for joining us today for a session on Synthetic-Aided Computer Vision Algorithm Development. I have here Tushar Dadlani, who is a Computer Vision Engineering Manager and who's building the next generation of perception algorithms at Standard AI. And also, we have Mattia Santachiara, an Engineer at Standard AI and also the founder of Santasco Software. Tushar and Mattia are going to talk to you about how Standard AI has used Unity to reduce the financial cost and time of algorithm development for machine learning training. Feel free to use the Q&A tab to post your questions, and we'll use the last 10 to 15 minutes towards the end of this session to answer those questions. Now I'm going to hand it over to Tushar and Mattia. All right, thanks a lot for that introduction, Sharmila. Good morning, everybody. Good morning, good evening, good afternoon. Thank you so much for taking the time out of your busy days to join us here live at this webinar. I am Tushar. I'll go over a little bit high level of what exactly does the title even mean. "Synthetic-aided Computer Vision Algorithm Development." That's quite a long title, but just to break it down a little bit. One of the challenges in algorithm development is how do you validate algorithms? And one of the challenges in computer vision is recreating the real world is pretty hard. If you take a shopping example, like one question we constantly get asked is what if I cheat the system in this certain way, how will your system react? That's a great question. And oftentimes in algorithm development, you don't know until you actually observe the phenomena in the real world, or you create that scenario in the real world. So, to address such concerns, we use synthetic data to be able to validate some of our algorithms. We're taking the next slide. Today, we'll be talking a little bit about Standard Cognition, telling you how it looks like a little bit behind the scenes, what are some of our challenges, how do we leverage some of the Unity tools, and show a small demo with a little bit of peek into the future. What does Standard Cognition exactly do? Each of you have probably shopped at a retail store in the last six months or a year. If you haven't, then that's a little uncanny, and I would not expect anybody to not have visited a retail store. When you go into a store, often you go, you buy the items you need, and then typically, you go and you run into a cashier, and the cashier there is going to give you a receipt or will look at the items, will tell them and say, "Hey, you pay me dollars, euros, whatever." And you pay them that money either through credit card or through cash, and then you leave the store. And what we do at Standard Cognition is we take, we install cameras and capture all this information of the real world, and use the exact... use computer vision algorithms to generate such a receipt. You can go to the next slide, and you can play the video. To give you a little bit of a sneak peek into the kinds of things that we need to figure out is when we have shoppers in a store picking up items and knowing there are different parts of the problem. One is, where are the people in the store? And there are other parts on. You can do all kinds of things when you interact with products. It's not just limited to... Like all shoppers don't behave in the same way as you might imagine. Sometimes you will notice, if there are items really at a high shelf height, especially in the United States, there are a lot of shelves which are really high, a lot of people can't even reach those shelves, so you have to go there and help somebody grab an item and give it to them. There are all kinds of different scenarios that you encounter when you start building a solution which is in the real world. And there are all kinds of problems with item placement, and where are the people, what kind of items are they holding? People will take items, keep them in other places. It's all quite common in the real world. We basically need to answer two main questions. You can go to the next slide. Number one is, where is the shopper? You can think of it as we need to know where the shopper is, to know whether the shopper is in the store or outside the store, and doing this in a way without any biometric information. Such that, it is quite a hard problem, and doing that in a robust manner. You don't want person A to be confused with person B if they are wearing the same kind of clothing. You don't want person A and person B to be interchanged. It's like going on amazon.com, and your shopping cart gets exchanged with another shopper who's shopping something completely different. You don't want those different kinds of scenarios. That is the first part of the problem, to know where is the shopper. You can go to the next slide. And then, what is in their cart? You can think of it as the second part of the problem is much harder because there are a variety of products. Even if you look at Coca-Cola, for example, there are different packagings of Coca-Cola all across the world, and it's sold in different types of containers, it's sold in different shapes and forms. It's very important to understand. You're trying to understand all these different things with just a few cameras in a store. That is the crux of what we are trying to solve. And with that, I'll hand it over to Mattia who will talk a little bit more about our synthetic data tooling. Thank you, Tushar, and thanks to everybody here that is listening. The first question that we need to answer in order to understand the context and understand how Unity can be useful in this field is the following. What does our data look like? What kind of data we need in order to train our system in order to stress test it and therefore improve it? Well, we see here on the slide, our data mainly are made from two parts: one is an image, a frame, and the other is a JSON file. This JSON contains, among others, the description of the skeleton that we see overlaid to the man in the image. This JSON also contains the screen coordinates of each one of these red joints that we see on this man in the image. Unluckily, the situation is not always so ideal with only one person and very visible in the image, but most likely we have a situation like this with a crowd. Many people in the same image. Some are near, some are far, less visible, some occlude others. And also, we don't deal with static images, but with video streams, and these already bring us some challenges. First of all, all the process to generate this kind of data is human-made. We need actually to gather real people in a real store, and then record them and then label the video that we record. This actually brings us these kinds of challenges, for example, the accuracy of the data. Since that is human-made. it leaves space to human errors. Also, we are limited in terms of the scale of our data. We are limited in the amount of people that we can bring together in the store. We are limited by the size of the store, because we need to use a real store that we can assess in that specific moment. And obviously, this store must exist, so it means that, also, the camera must be already installed. All these points result in a cost, in a high cost of these data, the cost in term of time and, of course, cost in term of money, because, for example, once that the video is recorded, there must be some human worker that are labeling it. And that means that they need to click each single joint per each single person, per each single frame of the video. This is a very long and expensive process that also affects the developing time of the whole system because we can only produce a limited amount of this kind of data per given time. How do we address these challenges with Unity tools? First of all, we needed a virtual Scene, a Synthetic Scene of our real store, and this was obviously possible with the Unity engine. But also in this Scene, we needed our virtual customers, and these virtual customers had to be able to walk through the store independently and realistically, and we could achieve this with the navigation system. Also, while they are walking, of course, they need to behave and move like humans, and in this case, the animator and rigged models were very useful. And as well,  while this Scene is running, we needed to frame capture and produce data from it. About this last part of capture the frame and producing data, Unity Simulation was the answer to our many questions, because first of all, it was easy to integrate in our project, and it was easy to customize the kind of data that we needed to export, and in our case, we speak about joints and the skeleton that we saw before on the image. But most of all, it provides us with a very fast frame capture, together with a very reliable JSON data output. And this is crucial for us. It's very important, because we need the JSON data that we produce match perfectly with the frame that we are recording in any given time. Otherwise, the data that we produce become useless. And luckily, Unity Simulation gave us this solution. Also, thanks to Unity Simulation, we can generate our frame in a different variety you see here. Three of them, let's say, the standing one in big. Then we have the occlusion mask that is useful for us to understand what joint is visible and what isn't. And then, we have the rendering as well. After all, we can also  configure the simulation. It means that we can configure several simulations that we want to run one after the other. And we can run them in the cloud, and this is very comfortable, because once it's configured, we can launch it and forget about it, let it work until the data are ready. And then we can retrieve the data. About the Unity navigation system, it is another component that we use heavily. Since that we need our virtual customer to navigate through the store, and of course, they need to do it in a realistic way. And thanks to the navigation system, we could set in a real-time workable area that we see here in blue and configuring our virtual customer in a way that they will avoid each other. They will avoid obstacles that we mark, in this case, the shelf, for example. And they will walk through the store, always in a different way, in a different path, because the path is generated in real time. Thanks to Unity navigation system, we could actually achieve a scalable Scene because we have virtually no limit to the amount of people that we want to instantiate in the Scene, we can decide we want a single customer, 10 or 30, 40, just on demand. And also, we can let them reach a different target position that, in this case, we see them marked with these red boxes, and this target position can change at any moment. So, also, the Scene becomes scriptable on a certain level, and we can generate and control a realistic flow of people through the store, according to our need. As I told before, we not only want that our virtual customer translates through the store, but they actually need to behave realistically. To achieve this, we use Unity Animator together with a rigged model that we produce externally. And in this case, the Unity Animator can apply to rigged models different animation clips that can be imported from different sources. And manage the transition between these different clips. Also, this transition can be controlled via several variables and achieve a higher degree of realism. In our case, we coupled them with the navigation system, and we could achieve a realistic transition between, for example, walking and running or during animations. This always happens in real time according to the behavior of the NavMesh Agent. Also, thanks to the animator, we can play on demand other animations that we can add, for example, picking of an object, crouching of the customer and others that we can add in the future. This also gave us another level of scriptability in our Scene. Here, we have a simple demo of what I was talking about. This time we have our virtual store with several models. They walk, some walk, some are behaving in a standing position, but still moving. And while we see this, we are at the same time producing data of this specific thing. Once the data is ready and is given to our system, this is what actually our system sees. This is what we call "machine eye." And we see that these data basically can be perfectly manipulated by it, and used for different tasks. Now I'll let Tushar finish the presentation about the results. Yeah, thanks a lot, Mattia. Yeah, basically, just to conclude, I think there are a couple of interesting things that you want to think about is. One is you want to test edge cases without having to recreate them in the real world. So, an example of this is now that we have the pandemic happening, and we are hopefully following social distancing rules in most parts of the world, there is an importance on will the system work after social distancing goes away? And are you overly optimizing for a system where people are not are socially distanced. Because that's an important edge case that is hard to recreate in the real world today, which might have been feasible about a year or two back. The other part that is really valuable is the ability to validate against algorithmic performance bottlenecks. If a certain algorithm is high CPU, high I/O load, we really want to understand that before we deploy our system, because once we deploy our system and deploy it to multiple sites, it has customer impact, and we want to minimize as much customer impact as possible by validating performance internally. The third part I think where synthetic data has been really useful is the ability to understand complexity in new environments. And by new environments, this means completely... environments where there is no prior... like we've not deployed our system before. This could be like a new type of store. This could be a new type of lighting scenario. This could be a new type of traffic pattern, a new type of age demographic. We can build validation of our system in different scenarios, whenever required and this has been really awesome. I can go to the next slide. There are a couple of things where I think Unity was able to quickly come in and address some of these problems. One was the time to validate. Previously, if we had a new idea and see how quickly we can validate this, if our algorithm will work in this scenario or not, it would take months because you had to assemble some people, put them in a single store and collect that data. After using Unity, we've been able to cut down that time to hours where you just render a new Scene and run the algorithm and just try it out. Similarly, with the number of people. We've been able to... Like if you want to validate a system with X number of people you need X number of people in the store. And getting X number of people to a physical location has become all the more harder in the last six to nine months. That's another big win for us where we can remotely just render and generate some of this information. The number of store variations, I think that's an important aspect where we want to be able to validate our system in hundreds and thousands of store variations, and it should work. That's where we have started using a lot more synthetic data to try out in different store variations because it can be in different kinds of scenarios, it's not just at a single type of store. There are stores within buildings, there are stores in varied kinds of scenarios. And I think the most important win for developer velocity has been an iteration of the algorithm development would always involve thinking of certain edge cases. And from those edge cases, you kind of want to... the iteration cycle is reduced, because now if you have a new idea, and you don't know whether it will work in this certain edge case that you're imagining for your algorithm, you can just generate the synthetic data and validate your algorithm against that new edge case with that data. Overall, that's been the biggest wins. You can go to the next slide. Just to wrap up, what is our future direction? I think the important parts are what happens, the "What if?" question is where synthetic data can come in and really give us at least 90-95% of the answer, and the last 5%, you can't predict till you encounter them in the real world. And then, by answering questions like, how many cameras are enough? Can we create different types of cameras and see if our system actually performs, and then object detection using the Unity Perception tool. Overall, we are still very early in our adaptation of synthetic data and simulation at Standard, but our direction is towards minimizing the cost of data collection, minimizing the error in data labeling, and maximizing overall performance of our system. Thanks a lot. We can move into Q&A. [SHARMILA] Good, thank you to Tushar and Mattia. We have a few questions, and everyone on the session, feel free to ask more questions. The first question we have is "What do you do with privacy concerns or GDPR regulation?" I can take that. I think when you look at synthetic data, there is no GDPR regulation necessary around it because this is not real data. If the question is around the actual data that our system generates, a lot of this data is already very hyperlocal, like the cost of sending data across data centers, and replicating them across geographies is very expensive and the cost prohibits us to have bad privacy decisions. That's sort of how I look at it. I hope that answers the question. [SHARMILA] Awesome! Then the next question is, "How do you handle extreme cases, like a Black Friday shopping spree with extremely fast shopper dynamic and crowded or camera obscuring cases?" Mattia, do you want to take that? In terms of simulation, there is no problem. We can just write 100 or 1,000 as the number of persons we want to instantiate, and we will have them. And in the way that the system is built, we can also make sure that they will not compenetrate inside one of the other or inside of other objects. So we can definitely produce reliable data in a Black Friday scenario. And this is what I can answer in terms of synthetic data. I don't know if this was the question. [SHARMILA] Yeah, yeah, sure. And then, do you have a system to procedurally generate different types of stores? Mattia, do you want to take that? Yeah, we do have a system. It is internal, so we don't want to share more information about that, but at a high level, we do have a system to procedurally generate stores. And the thing is, procedurally generating stores can mean a lot of different things to a lot of different people. It could be as simple as a simple geometry of a store. It could be something which actually renders the shelves, like in a photorealistic way. And on that spectrum, you can have any... we generate data based on our needs, and not based on what is necessarily the best looking or the best procedurally generated process to generate a store. I hope that answers the question. [SHARMILA] The next one is "Did you notice any powerful algorithm that was created by machine learning itself so that machine learning algorithm is solid and then you can trust them with human brain decisions, For instance, ML brain that is much more intelligent than just one task?" I think that promise exists, but the reality is a lot of practical problems are encountered which ML can't answer. An example of such a situation is how do you even pick up an object, like building an ML to pick up an object in a deep learning method is fine. But as soon as you come into the real world, and you have a robotic arm that needs to pick up the object, the problem just becomes ten times harder. I think the idea that you can build something which learns on its own is great, but when you're interacting with the real world, a lot of the assumptions the ML system makes actually break, and most of your time is spent on solving all the assumptions that broke in the real world rather than what the ML system was actually performing at. It's good for tiny tasks, but when the task gets complex, ML has not yet been able to prove positive in that sense. I hope that answers the question. [SHARMILA] OK, cool! "Your videos or images do not look very photorealistic, is this important?" [TUSHAR] Mattia, do you want to? [MATTIA] Yes, I can take this. As we told, this is still an early stage of this synthetic generator. And at this point, we use these data only for tracking purposes. And in this moment, the realism was not needed yet. But we already have the plan to move on to the HDRP rendering pipeline, so yes, we will get closer to realistic images. [SHARMILA] Kind of similar question to what you answered, Mattia. "How do you deal with the domain gap between realities?" I always thought that this was one of the points or one of the next points. I was working already on this specific thing before joining Standard, and what I can say is that, at this moment, the artificial intelligence, they still understand if an image is synthetic. It doesn't matter how photorealistic it is, because there are several other aspects that are not about realism, but they are more about camera noise, other physical aspects that are embedded in the camera, for example, or in a physical system. Basically, these kinds of features are invisible to the human eye but are visible to the artificial intelligence, and it is still an open question. At the moment, there is not a 100% solution. When I was working on it, I could achieve some progress, adding noise filters and, for example, also chromatic aberration, and these kinds of effects. But my opinion at the moment is that they still feel synthetic for an artificial intelligence eye, so I predict that in maybe some years, there will be an answer at this point. [SHARMILA] Awesome, another question was "How to ensure there are no blind spots for both people tracking and especially product tracking?" From a synthetic data perspective, I can answer it. I think you can simulate those blind spots. [SHARMILA] Actually, the question also refers to the real application of the models. That's what Claudi who asked that question said, if she's referring to the real applications of the models. Yeah, that I think is a fair question. We solve problems that we encounter in the real world, and we predict as much ahead of time, like what kinds of occlusions might occur in the real world, and that's a part of the journey of figuring things out with a mixture of the real world and that's the last 10% that I was referring to. Synthetic data can solve some parts of the problem. And the last 10%, you have to observe the real world, and then observe patterns and trends as you get into more and more stores to understand what are realistic scenarios, and what are situations that we are imagining. It's mostly an iterative approach where if a certain situation occurs in a certain place at certain frequency, then we will address it. But if it occurs once every 100 days or something, we might decide not to address it at that point in time and let that situation sort of reoccur enough number of times before we prioritize some of that work. I hope that answers that question. [SHARMILA] OK, the next one is a question that I can answer, because John had asked, "Do I need Unity Simulation subscription to use the Perception repository?" John, you do not need Unity Simulation subscription for using Perception. Perception is a completely separate package, and it's available on GitHub, and it's open source, and I had provided the link in the chat window. And the next question is, "You mentioned object detection. Can you expand on how you're using synthetic data for this, and what challenges you've found?" That's currently a work in progress. We've just started that initiative internally, so I can't speak much to it. I hope that's OK for now. Maybe at a future time, we can do another session about that. [SHARMILA] Yeah, keep in touch with Tushar, and he can let you know when he has an answer. Then the final one is "For different types of cameras, like with different field of views or some distortion in the lenses, should it be trained with different camera simulations, or is there any other suggestion?" I can take that question. The way we look at different cameras is we want to understand what cameras are needed for our specific problem. It all depends on your problem. If your problem has multiple cameras in it, and you want to train a single model that works across multiple cameras and multiple lens distortion, and different fields of view, you should train it based on that. But a lot of times, since hardware is so expensive to replace and service, it makes more sense, in a lot of situations, to just use the same camera, and then process the image stream in software rather than in hardware, and use different pieces of hardware, because the cost of managing hardware is so high that we prefer to just standardize on the camera rather than use different types of cameras and different lenses, because they create different kinds of problems. As you increase the variance at that hardware level, you want to minimize what gets installed in a store in terms of the variance of hardware. I hope that answers your question. [SHARMILA] Yeah, again it's on the similar camera thing. "Can you use real-world capture data to train your AI characters how to behave? That way, you can record some base cases and then let the AI randomize many variations for the computer vision training scenes." And the person also said, "My thinking is that programming the AIs rather than training the AIs might introduce a bias. That, I think, is a fair point. Maybe if I just try to understand it correctly. You want to record some base cases and have AI randomize them. This is fairly traditional. It's fairly common in any deep learning application where you randomize the data, and that's not necessarily... We don't use synthetic data necessarily for that. But you kind of perturb the data with some variation and make your model robust enough to that already. Where I think synthetic data shines a lot more is when you are trying to do a very complex task, and there is a lot of sensor fusion happening, there are a lot of systems in play and working in parallel, and you want to work off a common data set and not operate in isolation. Whenever you are taking isolated problems, that method is typically what is done in most deep learning applications, any which ways, so... [SHARMILA] Awesome! I don't think there are any other questions. Thanks, everyone. Those were some really wonderful questions that you've all asked. And in case you have more questions, I don't know, Tushar, do you want to give yours or Mattia's contact, or some LinkedIn profile so that people can reach out with some questions. I'll put my Twitter there. [SHARMILA] OK, awesome. Feel free to reach out to them and check out all the other links that are provided there. Any final words from you, Tushar and Mattia? I don't know. I just think we are in the very early stages of AI. The real world is harsh, so don't assume that synthetic data will solve all your problems, but also use it as a tool whenever it's cost-effective and speeds up your development. I don't know, Mattia, what do you want to do? Well, I totally agree. I think this is a very new area, but I also think that in some years, we will have a lot of answers that today seems impossible. [SHARMILA] Yep! I guess what you're saying as a conclusion is synthetic data is not a magic wand, but it's just like one of the several tools in your toolbox that you can use to solve the very interesting challenge of making your autonomous systems behave as close to humans as possible. That's awesome. Thank you so much, Tushar and Mattia.  This has been great. And thank you for walking through all your slides and showing us this awesome work you're doing at Standard AI. Thanks so much, and thanks, everyone, for joining in this early in the morning and across different time zones. Not just early in the morning, it's late in the night and across different time zones. We appreciate it, thank you so much. Have a good day. [TUSHAR] Bye. Thanks a lot. [MATTIA] Thank you. Bye.   [MUSIC]   