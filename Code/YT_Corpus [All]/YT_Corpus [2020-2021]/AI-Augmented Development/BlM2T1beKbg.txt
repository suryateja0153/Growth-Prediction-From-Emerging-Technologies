 [Music] before we do a deep dive on the fundamental principles of AR design I want to give you a quick recap of what's new and aerocorp this year specifically five things seen viewer improvements to cloud anchors improvements augmented images augmented faces and a new set of lighting api so we call environmental HDR let's take a look at each of these in detail scene viewer is the 3d AR immersive viewer that lets you run a Maitre that runs natively on Android and allows users to seamlessly put any 3d content that they might encounter on a website into their world here we see an example of how NASA is able to quickly let users place a Mars rover in their living room just by hosting the gltf file on their web site and developers and designers don't have to worry about all of the subtle details of AR object manipulation because you get all of this functionality for free next up cloud anchors cloud anchors enable developers to create multiplayer and social AR experiences technology at times can be a bit isolating but that changes when you're designing applications in a our thanks to cloud anchors users aren't just engaged with the world around them but they could be more engaged to the people around them as well as here we're also adding a number of improvements to augmented images AR core and these can be used in a wide range of different use cases you know things like retail and educational experiences here we see a user that's learning about features and expressive machine and this is you know sort of a small-scale use of it but it can also be very large scale and it can scale all the way up to large venues at i/o this year we've actually placed 50 sign posts throughout the campus at various different locations these help users three wayfinding so if you're if you're not at i/o and you're watching on video I here's how this works you can look at one of these one of these markers these documented images and then we get an error layer that's superimposed on top of the conference you can find all the different venues or sandboxes that you're trying to navigate or way find you to of course the front camera can be just as important if not more important than the back camera on devices so this year aircore is also adding support for augmented faces Air Corps provides a four hundred and sixty eight point 3d mesh the track head movements and changing facial expressions and we use machine learning to do all of this without requiring a depth sensor or any special hardware on the device and it's worth of existing phones out in the market they are core teams also leverage machine learning to make really dramatic improvements in our light estimation API which we call environmental HDR here's a quick example I'll start by placing a rocket out in the scene starting with no light source then I'll just add a quick directional light source and this is you know how air has previously worked it's it's not yeah particularly well lit but now we can actually add ambience fearful harmonics a cube map it turned the cube map on and then finally environmental HDR and working together these lighting api's actually you know make the shadows and lighting considerably more realistic here's the the final stage here as we turn in turn on environmental HDR takes it a bit of a moment to understand the scene you can see that kind of start to lock in and then as you move around you get really realistic lighting reflections and shadows on the on the objects here's another example with environmental hgr this time is just being turned on and off the mannequin on the left is in AR and the one in the on the right is we're real I'm not not actually real it still mannequin but get the idea and you can see really dramatic change as environmentally she chairs being turned on and then finally here's one more example in this case the light source is actually moving and you can see the system actually adapt in a real-time and start to change the virtual light source in response to the change in the real light source the team's really made significant jumps this year and the fidelity realism and immersion that air apps can provide to users but as great as these new technologies are it's equally important that your air app is also really well designed cruise foods remain in Eugene OR from the air core UX design team and they'll get you up to speed on the fundamentals that they are interface design thank you Alex hi everyone I'm khushboo UX researcher an air core I'm here to share some insights about air design and we can Group a our design fundamentals in for rough categories environment user air content and interface sorry so first let's talk about environment in air we define environment as physical environment as well as augmented environment physical environment is a real world inhabited by users it could be endorsed as you see all outdoors so the first step to designing an ER experience is to define where user will be using your app and then accordingly select the environment up front and we recommend that once you are defining the environment up front help users identity identify what could be the ideal conditions for using your app in their environment or space an augmented environment a virtual environment rendered by a Akoya by integrated virtual content with the real world in order to create an augmented environment Airport is continuously improving its understanding of the real world and providing you with surfaces where you can land your assets or build your experience further why are you creating an eard experience you can leverage air force capability to build surfaces over time to your advantage for example as you see on the screen here frog is jumping from one surface to another and encouraging users to explore the space and here while the user would move around you can slowly build the experience going forward well the journey of understanding the real world comes with its own set of challenges for instance as you see on the screen when the environment gets too dark Airport can detect such situations and provide feedback and today our call can detect insufficient features too dark or low let's out of environments and excessive control you can now use these capabilities to understand users context and prompt them with contextual guidance and avoid any kind of friction now after you've selected the type of environment your second step would be to define the experience size and experience size could be a scale as you see on the screen room scale of world scale when we design for table scale it allows users to engage in AR on the smallest of surfaces and a tabletop experience would influence how you design the scale of your assets and the potential play space whereas with room scale it basically expands the potential for delight and magic so you can leverage multiple surfaces create larger-than-life assets for users to explore and immerse in the experience with world scale which we have no screen size limits you can leverage procedurally generated content and build your experiences with time however public spaces you might encounter some unique challenges ranging from a difficulty in tracking and occlusion depending on the number of objects and people present in the scene so from the private space of a room to the vast openness of a real-world environment consider where users may be using your app and anticipate and communicate any kind of potential challenges that that might hinder their physical capabilities of movement in the same and big spaces that are most conducive for the task at hand users what's an experience without our users right and they are the most excited and ready to be delighted lot well with air we have transitioned from the 3 2 inch 3 the 6 inch screen that used to be in our mobile phones to a screen size which is which has no limits today and this creates a completely new and exciting paradigm for users to experience however several years of our smartphone use and the field of view limited to that small screen has led to some behaviors and habits that can influence users behavior in an immersive experience so I am going to be talking about these behaviors and specifically touch upon perception movement and comfort as three aspects so now let's talk about perception and how users perception can be defined by a screen while using smartphone applications user attention as on the flat screen of the 2d screen that you see whereas in an AR app an immersive environment of the user they screen sizes limitless and that can be defined as seen and how could seen influenced user behavior and perception let's figure it out when users are experiencing an error environment their attention is now being pulled from what you used to be the screen to the scene and creating some sort of emotion and emotion is great that's what we are looking for right users a truly engage with your app and it's good to go but there are some kind of design challenges that have that we've come across we found that users can completely forget about their surroundings so they'll be so immersed in the scene that whatever that's around them they'll forget about it and immersion can limit users peripheral vision and what we call as cognitive tunneling the best way to understand would be that they have blinders on and these blinders are aren't just to do with just the surrounding environment whereas my users are immersed and engaged in the experience don't be surprised if they've completely forget about the UI that you have on the screen and we recommend you to reduce the number of UI elements that you have on the screen and try and introduce them in the scene itself and wherever we are building any kind of flows where the user is allowed to move back and forth from a screen to scene try and make it either less frequent or try to away avoid that completely now here I have an interesting concept but before I introduce that to you could you tell me what's happening on screen right now anyone anyone okay so yes a little pet is being reduced in size but what's happening here is that one of the left is reducing in the size and on the right is translating in the scene so what we've observed is that there's a altered perception in terms of depth distance and scale and this is one of the common behaviors that our users encounter when they are experiencing an AR app once they are translating an object in the scene they tend to perceive it as a change in size and one way to solve this possible challenge is to encourage user movement so that users can correctly interpret the interaction that you're building in now we talked about perception let's talk about movement mobile air is inherently an environmental medium that invites movement and motion from the user and coming back to smart phones again as I was talking about it smart phones have a strong impact and user behavior and it most evidently can be seen in movement and if you look around we feel that we are so immersed in our smart phones that we've developed some sort of an inertia so if you are stuck in a place we're just stuck in a place and we would move around until there's a major reason to change that and many of our users have never experienced a 360-degree environment they're still learning to comprehend the ability to use their spaces as an expansive play space here from an engagement standpoint they are not really engaging with your experience but now they're actors in your experience as well and you could use movement as an input for the experience that you're building and I'll be talking about that going forward so now they're stuck at a situation or others at a pose how do you encourage them to move and one approach that we have here is to leverage any kind of off screen indicators that encourage users to flow with them and once users start moving it will be hard for you to figure out what direction they're going to move in so it will be good for you to guide the users about the kind and the range of movement that you intend for your app and by creating a clear directional path as you see in we'll hear another approach to make use a moment as an input to your game play could be that you could leverage users curiosity by placing objects that are partially occluded to encourage them to move and it also gives them more agency to start exploring the space and while we're talking about movement I'm sure you must be thinking if it is critical to your experience or not is it really necessary so one thing that we can keep in mind is trying to figure out what are you going to use movement for is it going to be an input to your experience or is it something that you're using to explore an object or play space for the user so for instance as you see the visual in the screen here there's a bar graph so for while you creating a utility app experience spinning the graph might just be easier instead of forcing the user to move around it so be mindful of how you are using user movement while creating your app and now that we've covered perception and movement I'll be talking about safety and comfort because that's crucial for any our experience we've observed sometimes our experiences are built for long-duration game plays and holding your phone's still for a really long time can be very very tiring and you can build your experience which can encourage either natural movement or try and build resting points in the play cycle and when user wants to pause or let's say walk out of the scene help them store the progress not lose their data and spare them any kind of effort that they're going to put in to reset everything when they return avoid making the user do anything which is physically demanding uncomfortable or abrupt avoid interactions that require the user to walk backward like the way you see on the visual here like pulling a slingshot sort of a thing remember we talked about the blinders behavior correctly planning for this behavior is even more important when a user is outdoors for example I'm sharing an a I'm sharing a or navigation with you that we are building right now and if a user starts walking with the phone we strike give a subtle match to pay attention to their surroundings and if they still continue to walk forward with their phone up we insert a bit more with using a screen overlay that effectively prevents the user from continuing to use AR so this is just one example that we have where you can optimize for user safety together our role as creators is to make sure that a product is not only helpful but also responsibility designed and now that we have understood the fundamental concepts of environment and user Jermaine will be talking about how to utilize some of these concepts and build a account and in your experience thank you yeah hi everyone my name is Jermaine I'm a UX designer on the El Corte team and I'm going to walk you through the next big step in your journey of developing in your app which is the our content so basically for the next ten minutes I'm going to talk about how to display our content and our to interact with it so the first question you should ask yourself before thinking about adding content to the user environment is how would this content fit in their surroundings basically how to make this object look real and when I'm talking about real or realism it's not much about being realistic object in the scene but more about entering the presence of any kind of object in the environment the goal here is basically to maintain the user suspension of disbelief by making sure that your augmented content fits present and credible this is really important so the user can be immersed in your experience and believed in the story you're trying to tell them in order to achieve this presence there are multiple lever you can play with and I'm going to walk you through three major ones the first one is actually happening during the creation of the asset itself because today a or mostly means smartphone AR it comes inherently with certain constraints specifically when it comes to graphic so you need to find creative solution to achieve the aesthetics you draw you're targeting a very cost efficient way of improving definition and level of detail the model is to use a multi layer texture model such as PBR physically based rendering model supported by most a or development tool this is the cheapest solution to achieve an advanced degree of detail for your objects and it works perfectly fine in AR for example you can see on the screen by adding a normal maps to the object I'm adding a lot of detail and a lot of wrinkles to the asset without any adding any additional geometry the second element you want to pay attention while designing your experience is the lighting as Alex mentioned previously our core is now coming with a new set of API s that will help you to create dynamic lighting and shadowing error contents would then react accurately to the real word changing of lighting judgement of lighting this is a very crucial factor to ensure realism as a wrong lighting with instantly break the immersion and if you want to go beyond with dynamic lighting you can also imagine using lighting as a trigger as a trigger behavior or condition for your gameplay as you can see here not only the asset is reacting to this changement of lighting but it's also adding a new and u content based on the fact that the lighting is changing and finally another element that you can play with to increase the credibility of your experience is to imitate real like physics and apply this to your virtual object this will not only bring a lot of realism but can also create some fun scene and and and delightful situations such as the one here where basically an object is moving from one object to another without any user input so now that we have defined our your object will look like in a or it is important to think about how they would behave and more specifically over the user interact with the assets for the past couple of months we've been connecting our research to define or object manipulation model and we're really happy to share it with you today we basically found 10 attributes that we feel can contribute to create a good object manipulation model for AR the first attribute is to always have a selection state regardless of the number of element you have in the scene it is important that the user can precisely select one of them before are planning any a transformation to it also it is important to visually communicate that the object is selected we call that selected state so as you can see here we're using a ring that on only show that the object is selected but also show the surface it is currently placed on there is many different way of showing that an object is selected feel free to create your own that would feel best with your aesthetic the second attribute is for user movement so one of the first thing every user would do when trying an arrow experience is to move the object around that's why we decide on creating a very simple interaction a one ticket touch and drag it is a very common gesture and people are really used to it from 2d apps if the user is trying to move an object further away they can also maintain the touch on the object and move physically with their phone in the space it is also crucial to make sure that the user can select the object and move it in one single touch this would avoid a lot of friction while moving an object from one surface to another be sure to maintain a height of the object so it always stays under the finger of the user also it can be a good idea to highlight the destination point of the object on the next surface 12:00 the user precisely place it when a user is moving an object in distance it is important to increase the size of the invisible touch area or a box in regards to its distance from the screen otherwise the user won't be able to bring back an asset that if pushed away without having to physically move towards it and for the exact same reason you want to make sure that you have a minimum and maximum placement distance ensuring that the user can always interact with the assets without having to walk and look for them when it comes to rotating an object there are basically two method that we recommend we found that most user which first try a two finger twist on the screen to rotate the object this is like the most commonly used interaction into the apps however one finger rotation feels more natural in a yarn since the swipe gesture maps to the axis of rotation of the object so we basically recommend to implement both methods to support both discoverability and also more natural interaction the next attribute is about scaling so this would work exactly like you would expect in any order to the app performing a pinch to change the side of the object since two finger also used for rotation adding a slight threshold can be helpful to disambiguate between the two interaction and because we are in AR is crucial for your object to have a minimum and Max sighs so this will ensure that the user don't make objects so small or so large that they can no longer interact with them you can communicate that the user reached the boundaries by adding a small bounce effect as you can see on the screen the next attribute is about elevating an object after a lot of research we discovered that the most important way of elevating an object is to perform a two finger vertical swipe on the screen up and down also I want to bring your attention to the fact that it is important to visually connect the assets to the surface it's placed on two this will help the user understand both which object is selected and what effect they are having on this object and the final attribute that I'm going to talk about today is about negative elevation in two words do not let objects sink into the surface they were placed on not only would break the rule of physics and instantly breaking the immersion but also he can on a more practical level lead to a situation where the user might lose assets by bringing them below the ground level so basically to sum up we've put a lot of research and refinement in creating or a on object manipulation model and we hope that our findings can really benefit to all of you and because we know that a good demo is always better than words we've created an open-source demo as part of the arrow SDK that you can download today and that implement all these attributes for you you can download the SDK today and quickly add object manipulation into your project so now I'm gonna give the stage to Eugene it's gonna continue walking you through the journey of developing an e-wrap and talking about UI components thanks Jamie hi everyone I'm Eugene I'm also an interaction designer on the Eric or UX team today I'll be introducing you to the last building block of designing for AR being an interface historically every successful platform has introduced a set of UI widgets for developers to craft applications and this air is a nascent field itself we are also exploring which set of UI components makes the most sense for volumetric applications if you think about it the interface in the viewport for AR the house magically bridged the gap between users in the environment the size of a phone screen is literally the size of a user's field of view it also serves as a control panel for your users to magically manipulate error content through gestures or UI widgets and knowing that the interface accounts for both view and input as creators who want to create an interface that's engaging intuitive yet claim to now block users view and to start with some familiar 2d screen UI patterns do carry over to AR especially in initialization and settings controls a lot of applications features a combination of 2d and error modes and in those apps the journey of AR starts from the moment when a user tab on the launch into error mode button Google has now created a set of call to action icons for accessing air with different intentions the icon set features a consistent bracket frame metaphor symbolizing launching into air experiences in the camera life in the camera viewfinder and we believe that using consistent UI icons across applications will really help acknowledge user entrance into AR and as indicated in the iconography air experiences are heavily dependent on camera Caesars however through research we have found that majority of users especially those who are not familiar with AR would drop off during initialization stage when they're asked to give permission to their camera usage which is simply not something that is required for most 2d applications this means that most of user don't even get to experience air and have already left the experience without realizing it acknowledged in that we weren't meant you as creators to communicate the intent of camera usage explicitly before permission prompts to help give your users more context on why camera usage is necessitated for the experience you have built and in cases where 2d screen you eyes are present during runtime like settings controls or 2d galleries it is important for these UI components to feature responsiveness the UI components should remain at the same location but rotate in reaction to the device orientation to enhance for user comfort in meanwhile you don't want to interrupt the camera feet with a rotation animation the feed itself will give you the visual feedback knowing that the real estate of your phone string is really limited and precious a great way to help make a more room on your screen and with realism into the air fabric is through leveraging more volumetric UI UI components like icons chips and cards are translatable to annotate air objects with different volume of information with muons and spatial behaviors we will love to think of these behaviors as volumetric responsive design uniquely air annotations can vary their physical forms in reaction to user movement and interactions such as focus and select to maintain eligibility of information and a delight we suggest you to define a minimum default a maximum target size for annotations to be displayed a different movement range with varied amount of info this rule also applies to different selection States to help inform context when users are moving their devices between different annotations you will want the one that's currently being an unfocused or that's currently being selected to be shown in its full size or form where reads the other ones can optionally stay at a reduced form on top of that error annotations can also rotate to preserve readability and deliver context like what you can see here on the left having an annotation to always rotate to facing users can be overwhelming at times especially when your users it's constantly moving in their space or when there are multiple annotations in the scene at the same time knowing that we suggest you have annotations remain static and only rotate when it is on focus so that we can maintain readability of these annotations and inform context when necessary another thing to note at that you're designing for these rotation behaviors we suggest you to be mindful of the spatial relationship between the annotations and other virtual objects to avoid collision similar to the responsiveness of text objects can also adapt the size to environment you can use a hologram to offer a preview for users to understand the volume and scale of the object this greatly helps set their expectation before placing or anchoring in an object firmly at a location and this will help to set routines with a lot more success and in cases where a virtual object does appear to intersect with a real-world entity like a wall in Rome you also want to communicate visually how much space is needed for a proper placement so that you can prompt a user to move and reenter the object another way to look at the responsiveness of object is the adjustment of elevation physical environments are not always perfectly designed for AR experiences for instance let's say you're designing for a tabletop experience like a chess game but your user simply don't have a table in their environment what should we do we suggest you just go ahead and create a virtual table that's generated based on your device orientation in the distance between the device and the detective service this gives your user flexibility to align the experience based on their eye level and optimize for ease of access and user comfort and like who shmoo has mentioned earlier when users are fully immersed in the air' story most of the times we don't want to interrupt that we have observed situations when users are freely exploring around in the air experience and would occasionally approach their device closer to an object to simply test and see if there's a point of interest which sometimes ends up in surface collision between the device screen and the surface of the virtual object causing the virtual object to suddenly disappear from your air experience in order to inform your users that this is an unintended behavior instead of triggering an Enter - warning notification we suggest to handle it more elegantly one way you can do it is to have and use camera effect to subtly convey the idea that users are entering an unexpected territory while still keep their attention in the scene in 2d interface design notifications sometimes show up in forms of popups in AR we want to avoid its usage unless users intentionally choose to launch them the ones that are showing up out of blue can easily take over the viewport and instantly breaks realism by shifting your users mental model from the scene back to extreme based browsing experience now that we have talked a lot about design basics for building quality air we're also super super excited to introduce you to a set of tools that are available over our core platform to help you as creators to embark on a journey of air production we're constantly launching updates for air core with a set of SDKs that you can simply grab off the rack and hopefully expedite your project development in a recent release we have shared a couple of demo scenes with MA tracking and all above-mentioned behaviors of object manipulation that can use and customize you can also download the air core elements app from Play Store where you can find and play with all the UI components that we have shared today and if you're into reading we also have a textual documentation of all of our learnings packed into an augmented reality design guidelines that's available online feel free to check out these resources and keep them by your side when you're working on your air projects and the air is peaking over its horizon we'd love to see more inspiration spawned from your project and hopefully we'll be able to see more lessons learned and questions raised about AR thank you for joining us today in person or on livestream hope you enjoy our time at AI all [Music] you [Music] 