 Hi, I'm Whitney Espich, the CEO of the MIT Alumni Association and I hope you enjoy this digital production created for alumni and friends like you. I am chief executive officer and publisher of MIT Technology Review, and it is a pleasure to be with all of you today and moderate this virtual forum. This year, by popular vote of alumni, our topic is geniuses and game-changers. We are going to spotlight four prominent MIT thinkers whose impactful work has been helping to make a better world. You've just heard from our first two speakers, Paula Hammond and Deb Bloom, and I'm glad you're joining us to hear from two more, Josh Tenenbaum and Ed Boyden. After these two speakers share some of their current work and research with us, I will moderate a Q&A with them using your questions from the Q&A form on the page. Please feel free to add your questions to the listing there, or upvote other questions you'd like us to have the two panelists answer. Click on the thumbs-up icon next to your favorites. Please welcome our first speaker, Josh Tenenbaum, a PhD from the class of '99. He is professor of computational cognitive science in the MIT Department of Brain and Cognitive Sciences. Welcome, Josh. OK. Thank you. I am really excited to be able to share with you all today the work that we've been doing, not only in the Department of Brain and Cognitive Sciences, but also in CSAIL, the Computer Science and AI Lab, the Center for Brains, Minds, and Machines, and the MIT Quest for Intelligence. And the work that we're doing here is, as we like to say, at the intersection of the science and the engineering of intelligence. That means our deepest motivation is the basic science question of how does intelligence arise in the human mind and brain? And we put the insights that we learn to use in engineering more human-like systems for machine learning and intelligence. I want you to start off by asking yourself, why do we have all these AI technologies, but no real AI? That means we have systems that do very useful things that we used to think only humans could do. But we have nothing like a flexible, general-purpose common sense that each of you can use to do each and every one of these things for yourself. So why not? What's the gap. Well, today's AI is based on systems for deep learning and reinforcement learning that themselves were based on science done in the 1970s and '80s, very simple equations worked out to capture the most basic kinds of animal learning processes. Think Pavlov's dogs or a rat in a maze. These systems are finding associations, detecting patterns, but it's not really understanding. And as a result, it's just a small step, at best, towards real intelligence. Real intelligence is about modeling the world. It's not just recognizing patterns, but explaining and understanding what you see. It's being able to imagine things that maybe you've never seen, maybe that nobody's ever seen, but that you could make real by making plans and solving problems that come up along the way, and then building new models as you learn more about the world, learning from your successes and your failures. My goal is to capture this kind of intelligence, to write down its equations, and then to use what we learn to build smarter machines. And we're starting with young children because they are the original model builders, and we still don't have any AI that's as smart as even a 1 and 1/2-year-old child. But imagine if we could get there. Imagine if we could build machines that grow into intelligence the way a person does, that start like a baby and learn like a child. This may not be our only AI, but it could be our best bet, because think about it. A human child is the only machine in the known universe that reliably, robustly, repeatedly grows into human-level intelligence starting from much less. And we know that even small steps towards that goal could be huge. So we're starting with the most basic kind of intelligence that's in everyone [INAUDIBLE] but no AI, intuitive physics, like you can see here, that a child uses when they're playing with blocks or stacking up cups. We don't have any things like this in robotics, despite great advances. Or intuitive psychology, like you can see in this kid here. The ability to read minds, to see somebody doing something that you've never seen before and to figure out what they're doing, and why, and even how to help them. Watch what this kid does here when the adult stands by. If we could build machines with this kind of intelligence, this cleverness, this tendency to help, it would be amazing. In order to do this, we've had to make a number of technical advances, and we're just getting started. One of the most important is that we've had to develop new AI programming languages, what are called probabilistic programs. These build on, but go far beyond, the deep learning, which is the big AI advance that you've all been hearing about and many of you might even work with. Probabilistic programming languages allow us to combine the power of deep learning and neural networks with other great ideas from different eras in the history of artificial intelligence, which don't fit so neatly into the neural network toolkit. That starts with symbolic languages for abstract knowledge representation and reasoning, or probabilistic and Bayesian inference, the ability to reason about causes and effects from patterns of sparse, uncertain data. Bringing these ideas together-- symbolic, neural, probabilistic approaches-- let us engineer new kinds of AI and reverse-engineer the roots of human common sense in a way we couldn't do with just any one of these paradigms alone. Everybody's asking me, what comes next after deep learning? Well, my bet is probabilistic programs are what you're going to be hearing about in a few years, if you haven't yet. Now, another kind of programming language we use are game engine programs. You might not have thought of these as tools for AI, but think about it. The tools that allow a videogame designer to create a new game without having to start from scratch-- physics engines for simulating a player's interaction with a world of 3D physical objects, graphics engines which simulate what the world looks like as the player moves through it, or AI planning engines that allow the game to model the behavior of non-player characters-- these kinds of tools wrapped inside a powerful framework for probabilistic inference allow us to model the basic kinds of commonsense knowledge that are built, we think, into babies brains, where children start learning. These tools, coming over to the engineering side, have enabled us to put intuitive physics into machines. Just in the last couple of years, I'm showing here a few projects that I collaborated with a number of robotics groups. We're using these tools. We've given the robots some kind of common sense intuitive physics, the idea that they can not only stack up blocks, but even play the game Jenga, or figure out how to use tools, even new tools they've never seen before. Figure out how to make sushi, or at least the rice, to pour ice water, or even learn to walk. The next challenge is model-building. How could a brain, a baby's brain or a machine, learn an intuitive physics model, let's say, for itself? If these models are probabilistic programs, then a learning algorithm has to be an algorithm that writes algorithms, a program that writes programs. It's what you might call the child as coder, or at MIT, we would talk about the child as hacker, because think about it. We all know this. The things you do when you're hacking on your code to make it more awesome, or just working on any product that you love, that may be the deepest kind of learning, as well as some of the most fun kinds of play. Learning as coding or as writing code is much harder than learning in a neural network because there isn't a smooth optimization landscape. The space of programs that you have to effectively search if you're trying to build a piece of code is much more complex and the search problem is much harder. Yet somehow, children solve it. So machines can, too. Another way to think about the challenge is to think about all the activities that a true MIT hacker does in any given day trying to make their code better. So that could include tuning the parameters of your existing code to make it run faster or more accurately. That's kind of like training in the neural network or reinforcement learning. Think about all the things that actually involve writing code or rewriting code, writing a new function, debugging old functions, refactoring your code to make it more general, writing a whole new library that lets you solve many new problems in a domain, or even writing new programming languages. All these activities of coding have analogs in children's learning as they grow their minds, starting from the beginning all the way up to adulthood. And so if we want to build machine learning that learns like a child, we've got to have machine learning algorithms that capture all these ways of creating algorithms. It's a really hard problem, but we've made a few small steps. So our first one is a system that learns code that captures a very simple visual concept, like a new handwritten character. You can learn thousands of new handwritten characters, even in alphabets you've never seen before, from just a single example, in contrast to, say, a conventional machine learning system, which might need thousands of examples for just one concept. How do we do this? Well, our system, which we call Bayesian program learning, uses probabilistic programs to capture the causal processes that put ink a page. Think like writing or drawing programs, the action programs that guide you when you're drawing a character. And then it can run those programs backwards from observing one of their outputs using a kind of Bayes rule to figure out, what was the program that produced what you saw? And that new concept that we've learned lets you generalize very well from just one example, and even pass simple kinds of Turing tests. So here, we can ask both our algorithm and people to imagine new instances of a familiar character. Could you tell which of these were drawn by the human or the machine? Try again. I bet it's pretty hard to tell. So this shows that we're passing a very simple version of a Turing test, being able to capture that creative generalization ability in this small domain. It's a small step, but it can scale. The same kind of idea can learn 3D shape programs to describe the shape of a chair, to answer questions about pictures, or even to learn to play a new video game a thousand times faster than a typical reinforcement learning, and almost as fast as a person. I just want to leave you with our latest step, which is the incredible PhD work of a great PhD student, Kevin Ellis. He is just about to graduate from MIT. And working with a team of other students and researchers, he's built a system called Dream Coder. This is our latest Bayesian program learning. It's not even published yet. This system is inspired by the observation in science that much of the deepest learning that goes on behind our everyday activities takes place not during the day, but at night while you're asleep, replaying and consolidating the memories of what you learned during the day. Our system, similarly, tries to abstract out the concepts in common to the solutions it solved in each new batch of tasks that it's given, and then it replays those solutions and practices solving new, related problems in something like a dreaming or sleep phase. And it does this iteratively in a bootstrapping cycle that allows it to learn to learn better each day, and even to build whole new libraries of concepts that capture the underlying abstractions in a given domain. So we can illustrate this with some creative visual tasks, like building towers of blocks, or drawing designs like these using the programming language Logo. So the system here might be given these designs, for instance, and just given the basic Logo drawing primitives-- so just the ability to pick a pen up or down, to move it in some direction or to turn-- and then starting just from that, it learns to draw all these designs, and it does it by learning these new functions that capture concepts, like a semicircle, a circle, a spiral, an S-curve, a polygon, and so on, and even higher-order functions that can take one of those more basic functions and then, say, repeat it in a radially symmetric way around a circle. So this is one example of a language of concepts used to capture, in this case, a domain of drawing, but the system is quite general. And I should say, by the way, maybe this is the best part. Here, I'm showing you its dreams, where it takes the concepts that it learned and it recombines them in new ways to imagine new kinds of problems it can solve. When it starts off, the dreams aren't very interesting because it hasn't learned anything yet. But after 20 cycles of learning, you can see it's coming up with all sorts of crazy, creative ideas. And that's showing you, maybe, the best insight into the models that it's built. Now, to show some of the generality, we can give this system physics problems, for instance, drawn from the cheat sheets you might have used in AP Physics or 801 and 802, and just a very basic functional programming language, and then given problems from that set, it learns for itself a language of vector algebra and then basic principles of classical mechanics and electromagnetism. Or starting with the most basic kind of programming language that you might have seen in 6001, so Lisp circa 1959, and then some basic programming exercises, the system can construct for itself the conceptual system of modern functional programming, so language tools like fold, map, unfold, and so on. So these are just a few examples of what we're starting to be able to do with the idea of program learning, and I want to leave you with a roadmap of where we see this going over the next 5, 10 years, and surely more. We think these tools can let us capture not just the infant stage of common sense, but the next big stages of learning that come after that. So think about, what's the most important thing that every child learns from, say, one and 1/2 to three? It's language, and then using language to learn everything else from, more or less, age three on up, to tap into the full spectrum of human knowledge that builds culturally across generations and even across societies. If we could build AI like this, this would be AI that truly lives in a human world. This would be AI that humans could talk to, teach, and trust the way we've always done with other people, even people that we're just meeting for the first time. This could be AI that makes us actually smarter and better off Thank you, Josh. Thanks for joining us and for more information on how to connect with the MIT Alumni Association please visit our website. 