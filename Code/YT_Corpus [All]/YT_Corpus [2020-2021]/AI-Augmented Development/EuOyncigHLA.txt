 Hi. It's me, Juste again and I am back with a brand new episode of the Rasa Masterclass. Since we did a comprehensive introduction to dialogue management and spoke about the main components needed to build dialogue management models, I think it's a perfect time to drill down to the very core of the dialogue management with Rasa and talk about the training policies. In this episode you will learn what training policies are available at Rasa, how they work, how they differ from one another and in what situations you would consider choosing specific policies for your models. So let's get going! A policy is a component responsible for making a decision of how an assistant should respond next. Training policies can be as simple as models mimicking the conversations that they were trained on, to quite sophisticated machine learning models capable of predicting the next action based on a lot of details like the history of the conversation, the context and other information. Policies, just like NLU models, are configured in a config.yml file of the project directory. In the previous episode of the Masterclass we trained the first version of the dialogue management model using the default policy configuration generated by the rasa init function. In general, the policy configuration consists of the policy name and a set of parameters which are used when training the models and can be configured by the developer. Before diving into the policies let's talk about two hyper parameters which are very important for dialogue management and are used in pretty much any policy. Max_history - this parameter defines how many conversational turns an assistant looks at when learning to make the predictions for the next action. Let's say you have an intent out-of-scope which corresponds to user's input which doesn't really relate to anything that an assistant was designed to do. You would like to train your assistant to respond with a message where it explains what it is capable of doing if it sees this intent a few times in a row so your training stories may look like the following. To make sure that your assistant picks up this behavior you should set your max_history parameter to at least three. The number you set for the max_history parameter depends on the length of your training stories and the patterns you would like your assistant to remember. However, it's very important to keep in mind that setting max_history parameters to a higher number makes your models bigger which increases the time taken to train them. If you would like your assistant to take into account specific dialogue turns far back in the history, you should set those details as slots instead of setting max_history parameter to a higher number. This will likely improve the performance of your models and reduce the risk of facing the performance issues. Data_augmentation is another very important parameter when training in the dialogue management models Rasa creates longer stories by putting together some shorter stories like the following. This usually improves the performance of your models and allows your dialogue management models to ignore the history of the conversation when it's not necessary and predict specific responses which are usually quite common for specific situations. Data_augmentation can be altered by setting the augmentation flag in your policy configuration. Setting this flag to 20 means that there will be 200 augmented stories created. Keep in mind though that using augmented stories increases the number of training stories your model has to process which means that it will increase the training times so if you have a lot of training data already and your models are performing well using data augmentation may not be very helpful for you so you should keep this number low or completely disable data augmentation by setting this parameter to zero. Now let's talk about the actual policies which are available at Rasa. Memoization policy - this policy is one of the simpler policies available at Rasa. It mimics the stories it was trained on. Depending on what max_history parameter was set it tries to match the fragment of the current story with the stories provided in the training data file. If it finds one it predicts the next action from the matched story with a confidence one, otherwise it predicts none with a confidence zero. Memorization policy is not intended to be used on its own and is usually combined with other policies. This is because this policy is optimized for the precision and not for the recall. This means that every mistake that this policy makes is fatal because the policy is 100 percent certain about every prediction that it makes and there are no other policies to correct it. There are a few parameters which can be configured for this policy. One of them is max_history which defines the number of dialogue turns that an assistant should look into when making the prediction for the next action. The default configuration for this parameter is five and another one is priority which defines the priority of the policy. We recommend to leave this parameter as a default. Mapping policy - this policy allows you to add some business logic to your assistant. If you know that a specific intent should always be followed by specific action regardless of what happened in the conversation before, this is the policy that allows you to do that. To enable this behavior you have to edit your domain file and specify which action should be triggered when a specific intent is predicted. With mapping policy you can map an intent to only one action. After the mapped action is executed the predictions of the next responses are handed over to other policies in the configuration this means that the mapping policy is usually used in addition to other policies and is not really used on its own. The predictions of the mapping policy by default are taken into account by other policies for making the predictions, but you can change this behavior by using Rasa's in-built events like UserUtteranceReverted which tells an assistant to remove mapped actions from its memory so that they would not impact the predictions made by other policies. To configure this policy all you have to do is define which intents should be mapped to which actions in your domain file and list the policy itself in a policy configuration file. Keras policy - this policy uses the neural network implemented in a Python deep learning library called Keras to predict the next action it takes into account a lot of details when getting the next action. For example, what the last action was, what is the intent and what entities were extracted from the current user input, what slots are set at the moment. In addition to this, this policy takes into account the previous states of the dialogue. How many of those states are featurized and used when predicting the next action is defined using the parameter max_history. The default architecture of the model is based on LSTM but it's something you can change by overriding the architecture with your custom one. Let's take a quick look at the high-level architecture of how the next action is predicted using the Keras policy. Let's say we have a user who is asking a question. An NLU model does the job of predicting the intent and extracting the entities from this input. Those details are used to create the feature vector which the dialogue management model will learn from in a few steps. First, the model takes in the previous states and the amount of those states is defined by the max_history parameter. After that, the feature vector of the current state is fed into the model and the model predicts the next best action. If this next action has some kind of backend integrations, for example making an API call and extracting some additional details, those details can later be featurized and used to make the predictions of the next actions as well. Keras policy is one of those policies which allows you to utilize the power of machine learning for dialogue management with Rasa. It learns from the training stories you provide and with enough training data it can enable your assistant to handle quite challenging situations and drive quite natural conversations. This policy can be combined with other policies to achieve the best results. There are quite a few hyper parameters which you can specify in your policy configuration. The most important for you may be parameters like max_history which we already discussed. You can also specify things like the number of epochs which defines the number of times the algorithm will see the training data where one epoch equals to one forward pass and one backward pass of all training examples, validation split defines the amount of data used evaluating the model during the training and random seed helps you to get reproducible results for the same inputs. To achieve that, set this parameter to an integer number. Transformer Embedding dialogue policy or in short TED policy - that policy is one of the latest additions to Rasa. It outperforms other policies like Keras when it comes to multi turn dialogue modeling. The main difference between this policy and other machine learning based policies in Rasa is that instead of relying on RNN architecture this policy is using transformer to learn the patterns and make the predictions. There are quite a few advantages in using transformer for dialogue modeling - it allows for simpler architecture and faster models but also transformers tend to deal a lot better with achieving lower perplexity across different corpora and deals a lot better with unexpected user inputs like chitchat. Let's briefly look at the architecture of the model. First, user inputs, intents and entities, as well as previous system actions and slots are concatenated into an input vector and fed into transformer. A dense layer is applied to the output of the transformer to get the embeddings of a dialogue for each time step a dense layer is applied to create embeddings for system actions for each time step and finally the similarity between the dialogue embedding and embedded system actions is calculated. Based on our experiments TED policy tends to outperform LSTM based policies while at the same time being faster and lightweight and it's a policy which we recommend developers to use for dialogue modeling, especially when there are unexpected inputs like chitchat involved. Rasa's research team recently released a research paper on using transformers for dialogue modeling and I highly recommend you to read this paper to learn more details about this approach. Just like with all other policies TED policy has to be configured in a policy configuration file by providing the name of the policy and the set of parameters you would like the model to use. There is a long list of parameters which you can tweak things from max_history to other parameters which correspond to the architecture of the model. All of these parameters have predefined default values which you can find on the Rasa's documentation and it's something you can tweak to best suit your project needs. Form policy - sometimes your assistant will need to collect some information before it can go ahead and run a specific action. For example, our Medicare Locator assistant may need to collect the user's address, name, age and maybe other details before it can go and look for a specific facility and make an appointment. While we can model such situations by writing stories and using slots it's not the most optimal approach because when there is a lot of information that needs to be collected you will need a lot of training stories to build models that can handle at least a happy path. Form policy is very useful here - it predicts form action which can be used to enable your assistant to fill in the forms, define what information is crucial for your assistant to run specific actions and make sure that those details are collected before those actions are predicted. Once Form Action is called, Form Policy will continually predict Form Action until all required slots are filled. Forms a very powerful feature of conversational assistance with Rasa and we will dive into this topic a lot deeper in the upcoming episodes of the Rasa Masterclass. To configure Form Policy all you have to do is provide the name of this policy in a policy configuration file. However to use forms with Rasa you have to implement Form Action which defines which interns are required and lots of other details. As we build our Medical Locator assistant we will definitely implement forms there so you will see how the implementation of Form Action looks like in practice. in the upcoming episodes of the Rasa Masterclass. Foldback Policy - fall back is very important for dialogue management. When dealing with conversational AI it's impossible to avoid the situations where the users ask for something that an assistant wasn't really designed to do or when users provide inputs that make NLU or dialogue manager models rather confused. because an assistant doesn't have enough knowledge how to deal with certain inputs. While it's impossible to enable your assistant to deal with every possible situation out there it's important to enable your assistant to at least acknowledge these situations and handle them gracefully. This is what Fallback policy is for - it allows you to set the thresholds for the NLU and dialogue management models and if they are not met a fallback action is predicted by the fallback policy. That fallbackk action can be a response like "Sorry I didn't get that, can you rephrase it?", " I don't understand" or anything else you would like an assistant to say in such situation. To configure the fallback policy you have to provide four parameters - the nlu and core thresholds which correspond to the minimum confidence needed to accept the predictions of the NLU and dialogue management models. Another parameter is ambiguity threshold which corresponds to the minimum amount by which the confidence of the top intent prediction must exceed the confidence of the second highest NLU prediction and the last thing to configure is the name of the fallback action -an action which is executed when a fallback is predicted. For the fallback action you can use some default Rasa in-built actions or create your own custom action for that. If you do so make sure to include it in your domain and stories files. A more sophisticated version of the Fallback Policy is a Two-StageFallbackPolicy. It works in a similar way as the first one, the main difference is that instead of immediately running a fallback action once prediction thresholds are not met this policy enables your assistant to run a two-stage intent affirmation process. This is how it works. If the NLU prediction confidence thresholds are not met an assistant will ask the user to affirm a specific intent. If the user does that, the conversation goes on, otherwise an assistant will ask the user to rephrase their input. Then, if the NLU prediction on a rephrased message is confident, the conversation will go on as usual otherwise an assistant will ask the user to affirm an intent once again and it will provide a suggestion of what an assistant thinks the user actually meant. If the user confirms this suggested intent the conversation continues otherwise an assistant triggers the fallback action. The policy can be configured as follows and all you core and ambiguity thresholds stand for the same thing as in fallback policy. Fallback_core_action_name stands for the name of the fallback action to be called if the confidence of Rasa Core action prediction is below the core threshold. At the same time fallback_nlu_action_name stands for the name of the fallback action to be called if the confidence of the NLU intent prediction is below the defined threshold. The deny_suggestion_intent_name stands for the name of the intent which is used to detect that the user denies the suggested intent. As we keep working on our Medicare Locator assistant, you will see how this policy can be used in practice. An important thing to remember about fall back policies is that you cannot use both of them in your policy configuration - you have to choose between fallback policy and TwoStageFallbackPolicy. Which one to use is completely your choice depending on what kind of situations you would like to model and what kind of behaviors you would like to enable for your assistant. Your policy configuration can consist of multiple policies. If that's the case then the policy with the highest confidence score of the prediction wins. But what happens if multiple policies predict the next action with the same confidence score? This is where the policy priority becomes important. Policy priority is automatically set by Rasa to ensure that your assistant behaves in the most feasible way. The default policy priority configuration looks as follows -  here the higher number corresponds to a higher priority. For example, if mapping policy and fallback policy predicts the next action with the same confidence score fallback policy wins because it has higher priority. This is intended to make sure that even if a specific intent was mapped to a specific action but the NLU threshold wasn't met, an assistant can still fallback. While policy priority is something you can modify as one of the hyper parameters in the policy configuration, we highly recommend to use the default configuration provided by Rasa and make changes only if absolutely necessary because if not done carefully, changes in the policy priority may result in undesirable behavior of your assistant. I really hope that this video gave you a deeper understanding on what stands behind the mysterious terms like Memoization, Keras, Embedding policy and more. Using the policy configuration generated by the rasa init function is a great way to train your first dialogue management model and test it in action. But later on throughout the development you should start noticing where additional policies or even custom-built policies will help you to take your assistant to the whole new level this is where we are heading to with our Medicare Locator assistant as well. Starting with the next episodes of the Rasa Masterclass we will focus much more on the actual development of an assistant, updating the NLU and stories data, working a lot more with slots, implementing forms, various custom actions and it will all eventually lead to us sharing our assistant with real users and deploying it in production. I hope you are excited about this part just as much as I am I'll see you in the next episode. Take care. 