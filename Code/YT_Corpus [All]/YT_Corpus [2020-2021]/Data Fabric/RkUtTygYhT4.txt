 [Music] hello everyone thank you for joining hope you are having a great experience attending microsoft ignite from the comforts of your home my name is asrar and i'm a senior business development manager for microsoft startups team today i'm joined by our esteemed guest garish pancha ceo and founder of stream sets which is a very strategic partner of our microsoft for startups program in the next 30 minutes you'll learn how stream sets has helped customers speed up their adoption of azure clouds with its data ops platform for smart data pipelines at this time i want to remind everyone of our code of conduct at microsoft we seek to create a respectful friendly fun and inclusive experience for all of our participants we encourage everyone to assist us in creating a welcoming and safe environment thank you in terms of the actual agenda for this presentation we'll provide a quick overview of the microsoft for startups program and then deep dive into the challenges that streamsets is solving and the difference it's making for enterprise customers now as you all know microsoft's mission is to empower every person every organization on the planet to achieve more which includes startups now how can we help great startups anywhere in the world to empower their businesses that's really the question we ask startups just like yourself and set ourselves to deliver microsoft for startups a program designed from the ground up to reinvent what our role is in helping startups grow seeing startups as a true partner across all microsoft platforms products and business motions now microsoft for startups is an exclusive program that's dedicated to helping qualified enterprise ready b2b startups rapidly scale their companies we do this by providing access to trusted technology including azure github office 365 and much more combined with access to customers who are microsoft sales and marketing engines which provides a streamlined path for startups to connect their innovative solutions to the world's leading enterprises now the benefits of the program are focused on these two pillars are on technical enablement and business and sales acceleration the first pillar really is about access to technology which includes access to azure cloud powerful developer tools including visual studio and github enterprise in addition to microsoft power platform and collaboration tools like office 365 premium we also offer enterprise style technical support and architectural design sessions along with one-on-one consultations with our product group and engineering teams similarly the second pillar is centered around business and sales acceleration really connecting innovative startups with fortune 50 100 500 1000 customers with streamlined path to partnership with startup engagement manager who's dedicated to help startups navigate their partnership with microsoft we also have startups in getting your startup solution listed in our commercial marketplace so it's available to customers all around the world and most importantly connecting microsoft sellers who are compensated to sell your startup solution into their enterprise customers and retire their quota finally we showcase startups at events like these microsoft ignite as an example today where we essentially showcase our startups to the entire world and in our audiences both at microsoft first party events as well as third-party industry and startup events attendees can always learn more by visiting startups.microsoft.com now with that i would like to hand it over to greece panchayat ceo and founder at streamsets thank you over to you garish thanks esrar and we're very excited to be part of this show ourselves and be part of the microsoft startups program here i'm going to give you a brief presentation here on migrating and continuously ingesting both on-premise and cloud data to microsoft azure data stores and platforms since our founding in 2014 our sole focus here at streamsets has been in building a cloud-native platform to help data engineers build enterprise-grade data pipelines the data ops way in support of next-generation data analytics now our point of view here at stream sets is simple for too long the focus on data engineering has been on ease of use and developer productivity while this is of course a minimum market requirement the singular focus has meant that all tooling to date has only enabled data consumers to create ad hoc point-to-point data pipelines and to grab-and-go data such approaches miss a second key market requirement that of operationalization operationalization requires being able to collaborate with different personas reuse artifacts created by peers support pipelines in production and as requirements change evolve quickly and with confidence in today's uncertain world the business is clamoring for more data than ever and data engineering is at the heart of delivering that data wherever it originates and where it's needed which nowadays of course is in cloud and hybrid platforms but data engineers face many many pressures and challenges first the project backlog is big and growing this is happening because business demands are evolving beyond traditional reporting to modern analytics data science and aiml second upstream data changes are accelerating both small and large changes to data structure and semantics are unending are almost always outside your control and very difficult to triage we call this phenomenon datadrift the effect of datadrift is to create a huge burden on engineering and also on operations in order to keep the lights on and to respond at the speed of business and thirdly as data platforms evolve for example from hadoop into the cloud data engineers are on point for huge re-platforming projects while still juggling their daily responsibilities now there are various options out there for data engineers ranging from traditional etl tools to simple elt data loaders to hand coding using a variety of programming languages all of these approaches make life hard for the data engineer why is that existing tools are too hard requiring a lot of specialized skills or they're too simplistic making it easy for first-time data load but making it painful to operate data pipelines on an ongoing basis all existing approaches lead to brittle mappings or pipelines that require significant rework every time anything changes in the source or destination so engineers end up spending 80 of their time on keeping the lights on leaving very little time for new value-added work and when these re-platforming projects come up it ends up being a full rewrite it becomes a huge project and thus slows down adoption of the new platforms and technologies such strange sets we had lived through these problems before and envisioned a new world a world of smart data pipelines as you're engineering pipelines to azure for example smart data pipelines let you go fast as you get the data to the business but also allows you to be confident that the pipelines you're building will hold up for ongoing operations in our worldview there are three ways we make pipelines smart the first is we provide full life cycle tooling that is powerful enough to solve any data engineering need and this is done through extensible apis and sdk and through the ability to automate much of your work while at the same time keeping the tooling visual and abstracting away the complexities of pipeline development providing a single on-ramp for drag-and-drop etl developers to do kind of full-fledged data engineering work without needing to code secondly smart data pipelines are resilient to all the data drift that's happening all the time in the world around us it could be a schema change could be a database version upgrade it could be a file format change smart pipelines are designed to handle such changes with minimal to no updates required so they run non-stop even as the data drift is non-stop and finally smart data pipelines are portable lots of vendors checkbox support for on-premise and multiple cloud systems but they require multiple tools or rewrites to support the variety of different clouds but with pipeline portability you can take them from on-premise to microsoft azure without rewriting the pipelines simply update the destination and you're done so what is the technology secret sauce behind these smart data pipelines is there anything real behind these claims there is the secret sauce is the architecture of these smart data pipelines an architecture that provides the abstraction to separate the what of the data from the how of the data the what is the business meaning and the logic of the data and its pipelines and it's what the business actually cares about the how of the data is the technical implementation detail like what database or what messaging bus it's in what the schema data structure is what the format is etc these are things that the business doesn't care about yet 95 of changes in the data are driven by the how if we look at a typical etl mapping or a typical data pipeline every single step in the data flow embeds details on the how that's to say the structure the schema the semantics etc if anything changes you have to update every single step in the pipeline smart data pipelines however abstract away as much of the how as is possible each stage is decoupled from the others and focuses only on the watt so if there's a change to a source system say a column moves or a database replaces a json file all you have to do is update the source stage and the rest of the pipeline is unaffected so dumb pipelines data engineers spend 80 to 90 percent of the time doing maintenance and upkeep work on changes that are trivial with smart data pipelines that time is freed up to deliver new value to the business so there's a typical view of how string set smart data pipelines fit into an azure environment streamsets helps you ingest data from source systems such as databases files apis or kafka and ingest them on a streaming change data capture or batch basis into raw landing zones including azure storage azure synapse azure event hub and so on stream sets also migrates data from your legacy data lakes and big data platforms such as hadoop into the azure storage layers from there etl and data processing pipelines help transform filter and structure the data either in a curated data lake such as synapse or hdinsight or into a report ready conform data warehouse such as synapse again or sql server these etl pipelines execute natively on spark also running on azure so you can take advantage of spark's processing power and scalability and streamsets provides a single control hub to manage and monitor all of the data flows across all of your pipelines both on premise and in the cloud in real time in a single console a final key difference for stream sets is that we've always focused on data ops that of automating and monitoring the entire data engineering lifecycle and ecosystem many etl tools will let you design test and deploy a pipeline however there's much more to data engineering than that data engineers need to be able to monitor the health of their deployed pipelines and troubleshoot exceptions in real time data engineers need to enable more casual users including point-and-click etl developers and data scientists enable them to design and deploy their own data pipelines enabling self-service access to data for them and platform operators need to be able to easily manage the data pipeline infrastructure while also accelerating the shift to the latest and greatest data platforms such as upgrading to sql server 2019 standardizing on adls and more recently adopting synapse they need the ability to monitor the data pipeline held across all pipelines no matter who built them and where they're executing only stream sets supports the end-to-end data engineering and operations of these smart data pipelines now we have thousands of businesses using our software from boutique systems integrators to mid-market enterprises to the largest enterprises in the world across every vertical in every continent but we're particularly proud of many global 2000 companies that have made significant investments in both stream sets and microsoft i'm going to highlight just a couple one is royal.shell number nine the global 2000. they use stream sets for their shell.ai division which is tasked with bringing machine learning and ai to all lines of business that shell these data scientists need access to all types of data from traditional downstream customer data to upstream and midstream data such as data that originates in wells refineries iot and also specialized lines of business such as energy trading at shell they were able to onboard hundreds of data scientists so they could self-service data for their ai and ml needs and by doing this on azure they were able to dramatically lower their capital commitment to sap hana their on-premise data analytics platform a second example that i'd like to highlight is that of humana which continues to climb up the fortune 100 with revenues of 65 billion having grown more than 30 percent over the past two years humana has implemented a common core data fabric with stream sets and other technologies cloud and on-premise to support their business mission of making their members 20 healthier by sharing data at any and every touch point with their members the common core data fabric uses the fire standard and ensures all compliance requirements are met with this they're able to provide information provide context and provide the smarts for built-in built-for-purpose applications you can see video keynotes of these examples from our data ops summit and many more at streamsets.com customers all right enough talking from me for now a picture is worth a thousand words let's take a quick look at the tooling in this pre-recorded video here's what a data engineer can do with stream sets use data collector for building and executing smart data pipelines for batch streaming or change data capture use transformer to design etl or complex data processing that runs on spark transformer comes pre-built with multiple processors but gives the data engines flexibility to extend capability with their favorite spark based tools with multiple data flows running simultaneously across your environments control hub gives you the management and end-to-end monitoring of all your data operations let's take a look at how you can migrate external data into azure using stream sets transformer the pipeline here is configured to migrate delimited files into azure storage while it's migrating it's also going to optimize file creation in azure storage by converting data to parquet data format creating large files and auto partitioning the files based on the business logic as you migrate data you'll get real-time monitoring allowing you to track your migration process in the next example i'll show you how you can build an end-to-end data processing solution in stream sets using native azure services for real-time ingest and data processing here the first pipeline is set up to ingest tweets in near real time as the tweets are ingested i archived the raw data in azure storage and at the same time with simple configurations i'm going to cleanse and normalize the events before processing them in synapse i'm also sending the cleanse data to azure event hub for further processing in the next pipeline i perform sentiment analysis on each tweet coming through eventhought if you already have a piece of code for doing this you can easily plug it into a pipeline and run it i am leveraging the text analytics api from azure's cognitive service to directly analyze each tweet the output scores of the analysis can be persisted in any storage of your choice a score value closer to one indicates a positive sentiment and a score value closer to zero would suggest a negative sentiment i also set up a simple pipeline that allows me to analyze tweets over a period of time probably to identify influencers and understand their sentiment as time progresses a pipeline for performing analytics like this can be run at scale over a spark cluster on your azure environment now given i have multiple data flows each executing in its own silo i created a topology and control hub to oversee operations of all of these data flows and visualize the data operations in real time as you can see it shows me a single pane of class for all my data pipelines that are running across different environments and across different executions while showing me how much data is being processed across each stage you saw how easy it is to ingest data and apply machine learning in near real time without writing code you also saw how transformer allows executing a scalable etl and data processing logic on top of spark and lastly also so how control hub is a single place for designing deploying monitoring and managing all your data pipelines data processing jobs and execution engines from a single hub okay now that you've seen the tool and pre-recorded action you can check it out live data collector and transform products are available to try out on the azure marketplace in a variety of configurations just fire it up and give it a try you can also visit the streamset's website to learn a lot more about our support for data ops and modern data analytics and you can get access to a free trial of control hub so go fast be confident and good luck engineering smart data pipelines over to you sir thank you so much chris super helpful and great demo as well i actually had a quick question so stream set solution in some regards is very complementary to microsoft's first party offering and given our partnership of working together can you please touch a bit more about how stream sets helps enterprise customers and how you're actually using azure marketplace and how it has been beneficial for your business thank you absolutely uh well first maybe answering the marketplace question we're very excited about the azure marketplace especially in the covet era where it's difficult for us to expand our reach in a physical manner we're able to reach more prospects more customers globally and get traffic that we otherwise wouldn't get in fact the azure marketplace has been the most popular cloud marketplace lifetime to date for stream sets products what we find is that these users end up doing a lot of their proof of concept no poc on their own get some development going and when they're ready to commit they're able to transact quickly and seamlessly both enterprise subscriptions and commit to consume subscriptions through the marketplace for both our private and standard offers so we love the marketplace uh with respect to other microsoft first party services no we generally end up coexisting with them as an example microsoft's azure data factory is a standard along with stream sets and both shell and mana the two examples i gave you where we find our sweet spot is when customers have a much more of a hybrid problem so where they have still some data and compute sitting on premise where they have a very fragmented data platform landscape and where they care a lot about operationalization and data ops often the largest enterprises need that service level agreement to their end users and we're able to bring that to the table super helpful thank you for driving clarity to that and the differentiation and we're super excited that azure marketplace has been very helpful and great to know you're already transacting and reaping the benefits uh so again thank you everyone in closing i also want to share some of the resources and calls to action if you want to learn more about the microsoft for startups program at startups.microsoft.com for stream set solution on azure marketplace you can go to aka dot msl stream sets and then follow us on our social handles as well and then finally please continue your learning journey with microsoft by going to microsoft.com learn again thank you everyone for your time today and enjoy the rest of microsoft ignite thank you 