 Hello, I'm Ken Mitchell and welcome to AMD GPU open. Today, the AMD Ryzen(tm) processor software optimization slides will be given from the safety and comfort of my home office in Austin, Texas. This presentation includes an introduction to the AMD Ryzen(tm) family of processors followed by advanced optimization topics. These topics include the high performance AMD "Zen 2" microarchitecture, profiling tools, and code optimization opportunities. Examples may include C/C++ assembly and hardware performance monitoring counters -- fun and exciting stuff for software engineers! For those interested, I'm a principal member of technical staff at AMD. More importantly, I am the CPU dev tech team lead in the Radeon game engineering team. Previously, I worked in SoC modeling and benchmark development teams. Unfortunately, that is not my car. The agenda for today is success stories, "Zen 2" architecture processors, AMD uProf, optimizations and lessons learned, and contacts. Success stories! This past year, our team optimized several top game titles including Borderlands 3, Gears 5, and World War Z just to name a few. It's a fun job with high visibility and access to leading-edge technology. Thanks to our partners for their support. We look forward to seeing you again soon! "Zen 2" architecture processors. "Zen 2" based products exist in a wide variety of form factors. AMD Ryzen(tm) 4000 series mobile processors, code-named "Renoir", offer the most cores available for ultra-thin laptops -- typically at only 15 watts. Next, AMD Ryzen(tm) 3000 series mainstream desktop processors, code-named "Matisse", offer up to 16 cores at up to 105 watts. Amazingly, an AMD Ryzen(tm) 3950X 16-core processor, cooler memory, NVME drive, ITX board, and Radeon GPU can fit into a backpack! I know this from experience! Finally, AMD Ryzen(tm) 3000 series high-end desktop processors, code-named "Castle Peak", offer up to 64 cores at up to 280 watts! Roar! These high core count products are great for creators and developers! Microarchitecture. There have been many advances in "Zen 2" microarchitecture since "Zen 1". For example, 15% IPC improvement, double micro-op cache capacity, reoptimized L1 instruction cache, added a third address generation unit, doubled floating-point data path width, doubled l3 capacity, improved branch prediction accuracy, added hardware optimized security mitigations, added guest mode execute trap, added SMT fairness, and improved write combining buffer performance. In other words, "Zen 2" is fast and efficient! Congratulations to Mike Clark and the "Zen 2" core design team! Dataflow This slide shows an abstract diagram of an AMD Ryzen(tm) 4000 series mobile processor, codename "Renoir". Imagine data moving from DRAM on the far right through the data fabric caches and cores on the left. Each unified memory controller controls 64 bits of DRAM. For example, one 64 bit DDR4. Or for example, two 32-bit LPDDR4. Note this is a single monolithic die SoC design where the orange-colored core complex L3 cache connects to the blue-colored data fabric at 32 bytes per cycle read and 32 bytes per cycle write. Unlike most AMD "Zen 2" desktop products, this particular product has four megabytes of l3 cache. Next, this slide shows an abstract diagram of an AMD Ryzen(tm) 3000 series mainstream desktop processors, code-named "Matisse". Relative to the previous generation mainstream desktop product, the l3 cache has increased from 8 MB to 16 MB. Additionally, the I/O hub connection to the data fabric has increased from 32 bytes per cycle to 64 bytes per cycle to support PCIe generation 4. Also the L1 data cache increased from 16 to 32 bytes for load or store ops. Note this is a chiplet design. This example has two core complex die (a.k.a. CCD). Each CCD has two 16 MB L3 caches. However, the L3 cache complex within a CCD share a single link to the data fabric at 32 bytes per cycle read and 16 bytes per cycle write. Finally, this slide shows an abstract diagram of an AMD Ryzen(tm) 3000 series high-end desktop processor, codenamed "Castle Peak". This topology with up the 8 CCDs is so large that I have shown it in quadrants for simplicity. Each quadrant looks similar to "Matisse". However, "Castle Peak" has a much larger I/O die with additional unified memory controllers and PCIe lanes. In short, you can add many GPUs and NVME drives to this beast. Instruction set. Few new instructions have been added to added to "Zen 2". These are intended for driver developers but feel free to get creative with them. CLWB and CLFlush are generic CPU vendor instructions. CLWB will write back the specified cache line from any core or cache in the system, and not just the core cache they call the instruction. The CLFlush instruction will invalidate the cache line in a similar manner. These new instructions may be used to flush dirty data to NVDIMMs for persistence. WBNOIND is an AMD CPU vendor specific instruction. It will write back all modified cache lines in the internal caches of the processor -- including its L3 -- to memory, leaving the line valid in the internal caches. This execution only applies to the core which called the instruction. A software prefetch level instruction loads a cache line from the specified memory address into the data cache level specified by the locality reverence T0, T1, T2, or NTA. If a memory fault is detected, a bus cycle is not initiated and the instruction is treated as a NOP. Prefetch levels T0, T1, and T2 are treated identically in "Zen 1" and "Zen 2" microarchitectures. The non-temporal data cache fill hint, indicated with prefetch NTA, reduces cache pollution for data that will only be used once. It is not suitable for cache blocking of small data set. Lines filled into the L2 cache with the prefetch NTA are marked for quicker eviction from the L2, and when evicted from the l2 are not inserted into the L3. The operation of this instruction is implementation dependent. Prefetch fill and evict policies may differ for other processor vendors or microarchitecture generations. Additionally, there is no direct interaction between the prefetch queue and the write combining buffer. Prefetch NTA evictions from L2 do not use the write combining buffer. Non-temporal access is intended for use once data. It is not intended for communication or producer consumer type buffers. NTA may be beneficial when accessing a data set which is significantly larger than the l3 cache and not likely to have near-term reuse. This may allow another smaller active data set to make better use of the L3 capacity. "Matisse" cache and memory. Optimizations have been made to the micro-op cache, L1 instruction cache, and L3 caches. Prefetch performance has improved thanks to higher load store bandwidth. The L1 instruction cache has decreased from 64K to 32K, but the micro-op cache has increased from 2K to 4K. There is a fast private 512K L2 cache. The L3 is filled from L2 victims -- mostly exclusive. L2 tags are duplicated in L3 for probe filtering and fast cache transfers. Note L3 latency increased from 35 to 39 clocks but doubled in capacity. In my experience, large last level caches are great for gaming. Cash refills within the same CCX may be relatively low-cost. Better yet, some OS schedulers are CCX aware. The normal critical path for a load operation is shown in red. Additional probes are shown in gray on later slides. Latency depends on core clock in this example. There is additional cost in refilling from another cores L2 in the same CX compared to refilling from the same CCX L3, and more additional cost in refilling from another cores L1 in the same CCX. In short, refills within the same CCX may cost around 10 nanoseconds. Cache refills from local DRAM may be more costly. Latency depends on multiple clock domains such as core clock, fabric clock, and memory clock. After core load operation misses its L1, L2, and L3 caches, a CCX may send a request to its CCM. This CCM originator sends this request to the CS based on the memory map. The CS sends DRAM data to the originator and sends probes to all CCMs. CCMS send probe responses to the originator. The CCM originator then sends a single fill response to the CCX. In short, refills from local DRAM may cost around 80 nanoseconds. Here's another example of a cache refill from local DRAM, but this time the memory address is mapped to CS1. Cache refills from any other CCX cost may be similar to memory latency. Here's another example where the memory address is mapped to CS1 but the payload data actually comes from a core in CCX3. In other words, avoid shared global memory, avoid false sharing, and use thread-local storage. Additionally, queues and producer/consumer threads within the same CCX may be more efficient. AMD uProf profiler. New features in version 3.2 include a thread concurrency chart, a flame graph chart, and improved symbol support. Here is an example of the thread concurrency chart using 7-zip as an example. Look at all those cores! Here's an example of the flame graph view using the Nvidia PhysX Kapla demo as an example. Analyzed metrics tables are of course still supported. Source-code views with performance monitoring counter metrics are also still supported. Additionally, AMD uProf can be used to collect some power management information such as system energy and system temperature. For profiling applications, I recommend using the assess performance extended profile. Enabling call stack sampling with frame pointer omission for frame graph analysis is also recommended. A five-second delay may allow you to change the foreground window before profiling starts. I often collect 30 or 60 seconds of samples. Enable symbol loading from the Microsoft symbol server, especially if you have not defined the _NT_SYMBOL_PATH environment variable. Note AMD uProf version 3.2 does not support the cache*&lt;folder syntax. Try using the srv*&lt;URL syntax instead. Optimizations and lessons learned. Finally! Topics include general guidance, use best practices with scalability, verify parallel DX12 pipeline state creation, verify parallel dx12 command list generation, use best practices with locks, reorder hot struct members, and use prefetch level while iterating a standard vector of pointers. General guidance. Use the latest compiler and SDK. I prefer whole program optimization, AVX2, the multi-threaded static C runtime, FP fast, and favor blend compiler options. Some of our software dev peers within AMD have benefited by using jemalloc. Note Microsoft Visual Studio 2017 version 15.9.14 and later may improve AMD Ryzen(tm) memcpy and memset performance. Regardless, I recommend Visual Studio 2019 due to its build throughput improvements and additional optimizations. Today, most applications use SSE2 since all windows 64-bit operating systems must support this instruction set. However, AVX and AVX2 have grown to be quite popular. Better yet, these flags may improve code generation of inline code including memcpy and memset. It is worth noting that AVX-512 is not supported by AMD processors and currently has a very small user population. Use all physical cores. This advice is specific to AMD processors and is not general guidance for all processor vendors. Generally, applications show SMT benefits and use of all logical processors is recommended. However, games often suffer from SMT contention on the Main or render threads during gameplay. One strategy to reduce this contention is to create threads based on physical core count rather than logical processor count. Profile your application or game to determine the ideal thread count. I recommend adding game options to set max thread pool size, force thread pool size, force SMT, and force a single NUMA node -- which will implicitly force a single processor group. Finally, avoid setting the thread pool size as a constant. Disable debug features before you ship. While investigating open issues, developers may submit change requests which enable debug features on test and shipping configurations. These debug features may greatly reduce performance due to disabled multi-threading, cache pollution from stats, and increased serialization from logging. Some examples of UE4 settings to verify are shown below. These include FORCE_USE_STATS and STATS definitions as well as various parallel rendering CVARs. Use best practices with scalability. Optimize scalability for integrated graphics. My goal is greater than or equal to 60 FPS at 720p very low settings. To achieve this, try changing the following: scene color format, shadow map quality, volumetric fog quality, and ambient occlusion. Use a proper video memory budget for integrated graphics. Recently, the AMD GPU services library added an is APU flag. If true, set the video memory budget to system shared memory in bytes. Otherwise, set the video memory budget to dedicated local memory in bytes. Verify parallel dx12 pipeline state creation. I will use the Unreal Engine 4.24 Infiltrator demo and dx12 RHI as an example. UE4.24 fixed a lock contention issue which allows parallel pipeline state creation. Notice the CPU usage precise graph peaks in the Windows Performance Analyzer after the code change. Forcing a cold shader cache makes this verification work much easier. Note some applications may have additional shader caches on disk. Regardless, here's how I verified this example. Basically, I used the Windows Performance Toolkit GPU view log command script to collect data while the game application was starting up. Afterwards, I used the Windows Performance Analyzer to open the merged ETL log file. I then added the CPU usage precise graph as well as the CPU usage sampled flame by process stack graph. Next, I did a find all d3d12! CDevice::CreatePipelineState within the flame by process stack. This find command highlighted the samples of interest in the CPU usage precise graph. Finally, I can see high total CPU usage during these times. Here's the same workflow applied to a warm shader cache. Note these results are much more difficult to analyze since they executed very quickly. Verified parallel dx12 command list generation, Again I will use the Unreal Engine 4.24 Infiltrator demo and dx12 RHI as an example. Again, I used the Windows Performance Toolkit GPUView log command script to collect data, but this time I collected the samples while the game was actually running rather than starting up. Afterwards I used the Windows Performance Analyzer to open the merged ETL log file. I then added the GPU utilization graph, CPU Usage precise graph, as well as the generic events graph. In this example, I zoom into a single frame between to present markers. Afterwards in the generic events graph, I moved the CPU column next to the task name column, then filtered and expanded command list. This proves command list generation is occurring on multiple CPUs. Local maximums are also visible at similar times in the CPU usage precise graph. Use best practices with locks. In short, use modern OS sync API's such as standard mutex, standard shared mutex, slim reader/writer lock, and enter critical section. These API's may allow more efficient scheduling and longer battery life. Otherwise, for user spin locks, use the pause instruction, align the lock variable, test and test-and-set, and avoid lock prefix instructions. Note the OS may be unaware that threads are spinning thus scheduling efficiency and battery life may be lost. Use spin locks only if held for a very, very short time. Note some badly written spin lock code may run inefficiently on "Zen 1". Fortunately, "Zen 2" improved SMT fairness for ALU schedulers which helped mitigate badly written spin lock code. This chart shows elapsed time where less is better for an exclusive lock micro benchmark implemented with various sync API's. The data labels shown are percent of slim reader lock. Note the good user spin lock, standard mutex, standard shared mutex, slim reader/writer lock, and enter critical section show good performance in this view. Battery life is very important to our customers. Another view of this exclusive lock micro benchmark is the percent idle where higher is better. In this chart, we can see that a good user spin lock is bad for battery life. Thus our recommended sync APIs are simply a standard mutex, standard shared mutex, Slim reader/writer lock, and enter critical section. Here are some profiling tips. In my experience, bad user spin lock code often shows greater than 3000 ALU token stalls per thousand instructions in the AMD uProf profiler. If you find such user spin locks, consider replacing them with modern sync APIs. Additionally, this slide shows a command line example of using the Windows Performance Recorder to collect call stacks. In this example, I am sampling every 0.1 milliseconds using the power profile and recording in file mode rather than memory mode. This slide shows the source code for the main function using the exclusive lock micro benchmark. Basically, it times how long it took all threads to execute the callback function. Here we see the bad spin lock and its callback function. Note the bad spin lock is not using best practices. Most notably, its while loop does not include a pause instruction. Thus, it may consume core resources in an overly aggressive manner while doing non payload work -- most unfortunate! Here we see the bad spin lock implementation in AMD uProf. Note the callback function shows greater than 3K ALU token stalls per thousand instructions. Looking deeper at the source code level, we can see most CPU clocks are spent at the lock! Worse yet, we see over 26,000 ALU token stalls per thousand instructions! Ouch! Looking even deeper at the disassembly, we can see there is no pause instruction in the hot loop. We've found our problem! In our next next example, we have an improved user spin lock which uses our best practices. Most notably, it includes a pause instruction. Otherwise, the callback function has not changed. This time, AMD uProf shows high CPU clocks in the callback function. However, ALU token stalls have been reduced from over 3,000 to only 1. This is fantastic progress! Looking at the source code view, we now see the lock stalls have been reduced even further, from over 26,000 to near 0. Amazing! Finally, looking at the disassembly, we can clearly see the pause instruction in the hot loop. This is a much better solution. However, it is still not efficient from a battery life perspective. This is where modern sync API's come to our rescue. Note our callback function no longer uses custom locks. In this sample, we simply lock and unlock using standard mutex. Using the Windows Performance Analyzer, we can find the C runtime msvcp140!mtx_do_lock. In our next example, we use a standard shared mutex. Surprisingly, the compiler has substituted the standard shared mutex lock for slim reader/writer lock. In our next example, we use a slim reader/writer lock. Note this requires some initialization in the main function. Using the Windows Performance Analyzer, we can find ntdll! RtlAcquireSRWLockExclusive. Next, we use a critical section. Note again this requires some initialization in the main function. Using the Windows Performance Analyzer, we can find ntdll! RtlpEnterCriticalSection. Finally, we use a WaitForSingleObject along with CreateMutex. These two APIs have been around for a long, long time. Note again this requires some initialization in the main function. Using the Windows Performance Analyzer we can find ntdll!WaitForSingleObject. Remember: if you find WaitForSingleObject usage in your application, you may get better performance by replacing it with a lower cost modern sync API. In short, use standard mutex, standard shared mutex, a slim reader/writer lock, and enter critical section. Avoid user spin locks and avoid wait for single object. Reorder hot struct members. Use AMD uProf to find plateaus of hot functions where there are many data cache refills from DRAM. If the hot function includes a loop which indirectly accesses struct data members spread over many cache lines, try reordering the struct. In this simple example, the hot loop uses only variables x and s in the struct. Thus I have moved s to be adjacent to x. Here's a better example where I use the popular NVidia PhysX SDK Kapla demo, a rigid body simulation, running on a "Zen 2" based processor with only 4 megabytes of L3 cache. Under these conditions, performance improved 12% after reordering hot struct members. AMD uProf can be used to view a flame graph. The horizontal represents normalized inclusive samples where the counter is CPU clocks. The vertical is the call stack. The color is the module name. In this example, we see three large plateaus at the function getGlobalPose using many CPU clocks. After changing the flame graph counter, we see these same functions, getGlobalPose, have many data cache refills from DRAM. Looking at the source code, there is no loop iteration in getGlobalPose but there are many cache accesses where misses are refilled from DRAM. Looking at the disassembly there's a load effective address instruction which used many CPU clocks and showed many refills from DRAM. This register and offset represent the hot struct number mLocalPose. Going up one level on the call stack leads us to ConvexRender::updateTransformations. Here we find a loop calling the hot function getGlobalPose. Additionally, we find another hot start member mMaterialOffset with many refills from DRAM. Next, we find more hot stuct member data in the hot loop with many refills from DRAM. Specifically mSurfaceMaterialId and mMaterialId. Here are code sample snippets from the convex base header file before and after the reorder hot struct members optimization. Note this change is relatively simple and low-risk. Using the Windows Debugger, we can use the display type command to see the member offsets for the struct KaplaDemo! Convex. We can then find the offsets for the hot data. Before the optimization, hot data is spread over many 64 byte cache lines. But after the optimization, hot data may fit within a single 64 byte cache line assuming the data were aligned. Very nice! Here's a visualization of the data access pattern. Basically, we have a loop which accesses four cache lines after a pointer chase. After the optimization, we have a loop which accesses only one cache line after the pointer chase. Here are some AMD uProf screen shots for reference. This is a flame graph of CPU clocks before the optimization. After the optimization, plateaus are a bit smaller for update transformations. This is a flame graph of data cache refills from DRAM before the optimization. After the optimization, plateaus are again a bit smaller for update transformations. Here's the source code view of the function getGlobalPose before the optimization. After the optimization, instructions per clock are higher at lines of interest. Here's a source code view of the function update transformations before the optimization. After the optimization, instructions per clock are higher again at lines of interest. But can we do better? Yes we can! Use prefetch level while iterating a standard vector of pointers. Use AMD uProf to find hot functions where there are many data cache refills from DRAM. If many refills from DRAM are while iterating a standard vector of pointers, try using _mm_prefetch to improve performance. The distance to prefetch and the prefetch level may require some tuning. But try prefetch distance 4 and prefetch NTA. In this simple example, I am prefetching the address of x and s four iterations into the future. Again, here's the Nvidia PhysX SDK Kapla demo. Performance improved 60% using prefetch optimizations. Here's the code sample. Our prefetch code changes are simply between the #if and pound #endif directives. Note that I used offset notation since the data members are protected thus this code may not work if the convex struct definition changes in the future. Again. here are some AMD uProf screenshots for reference. This is a flame graph of CPU clocks before the optimization. After the optimization, plateaus are smaller for update transformations. Additionally, the render callback has smaller contribution. Again, this is a flame graph of data cache refills from DRAM before the optimization. After, the plateaus are smaller for update transformations and the render callback has a smaller contribution. Here's the source code view of the function getGlobalPose before the optimization. After the optimization, IPC is higher at lines of interest. Here's the source code view at the function update transformations before the optimization. Again, after the optimization IPC is higher at lines of interest. IPC is also high at prefetch NTA lines with sufficient samples. Thank you for your time. If you found this presentation valuable, these other links may interest you. The software optimization guide for AMD family 0x17 models 0x30, The Path to "Zen 2", and the AGS SDK. If you have questions, feel free to contact us. You may reach me at Kenneth.Mitchell@amd.com. I am also on Twitter at KenMitchellKen. Thanks to my teammates John Hartwig and Elliot Kim for their hard work and contributions. Bye! 