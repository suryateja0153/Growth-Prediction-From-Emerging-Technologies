  Hello, and welcome to another edition of Teradata TechBytes. My name’s Tim Miller, and I work in Teradata’s Technology and Innovation Office. Today we’re going to talk about the process of data science and how the latest platform from Teradata enables that process. There are a lot of buzz words being thrown around data science today. When people talk about it, I think they tend to focus in a little too much on the actual algorithms: Talk about stochastic processes, auto-regressive algorithms, general linear modeling, random forests, et cetera. That’s not what data science is all about. You need to approach it from the standpoint of a process, and we’ll talk in detail about that in a little bit. But whenever I get overwhelmed with buzz words in the industry, I always look to someone who I respect and see what they’re saying about it. Gregory Piatetsky-Shapiro is a well-known expert in the field of data science. He’s the president of KDnuggets, a very well respected and leading website for analytics. He’s also the co-founder of KDD (Knowledge Discovery and Databases) and Data Mining conferences. I attended a KDD conference in 1991 and met Gregory, and believe me, he knows what he’s talking about. We’re really not inventing the wheel here. His quote is, “Although the buzz words describing the field have changed from knowledge discovery to data mining to predictive analytics and now to data science, the essence has remained the same. It’s the process of discovering what is true and useful in mountains of data.” So what do we mean when we say data science is a process? There are few very well documented processes out there that I’m showing here on the screen. The first one is CRISP-DM. This stands for the cross-industry standard process for data mining. The one in the middle is called SEMMA. This is a process that SAS advocates. It stands for Sample, Explore, Modify, Model, and Assess. And then KDD has their own - Where they go through a selection process, transformation process, data mining process, and finally evaluation of the models that come out of the data mining process. If you break all these down, they’re really empirical methods. Right? They boil down to a problem statement, hypothesis formulation, experiment design, data collection, and analysis and conclusion. I’ll talk a little bit more about the CRISP-DM process model, mainly because Teradata, then NCR, was a founding member of CRISP-DM, along with SPSS, Daimler AG, and OHRA, a Dutch insurance agency. This activity was in the early to mid-1990s, so again, we’ve been doing this for a long, long time. The end goal in any data mining engagement is to operationalize your analytics model. Some people call that Analytics Ops. But the key to actually doing that is to be following one of these types of processes. So let’s break down the process. First off, an important point is that any effective process is iterative. So when you’re on any given step within that process, you may have to move forward or backward. One such example is when I go through my data transformation process, I’m always going to want to explore that, do some statistical analysis on that, look at the distributions of the data that I’ve created as part of my transformation process. You’re constantly iterating through the process. If you break the CRISP-DM process down, You’re basically going to be exploring your data to understand the physical properties of it. What are the data distributions? What is the overall quality of my data? Do I have outliers? Do I have nulls that I need to deal with? Next, you’re going to transform your data. The nature of the mathematics is such that you need a single row of data at whatever level you’re modeling. You may be modeling at the customer level, the product level, or the household level. Whatever the case may be, you need one row of information. That’s how the mathematics needs to look at the data. You spend a lot of time in these first two pieces of the process. Then you finally get to modeling your data. We’re going to run it through an algorithm. Going to run it through a random forest. I’m going to run it through a regression model, et cetera. Next, we evaluate it, and part of that evaluation is you’re going to score it as well. But the evaluation piece is when you look at statistics about your model, statistics about the variables within your model. What’s really not addressed, and is an important to talk about, is operationalization: Maybe the developers of these processes think that if you just automate that process, that’s enough. But there’s more to it than that. You have the whole area of, “How do I manage the lifecycle of these?” Over time, they’re going to decay as my data changes, as my business changes. I need to manage those. They could do more harm than good, potentially. How am I going to refresh my analytic data when the detail data changes? How do I save every step of the process and stick it into a repository like GitHub? And how do I manage the overall workflow? And on top of that, it better be backed up by the same type of resiliency that Teradata gives you: Scalability, availability, query performance, user management, and workload management. And really to me, that is what the Teradata Vantage is all about. Our vision for Vantage is really around enabling the process of data science. We want data scientists to be able to use their favorite analytic tools and languages. We want you to be able to assess the variety of data sources that are out there and data types that are out there. And we want to give you the best engines to do your analytics on. Let’s talk a little bit about the data. When I began my analytics journey in the late ‘80s, early ‘90s, I was working with a group of SAS programmers. They were using multivariate statistical techniques to build models for Teradata customers. These guys dealt with numbers, and they dealt with transforming categorical data into some sort of a numeric representation. This is not something that they had to think about. And this has just made the actual data transformation process that much harder in today’s age. We got all these kind of data formats - JSON, BSON, voice, video, images, XML, CSV files. How do you transform that into a form that the modeling algorithms can use? I’m sure by now you’ve heard of Teradata’s 4D Analytics. The idea behind it is discover new use cases and insights based on when something happens and where it happens. So we’re using time series, temporal analysis, in addition to geospatial analysis. And finally the data sources. You got relational data sources in Teradata. You got Hadoop. You have data lakes built in S3 today. So there’s a lot of different data sources that you’re going to have to deal with. This just makes the overall process that much harder. So now let’s go through the process step by step. And let’s look at the functions that we have available on Vantage that help you in that step of the process. So first, exploring your data. Some people call this exploratory data analysis. Some people call it data profiling, data quality analysis. But you need to do this against your raw data as well as your transformed data. So some of the functions that we have available on Vantage include, “Let me look at my univariate statistics. What are the basic statistical properties of my data?” Mean, max, mean, standard deviation, variance, et cetera. “What are the distributions and distribution matching of my data?" - whether it be categorical and I’m looking at frequencies of occurrences. Or it be continuous and I’m looking at a distribution? Things like percentiling. Moving averages. Correlation analysis. How are variables correlated? Pathing analysis. Identity matching. Outlier detection. There are literally hundreds of these functions that help you do this directly on Vantage. Next let’s talk about transforming your data. Some people call this feature engineering. We used to call it data preparation. But the idea is taking your data and turning them into variables. Couple different variables you need to create. You need to create a dependent variable. What it is I’m trying to predict. If it’s a churn, did this person leave me as a customer? Then you’re creating a massive number of independent variables. These are your predictors. These are what the algorithms are going to look at in order to predict the value and the probability of your dependent variables value. So when you look at feature engineering in general, the first thing that I like to say about that is that SQL’s an extremely powerful variable creation tool. It’s very easy to write a formula, very easy to join across different tables, and it’s very easy to dimension these variables that you come up with. Typically you’re going to have to dimension them by some time period or location or what have you. Some of the other functions that we have in addition to just raw SQL and the power of Teradata is the pathing analysis. Pivoting and unpivoting. Various parsers. So when I have JSON or I am looking at XML or I’m looking at Apache logs, I can pull out the necessary information that I need to create a variable in my analytic data set. Various scaling or rescaling. Design coding. I might have to transform a character-based data, a labeled bit of data, into a numeric equivalent. Sessionization. There are hundreds and hundreds of other functions we have on Vantage to help you transform your data into variables. So we’ve explored your data. We’ve transformed your data. Let’s talk a little bit about the algorithms now. But rather than getting so bogged down at how many there are to choose from, let alone what’s coming next out of academia, let’s focus instead on what are the most widely used categories of analytical techniques. All of which are supported by Vantage. First off, you have classification and regression. This is sometimes known as supervised learning. These predictive models use relationships and your historical data, your variables, to predict future outcomes. Classification is used to predict which class a discrete data point is part of, such as will someone churn or will they not, while regression is used to predict a continuous value, like a revenue field. The types of algorithms that fall in here are things like general linear modeling, decision trees, neural networks. Next you have clustering and segmentation. This is sometimes called unsupervised learning. These algorithms group together individuals in a population by their similarity. The objects within clusters are similar to one another while objects across clusters are dissimilar. The types of algorithms that fall into this category are things like k-means, k-nearest neighbor, and Kohonen networks. Next you have time series or path analysis. So obviously time series is an analysis used for analyzing data varies over time. Pathing is a special case of that and something we’ve done in Teradata for quite some time. It can be used in various time-based applications, chief of which is finding the golden path that customers take leading to a purchase. Finally you have text analytics. This is where we want to obtain some sort of meaning from natural language text. How we do it in Teradata is that we convert that information in the text fields into work frequencies, which then become numbers that can be analyzed. So what type of modeling algorithms do we have specifically within Vantage that help you move from variables to models? There’s too many to list here at a single screen, so I’m just giving you a sample. Generalized linear modeling, a variety of clustering methods, such as k-means, k-nearest neighbor. Decision trees and Forest. We have association analysis. We have Naïve Bayes. Neural networks. Time series analytics, a variety of them. Graph and pathing analytics and a variety of text analytics. We talk about modeling evaluation methods. These are completely dependent on the algorithm itself. So very algorithm specific. Sometimes you get model statistics, statistics on that whole model like an R-squared value from regression model. Sometimes you get variable statistics or coefficient statistics for your regression models. We’ll provide you a confusion matrix so you can look at, “What are my true positive, true negative, false positive and false negative counts?” We’ve got centroid statistics for the various clustering algorithms. We have entropy, Gini index and Chi-square for the decision trees. Support, confident, and lift for the association analysis. And finally, neural net layer weights. There are literally thousands of these, as I said, too many, to list. Next, we’re going to score your data. Moving you for the modeling phase into the first step of deployment, which is scoring. Each modeling algorithm that I’ve talked about previously has a corresponding scoring algorithm. In addition, we have something called the Teradata Scoring SDK. In this case, we generate an intermediate representation of the model, which we call an AML file. And then AML files can be used by the SDK to be ingested on any platform, including edge devices. So that you can score any model created by the Teradata Analytics Platform. And as I said, this is really the first step in model deployment. Most importantly, having all parts of the process enabled by Vantage makes the ultimate goal, operationalization, possible. We talked about one of the visions for Vantage being the ability to provide our end users with capabilities to use the languages they want and the tools that they want. Two of the most popular development tools include the products from Project Jupyter as well as the Rstudio family of products. Let’s talk a little bit about Jupyter products first. What we’ve done on our end is we built a SQL kernel for Teradata that allows you to use what are called Jupyter SQL magic commands right within the notebook. This is in addition to the R and Python language packages which we will talk about in the subsequent slide. With JupyterHub integrated with Vantage, you can build SQL, R, and Python notebooks leveraging the power of Vantage. We have also integrated magic commands for Vega-Lite a popular visualization library. And there’s a data dictionary browser, so you can look at the tables and views available to you, and even get some basic statistics on your data. On the RStudio side, they have built in a Teradata dictionary browser. RStudio is also the author of dplyr and dbplyr, which we use as the basis for Teradata’s R Package, tdplyr. Obviously, you’re going to get the plotting power of R itself, and many more features. So now, with Vantage, you don’t have to just use SQL anymore. Of course for the SQL jockeys out there like me, you can still do that. But we also have R and Python. So let's talk about Vantage support for Python and R in 2 distinct ways. First, you can run R and Python on a client of Vantage and push the processing down to the Vantage platform. So there's no need to copy or move your data to the client. We do this through 2 readily available add-on libraries for Vantage called teradataml for Python and tdplyr for R. You can think both of them as the SQL backend to connect, generate, and execute SQL on the Teradata Vantage platform. What's really cool about this is that it's built on top of popular open source packages. So in the case of tdplyr, it's built on top of dplyr as well dbplyr, whereas pandas and SQLAlchemy were used as the basis for teradataml. When you use these packages, user codes in the language of their choice using these Teradata add-on interfaces and what you get are literally 100s of Vantage Machine Learning and Graph functions interface to them in R and Python as well as literally 1000s of Advanced SQL functions; so all of the dplyr verbs and SQLAlchemy verbs are available to you when use these packages. Now method we had quite some time to run R or Python in-database - with no need to copy or move your data to the client. in a way that runs on every unit of parallelism in Vantage; so have 2 different table operators. One is called SCRIPT. And SCRIPT pipelines the database I/O to R or Python scripts - either one - called within a SQL statement. The second table operator is called ExecR. It executes R scripts embedded in a SQL statement . And database I/O, this time, is coming through API’s. The same APIs you use when writing an ordinary UDF. These 2 methods are really great for row or partition-based operations such as scoring or simultaneously building many models based on a partition. If you want a global operation, then you need to do some Map Reduce style programming in order to get that. Recently the Engineering team provided support for the SCRIPT table operator within the teradataml and tdplyr packages. What this means is now you can take arbitrary R code and actually do an "APPLY" function directly through the SCRIT table operator without having to write a SQL statement. Now you can completely stay within your R or Python environment and use both of these techniques to process R or Python. To summarize, we’ve built Vantage to be the platform to help you realize analytic operationalization. This implies that Vantage enables the entire process to be performed on the platform in addition to supporting analytic tools and languages you prefer. More importantly, Vantage – a next-generation evolution of our industry-leading analytic engine, Teradata Database – has capabilities that enable your analytics to be operationalized at enterprise-level and at scale, using all of your data. These capabilities include linear scalability, mission-critical availability, sophisticated workload management, and query performance, powered by the industries best adaptive optimizer. Thank you for watching this TechBytes on Teradata Vantage and the process of Data Science. For more information on Vantage, please visit our website at www.teradata.com/vantage. Thank you and have a wonderful day. 