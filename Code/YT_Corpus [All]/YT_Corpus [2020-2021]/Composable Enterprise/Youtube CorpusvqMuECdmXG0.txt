 hi my name is Dominique Brzezinski I'm a distinguished engineer and apple information security and gonna talk about Delta Lake patterns and insights the assessee is a mother of all invention and building big data systems there are a lot of challenges about three years ago I was at spark summit and I ran into Michael Armbrust and talked about a system that I had belted a prior employer that was pretty large-scale doing hundreds of terabytes a day data and I had to build a similar system at Apple and I described some of the stuff that we've gone through that we had done and challenges that we'd had with certain components and I saw a flash of you know inspiration terr brilliance in in Michael's eyes and he said yeah I don't think our architecture is gonna hold up to that now but I have this thing in my back pocket and as we discussed it a bit more it really resonated with where I wanted to be and what I wanted to do for this project so a couple months into a POC Michael reached out and said hey I have some code for you to try and that turned out to be basically the Alpha of Delta Lake it was code named Tahoe at the time and we after running a few like tests and verifications we just went all in and within a short period of time we were doing tens of terabytes a day into Delta lakes and now we're much bigger hundreds of terabytes protein of how to buy today data that we ingest through our ETL pipeline and then we process much higher volumes of data from a kind of daily working set so we learned a couple things along the way and had some insights I'm not really going to talk too much about just sort of standard meat and potatoes of having data in a table like format but I'm going to go into a little bit more about some of the more novel or interesting things that Delta Lake has allowed us to do and then some just kind of operational insights and and gotchas that we've experienced um hopefully a little bit to tips and tricks kind of mixed in with us so first I'm going to talk about patterns and first up since I said we run a big ETL pipeline is the inversion ETLs extract transform load but in big data ecosystem we usually do something more like extract load and then transform and that's exactly what our system looks like we actually have an extract system that dumps a bunch of data into s3 and then we pick up that data we load it into a staging table and then we have a bunch of streams off the staging table that are doing transformations that result in unique data tables that have well-defined schemas and present really a stable interface for our downstream consumers of that data but before we get too far into these patterns I'm going to stop for a second and talk a little bit about our parsing framework that we use because I'll show some sample data down the road or some sample code and it'll use this and so it's easier if we just pause for a second and take a look at it so we have an abstract class called parser and a few of the unique things about this is that we actually kind of broke it up into some base steps set of really giving us flexibility and so we have a prepare stop and our parsers are basically data frame transforms right so they take a data frame in and they pass a data frame out and so prepare given the source data frame it's meant to do simple things like filtering or simple extractions that would give us additional information that we might need to use in parsing and just fundamentally prepare possibly drop some tables are sorry some columns if necessary coming in but other than that you know not too much processing in there parse is where we do a majority of our extraction and transformation of the data and then complete is an opportunity once parsing is done once validation has occurred for us to do kind of any final fix-up of the data before it actually gets committed to the table our output from the stream and so an interesting thing is basically apply function on this this is all Scala then notice is that it it runs these steps in a fixed sequence and in between some of the steps so for instance we were unprepared but then what it actually does is we create a struct of all the columns in the incoming data frame and that goes into an origin column and so we basically just transposed all the columns into a struct put them in one column that we start with and then we run the parse method and then we see this set parse details is automatically run and the interesting thing that it does is it actually creates a column with a struct in it and it uses as valid conditions expression and well as the additional details expression in order to validate the row and then possibly capture additional detail about it that could be useful downstream and either I'm dealing with a parsing error or tracking lineage of the data and it also puts a timestamp on it so we have a marker from when the data was actually parsed and once the parse details has been set finally the complete method is called so this is sort of a layup for what we do through our ETL pipeline so when we get into the extract part we actually have a fairly complicated system upstream it's actually a couple systems that is responsible for collecting log data our telemetry or extracting data from from upstream systems databases api's and fundamentally what it does is takes the raw data wraps it in and JSON a layer that includes for instance the event type the source of the data we get time stamps for not system so we can we can track Layton sees but a fundamentally its metadata and then there's a raw field in that that includes the actual log event or piece of telemetry or data in it and the extract system plots that into s3 and from there we actually use the s3 sqs source in spark and that's tired tied up to notifications on the s3 bucket and that's how we consume high volumes of data without having to do expensive list operations and slow list operations on that three bucket and so that's the source of our data on roar I said it's in the JSON object per line format but a note here is we don't actually use the JSON input format for spark streams we actually use file format to capture that it's just a single text line and I'll talk a little bit more about why we do that the advantage so as we're loading the data out of s3 here's our staging parser and you can see our validation conditions is just that it has a valid timestamp and in on additional details one of the things we capture is actually the input file name so this is giving us lineage from this row back to the file and s3 that it came from and so this is just to help with debugging break fix type information but it establishes at least a baseline history for the data and then our parser is really simple we're basically just extracting the JSON from the the text file input creates a single column called value so that's been transposed into this origin struck so we're just using from JSON as you notice here we've actually extended from JSON to include some fancy timestamp are seeing stuff that we do and add some semantics that it's consistent for our use case so we know the schema for this input data because it's the standard metadata wrapper so weird extracting those fields and then we're basically transposing them again out of the resulting struct in the top-level columns and then we're preserving that origin column and so we have the the raw data as it came in and in origin value and then we have the actual extracted data in those columns and so then internally the parser is doing the validation and checking whether or not that there's a valid timestamp and so in our complete method we actually come along and we look to see if if the timestamp is null and if it is we substitute in the parsing time for that and if not we use actual extracted timestamp and then we capture all of those standard columns and the parse details as well as that origin that gives us lineage from this so the important things here are that we always end up with a valid record even if we fail to extract the JSON we have original string its lineage that comes through here and and then we have parse details and so we'll know whether or not it was correctly parsed and we have a lot of information to help us if we had a problem loading and the reason why we use that text file input at this stage is because if you just try to use the JSON reader and it comes across a bad line or worse it comes across the line that doesn't match your extract schema you would just get all nulls and you won't know exactly what happened and so what we do is we capture the whole string in case that fails so we have a copy of it and we have the lineage of it and this is really helped a lot in dealing with when data shape changes upstream coming into our table and allows us to actually reprocess against the same table and overwrite if we need to it's had real advantages from a kind of break fix perspective so then we get to the transform phase so off that staging table that that last one into its partitioned by date and event type and so mostly then our actual data parsers have a stream reading off of the staging table that has a predicate on it looking for a particular event type or maybe is possibly poking into the rock the raw log itself looking for some kind of fixed tokens in it and and and that's driven into the stream and so in our prepare phase we actually dropped that origin struct from the prior table because we're going to create a new one with the incoming data from this so we drop origin we have all the other fields it'll create a new origin and then by parsing it a lot of our data sets are actually JSON themselves so I'm just giving an example of doing an extraction from JSON using the schema again and transposing that back out some of ours are more complicated in this this parse method would you know have a bunch of code there that has to deal with you know ugly raw semi structured events so that just really depends on on the source type the output from this will then go to an actual clean data table somebody you might call that silver or gold you know kind of depending on it for our users that data is its first class and super important there are a lot of derivative datasets from those but the events themselves actually have a lot about you in our environment so that's extract load transform so once we have these tables we want to start doing stuff and some of the unique features of Delta Lake are being able to do things like acid up certs at a very large scale and so in an absurd you have a table and you have new data coming in and based off of essentially like join criteria you know whether or not certain columns match or have a relationship with one another you may want to choose to update an existing row insert a new row or delete a row and that's super super powerful we'll talk a little bit more about that in some of the use cases but it turns out some time your data doesn't look like you think it does or your merge conditions essentially the on Clause aren't as specific as they need to be and so what will happen is multiple rows from your new input data will actually match a single row in the existing table and that violates a constraint for merge and it will throw an exception and you're left going hmm I wonder if it was my on criteria or I wonder if my input data actually had duplicates in it and you're sort of stuck so merge logic debug it turns out oftentimes you're doing if you're doing merge in a stream you're doing it in a for each batch writer and are sorry for each batch method and a little trick that you can do is the input data so you get this micro batch data frame you can just take that and you can write it out to some other Delta table and just overwrite the data in it and if then it's Rosanne exception in your actual merge you'll be left with the actual input data that came in and violated the constraints and you can just go manually run a join with the same on Clause conditions and then validate whether or not you had duplicate rows in your input or whether or not you're on conditions or actually too wide and matching multiple rows that aren't duplicates and it makes it super easy to debug when you hit that case I wouldn't recommend leaving that intermediate right in is it'll add latency in production if you're not latency sensitive it's not a terrible idea just after each batch that table will represent so you know the last batch that was processed but in development super good and then if you have a production stream and it breaks you can just go and wedge that little bit of code in there run it against this existing checkpoint it'll try to do that last batch it'll fail again and you'll have the data that you need so super helpful thing so another thing that we do with merge and this really marries a powerful feature of SPARC structured streaming which is essentially stateful processing with merge and where we use this a lot is creating things like DHCP sessions VPN sessions so any session ization makes a lot of sense or when you have a stateful change or update to something that you want to do then these two marry together really well because you can run basically on that groups with state or a flat Maps group with state and so in our case of say take DHCP when a machine gets a new a new session and IP binding we want to admit that as soon as possible so any other event data that we have coming in we can map by IP to the correct machine identity and so we want to make that open available super fast but eventually that session will close and it will have an end time to it which becomes really important for processing old data and making sure that you're only assigning the host to the IP within the right time frame with regard to the time series data that you're looking on so what we do is we emit an open and that comes in and that basically gets through a for each batch writer gets appended to the table but then we emit a closed when we see the end of the session and through merge we actually find that open session the corresponding open session and we just update that row in order to be the closed session with the addition of the end time and some other state and attribute information to it and that keeps then that session table really small right we either have an open or we have a closed for that session we don't end up with an open and a closed it keeps the processing logic I when we're doing a join in order to enrich other data with that it keeps that logic more simple and we keeps a very accurate it's easy to reason about the gable data you see in the table as well so mat groups with state or flat maps group with state married up with Delta Lake merge um they're really peanut butter and jelly so another big one that we've done and this is sort of a crazy thing that we did but we have we have a few dozen tables that have columns that have either an IP in it or have a comma delimited list of IPs or an array of IPs and it is really common for us to have to try to find all the data are tamo true we have around a given IP or Isetta IPS or IPS that are in some cider range and this is a very expensive operation even with things like dynamic file skipping and z ordering on those IP columns since some of them are just a single IP and they're actually a list of IP s and you have to do like it contains or reg X really terrible you're doing full table scans so in the past when we've had a set of IPs and we had to look over our long retention window across all those data sets that could take up to 48 hours on a very large cluster we decided we needed an index and but we have 38 tables and by the way these tables are crazy big where we're writing hundreds of terabytes a day across these tables so it's a lot of data so first naive thing was create a stream off of each one of those tables that was pulling out IP addresses and the date partition and the table Union those streams together do an aggregation on it and try to write it out to a table you can imagine how well that went even on extremely large clusters that wasn't going to work glue up kind of beyond the computational resources that we have to do it so we had to start decomposing the problem a bit and and the really interesting thing is that that Delta Lake semantics and operations you can do on it in some ways really complement structured streaming or can be used I instead of certain features on structured streaming so what we did on this is we basically hang one or more streams off of each source data set where we're doing a for each batch group by source IP des type e in the date partition we also include the source table name and column information in that and so this is for each batch means we're running this aggregation just on each micro batch on the screen but not in totality on the screen itself and we take that and we append that to the step 1 table so we can have a bunch of source tables they each have streams coming off of them and we'll just have all those streams append to one table within some some reasonable balance and that creates this step one table and then off the step one table will do another for each batch actually two operations one where we're doing a group by source IP in the date and another one by the desk type in the date is ultimately our index it's going to be about one IP to the the tables columns and date partition that it showed up in and so we do the we do the two group eyes and we union those together and then we append to the step two table and then from the step two table we basically do a for each batch and here instead of doing a group by we do actually a window aggregation on it turns out more performant I haven't really dug into that and then we take the output of that aggregation and we actually do an up cert into the index table and so what they'll look like the final index table we actually have the original value from the column we have an individual extracted IP from that value we have the date partition and then we have an array of the data set names and columns where that IP showed up in that original value on that date partition and then we also have a sum total row count that we keep for that which gives us a good approximation of how much activity there was for that IP on that day and so over the day our as new data comes in that array may grow in that total account bro but we won't be creating a bunch of other roads and so this gives us a much smaller more compact table that we can z-order by extracted value on and we can very rapidly search for rare IPs and then from that we got some metadata about what data sets they we're in and the date partitions and some real counts and we can do some interesting stuff with that or we dynamically then compute queries against the source datasets using that information so now we have a date partition predicate as well as the original value so we're doing the Equality search of that column value in that table and we've taken the one that took 48 hours using the setup to get the real data from the source tables took about a minute so that's a radical improvement so we're spending some time up front to compute this but every time we go to do that type of search for saving massive amounts of cost or resources and we're giving our users much faster response for the data that they need so interesting thing about tables that are merged into so again I'm talking about hey there's all these great we build these index tables accession tables and we're doing these up certs into them the problem is is if you want to actually take that data and you want to get say like a slowly changing data feed of it so you see the new sessions or you see the updated state of a session or you see the new index value that's coming out of it the problem is if you just try to hang a stream off that table soon as there is an update or a delete the stream will throw an exception because normally that violates the standard semantics so now you have to throw on the ignore change option onto the stream which when you're doing just updates and inserts essentially what happens is if a underlying park' file is read because one row needs to be updated in it that row gets updated and a new Park a file is written out and replaces the old one in the table with a more changes set the entire contents of that Parkay file will get played down so you imagine you have tremendous amounts of duplicates coming down so you're not just seeing new inserts and you're not just seeing the roads updated you're seeing all the other unchanged roads from the same park eight miles so a little trick that we do for this to get kind of a clean just insert and updated row key is we actually hang a stream off the table that's being absurd into that stream uses ignore changes and then it does another absurd but using the D do pattern and so the D do pattern is essentially you just check whether or not all the columns match and if they do you just ignore it and if they don't then it's a new value that you haven't seen in the table ie it's inserted or it's an updated record and then we insert that into a clean table and so now that clean table can be a nice source of a stream that just gives you like an sed type update feed and so this is you know one extra step you add a little bit of latency but we've now solved an interesting problem by just kind of understanding the semantics of Delta and kind of marrying that up with streaming in an interesting way and kind of chaining merges together I'm using different semantics so that's a really really interesting and powerful pattern that we use in some places too so that's it for patterns but now I'm gonna get into some of the insights we've had and so obviously we run some very large infrastructure I said we take in a petabyte a day we have a long retention window as well so we actually have tables that are on the order of two petabytes and we have a ton a half petabyte table sitting around that we operate against regularly and so from our prior experience we knew that you can have performance problems with s3 if you don't have enough entropy at the top of the path file on top of the pass so that the s3 bucket can shard well and distribute your workload so when we started this we said hey we know how to do so we can get really high i/o on a bucket so let's just put all our tables in one bucket you know kind of easier turns out it's not easier and it turns out still under certain conditions or s3's having a bad day we might get throttled and sometimes that throttle can impact other tables in the same bucket all right if they happen to share the same shard or something like that so we found best practices for for most tables unless they're incredibly small and bounded we just put each table and its corresponding check point in its own bucket and we enable random prefixes on that bucket are put on that Delta table I should say and this means that the data that's written that table is spread nicely across a nice hash or partitioning ball pass space and then we get really high i/o for for s3 and we don't get throttled for those and it also has some other nice advantages where the bucket then becomes a proxy for table access so we can just use you know standard I am Ackles on that and and so we can actually share individual tables with with non SPARC consumers so for instance if you wanted to use hive with the Delta Lake connector or presto starburst presto with the Delta Lake connector you could individually expose tables by just using echoes on the bucket off to those things even potentially running in other accounts which is a lot easier than doing that goes on prefixing and trying to track the whole thing just a little bit more shines and also if you want to replicate a table to another region you can just use bucket replication and largely it's gonna replicate the data and the table and also the metadata and also the checkpoint data so under you know under most conditions that other table within a reasonably short sorry the other bucket in another region will have a corresponding version of the table and even a checkpoint you can restart problem so some nice properties there so the Mexican site we had and this shows up in the parents's we really realized that Delta Lake and structured stream and SPARC are really composable and if you understand the semantics that you can get through the various configuration options and using normal stream or using for each batch in the stream and then the semantics around Delta Lake we found that we were able to overcome scale hurdles that we might have so for instance if we wanted to keep a running aggregation on a huge stream and in our case oftentimes the timestamps on our streams are actually have a long tail distribution so trying to set up a window and a watermark is either not possible or might be possible but the resource consumption is kind of sketchy on it and might not be super stable and we're not well protected against kind of huge spikes in data but what we found is we can just go do for each batch aggregations on the string which don't have to keep in a batch State and then use the merge operations on Delta Lake to keep updating the aggregation output there so we have a running total on the stream and so now we're able to use Delta Lake semantics to overcome scaling issues that we might run into on SPARC structured streaming and when you use for each batch you give up exactly one semantics and get at least once but if you include a batch ID column and a Delta Lake and you're merging into it you can actually include that in the on conditions to make sure that you won't merge your results from a batch that you've already seen and now you're back to exactly once so these type of you know kind of interplay between the capabilities in in Delta Lake and structured streaming gives a super composable building blocks and and we found that we've tackled many problems including building a giant streaming inverted index without actually having to introduce any other technologies into our stack and just using our expertise on those two so a little thing to know and this is actually covered well in the Delta Lake Docs but schema ordering and the interaction with stats is something that has bitten us before even though it was documented so just calling it up again and essentially stats collection in Delta so for for the first thirty two fields let me and I'll clarify fields and columns in a second have stats like min and Max generated on them in Delta and that's capped it up her parka file basis within the metadata why I say fields and not columns it's bad terminology but Delta Lake tries to not have exceptions around type so for instance in a struct it considers each member of the struct to be a column essentially so it's not the first 32 top-level columns that have stats collection if you have a deep struct in you know in the early column index then you may eat up those first 32 by the first couple columns and then that big struct that's there so you got to make sure that you kind of count over and then down as well if you're trying to figure out you're within that you can actually modify the 32 using the data skipping num index call setting and you can set it to -1 if you want and on everything or any value that you want so dynamic file pruning uses the min max column stats when you have a predicate on a column value if it falls outside the min and max for that file then the answer can't be there and it can choose to not read that file and reduced io is increased performance so Z ordering also in in the data bricks product essentially maximizes the utility of min max by doing a clustered sort across one or more columns I'm so this gives you tighter min max ranges without inner file overlaps and so this is really going to maximize a number of files that you can skip based off of a predicate like that so the key is make sure if your Z ordering or your sorting columns that those columns have stats collection on I either within the first data skipping them indexed or by default 32 fields otherwise you know happily go on and see order and do nothing for you um it might catch that these days but originally it didn't and happily would optimize on a field that didn't pass that's collection another little hint there is that it's expensive to collect stats on long screens I don't have an exact number for what constitutes long buy I'd say you know when you're talking about hundreds of bytes you know to a kilobyte plus um you're crossing the threshold you know if you're under a hundred bytes like you're definitely fine but it's expensive to do it so if you can move your long strings to the end of your schema beyond the data skipping on index columns value if you're you know column width is not 32 if it's less then you can tune down that value for your table and put those long string columns outside of that and that'll make your writes faster if you really need stats on them though they're less helpful on strings but you know then you you pay the price on rights so depends on what you're really tuning and optimizing for don't over partition in your Delta tables so this is worth kind of talking through a little bit when you add partitions to your table you genuinely are increasing the object or file count underneath the table on the storage system and that is because if you have no partitions you can optimize the table to have essentially the fewest number of one gig or whatever you kind of tune optimize for objects so you minimize a number of objects and get nice big ones that give you great throughput from the underlying storage as soon as you introduce partitioning you're probably going to a larger number of objects possibly smaller or uneven size going into those partitions even if you optimize it the partitions are forcing you know a break between those and how the data is commingled so you're increasing object count when you increase object count it has a performance impact on full table scans on you know large aggregations on joins all those things unless you're using the partition values within your predicate liberally and and efficiently and so basically if most of your queries are join our aggregation operations can specify partition values as a predicate in the operation then probably you're picking something reasonably good to partition on I think Michael is suggested that a partition should have at least two gig of data in it as a general case so that can just give you a you know a role by but we often see cases you know of ten even hundred gigabyte tables that the way that they're commonly interacted with partitioning actually doesn't make a ton of sense and it actually is faster if it's just unpartitioned the caveat is if you have to have a retention window or delete certain data at time if you can set it up to where you can delete by the partition value say a date or a month then that makes deletes cleaner and disjoint from other operations on the table but again that's often paying the cost of higher object count which can reduce certain certain performance of operations so the one case where over partitioning makes sense is if you're trying to make a table that purely services point lookups on partition values and usually a small number of partition values if you're only appending data to that table and you're only deleting by partition values and in which case then if a partition ends up with a very small amount of data but you're intentionally only trying to look for the data on that partition it's okay you're not really paying a throughput performance problem so that's the narrow case of we're over partitioning can be advantageous to make high-performance point query but soon as you have to mix like a point query load with you know aggregation or join stuff it's probably better to right-size partition or even under partition a little bit and then fall back on something like Z ordering or sorting on your primary key column to maximize the dynamic file pruning the min/max stats so one thing that you'll run into with Delta Lake is sometimes having conflicting transactions and this really comes about when you're doing things like deletes and updates on a table if you're only doing append type operations then you don't really have conflicts so where we really run into this is on tables that we're doing up certs or merges into because the table is changing all the time and then if we have to do something like a Z order to optimize on the table and optimize takes a little while it can stumble across the table changing out from underneath itself and so our way of handling that is when we have sort of two related operations that conflict you can either make everything partition disjoint which means that are not operating on the same partition at a time but if you have something that really operates on the full table I can optimize the order what we do is we just inline it like in the same method that's doing the up cert and then for instance in a for each batch we're just doing a mod on the batch ID and and every end number of batches we execute the optimized and this way they're serialized they don't conflict we get a little bit more latency on that batch but we're also ultimately keeping latency down because the table is is well ordered and nice and compact and one last thing is we have some very large tables I mentioned you know one and a half petabytes two petabytes with millions of objects in them and so that means the metadata is larger and so just one thing that we found you can use the snap cup snapshot partitions setting in order to essentially use more partitions or tasks to read metadata and so if you have a larger cluster sometimes boosting that up into the you know hundreds or even 1024 reduce the latency on reading the metadata until we see some coming hopefully optimizations I'm on metadata operations and then sometimes you might want to twiddle down like your log retention again if you're adding tons of data to a table on a table with lots of metadata you're gonna you know build up more more metadata and more underlying metadata files and changes and so you might want to shorten the interval that those are kept for that can be impacts to time travel and other stuff like that so you know dig into that detail but that can just be something that can help kind of keep your metadata size down a bit so really it's adding more parallelism to reading the metadata and then keeping them at a data within a reasonable size through settings we largely most of our tables we don't touch anything but we set snapshot partitions on a cluster wide basis and just you know based off sort of cluster size how much resources we want there in order to read these large table metadata so that's all for kind of the patterns and insights I have thank you and get in contact with me if you have any questions I'm also on the Delta Lakes Lac often and happy to answer questions or work through issues with you thank you very much and take care stay safe and healthy you 