 hello my name is scott ellis and i'm a product manager on google cloud and today we're going to talk about managing sensitive data in hybrid environments what we're going to cover today is a quick overview of our cloud data loss prevention or dlp platform and then we're going to go and see how we can use dlp in hybrid environments and after that we're going to show a demo of this by by using dlp to inspect data in a mysql database that's running in a virtual machine and then we're going to migrate some of that data into bigquery and use dlp again to do data masking or data redaction let's get started so first an overview of our data loss prevention platform um first we feel that protecting sensitive data really begins with knowing where that data exists and so one of the core features of our platform is discovery and classification and our data last prevention platform again is a fully managed service that helps you discover classify and protect your most sensitive data and we do this again through discovery classification as well as a set of tools called de-identification which is a way to transform or mask sensitive data and this platform is built to work on structured data unstructured data as well as workloads that might be in real time or kind of in at rest or in batch um to cover this a little more detail when we talk about discovery and classification of sensitive data what we mean is being able to look at data and identify different elements inside that data that might be sensitive an example of that could be things like personally identifiable information or pii this would be things like names dates of birth credit card numbers and a variety of different things and we have different types out of the box that we support that are both kind of global as well as country specific identifiers we also have some special categories like credentials and secrets that help look for things like passwords or off tokens whether it be on our cloud or other clouds and these are things that might not identify a person but are sensitive and might give a kind of a credential or access to privileged access to resources you can also customize all of this either by defining your own own info type detectors or by using detection rules to change how our detectors work or to tune them for your needs that second set of capabilities again we call de-identification it's often referred to as things like data masking or tokenization and this is where we go in and actually manipulate the data to try to de-risk it or obfuscate it somehow and if you can see in this kind of before and after animation what we're doing is we're applying those techniques to different columns in this table in the first three columns we are applying a technique that basically transforms the entire column and then in that fourth column the comments field we're actually using a combination of our inspection and our masking to look for data inside of the cell and only redact that item so you see in that first row we actually find an email address but instead of removing the entire cell or the entire comment we just remove the email address and leave the rest intact so that's an example sort of how we combine our inspection and our de-identification to work on unstructured data or semi-structured data and you know where this is a may uh be applicable or where you might use this um one example is to help prevent exposure of raw secrets or sensitive data um so imagine again you had let's say uh your public website your company website and all of those public assets were hosted in a cloud storage bucket and these are assets that power your website that are generally available to anyone when they're viewing your website but what happens if somebody uploads something into that bucket or into that folder that is uh shouldn't be there maybe it's sensitive so running something like an inspection scan of that bucket we could identify for example here if somebody uploaded a gcp credential into that bucket which again you would not want to expose that publicly as i could give privileged access to some of your resources another example would be to kind of unblock workloads maybe it's a machine learning or artificial intelligence workload or some sort of a cloud migration workload and here we would look at moving data into those different environments and applying that de-identification to obscure or mask the data but still allow that analysis or that machine learning to happen another use case would be really just to understand and classify data across your entire enterprise and our cloud platform has an api a rest or a grpc api that enables you to call it from virtually anywhere and that's what we're going to focus on in the next section is how we can use this in in hybrid environments that may be on or off cloud or on or off google cloud and let's get into that so what do we actually mean by hybrid environments um well really we're thinking of again taking those core capabilities around inspection classification and masking and applying it on data that might be in other clouds might be on premise could be in non-native storage systems this could be like a database that you're running inside of a virtual machine maybe it's running inside of an application like a web or a mobile app or it could of course be running on google cloud so a few examples of that maybe you want to inspect data that's running in something like aws says rds system um or maybe it's a mysql database running inside of a virtual machine or again a database that's running on premise we're actually going to show a demo of this later looking at mysql running in a virtual machine maybe you want to inspect and tokenize data as you're migrating that data from on-prem into cloud or between environments like production development and analytics another example could be inspecting and redacting transactions from a web or mobile application before you store that data at rest so to do this we're going to talk about two of our methods these are two methods on our cloud dlp platform one called content methods and one called hybrid methods now hybrid methods are a newer method that we've released recently um but let's go over both of these methods so first content methods you'll notice actually that these are pretty similar as far as the kind of data sources that they apply to this could be data running on any cloud on-premise and virtually any data source but with content methods it's what we call a synchronous um like request and response these are methods where you make a request you send data in that request and you get a response back in that request and that response is sort of the answer it's the classification or the mass data these methods are stateless and they allow both inspection and de-identification capabilities now a new hybrid methods um are similar but a little bit different what they do is they actually take your data they allow you to sort of do a fire and forget where you can send this data in for inspection but then move on you don't have to wait for the response or process the response and here dlp will aggregate that data and put it put it together and aggregate it so that you can store that data or analyze it for example in something like bigquery or make reports in data studio or publish other types of alerts and for now this is only for inspection and that's what we're going to talk a little bit later and demo this later as well how we can take information send it through these hybrid methods and then produce an audit report using bigquery and data studio but again these are kind of the two methods you would use to handle data in in hybrid environments and here's an example of what that kind of audit report might look like so again imagine you're scanning data across several different assets and you want to collect all of this in one place analyze it using bigquery or visualize it in some sort of a audit report like we're showing here using something like data studio in this example we're showing several different things that we've inspected and as you see we're getting results about what kind of database we looked at what tables and columns and dlp findings we have there now another aspect to this is what we call region regionalization and with dlp you also have regional endpoints and this means that you can actually specify where your data is being processed so for example let's say you want to inspect data from on premise but you want to make sure that the inspection part that's running on our platform is limited to a particular location maybe you want all your data to stay in australia or in europe or in the us you can specify a regional endpoint which means that your data will be processed in that region and it gives you the control whether you're doing it for data residency purposes or uh you're planning out sort of an availability or um kind of a performance or latency requirement you can pick those regional endpoints if you need to great now let's go in and kind of go a little bit deeper on what these hybrid methods look like so when we use the new hybrid method um what we do is we create a hybrid job and this job it really is an active listener that's going to be listening for requests and then aggregating all those findings uh for you so that you can come back later and see the results and when we define this request we're going to basically create a job and instead of specifying a storage location like cloud storage or bigquery we would pick a storage location of hybrid we would then provide additional metadata for that job now this metadata is important because that's the information that we're going to use to aggregate everything together you can specify some of that in the actual job or in each request and let's go into a little bit more detail of what a request might look like so here is an actual code sample where we're showing what a hybrid inspection request would look like on the right side we're actually this is a code sample in java and what we're showing here at the top is there's some content that we want to inspect in this case we're going to be inspecting tabular data so we provided headers and rows of that data then we have our hybrid our actual hybrid request in this case that's where we're supplying all of those details uh all that metadata that we want to aggregate around so here's where we would pass it things like a mysql database type or a version or maybe a database name or table name because we want to add those kind of things to our report later on and then we send the request and in this example again this is just showing the snippet that will basically provide all that information and send that request in and then on the server side dlp will inspect that data aggregate it together and allow you to then go in and analyze that information uh using something like bigquery or data studio great now that we've shown you kind of what a request looks like let's move into an actual demo of this and what we're going to show in this demo is we're going to we're going to scan data that's running in a mysql database that is running on a virtual machine um we're going to inspect that data and then we're going to migrate one of those tables over to bigquery using dlp again to do masking so we basically can kind of protect that data as we're migrating it from this virtual machine into a managed service like bigquery great let's get started on the demo we're going to show a demo in two parts in part one we're going to inspect data from a mysql database that's running in a virtual machine and to do this we're going to use a custom script that's going to read this information send it into our cloud data loss prevention platform using the hybrid inspection method which is going to then write data into bigquery and we're going to visualize that in data studio the second part we're going to migrate a table from that mysql database using cloud dlp and data flow to tokenize that data as we move it into a bigquery and migrate it from mysql into bigquery let's get started great the first thing we're going to do is create a new hybrid job to do that i've navigated to security and then data loss prevention in the cloud console and now i'm going to click on create and then job provided name for the job and then pick a storage location of hybrid now hybrid again means that it's going to create an active listener it's going to be listening for request streams each of those request streams will include the content that we want to inspect as well as any labels that act as metadata so again these labels could define the type of data that we're scanning maybe a database in our demo will be a mysql database the database name and the table name now we can put required labels here which means every request is required to have that label filled out as well as optional labels that we can apply at the job level but they can be overridden by each request now the rest of this job configuration is similar to other job types i define the types of things that i want to look for either by using a template or by putting the configuration as i have here in the job and then when i'm done i simply click create great now that we have our job we're ready to use it so uh i switched environments here to a project that is running our mysql vm now in this case we've chosen to use google's compute engine to host the mysql instance and run the script but again this could be a vm running you know anywhere either clouds on premise and again the script here that we're running is connecting to the database using a standard jdbc protocol and so we're in the vm let's go ahead and just see what kind of data we have here so what i'm doing here is i'm just simply selecting a database called company2 and then we are seeing the tables that are in there looks like we've got a handful of tables and we're going to use dlp to inspect these tables now in the job we've configured it to write say write and save findings into bigquery and so that allows us to then go do analysis either in bigquery using sql or in a tool like data studio where we can visualize things all right let's get started so we're going to run a script here now this script again is just going to look at the the information stream a sample in this case we've set the limit to 10 000 rows per table it's going to stream a sample of that data dlp is going to inspect it and then generate those findings also note here that um even though we have a username and a password in this command line that's not the actual database password we are using our secret manager which is a tool that allows you to protect enterprise and production secrets in instances like this so as you see we're running this script here and you see that with a little note here retrieving password from cloud secret manager and we're running the script in just a few seconds we've gone through all the different tables read a sample and done some inspection here and again we we name this next 2020 demo one so when we go into our report let's look for that data so i'm going to switch over here to data studio now this is a report we've been using for a lot of test runs so we already have data in here and now we've run test runs on local instances uh ones running in in compute engine uh one's running in different systems from other clouds here as well as even running against systems like bigquery which also have a jdbc connector so again it's a very versatile script and again leveraging our hybrid methods we're able to see all of this data from several different runs all collected in this audit report but i'm going to go ahead and refresh the data here and let's see if our demo data is available yet so again we sent those sample data into dlp it inspected it now we're just kind of refreshing to see if that data is available okay so we see here we have next 2020 demo one let's click there and see if we can see any findings here it looks like we found 90 000 different things again sending 10 000 rows per table there could be multiple findings per row so that's probably where we got into the 90 000. great um so we see a couple different tables here and we're going to look at this first one the cars table again as we see here it looks like we were running my sql version here again we scanned database company 2 it looks like we found something in the cars table we found a vin number and an email address so again this is just a quick way to show the power of hybrid so what we did is we ran the script streamed in a sample of data and within a few seconds we scanned all the tables and a few seconds later that data was inspected by dlp and made available through bigquery and data studio for reporting now again for part two we're going to actually take some of that data from that database and migrate it into bigquery and use cloud dlp to tokenize that data along the way so if you remember we found we're going to use this cars table here and if you remember we found sensitive data vin number and email address so what we're going to do is we're actually going to migrate that data so we're going to export it from my sql and we're going to use a pipeline that is powered by uh data flow in order to process that data and ingest it into bigquery now one of the first things that we're going to do is define exactly what we want to de-identify so to do that we're going to navigate back over to our security and data loss prevention in the cloud console and we're going to create a de-identification template so i click here on create go to template we're going to do a de-identification template now d-identification is a term that's really to a collection of techniques to transform this data in this case we're going to do a record transformation and let's just call this test vid 2. continue here now when we're doing a record transformation what that means is that we're going to apply transformations on an entire column now we can leverage our info type inspection as well that's the thing that we use to find that vin number and find that email but in this case we've already done that remember we had a column called then and email that had sensitive data in it so we're just going to create a simple template here for the field then in email we're going to do what's called a primitive transformation so we're not going to we're not going to rely on it finding the email in the vin because we already know that column has it and we're going to simply go in and we're going to do a deterministic token or pseudonymization what that's going to do is convert the vin and the email into a token that keeps deterministic and referential integrity i'm going to cancel this because i've already created that template that we're going to use which is listed here as next 2020 cars did one now i'm going to navigate over to dataflow now in dataflow we've actually already got this pipeline running but just to show you how we got there we create we'll click on create job from template we name our job this will be test job one and we actually pick an existing template here called masking tokenization from cloud dlp to bigquery this is going to ask us for a few parameters again it wants to know the path that we're going to land or drop our csv files into where do we want to ingest them in bigquery and again that reference to that de-identification template that we just created so again filling this out clicking run job that's how we get this pipeline started let's actually go and show this pipeline in action so i've got this pipeline running here and i've also got the bucket that i'm using to ingest this data now um we already downloaded or pulled that data out of my sql and so just for the purpose of this demo what i'm going to do is go ahead and upload that file great so we just uploaded a very small sample of this file again we're trying to show sort of that end-to-end workflow here as we see here it's pulling for the input file it looks like we processed some files here we've run some tokenization i'm going to go over to bigquery here let's just refresh this so we can pull up that data set that we're writing the results to and let's see if it's there yet okay great so you see here we have a cars table that's just been created let's just go look at a preview here and as you can see the email address and the vin number have both been obfuscated and basically de-identified so that we're protecting that information what this means is someone can now analyze this data that we've extracted and moved over from that mysql database without revealing the email address but they can still do joins they can still do aggregates because we've chosen to use a deterministic tokenization method that preserves that integrity so again the same email gets the same token same vin number gets the same token but we're not revealing the underlying sensitive data and this concludes the second part of our demo where we wanted to show sort of a hybrid migration taking data from a hybrid system in this case mysql and migrating that into cloud in this case into bigquery using cloud dlp to protect that data through tokenization and de-identification thanks again for your time great well i hope that demo showed you some of the power of cloud dlp and how you can use that in hybrid environments for more details check out the link below where you can learn about our data loss prevention platform as well as see more examples and use cases for how to apply dlp both on cloud and off cloud in these hybrid environments thank you again 