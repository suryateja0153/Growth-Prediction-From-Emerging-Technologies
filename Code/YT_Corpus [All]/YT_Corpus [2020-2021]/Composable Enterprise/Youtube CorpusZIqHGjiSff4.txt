 hello everyone i'm extremely glad to talk to you today about how i can integrate ml flow with redis ai ml flow being the market leader for life cycle management tools ready say i being one of the most performant deep learning runtime integrating them together have been fun for me so this talk is focusing on how you can use this integration in your stack without polluting your existing stack or without adding more dependencies to your existing stack let's get right into it right so myself i'm sharing thomas i'm an engineer author educator i teach at full stack engineering dot ai about artificial intelligence to the students in the remote places in my country i'm a farmer in the free time i go to the small plot i have available and do stuffs there my intention is to use technology to help farmers in my place so that's something that i do on my side um i'm a senior developer at tensorboard tensorwork is a deep learning infrastructure company we work on making tools for deep learning engineers so um let me introduce few of the tools that we build first very quickly hangar is one core tool that we work most of the time apart from working on redis ai hanger is a data motion controlling tool uh i know you must have heard about you know hundreds of data versioning tools that or or you know call themselves as a git for data hanger is trying to being different by storing the tensors rather than the real world data type that has few fundamental benefits um like we can optimize the backend storage so much that um so much because we know what is the that what is the data that's going to come in it's always going to be numpy arrays or tensors the idea is that um the data is being read by your program much more than you as a human being reading the program so we try to optimize that part uh hangar is a fully fledged close to 1.0 tool right now it supports all the popular operation of git like 2 like branching merging time travel through the commit history full traceability uh it will let you clone or fetch or pull all those operations it would also allow you to partial fetch like if your data is let's say 3 tb and your laptop only has 100 mb space left you can clone only 100 mb and work on that data you can add data even if you don't have the whole data in your local machine um without uh polluting the comment history so different people even even if they don't have the whole commit history they can add collaborate and work on data and push data to the essential repository without having any conflict or issues um partial cloning so that's one of the core feature of hanger a data loader for all the main deep learning machine learning frameworks this is a repository we could definitely use a star from you make sure you visit this repository and try to use hanger stockroom is another project that we work on so it's like another layer on top of hanger but specifically helping you save experiments um data and model now this being a ml flow community one information that might be useful for is that um we're trying to integrate stockholm gml flow so that you can use all these amazing fancy apis of uh ml flow while still save data to stockroom we're still working on that we're working towards making that happen in next one or two months so this is a repository for stockroom it's like super early stage we applied for white torch global hackathon um we tried to make it to 1.0 in next three to four months ml flow so this being again an mlflow community i don't have to explain you about ml flow again um this are as per the ml flow website the core features of ml flow or i would say the pillows of ml flow model registry for storing stuff ml flow project as a packaging system for data science code the tracking system for recording your queries and experiments and operation i'm also models for deploying machine learning models and that is what i want to focus on today so by default out of the box ml flaw comes with few of the back uh packages to help you deploy stuff but um in the 1.9 release of ml flow it started allowing you to write custom uh targets for deployment so if you have made a custom best box solution in c plus or something you can make a custom plugin and then use the ml flows um amazing api to deploy to your custom target so the integration uh by the integration what we did is uh using this api to make a plug-in for you to deploy your ml flow models directly to redis ai without doing a lot of magic it's just one line of command and this will take your ml flow model and deploy that ready say aye as far as the model is compatible so um it it might be helpful for you to go through this documentation this is available on this url i have mentioned on the top the important commands are listed there i think create is probably being the most important create will help you deploy your model to any target so the deployment nitty gritties the specifications will be taken care by the plugin delete is obviously for delete update is probably another important command so if you are changing your model version or let's say you're doing live training from um the data from production and you want to upgrade your model uh the update command is for that um then list and get is for getting the metadata of the model run locally is for you to experiment with this feature you can run the stuff in your local machine help is for help so in this um session we will try to focus on one of one or two more most important commands so yeah like i said what we are trying to do is integrating ml flows fancy apis or well thought design with redis ai's high performance engine through this plug-in system but before going into that integration i think it might be better if i start explaining what is ready say i like very quickly to try to give you an intuition about um why should you choose redis ai maybe over some other tools so what are the current production strategies um we have a bunch of options the most no wise way is to write our own flask or django based server that takes the data runs the model in pure python get the result back send it back to the user and if you want to go async maybe you choose fast api or django channels or something on the async line that can start processing your request as synchronously that's good but not all be scalable unless you do some kubernetes magics um the one of the other choice is to go with execution providers based out of cloud like sagemaker or azure ml they provide nice apis it's just it's just that they are costly but if you have money that's definitely a good solution to go because they can scale without you being a devops engineer or you'll be worrying about your scalability another option is to rely on runtimes tensorflow comes with tensorflow serving torch comes with torch serving there are um serving inference engines from nvidia like tensor rt um clipper um and there are a bunch of such solutions or you could build your own best box solution based out of c plus plus or c or whatever language that you are comfortable with you can kubernetes size all of this and make sure you scale it's just that you need to have um a bunch of devops uh forks in your team hardcore devops folks who understand how to scale the system uh for your need to you know thousands or millions or billions of requests depends on what you need what exactly is the production requirement so like you see in this in that jeff the tetris game it's you need to be carefully make a decision about how uh something new is being introduced you uh introducing to the stack right so that's that's sort of risky so you must fit the existing tech stack otherwise you're running the risk of adding something extra to the stack it should run anywhere any size sometimes you would be running an edge on edge device i mean when somebody build a production strategy production tool they need to take care of that system to run on big machines or you know something like a dgx station or maybe on raspberry pi or jetson nano you know it it should be able to run on different uh type of devices uh if you have composability if you're composing multiple models let's say you're doing a b testing where you need to compose two models to take from the same input things like that should be possible which try to limit the amount of moving part and the moving data um because data is costly if you have to move the data around that's going to cost you a lot it must use uh the resources effectively so those are the production requirements some of the production requirements you might have more but these are probably the most popular one here i like to introduce redis ai redis ai is a reddish module um that let you have tensor as a data type uh if you know redis already radius is um if you know redis already this is an easy slide for you but if you don't know redis already radius is an in memory caching system or in memory database um that has that supports a few data types uh by default um like in florid double list etc um but what redis ai does is it introduces a new data type called tensor so now with redis ai you can store or cache tensors directly in redis data store earlier people used to you know pickle or serialize the tensors and store the seedlines to form but now that you can rely on reduce ai for caching the tensors another benefit with redis ai or another thing that ready ci introduces is the um is that it makes redis a deep learning execution engine and uh it could help you run on cpu or gpu so it essentially turns red is the good old redis into a fully fledged deep learning run time while still being redis so you get all the benefits of redis like its scalability its performance its um reliability everything for free but still um you get the more benefit of your system being a deep learning engine plus tensor asset data type so with the radius ai being in the picture this is a typical uh diagram that explains what redis ai does so right now it has pytorch tensorflow and ionex backend tensorflow lite is under the backend that we support you have a new data type tensor that could go into your redis ai there is something called tar script you will see this in the uh demonstration it could run on cpu gpu it can um clusterize your machine or you can build up a master replica so that you have a fault tolerance where to get it you can go to redisai dot io that's where that's the home page of redis ai github dot com slash already say i slash redis ai for the code but if you are you know not patient enough you can just run docker ready say i slash ready say i will get you the latest ready say image replication is currently made possible um by having a master replica system or a cluster so you can you know spin up clusters uh cluster with a lot of nodes and radius will take care of handling the load handling the cluster channeling etc if you have a system where you have a master and replicas and if your master goes down radius will automatically choose another um instance in your system as a master and make sure you have high availability always all of this is for free right this this just happens automatically this is part of the redis uh the redis engine already so you get all of this business benefits automatically if you are choose to go with redis ai um client libraries right now there is library in python java go and javascript this doesn't mean you are your scope is narrowed down to these four languages the native redis has clients in almost all the popular languages redis ai clients are only in this languages that that what does that mean is you essentially can hit the redis ai server with any native redis client it's just that if you use this four clients you get nice uh user experience but apart from that user experiences fancy rappers um any redis client any normal redis client would work with redis ai so the example repo that i have shown here is a good place to start we have shown some of the examples where you hit the server with the native client rather than using the radius ai client currently the features are the more important features of redis ai are you can keep the data local so um let's say you're building a chat board where your user may sends a request with a dialog and the dialog goes to redis ai for inference already say i makes the prediction sends the return dialog back the response back now there is a context vector right so this context vector is essential for making the future predictions based on the next conversation from the user if there are five users there are five context vectors currently the solution is to keep this context vector somewhere maybe set lice and store it in the disk which is risky or use a caching system like redis where you serialize the tensors to some serialized form and save it in redis but all of these are workarounds right then none of this are good solution and essentially you are um passing around this data which is huge the context vectors are huge in size so if you have like 10 000 users requesting this 10 000 contact vectors but with the inventor of redis ai you can keep the stencils inside the redis ai itself where your execution also happens so you're not really moving the data around which is a huge cost saving um so that's that's probably a good selling point after to say ai it runs on multi-back end so if one of your team working on tensorflow another team working on pytorch you are not anymore working on deploying two different system with redis ai you can run both of them together in the same engine support multiple devices cpu and gpu now traditional ml on gpu with the inventor of oil nx runtime or pi torches hummingbird you can take your traditional ml system or ml algorithms like a model from scikit-learn or xg boost or apache spark convert it to iron x or hummingbird and deploy that radius ai and you get the benefit of those model being running on gpu so that's that's awesome uh you have scripting you will see an example very soon auto batching is something that almost all the deep learning runtime engine or all the popular deep learning runtime engine provides you so if you have you know like 10 20 requests coming together 1000 requests coming together uh ready say i can batch them together execute them together then take the results back split them up to each request and send it back so again this happens automatically in the back end you don't have to really do as a user you don't have to really do anything to make it work ease of deployment even if you need a skill to like a multi-cluster setup um the deployment story is extremely easy it's just bunch of shell commands you don't have to be a expert envelopes engineer or a devops engineer to scale your system uh to millions or billions of requests it's very easy high availability with master replica and clustering um run everywhere registrant so um readers is not to be run on different type of architecture so you get the benefit of redis ai uh running on different machines uh you can keep the stack short um if it's very likely that you're using redis already uh if that's the case you're not introducing anything new to your existing stack language independence ecosystem so redis has a lot of tools in this ecosystem already like if you have a stream of data coming in you can use red stream to take that then push that to ready say i take the prediction push the output to another stream and push it to the user so uh or all you know tools like redis graphs like all the tools in the redis ecosystem is already in your um in front of you to start using it diagon panel execution so if you have like multiple models running in a pipeline or like i said in the previous um slide if you have like an a b testing setup where your data comes and you have two models running in production live then you take the data output from both models and one goes to testing and one goes as an output to the user um especially let's say in an a b testing setup both models run independently does not have a single dependent uh it does not have dependency so both model can run in parallel so all of this will happen automatically so redis ai compile your execution um commands to a single dag and look for optimization options and then optimize it to run to run on different machines so some of the features are we still working on some of them but um you know already say is currently equipped with all the cool features that you can use to start using redis ai there are more coming into the redis ais ecosystem uh we're trying to build more features to make your life easier um so this is the integration plugin that i have been talking about so if you want to integrate ml flow and redis ai this is the plugin that you need to install um pip install ml flow underscore redis ai will install um this package and then you can start using uh radius ai as the target system for your emergency model so now that you understand ml flow which you already knew and i think i have given you a basic understanding of redis ai let's uh let me show you how um how how this can be done so the command would be like this um ml floor deployments create hyphen t redis ai so hyphen t ready say i will pick up that particular plugin and then everything comes after that will go to that plugin and the plugin utilizes information to make the deployment so in this particular example hyphen iphone named resnet so press net would be the radius ai key where the model is deployed so i'm going to show a demo uh for this very thing um so we will train a dummy model in mlflow then we deploy that using this command to redisai then from the client library we hit the redis ai server and see how you can interact with redis ai so this is my ml project file so i have a python train dot py command and i'm passing all these arguments my train dot py is here so the the kodi is very small uh this being a demo i did not do a lot of fancy stuff um so i'm using a pre-trained resnet model and i have a train function this doesn't really do currently it's just like a dummy function so it takes the model arguments it's supposed to train the model and once the training is done i convert the stuff to a scripted model then i use mlflow.script to save this model dml flow now if you're using the master of ml flow or a released package you won't have you will not have dotscript available um so this is a pr that we have opened for this thing once this is merged you will have javascript available i hope to have those merged really soon all right so right so let me run the training here mo flow run no conduct that's gonna run the training it's gonna be really fast uh because there is no actual training happening this is the id so i'll now deploy to redis ai the command is exactly same as you have seen in the slide it's just that the id is different so for this to work i need to have redis ai instance running in my machine so docker run will spin it up now i'm gonna deploy so if you look at this here you see the torch back end is loaded because it's a torch models the deployment happened uh the success message is here for you right we can start exploring this command let me actually list what's in redis ai at list as resnet i can get what this resnet model is so hyphen d ready is ai and i'm passing the name resnet so this will give me some information like it's a torch model it's running on cpu you know a few things like batch size etc right so that's good so i have the model deployed um so this is my code the client code so i'm doing some imports here making a redisai client connection object so i'm gonna hit the server with the model get this will spit out the same information as we have seen before when we hit the server with ml flaw get the same information let me run the example i'm gonna open this cat image i have this here so i have two functions a preprocess and a post process function um those functions are essentially the preprocess function will take the image and do the normalization and all those pre-processed stuff that require for the model post processes will take the prediction probability and uh do an arg max nothing else so those two functions here um so i'm converting that object to a number array and then calling preprocess on that image the first thing that i'm going to do with that image is i'm going to tensor set so tensor set will set the tensor in redis ai then i have a model run model run take the model key input as input key so the input is being something that i have set with tensor set the output will be set to prediction or tread then the final uh line i'll take that prediction back in 10 second so i have that in the prediction variable that prediction variable get will go go to post process and then i print the output to the screen so that's uh tabby cat um right so this this whole pipeline this thing that looks like um a dag like a pipeline right so i can utilize ready say ice dac feature here so if you look at this commands these are exactly similar to this three commands so i have three of them here the only difference is that it starts with the dag and it ends with a run so what result what that does is it gets you the result of each of this command inside a dag and that's why i have the length of the result is three but what i am focusing on or what i want to fetch is only the result of the last one which is something that i could get with -1 as the index so i'll replace this with this and i should essentially get the same result because what happens internally is the same thing the only difference is that this three lines this executions will not go to redis ai until um i run the run command so all the commands go to ready say i together now ready say i fuses those commands and optimize the commands and then run it for you in a single execution pipeline so that your run is super optimized i would also like to show you the scripting options so here if you see we have these two preamp cross process functions this are python functions running in python interpreter now with um the scripting possibility so i've saved those same function to a file here and in the demo here i'm reading that's that function to a script variable and i'm setting that script so there's a script set command i'm setting that to a key and i'm running a script get here so i get that same command i'm the the same code right so you you see that string here that whole file the string from that file so the idea is that um this file is not the script is not running on a python engine there this code in this file the python functions will be converted to a graph and the graph will be executing on sc plus plus highly optimized runtime if you are a torch user you already know what i'm talking about it's a torch script thing but if you are not a torch user just ignore what i just said about torch now so um yeah so this will be executing on an uh optimized runtime rather than python engine so essentially what you have is you set a tensor this is our initial object this is not that pre-processed image um then i run the script with the preprocess function from the script i pass this input the non-preprocessed image output to a preprocessed image then that pre-processed image will go to model run the rest net here as input then the output is prediction which will go here as an input to my script post process then the output of the script will come out in the final so here the the steps are same it's just that the preprocess and cross process now move to redis ai so that all of this could run in radius ai as a single unit so that's what happened here all right so with that my um demo is done um so uh one of the other thing that i'm working on currently is to make a dag like stuff uh where you have models from different frameworks so i'm building a face net um prediction the face recognition so i have like a tf mdc in a model that runs on the input phase the output features will go to torch face net that's built on pytorch and then the the feature from torch face net will go to cycle on svm model um the output from the scikit-learn is the prediction that i will get out of 10-second so models from different framework frameworks as far as support by radice ai can run together in a single pipeline that's what my intention with this particular example so i have written a blog about uh one of the example that i have been working on uh pytorch and ml flow reduce ai uh stuff uh so if you wanna read about about this more please go through this blog this is a nice explanation of how things can be done with that i think i'm done with the talk so thank you so much for bearing me so far i really hope you have learned something new today redis ai um being a highly efficient uh runtime it's easy to uh plug this with ml flow now uh the plugin does not support um oil nx or svm or i mean scikit-learn or apache spark or xg boost model right now but we are working towards making it happen really soon as soon as possible actually so once that is done the plugin would be ready for taking any of your models and deployed to production with just one line of command all right so thank you so much if you have issues raise them in github or reach out to us through the redis ai discussion forum it's built on discord so you can easily access it hope to see you there 