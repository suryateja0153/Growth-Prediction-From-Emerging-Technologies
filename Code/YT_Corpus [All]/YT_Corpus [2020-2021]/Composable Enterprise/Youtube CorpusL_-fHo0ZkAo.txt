 - Zhamak Dehghani is joining us from United States. She's our very own, oh, let me think of an artist, I'd say Tash Sultana here in Australia, you can look her up or maybe Lady Gaga in the USA and don't take me wrong there. They're both incredibly accomplished artists look up the Tasha's "Jungle," so complex as artists. She's so engaging for an audience and always something new and fresh and that to me describes Zhamak. She's been a ThoughtWorks principal consultant for some time, she works with amazing clients. Her clients stories are on their own, a keynote, a focus on distributed systems architectures and digital platform strategies at the enterprise level. She works with high stakes clients. So like health care, those kinds of things where she's not putting the ads on the internet, she's dealing with lives. A member of the Tech Advisory Board at ThoughtWorks, contributes to the creation of our Technology Radar of course. Has worked as a software engineer and architect for a long time in the areas of distributed computing communications like it doesn't get much more complex than that as well as embedded device technologies. Gosh, that was the starting point of so much amazing work in the way we work today with Agile and has contributed to multiple patents on embedded mobile sensing devices so, that it doesn't get more hipped than that. Today she's going to talk about an introduction to data mesh the motivations behind it. It's a concept you absolutely have to get on top of failure modes of the past paradigms, is a big data management. Gee what, gee lords I'm just catching up with likes and now it's meshes, but I think meshes makes so much sense and especially the way Zhamak's about to explain it. So I'm going to hand over to you Zhamak, so lovely to have you and coming all this way from America, thank you very much for joining us. - Hi Nigel, thank you so much. I don't know how I follow that introduction, I try my best. It's wonderful to connect with you from smokey and foggy San Francisco as you can tell from my voice, we've been coughing here for a couple of weeks, but that aside, it's wonderful. It's wonderful really to connect with folks in Australia. I joined ThoughtWorks in Australia and then came to US for supposedly a one year stint and then I stayed here longer than that but Sydney is still home for me. All right, we're going to talk about data mesh today, a paradigm shift in analytical data management architecture. I wonder if there are any "Star Wars" fans in the audience today. I can't see your face, I can see your hands but if there's anybody just say hi on the chat. I'm going to disappoint you, we're not going to talk about "Star Wars," but somebody who I guess inspired and influenced George Lucas is Joseph Campbell, an American professor of literature and author said, "Everything starts with a story." So I thought perhaps today, I start with a little bit of a story as how the idea of data mesh came about. It was about 2018 towards then I think mid, late 2018 that we started seeing a trend, a pattern in US that many of our clients that who've been pioneers in big data management, they had first generation, second generation big data platforms failed, and yet struggling after millions and millions of dollars spent they were struggling with scale. And when we started looking at, I guess industry, the story wasn't anything better than that, it was actually quite green. And these numbers that I share with you is from an annual report from NewVantage Partners that come out every year interviewing and getting service from the executives in many organisations. The numbers haven't changed so much, but this is from when this story of data mesh began, which really demonstrated as we accelerate investment in big data, and there is a massive acceleration in the amount and also that the pace of investment in big data, the business executives, we're still seeing unproven results and their confidence in big data helping them is reducing. And all of them unanimously measured themselves failing on metrics that were transformational, metrics that mattered, be having a data driven culture, treating data as business assets and so on and competing or ultimately competing using data analytics. So, we started looking at what is the underlying problem with this and did a bit of a root cause analysis and also started looking at projects that were doing well globally. And this is what we've found, just to orient ourselves, when we talk about data, the problem starts with these two isolated camps. Scott and Martin touched on this in their keynotes a few hours earlier that the world of data is really divided into two planes, operational data, which is essentially databases sitting behind our applications, sitting behind our services line APIs and serving the business. Often these databases capturing the current state of applications, and we're running a bunch of CRUD operations and updating these estates, we performing e-commerce operations so the shopping card gets updated or the customer status gets updated and this is where the data behind kind of operational systems reside. And we've been mastering how to deal with this data for quite a while now. And on the other side of the organisation, and often when I talk about architecture, it's hard to not talk about organisation because architecture and organisational design are tightly related and we've been talking about conduits like for quite a while to demonstrate that. So, both architecturally and organizationally, completely different planes, sits the analytical big data. The purpose of it is often optimising the business so whether you're creating some reports to see how the business is trending or how the business is functioning so they can change and respond to those changes or you're training machine learning models to again, optimise what arguments do functionality with machine learning and automated intelligence. And the challenge is, these two worlds apart in between them, sits this fragile, labyrinth of pipelines that takes data, flows data from one end of the organisation to the other. And this process is somewhat broken. So, let's go back to see on the right hand side, when we talk about analytical data management, what has inspired the architecture behind that? What has inspired this flow of the data from one end to another? These pictures are taken from Martin's article in 2015 on data lake and he introduces data warehouse, which is a multi decade concept and in a data warehouse architecture style of managing big data, that data still comes from operational systems from the top. They're a bunch of scripts and pipelines that extract this data then transform it to some perfectly model data and load them to some sort of a big data relational, often relational representation of the data so that people can, or application can access that data through this evergreen language or sequel and generate reports, generate dashboards. And I've just sprinkled here, a bunch of technologies that satisfy this architectural style that ultimately serves kind of batch or real time reports. So we've been doing this for quite a few decades and it came about 2010, 2011 this idea of data lake. The analogy, the water analogies really have captured that imagination and that was one of the reasons I didn't use any water analogies when naming data mesh and please do not do that. But it came this other analogy, which was around data lake so instead of moving data from these operational systems and try to perfectly model them in this universally defined, canonical model that would never change, this is not the reality if it's constantly change and provide them as virtual data. James Dixon introduced this idea of data lake who said, "Well, why don't we just get, extract "all of these data from operational databases "and then load them in their low format "in some sort of a semi-structured files "and unleash some data scientists "to gather insights from that, "and if we do then find some specific fit for purpose "data sets in among those, "then we would model them downstream "and provide them as this lecturer marts, "and ultimately then the applications or individuals "can access them through SQL and APIs." And the technology stack that grew to support that again, this is just a sample, involves semi-structured large blob storage, pipeline management orchestration to run these jobs and transform the data and so on. The clients we were talking to, they did have all of that. They did have multiple of that and there were aspiring to now repeat that, but on the cloud. So what you see here is just a copy paste of one of GCPs data lake architectures. It's pretty much the same if you go to Azure, they're billions who use the different icons, but ultimately it's the same story. Get data out of operational systems, put them in some giant storage of often file-based storage and then downstream model them if you want faster or slower, or however you want to query in some sort of a queryable model or a system. So what is wrong with this model? What are the underlying characteristics that limited those organisations that were struggling with scale, that were struggling to respond to change, that were... The same stories that we heard in operational systems since the, before the microservices started, that we're still dealing with, now bring those problems or challenges of scale and responsiveness and agility to the world of data. So one of the underlying characteristics, if you zoom out 50,000 foot view, is that you've got a centralised, monolithic architecture that's supposed to response to this ubiquitous data coming from operational systems. And also not only operational system within the organisations, we're getting data sources now beyond the organisations, data sharing and data markets happening across organisations and it's centralised system supposed to response to this proliferation of applications that runs the engine of innovation in our organisations. I don't have to tell what's wrong with that picture, right? You know what's wrong with this picture. Centralised systems are always a bottleneck. There are points of friction intention as you scale, they're perfectly fine for a certain scale. The second characteristics surround people. So, we have now a hyper-specialized set of tooling that runs these big data applications and with that comes hyper-specialized data engineers to run that tool link and the challenge with that is the signers on the walls between those folks and where the action actually happens, where the data gets generated and where the data gets consumed. And or every organisation you talk to is in this race to get the right number of data engineers, to support those ever growing number of users and providers of the data and we will never win that race. So this, I don't have to again say what's wrong with this picture. You'll probably guess it, is full of friction. And of course I've super simplified here in show you one giant box but of course a giant box has many little boxes inside it. One way that architects, big data architects try to kind of decompose this world is they get some level of operational scale and by scale, I mean number of people that can independently operate and deliver value is to really divide the job around these pipelines for ingestions and transformation and serving, and try to optimise each of these boxes. But no matter how much we do that, the actual value features, changes happens are still going to lead to this structure. To get a new type of data served, you've got to kind of make changes. Yes, you can make generalisations in one or some of these boxes, but at the end of the day value gets delivered when you cross all of his boundaries, or that's a recipe for a release headache, right? Tension and friction. And now finally, the way we execute building these platforms with this centralised silo model is fairly disconnected. Disconnected from who's actually using it, disconnected from who's providing the data and everybody in the mix is unhappy platform people are unhappy, users and providers and providers of data actually kind of oblivious because they don't have any accountability often for providing the data. It's the consumers and platform providers that feel the most friction. So for decades, we've been building big data platforms based on centralised architecture by group of site, hyper-specialized silo people through disconnected execution. And no wonder that we're struggling, no wonder these are the failure symptoms that we kept seeing with our clients, either they're difficulty to bootstrapping building such as a large platform or difficulty, once they pass that stage, difficulty scaling and ultimately difficulty materialising any value outfits of course not any value, but value out scale, differentiating value, so where do we go from here? So data mesh came as I said, by looking at principals that worked in pockets of our successful projects. So, not everything here is new. It's a new articulation and a new, I guess naming and bringing it all together as a concept while many of these individual principles that I'm going to introduce have been used for quite a while. So, my apologies, I have a really bad cough. Let me see if I find the mute button somewhere, so to mute you. So what could it look like? So what if we broke down this monolith instead of breaking down based on pipeline and pipeline stages, we broke it down around the concept of domains. We're familiar with domains, we worked with domains, that's how our business is decomposed, that's how often our technology serving the businesses is decomposed, is around business domain concepts. We've been doing domain-driven APIs for quite a while, domain-driven services. Why not the data, the analytical data, this historical view of the customer changes, historical view, analytical data often is immutable historical representation of the facts of the business, right? Why don't we make that, serving that a responsibility of a domain, take that responsibility as close as possible to the people that actually know the systems, know the data, that they're the ones that generating it, or they're the ones that are going to use it. So what you end up with as the first class decomposition concern, or component of your architecture is the data, analytical data aligned with the domains. And these domains somewhat, sometimes are aligned with source, they belong to the teams that are responsible for producing the data and their point of origin. And sometimes they are aligned with the consumption so there are more fit for purpose data or aggregation and sometimes something in the middle, right? So if you have online claims and call centre claims, you might decide just one claims and encapsulate all of the claims and historical changes to the claims that are happening and inside snapshots of the claims in the middle. So the first pillar of data mesh is decomposition of the data ownership, most importantly ownership and then yet the second we're considering architecture on around the boundary of domains. And if you ask, what are the domains? Just look at your business and you will discover them because that's how you've structured yourself. The second concern or second pillar is around, I guess addressing the challenge of this distributed ownership because the moment you say, now the data is decentralised and distributed everybody will kind of look at you oddly that, well now are we going to go back to that silos of data that we can't really find and they're only inter... They're not interoperable and we can't really use them, we can't find them, well, no. That accountability with that ownership comes in accountability and responsibility that each of these data sets should be served as a data product. Every node on the mesh, I call it a data product. Naming things is hard, that's what I call it. I want to move away from data services that has a different connotation, but every node on this mesh is a product. What does it mean it's a product? Well, a good product delights experience of its users. Who are the users? They're the data scientists, and that they're in earnest they're the data users in your organisation. So what does it take to delight their experience? Well, first and foremost, these data sets need to be really discoverable, easily consumable, understandable, good documentations, good examples, addressable, trustworthy, like the list goes on and on, and you can kind of then map that to the technical capabilities that needs to be built into the code that provides this data to support those capabilities that are kind of enumerated on this slide. What does that all come with? Good intentions, no. We have to introduce these new roles, domain data product owner, whose accountability and KPIs are around the delight of their customers. There are all around how happy the data scientists are, how quickly they can get access to it. What is the lead time to actually generate value after discovery from using the data? And I'd want to just double click for a minute on these data product as a concept, because it's such a key concept in data mesh, a lot of people just have such a hard time to kind of get it, and I'm going to to do it I guess in an abstract is still not a code level, but at abstract level to just describe that like microservices that are the architectural quantum, the smallest unit of your architecture that can change independently and serve value independently. This is also a data product on a mesh or a note in a data mesh is that architectural quantum. So it provides historical data read only access, this is not an API to kind of update your customer, you go to the customer microservice or application to do that. This is the API to actually see the historical changes of a customer, that historical view. So these units of architecture encompasses code, code that receives data from somewhere ultimately, either from the customer application or microservice or from an upstream data product through this kind of polyglot input data ports, and has code and interfaces to provide data out for big data style of consumption through this polyglot output data ports provide data in form of SQL because people who writes the report still are in love with SQL for provide data in columnar format, because machine learning training, you do feature design and train on many rows and some columns and provide some control ports to control it. So audit it to discover it, to set the policies, existential policies and so on. So these names, input data ports, output data ports, control ports, are the names that we up with to define the blueprint of a data product which is ultimately the code and data that can be independently deployed, owned, and have an independent life cycle. But this is a point in the kind of introduction of the concept that people who spend the money, they start looking oddly at me, and they would say, "Well, are you telling me "that the amount of money I'm spending right now "to build all of these infrastructure "I have to now exponentially grow that, "repeat that per team, "because every team is going to set up a Databricks cluster "and a bunch of blob storage "and some I don't know, airflow pipelines?" And as the answer is no, we have answers to this question, we've done it in operational world and the answer is, well, in this world the true autonomy comes through abstraction of complexity into the layer of data platform. So abstracting all of those technical complexity that allows to bootstrap one of these data products into the data platform as a product itself that is maintained by the Data Infra Team or Data Platform Team. So what is that technology? What does that look like? Well, at a highest level of abstraction, you really don't want to, this data product developers to actually be data engineers anymore. Do you want any general software engineer be able to just produce this data products, that you want them to just focus on the logic of transformation or logic of cleansing or whatever they need to do to provide that data with those quality metrics that we talked about. We don't want them to worry about setting up infrastructure, so what we have defined as an interface to this layer is literally simple declarative... what about if I can say that word, declaration of data products that defines, okay, what are my input ports, output ports? what sort of access policy has code that I need to define to say who can access what ports? and really pretty much it, the rest of it automatically would get configured for them and provisioned for them. So infrastructure takes care of provisioning of the bootstrapping and a skeleton for these data products and I listed a bunch of capabilities there that often end up in there. And finally, this is the point of conversation that the traditional data governance people are just puzzled as how this is ever going to work, because right now we are struggling to get one canonical universal model for our data and now how are we going to create these models when everything is distributed and autonomous and all of these good words, nice sounding words and phrases. Well, the same way that the web works, the same way that APIs kind of work. We have to define a governance model that really is based on standardisation, is really based on interoperability. So the model that we are experimenting with right now is this model of federated governance which is a group of DDoS data product, domain data product owners coming together with the infrastructure data platform product owner and the kind of the decisions that need to be made global, whether every made decision needs to be made globally actually you want to minimise the number of decisions need to be made globally. But there are some decisions, right? Okay, we have these particular data where entity, where does it belong? Does it belong to domain A or domain B and decisions around that, that is a continuous conversation, it's not a one-off conversation. Often those poly scenes, right? That across different domains or domains that don't exist that need to be recreated. There are aspects of the data modelling that absolutely you want to localise to those domains, domains know how to model their data we don't have to dictate that globally. But they're the same concept that cross boundary, right? You want to identify the customer with a global unique ID when he comes into the data product, because you need to combine this type of customer in the customer domain to the claims domain and meaningfully create a joint between these two domains. So, there are some decisions that need to be made globally. Metadata needs to be, metadata structure or how we define as silos for that data products and measure quality needs to be defined globally. And once that's defined globally, you want to codify that as much as possible and embed that standard as code and automation into the platform so nobody has to repeatedly make this decision. So it's just part of the platform, you get that for free when you build the data product. And finally execution, in this slide itself can be a whole talk so, I'm not going to do judgement , I guess justice, but if you want to build this link and I know at this point people kind of feel overwhelmed that they think, well, bootstrapping one big lake was a problem, how I'm going to bootstrap this many moving parts? And ThoughtWorks is a big fan of iterative developments and use case driven and so on since slicing the whole, that paradigm works here. I've done it, we've done it, it works. It's hard, but it works. You start finding those use cases that closes the cycle of intelligence, that closes all the links from the applications that are points of actions and interactions to the data that they generate as data products, to they intelligence that would use that to make better decisions that humans do with automated, I guess decision-making. And you find those use cases that close that loop and for each of those use cases, then decide what data products we're going to bring online and create what parts of the platform as a minimum we need to bootstrap to support them and over and over you trade over this and you get to the maturity that you would like for a number of scales. We've defined kind of evolution models for that clients that, if you're in a bootstrapping phase, what's number of data products you should be thinking and supporting if you're in a sustaining or a scaling, you can define these phases of maturity or evolution for your platform. All right, in summary, pillars of data mesh are domain oriented ownership and decomposition of the data. Each data is treated really as a product with those quality metrics so that we don't end up with the silos of independent data that they don't play well with each other. Self-serve data infrastructure to create autonomy and remove complexity from each of these data product and data teams. So once and for all we can stop and take a breath, that we don't need to continuously look for this magical unicorn. of data engineers, their skills are highly valuable and we need them where can also use generalist developers to cross that chasm to become data aware by using higher level abstractions. And finally, a federated governance that focuses on ecosystem thinking. Just go through example quickly, an insurance, on my original writeup on marching site, I use a digital media, which is a much more fun and exciting, but I think insurance, health insurance is more and that's right now, human, whatever one's going through globally. So, imagine you have an insurance domain. So this purple orange thing is the actual domain of insurance. And we have a call centre application, it's one of those 40 year old Coldwell applications, nobody knows what the application does, we can't even touch it, but we really want to get all of those claims that are coming through the call centre, a historical view of let's say, daily snapshot of this. So God forbid we have to create one of these data products that run CDC, change data capture from the database of that claim centre or the data models of that claim system and create now a data product with an output port that shows daily snapshots on some sort of way, let's say, the data lake storage like ADLS or S3 or so on, and provide that as an API that can be accessed through the output port. And the other fancier part of the organisation, more modern, we've got this great microservices event driven as far and it talked about online claim system. And those guys say, "Okay, we're happy to give you online events has happened, "but we're not responsible for retention of this forever. "It was just a typical 70 retention model." Then the team, the online claims team creates this online data product that consumes those online events, low retention, short term retention, and turns that into this events, infinity event logs of claims data on an output port of the online claims data product. And they also think, even though we love events, we're still in the world of machine learning, despite all of the hype around streaming. A lot of other advanced companies that we work with, they're still quite happy with some sort of a batch processing and batch training of their models so, we still want to keep those data scientists happy and we give them that. Also of the same data, so daily snapshot or daily change snapshots of columnar let's say porky files of those claims and not everybody wants to deal with these different types of claims independently, maybe some applications do, or some machine learning models do, or som reports do, but we also think claims domain would be nice to have an aggregate claims that aggregates data from these two and have a data product that provides claims as a whole regardless of where the source is. So then in the other corner of the organisation might be a members domain, right? Which has deals with the registration of members, or cancellation, or change of their address, or change of their status and improvise this member changes, snapshot or files and we really want to create a new data product that is helpful to the care assistants in this insurance company, to be able to assist the members that seem to be falling behind their health. And they need to be called and they need to be helped out maybe they've changed their address and they don't have a primary care and they need the primary care. So, it really trickles all the way down. So these products, you can just imagine the new ways of aggregating it and some of them you just run a simple pipeline and some of them you run like the last one, members one you run a machine learning model as part of your pipeline embedded in to that data product. All right, so hopefully so far it kind of made sense. There's a lot to consume and there are a lot of technical details behind the scene that I haven't really shared, but what's going to happen next where we are today, I love this quote by Alan Kay, "The best way to predict the future is to create it." And that's what we're doing here, we're creating the future. It's really, this new paradigm shift is really changing us from this past of centralised ownership to decentralised ownership and a new model of governance a model of governance that's once it embraces change not embraces static world. We're moving from monolithic systems to distributed system. One of my hopes is that we kind of get rid of this word pipeline, or even if we use it, we just hide it behind something that's a better encapsulation of architecture and it respects some of those groups programming, I guess principles, and we use really domains as a first class concern and pipeline as the second class implementation concern. And instead of thinking about data as this thing that comes as the exhaust of our operational system as the byproduct that becomes a project itself. So you e-commerce team get also measured by the sanity and the usefulness of the data, the analytical data that you produce. A different language, once you learn a new language you see the world perhaps differently. I think we need the reverse of that here, we need to see the world differently and for that we need a new language and I hope we can together create this new language. I like to see less of the words or the phrases on the left hand side and more of the phrases on the right hand side. Catch yourself every time you feel that you're using this as still paradigm of flowing things from A to B, ingesting and loading, catch yourself. That's the paradigm of the past, think about really serving these data through APIs, through these ports, consuming, discovering, and consuming them and linking them together. If anybody came to you and said, "I've got a data on my system I can sell you today." Runaway, because this is not a system problem this is an echosystem thinking. So instead of thinking of the centralised system that's going to solve all of our problems, look for those tools that are happily playing in an ecosystem and give you that ecosystem distributed kind of ownership the same way that API frameworks have given us, the same way that infrastructure orchestration tools like ordinaries have given us. So yeah, use a new language, we need the new language for this. So right now, according to the InfoQ early 2020, we are in the innovators curve of adoption for a data mesh, I hope that we're going to get to early adopters and majority adopter soon and the questions of course, next time we talk is not going to be, what is data mesh? Is going to be, how to do the data mesh and then for years we're going to argue how to do data mesh right. And hopefully we going to close this, get rid of this fragile system in the middle and really close the gap between this operational and analytical and these two becomes two layers, that much closer to of the same plane in a way, because they're going to be owned and served and used by the same people, people aren't taking structure matter here. So, and each of those really it's a flow of information from one to the other. I use the word flow a lot but it's really the serving of the data from operational system serves, the analytical systems and machine learning that ultimately augment those operational systems so it's a continuous flow. And I still want to respect the differentiation between the two. Managing an operational database is very different from dealing with analytical data on a lake. So we still needed a different technology stack, we still need a different engineering and so on, but the ownership can be closer to this planes. Well, thank you so much for your attention. - I love the way in fact, the way it was emerging, you were using some sort of psychic event pipeline by answering questions exactly as they were asked on the channel. So it definitely got answered, - Oh, really. - But there's some really good ones, I've got a minute to ask you one and is the book winning question. I'll ask you because we've got Martin's book, we're giving away several copies today. And so Brett calm managed to cross the streams today of refactoring and the question is, can you refactor your way from like to mesh, or is this a revolution? - Yeah. Okay, so I would say yes of course, but you have to be really aware of the traps because it is a paradigm shift. So the argument of incremental changes to the existing infrastructure and existing structure that we have has quite a bit of few traps, and I can talk to those. So we have clients that actually, had data warehouse a deer, they got a house with lots of golden datasets, hundreds and hundreds of data sets like are in fact thousands. And as a starting point, we said, okay, while we're bootstrapping building this infrastructure, this self-serve infrastructure, why don't we just shift bringing data from the warehouse and create these domains aligned to the source, bring the folks from the source teams, the point of origin, those origin systems together and create these data sets but we use the warehouse as a intermediate source. It works, but the challenges, those data warehouses are downstream from, bounce from many processes from your source systems and the way those tables are designed, are designed for different technology, they're designed to optimise star schema queries. They're optimised for, they have, they are... I guess they are subjected to limitations of those technologies so you end up with this mapping of tables to data products that may not be the best mapping. So as long as you're aware of those traps, you can migrate from an existing lake or existing warehouse for that to be an intermediate step before you retire that and just directly go to the source. - Superb answer and if you have the energy, love you to drop into the channel where everyone's hanging out and what a great way to start our afternoon. Thank you, Zhamak for burning through - Thank you so much Nigel - And the best wishes in San Francisco 'cause things are a little bit weird there at the moment. - Yeah, thank you so much. Take care everyone, bye, bye. - Okay, bye. 