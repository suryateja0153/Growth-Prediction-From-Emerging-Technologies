 hoi ik ben Nick Dyer een Solutions Architect van HPE Ik ben hier vandaag om met je over HPE te praten Cloud Volumes. Op het scherm hier Ik heb Amazon Web Services getekend en Microsoft Azure Ik heb er ook wat getekend on-premises infrastructuur met sommigen missiekritieke applicaties zoals SQL, Oracle en SAP. Ieders dezelfde vraag stellen: als ik loop on-premises infrastructuur vandaag met deze missiekritieke applicaties, hoe gebruik ik de cloud het beste? Hoe verplaats ik mijn applicaties naar de cloud? Moet ik tillen en verschuif ze of moet ik opnieuw ontwerpen ze voor wat we cloud native noemen? Hoe lever ik een strategie voor twee klanten en ook hoe bescherm ik mezelf ertegen mislukkingen? Hoe zorg ik ervoor dat ik het kan een back-up maken en herstellen in de cloud en hoe kan ik mijn gegevens terugbrengen op locatie? opnieuw voor het geval dat nodig is? Ook zijn er andere vragen rond nou, hoe zit het met cloud lock-in? Hoe doe ik overstappen van de ene cloudprovider naar de andere zou ik dat willen en hoe zorg ik ervoor dat ik een deel van het inherente werk rondloop complexiteiten van de cloud zoals lager failure rates of resilience rates van drie negens duurzaamheid of bescherming? Dus we hebben een oplossing die nu wereldwijd beschikbaar is dat we HPE Cloud Volumes noemen. Dit is een cloudtechnologie waarmee we datasets met elkaar kunnen verbinden Amazon Web Services of naar Microsoft Azure. En ik kan verbinding maken die datasets voor welke prestaties dan ook vereisten. Ik wil dat ik het misschien wil. 5000 IO's en ik kunnen ook precies zeggen hoeveel capaciteit die ik wil, dus ik wil er misschien drie terabytes aan opslagruimte, en hier Ik kan zeggen dat ik misschien wel duizend IO's wil en misschien een opslagcapaciteit van 110 terabytes. Zo Ik kan de. Onafhankelijk veranderen prestaties in de capaciteitsvereisten van mijn Cloud Volumes aan de cloudproviders Ik kan nu veel onafhankelijke gegevens nemen bescherming snapshots voor onmiddellijk herstelt binnen cloudvolumes en zij kan applicatie consistent zijn, maar ik werd niet aangeklaagd voor hen omdat het zijn geen volledige kopieën van mijn gegevens, dus ik wordt alleen in rekening gebracht voor de wijzigingsgegevens met binnenkant van de gegevensbescherming herstelpunt waarmee ik kan opslaan enorme hoeveelheden geld. Maar die gegevens momentopname van de beveiliging is seconden lang in plaats van uren of dagen en lengtes dus Ik kan uurlingen nemen, halve uurtjes, dagbladen, weekbladen - waar ik ook voor nodig heb missiekritieke workloads die binnenkomen de openbare cloud. Ik kan ook draaien veel verschillende klonen van mij datasets zonder de gegevens te kopiëren, dus ik kan vervolgens mijn gegevens klonen voor nieuwe volumes voor UAT om hun werk te testen. Misschien voor containers voor met kubernetes en Docker. Maar nogmaals, ik krijg geen rekening voor een volledig exemplaar van gegevens. Ik word alleen in rekening gebracht voor het incrementele wijzigingen die zich in die clients bevinden dus ik kan duizenden klonen nemen zonder extra kosten te maken enkele van de echt coole dingen die ik kan doen met deze on-premises infrastructuur is Ik kan nu repliceren als ik HPE heb opslag op locatie kan ik nu repliceren die gegevens ingesteld op een client, dus ik kan het nu gegevens worden gerepliceerd naar de cloudgebaseerde back-up of zelfs naar de cloud noodherstel. Ik kan het opdrijven en dat live uitvoeren en dan kan ik verbinding maken het naar AWS of het kan het verbinden met Azure. Ik kan het ook weer repliceren dus nu kan ik gegevens laten lopen in de public cloud live replicating back on- bedrijfslocaties en gebruiken ze zelfs lokaal voor noodherstel of repatriëring de data. Ik kan daar ook een volume hebben mogelijk vandaag verbonden met AWS maar eigenlijk wil ik dat misschien veranderen en ik nu wilt wijzigen om verbinding te maken met Azure. Dus eigenlijk is alles wat ik kan doen gewoon ga naar onze gebruikersinterface van Cloud Volumes, koppel het los en vertel het opnieuw verbinding te maken met Azure en binnen enkele seconden verbreekt de dataset de verbinding van Amazon en maakt opnieuw verbinding met Azure. Fysiek de gegevens verplaatsen naar allemaal van de ene klant naar de andere, dus nu ik kan dual-cloudflexibiliteit hebben, dus nu ik zou een dual-vendor-strategie kunnen hebben openbare cloud. en ik kan nu flexibel verplaats workloads tussen AWS en Azure zoals en wanneer ik wil. Het echt echt cool ding over dit alles is dat wij daadwerkelijk onze eigen cloud draaien die we noemen HPE info site info site is een kunstmatig intelligentie en machine learning platform beschikbaar voor al onze klanten die on-premises werken infrastructuur en dat stelt ons in staat om voorspellen en voorkomen van meer dan 86% van de problemen voordat ze echt gebeuren. Gebruik makend van machine learning waarmee we kunnen omgaan deze technologie is nu dat we dat kunnen uitvoeren voor openbare cloud, dus als u HPE hebt opslag op locatie waar u van profiteert InfoSight vandaag, maar nu kunnen we profiteren InfoSight voor cloudvolumes, zodat we het nu kunnen doen geeft u bewezen duurzaamheid met behulp van de machine leren en kunstmatige intelligentie, maar we kunnen nu ook voorspellen en voorkomen mogelijke problemen die kunnen optreden in de toekomst in je cloud. We kunnen ook doe crowd-sourcing dus als we andere zien klanten met vergelijkbare workloads u kunnen we u dan eigenlijk vertellen waar je moet misschien iets afstemmen op hoe we zien dat andere klanten naar binnen rennen on-premises of de cloud, en dit doet niets extra kosten. Dit is alles inclusief als onderdeel van de HPE-opslag portefeuille. Net zoals alles wat ik heb vandaag gesproken over snapshots en klonen, datamobiliteit alles wat je hoeft te doen is het kopen van HPE Cloud Volumes. Al het andere is alles inclusief. Heel erg bedankt voor deze video bekijken. Ik hoop dat je het hebt gevonden nuttig. Als je meer wilt weten ga alstublieft naar HPE.com/storage of neem contact op met uw lokale HPE-partner of accountmanager. Dank je. 