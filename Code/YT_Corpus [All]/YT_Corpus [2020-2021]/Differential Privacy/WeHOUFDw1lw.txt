 My name is Joely Nelson, and for my  final project for computer security   my presentation will be on a summary  of security risks of machine learning. Machine learning is the study  of computer algorithms that   improve automatically through experience. By looking at data they're able to  learn some model to describe that data. There are a number of important  assets that we want to protect. The first is the model itself. A lot of effort  goes into designing, training, and deploying   the model and we want to protect that from  adversaries who want to steal that model. Another is the data. There could be  private data involved in the training,   the testing, and any live production of the model   and we want to protect that from adversaries.  We want to keep that data from being leaked. A third asset is the integrity of the  model output. We want to make sure   that the model is giving good predictions  even if it is encountering an adversary,   especially if that model's output is  being used to make critical decisions. Like any technology there are security  risks associated with machine learning.   Some are not exclusive to machine learning. So machine learning requires a lot of data.   There's just an inherent security  risk to storing lots of data.   When you have a lot of data lying around there's a  high probability that bad things can happen to it. Machine learning is a relatively new  technology. Vulnerabilities are not   well studied in production, and people  are just very excited about deploying   it and they might not be as focused on security. There are some security risks that are more  exclusive to machine learning, and these are   going to be the types of risks that I'm going  to be talking about in this presentation. So the first one are model  extraction attacks. This   is when an adversary reverse  engineers and steals a model. Second are model inversion attacks.  This is when an adversary uses the   model to leak information about the training data. And the third are adversarial example attacks,   and this is when an adversary feeds in data  that leads to incorrect model predictions. So the first attack we're going to talk about  are model extraction attacks, and this is where   an adversary reverse engineers a model that's been  made available to query, and by doing this they're   able to replicate the model for themselves. So  here we have an example of a vulnerable model.   This model takes in text data and it outputs a  class for that data. So in this case it takes in   some text and it outputs the sentiment associated  with that text whether it's positive or negative. So how this attack works is when attackers  send a large number of queries to the model   to get back labels. These queries don't need  to make sense, they can just be gibberish.   So like in this example here we have an adversary  they're sending in this text that doesn't really   make sense. They're sending it in the model and  they are getting a label for it. So basically what   the adversary is doing is they're they're labeling  their queries and now they can use those labeled   queries to train their own model. And even if they  don't have the exact same underlying architecture   they basically can use that model that  they've extracted to achieve their goals. So why might an adversary want to extract  a model? Well one of the reasons might be   theft they just want to stop paying for the api  and launch a competitor and by reconstructing   the general behavior of the model they can do  that. Another reason might be reconnaissance.   Stealing the model can be a stepping  stone for other types of attacks.   Having that copy of the model, and even if the  architecture of the the model isn't exactly the   same, it still makes um model inversion attacks  and generating adversarial examples much easier. So what are some possible defenses for this?  Well one of the things we could do is try to   detect queries that could be part of a model  extraction attack and we could block those   queries. But this isn't very robust  attackers can find ways around this.   Another possible defense is perturbing the output.  So basically what you do is you do some extra   calculations to vary the output of the model, so  not giving it exactly what the model's prediction   is you do an extra calculation to change it a  little bit. Depending on the model there may be   some ways to reduce an attacker's accuracy while  keeping the original model's accuracy pretty high.   The papers that I saw that talked about this  there was a lot of complex math, I don't really   want to go into it, but it's a pretty cool area of  research. It's relatively new. Most papers that i   saw on this had been published this year, so I'm  really excited to see how this this area develops So the next attack we're going to  talk about is model inversion attacks,   and this is when an attacker uses the model to  leak information about the training data set;   and to do this attack the attacker  doesn't need the data set they just   need to be able to query that model and  get some kind of output from that model. So this works by the attacker querying the model  by giving it some input and then they receive   some output about their query. So  if the model is a regression model,   which means that it predicts a numerical value,  they might see that numerical value that gets   predicted. If the model is classification they  might receive a confidence score that their input   is of a certain class, so like I'm 75% sure  that you've shown me in a picture of a cat.   Then the adversary will use that output  to estimate if the input they gave   might be from someone in the model. The specifics  of this attack depend on the type of model. We're   going to be talking about it in a context where  it's used in a facial recognition neural network. Okay, so say that we have an attacker that has  access to this facial recognition model that   takes in images and outputs who it thinks  that person is with a confidence score.   So here we've given the model a picture of  someone's face, face.png, and an output that it is   55.9 blah blah blah percent sure  that this face belongs to Alice. So basically what an attacker can do  is that they generate a lot of faces   and they check the confidence scores and then they  can use that to generate better and better faces   and they use gradient descent to do this. So  gradient descent is an optimization algorithm for   finding a local minimum of a function. In our case  our function's input might be the various parts of   the image like maybe the individual pixels and the  output is the confidence score, and basically what   we do is we use gradient descent to to find the  image that gives us the highest confidence score   and how it does that is basically for every  image when you want to generate a new one you you   kind of look back and see like what's changed if  something that we changed from the last image made   the confidence a lot better then we want to keep  changing that thing in the same way. So like in   this case we give it a face we see that it's 55.9  sure it's Alice. We generate a new face and that   goes to the model and it says that now it's 68.4  sure that it's Alice so we're getting closer we   should we should keep generating faces kind of um  changing those same features to generate new faces So in practice um this took less than two seconds  to reconstruct a face. This is an example from um   the paper that that demonstrates this the image  on the left is a reconstructed image via the   model inversion, and the image on the right is  the actual image from the training set. So you   can kind of see the reconstruction isn't perfect,  the image on the left doesn't look exactly like   the image on the right, but when researchers gave  people um the image of the reconstructed face and   they asked them to identify what original face it  was reconstructing, they were able to do this 95   percent of the time. This is definitely leaking  substantial information of the training set. So why is this a problem? This isn't as much  of a problem if the data is already public,   but if it's private this is a huge problem  if your model is using medical information,   or sensitive survey questions, or pictures  of faces that aren't meant to be public.   This is a huge problem and it's a  problem for a number of type of models. So some possible defenses. If possible have  the model output less specific information.   In the case of the facial recognition network what  researchers did is they rounded the confidence   scores that were output, and even with relatively  low rounding the adversary couldn't produce   recognizable images. With decision trees  researchers found that if they were split   on the more sensitive features lower down, then  the adversary couldn't extract those sensitive   features. Another thing in general is to anonymize  the data try to make the data as anonymous as   possible when training the model because even  though you're not releasing that that training   set it's possible for an adversary to extract, so  it's something to keep in mind. This isn't always   possible, like with a facial recognition data set,  but if designers can do this then they should try. So the last type of attack that i want to talk  about are adversarial examples and this is when   a model is given some input that causes it to  give a wrong prediction. This attack is mainly   studied in neural networks so in this example  here we have the original image on the left   that the the classifier is taken  in and it's classified as a panda   with 57.7 percent confidence. That makes sense.  But then what happens is some noise is added,   this very small amount of noise, to create the  image on the right. If you or i were to look   at these two images they look very similar, but  the network is predicting panda for one of them   and gibbon for the other, so just that very small  amount of noise completely throws off the model. So there are two types of adversarial example  attacks. One is digital, so like in the example   of the panda we are modifying the digital data  in some way. In this case adding on noise. Another type are physical perturbations and  this is when the attacker modifies the real   world in some way. So for an example an  attacker might place a sticker in a scene   that causes the model to produce an incorrect  prediction. In this example we had a classifier   that took in images when it was shown an image of  a banana it predicted it was a banana with high   confidence. That's great. But then when  a sticker was placed in the scene, this   is kind of weird looking thing, it now suddenly  predicted toaster with really high confidence. So why is this a problem? Beyond  ai mistaking pandas for gibbons,   this could be a major issue for any system that  uses machine learning to make critical decisions.   So on the left we have these  glasses that full facial recognition   and they allow someone to impersonate someone  else. On the [right] we have an example where   these stickers and graffiti were placed on stop  signs that made it so neural networks weren't able   to recognize these as stop signs. If you have a  self-driving car that's using a neural network to   make really important decisions, like to stop at  a busy intersection, that could be a huge problem. So how does this attack work? How  do we generate adversarial input?   Not just any perturbation will do. So in that that  banana example when a literal picture of a toaster   was placed it still thought it was a banana,  but when this kind of weird looking sticker   was placed in the scene then it predicted  toaster. So basically what you want to do   is you want to use the model and find the  smallest perturbation that will incorrectly   classify the image while not violating any  constraints. And what I mean by that is like   in the the stop sign example the sticker needs  to be on the stop sign we can't put the sticker   in the sky. And this ends up being a constrained  optimization problem that can basically be solved. So some possible defenses. One is adversarial  training. This is where you pretend to be the   attacker and you generate a number of adversarial  examples and train the network on them   so it won't be fooled when it sees them.  This isn't very robust. Attackers are   going to come up with new attacks, you're  going to have to come up with new examples,   and the cycle continues. Another possible defense  is defensive distillation. So distillation is   when the knowledge from a pre-trained  network is transferred to a new one,   and you can actually use distillation to smooth  the model and help the model generalize better   to samples outside of its training set. And  this is going to make it harder to be fooled   by these small perturbations that  are used in adversarial examples. I want to wrap up this presentation  by talking a bit about the ethical   and legal issues surrounding this topic.  When it comes to model extraction attacks,   machine learning engineers may have to  trade off model performance for security.   They're going to have to consider if this is  a worthy trade-off especially if that model is   being used for critical decisions. Is it worth  it to provide that extra security for the model   if it means that the model is going to  be worse and possibly put people at risk? Legally there's nothing inherently  illegal about querying a model,   and yet it's the basis for all of these attacks.   We're gonna have to consider how to implement  laws to police these attacks for the future. Also machine learning engineers may not  feel like they have a responsibility to   make sure their input will not fail in the  adversarial example situation. In the case   of road sign detection it's already illegal to  deface stop signs. Engineers may not feel the   need to consider that case because they're already  struggling to get models to work on normal output,   but failure to deal with these examples  could cause an injury and death of customers.   Especially with how easy some of these  attacks are it needs to be considered. Thank you so much for watching my  presentation! I hope you enjoyed 