 okay um good morning everyone uh we start our morning session of the second day of the conference in the morning we will have two speakers uh professor sanjiva aurora and professor vera semenola let me quickly introduce professor aurora he's well known beyond theoretical computer science community so he doesn't really need an introduction i will only briefly mention that he is a co-author of the popular text book on computational complexity and he is now investing a lot of efforts into machine learning for instance over the last three years he has been pushing this frontier from the intellectually stimulating premises of the institute for advanced study before we start an organizational note to the audience please feel free to ask questions in zoom q a section or directly into chat and we will deal with them in the end uh okay let us open the second day and welcome professor sanjiv aurora uh thank you so today's talk is about opening the black box the black box of deep learning and how people are trying to get a mathematical understanding of it so uh deep learning is all uh obviously in the news and we're all very interested in that um self-driving cars playing gold detecting cancer in x-rays etc etc so uh so it's been so successful uh in practice but there are some very difficult mathematical questions which we'll talk about and also why they are difficult i'll give examples of mismatch between traditional frameworks of thinking about learning and optimization and deep learning phenomena and i'll give a survey of some understanding and new puzzles from recent years and then i'll wrap up so remember that machine learning is primarily like curve fitting the classic curve fitting that we learned namely you have data and you're trying to fit some kind of a a curve to that data so machine learning is also like that but it's surface fitting with many more variables millions or hundreds of millions or even a billion variables these days so one statistical issue that arises in all of data science and also in machine learning is what does it mean to learn and uh the simplest definition of that although now uh research has given other definitions as well but the simplest definition is that there is some fixed distribution on data points so uh the data point is input and label for a classification problem and training involves taking a random sample from this distribution and use it for training so that's the training set and now to test whether this model has learned you check its output on a random holdout set which is a proxy for estimating the trained models error on the full distribution so this is also called test error and if the test error is much more than the training error then the model is overfitted training hasn't worked very well it has failed to generalize so the model did well on the training data but didn't generalize to test data to the rest of the distribution so for today's talk we'll just stay with this simple definition of learning although people are extending it to other definitions so now deep nets a quick reminder of uh what are deep nets so for simplicity here what i have is something called fully connected nets uh so input comes in on the left um so uh on over here can can you see my pointer or not yes yeah and it comes in and you apply a linear transformation matrix m1 and you get m1 times x and then it you pass it through a non-linearity then you get a vector x2 you pass it through another matrix linear transformation you get m2 x2 etc and uh and at the output you get some uh some number or vector and theta here is a set of parameters in this net which is the matrices so the entries in these matrices and the simplest non-linearity that's used is what's called relu rectified linear where given a vector you turn all negative entries to zero okay and as i mentioned this is a very simplistic view of deep nets you uh modern deep nets have other features like convolution bias skip connections many other loss functions not least square et cetera but uh the talk for the most part will not need those uh that detailed description so uh yeah you have training data uh pairs of uh vectors x and y sorry there's x one here but it's x and the loss i'm assuming is least square so in order to train this net you try to make this loss small so you change the parameters theta so that this loss becomes small and so the algorithm is great in descent that that's used universally where you move theta always in the direction or opposite to the gradient of the loss the gradient is a direction where the loss increases the fastest so you move in the direction opposite of that make little steps and the famous back propagation algorithm computes this gradient for this network because this network is a composition of various transformations and you use chain rule uh in in back propagation so uh this uh i've only shown this very simple net but as i indicated there's a much bigger paradigm of deep learning now uh with many different architectures and there's fully general paradigm is called differential differentiable computing sorry i shouldn't call it differential differentiable computing uh where you can chain together these units and just do back propagation through the chained unit all right so what is the counter-intuitive thing about all of this so let's view it very simplistically and then it'll be clear so you have these n actions different layers and they put passes through that and the output comes out let's say zero for dog and one for cat you know inputting pictures into this net and these parameters in the various layers are you can think of as knobs kind of like knobs and that radios have in the car um but so and gradient descent is just uh using uh a large training set of label inputs and adjusting the tunable knobs to make the nets output match the label outputs as closely as possible now the complication here compared to the car radio is that the knobs action depends on all the other knobs so if you change one knob and then when you go back and look at the other knob you know its action has changed so that's uh all of these knobs are interrelated and that's why this is a very counterintuitive fact counterintuitive phenomenon that this simple tuning algorithm can work in practice so that's the first mystery that gradient descent uh which i'll call gd quickly makes the training law zero for these very large nets with large data sets and here i illustrate gradient descent so gradient descent is this algorithm where at the t step you make a small step given by a eta which is a step size in the direction of negative of the gradient so on this picture on the left you see an example of what's called the lost landscape you have two parameters these are indicated by the knobs and the loss function is the third coordinate and this contour is giving the value of the loss function for different combinations of the knobs and you start from some random point in this landscape so random setting of the knobs uh and then you do this gradient descent and so you follow along some path some trajectory and end up at some point where you can't move very much anymore and that's where you stop very simple and even naive algorithm and the reason this is uh this is so counterintuitive that this works is that this landscape as you can see in this picture already is non-convex so something more like this rather than convex and so in a non-convex landscape as you can see even by inspection where you start from and what exact path you take really determines where you end up so there's not a unique place you could end up because you're starting from a random starting point so uh it's kind of counterintuitive that this landscape is so well behaved that great understand finds very good solutions now so that's you know about training as as i mentioned at the start there's also the issue of how well that training has worked how good is this model on held out data so that's the second mystery about deepness concerning overfitting so as we recall in just all of statistics if you use a very complicated model uh like something like this instead of the linear model you will overfit you will fit on the training data but not do well on the held out data and this is of course the classic occam's razor that you want to use the simplest possible explanation for data all right so and uh the mystery here in today's deep learning is that the nets are way over parameterized so you may have 50 000 training examples and 50 million parameters in the net so way way more several orders of magnitude more parameters and data points and somehow these highly over parameterized nets outperform smaller models so in other words it's like in the sherlock holmes story you know the mystery is not that the dog barked but that the dog did not bark so the mystery is no overfitting so there are many other mysteries some are later in the talk uh that we'll talk about so let's talk about the hurdles for theory in this field so the hurdle is the main hurdle is that the loss function is currently a black box to mathematics since it depends on complicated training data like dog versus cad english to german et cetera so the black box is so the loss function is known we have the expression for it but as far as mathematics is concerned we don't understand that function because it's talking about you know what makes this picture or this set of pixels a picture of a dog and we don't have any mathematical characterization of that so we might as well think of it as a black box as far as mathematics is concerned but the trouble is that we know that no fruitful theory is possible for black box that is fully general non-convex function you can show that there's no efficient algorithm to find optima let alone optimally generalize so you have to open the black box in order to do any theory now here's another thing that's become very clear to me which is that this optimization viewpoint that everybody uses uh which i showed in the previous slides it's not even clear to me that it's the right language for even understanding the current deep learning so what do i mean by that so by optimization i mean the following that optimization is concerned with minimizing a loss function finding optima and doing it as fast as possible because we are interested in uh in fast algorithms so if you interpret optimization in this in this way you know just that it does this it's not even not even clear it's the right language for understanding deep learning so to illustrate what i mean let me recall the old debate in neuroscience you know does the brain which is just a spiking which is spiking neurons in a vat of chemicals among to of the optimization of an objective and that's still an open question in neuroscience and the suggestion today is that deep net training is also very imperfectly captured by the value of the objective and the reason is that there are multiple minima as i indicated in the picture there are many optima and and certainly if you're throwing local optima there even many more and you're starting from a random point in greater descent and if you do any tweak to the training there are all kinds of tweaks that are known in great understand you're going on a different trajectory with a different solution so therefore uh it's a trajectory properties that determine where you end up in other words whether or not there's generalization so that's sort of missing from this narrow view of what optimization is right the what what trajectory you follow is almost not important in in traditional optimization just what matters is the value and how fast you got there so this is a a phenomenon that many theorists have focused on in recent years and it is a very nice phrase implicit bias of greater descent that came out of the group at tti chicago and so we'll talk more about this all right so the agenda for theory is to open the black box this black box of deep learning as you saw the the paradigm the computational paradigm is just to treat it as a black box you just do great understand on this loss you don't try to understand it but in order to do theory you have to open the black box and so the hope is that you'll gradually analyze more and more complicated deep nets and see if theory can explain these mysteries that i alluded to earlier and uh an example just to give an example uh you know an analysis maybe you understand trajectory of great understand for a three-layer net on some simple data set and just understand you know by looking at the trajectory and understanding how the trajectory evolves this process of reaching zero loss and good generalization on that simple data set even that is currently fairly difficult and the difficulty is that what we're talking about are notoriously hard problems in mathematics like evolution of systems of tens of millions of parameters uh and how they evolve on the gradient descent these are quite close to famous unsolved problems in mathematics and yeah as was alluded to i was at the institute for advancity for the last three years running a program there and yeah i talked to uh to mathematicians and physicists to try to get ideas for how to proceed in this and it was clear that there aren't really techniques out there to deal with this so we have to roll up our sleeves and just create this mathematics so i'll in the rest of the talk i'll give you some vignettes of some results that have been achieved and it will give you a taste of where this field is going so the first vineyard is training of infinitely wide deep nets now infinitely wide i'll describe in a second but as the name suggests it's some kind of infinite limit as you see in physics so like thermodynamic limit in physics or there's a gaussian process we have deep learning etc the point here is because it's the size of the net is going to infinity there are infinitely many solutions and somehow gray anderson picks out an interesting and meaningful solution so what do i mean by an infinite net i'll illustrate with a very simple example so suppose the input is very simple it's 17-dimensional tiny input number of training samples is 339. i took this data set out of this classic uci data set that was very popular a decade ago or two decades ago all right so we want to train fully connected five layer net on it seems a little bit of an overkill but uh all right now i'm going to make it even more complicated and more uh ridiculous i'm going to make the net infinitely wide so what does that mean i'm going to keep the input and output layer fixed so we can still feed the same input into the net and get some the same kind of output a real number but you allow the width of the inner layers to go to infinity so we're doing some kind of a mathematical limit and and studying the behavior as goes to infinity and this infinitely infinite net is scaled with initialized with some suitably scaled gaussian so the expected node value makes sense all right so you look at this and you say well this is too expressive we know that even if you make even two layer nets arbitrarily white they can represent every finite function so the number of zero loss solutions will go to infinity uh plus it's infinite so it's infeasible to train so this is not a realistic net true but turns out you can train it and the answer on this data set the prime tumor data set is that the test accurate is 51.5 remember this multi-class so 51 accuracy is actually reasonable and the interesting thing is that the old champions for small data tasks like random forest and gaussian kernels do a little bit worse so this actually is a pretty decent model for small data examples similarly for c410 you can do you can calculate what the infinite convolutional net is doing all right so what is the motivation for this infinite limit and and what exactly is going on so remember the original thermodynamic limit in physics so you know molecular motion and gas and at every time step uh you know there's finite number of molecules and there's some kind of a variation about a distribution so there's the velocity distribution is changing it's time varying and the insight in the 19th century was that if you go to the infinite limit this distribution of velocities and energies reaches some kind of a limit and you can calculate that limit by calculus so something similar happens with neural nets and what you can show is that the following are equivalent for any finite data set you have an infinitely infinite weight fully connected net or convolutional net there's a theorem for that too trained with greater descent with infinitesimally small learning rate so that's one model the one i just described and the second is kernel l2 regression which is a classic machine learning idea with respect to a new kernel called the neural tangent kernel which is in the title or convolutional neural tangent kernel and in this paper that i mentioned on the first slide we gave an efficient and gpu friendly algorithm for computing this convolutional kernel exactly using dynamic programming and the idea in this theorem and this algorithm is that as i alluded to earlier in the thermal dynamic limit the distribution of values at a layer approaches some fixed distribution and you can compute that by dynamic programming so now this classic kernel regression idea with a new kernel can be used to compute the exact performance of infinite with nets on finite data sets so i just want to unpack the previous slide a little bit so this is a reminder about what kernel linear regression is so um you you have the input x and you're going to lift it to an infinite dimensional space via some mapping fee so phi of x is some vector in an infinite dimensional hilbert space and the various popular kernels gaussian kernel etc and now you you're going to do classification using this infinitely long representation and the kernel trick in machine learning says that you can do machine learning on this infinitely large vector if you can certainly regression you can do regression if you can compute inner product of fee of x1 and phi x2 for any two data points x1 and x2 so that's all you need to do regression with this infinitely large representation and the neural tangent kernel is just some new kernel where in phi of x this hilbert space each coordinate corresponds to a parameter w in the net and remember the number of parameters is going to infinity and the corresponding entry is the partial derivative of the output with respect to the parameter at time t equals zero so to do regression you only need an algorithm to compute this inner product which is what our dynamic programming algorithm does and here's the uh the result i alluded to earlier that on this classic uci data sets we took 90 data sets that have been studied before in this earlier paper and there they found that the world champions were random forests and gaussian kernels and the neural tangent colonel beats those old champions by a little bit all right and you can also do a souped-up version of the tangent kernel for convolutional nets and now for the first time you got a kernel that beats the original hinton paper or matches the original hint and paper on c410 which is a well-known data set in vision now so now we've shown that at least in the infinite limit deep nets become the simpler model but uh now that we realize it people have gone back and looked at generalization for kernels and realized that actually the generalization results even for kernels were not very well understood so that's another interesting open area at this point so i'll uh i'll i can take a couple of questions here before i go to the next vignette any questions in the chat box there is one question in the chat box actually uh very uh relevant uh so it's a technical clarification so what do you mean by exact performance you can compute the output of this infinitely wide net on any on any data point it's infinitely wide but by dynamic programming you can compute the output okay uh no other questions so far okay so in general yeah i'll i can take pauses for a question or two after each vignette i know we've got one um okay what do you mean by entry on the previous slide oh uh so this one you know so and on the left i have the this input and then this kernel representation v of x so this is the coordinate oh i see by the the entry in that coordinate in phi of x that's what i mean the vector with real values and the corresponding real value is this okay while we are at it another question what how do you calculate accuracy for regression okay so um what we're doing here in training is uh l2 loss but at test time we are actually uh looking at classification loss what you want to do yeah the the reason we're using l2 instead of some other losses that the uh this ntk theory is not fully developed for all kinds of losses l2 is the simplest okay one last question and then we proceed how does the statistics of the input affect the trajectory is there something special about the statistics of the input totally i think the input is very important i emphasized this when i said you know the that this function is a black box it depends on the input and that's a black box like what what are the properties of the input so it's definitely very very much dependent on the properties of the input but mathematics doesn't currently have a way of describing how like what are the properties of inputs like what makes a bunch of pixels a picture of a dog right the only good description we have mathematically is a deep net that we've trained okay all right so second vignette solving matrix completion via deep linear nets and the subtext here is that gradient descent is amazing even in very simple models you don't need a very fancy net to see this effect but exactly mathematically formalizing its effect can be tricky so this is a paper implicit regularization deep matrix factorization so matrix completion is the following simple problem that there is an unknown low rank n by n matrix m capital m and entries are revealed in a random subset omega of locations and the goal is to recover this matrix so i forgot to say that this well this became popular about uh 13 years ago uh as a result of this so-called netflix problem where the company was trying to predict how well would a user like a movie and it has some partial so that's this matrix user by user times movies and the company only knows a few entries in this matrix that the user has rated and was once to break the other entries so that's why this problem uh came from and became very popular and uh the algorithm uh that everybody settled on was this so called nucleon minimization so you're finding the matrix with the best squared error so the revealed entries are omega and m is a matrix you're trying to find so you're trying trying to minimize the least square error between the revealed entries and the entries of the matrix you find and then you add this other term in the in the objective which is called the regularizer which is the nuclear norm the sum of singular values of m so you're finding a matrix with small nuclear norm that's what this is saying which fits the revealed entries so there's some kind of a continuous relaxation of low rank and candice and rex shows that this convex relaxation is statistically optimal and it's convex so you can solve it exactly wonderful so 10 more than 10 years later about a decade later people revisit revisited that and this lovely paper gun secretary said okay suppose we forget about this regularizer and convex and everything you know with modern in in the modern era we're just willing to solve non-convex problems anyway so just find m as a product of two matrices so what's called the depth tool linear net so now instead of m you have w2 times w1 so that's your matrix and you're just fitting it to the revealed entries no regularizing so now you just do gradient descent now there are infinitely many solutions right there these product of two matrices there are many possibilities including those that fit the revealed entries because there's no rank constraint at all however they found that empirically grade anderson finds solutions as good as the nuclear non-minimization so there was no need to go to this more complicated relaxation just do great understand and so their conjecture was that in depth two linear nets gradient descent is implicitly minimizing the nuclear norm so that's an example of the implicit bias of greater sense it's doing something interesting mathematically well so in our paper we study we tried to study this conjecture and we first decided to go deeper so like more than two layers so now you have n layers and the matrix you're trying to find is a product of m n matrices w1 through wn and this entire prior matrices you can think of it as a d as a linear net where you're just applying a sequence of linear transformations and uh and and this end-to-end matrix is the product of matrices uh and you're just fitting it to the revealed entries so it's taking this deep learning paradigm to the limit right we have a problem for which we had a good solution that we thought was physically optimal we're going to ignore all that domain knowledge about low rank and nuclear norms and we're just going to trust back propagation simple gradient descent now here's the first good news empirically this solves matrix completion better that is with fewer revealed entries than nuclear non-minimization and there's a mathematical explanation for it now you might be saying wait i said that people had shown that nuclear non-memorization was statistically optimal but it's only optimal asymptotically like you know up to some constant factor and that constant factor is better for this algorithm so you see the like on the left you see depth 2 which is like nucleon minimization depth 3 is in this case a lot better and depth force just slightly better all right so and there's a mathematical analysis we could do you know we could analyze this as what i alluded to as this system of differential equations of evolution of these parameters and we can analyze how this end-to-end matrix is evolving and what we see is that there's some rich get richer phenomenon that the large singular values that somehow get a little bit large tend to grow very fast and that's what gives it the tendency to be low rank because the the few large singular values keep getting bigger and bigger and so the matrix tends to get low rank all right so the interpretation is that the gradient descent is building up the matrix one single vector at a time not all at once and this building stops when the gradient of loss goes to zero so that's the way in which this load that a low rank evolves all right so it turns out there's evidence that they're going to take our conjecture is false and there's been subsequent work that shows actually this falls any questions here well we have a question uh it's a little bit awake how are you calculating uh can we perform simulation study to answer accuracy issues these are simulation studies i do have the plots here but this on synthetic data that's it okay so third vignette uh is exponentially increasing learning rate this works for deep learning so this is a uh something that i found quite surprising paper with student journal e so remember grading descent you're updating the parameter vector in the in the direction of negative of the gradient but the magnitude of the step is the step size eta so that's called the learning rate or the step size and that's plays a key role in optimization and machine learning and there's a big field of papers on this and the standard schedule and deep learning people have ended up with is that you start with some learning rate and decay over time there's some extensive literature and optimization that justifies this although there's been some lately some papers in the last couple of years suggesting that there's something wrong with this for modern deep nets that you can have the learning rates that oscillate and so on like cosine and that's also uh that's okay well so what we do what we show is something very surprising so the first result is just purely empirical it's possible to train today's state-of-the-art deep architectures while growing the learning rate exponentially so at every iteration you're multiplying the learning rate by one plus c for some c bigger than zero now i know all of you know about exponential increase so indeed what this is saying is that with you know because it's the training is going for thousands of iterations and uh remember that an iteration involves only a small batch of inputs so that's why there are a lot of iterations um at the end of the training the learning rate goes to billions or trillions you need full precision arithmetic to even keep track of it so it's massive the learning rate but somehow the training works why did we even try such a crazy experiment we tried it because we had the theory we knew that this would work so we had we have a mathematical proof that nets produced by existing training schedules you know which have something like this this form that i show can also be obtained in function space so not in the same parameter space but in function space meaning the input output behavior via such exponential learning rate schedules so that's a mathematical theorem that you can do training of today's deep nets via these very counterintuitive training schedules and and this happens in all these nets that use batch norm other layer normalization schemes and that's that holds for most state-of-the-art nets today so i won't describe batch norm but it's some normalization idea and which uh turns out has mathematical properties that allow these kinds of exponential learning rates so i'll explain that uh this mathematically what this means exponential learning rate so the general training algorithm for deep learning today is that you have something called momentum which is some kind of memory of past learning rates and you have the l2 regularizer what's called weight decay so this is a regularizer which penalizes the l2 norm of the or the square l2 norm of the parameters and there's a learning rate all right so the informal theorem is that for above nets you know with this kind of training algorithm the falling is equivalent to above you zero the weight decay parameter this this lambda parameter momentum stays unchanged and the learning rate schedule becomes this exponential learning rate where alpha is a nonzero root of this quadratic equation and this non-zero root is well-behaved you can show okay so that's the that's how you uh get exponential learning rates basically you make the weight decay zero all right and the proof uses a trajectory based analysis as i was alluding to earlier that we have to understand trajectories of training it's not just an optimization question uh and the scale invariance created through batch norm okay scale in variance is that uh the net represented by parameter vector theta is the same as a net given by parameter scaled up by a constant c and so that has certain properties about the gradient that the gradient also scales by some constant when you change the parameter vector and then we can give a mathematical proof that the true trajectories i mentioned are actually equivalent by an induction over time all right so that's what i have to say about learning rates any questions i actually have a question about learning grades i've seen a paper uh which argues that well we don't really see the point of using decreased learning rates so we are going to use flat learning rate and they did fine and it looks like with your uh i don't i don't um in which setting i'm not sure that's true flat really small flat or really the big flat uh well i it's probably it's probably very small flat so that we have a paper to explain that phenomena which i haven't i mean it's not out yet uh uh it's not even an archive so i didn't talk about it but uh yeah so there is definitely interesting stuff going on you can do things with constant learning rate as well which is a different paper but but you can't keep it large and constant so if you so it's still true that if you want efficient training you have to start with a large learning rate and go too small as far as i know in all the current schedules okay we have another question is this result true for all type of neural networks or only for matrix multiplication with the identity activation oh this is not this is not linear nets this is state-of-the-art nets as i was saying state-of-the-art yeah yeah state-of-the-art uh you know resonates dense nets etc applies to all of them because they're using some kind of normalization which which produces a scale in variance okay so i'll i'll now get to the last vignette which is how to allow deep learning on your data without revealing your data and this is this paper in icml uh with huang uh yang zu hwang zhao song and kylie okay so preamble okay so uh so obviously this is some kind of a theory question it's reminiscent of cryptography that you may have seen and uh uh cryptography has tried to exactly give you primitives like this right you can do online transactions without revealing your credit card number etc so now we want to do deep learning on without revealing your data and people have been thinking about it but before i do that let me give you the preamble why we got to it because as i said we're trying to understand deep learning and trajectory and so on in landscape so this is very bizarre uh method uh called mix up it's a data augmentation method so if you uh you have a certain training data set uh let's say n data points and data augmentation converts n training points to more training points that's why it's called augmentation and mix-up is a very funny one so the idea is you're going to teach this deep model right which is non-linear and so on to behave linearly on training data so what do i mean by that images think of them as vectors and minus 1 1 to the d you know the coordinates are pixels pixel values rescale to minus one one and labels are one hot vectors in c dimensions where c is a number of classes so if images come from c classes see they're c labels you're going to think of the label as a one hot vector so it's zero everywhere except the one coordinate which is the correct label so mix up is the following you're going to take convex combination of images and also convex combination of labels these label vectors okay and you're going to train the net to produce that mixed label on this mixed image so in pictures you may have the picture of a cat this is from c410 you know these pixelated images you're taking 0.6 times this cat image you're taking the car image 0.4 times the car image and you mix them up as vectors it's vector sum coordinate wise each pixel is added like this and you get this combined picture where you can if you squint you can see both the cat and the car and the label is mixed too so you're going to train the deep net on only these mixed up images now of course no human ever learned to to see and label using mixed up images well but you train the deep net with only these mixed data points and surprisingly enough when you test it at the end on normal images it does better it's only been trained on these mixed data points but it does better on normal images than than the usual deep net so it's very bizarre that somehow training the deep model to behave linearly in this way this counter-intuitive way improves its accuracy so we've been trying to understand this and i don't have theory for it but at some point we realized that we could use this phenomenon in an interesting way in the security framework so the takeaway from this slide is that the training data is malleable like in all of statistics and machine learning you think of data as what's given to you you don't mess with the data the data is the data but this says don't think of data is unchangeable it's malleable you can mix it up and do things operations on the data all right so now we get to the privacy problem so uh a a standard issue a standard example for why you need privacy well deep learning is that there are multiple parties with private data which want to collaboratively train a deep model like hospitals or think of google keyboard you you want to train it using your text and maybe your friend's text but you don't want to reveal your private data to google so uh ideally that's what that's the kind of functionality you would like and so in such settings there's a framework called federated learning where the server shares the current model with the so the server is training the deep net and every step it just shares the current model with the parties so it has a model and it shares the model and the parties update the model they compute the gradient on this model using the data and send the gradient updates back so that's the federated learning with private data so the parties hold on to the data now this is not really secure because the uh the the gradients that the parties are sending back contain information about the input and in fact there are attacks showing that you can recover the inputs from the gradients you may have seen some of those attacks so one approach to building some privacy is the popular differential privacy approach where each party shares model gradients computed using their data but after adding noise to it so this is the differential privacy now the pros of this is that there's provable privacy guarantees and the cons is that there's large accuracy drop due to the added noise approach two full-blown cryptography so the friendship probably is just adding some noise this is full-blown cryptography multi-party computation you can uh give the strongest possible privacy guarantees but the cons is very high computational overhead roughly what happens is that these cryptographic ideas break down all the arithmetic computation during deep learning into some finite field computations and that's just very high overhead it's infeasible for modern deep learning to do it at scale so clearly we need an encryption method for data for this deep learning application which does not interfere with deep learning in contrast to usual cryptography which lifts arithmetic operations to finite fields or lattices so we decided to take inspiration from mix-up so why is mix-up uh some inspiration for hiding information so this is because you uh you can com you can get computationally hard problems out of just mixing up vectors so here's a k vector subset sum problem so you have a set of k n public vectors v 1 through v n if you just pick k of them k is something small like 5 or 10 secret indices out of this and release the sum of the corresponding vectors okay so to pick five random indices and output the sum of those or ten random entities i'll put the sum of those now the exponential time hypothesis and complexity theory says that finding these secret indices for somebody else should require a lot of time depending on the size of the public data set of vectors so even k equals 4 for instance is pretty hard you know if n is large like if n is 100 million which is not so difficult right public images out there on the internet uh this is n is pretty large and so this is a tough problem so here's the insta height idea so we want to do deep learning where we don't want to reveal our data so here's a private data point this cad image we're going to mix it up with images from a public data set which everybody knows instagram or imagenet or something so what this means is that i've taken the cat image with its label this one hot vector i've taken a bunch of public data set images these are these are not labeled these are just images and i mix them up with the private image why public data set so we can have a large data set which nobody needs to like store on their hard drive they can just get it off the web as they need images and uh there's no special preparation no setup as opposed to many cryptographic ideas where you need some kind of a setup like a certain public key infrastructure and so on so the parties can just participate with no setup and the lodge is so that you have more security because you know recovering mixed up information out of mixed up vectors when case when this data set is large becomes hard okay so you mix it up and now you get this mix of the cat and the bird image and the label is still capped because the bird image was unlabeled and then you flip signs randomly okay so there's a one-time private key that's used only for this training image where you flip the sign randomly in this mixed image so it hides it even more so it becomes this kind of vector now this flipping of sine is uh is just a analogous to a private key in private key cryptography the point of this flip is flipping the sign is it's kind of like forgetting the sign so that's all that's going on so it's kind of a bizarre operation this linear operation and then this sign flipping operation which is multiplicative in in a longer talk i'll give a little bit more background on why these are natural from a cryptographic viewpoint but anyway at the end of the day this is the final install method you take k over two training images so like cat and car these images add to them k over to um public images and uh and then mix it up and then flip the signs randomly and now you're going to train only on these these crazy images which look like random pixels and with this labels and somehow at the end this train net can do normal classification and at the same time during training you haven't revealed any of the images so we conjecture that finding extracting information about the training images out of these mixed up images is difficult that's the conjecture and this this is not very secure like you know cryptography that you use for doing online transactions but it scales much better and you can do it with these very large images and large data sets and large neural nets okay so i'll quickly conclude and then take more questions so understanding why and how deep learning works is a new frontier for mathematics attempts to open the black box leads to new insights and new methods for instance exponentially increasing learning rates and insta high and it'll be a fun ride in the mathematics there's no yeah ignore robin was as hilbert said thank you very much thank you sanjeev we have one question already submitted aren't pixels generally unsigned integers so oh you just look at them so that meaning shift them so that minus one one this is just for convenience yeah i mean i could define sign flipping in a different way but if you want to think of it as sign flipping you want to change the coordinate system so that you have plus one minus one between plus and minus one another question is about encryption uh so if you add random public pictures to encrypt the original how do you decrypt it you don't you don't need to decrypt you only train the debt on those encrypted images you never decrypt right so you encrypt it and you send that to the server server will not be able to decrypt it that's what we believe and nobody needs to decrypt it you just train on the encrypted images so you in principle cannot decrypt it nobody else can if you don't know you know which public images are used but nobody else needs to they just train on your encrypted images yeah i see a confusing question it's hard to interpret um another uh okay so we have a question in q a section are are there good generalization error theoretical results for deep players instead of white ones so there are no tight generalization results so understanding rigorously the sample complexity the number of samples you need to train a large net is is still an open problem but uh they're better generalization bounds in the last four or five years people have come up with a sequence of better methods to estimate the number of training samples it's still not giving realistic bounds but it's a lot better orders of magnitude better than before thank you another question is there a loss of accuracy due to the encryption and how large there is a small loss yes i forgot to mention that uh yeah there's a small loss in accuracy and how large did it compare to other methods like today so differential privacy has a huge loss like uh for c410 uh the best accuracy people have been able to get is 75 which is uh which is worse than you know simple classic methods so basically differential privacy if you want provable guarantees is not a viable method for deep learning in that setting yeah this is not often mentioned people use differential privacy a lot but if you want to get probable bounds you have to add so much noise that basically uh the model becomes not so useful okay we have another question um i've read a paper from you you shin yi i've read a paper about explaining and he provides learning by using statistical physics is it possible for explaining more general case so i didn't talk about unsupervised learning in this talk uh there's all kinds of papers i mean explaining is a strong word in any of these you know mostly what people are doing is either you know if you have a rigorous explanation then it'll be a relatively simple model uh and if it's uh and if it's not rigorous then you're using some heuristic from some cisco physics and so on which sort of uh resolve some of the hurdles for you on zots of some kind okay uh we do not have any new question at the moment great but let's see if people come up with something sorry for the background noise looks like somebody's gardener is out next door i i i wonder what is your uh general view on data augmentation besides these privacy issues but like for for improving training or for regularization purposes um yeah so data augmentation is uh is very interesting right especially mix up it suggests that deep nets should behave almost linearly and in fact people have now stronger versions of mix-up where they actually insist on linearity on the intermediate layers which helps even further a little bit so um it's very interesting um there are some efforts to try to understand it but it's not clear why this works and it doesn't feel like that's what animal vision is about but somehow human machine vision is benefits from that there is one one one technical comment that your of convex work files uh size seems to be down yes we are trying to uh find out what's going on i think it may be some kind of a domain naming error or something we're trying to track that down we thank you so should i mute myself well we still technically we still have two minutes uh your session so let's let's uh if the other questions give people a chance yeah especially again that we started a little bit later okay yeah i was trying to finish by 11 10 so that's fine uh well okay we will have a question um how safe is the encryption how hard is it for the server to uncover your data so it's not as secure i think as normal cryptography uh you know which just to decrypt your credit card number will take whatever millions of years so uh it's just not easy for the server to do it you know so you could use public images that uh you know with a very large set of public images so and uh you could even use images from again uh generated models so you could mix with all kinds of things and uh for the server to figure out the original inputs right now seems to require a lot of computation so it would just impose a big cost on the server but it's not completely undoable if you have you know 100 million images it's conceivable that the server could try all possible combinations it's just not very feasible right now okay we are done with question for now okay all right thank you very much thank you 