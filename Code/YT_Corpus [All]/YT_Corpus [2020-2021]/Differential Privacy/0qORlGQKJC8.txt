 all right thank you for joining us for the last session slot of build hope you've had a chance to go to some of the as machine learning sessions or visit us in the in the booth I encourage you to go and try it out at at home run through some of our jupiter notebooks see how the service works then take it to your organization your work your your school start a pilot see how it can solve your your problems when you want to go and use it in production you're gonna run you're gonna you're gonna run into a bunch of people ask asking questions the cloud architects these security teams you don't want them to say no so we're talking about today are some of the things to think think about and how you organize and set up as machine learning to go and run in in it in an enterprise address questions around enterprise security cost and different and different hard Hardware choices my name is Alex Sutton I'm the lead p.m. and the Azure machine learning team focusing on AI infrastructure and training and I'm joined by Ted Ted way from our inference team who will talk about hardware and FPGA in in particular as machine learning provides a great experience for machine learning and AI development at at all levels from custom AI and really getting into the code understanding tends to flow and deep in deep learning to traditional machine learning and automated machine learning and then we help me go and prioritize those models and use it in real time inference in batch inference use it in data bricks for a big big data push models out to I I would he'd I oh T edge or other other devices and operated in an in an automated away but doing that requires thinking about how it fits into your enterprise in infrastructure so we're gonna talk about today is what it means to be an enterprise ready what are some of the concepts in as machine learning how do you organize the workspaces and artifacts like storage how do you think about authentication and authorization data and encryption and networking how do you go and automate a deployment and monitor the service and then how do you think about cost how do you understand what hardware to use when to use CPU versus GPU what types of GPUs does that does that sure have what can what benefit can the FPGA is bring bring to you and then how to think about sort of mundane things like quota very important if you're gonna go in and train at scale how do you go and ask for the right sizes in in a sure this is not a data science talk but it's for the data scientist I want to go and know how to talk to their enterprise architects the cloud security teams the service engineers and DevOps that are go at that we're gonna go and provision a provision azor and get things set set set up or for really for it for it anyone that that needs to wear multiple of these hats we have both a great getting started experience in our machine learning and one that really is and enterprise ready I do want to thank a couple of our partners I've been working with us the last couple of months to get our machine learning you know ready for their use particular KPMG who cares very much around data security protecting the assets of their clients as they build models and do AI inference on on them and then as well as mike microsoft research who is moving all of their interns this summer and all of the research teams to start using Azure machine learning they care a lot about security too but also the productivity of their users how quickly they can deploy and re and reconfigure things and also a cost so we'll start on kind of hot of how you organize work workspaces and what some of these kind of concepts are so kind of the typical process in in machine learning you go and and prepare data clean it up I understand features get I'll get it ready for training go and build the models I'd id8 go and up optimize them the case of deep deep deep learning to do hyper parameter to tuning package up a model and then and and then use it whether it's in real time data or both or batch that's scoring but this is not a not a linear process there's a lot of iterations loops on preparing experimenting deploying getting telemetry back at every stage and then orchestrating things so you may use our new data drift drift features notice that a model in in production doesn't have as good active accuracy the data that I trained on is different from the data that I'm seeing into in a production trigger retraining of the model validated and then a push put push it out doing this in at a production scale really requires a lot of thought on how to organize and architect things kind of start starts with how do you organize the resources in Asia I sure AM L a workspace is the top-level arm resource they think of this as the as the entry point and the container for everything else it really is just a container it's a security boundary it's a quota boundary on how much someone can can spend we then have a number of resources underneath a workspace that we manage experiments and runs the models that you're gonna go and train can a computer targets out that I'll do for training or for or for inference trained models that are stored or that are tracked in the amount model registry I then create docker images that I can use for a deployment we can go and track those deployments and apply them say to a coop to a cuckoo bananas cluster you know this then also needs to map onto onto a sure and so in some of the key concepts is everything starts with a subscription actually starts with with an account or organization which is the credit card or the enterprise the enterprise billing that that an organization is is using and most enterprises I'll have many many sub subscriptions the key thing for us is a subscription is the home for quota quotas the number of VMs or GPUs that that that you can use and we use both compute quota and what we call as your machine learning can compute quota so quota is on a on a subscription in a region for a VM family and region is the next concept our machine learning like most every service in Azure is specific to a region so you said go and give me a workspace in u.s. west or us or us us East we are looking at relaxing some of our our restrictions to say I will do all my management in u.s. East but I want to have a training cluster in u.s. West and us north north north central or Europe or I want to go and have the one spot or on managing all of my deployments but I'll have the web deployments or around the world so we can give you kind of a single place to view and monitor but have activities running in different regions around the world next concept is reach resource group it can be one of the more confusing things in in Azure but everything is rooted in a in a subscription a resource group and then the research and then the research resources a resource group is an easy way to organize projects or things I can say here are all of the workspaces the storage the key ball it's everything that I need for this group it's also a security bug boundary so you can have a resource group say the owner of that resource group can manage all the workspaces there which is separate from storage so you can still connect it workspace to storage but separate the the mid management and then while a resource group is is created in a region the resources can be across multiple regions next thing are the resources virtual machines Azure machine learning workspaces data Lake storage anything else in Azure what you think of as as an azure service when you're in an enterprise you need to connect that you typically connect everything with a ever virtual Network so the resources can talk to each other I can talk back to the corporate network over a VPN / Express route effectively that's the IP address in Azure that's part of your your your company's net a network space the other goal here is often to keep it resources not not exposed out on the on the public Internet then the last thing are roles you know every every subscription resource group and object can have one or more roles you know owner contributor or a or a custom custom role now we'll talk about you know how this applies to a sure or to Azure machine learning the key resources that we that we need to run a workspace is obviously the machine learning service itself that's the work we're workspace we then by default create storage as your key vault a container of registry the first time that you build a image we use application insights from monitoring the real-time web service and as your monitor from monitoring the health getting auditive vents GPU counters and the NDS such if you go and create a workspace and the portal or from the SDK will auto create all of these things if you want full full control over all of the resources you can create them separately with scripts and simply attach them to the workspace when for creating it canna drilling down one a one level into the compute that's used in a in the service the main computer resources that we use our machine learning compute which is the compute cluster used for training and bat scoring is based on the old batch a I service and builds on virtual machine skins this scale sets a lot of customers are using the data science virtual machine as an interactive development environment we're starting to provide that as a managed service first with the notebooks preview that we that we released at a build people use the kubernetes service for the real-time scoring we can integrate with IOT hub for deploying models out as well as data bricks for both data preparation as well as scoring something that a lot of people asked is how do I organize the work spaces I might have tens or hundreds in in my organization how should I think the thing about it so I wanted to share one of the approaches that or actually some several approaches like we in Microsoft Research are thinking and about you know sometimes it's an individual or a team or a project they're trying to get away from an HPC cluster approach where I have hundreds of users sharing a cluster and I have to have very complex policies for scheduling and priority instead give an isolated workshop workspace to a user or a team give them their quota and they can work in their in their own isolated environment a lot of enterprises think about this in terms of a sandbox where researchers and data scientists can do whatever they want download packages from the internet kind of learn and figure out what the models look like then as soon as I started training in the corporate network with corporate data everything has to be locked down and think about I've got a lockdown training environment I can only use certain certain containers every document Ainur has to be scanned with say twit twist lock and I can only pull down public the public packages that have been scanned by my company so you can still provide productivity for the day data scientists but in a more more controlled in the environment then if I'm doing an automated pipeline I may have one workspace for training one for testing and validating models and one in production workspaces can also be used for a security bound of boundaries so I may have one that's that's open for everyone I can share models I can share data I may have other workspaces that are locked down because they're sensitive data cuz to the customer data you have to have a lot more control over who's axe axe accessing it another way of thinking about organizing is is is on is Aurra am i building my workspace for interactive users or for these interactive runs you know automation say through as your dev ops or or ml put pipelines every workspace has to have a storage account to share the output logs the models the other artifacts a lot of our customers are also setting up shared storage office shared data set so say your training image models on image net you don't want to download it every time you want to have you know shared storage accounts that all of the users can use the other reason that we think about organizing into separate workspaces is around quota or budget how many GPUs is this group allowed to spec use how much money are they allowed to spend we've only got sort of course control over quoted now but working on how we provide more of a core hour or $1 dollar spend limit on work spaces good next step is you know once you have your arc rket architecture how do I set it up so it can be an enterprise ready and actually be I'll be used in my organization and the questions this we heard the security teams are gonna ask is what's the authentication authorization and roles what's the data security story and as you as your machine learning what's the what's the network suits security so the first line of authentication is to a a work workspace it's an azure resource it's it's deployed and managed through arms so we use Azure Active Azure Active Directory for the author authentication to the workspace in any actions like creating a workspace some submitting jobs managing a date-date data stores multi-factor authentication is available if enabled in in Azure Active Active Directory we also support using service print principles for automation it's effectively creating a user for code that's going to run and providing that code with a key within a workspace we have Azure mo mo tokens that are that our service did generates and uses on the compute nodes for us to send data back to the back to the service we do this so it's got a very low privilege and scope just just to that work work work space we also support ssh into a compute nodes this is the azure mo can compute cluster so someone who wants to work interactively or a debug we support s ssh this NIEHS tooken feat to be configured right now when setting up a cluster there's an advanced option in the portal to specify a username and password or ssh key we're working to implement the aad of authentication that's available for linux viens and just have to work with how we install with a vm extensions for the new managed notebooks our service we're using Azure Active as your ad authentication there so the user has to be a user of the workspace they were signed to a notebook server and then it's it's simply single single sentence sign-on and then the last piece is when you're creating a real-time interring service in inferencing service effectively a web service or rest in endpoint we automatically generate a key whether you're deployed to IKS or as your container instances this is then then passed as the authentication token in the HTTP header it can also be done through the azure St SDK run run method where we will go and fetch the key from the service and format the request for for you this is used if say you're you're building an online if inferencing service service you've got a front-end your web that's getting say the image that you want to class classify it'll pass to the back-end service using this of authentication key the next level down is around roles and scope every Azure service supports the three standard roles of owner contributor and and reader an owner can do everything a contributor can do everything except add additional users and a reader is is read is read-only there's a little bit of subtlety here for example the owner of a subscription is the only one that can make support requests including quote quote quota requests so we may show an error or we have the ability to say I'm submitting a job if you don't have enough quota will will show and an error message in a recommendation to request quota there's a button to request quota in the portal the user does not have the rights to submit that a request it needs to be done by and by an administrator many services have pre-built roles working on a set of roles now for like a data scientist role a data engineer a DevOps role a workspace node administrator will be shipping those late later in the year what we do have now is a custom role for the two things that are in arm so managing workspaces and compute clusters so we have in our documentation on the security enrolls the template for a date data scientists role so they can run it run experiments submit jobs but they can't create or delete compute or delete the over a work space we're doing this reason this internally at Microsoft for Microsoft Research research where the users can go and submit jobs but they don't want them changing the size of the cluster and going over their quota or spending - too much so they want separation of who manages how many resources we can use from the people that can submit jobs working on much finer grained our back or role-based access good control on all of the resources or all of the objects in that work a workspace who can create data stores whom create images who can read them who can change them so for example if you want the enterprise to say I can only use enterprise approved and scanned images one person can go and create and submit those images and users can can only access them and can't change just here's an example of how we're using these roles and thinking about the deployment in a Microsoft Research so we have the subscription owner that owns the budget for the team where they're going to a deploy they have the notion of all of the projects we're typically doing this at the research lab lab level and it's like five or six labs are around the world so the subscription owner then is going and creating one or more resource groups and assigning a capacity champion for that for one of the reach research teams as the person that will go and manage the workspaces and storage and ant security for that group this is actually more of a operations or IDE IT role than a then every researcher role so they have their computer support team mint managing this we work with them to odd to automate a deployment so when a new team come come' comes in they have a programmer to parametrize script that will create the workspace the storage the security the roles the the quota and they can decide at what level they want to share or separate out groups we then have one person in that group who's the workspace owner because they're the owner on the workspace they're allowed to add other other users they can also then go and manage all of those the data stores the images and everything else in some some teams that's completely open and everyone's on an owner and other teams that they want to lock it down and just have these a specific user of roles so it's really really thinking about is my purpose around experimentation and learning is the purpose a a research group or is a more of a production and environment where I need to separate out who is doing the data data preparation who's training models who validates models and who pushes things out into into production so there is some new lots of thinking about how I organize my my workspace is how I organize these these rolls you can have one large workspace with all the users all the roles and everything else it gets cumbersome to to manage on the other hand you can have tens or or hundreds of work workspaces that could be hard to manage too so it's really thinking about what approach is gonna work best for the problem that that doesn't go to solve kind of the doing the data science parts relatively easy operating it at scale in a production environment is a hard we've got a lot of investments in MML ops and pipelines and everything else there's another part of how I organize in Azure that needs to be thought and thought about too and there's one last piece to identity that that we get asked asked about is we need to have permissions for the azure ml service itself to talk to your storage and your key vault and your your container registry we're often handling those uploads and reads or pulling containers down on the onto the compute nodes we do this through what's called a and manage ID ID ID identity is just another user type in Azure act Active Directory same name as the workspace when the workspace is created we will give ourselves pros permissions to all of those to all of those research resources sometimes this is a a concern to customers once they understand what we're looking at and how they can segment things it's usually fine we are looking at are there opportunities to reduce these roles or or eliminate these roles so these are to some degree design decisions on our side but it has an impact on on customer deployments the most important thing is understanding what we're doing and why what does the dataflow look look look like and we do provide data flow diagrams in our documentation so you can see how we interact with key vault how we interact with container registry storage at etc next part is did date data security or at a very simple level is all my data did at arrest Azure ml does not itself manage any data we rely upon Azure storage storage that we can either create for you which is encrypted by default or you can go and create your own storage accounts use cut use use your own keys lock it down with a fire rules and access rules that that you want and simply give simply give a permission to the workspace to to use it there's also encryption in in container registry within the service itself we store metadata about your about your jobs and and and metrics it's stored in in cosmos DB that's our and implementation that's encrypted also with with key is managed by the service and then the ml compute the compute that compute cluster the backend OS disks are stored encrypted in in storage again with the service service keys because the compute is effectively a path service we don't support custom customer manage keys there does they're effectively stateless stateless vm's and the data is actually stored back in storage all internal traffic between our our roles talking to storage talking to key fault is isn't encrypted and we make it easy to set up a secure endpoint for the online inference with with a with Auto s SSL we do need to store connection strings to dead data stores and some passwords we do that in Azure keep keep the key fault standard standard practice for for for certain services we do our own key rotation and have the ability to support your key key rotation strategies as well and then there's a link to the data data flow diagrams and our documentation this sort of enterprise readiness topic is a a live living thing we're continuously making improvements in the in the service we're taking advantage of new capabilities in Azure and answering questions from customers we sometimes didn't think think about and so we're regularly updating these Doc's and spanning them with additional samples as well then the last piece is networks security this is a much more complex topic because it really depends upon how your organization interacts with Asscher are using Express route or not public peering or probably private peering and kind of how complex of a solution do do you want getting started you're typically gonna have your workspace expose out to the Internet it's it's it's locked down we support the ability to ssh to compute nodes but that's optional we support the notebook VM so you rather need to you have it exposed out on the internet so I can connect to it or put it B or put it behind a virtual network or v-net and connect to it over VPN or Express route the main requirement we we hear from enterprises though is that all of the azure resources are not exposed out on the public internet without security and in many cases the compute nodes don't talk out directly to the internet all the traffic has to be tunneled through the enterprise network so kind of a database we we make sure that we can tunnel all of the traffic like if I'm if I'm fetching Python libraries or data from the internet it's gonna go out through the corporate network through the firewalls through through the scanning and everything else this is not an azure machine learning thing it's it's how you configure the azure networking but we make sure that we can deploy into your into your V net and work well in in that environment right now we do need to talk to the internet front-end for storage and the azure bats that's a certain service which is doing the underlying VM resource management and scheduling scheduling for for us and the bats service needs an inbound port into the into the v-net this can't be locked down with a network security role or NSG to say to be only traffic from those as your services in that region we publish a list of those IP addresses and are moving to as a new feature called service tags where you can simply say allow inbound traffic from batch service on this port and not have to worry about the lower-level configuration what we're looking to is move towards what's called service and endpoints which tunnels all of the traffic over the bet over the azure in internal network so I can go from your your V net to a storage and have no traffic go out over the over the Internet there's a little bit of complexity here and how we're using the various services are working with the azure network a team on how we can onboard to other service end and endpoints and potentially not have to use some service tags and the more command come the complex rules then the finally piece is how can I make sure that the enterprise people you know on their laptops or desktops over a VPN can go and connect to the work workspace and mid jobs work with the notes notebooks trigger a pipeline and then as necessary those machines can can can can talk out this is some something that if you're in a large organization the Microsoft CSA teams can work with you on it does require expertise the network engineering teams tend to see a picture like this and our documentation and then it's it's pretty straightforward to figure out then the last piece is how do I go and deploy and up up up operate this so really what you what what what did people want is consistency in in how I'm I'm deploying figure out the recipe wants and they go stamp it out for different projects different teams you often want to have consistent naming so if I'm gonna have storage and key vault and everything else and they're not and they're not stored go and give it a go and give it a can consistent name what well we have a great getting started experience through the portal and SDK every enterprise is not going to want to use our auto are to create they're going to want to make sure that they go and create they storage the key vault the container registry and and monitor for their for their corporate standards so it's locked down and secure the way that you a need and then and then and then connected to the workspace that's part of the sod automation you you can also configure roles us assign users request quota and and create clusters the quota requests are manual now or manual now through the portal there will be a CLI support for that coming over the summer then the way to think about this is infrastructure as code all of this that I've described can be done through arms arm scripts as a resource manager the CLI or our SDK waited great way to get started is go and create a workspace in the portal there's an option to view the JSON file that's the arm template for deploying this you can then go and parameterize it look at it and create your and create your own deployment the other approach is to use a tool like tariff farm or ansible there's a number of services and software that know how to deploy and manage to to a sure we have one document that explains what the azure resource the arm template is how to customize it we're also gonna looking in the next couple of months of expanding our documentation here and providing a sample deployment template that looks like some of these more complex topologies that I described to help help help you get started and the last piece is month monitoring we're starting to push all of our telemetry and metrics to Azure monitor things like restore its status state changes when is a job start or stop or fail does the compute node in in or out on the access or audit events on the various resources and roles things like GPU counters these will go in our monitor first then we'll build some pre-built our reports start to pull things like the GPU counter into the portal and into our tools this is kind of the the standard as your way now of doing login analytics monitoring and reporting we also do use app insights for scoring quest for the RHIB real-time inference service and we have a higher level activity view in the workspace as targeted more at the data scientist using it then the operations team or the enterprise team that meet that needs much more finally to find that fine-grained logs then the last piece is how do I manage cost performance and think about the different Hardware choices for the workspace I'll hand it over to Ted thanks Alex and good mor good afternoon everybody for the benefits of the people who are watching this recording I can't believe over 500 people showed up at the last session on the last day of build but well be talking about managing cost and performance in the context of hardware acceleration of AI models so as you think about the models that you need to train and the models that you need to run what are the options that are available to that I'm a program manager on the inferencing site and so what I care about is taking your machine learning model your AI model making it run super fast in a very cost effective way but taking a step back because I think about the AI space and think about the AI platform these are the axes in which we want to support you and the first one is really around what kind of AI are you going to run are you going to run custom AI I'm started pre built a are using our cognitive services maybe you've seen some examples of that maybe you saw at the booth you know you take a picture and it'll identify your age and your gender and your emotion based on that or you can do object classification or even this real-time captioning that you see here is based off of our pre-built AI models all the way down to custom models so we'll focus on custom models and the tools that you have for custom models so what kind of AI do you want to run where do you want to run the AI do you want to run this in the cloud do you want to run this on the heavy edge which is basically a very powerful server a server grade class hardware or PCs and also the light edge which are essentially you're intelligent cameras and so what kind of AI do you want to run where do you want our water and we run it and then how do you want to run the AI do you want to run it on CPUs do you want to run it on GPUs do you want to run it on FPGAs and this is what we'll cover today and and we also have the option of running it on a6 such as the Qualcomm chips so as we think about cost management some of the things that we offer to help you around that is really just the starting with auto scaling of compute clusters so you start with some number of nodes and you have the capability of scaling based on that workload we also have low priority compute so if you might have a training job maybe it's a big batch workload or an async job we have surplus capacity in which then you could use in a very cost-effective way there preemptable so you don't want to use that for anything in which you won't be able to have that flexibility but but that's available to you we also have reserved instances and promo pricing so this could save you as much as seventy two percent over pay-as-you-go so a lot of competitive pricing when it comes to GPU instances and then in the resource page on the usage and quotas view under your Asha machine learning workspace you also have the ability to look at your resource usage and cost reporting and we're building up that view to to help you to help you there and then when we talk about the types of hardware that's available in the data center so we have CPUs CPUs are very flexible they run pretty much anything and so we have a lot of general general purpose CPU machines for you to be able to do your general-purpose machine learning on we have GPUs for you to train and to deploy and run your deep neural networks and we also have FPGAs which are specialized hardware that will help you accelerate these ai models and we're very happy to be announcing that we are making Hardware celebrated models on FPGA is generally available and that means you can train your deep neural network and accelerate it on an FPGA so we'll cover all of these in turn so starting with GPUs in the azure family of GPUs we have basically the NC family which is a general-purpose and this goes for anywhere from twelve to sixteen gigabytes of memory for the GPU and and so these have the InfiniBand networking for for that second network and that gives you some really good capabilities when you're doing when you're using these general-purpose GPUs we also have the nd family which is specifically for deep learning so the nd v1 which gives you 24 gigabytes of memory and then also the ND v 2 VMs in which the box has eight GPUs all connected using and v-league and then that way they're able to communicate among each other very very fast and so these are just the type of GPUs that are available this summer we'll also start rolling out nickel 2 clustering and so this this gives you the ability to scale out your on your distributed training jobs really really easily and this way using the SRO V this this extends that InfiniBand and that way you can run nickel 2 over over infinity band and so azure is the only cloud GPU provider was with this InfiniBand based dedicated back-end Network and so really wanting you to scale out that that training just a note on quotas so quotas are limited by the number of course and basically every single VM family per region for a subscription has some default quota and if you want to request more you can just do that through the portal and request more quota and just a note that there's a the subscription quota for virtual machines and there's a separate Azure ml4 for your azure ml compute and so just to give you a sample here let's say you have a 48 course for the NC v3 and so these are just some possible configurations of clusters each GPU has has has with it 6 V CPUs and then that way you could configure your nodes based on the different types of GPU machines in this way so so those are the the details about GPUs so let's now talk about FPGAs and FPGA is our field programmable gate arrays and their reprogrammable silicon so you can think of these as a reconfigurable chips and they have a lot of logic gates on it in which you can reconfigure and say okay if I get this specific input I will see I will get this output on this on this logic block and then I have a ton of those that I can reconfigure it directly over software and I can reconfigure the connections among all of those logic blocks and so the nice thing about them is that now I have this piece of hardware it's put in the data center I can put it on into my manufacturing plan I can put it into my retail store I can put it into my oil refinery and I don't have to change that Hardware anymore so I can deploy my AI model directly onto this chip and run on the chip reconfigure that chip six months later if I have a more optimized way of running something I can then reconfigure that ship with the latest circuitry so now the model is running directly on the hardware which makes it super fast and it's also reconfigurable the FPGA uses what's called line rate processing so typically in a CPU what the CPU is doing it's reading data from memory it's processing the data giving you that result and then reading data again from memory and then and then doing that processing an FPGA what's essentially happening is that the data goes in it's flowing through all these different logic blocks and the output goes to the next input and it's flowing through this chip and so with this line rate processing it's really effective and be able to process data super fast in that sense if you do a beam query today we actually put a lot of our queries directly on FPGAs some FPGA models are very large and they span multiple FPGAs and the being data centers in Azure if you've ever provision a VM and selected accelerated not working on that data that traffic that network traffic is going through FPGAs and so we're using a ton of FPGAs today and we're also making them available to you to be able to run your AI models on and so just to kind of summarize if I think about the difference between a GPU and an FPGA I like to think of GPUs as kind of your favorites car dealership and let's say it might have 10 repair bass there's a repair shop in this car dealership and if you have 10 cars and each of those cars needs to get its oil change and its brakes replaced so what's happening is these 10 cars drive into these 10 repair base there's 10 technicians and they're changing the oil on all of the 10 cars and then they're change changing out the brakes on all of those 10 cars and then those 10 cars leave the the repair shop and so that's essentially what a GPU does GPU takes in a bunch of data processes in parallel and then that result leaves a GPU so you have high batch workloads if you're trying to process if you have a million images to process and you can send 256 images at a time to a GPU that's where a GPU will be really really effective GPUs are also very effective when you're doing deep neural network training because of the ability to be able to do all of that parallel processing one of the things to consider about the GPU is that let's say only one car goes into that repair shop so one technicians working on that one car nine of those repair base are going idle and that's essentially what's happening if you only send one piece of data to a GPU so you have a lot of this processing power that's going idle if you're not utilizing it fully but where it works where it works really effectively is if you have a ton of data that you want to process at the same time so GPUs like that repair shop so if GPUs like that then I would think of the FPGA is kind of like a pit stop so you think of going into that pit stop the entire pit crew is dedicated to working on that car and then that car leaves and so that's basically what's happening from on an FPGA perspective you have an image that's coming in it goes into the FPGA it's being processed and then it leaves so some of the use cases for something like this would be in things like manufacturing defect analysis so let's say you're you have an assembly line and products are moving down the assembly line you might have an AO I camera that's taking pictures of the products these images are going to an AI model and you're trying to identify and predict whether there's a manufacturing defect by looking at that picture and so basically you can process it really quickly get that resolved quickly determine whether you should go down to the next assembly line step or rework or or do anything else so that it doesn't move to the next assembly line stuff if you've seen if you saw Scott Guthrie's talk earlier this week then you saw what Kruger was doing with identifying what's out of stock and so the idea here is you might have some cameras it's looking at a retail shelf if you take an item off the shelf it's able to identify that there was a void that there's a there's a basically no product in that area and then it can identify that send you an alert basically saying restock that shelf so those models were running on FPGAs on a data box edge device so basically these are this is the type of hardware that's available to you in Asscher depending on your need so we have the suite of hardware where you where you could utilize basically to get the best price in performance so that's the from a hardware side now from a model side we also have the onyx where the open your own network exchange format and the runtime so I like to think of onyx as being the PDF of deep learning so you think about the types of frameworks out there there's a ton of frameworks you walk down the street you bump into a data scientist and you you know you might have a problem for them and you say okay you know take a look at this picture and identify that manufacturing defect most likely that that data scientist would build something using tensor flow right nowadays more and more people are using PI torch but the models that they train based on those frameworks are not interoperable and so what we need is essentially that PDF so now with this onyx format open your own network exchange format you can convert this model into the onyx format or some of these frameworks have native support to be to to to become onyx models represented and so now I have this onyx model and then I also have the onyx runtime where I can run this onyx model and so the onyx runtime you can use it in Azure machine learning you can use it on Linux or Ubuntu VMs and also Windows VMs or other types of devices and the nice thing about this is now we decouple the training framework from the execution framework so from my perspective my job is to help you run your AI run faster and if you build a model using tensorflow then I need to worry about how to make that tensorflow model run fast if you build a model using PI torch I need to worry about making that PI torch model run fast and I'm always going to be chasing you and I'm always going to be have to follow and unsee which how do I support all the different frameworks out there so now if I can decouple that if I can convert all of your models into onyx then I just care about making onyx models run super super fast and so that's the approach that we're taking so that whatever framework you're using convert that into onyx we run onyx super fast and so tensor RT supports onyx for GPU based workloads were where we're running onyx on the FPGA and in that way we can decouple the training framework from the from the from the runtime so we talked about the hardware we talked about some of the way some of the work that we're doing in terms of the AI models and now the next thing I want to talk about is deploying these models and where you want to run your your AI and so from the left basically you have your MCU based on small IOT devices these are just basically some of them can I mean they can run these custom AI models in the onyx format but as you start getting to some more powerful devices whether it's gonna be the edge devices as devices again could just be pcs running the IOT edge runtime could be these more powerful server type devices it could be the edge cloud Azure stack basically many azure cluster of machines or it just could be asher itself or the cloud so from Azure machine learning you can train your model it's containerized in a docker container and then you can deploy it using IOT edge to these various edge devices and so again with ascots demo you saw that identifying the voids on the shelves that was running on data a box edge device running in the store itself essentially so now your cameras are taking pictures they're sending those pictures directly to the edge device and then that edge device is processing the data so you don't have to send that data to the cloud to be able to analyze the nice thing about this is you can be anywhere along the spectrum so you might have a camera that's sending a picture to a to an edge device like that data box edge and then you can decide what image you want to finally send to the cloud you there's a lot of very powerful processing or you might want to process data directly on the camera itself the camera itself may not be able to run very powerful models but it may be able to do things like detect whether there's a person in the model or whether there's an object in this picture maybe you're looking for pic objects where maybe you're looking for a truck if I I detect a truck or a car in the picture then I send off to my server my server can then do the more powerful processing I can then send that to the cloud if I wanted to aggregate data from across the world for example so now you can put AI into these various places all along and so that's now and that's how we're covering the types of AI that you can run and where you want to run the AI and how you want to run the AI so let me just run through a quick demo real fast and just give you an idea of some of the things that that we've been working on and and you can see just how how this would play out in an enterprise so let me switch over to my machine here so what you're seeing here is a jupiter notebook and the main thing i want to call out is that as a as a data scientist or the data scientists in your enterprise they're using Python and tensorflow so there's not anything new that they have to learn they're just in an environment that they're already familiar with they're using a language they're using a framework that they're familiar with what we're going to do is train a model that will detect what's in an image and so first we have our ashram machine learning workspace on the things that Alex had talked about and and so what we're going to do is do some pre-processing of the image and then put in a feature Iser a deep neural network and then and then put a classifier on it so I'm just blazing through this you know as Alex mentioned this is not a data science talk but again the main point here is that this is going to be think that a data scientist would be able to do very easily right here we're registering that model and again as Alex mentioned what we're seeing a lot in the enterprise is that the rigors of managing the data science workflow is not as far along as the software development workflow so we're seeing data scientists create models they're throwing them over the edge to DevOps and DevOps is running them in production somebody takes a look at the result of the machine learning model doesn't believe the result or doesn't really believe that the model is very accurate and starts asking who created this model I don't know where's the source code for this model I don't know you know where was a training data used to train this model mmm right that's basically what we're seeing a lot and so that's why there's been such a big push for ML ops that you've seen here I build and things like the model management service where you could register the model that you that you that you that you build and so let me get that and so now we're going to go down to container rising this image and then and then we can do the actual deployment so we're going to deploy this model into an IKS cluster on an FP JVM and again the main thing just to call out here I'm just still using Python I don't need to know anything about FPGAs I don't need to know anything about the very log or VHDL what's going to happen here is this is just going to go out and then I'm gonna create that IKS cluster and then I'm going to deploy it from my docker image and the nice thing about this docker image is that the same image that's this that's deploy to the AKS cluster can also be deployed to that data box edge device so data box edge device every single one of them ships with an FPGA card in it and so you can have this image in your Azure container registry deploy to your cluster or deploy to the edge so now you can run this AI model also on the edge so this is using a ERP C API and then in this case I have I have a model that's recognizing I'm sending it a picture of a Snow Leopard and and what's going to happen is I'm going to see a result so I'm just going to click run here again you can see here that when I run you can see that the result is that it's predicting a Snow Leopard with pretty high confidence and so that's just how easy it is to be able to then train this model containerize it and deploy it where now I'm able to send pictures to it and is able to predict what's in that what's in that image one of the other things that we're doing is also working with Fermilab and I don't know how many of you went to Eric Boyd's talk earlier this week Eric boys talk monday monday afternoon okay so there we were talking about just some of the things that are being done with Fermilab and actually let me just roll a quick video to give you an idea of of what they're doing to when it comes to particle physics so let me so here hadn't gone into computer science I probably would have gone into particle physics but I'm not sure I was bright enough for it and so worried sorry category Sony bruh [Applause] it's a little-known fact if I hadn't gone into computer science I probably would have gone into particle physics but I'm not sure I was bright enough for it but we've been working with Fermilab and so let's see a video of some of the things that they've been doing is they try and push the envelopes of science and how are your machine learning is helping them [Music] what is the origin of our universe researchers at the Large Hadron Collider the world's largest particle accelerator are trying to understand why anything in our universe every bit of matter exists at all the LHC smashes protons together as they travel a 17 mile loop at close to the speed of light producing more than 500 terabytes of data every second researchers then need to filter that raw data in real-time to isolate the most interesting events scientists at Fermi National Accelerator Laboratory serve MIT the University of Washington and other collaborators working together with Microsoft have prototype their data analysis problem on Azure machine learning and demonstrated significant gains in speed with the ability to accelerate the building and training of sophisticated models Azure machine learning has the scope to handle the LHC's zettabyte sized data challenge soon it may help researchers identify the handful of collisions among millions that might give insight into the moments after the Big Bang and so that's really some of the things that we're doing when you have these AI models and you want to be able to run them superfast and so one of the models that we took from Fermilab is really taking data from their Large Hadron Collider and determining whether they could detect these top quarks in there and and so let's see what's happening here is I have that model I deployed it on to a CPU and you can see that I'm getting about eight images per second and so going through the same process that I just showed you using that notebook deploying that model and running it on an FPGA you see you can see that I'm getting about close to 500 and sometimes over 500 images per second and so this is really what the benefits are when you have accelerated Hardware that is able to help you run that AI faster so now there's this there's this model on a typical CPU when you can own process about eight images per second versus 500 images per second in a very cost competitive and price competitive way now you can process a lot more data you can process a lot more data you can get a lot more insights that will lead to better models that will then lead to more value out of the data that you have and so this is a lot of promise that Fermilab is seeing as they're getting that data and trying to try to detect just some of these subatomic particles so just to summarize some of the things that that we just saw it's really just ash machine learning with a model management service when you train that model and register that model it you have this container image that you can store in Azure container registry whether it's going to be a model that can run on a CPU a GPU or an FPGA this can then be deployed on the right VM in IKS and in this case of the FPGA we have a gr PC API for CPUs and GPUs we have a REST API in which you can access that model and this would be our cloud story where now you have a ladies cluster where you can scale out an inferencing perspective we also have the ability to deploy to the IOT edge so using IOT hub you can connect that to an edge device in this case this is a data box edge device but for FP J's but for CPUs and GPUs any Linux are a Windows device that can run IOT edge runtime is you can you can support that from an IOT edge perspective the runtime is another docker container running on this edge device you would create a deployment manifest and IOT hub this deployment manifest is a JSON document it gets pushed down to the runtime the runtime says I need to pull this container from this registry instantiates that container and now I have my model running on this edge device so in the case of what Kroeger was doing what you saw there basically there's you might have some client code it's taking in the picture from the camera it's sending it to the G RPC API and now in the Azure machine learning container it's running that model on in that container on the FPGA and then you can send the results back out through the through they runtime back to the cloud or you can send it to the next next part in the pipeline and that's really what's happening on this edge device so so this is just the architectural view from from a deployment to you the edge perspective and so just bringing it all back together data science and AI is just that a really really exciting time we're seeing a lot of maturing of the data estate and now enterprises are doing a lot more machine learning and AI but it's still super critical to do it in an enterprise way that would be safe and secure but also cost effective and so we talked about organizing the workspaces in a way that makes the most sense making things enterprise ready deploying operationalizing it and also managing your costs when you do your training and when you do your inferencing so with that like to thank you very much for your time you can train and deploy models using Azure and Azure machine learning we have a lot of documentation but most importantly we'd love to get your feedback we'd love to just learn about the types of proof of concepts that you're doing the types of problems that you're trying to solve and we're always and we're here to make that better so I'd like to thank you very much for your time we're more than happy to take any questions thank you [Applause] 