 typically the goal of machine learning is to generalize about a set of data however sometimes it's possible to recover information about specific training examples from a train model how can we provide more privacy and ensure our models captured general patterns rather than specific examples stay tuned to find out welcome to AI adventures where we explore the art science and tools of machine learning my name is Yu Feng Guo and on this episode we're going to take a look at tensorflow privacy a new tool aimed at making and easy to train models with privacy last year Google published a set of responsible AI practices focused around fairness interpretability privacy and security one of the recommended practices in the privacy section States appropriately safeguard the privacy of m/l models and recommends that we train ml models using techniques that establish mathematical guarantees for privacy this is all well and good to recommend but it does leave you wondering how exactly do I do that when you train a machine learning model the goal should be to learn these general patterns not facts about specific training examples otherwise the model ends up memorizing a set of specific facts which may not generalize well to previously unseen data and may lead to possible exposure of the underlying data that was memorized this is especially important when that data set corresponds to user actions or user attributes of course it's not enough to say that your model probably doesn't memorize specific users data what we want is to use techniques that offer strong mathematical guarantees that models do not learn or remember any details about any specific user tensorflow privacy does just this all without needing any expertise in privacy or understanding its underlying mathematics so what's happening under the hood in order to ensure that no single example can impact that final train model tensorflow privacy introduces some modifications to the way that gradients are calculated during the training process gradients are a way of expressing which direction and how far a model should update its internal state represented by weights and biases or put another way of large matrix of numbers and typically during a single training step we use a batch of data just some subset to determine the gradient and then apply it to update the value of the weights so in other words we look at some bit of the data figure out based on that which direction and how far to shift our model and then we go there tensorflow privacy makes a few small but key tweaks to how that gradient is computed if a single example is providing an outsize effect on that model's gradient it may cause it to stick out so to speak and thus potentially be exposed in the train model one example of how tensorflow privacy addresses this is by averaging together multiple mini gradient updates on a subset of the full batch so each mini gradients value is clipped limiting its range of possible values and so that restricts their individual impact these mini gradients are then averaged together and finally we add some gaussian random noise on top of this average so by doing this we're ensured that no individual example will be memorized by the model in practice you don't need to modify your model architecture or training procedures to take advantage of tests or flow privacy instead to train models use a different optimizer provided by the tensor flow privacy library and we'll fill in some values to customize the optimizers clipping noise level and number of mini batches to average together yes we're gonna introduce some more hyper parameters for you to tune but hey it's better than than rewiring your entire model and it's better than exposing your users data now let's take a look at an example of what tensor flow privacy can achieve in their blog posts announcing the launch the team highlighted an interesting situation with a language learning model which is supposed to identify English sentences that look like they come from financial news with the two models side-by-side one trained using tensor flow privacy and one without there are many sentences where both models agreed on say sentences that look like financial news such as South Korea and Japan continue to be profitable but then there were the ones which the models disagreed on these were notable because while the data set had marked these sentences as financial news they definitely don't look like they belong in financial news what's interesting here is that not only do these anomalous sentences account for most of the differences in accuracy between these two models but this really illustrates the idea that going by metrics purely it's not always the right decision it also suggests that perhaps we can utilize tensorflow privacy and similar tools to identify aspects of a data set which perhaps shouldn't even be there in the first place tensorflow privacy turns the science and math behind differential privacy into a tool that you can use and while the tuning of hyper parameters does remain a bit of an art the team has offered up some reasonable initial values to begin your exploration for more details and examples head on over to the blog post I've linked below in the description thanks for watching this episode of cloud AI adventures and if you enjoyed it hit that like button and be sure to subscribe to get all the latest episodes right when they come out for now check out tensorflow privacy to make machine learning safer for your users [Music] 