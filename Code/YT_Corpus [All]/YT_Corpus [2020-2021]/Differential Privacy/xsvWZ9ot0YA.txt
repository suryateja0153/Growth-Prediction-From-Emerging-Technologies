 All right, everybody. Welcome back. So in the next 45 minutes, we'll have a fireside chat with Marshal Hibear as retwick put it the one and only Marshall Hibear. So Marshal is a professor of robotics and Dean of school of computer science at Carnegie Mellon. His research interests include computer vision and robotics, especially recognition and images. And video data, model building and object recognition from 3D data and perception for mobile robots and for intelligent vehicles. His group has developed approaches for object recognition and scene analysis in images, 3D point clouds. And Video sequences in the area of machine perception for robotics. He scooped and he has developed techniques for people detection, tracking and prediction. And for understanding the environments of ground vehicles from sensor data. He currently also serves as editor in chief in an international journal of computer vision. So, now he will give us a first, give us an overview of some challenges. And, challenging projects at school of computer science that deal with complex, data. And then we will open up the floor to have audience ask some questions and we'll have a informal conversation. So Marshall.  Thank you so much. Welcome, everybody. Yes, so I thought I would share with you a couple of challenges. That we are exploring in using and we re-using data. And I'll talk about, three areas that I think are interesting. Those are not the only three areas but I thought I'd point out those three areas just to get started with our conversation. In illustrate all three areas with research that we're doing here In the School of computer science. So the first area has to do with the nature of one particular type of data that is becoming increasingly important. And requires kind of new tools and new way of thinking about the data. And that's what I call, I have to find maybe a better name for it. Let's call it a human generated data. So those are things that involve data from a human evaluations, ratings, report, surveys, things like that. And the problem with that data is that it is typically very noisy by definition because we people are very noisy. And these organizing our thinking on certain it's incomplete. It's very biased because we all are biased in a variety of ways. So that makes it very challenging to deal with this data. And those are a few examples of the things you cannot satisfaction surveys various types of evaluation, various types of human ratings. So for example, Amazon Mechanical Turk is one way to collect that kind of data on a very large scale. And in fact, if you look at all the potential application, it goes across the board in all the application. You can think of, you including Healthcare various product trading even in things that do affect decision. That decisions that could have far-reaching consequences in terms of bias and equity, like your admissions and hiring. And of course, various datas from crowdsourcing. So again, this is a different type of data than what you think typically about when you think about laboratory data or scientific data. And the problem, again, is that to be able to really handle the data. One needs to have a deep understanding Of human behavior, psychology, social sciences, et cetera. This is, by the way, those are examples from the work of Neha Shah, in the machine learning department. Who specializes in this in this aspect of the research. So this is a new strategy of example here, and again, this is on one particular application. Which is probably important for many people here on this call, which is peer reviews of papers, right? And if you look at the data from peer reviewing of paper, by definition, it's going to be extremely subjective. So you have to now normalized, processing of that data based on the model of that subjectivity. It's biased people, and we know that, Raiders have different biases based on the origin of the papers and so forth. So forth of the topic and so forth. Miss calibration and this is, of course, the well known issue with human raters that we all have a different internal scale. How to calibrate those scales is another another issue. And finally noisy. This number of reviewers may not be qualified, there's noise in the data and so forth. So those are the kinds of things that are not as or not as directly characterized the ball as in other types of scientific data. That require again, a deeper understanding of human behavior, human thinking of some cyber aspects and so forth, so on, so forth. Here are some of the things that need to be done there is to new algorithms for fairness. Perhaps design those new algorithms the way that rigorous mathematical guarantees can be derived which is Very difficult. Something that is possible for other kinds of data. We connect some strong statistical guarantees. Some strong tests of fairness of data for example, which is very difficult in most cases. As I mentioned, reaching out very far from what we think typically as data science into psychology and economics. And social science, et cetera, to be able to deal with that data. And finally, deployment at scale. The ad scale being of course, the issue in doing this. So, that will just a quick mention of that that first kind of challenge. How to deal with that, that kind of human generated data, human generated evaluation data at scale and with formal tools. So that's just one aspect. Of course, another aspect when we talk about data is that we want to be able to process the data or use the data. Where that guarantee privacy and security. And when I mean privacy, I don't mean just privacy of, say, human subjects, which is, of course, an obvious issue. But also privacy to protect from perhaps how the data was acquired or some other knowledge about the data that needs to be protected. This is particularly important in we using the data. Because we use of the data is often prevented by various aspects of the data that need to be preserved. Preserved and remain private. So how to use the data while retaining privacy of some aspects of the data is another, of course another major issue. So [COUGH] there's a few ways that this can be done. You one could imagine outsourcing the sorry, outsourcing the processing of the data to the cloud or one could imagine instead of that having collaborative computation meaning, distributing that computation that use of the data to different agent for different types of privacy. So there is two major techniques that one can use for this one is what is called enclaves which is basically creating basically a box that is secure within which that processing can take place. A second approach is to say, well, I'm not going to have that box that secure, but I'm going to have the way the data is distributed and shared. To be secure basically using a uncoded protocol cryptographic protocol to share that to share the data. And by the way, those examples are from the work of wenting Zions group in the School of computer science. So this is just an illustration of what we mean by on enclave, which is basically an environment that has a self contained set of tools to do the data processing, so that the data can be isolated in that enclave. We can develop code within that enclave. In one code on the data in a secure fashion meaning completely isolated from the outside. So some of the research issues here is to be able to support a wide enough range of functionalities so that we can do a rich enough processing on the data and also to protect against external access doing this. The other option, of course, is to due to the cryptographic sharing of data, this is again an example from Wenting Zheng. This is a system called Helen which basically develop a specialized protocol to be able to share data securely across different agent. Now, the main issue here is to be able to do this within a reasonable amount of time. We know very well how to ENCODE data securely and to share data security. The problem is to be able to do this, in a way that is efficient from a computational standpoint that that can scale to large number of samples. So that's a key research issue that's being addressed here. And again, the reason for looking at those reasons research issue is to be able to use data in a variety of modes from completely open to having aspects of the data that are kept private throughout the entire computation on the data. The latter being again, particularly important for reuse aspects where some of the aspects of the data must be must be protected. So, moving a little bit further on this idea of distributing the computation, we can look at ideas in using data in a completely distributed way. So typical, a centralized view of machine learning, for example, would say well send all the data to a centralized location, run some running algorithm, in this central location and output, produce the output, let's call it W. An alternate view of that would say, well I don't want to share that data, I don't want to, because some of that data needs to remain private. So I'm going to do some computation locally and that's illustrated here by those nodes in that graph. And I'm going to somehow gather the output of that computation to generate my final output. Again, important for any use of data when we want to keep some aspects of that data private. There are many examples of this you can think of medical data for example, that needs to remain private, but that you can learn intermediate result that can be a transmitted computation at the edge on personal devices, phones, watches and so forth for home devices. So the idea here is to be able to address this issue of privacy by doing a massively distributed processing of the data into then connect those nodes together in a way that guarantees that privacy, okay? The examples here, by the way, are the examples from Ameet Talwalkar and Virginia Smith from the Machine Learning Department. So, how far can we go in those ideas and still maintain privacy? So this is a graph again from a meet on Virginia's work that show for particular tasks. It's not terribly important where the task is it's a classification task, but it basically shows the accuracy as a function of the number of samples in a learning task. This graph is with non private learning, meaning no attention is paid to keeping any data separate or private. Now state of the art, which is standard differential, differentially private learning is this bottom curve. So we basically as soon as we try to enforce some kind of privacy in a standard setup, we see a massive drop in inaccuracy, okay, in this case about a factor of almost four, in the accuracy. And now if you do the kind of approaches that they're suggesting you can go back to something that is close to the non private learning approach, okay? So this is basically a long winded explanation to say that there's a lot of opportunities. In this research in developing new ways of thinking about how to process the data, how to protect privacy to much higher degree than was possible before and again by privacy does not mean. Just privacy of subjects but privacy of the various facets of the data issue if you will. Okay, and the third thing and that's the last one. Don't worry, I'm not going to talk very long. The last one is something that we call a versioning, I have to learn how to pronounce that word versioning, and the idea is this. If you have. In many application, you have data that that evolves over time, in fact, in most application actually. And so for example, in autonomous navigation, you're going to typical data. You're going to recall the stream of images of distances and velocity, and in many situation where you recall data over time. The initial measurements are only provisional, and they may be later replaced and we find with more accurate version. A process that can repeat multiple times so the data can actually be updated, the data in the past can actually be updated multiple time. So for example in econometry we rely on measurements of prices and wages and employment levels etc. For those values, provisional values are first available within weeks, but then they get repeatedly updated for several quarters after that. Right, so you have different versions of the data over time, you have the data that you get now. And then later on two quarters later, you have the same data that was there that is now reevaluated based on what we know now. Okay, so you have basically an evolving set of data, so the problem with this the challenge is that. When this kind of technology is applied in real time, only the provisional estimates are available for the most recent indicator, okay. And the problem then is that statistical and machine learning models must be very carefully trained with the right version of the data, right? So in the provisional records are part of that data, all right, so from a data technology perspective. We must develop ways to record, organize and serve various versions of each point of view of interest. So one way to think about it is that we need to move from what do we know, to what did we know and when did we know it? Right, this is what we call data versioning, because now we have multiple different version of the data that need to be maintained. From a reuse standpoint that means that we need to understand not only how the data was obtained. But also when the data was obtained and which version of the data we are looking at and the process of versioning must be understood as well. This is an example that I take from the work of Ryan Tishibari and Roni Rosenfeld. They lead the Delphi project, which is basically a large project on building models that are predictive of COVID propagation. And trying to build models that are the level of individual zip code that maybe have a long time horizon. This is an example here on the interface that shows some of the status of COVID cases across across the country. So of course the way they do this is by accumulating a large amount of data, many different sources of data from doctor's visit. Self reporting on Facebook, Google, other platforms, very heterogeneous data of course that has to be acquired continuously over time. All right, and if you look at how this data works, you have kind of a two dimensional axis, right? You have the time at which the data is reported in this little example here from time t to time t minus 4 in the past, okay? And then you have the time during which you're trying to make prediction. So what happens here is that the data is reported at a certain time set t minus 4 here at the bottom. Then at the next time of reporting that new data at t minus 3 is going to change this data t minus 4. Perhaps we're going to realize that we had actually more reports more COVID cases. Or more COVID motivated doctors visit at t minus 4, than we thought as being collected from the new reports, okay. And this happens consistently, right, this happens because of delayed reports, this happens because of error correction. This happens for a number of reasons and the experience here is that. Taking this properly into account is absolutely critical to be able to build any kind of accurate model. So if you look at this diagram, you can see that you now don't have just one linear set of data acquired over time. You now have kind of a two dimensional graph of data, we do multiple versions of the data. And understanding this versioning process and how the data has changed is actually critical in being able to build those model. So how does one represent that, how does one store and manage that data. How does one represent how this versioning takes place is an interesting set of issues in modeled data. This is an example, here, I'm going to try to see if I can do this yeah, this is an example here of data on the horizontal axis is time. This is doctor's visit that are reported and the vertical axis is the number of doctors visit, okay. The interesting thing is that the, I cannot get my pointer to work here, sorry. So the orange bars here are the visit reported up to the 8th of April, and the other columns. Indicate visits reported in the few days after that, so 10, 14 and 17 in the upper graph. The point here is that the data back in time is changed based on the current data. That's what is illustrated on the right side of that graph that you see at the top here. So this is just a graphical visualization on actual data, for this particular COVID prediction. COVID tracking project of what it means to add those multiple versions of the data that one needs to keep along. So again, this is a relatively simple example on one number graph like this. The problem now is how to do this in a systematic way, with systematic tools on heterogeneous data. So, those were a couple of things that I wanted to mention, the idea of human generated data. And all the new tools that data requires, privacy and security and paving the way for completely distributed and secure data processing. So that we can maintain whatever proprietary or private nature of the data it is that we want to protect while doing the processing that we need. And finally versioning which has to do with data that evolves over time for which we need to Keep multiple, multiple versions. So those are a couple of things that I wanted to, to mention and I will stop here and and give it back to you before by to you version.  Awesome. Thank you Marshall. Thanks for the great overview of the activities that's happening at the school of computer science and all these relevant questions related to data reuse. So now I would like to open the floor for the audience to ask questions. So you can either since we have manageable. Size, I would say, you can either raise your hands or chat the questions to me. So, unmute, you can go ahead and unmute yourself.  Hey, this is Ahmed from Martin science department. Hi, Martial. That's an excellent dog. I have a very specific question. Maybe fall into what you show, maybe not, but I would just go ahead. So within the context of human generated data, you also showed that peer review process also have can at times have a multiple way of representing the similar kind of information, so given the ontological discussion which we are having since morning is it possible to extract some. Sense of what sort of ontology exists by training some sort of an NLP algorithms from the peer reviewed journal articles itself. So coming so I'm looking in a way and not the top down approach of designing the ontology and then extracting information from the papers from a bottom up. Perspective of developing an ontology schema by scanning thousands of papers. Any thoughts on this would be great.  Yeah. That's an interesting idea and actually some things some attempts like this are being done not necessarily in the context of. [COUGH] Not in the in the context of reviewing, but in the context of summarizing and in the context of identifying trends also, okay, which in those cases, you need to have a bottom up aspect. So I realized this is not the same as what you suggest in terms of the ontology definition, but the tools, the basic bottom up tools that you would use, are kind of the same, right? So that would be an interesting thing to look at.  Thank you.  So any other questions from the audience? So this is sort of informal conversation between all of us. So please don't be shy and ask questions. I guess I can just ask one question that always always does very. I always wonder about so there are a lot of secondary data out there and there's like Marshall, you'll also have mentioned there's a lot of human generated data and they're messy and heterogeneous and hard to use. So From your experience like what makes you like when you see a data set what makes you to trust that this data set is usable or at least manageable before you actually download the whole thing.  Yeah, so I think the key thing is to have formal data. So the key thing is is not the data, it's the metadata, okay? That will say all that data was quiet. That is the most important. The most important thing. You know, as you mentioned in the introduction I come originally from robotics in data sets are extremely important in in robotics as in any field actually. But one thing that is particularly difficult Is to document exactly the conditions under which the data was was required, especially when we talk about physical data, machines moving or interacting with your environment, right? So that's the main point, I think, how was the data collected. And by that, I don't mean just statistical characteristics of the data, you mean in terms of bias or things like this. It's more how the data was acquired, how it was transformed and so forth. One of the issues I think, is having formal ways of describing that. Okay, in formal ways of describing that, that can be shared across domains. Okay. I think a lot of that is still a bit of an art, right and it's still very much data and domain dependent. So it's difficult to have common best practices or even better common tools that would help describe how that meta data.  So yeah so meta data is a key I guess like some of the many of the speakers in the morning also mentioned like difference Implementing different metadata schema, I guess for the for those in the audience. I also I kind of want to post the question we, you know, we're dealing with very like many all different kinds of data schema and  I guess the question is when you're facing all these data schema, how do you chose which one to use? And how do you forward them? Is there any effort in harmonizing these standards?  Okay, I don't see any response.  Right yes, that means the answer is no right?  [LAUGH] I suppose yeah I yeah.  My sense is that this is the tough question for, for most people to answer. And if there's a, I saw in the morning, there was some,when Lily was talking about the dimensions data and, I think Melissa suggested, okay, add this ontology to it. So,  You so the audience if you feel like join the conversation, please go to the slack and contribute to that thread site. I guess the, Marshal, the next question I guess from here, I also want to say that from university we generate a lot of data from the sciences and from the robotics as well, I guess. But also from the domains and disciplines that are not traditionally data heavy like music, arts, and humanities. They're also generating a lot of data. And everybody wants to use data and do some sort of the AI is becoming more and more of a standards application for these domains. So as a higher addict at institution, how do you see our role in helping people to use all these data?  Yeah, so one major direction that we need to pursue, that we are pursuing to some extent, is the taking all those ideas and being able to develop tools. That can be used with minimal knowledge of those issues as you said, minimal expertise as to use a term I think that you use. And that involves actually work research not just in data science and AI and related fields, but also work in HCI. And in injecting into conversation modeling of how one uses the data, right? This is similar to some work in software engineering, for example, that actually involves a lot of human modeling. That has to do with how people think about code, how people design code, how people track down arrows, do we find it and things like this. So we need to have similar models as to how people deal with data generally. So it's kind of a strange thing that to address the problem that you describing you need the technical work of building tools that are well engineered source so we can lower the barrier to entry. But we also need a lot of work that comes more from the human studies and psychology and human behavior. And I'm a great believer that a lot of the progress that we're going to do in computer science related to AI etc, will come, not just from technical progress in computing related things AI etc. But we've gone from better understanding and better modeling of human interaction. In this case, human interaction being how one goes about using data but interacting with data.  So we have a question from Keith, I guess Keith, do you wanna just unmute yourself?  Just done that Guchin, thanks. Yeah, Marshall, thank you fascinating presentation as I expected and you make me wonder about incentives for data shitting. And we all know that apart from the sheared criticism of car parking that most faculty in the University are united by their discipline rather than by their institution. And therefore I can imagine for example, roboticists around the world, sharing data with each other, so that they can broaden the analysis of testing of models. But in many instances I've seen or heard of effective reuse of data by people across disciplinary boundaries. My first and favorite example of that was when I worked in Australia and we had as well or just tracking kangaroo movements and whose data turned out to be hugely important for climate scientists. So I wonder about incentives at the institutional level. And further, I will be gearing up for our promotion and tenure committees in a few weeks time. Should we be thinking about how we recognized a faculty members sharing of their data and the reuse of that data as part of their career trajectory? Or are there other incentives we should be thinking about to encourage people taking on the creation of metadata, the proper and curation and sharing of their materials?  Yeah, that's a great question and thank you for mentioning the parking too. I was wondering before you got there.  Yes, [LAUGH] so I think, I would think in those terms of the evolution of how research is evaluated. It used to be that research was evaluated say on journal papers, right? That was kind of the measure we brought of productivity. And then we went into more other instruments of publication which are not necessarily journal papers but they're still publication, they look like articles and things like this. And then the next step, and again, I'm talking a little bit more in my world, but I think it applies across the board. We're talking more about software and things like this that can be released to the public. And that can be used and we use by large number and in some cases, in the case of deep learning in a large number of applications that are across the board. And then what we see now, is in the section since you were using that metaphor of reappointment and promotion in that section, which is not on your core publication that is called products. We now see more and more of data and datasets, okay? And the measure of success for those dataset is precisely what you said, is basically how widely is it's used? How much of a reference dataset is it, and more and more, how broadly is it used, okay? So I think that's the evolution and that's the evolution that we see already that, the dataset that is recognized as enabling new work and new research is a metric as powerful as publication or code or other artifacts. And I think we're going to see more and more of that in the future, especially those that can be used, like you said across many disciplines.  Right, thank you. And I see that Gracchi has put a couple of helpful links in the chat for those who want to explore these issues further.  So this is a link shared by, sorry, I'm sure I'm reading your name wrong. Nooka, would you like to say a few words about this link you shared? From what my understanding looks like this is one of them, yeah.  Yeah, hello, I just wanted to point to this European project. It is funded by the European commission, one of many on establishing the European open science cloud. And they have very good deliverables in the aspects of fair data, and also facilitating implementation for repositories and other stakeholders.  Thank you. Thank you very much. So I don't see further questions from there's a one hand, Ali, go ahead and unmute yourself. Yep.  Yeah, you didn't see it because I just put it up there. So thanks for catching that.  Awesome, all right.  This is a little more of a high level question. And it's just something that I wonder about AI and machine learning in general. So I'm sort of adjacent to the field rather than really working in it. And my question is sort of about the idea of error propagation. So if we think about these big data methods, And how they're probabilistic and how they're statistical. And they're really sort of for making these decisions with a ton of data and not much time. We can imagine that works very well for a business context or maybe even for a policy context. But I'm really thinking about it when it comes to research. Particularly research in the sciences where I have some concerns about accuracy. And sort of particular values being discovered or represented rather than sort of this more pro, and of course a lot of science is probabilistic too. But I guess my question is How can we avoid the danger or if it is even a danger of error propagation. So for instance, if you Xerox something and then Xerox the copy and then Xerox the copy with that corresponding loss of quality each time. The more that we aggregate and the more that we develop tools that are based off another aggregation and based off another aggregation. I worry that we're sort of losing accuracy even if we maintain the appearance of precision. So can you comment on that?  Yeah, that's very interesting. I truly it depends on the perspective there because from a purely statistical perspective, you could argue depending how the data is used, right and exactly what. It's dimension is done from the data. You could kind of argue the opposite right that in fact, that's that's the position is taken in some areas in Ai and Robotics and so forth. Which is that if you accumulate enough data the local errors in the data are going to, kind of be averaged out, right? So, and so, in that case you get better and better more in fact. That makes a couple of assumptions, that makes the assumption that there's no, systematic error or bias or something in the data. That also makes the assumption, that the processing, with the deep learning going on with something, actually does what I just said. Rogers data instead of locking on to some artifacts of the data. So I think the key ingredient that is missing here. In fact across the board in AI on ML is to have tools of models or theories that understand more deeply. If I have this data set here, if I now perturb that there are certain in a certain way, okay, how is my model for sure right. And that's basically kind of, what you would call an error propagation model, right? And this is really important, not just for what you said of the error propagation. This is also really important to understand, how the model that I have learned on this particular data set. And again, I'm referring more purely machine learning approaches, right? How is this model that I've run on this particular data set going to transfer properly to know my task or my domain, right, which is not exactly the same as this, right? So there is all this Issue of understanding the effect of disturbances if you will in the data that is still to be worked out. We have to from this from statistics of course, but they make very strict assumption. On the data on how it's used, right, if we within those assumptions then there they are good statistical tools to, to do this kind of analysis. But once we get into blackbox, machine learning things, you know, deep learning things and all that those assumptions disappear, right? And we don't know anymore, this kind of data perturbation effects. And that was a long winded answer, I'm sorry. [LAUGH] Any other questions? Sorry, I realized I didn't unmute myself.  [LAUGH] Sorry.  [LAUGH] Yeah, thank you. Thank you both. So in fact a very interesting topic, and in fact, we do have a speaker tomorrow, Will Thompson to talk about error propagation.  Yeah.  And data set, data decay yeah, so,any other questions? I guess this question is related to maybe all of us in one way or another. So related to the pandemic, so you know. The COVID has changed AI applications in many areas of research technology and on all areas of life, I would say. So, Marshall, do you see like how the COVID has changed our way of using data, interacting data or with data or think about data. And so do you see? I know there are a lot of challenges here, but do you see any opportunities moving forward with towards COVID?  Yeah, we're so I think it's It really emphasized if we did not think about that enough before it's not only on for size the importance of data sharing and data reuse. And also one thing that it on for size maybe further than other application is the fast pace.  There is necessary in this case, right, there is not the luxury of taking the time of, curating data and routine, more details as a result, Propagation seems like this. There is kind of an urgency that kind of motivates looking at different ways of using, looking at data looking at towards of Extracting information from the data. Even though we don't have the can I say all the elements all the time to extract the information that we want. So the Delphi project is an example of that right there, trying to do that. Continuous updates and prediction, right, which is very different from other applications where you can basically get the data, do the processing with your results, and so forth, in a much larger time constant. So I think this more, how to deal with this more immediate loop Of processing and prediction is something that is probably interesting. It's not unique to COVID, of course, but, I think it's happening. And it's on the last tail for critical application that's motivating, kind of new thinking on how to use data.  Thank you Martial. I think we're running out of time. But just a one more question from the audience. Do you have any thoughts on how to prepare today's PhD students for tomorrow's data handling? Thanks Alicia for asking that question.  Yeah, so I think at a high level, at kind of a non-technical level, it goes back to Keith question about interdisciplinary aspects. Is to really expose students to a wide range of data, right? So data does not mean just scientific data, it does not mean just human generated data. There's a lot of different types of data and they lead to very very different technical challenges. And actually it's not unique to this topic, but it's always the most important thing. To have a sense coming in of the range of the field, right? The biggest mistake that we can do is to narrow down data to one's field. I mean, I'll tell you the example in my field, specific in computer vision, right? We've had for a long time, a very restricted view of what data means. Which is basically, matrices and vectors of numbers and that's it, right? And so having a much deeper understanding of the different style of data is critical. And in appreciation of that is critical, very.  Thank you, thank you Martial for sharing your thoughts with us. And thanks audience for asking all the questions. So now, I guess we are having a break. Let's come back at 3:10. And in the meantime, if you wanna go follow up with some of the questions could go to. See you later.  See you  Thank you Martial  Thank you. 