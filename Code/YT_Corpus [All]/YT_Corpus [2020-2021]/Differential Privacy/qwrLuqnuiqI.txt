 Deep neural networks have shown unprecedented generalization for various learning tasks, from image and speech recognition to generating realistic-looking data. In this work We are interested in answering the critical question that what is the privacy risk of deep learning algorithms to individuals whose data is used for training deep neural networks? We go beyond membership inference attacks against fully trained models We take all major scenarios where deep learning is used for training and fine tuning or updating models When attacker only passively observed the model updates or actively influences the target model in order to extract more information We designed a modular attack model, which can be used in different learning settings the attack model gets the different features of classifier behavior on the specific instance such as output loss and gradients as input and predict if that instance was part of training data or not Finally, we evaluated our attacks in different settings our results suggest that even well regulates models will leak information about training data 