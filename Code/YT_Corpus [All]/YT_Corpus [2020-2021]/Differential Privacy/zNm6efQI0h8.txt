 You're not going to want to miss this episode of the AI Show, learn how to protect privacy in your data using differential privacy. You're not going to want to miss it. Make sure you tune in. [MUSIC].  Hello and welcome to this episode of the AI Show. We're going to learn all about protecting sensitive data using differential privacy. I've got a special guest with me. Hello, my friend. Tell us who you are and what you do.  Hi. I'm Sarah Bird, and I work in Azure AI. I focus on incubations and productizing emerging technology coming from research. I'm spending all of my time these days on responsible AI tools that we can use to build AI responsibly.  Fantastic. So differential privacy, what is it and why should we be thinking about it?  So differential privacy is an idea that was published in research, actually, quite a few years ago, and the idea is that if we do a certain type of statistical techniques, we can actually hide the contribution of an individual in an output computation. So if we make that less abstract, the idea here is that I want to do analytics or machine learning, but I want to be able to guarantee that I'm not revealing any individual who was in that data set for the analytics. A lot of people don't realize that, for example, when I'm building a machine learning model, the model might memorize particular data points. So if we think of a smart reply example, my model could have been trained on a lot of data, but there might be particular sentences that don't exist in the data set very frequently, as my model might just learn from one or two examples. Which means you could have a scenario where I'm typing an e-mail saying basically, my social security number is, and the model autocompletes with the social security number that it found in the data set, which would be a problem. We can have the same issue with statistics, where if you publish statistics or aggregate results from a data set, it's often possible to put those results back together and actually recreate the underlying data set. This has been used to find very sensitive information about individuals based on public data sets, for example.  So let's pause right there. What you're saying that then is even if I'm only outputting the aggregation, like here's the sum, here's the average, I can somehow reconstruct individual records, single items that are not in the aggregate?  Yeah. Absolutely. So it depends on how much information you have and how much computational power. But there's different types of attacks that in fact can be quite easy to do that will reveal private information, or at least give you a good guess at the private information.  I'd love to see that because that's surprising to me because most people try to obfuscate their data by only releasing aggregate data lake, average, whatever mean, mode. But you're saying you can actually reconstruct, I'd love to see an example of that.  Sure. Let me show you an example in my notebook here. Great. So what I have here is an example that is a synthetic example so that I can run it locally on my machine. But it's actually very similar to what the US census demonstrated this past year as well. So it's taking a type of report that the census releases, and instead we're filling it with fake data that's synthetic, but it's still holds the same. So what I'm going to do then is I'm going to take this data, and what I'm trying to do is I want to do an income analysis. So what I'm going to do here is I'm going to run, and I want to actually study the distribution of income for individuals with a high school degree. So I get a distribution here, and I can use this to do the analytics that I want to do. However, the attack that I want to show you is, what I've done here is taking a simple off-the-shelf SAT solver that you can actually just could do install. Anyone can use it. In this case I've assumed that I know just a little bit of information. I know very specific data points in my data set about a few people, and what I'm trying to do is guess that income. The income is the private information. So what I'm going to do is I'm going to let my SAT solver see if it can create a version of the database that matches up with the published results, the results that would've been in the census report. So if we run here, I'm going to click down. So what we actually see is it's still running and trying to find an example. What we found is there's 500 individuals in this data set, but we were able to correctly guess the income of 50 individuals exactly. There are more than 80 individuals where we were guessing it within $2,000, 120 individuals where we were guessing it correctly within $5,000.  Holy cow. So can we scroll up because I'm obviously not as smart as you, and I want to make sure that I understand. So can you scroll up just a little bit more? So basically what you're doing when you're looking at that formula, you're basically using a SAT solver by filling in information that's an aggregate, and because you might know a record or two in the database, you can use that SAT solver to reconstruct pretty closely everyone else's income. Am I getting it right?  Yeah. For about a third of the individuals, we're correctly guessing it. We wouldn't know which third we were correct for unless we run this many times. If I run the SAT solver over and over again, I could get a distribution of possible reconstructions and have a much higher chance of guessing the incomes correctly. So it's just an example to show how easy it can be to get pretty close on some of this information.  That's surprising to me primarily because, let's just say if you run it for a day for sure you could reconstruct pretty closely what's going on, you're able to extract sensitive information from aggregate data using off-the-shelf stuff basically. Is that right?  Yeah. I'm using a little bit of outside knowledge and some other published information, but information that it would be very reasonable to assume is published.  I see. So this is obviously a problem and there's a lot of data sets out there that maybe suffering from that problem. What is differential privacy and how can it help? Is there something we can do to fix this?  Yes. So the way that differential privacy algorithms work, is they're actually going to take two steps here in order to mask the contribution of individuals so that now I won't be able to do this type of attack. So the first thing that it does is we're going to add noise to that aggregate result. So I can't give you the exact answer anymore. I'm going to give you an answer with a little bit of statistical noise. What that does is mask that contribution of the individuals, so now I can directly connect it back. The second thing that we need to do is track of privacy budget. Because if I were basically doing too many of these queries, too many reports were published, then each of them might have a little bit of information about someone, but I could put them together and still recreate that underlying row in the database. So the idea is that we both need to add some noise, and then we need to make sure that we're not allowing too much information to be released.  So let me ask you the first question. I saw some of this earlier and it was confusing to me. What we're saying is that are we literally flipping answers in the database on the add noise step? Is that what's happening?  No. It works more, you could think of it as adding a small amount of it on top of the aggregates. So if my real answer were 60, you might actually get back 61 or 65. So I can actually show you how it works in my notebook.  Let's do that.  So if I move down here, what I'm going to do is now use the WhiteNoise system to publish the reports, and WhiteNoise is our differential privacy system that we've built and basically it contains all of the proven differential privacy algorithms that have been developed over the last decade in research and then the ones that we're finding are actually working well in practice. So the system sits on top of that data store and does the two steps we were just talking about. So it will take your query, add that bit of noise, compute how much information the queries would be revealing, subtract that from the budget, and give you a result back.  Let's pause. Is it okay if I ask a question or two because this is super fascinating to me. So you're saying that the WhiteNoise module sits on top of the query. So you're actually querying the privacy module. The privacy module asks the real data for the real aggregate information and then it adds noise to it in a way that makes the aggregate data still be valid, but make it harder to reconstruct the original data. Am I getting that right?  Exactly. Right now, the system, you would use it when you have access to the data. But just to guarantee that those reports that you publish or the model that you build has these differential privacy guarantees. However, in the future, we're also hoping to enable this to be a situation where the data scientists can't access the dataset directly, but they could do this query because it will enable us to allow more people, for example, to do research on top of datasets, do medical research, social science research. So we think it's a really exciting direction going forward.  Cool. The second question. The budget, and that's an interesting thing because what you're saying is that an attacker knowing, because it's probably some mathematical distributions that you're using to add the noise that are well-known. What you're saying is that if the attacker does a bunch of systematic queries, knowing the distribution of the noise, they should be able to reconstruct the data and so the budget is just saying, ''You can only query this so many times.'' Am I right on that? Because that's what I'm getting, but I want to make sure I'm correct.  Yeah. I think it's basically that if I reveal too much information, then you could basically remove the noise. You could understand what the noise that was added is, and so you could get that private information.  Well, let's take a look at a demo.  Great. So I use the WhiteNoise system above here to generate some histograms. Then what I'm doing now is, I'm first going to show that where we're achieving what we said we would, which is not allowing that type of attack. So I'm going to do the same SAT solver. But what we get back now is unset, which means it couldn't find a database that satisfies the output, the reports that were published. So it doesn't have any guesses in terms of people's income, which is great. So we've achieved one goal here, but we also need to show that the data is still useful. We're adding noise and so people often ask, ''Well, what am I going to do with noisy data. How useful is this?'' So I still want to be able to do that income analysis, but now we're going to do it using the private data. So if I generate my plot here, here are the two results side by side. I have my nonprivate version, which we saw above in blue, and I have my private version in orange, and what we see here is that in some cases is actually quite close. Very usable the noise is very small. In other cases, the noise is actually quite significant and that's because these are really small values and so the relative noise we're adding could be a lot more. For how usable it is, it's going to depend on basically how large your dataset is and how much noise needs to be added depending on what privacy you need. In this case, we're using a pretty small dataset so that I could show that attack in my notebook. But if you're using a very large dataset, which is the case for a lot of our machine learning and statistics, then the amount of noise that's added is often negligible.  So looking at this, my guess is that there is some slider between how much privacy you want and how much noise is added. Is that an accurate statement?  Yeah. Exactly, and how much budget you want to spend on a particular query. If I only want to do one query and I'm willing to spend all of my budget on that. I can get a lot more accurate information than if I want do many different queries and I only want to use a little bit of budget on each query. Then of course, how private you want it to be affects the accuracy, and then as I was saying for the scale. So if it's a very large dataset, you will still have that slider, but it'll look much lower in terms of the trade-offs.  I see. So basically you're putting the user in control over how private they want the data to be, how big the budget is, etc. It's all a user approach to this.  Yes. Absolutely.  Fantastic. So where can people go to find out more about this? Unless did you have any more you want to show in the notebook..  No. I think this is everything.  Where can people go to find out more about this?  So we have a GitHub repository. So you can go and check out all of the code on GitHub, contribute, see this notebook and lots of others that we have. Then also WhiteNoise is part of a larger community that we're starting to enable differential privacy tools and research in the open, and that's called OpenDP. So you can also check out our community and get involved.  Fantastic. Well, this has been very instructive. Thank you so much for spending some time with us, and thank you so much for watching. This has been another episode of the AI show, we've learned all about Protecting Sensitive Data using Differential Privacy. We'll see you next time. Take care. [MUSIC] 