 [Music] the starting point of our book is the observation that although machine learning has been around for a long time we are now starting to use it for increasingly consequential tasks as many people here at Google will know so for example as was in the news recently lending decisions like if you apply for an Apple credit card will you get a high credit limit or a low credit limit are now often made without any human intervention at all just just an algorithm HR departments make hiring and and compensation decisions use informed by machine learning algorithms and in a number of states including in Pennsylvania bail and parole decisions are informed by by trained models and so it's sort of natural when you start making when you start using machine learning to make important decisions about people that you start worrying that maybe those algorithms will violate some of the social norms that we would expect of human decision makers when they were making those decisions and indeed like there's lots of evidence that this happens we see news articles every week and there's a number of very good popular books that have pointed out the problem and here's three books here that we admire but what these books do is very much point out the problem but but where they say less is about what you can do to fix these problems right what the solutions are they they talk about the need for regulation for example which we by and large agree with but they don't talk about technically what you would do to make the algorithms better behaved and that's the goal of our book to explain in in plain English the emerging science of what we talk about as embedding ethical norms into algorithms then there's now a community of hundreds of people including us working on these problems one comment we often get in fact something that one of our early reviewers asked us is does the very premise of the book like the title of the book does it make any sense the ethical algorithm because algorithms are in the end tools they're human artifacts like hammers and well algorithms like hammers can be used to do bad things that can be used instruments of violence I could whack you on the hand with a hammer but if I did that we wouldn't think of this as some moral failing of the hammer right like you would attribute that action directly to me and that's basically how we regulate you know and and write law is about about violence you know induced by hammers right we say if I whack you on the hand with a hammer then I'm very likely gonna have to go to jail and I'll take that into account when I'm deciding whether I want to do that but algorithms and when we when we say algorithms we really mean models that the sort of trained models that are the output of machine learning pipeline are different they're different in a number of ways but but one that's salient for this discussion is that it's very difficult to predict the outcome in every situation of an algorithm that you've trained using the principles of machine learning ok so eat so many of you are aware of what the machine learning pipeline looks like but let's just briefly recount it you'll start with some data set okay a data set might these days consist of records of millions of people you might have hundreds of thousands of features for each person and in the best case right if you're lucky you're in the best case you're not always in the best case in the best case you understand this data set as the data scientist you know developing an algorithm in the sense that maybe you know how the data was gathered maybe you know what all of the features represent okay but it's hard to say that you really understand all of the information contained in such a massive objects then you use this data set to formulate some usually narrow objective function some proxy for classification error or maybe profit and you use some tool something like stochastic gradient descent to search over some enormous class of models to find the one that is best or you know very good at maximizing your narrow objective function and then you get out some some model right if you're if you're training a deep neural network this might consist of millions of parameters and it's hard to say anything at all about this model except that it's probably very good as measured according to the objective function you specified and so the problem is when and if this model goes on to inflict some harm on some person or some group of people you know it's it's typically not the case that this harm was the result of some mal-intent of the software engineer scientist who is sitting behind the scenes and and and building this algorithm right if that was the case the situation would be much simpler right existing regulatory tools could be used to weed out bad actors but the problem is that the harms that we see from algorithms are typically the unanticipated and unintended side effects of of optimization over large classes of models of the very basic premise of machine learning and so if we're going to if we're gonna prevent you know this this bad behavior by by learned algorithms we need to figure out how to embed our social values the the actions that we want the algorithms not to exhibit into the design process itself okay and that's that's hard right because words like privacy and fairness and accountability these are you know big and vague they mean many things but it's important to be precise about definitions when when you say privacy or fairness and you in particular when you say you want an algorithm to be private or fair it's not enough to speak about these things as a philosopher might for example because it's just at a very practical level if you're going to embed these things as constraints into some optimization you need to be mathematically precise it's also enlightening to you know it's an enlightening exercise even separately from you know the need to embed mathematical constraints when you're designing algorithms to think about what you really mean what are different kinds of privacy what are different kinds of fairness and the very act of trying to be very precise about these things is illuminating and can reveal new trade-offs that maybe weren't immediately evident so we've written these words and degree in decrease degree of greyscale starting with privacy ending with with morality and you know you can't even see it but but you know Michael assures me he wrote singularity in white at the bottom there in proportion basically to how much progress we've made trying to understand these things at a at a sort of mathematically precise level how much progress we've made thinking about the consequences of embedding constraints representing these notions into algorithms so it's not to say that we've solved privacy or or that we have precise ways of thinking about all of the many different kinds of privacy but as we'll talk about in a moment we've made some progress fairness isn't there yet but it's it's along a good path and and for these other things accountability interpretability and you know even more as you go further down the list and people are working on these things and they're important but we feel like we don't have the right definitions yet that are that are sort of a necessary prerequisite to making the kind of scientific progress that we talked about in the book okay thanks okay so what we want to do with most of the remaining time is just you know go through two quick vignettes one on privacy and one on fairness which as per Aaron's last slide are there areas work that we feel are in relative terms the most mature for the type of scientific research or algorithmic research that we're discussing and as Erin said sometimes like the very exercise of having to think so precisely about the definitions of these social norms is itself greatly beneficial and might not only reveal trade-offs that you weren't aware of but like flaws and your intuitions about these ideas if you just talk about them at the level that let's say a moral philosopher might and so privacy is a good case study we would all we argue in the book that we feel like there's a definition of privacy for at least of the type of privacy that I'm going to talk about here is the right definition which is differential privacy but it's preceded by definitions that I think we and others feel like are fundamentally flawed and unfortunately these fundamentally flawed concepts are the ones almost exclusively in force in practice these days so if you look at a you know and user license agreement or a privacy policy of a large company they will normally refer to policies if they're if they're precise at all about various forms of anonymization or remove a removing PII personally identifiable information and to give you a sense of why we think those definitions are fundamentally flawed I have here a toy example in which there's two different databases from two different hospitals of medical records and due to privacy concerns there's been some anonymization done here and the normalization largely consists of you know operations of redaction like just make entirely removing certain columns from a database or corseting in which you sort of fuzz up the information and then the hope is that somehow when you're done with this you have some sort of privacy guarantees so in this top database you know somebody's gone in and decided like well let's entirely redact an aim rather than giving precise ages let's group them into decades so you know are you 10 to 20 20 to 30 etc let's give some information about zip code but redact the last two digits and let's keep some of the medical information like whether you're a smoker or not we'll come back to smokey in a minute and what you know the particulars I ignore that you were given in your visit was and of course in reality you know these databases would be much much larger for a large Hospital like the University of Pennsylvania's there might be tens of thousands of records but the the conceptual flaw can already be demonstrated in this toy example suppose you know you have some additional information aside from this database like you have a neighbor named Rebecca who you happen to know is female and that she's 57 years old and you know this because she's your neighbor and you're friends with her okay so if with that side information you also managed to get hold of this allegedly anonymized database then already in it there are exactly two records which match Rebecca's your knowledge about Rebecca and they're the two highlighted in red and notice that already from this from this side information you can infer that your neighbor either is HIV or has colitis and she might reasonably already consider that to be a violation of her privacy alone now again in a in a real day at large database and in a real application of these methods you might go for a criterion like what's called K and so what's que anonymity que anonymity basically asks that you do enough of this coarsening and redaction so that any row of the remaining day in that in the you know kind of an allegedly anonymized database matches at least k other there at least k matches to that row k identical records okay so then you wouldn't know you know rather than knowing this is like a two anonymous database but in general we might you know hope to get more privacy by asking for one hundred anonymity rather than to annotating the real problem of course comes when you know you also know that your neighbor rebecca happened to also have a visits to a second hospital whose databases at the bottom and this hospital has also in an effort to provide some kind of privacy done some redaction the same kind of redaction and coarsening in their database and now three records match rebecca there and of course the real problem here is the join of these tube databases right which is sometimes called linkage analysis or triangulation or various other names when i take the intersection of the top red records and the bottom red records uniquely now know that rebecca is HIV and you might try to wish these problems away with fancier definitions or by appealing to scale but but the real problem with these types of definitions is that they pretend that the data set in front of you is the only data that is ever going to exist now or forever in the world and they don't anticipate attacks on privacy that come from you know triangulation of multiple databases other information you might have about people even publicly declared information that they weren't particularly trying to hide many of you might have seen the sort of mainstream news frenzy over articles that i think surprised probably very few people in this room you know one was about a month ago and it basically said you know here are eighteen apparently innocuous attributes that if I know them about you they serve as a fingerprint for you among all US citizens right so you know I'm not sure what they were but you can imagine do you tell me what kind of car you drive you tell me your zip code you tell me what color your eyes are you tell me whether you have dogs or cats each one of these things of course is like you know exponentially cutting away the remaining possibilities and it doesn't take long to kind of have that sort of innocuous information undo the privacy promises of these anonymity methods okay so these are bad privacy definitions as we discuss in the book what would be a good privacy definition well let me start by proposing a definition which has been I think proposed since at least the 70s which if you could get it would be a nice definition but we argue in the book that it's basically asking for too much in the sense that if you enforce this kind of privacy we will never be able to do useful interesting things with data including things like medical research studies okay so so what is the definition I have in mind so imagine you know and you can make this mathematical but I won't bother here imagine we basically said the definition is that no harm should ever come to you of any kind for as the result of a data analysis in which your data was included okay so let's think about that as a privacy definition for a second so certainly it's a strong privacy guarantee right I'm sort of allowing the notion of harm to be entirely general and I'm basically saying if your data was used no harm should come to you of that study okay so why is this asking for too much so imagine that it's 1950 and you are a smoker okay and if it's 1950 you are a smoker because in 1950 pretty much everybody smokes there is no social or medical stigma associated with smoking in fact it's seen as a glamorous habit and so you do it openly in public everybody that knows you knows that your smoke or maybe even your health insurer knows that you're a smoker who cares okay and suppose you're asked to contribute your medical record to the famous series of studies that were done in the 1950s in in England that firmly established a correlation or connection between smoking and lung cancer okay so your data was included in this analysis this analysis announced to the world that there is a connection between smoking and lung cancer and now we can say real harm has come to you as a result of this study right now you know everybody's posterior beliefs about the likelihood that you have cancer go up in this study and your data was part of this study okay and in particular real harms might come to you of the financial variety your health insurer might decide to double your premiums for example okay now the key observation okay so so in particular if we adopt this definition this study would have been disallowed right this would have been a violation of privacy of the privacy of everybody whose data was included in this study the key observation though here is that of course it's not the case that your particular medical record was the crucial piece of data that allowed this the link between smoking and lung cancer to be established right any sufficiently large collection of medical records would have been enough to establish this this fact because you know the the fact that smoking and lung cancer are connected is not like a fact about you particular or your data it is a you know we might call a fact about the world that can be discovered provided we have enough data okay so this brings us to the what we claim is the right definition of privacy which is differential privacy which slightly refines the definition I give to sort of account for this fact that your data wasn't the crucial missing piece in this analysis and this is a schematic but in English what is differential privacy asks it basically asks you to consider two alternative worlds one in which an analysis is done and your data is included in that analysis and let's say that there are n medical records total in the analysis and the other one is the same analysis is done but on n minus 1 medical records where the missing one is yours so what we want is that you know the haunt the difference but you know the harm that comes to you is basically identical in these two situations so whatever whatever your definition of harm is whatever it is you're worried about you know the the chances that that harm comes to you in the case where your medical record is included compared to the one where it's only your medical record that's excluded is sort of controllably close okay and as many people in this audience know the definition of differential privacy involves it's a property of an algorithm first of all not about a day that particular data set and algorithm either is or is not differentially private and general differential privacy is generally achieved by adding noise to computations so you have you moved from deterministic to randomized algorithms and you typically add noise in a way that kind of obscures the contribution of any individual piece of data in the analysis while preserving broad statistics okay and you know so the first time Aaron's been working in differential privacy much longer than I have and I remember the first time I saw the definition of it I thought like well that's a great definition but I'm still worried that it's too strong right it's got many Universal quantifiers and it right it's got a the algorithm has to provide differential privacy on absolutely any input database the definition of harm can be anything you want it to be and still the increase in harm as a result of including your data is controlled and so my first reaction was like you know maybe you would still won't be able to do anything with this definition either luckily that's turned out to be you know far from the truth and in particular pretty much any technique you know from statistics or modern machine learning has a variant it is not differentially private in its original form but it has a variant which gives differential privacy so you know for example back propagation in neural networks or stochastic gradient descent have differentially private's variants so differential privacy is kind of you know just in recent years started to make it out of the lab or maybe you know kind of more precisely off the whiteboard into practice and the big moonshot for differential privacy is coming up next year when the US census has decided that every single report or statistic it results based on the raw underlying census data will be released under the constraint of differential privacy and this is a huge engineering effort and it'll be interesting to see how it turns out and I'm gonna turn it over to Aaron now to talk about fairness a bit yeah so we're not we're not there yet on fairness so we sort of assert that if you you know think about differential privacy for a while you know read chapter 1 in the book that many of you will agree that at least for a particular kind of privacy statistical privacy and data set it's somehow the right definition it's capturing what you want there's there's nothing like that in the fairness literature yet there's you know dozens of definitions of what we might mean by fairness and for each one you know I could tell you one reason why it's you know lacking it's not capturing everything you want in fact we even know the study of fairness is gonna be more complicated in the study of privacy because there are already known you know different reasonable definitions of fairness that in isolation you would nod your head and agree yes that's something I would like that are known to be incompatible with one another nevertheless and so maybe you think about you know the study of fairness and machine learning as as where the study of privacy was 15 years ago nevertheless it's a it's a extremely important problem here on the slide are two headlines just from the last week two applications that have attracted new york state regulatory scrutiny one the lending application the apple credit card that you might have heard about there's a number of tweets from prominent prominent people alleging that the algorithm that determines what your credit limit will be exhibits gender bias the other article was about a widely deployed algorithm targeting healthcare interventions that seems to exhibit racial bias so i don't want to talk too much about definitions of unfairness because I don't think we've yet hit upon exactly the right ones but I do want to give some idea for why machine learning might be unfair in the first place because I think a lot of people's first reaction is that well you know bias is a bias of the sort that we talked about when we when we talk about like racism or sexism this is some human property and we're removing it just by removing human beings from the decision-making pipeline and you know using objective optimization procedures and it's a little more complicated than that here's a little cartoon to to illustrate why okay so suppose that Michael and I volunteer to help out with pan admissions and we're going to design a machine learning algorithm to help admit students to Penn okay so maybe in this cartoon we've got two observations about each applicant their SAT score and their GPA and there's some concrete thing we're trying to predict okay so maybe for example we're trying to predict whether students if admitted will graduate and at most five years with at least two 3.5 GPA maybe we're trying to predict whether within 30 years of graduating they'll donate at least 10 million dollars right whatever it is some concrete thing such that we're trying to admit the people who we've labeled as plus and we want to reject the people we've labeled as minus and there's all sorts of problems you might imagine gathering this data you might imagine that there's you know potentially the biases of you know past admissions officers embedded in this data but let's wish that all away and imagine for this cartoon example that the data really is what it says it is okay because I want to I want to show you that it things can be a little bit more complicated even in the best case when you've got good data so there's gonna be two populations you're looking at the green population now and there's a couple of things I want you to notice about them so first slightly fewer than half of the green population is qualified for college by which I mean there's slightly more - science on this slide than there are plus science second there's a pretty good although not perfect decision rule there's a line I can draw up through space and by and large although not exclusively the positive points lie above the line and the negative points lie below the line okay so that was the green population here's the orange population and again a couple of things I'd like you to notice about them maybe the first one you noticed is that the orange population is a minority by which I mean literally just that there are fewer orange points okay like in this context all it means to be a minority is that there's fewer of them the second thing you might notice is that the points seem to be drawn from a different distribution in particular they're they're shifted downwards on this plot they seem to systematically have lower SAT scores that could be for one of any number of reasons for example maybe the green points come from a wealthy population they take SAT tutoring classes they take the SAT three times and report only the highest score the orange points take it one scold that naturally results in a higher distribution on SAT scores for the green population but it necessarily make them more qualified for college in fact when you look at the labels when you look at the actual thing that we're trying to predict it's the orange population that's better here and the orange population is better in two ways first on average they're more qualified for college right there's you know half of them are positive examples here compared to fewer than half for the green population and second it's easy even easier to tell who's who there's now a linear decision rule that I can implement that makes no mistakes at all okay we've got two populations and in this example the minority population is the better one when I say they're better I mean they're more qualified on average and it's easier to determine who are the qualified ones and yet here are the two populations together and remember we're only giving the algorithm SAT score and GPA so you can see the colors of the points but the algorithm cannot and suppose what we asked for is the the standard standard objective in machine learning we would like to find the model in this case the linear decision rule that makes as few mistakes as possible okay what could be more objective than that to minimize the number of mistakes and what you get is just the rule that best fits the green population you can think about why that is right if I were to shift that decision boundary downwards I would make fewer mistakes on the orange population but I would make more mistakes on the green population and that wouldn't be worth it from the point of view of minimizing overall error because there are more green points and so mistakes on the green population count more for overall error okay so we had an example here where the orange population was better than the green population but drawn from a slightly different distribution and when I asked to find the model that minimized overall error it ended up rejecting every single member of the orange population despite the fact that they were more qualified and despite the fact that they actually had more signal in their features note by the way that were I allowed to use group membership color in this case in my model for example if I were allowed to build a decision tree that said well for green points use the blue line for for orange points use the purple line then I could have improved by this I could improve things for everybody right I would have had a more accurate model it wouldn't have changed the decisions for the green population and all of a sudden I'd be making the right decisions for the orange population and so two things I want you to learn from this cartoon the first is that if you just blindly optimize for error okay that will tend to fit the my majority population typically at the expense of the minority population not for any kind of you know underlying not not because there's any kind of like underlying like racism baked into the like objective function but simply because larger populations contribute more to overall error and second although it's a knee-jerk reaction to say okay like if I don't want it like racial or gender bias in my algorithm I shouldn't I shouldn't use those features it's like it's not always the case this is an example where using those features can actually make things better not just for fairness whatever that is we haven't defined it but for accuracy as well this is an example of something that intuitively seems unfair okay we have this better population and we've learned a model that nevertheless rejects all of them simply because there's fewer of them if we want to design algorithms that correct this we have to pick a definition we have to specify what we mean by unfair I don't want to dwell too much on definitions but for example in this thing you know in this application you might decide that the people who are being harmed by the mistakes made by our algorithm are the qualified applicants the positive examples who are mistakenly rejected by our algorithm these are these are the people who like it's really too bad that our algorithm rejected them they they would have done well had they had they come to our College and maybe the thing that you object to in this model is that the rate at which the algorithm is doing harm in these two populations in this case the rate of false rejections the false negative rate is drastically different between these two populations a hundred percent on the orange population is close to zero on the green population and so you could imagine asking and this has become a popular thing to ask for that our we should find a model that comes close to equalizing these false rejection rates maybe it exactly equalizes though or maybe it equalizes them up to you know 5 percent or 10 percent or 50 percent so you've got some quantitative notion of unfairness that you can ask for there's a knob that you can turn trading off this notion of unfairness with with other things you care about like error and what you find when you start designing algorithms that achieve these goals and then you've got this knob that you can tune and by the way differential privacy also comes with such a knob and so you can draw similar pictures when you're thinking about privacy what you find is that although there are inevitably trade-offs that you have to grapple with you can illuminate what those trade-offs are okay so these are Pareto frontiers these are on different data sets for a real machine learning task the optimal rate of unfairness that you can achieve here measured by the difference between false negative rates between populations that's what's plotted on the y-axis with the optimal rate of error you can achieve that's what's plotted on the x-axis okay so for a particular class of models you can achieve an error unfairness trade-off represented as any point on this Pareto frontier and it is not possible to go beyond to get a model that simultaneously improves on both of these metrics and what you can see you know if you're lucky like in the in the plot on the left you can sometimes get a dramatic decrease in this unfairness metric in this case difference between false negative rates at the beginning and only a very small cost to error okay that's that's what happens when this curve looks very steep of course these trade-offs become more severe as you as you start as you start asking for more and more stringent conditions and so as we described in the book you know the science can only take you so far it can it can elucidate what these trade-offs are but it can't tell you where on this trade-off curve you want to live you know as a society in a particular application and there's not going to be universal answers I will want to prioritize fairness or privacy more in certain applications will want to prioritize accuracy other things other applications but you know there's no avoiding that we have to make hard decisions and what the science can do is it can it can help us make those decisions with our eyes open what we described so far plus would the introduction gets us to about the halfway point of the book and in the mid mid mid way through the book we kind of take a wide left turn that we think is interesting and well motivated and I just want to give you a teaser for what that wide left turn is so in the different scenarios and applications we've talked about so far it was fair to a first approximation to think about individual people consumers as the victims of algorithms so you know you might be denied admission to a college you wanted to go to unfairly or you might have your privacy leaked by you know a data set or a computation and you might not even know it right and you might not also know that your data was being used to build these models that are being applied to decisions made about other people there are other situations in which there's an algorithm or maybe more precisely an app and there's a large base of users of that app and it's not so easy to entirely blame the algorithm alone for the antisocial behavior that it exhibits because then antisocial behavior is sort of a function of the algorithm but also of the incentives of the users who are using the app okay and this takes us into the realm of game theory and so in particular there are many many apps these days that we can really think about as you know the word that often used is personalization but we might think about the game theory term as being computing your best response right so one concrete example is commuting using apps like Waze and Google Maps where in response to real time traffic mainly that the activity of all the other drivers on the roads there's this app that computes your best response it basically says this is the short at the lowest latency or the shortest driving route for you to take from your point eighty or a point B and you might think like oh well what can be better than that I've got this thing that uses real-time traffic information right now and tells me which way it which route to drive but it is driving us all collectively towards a selfish equilibrium of some very very large applicated multiplayer game like literally the Nash equilibrium of that game and any of you that have had any basic game theory know that just because something is an equilibrium doesn't mean it's a good thing for you or necessarily for any of the players in that game and so in particular in the case of driving apps right and there's well known both toy examples and evidence that this happens in the real world even though we're individually optimizing all the times with these apps we might be collectively driving more because we're in this competitive equilibrium and in the book we we kind of you know take this you know semi metaphor and apply it to areas that I think are less clearly mathematically formula formulate able as a game as commuting including things like product recommendation on services like Amazon or what you see in your Facebook newsfeed and talk about sort of the tensions between individual optimization and self-interest versus the collective equilibrium that we're at let's say in the form of filter of bubbles or vulnerability to fake news in the case of Facebook things like that and then the final chapter of the book before we get to the the catch-all chapter that discusses everything from sort of interpretability to every AI alarmists favorite dystopia the singularity we talked about specifically sort of this the sport the competitive sport the machine learning has come become and in particular we talked about sort of game theoretic ways of thinking about that and the consequences that it has for things like the reproducibility crisis in the sciences so you know in a very quick nutshell you know I think many people in this room will be familiar with the fact that machine learning in some sense has become a competitive sport where there are these benchmark datasets it's very there's their selection bias in the reporting of results because journals won't publish negative findings for the most part and we really there's so many people in the field right now that we really have no idea how many experiments are actually being run and how to correct for the complexity a number of those experiments to make sure that we're not sort of going down the road that you know food science has already gone down where some significant track the published results are not reproducible and our kind of false discoveries so that's a teaser for the second half of the book and we wanted to invite Emily back up and and chat with us so I really wanted to start with one of the major theses in your in your work is that the solutions to the ethical concerns that are arising from this prevalence of algorithmic decision making systems should themselves be in large part algorithmic and so it's only if you could talk a little bit about how you came to this perspective if you're thinking on this matter has evolved at all in the past few years yeah I mean I think we came to that perspective through our technical research work right so I mean we were relatively early adopters of sort of the whole fate view of machine learning and algorithms like many people in this room and so we knew you know even while we were reading reports of our field violating based on basic social norms that we and others were thinking about well well you know you could wait for better laws and regulations or you could you could go fix that problem in the code this way like right now I definitely think our view has evolved and even the draft of the book of all this we you know we've been we talked we talked to many people outside of the you know computer science machine learning community who care about these issues like regulators like policy makers like people who work in social agencies that see firsthand the damages caused by you know criminal sentencing models that have gender or racial bias in them for example and I think it's the main the main evolution it had at least on the book is to you know is to point out that we don't think that algorithms can solve every problem and that there's still a great deal of room and importance for laws regulations and more traditional you know solutions and that also there are some problems that the you know the really hard problems remain are kind of social so you know if it's the case that you're police on the street are racially biased and who they decide to arrest or stop and frisk that's going to kind of show up in the data you may not know and the only solution for it is to like you know make police less racist right and that's like not an algorithmic problem it's not even in an easy regulatory or policy problem the only other thing I would say is that so of course like all of these problems are complicated and the fact that yeah and their solutions are probably it can't be derived from just thinking about some very narrowly scoped algorithm without thinking about the sort of broader social and algorithmic ecosystem in which in which they live but many of the issues that have come to light when thinking about for example algorithmic fairness like trade-offs between different reasonable notions of fairness yeah it's not that they're specific to algorithmic decision-making they've only come to light now because there's no avoiding when you're using algorithms making quantitative measurements and specifying precisely what you want but but these issues are you know these these trade-offs for example are fundamental to any like decision-making process they they apply also to human decision-makers and so I think many people think of like algorithm as a scary word of course you know I guess computer scientists and as folks at Google we probably think of it as less scary but it's not that it's not that there are it's not that you're just as you know simple tweaks algorithms can't fix complicated problems saying get rid of algorithms like also doesn't also isn't a workable solution doesn't fix anything something else that you talked about in the book is how the a lot of these outcomes are the results of professional scientists and engineers very carefully and rigorously applying principled machine learning methodology just to massive complex data sets and and so you do kind of get at this a little bit about how what is you know the things that are missing in this in the standard sort of methodology and so I'm really thinking that like this this points to how many different aspects of the kind of rigorous scientific practice that we would strive for actually fall outside the kind of standard machine learning sort of framing and educational training and so you know for example one of the examples that you gave just now on the screen was this algorithmic healthcare system that was sort of reproducing racial biases and the in the healthcare sector and if I recall correctly one of the problems with that system was this kind of equated health care with healthcare costs and so this is something that's been discussed a little bit in the algorithmic fairness community this kind of you know failure to really precisely articulate and justify the kind of you know operationalization of abstract sort of social constructs into precise variables that are then predicted by the machine learning system and so I'm wondering if you could just talk a little bit about you know the sort of new machine learning education and and practices that would kind of you know get at these things that fall slightly outside the kind of traditional machine learning thinking but are still kind of in this algorithmic frame yeah I mean I guess in some ways it's fair to characterize you know the fairness and privacy chapters of our book at least as as kind of a tutorial on you know what you can do to make things better or without leaving the field of machine learning and going and becoming a social worker okay right and by the way you know in writing this book we often would talk to people that would basically say to us like well if you really want to help you should like quit your day job and you know go become a social worker and I was like okay well I'm not gonna do that but but I mean so but but I think that maybe one of our points especially to an audience like this is that there are things that we can do that are just adjacent to what we're doing already I mean the hard part will be you know things like the Pareto curves that Aaron showed there will be hard trade-offs between you know error and fairness or a you know error and privacy but it's not like it's not like a different kind of beast and mean if I had to you know phrase it very dryly it's like the difference between solving the optimization problem that you're solving now to solving trained optimization problem where your objective is the same but now there's like fairness or privacy constraints and so I think you know in many ways for the machine learning community this is like low-hanging fruit it's low-hanging fruit that will result in perhaps difficult decisions with leaders of your business units when you tell them like oh this will make our ad placement more fair but CTR prediction will be this much worse which translates into this much less profit every year but at least you sort of put the discussion on scientific grounds and I think those parts are appropriate to put on scientific grounds yeah I think there's two separate things here so the discussion we had on the slide was in this idealized world where the data was it was clean it was correct the labels were were right and right even there there's something to to do to learn but but that's really the scenario in which you're you know talking about constrained optimization problems and having to deal with trade-offs in the United Health case the problem for those who aren't aware is this model was supposed to predict given a patient with with some collection of symptoms health outcomes so that new interventions could be targeted but they didn't have outcome data instead they trained on health costs how much how much did this patient cost the healthcare system down the line with the thought that patients who are sicker cost more and the it's thought the reason for the bias that the model exhibited right that sort of - similarly sick patients one of whom was Caucasian one of whom was was black yeah the model would tend to suggest more healthcare intervention for the Caucasian patient well the reason was because black patients who were similarly ill tended to cost less not because their health outcomes were better but because they had less access to healthcare so this is something this is the case where you you know you don't necessarily have to deal with a trade-off in the sense that you were you trained your model on the wrong data if you were able to go out and get the right data oven it would it might solve this sort of unfairness problem and simultaneously make your algorithm better at predicting the thing you really wanted it to predict but you know I think this is something that sort of made salient just because people are thinking about in this case fairness but this is like a part of like data science education that that would have been important even if we didn't care about fairness in the sense that you could have made the model even if you just cared about like overall accuracy you could have made the model more accurate by training it on the correct data and you know it's only because you know someone wrote an article in science about the unfairness of the model that it was brought to light yeah yeah that's kind of what I'm getting at is that there are sort of really rigorous scientific practices in related fields you know for kind of you know turning these abstract constructs into measurable variables and this kind of interdisciplinary you know kind of work I think is not really being adopted as much as it should be and I think could really you know I I don't necessarily view it as two entirely separate from the algorithm design I think I really should be integrated and so yeah I'm just glad to hear that I think it's also important okay one more high-level questions so you detailed a lot of different sort of troubling practices prevalent within the machine learning community and how these sorts of practices you know like biases in reporting you know kind of reliance on a small number of data sets these types of things you know these lead both to the reproducibility crisis but also to a lot of you know really sort of ethically questionable you know design and development of algorithms and so I'm curious what your thoughts are on how the community as a whole can start to shift its practices what types of new incentive structures you'd like to see in place obviously this is not a quick fix this is a very long-term thing but I think a lot of us are our members of this academic community and so I'd love to hear your thoughts on you know how we as a group can kind of shift in in a more socially and beneficial and ethically informed direction yeah I mean I guess my quick answer would be you know we do suggest some technical things in the book and we talk about you know the pre-registration movement and things like that which I think we view is too restrictive of a solution but you know maybe to answer that question to make a broader social comment I think it would be good for the field of machine learning to become less like a competitive sport what this is a relatively recent phenomenon and it's you know you know it's I think a byproduct of the tremendous empirical successes that areas like deep learning had and the need for these massive datasets and kind of you know kind of concentrated focused by a large number of people on the Minish you know in an intense period of time and that's all been great and I don't you know I'd you know no no not whatsoever on the actual advances in in those technologies you know log which are large I think in vision speech and and NLP but I think you know and I'll use my seniority here to point out that the field of machine learning you know it used to be that people were considering many many different types of learning frameworks of different learning models there wasn't this sort of uniformity of datasets to the extent that there is now or at least if there were they were really kind of toy datasets that nobody considered like a serious benchmark for developing and deploying services you know it's things like the UC Irvine dataset which he went to check your results on but it wasn't like okay on the UC Irvine dataset I've now developed this service that I'm now gonna unleash on a billion users and I think you know the field I'm hoping will organically balance itself back more to an earlier era where there's not this single-minded focus on sort of one framework for learning and a few datasets and so maybe you know maybe things like pre-registration or sort of smarter leader boards which we do discuss in the book you know maybe that's part of the solution but maybe part of it is just kind of you know a cyclical move back towards and kind of a more diverse research landscape in the field question Jack Dorsey from yeah made the announcement that they're not gonna do political ads at all and machine learning algorithms are being used to target you know users with those ads so algorithmic accountability and fairness is we get questionnaire and you know chiming in with aren't that there is some bit of you know not all algorithms are bad we are trying to make them better so a player leaving the field kind of you know creates this added pressure on the other players in the field to be really accurate about it briefly I don't have deep thoughts on this particular issue I think you know the policy to pull those ads entirely is better than having no policy whatsoever on the other hand I'm not sort of convinced that you know pulling you know things that are designated as political ads eradicate sky nathie penumbra of you know worries that people have around the politicization of social media right I mean you know like I don't think it directly addresses things like you know fake news and and the like but I think it's you know it's better to have a clear policy than to have no policy at all and secondly I do think you know it's good for the competitive landscape of the tech industry to have actors that take stands on issues and try to you know create internal pressure in the industry to sort of think harder about these issues and adopt them I mean you know to give an example you know sort of Apple has successfully you know you can debate whether it's how deserved it is but Apple has carved out a reputation for greater concern about consumer privacy and was an early adopter of differential privacy and I think that that does create kind of an environment where there's more internal pressure from the industry rather just than from regulators thank you I was willing how much do you think that your book is sort of a snapshot of a current moment in time so certainly a win to the made sense to have this book published like ten years ago and how much do you think it is really something that's an enduring set of problems and because of the the list of problems we understand less and less it's really creating an outline and a framework that's going to - you know have a significant impact over time so as we say at the beginning of the book like this is an emerging science and you know you might reasonably think that that means it's sort of too early to write such a book but we think that it's sort of exactly the right time because it's when it's when the ideas are developing that that somehow those like intellectual process of like thinking about them is most exciting so I certainly think that especially as you go down that list maybe even already you know fairness which was the second thing in the list that if you look at what the technical approaches are gonna look like you know 15 15 years down the line they might be quite different from what they look like today but I think that the basic the basic premise of what needs to be done and that's been sort of successfully carried out from white board to you know to product to national scale deployments for privacy is enduring which is that you know what you need to do is you need to think very hard you know in a in a rigorous precise way about what you mean when you say you want algorithms to be bla where block and represent any you know any any any word you want where you know a human being would would just know what you meant if you if you told him you wanted you know accountability fairness transparency but that is not obvious to an algorithm and then you have to after you come up with a plausible definition and coming up with the definition is the hard part but when you come up with a plausible definition you have to think about how to you have to think about the scientific problem of how to design models satisfying that definition and think quantitatively about trade-offs because typically these things don't come for free I think that general methodology is is going to have to be enduring and that even if the specifics of how people are thinking about these things 15 years down the line are gonna be different they will be thinking about these things I'm gonna throw in this Dory question really quickly what skills are there other than computer science that are most needed for work on ethical algorithms what advice do you have for successful interdisciplinary collaborations let's see I mean if by skill we mean sort of an academic or specific technical skill I think it's more I think what I would advise most is sort of a willingness and actual interest in talking to people in adjacent fields that think about the same issues but from a non-technical perspective so I think we benefited greatly for instance in conversations we've had with people with the law school at Penn who think hard about fairness and privacy including and technological settings from a legal perspective and just kind of understanding their views and is also especially understanding the constraints that come from their world and you know and and in talking to regulators it's quite revealing to talk to tech regulators and realize the handicaps that they face right so there it's you know these are smart people but these are smart people kind of with many many shackles on what they can and can't do that really kind of force them to lag in many ways the companies that they're regulating and so I think it's important to in working in this area even if it doesn't like oh you know you talked to some regulator and then you've got a research idea that you then you know go work on to just kind of understand that landscape more than any other particular field outside of CS or machine learning one thing that I've seem to notice is that when people notice that say an algorithm has maybe not fair then what algorithm designers I mean even society at large 10:00 to 2:00 is to come up to think of quick solutions on how to fix it when the solution itself may not inherently be fair to take an example for instance the the college selection slides that you've showed where you have two populations and two clearly the initial solution of having a general cutoff was a problem but like yugioh suggested one thing that could have been done as consider two populations i'm separately which seems like an okay thing to do people might even agree with it but then when you look at the data you do see that even the green population there may be some data points which are positives but they fall within the cutoff range of may be population - and I do think that this happens a lot in real life - that you know the easy solution is to maybe just say that one very easily observable variable this population type but maybe the actual hidden variable that you to consider is maybe like you mentioned maybe the income you know maybe couldn't takes out three times a sort of one so today what I see is maybe people from the green population perhaps as a result if there are somebody who falls for the cutoff then completely screwed as well yeah so I'm wondering as algorithm designers what are the things that are there how can how can this problem be solved because it seems like we are trying to optimize for maximum efficiency you have the curve but then as a result there might be some fraction of the people that always get left behind although overall it might be the most optimism yeah see I think you've put your finger on one of the main weaknesses of these statistical notions of fairness and then we talked about this a little bit in the book and it's it's actually one of the main focuses of our of our research so and by the way this is why we think of you know maybe the fairness and machine learning field as an academic field as you know 15 years behind privacy's so the claim is not that any of the existing definitions are very good so that I think what you're putting your finger on is when you look at these statistical notions of fairness that say things like well I'd like the false rejection rate to be similar between you know like orange people and green people say well you have to like the first step of even like enunciated Nath was you had to say okay well there are like these two groups I care about orange people and green people and usually it's not so easy and just because I you know guarantee some notion of statistical equality in aggregate over two large groups doesn't mean that the solution that we come up with is fair in various technical senses to you as an individual or even to even two large groups of people that you think of yourself as a member of if they weren't the exact groups that we specified up front so there let me just without saying too much about it yeah this is an active area of research there are things you can do there are fairness notions that are somewhat more satisfying than these they don't require and I'm seeing you know a small number of like pre-specified coarsely defined groups upfront there are ways to talk about fairness at an individual level and maybe we can talk a little bit offline but these are this is sort of the research frontier like we don't understand that much about methods that guarantee protections of this sort and their implication so it's a very good question and I'd say like there's people thinking about it you should go off and think about it's like it's not it's not a settled science yet thank you so much [Applause] you 