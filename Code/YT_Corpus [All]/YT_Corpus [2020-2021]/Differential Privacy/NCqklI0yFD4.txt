 Hello and welcome back to privacy at Michigan. I'm Christian Sandvig and I'll be a panelist, and also the moderator for this next session. Thank you so much for joining us. It's quite a distinguished group. I was noticing when I was preparing to introduce them that every single person on this panel has either the title of director or chief. I think that really means something. We brought you the best. So I'll introduce them. What's that?  Quick.  Quick. Yeah, thank you. I'll introduce them in their seating order, actually. So I'll start, it's the same. Okay, I'll start with Jag. So Professor HV Jagadish is the director of the Michigan Institute for Data Science and also the Galler Collegiate Professor. He's a fellow of the ACM. Because our panelists are so distinguished, I'm only gonna highlight one or two interesting things about each one for the sake of time. So I mentioned that he's one of the original signatories of the principles for accountable algorithm statement, because that's a personal favorite. Immediately to his left is Professor Lisa Nakamura, the Director of the Digital studies institute here at Michigan. Also the Gwendolyn Calvert Baker Collegiate Professor and a professor of Women and Gender studies English, film, TV, Media and Asian American and Asian Pacific Islander of studies. She's the author of the books, Digitizing Race and Cyber Types. Immediately to her left, we have Associate Dean and Professor Paul Resnick who directs the Center for Social Media Responsibility here at Michigan and is the Cohen Collegiate Professor and the Associate Dean for Research and Faculty Affairs at the School of Information. He's also the author of building successful online communities, as well as being an elected member of the CHI Academy that stands for Computer and Human Interaction. One more to the left and we're almost home, that's Professor Kayte-Spector-Bagdady who is the chief of the research ethics service at the Center for Bioethics and Social Sciences in Medicine and assistant professor of obstetrics and gynaecology at the medical school. I was impressed by the fact that Professor Spector-Bagdady was also the Associate Director of the Presidential Commission for the Study of Bioethical Issues under President Obama. Certainly the least distinguished out of this group, I am the director of the new Center for Ethics, Society and Computing at Michigan as well as a professor of information, communication, and media. So now that we have all that, the structure of the session will be as follows. I've invited this delightful and distinguished group to each. Give us a five to seven minute statement about the topic of this panel. Multidisciplinary voices on privacy and ethics, and I also have a five to seven minute statement for you. Hopefully five to set the tone and I'd like to start that statement with a personal anecdote. At my previous job, I took an appointment at a college of engineering and I'm not an engineer by training. I'm a social researcher. And in the hallway, the director of the unit where I worked saw me and he was introducing me to some visiting dignitaries. And he said, you have to meet Christian. Here's Christian. Christian is our resident artist. And I thought to myself, that's funny. I mean, I like art quite a bit and later would be more involved in art. But at that time, I had no involvement in art. And I said to him later, well, thank you very much for that. But I usually consider myself a social researcher. I'm a social scientist, my degrees in that. And he said, social researcher. That's right. I always mix those two things up, social researcher and artist.  [LAUGH]  And to me, I thought it seems very different. It doesn't seem like the kind of thing that you'd make up and I love the man dearly and respected and greatly, but I think there was a certain perspective in some parts of the academy that real interdisciplinary would be like what if you got the networking people from electrical engineering to talk to the networking people from computer science? And that would be wow, we're really busting through barriers.  So our topic today is cross-disciplinarity and I thought I would reflect briefly on that to start with. It's kind of a funny word, multicross-disciplinarity, interdisciplinarity, all these prefixes actually mean different things if you're pedantic and why not be pedantic? This panel is about multidisciplinarity. These are all words that you see valorized in a variety of venues if you're an academic. So if you go to the National Science Foundation or the National Institutes of Health and you want a grant, you'll see a bunch of language about how important it is to do all of these things. The inter maybe not so much, but definitely the cross and the multi and it's funny though. A lot of times, if you've worked in this area long enough, it starts to feel empty. Their valorize for no good reason. It doesn't seem like there's that much thought behind it. So my premise to begin with would be to say I don't actually think that any of us believe that disciplinarity is by necessity in all cases bad and something that we should avoid. Similarly, we shouldn't valorize any of the cross, the multi or the inter just for itself. There's something more that has to go on to make that worthwhile. So let's think about what that might be. One of my favorite quotes about cross disciplinarity is by the anthropologist Clifford Goetz and he said that there are two kinds of cross disciplinarity in the world. The first kind is cross disciplinarity like a bandit a nd what that means is that you as a member of a discipline, raid another discipline and take an idea, but after you've stolen it. This could be great for you. He didn't mean it to be negative. After you've stolen it, there's really no lasting connection the way there's not. Hopefully a lasting connection between a criminal and a victim. So the bandit has stolen the idea and then the idea has a new life in the new discipline. We encounter this whenever we talk to people from other disciplines and we find them using words we think we know, but it turns out that they're using it in some different inflection or connotation. So he said that, that was great for you personally, but the second kind of cross-disciplinarity is much harder and that's the cross disciplinarity to form a lasting connection where there's actually some sort of you could call it a pipeline or a channel or a conduit between two different communities. And the communication between them is reciprocal and ongoing, and it's a huge amount of labor was his point. It's not something that happens by itself. And often, it's not worthwhile like go ahead. Be a bandit, steal an idea from another discipline. Help your discipline. That's fine, but there might be circumstances where you actually need to invest this extra effort to produce lasting connection between two different fields. I'd say that computing, which is my interest and the topic of the center we're starting here at Michigan this month. Computing is one of those areas. I think that in computing, we're in what's been called a techlash or certainly a backlash. Privacy is to blame for some of this. I mean, you could reference the Cambridge Analytica scandal for turning public opinion or many other things. Mark Zuckerberg seems to be in the news every other day. They always choose the most unflattering picture. Feel he can't look that bad. The press really seems to hate him. If you think about this scenario, I think one explanation for it is that it might be that the practice of computing at least in the industry. I don't know about The academy, the practice of computing became too insular. It was devoid of some of these lasting, difficult connections between other fields. And I came to this idea, not as an industry practitioner, although I did that a while ago, but rather as someone who teaches. Because computer science students come to me primarily through the UROP program here at Michigan, those of you who know that program. And they say things to me like, this is so exciting because I didn't know what you're studying, which includes privacy and ethics, was related to computing. They didn't see this as related or having anything to do with computing based on their coursework. And they're delighted by it, and they're excited by it. But I think the message for me as an educator is that perhaps we let some of these areas of endeavor become too isolated. And they were too isolated from the hard work of connecting to, let's say, social researchers, or perhaps art to take my former boss's point, perhaps art. So at the Center for Ethics, Society, and Computing, this is something that we're trying to put at the forefront. I can't tell you how it's gonna work out because we launched last Friday. So I don't know exactly how it's gonna work out. But I'll give you a clue as to what we're trying. The center stands for ESC, excuse me, the Ethics, Society and Computing spelled ESC. You may have noticed that's the word escape. Escape is a key on a computer keyboard. Computers didn't use to have that key. It turns out that it was added when people realized that the operator needed a way to stop the computer from doing something they didn't want it to do. So we take that as both our name and our mission. And to get that mission, one of the strategies that we're pursuing is what I call radical cross-disciplinarity. This came from a University of Michigan initiative called the Humanities Collaboratory, in which Michigan faculty received bonus points in a funding exercise for being more cross-disciplinary. Now we all respond to incentives, and so we thought that sounds like a dare. Let's see how cross-disciplinary we can get. We can get all these points, and thus, secure this money. So my colleague, Sophia Brueckner, in the School of Art and Design and a former Googler and I assembled a team that was staggering in its breadth and sometimes terrifying. Including a musical instrument designer, a practicing artist, an architect and an artist, an industrial designer, someone who does cultural studies. And this is our team to address problems of computing and ethics. I'm not sure how it's going to work out. As I said, this is just the beginning. But one of the observations I'd make from it is that it has been some of the most intellectually rewarding experience that I've had here at the University of Michigan as a researcher. So I'll close by saying what are the prerequisites for that kind of positive experience? For me, an intellectual experience so far, although I hope to get you some output soon. One of the prerequisites is open-mindedness. One of the prerequisites is a willingness to be uncomfortable with sometimes radical differences in process, in meaning, in objectives, in assumptions. And another is a willingness to put in the work. I mean, we all have a lot of work, so no one sitting here probably wants more work. So I'm telling you the work to be cross-disciplinary and it's maybe a downer because you don't want any more work. But I think that's the future of solving difficult problems in the university is identifying where these lasting cross-disciplinary connections must be made. And investing the labor in time and in trust and an open-mindedness to make it happen. I'll stop there with my framing remarks. If you subtract the introductions, I'm on time. Just wanna point that out to the panel. And now let's precede each of the panelists in turn from your view, from the left to the right, will speak. And I would like to invite our first, well, I guess I was a speaker, our second presenter, Jack.  [APPLAUSE]  So thank you, Christian. So I'm a computer scientist. And in the spirit of interdisciplinarity, I am going to try not to use the word computer or computing in the next five minutes. I should spend just a few seconds talking about the Institute for Data Science, which tries to do all things data and facilitate people doing all things data. Including analyzing it and understanding the social implications of what they do with that analysis. And, Just to put things in perspective, when I say all things data or data science, this is the collection assimilation, the analysis using whatever algorithm or AI, or the C word that I shall not use in the process of doing that. The point I wanna talk about today, though, is just privacy. And I just have one point to make, which is how you think about privacy really matters in terms of scale. And this is not a quantitative difference, this is a qualitative difference. So I wanna give you a couple of examples. So if you're walking in public, you expect that every now and then, somebody will take a photo on the street, and that'll happen to capture you. And that is just part of what you expect. And I'm sure that has happened to each of us at some random time and we're in somebody's photo album at the corner of some picture, right? Now, let's go one step beyond that and say you have a nosy neighbor who's always sitting at their window and watching you go in and out of your house. And that's mildly annoying. You have somebody who knows exactly when you're home and when you're not. But they don't know much else beyond that just from the watching you go in and out, okay? Now you take that yet another step and say you've got a cellphone app that's tracking your location all the time. Now you've got a serious problem. Okay, let's look at this in a different example and see the same thing. If you do a web search or you post something once on social media, now you post because you wanna share, okay? You do a web search, almost any single web search that I've done is something that I'd be very happy to share in public and have you know that I did this and stand behind the fact that I did it, right? However, the totality of all of my web searches reveals a lot about me and is something that I certainly want to keep private. So Well, one other example I wanna talk about is this notion of de-identification. Our regulations with regard to privacy assume that if you take away personal identifying information, that things are all good. And that's absolutely no true. If you have a de-identified record without any identifying information, you probably cannot re-identify it if you view single such record in isolation. However, if you have enough history or if you have other correlating information, it is highly likely that you'll be identified. And so simply saying things are de-identified doesn't mean that you can share stuff. Right, where technology is today, we are not very far from being able to identify you from your writings just based on your public writings, okay? So this is stuff that in the academic world, people are able to do. So there are fields in which you have anonymous reviewing. If you've shown me enough of your published papers, I get your paper for anonymous review, I can identify you if I so chose to do. Okay, this is stuff that for general writing we're not yet able to do, but we're pretty close. Okay, so what this tells me, as a consequence, is that our traditional norms are outdated. When we think about what is ethical to do, or we try to reason about the law, we're usually reasoning by analogy. You're taking something that we understand, something that we say we already have a social contract on in society, and say, this is acceptable, this is not acceptable. Okay, if you are in a different universe, you can not reason by analogy. And scale is a qualitatively different thing. And so you cannot take stuff that you were reasoning with and just say, even though things are different with scale, what I said at small scale, or what we had all agreed as small scale, continues to apply. And that means that we have to fundamentally start from scratch and develop our own shared societal code of norms, a share societal code of ethics. With regard to how we wish to live and operate in a world where we have scale of information and with the attendant pressures with respect to privacy. We have a lot of discussion in terms of regulating a few big companies and things like that. And to me that kind of feels like stuff that, yeah, perhaps should have been done, but is fighting yesterday's battles, okay? We really should be thinking about, fundamentally, where are we going with this? And how should we be thinking about information? If we can identify all of your writings and you cannot write anything anonymously, this is not a problem with Google or Facebook. Okay, this is a question of how we exist as a society. And that's really what I would want us to think about. And just to close, as a part of getting people to think about this, I have a MOOC on data science ethics. And in yellow at the bottom of the screen is is a link to that. Thank you.  [APPLAUSE]  Hello, can everybody see me? I feel like I'm kind of low here. So I'm gonna be talking about what happens when ten humanist information scientist, feminist scholars, media scholars, ethicists and information studies scholars get together to try and make some new principles to govern privacy on the Internet using feminism as a tool. So, I concur completely with what Jag said, there do need to be some new rules going forward. I don't believe any of us thought we were ever gonna be so exposed in regards to what we're doing in every day life. So, the Digital Studies Institute, which I am the director of, offers an undergrad degree and a grad degree. And we are dedicated to bringing a humanistic approach to the big social problems that the digital has created, as well as some of the social goods. One of these has to do with the exploitation of data across unfair and differential lines. Some people give up a lot more privacy than others so as to get access to services, say. Or to be able to be on parole and not have to be in prison, by wearing an ankle bracelet which monitors everything that they do. So before I get into three pieces of what we're calling the feminist data manifest-no, I know it's a bad pun, I wanna talk a little bit about why we wanted to write a manifesto. Manifestos are not arguments. There's no evidence here. They're just a set of claims. And we thought a manifesto would be a useful thing to have because the rules have changed so much and we think feminism can be helpful in this regard. And so here's the story of how the data feminist manifesto came to be. It was written last summer as a result of a two-day collaboration funded by the Institute for Research on Women and Gender at the University of Michigan. Which brought together a group of people doing work in feminist data sciences and data studies to create something, to have a conversation. So the group decided they wanted to take all of the skills and disciplines that everybody had to write a set of rules that work across disciplines. So I'm presenting on behalf of the collective. I'm not the author, I'm one of ten authors. Here are the authors. This is annoying, okay. So, During the early part of the 21st century, I believe people were less concerned about social control via data than they needed to be. And we are now, I think, realizing that we traded what we thought was safety for control over some of the data that we are providing. The Feminist Data Manifest-No has 20 pieces. I'm gonna show you a QR code which links to the whole document. But I wanted to focus on three in particular which I think are relevant to this conversation. So I'll just read this one. We refuse to operate under the assumption that risk and harm associated with data practices Can be bounded to mean the same thing for everyone everywhere at every time. We commit to acknowledging how historical and systemic patterns of violence and exploitation produce differential vulnerabilities for communities. That is kind of jargony I will say. However, it does refer to things like say, in countries like Turkey this is set as ethnographic research. Medical professionals are now texting data about pregnancy to women because it's convenient to do. But oftentimes those phones are controlled by men, say by their fathers, right? So this is a form of, a systemic pattern of violence, inequality, oppression, which existed before cell phones, before medicalized data. Yet is continuing to perpetuate different kinds of vulnerabilities actually for the same kinds of people. I think it's well known now that some people benefit more from algorithms than others. Some people are unfairly characterized as criminals. Some people are deferentially policed, and they do tend to be the same people, people of colour, queer people and so on. We refuse a data cookie culture of ultimatums, coercive permissions and block access, that everyone can safely refuse or opt out without consequence or further harm. We commit to no being a real option and all online interactions and engagement with data driven-products and platforms, and to enact a new type of data regime that knits the no into its fabric. The last time you updated your software, did it ask you, do you wanna update now or in 24 hours? That is not really a choice [LAUGH]. It looks like a choice but you can't really say no, right. It's knitted into the fabric of the way updates work often times. So what if we had a new kind of software. A new kind of data regime that had a no option in every case, right? What if you didn't have to provide, every kind of data that you're asked to provide to use a certain service. What if you didn't have to agree to updates or things that you didn't really want that might expose you in ways you don't want. And the last one I'm gonna talk about today. We refuse to accept that data and the systems that generate, collect, process and store it are too complex or too technical to be understood by the people whose lives are implicated in them. We commit to seek to make systems and data intelligible, tangible, and controllable. When's the last time you took a good look at the terms of service of any social media platform or even game that you were playing? Pretty much, no one reads those because they're not meant to be read, right? They are way too long. They are written in unintelligible language. They're purposely difficult to understand. And yet, you have to click them. There's no, no to them. You have to click them to use the services. So, what if people had to write terms of service that were actually intelligible. What if people understood what it was they were agreeing to when they decided to use a new app or to log on to a new website? Things would look very different I think, if people had or given access to legible and clear and accessible explanations of what was happening with their data. So, I wanna leave time for questions. Manifestos do tend to be kind of shouty, and I know this was kinda shouty. And there's probably lots of nuance and challenges and things that it brings up. But I think that was the point of writing it, right? Was to be as up front and maybe radical about what could be done around data to provide a more feminist future. Thank you.  [APPLAUSE]  Did you get the picture? Okay. I wanna introduce our Center for Social Media Responsibility, whose mission is to help media platforms meet their public responsibilities. We got started in January of 2018. I will get to that. So, Garland Gilchrist was our founding executive director. He was with us for eight months until he left to run for lieutenant governor. The position he now serves as lietenant governor the state. We're very proud of him, but it caused us to sort of reboot our center. In May of 2019, James Park joined me as the Assistant Director. Our approach is not so much connecting disciplines, but connecting the academy with the industry.We have two kinds of things that we've been doing. One is metrics, and the other is translational research. And I'll give you some examples of both. Mostly not in the privacy arena, but I will promise I will get a little connection to privacy at the very end. So the first metric that we came out with is called the Iffy Quotient. We've been getting data for each day since beginning of 2016. And each day, one more data point on this graph. There are two lines here, one is for Facebook and one is for Twitter. It's a metric of how much of the popular content on these sites is coming on these platforms, Facebook and Twitter, is coming from sites that aren't very good in their journalistic practices? Euphemism there. Places that have misinformation. So we have a sort of a market research firm that gives us data about what are the popular URLs, we get 5000 popular URLs from each of Facebook and Twitter each day. We then classify the sites that those came from based on primarily service news guard that is employing a bunch of journalists to access the journalist who to practice off sites, those sites that news guide hasn't rated we fall back on a second categorizer. And if it's a site that doesn't have good journalistic practices NewsGuard calls it red. We're more tongue in cheek and we call it iffy. And then we just compute the fraction each day of the of those URLs that that were popular, what fraction of them came from the iffy sites? So and then you get this graph and you can see if I go back a few, you can see over time that during the run after the 2016 presidential election, a big increase in the fraction of iffy content that people were consuming on Facebook and Twitter. You can also see that Facebook had a lot more of it than Twitter. But that hasn't always been the case there was a sort of steady decline from early 2017 through about September of last year. There wasn't another big run up in the 2018 congressional elections. Looks like we might be in the midst of an uptick again since September. And also we see that Facebook and Twitter reversed their roles further now has more iffy content is popular than Facebook does. So this is an example of a kind of accountability metric that is, I call it friendly accountability. It's something that holds The platforms feet to the fire, but it is something that they can both win and lose at, unlike the sort of accountability by anecdote, or by gotcha that is more prevalent in the press. And I think this is one of the roles that the university can play is to capture these metrics over time. So, a second one metric that we're working on coming in hopefully second quarter of this year is gonna track conversation quality. We're scraping the comments about news stories from Reddit, from Twitter, from YouTube, and then passing those comments through classifiers that classify are we having good public conversations or not so good? And we'll again, be creating a metric that will be able to update over time. In the third quarter, we're working on a polarizer index for YouTube. You may have seen some journalistic stories of, hey, I created a new account on YouTube. I started at this innocuous video and lo and behold, a few hours later, I was following the recommendations and I was getting radicalized into a white nationalist group. We're trying to see if we can apply that same systematic methodology to, is that really what happens all the time or was that an unlucky thing that happened for the one person who was trying that? So those are things about metrics, the translational research is more aspirational at this point, but I wanna describe what our aspiration here is. A lot of academic research happens where we get to the point of publishing something, and that demonstrates a proof of concept that you could answer one question, one time. You could create a tool that somebody might find useful and you did some user study. There's a whole lot of work that would have to happen to make that tool be useful on a continuing basis by journalists by end users, by the platforms and we're aspiring to do that last mile. Take things that have had their academic success and turn them into something that has public impact. So for example, in the realm of content moderation, we have faculty who are working on various projects involving new content moderation techniques. When they've demonstrated that they have some promise, we hope to take them to the next level. Things around algorithmic fairness and how do you conduct an audit. A class of things that I'll call User Mirrors. Things that give users some reflection on What they have done, and this is one example that a doctoral student did almost ten years ago now. It tracks what news articles you read and it gives you feedback of how balanced were they. And this perhaps me reading only liberal stuff and falling off the tightrope here and being quite scared, the stick figures happy when you're reading some more balanced collection. But it's not only in this realm of what news you're seeing that that you could have a mirror that would tell you something. Think of privacy tools that way, I think user mirrors might be quite useful in the privacy realm, but also things that are actually developing new controls, new ways of controlling privacy. And we have faculty like Florian who are working on seeing, can people use this tool? Does it make sense to them, is it something that they can do? We aspire to take some of those things out into the public once they've sort of demonstrated a proof of concept, so thank you.  [APPLAUSE]  Thank you so much, hi, I'm Kayte-Spector-Bagdady, and I'm faculty at the Medical School, where I do many of the same things that my colleagues have been talking about all afternoon. But with a focus on health data and health privacy and biospecimens as well. So, just to give you a brief overview of the center that I work for, so I work for the center for bioethics and social sciences and medicine which is situated at the Medical School. Dr. Zkmund Jacksie is our Director, and we have focuses on clinical ethics, so how we better take care of our patients, education. How we build the next generation of doctors, and research ethics, which is what I lead, which is how we can take better care of our participants and do better research that will then feed back into taking better care of our patients. So this is just a brief overview, we have both a research and a clinical ethics service 24/7, 365. Somebody carries a pager for ethics, and you can actually have an ethicist show up in the room and talk to the family, talk to doctors, talk to lawyers talk to the team. So we do that both on the research and clinical side, so that's our consultation. And then we do a lot of work on policy and research as I mentioned. I'm gonna talk about the research that is near and dear to my heart, because they put the microphone in front of me. So I'm gonna talk about health data privacy, which is what I focus on, and I'm gonna make three observations. My first observation is that biospecimens and health data are governed under law by method of procurement, right? And this the point that Katie made this morning, so those there silos that we see are data and biospecimens that are coming in through clinical care which are governed by HIPAA and the privacy rule. Data and biospecimens that are coming in through our research, which is governed by the common rule, or the human subjects research regulations. And data that's actually coming in through industry which is governed by not much at all potentially the GDPR if it's European. Potentially California if it's under the new California regs, but really right now it's just under the Federal Trade Commission. And those terms and conditions and whether they're lying or misleading. So, these are how our current laws are structured to govern these types of health data. That is in contrast to what people care about, people don't really care so much how you got their data, people care about what you do with their data, right? And it doesn't matter whether I walk into the University of Michigan or some other institution with my patient hat one, versus my research participant hat on. I likely am one person and I likely feel the same way about privacy, no matter how I'm walking in the door. And we've actually done some really interesting work here to sort of figure out what are the margins of what people really care about. We know people really care about when their data changes hands, when we're commercializing their data. When we share it or sell it to Google, when we share it or sell it to pharmaceutical companies. And we also know that people really care about certain kinds of research. So my colleague Ray De Vries who is the associate director of our center did this really cool study. Where they asked people coming in as patients whether they would sign his broad consent form, that can send it to sort of any kind of future research. And about 70% of people said, note that this is people who've already agreed to enter into our research, right, so it's a little biased. But about 70% of those people said, sure, use my health data or biospecimens for whatever you want. And then the researchers went back to them and said, really? What about how to improve weapons of mass destruction? Really? How about how to research a better abortion drug? And then 70% of the 70% who had said sure said, can I actually change my mind? And we find this again and again, when people really understand what they're consenting to, they're immediately worried it's that they don't understand. The problem is that whether they understand or not specimens and health data that are procured differently that are coming in through these three different silos. End up being used for the same things. We're all trading data around, right? And Lisa was talking to us about, wouldn't it be nice if we could say no to a computer update or to a terms and conditions. Wouldn't it be nice if you could say no as a patient who needed urgent or any kind of clinical care at a hospital to future research with your health data or specimens? I bet some of you didn't even realize that you were saying yes. And you can't say no. So just as one example, this is a piece that we recently did with my colleague Nicholson price in the law school about the sort of updates to the GDPR and the limitations of law to actually handle this. And this was sort of highlighting the issue of mosaic thing, which we've also talked a little bit about, right? Because you as an individual might be comfortable sharing information about, for example, how far you've run with your feedback. How much weight you want to lose with your weight tracker app. Your political views on Facebook, your children on Snapchat, who you're interested in dating on Tinder, but we continue to think of these data sets as siloed. But through the concept of mosaic King, they're actually so this is about shadow health record companies. There are private industry companies whose job it actually is to gather all of those data and reassemble it together and sell it to people to do research or target advertising. So what are we doing about this at the University of Michigan? I'm gonna highlight one thing is that newly Dean, Dr. Dean at the Medical School Put together a Human Data and Biospecimen Release Committee that I'm the ethicist on. Which is actually involved in all of our medical schools' data and specimen commercialization with third party entities. So now when the industry is clearly sponsoring the research to begin with, and you know that you're enrolling in an apple trial or this pharmaceutical trial. But all of those individual agreements that investigators individually used to enter into is now coming under the purview of this committee. And we're ensuring that participants were notified and consented to that data sharing, so that is one drop in a very big bucket. But these are the kinds of things I continue to be concerned about. And I think health data is a really important component of what people are particularly concerned about. People are used to having privacy in their health data. And a privacy and the health data is being waived just as drastically as your privacy in everything else. Thank you very much.  [APPLAUSE]  I'd like to thank the panel for a provocative set of presentations. If you have questions for the panel, we have two mics. We have two. Yes, we have two mics and please stand up, I would invite you to ask questions of the panel that are about their individual presentations. And also perhaps if you're interested to ask questions about the topic, which is about cross disciplinary. I mean, it strikes me as I said in the opening that we have a very diverse Group of backgrounds here representing a variety of schools, and departments, and degree programs and topics. So something about that challenge will also be welcomed. I think I'll start by asking the panel to reflect on that, myself, and that, what do you think that your disciplinary position, has the game from any kind of cross multi or interdisciplinary effort? Given that, you're on this diverse panel with people with different training than you and different institutional locations? Would anyone like to answer them?  Sure  My discipline computing is the one that Making all of these things change. And we'd like to do it in a way that makes the world better for all of us. And if we aren't talking to people in other disciplines who can help us better understand the consequences that can happen.  I can speak directly to that.  Yeah, I'm in the Department of American Culture which is where the ethnic studies departments are. And when I heard this on Paul's slides about misinformation and disinformation. I thought about the rise of the white supremacist movement, and which was mostly around those specific platforms, and how racism is really agenda a lot of times and push out a greater rates than other kinds of thinking. So that's clearly a huge interest to people who care about racial equality.  I'll answer your question. I'll also pose the first version of the question, which is. So I roughly come from a computing background, but I've been hanging around the social sciences for 20 years. So I guess I'm sort of a computational social scientist. I would say the big thing that the computing field gets is the questions that need to be answered and sort of I'll leave it at that. That's maybe mostly cuz we are pretty good at answering questions and pretty lousy at formulating questions. I just wanna think about the flip side cuz I've been thinking about it a lot, which is, you ask what does my discipline have to gain. And I think we need to do a lot of thinking about what does that have to give as well. And sometimes we're not very clear about that. Massive disciplines and also, since I've been doing a lot of thinking about how does University connect with the industry, getting clarity of what is it, we have to give that that industry has trouble providing. And that's some answers, like credibility of a long standing institution, and we have independence. But it's worth thinking about those things of we had to get, I suppose. What we want to get.  Yeah, and I think that this is a really important question. I guess I would also answer it in both ways. I think from a positive viewpoint, so my disciplines are law and ethics. And I've been Assistant Professor of Obstetrics and Gynecology, if anybody noticed. And so what I hoped for lots of gain, and I hope for large gain from interdisciplinarity, is more in-depth medical and scientific knowledge about what we're writing the rules about. And this was an experience that was very personally related to me. When I was working for the Obama administration, we were suggesting new, cutting-edge technology policies. And my assignment would be, go understand what, These synthetic biologists are doing and how we can better regulate this. Go understand what these geneticists are doing with whole genome sequencing, so we can better regulate this. And I was tasked with reading hundreds of articles. And I wasn't trained necessarily to do that, which is what really drove me to come to a medical school to try to do that work with the right backgrounds. I would say that my criticism of this and I think that this is pervasive across academia is that despite the fact that we encourage, and we incentivize multi and cross disciplinarity. We still don't have the sort of structure and metrics to actually support that, right? As an example, I'm on the tenure track into medical school. And I am held to the same tenure metrics of external funding and publishing empirical research that my MD colleagues are. And wish me luck, I've got three more years.  [LAUGH]  But so great, I'm up to the challenge but that is something that might disincentivize some people from trying to really be interdisciplinary.  That's a great point to take up on. I think that we could just as equally have started this panel by talking about cross disciplinary fiascos. I mean I mentioned that the University incentivized crossdisciplinary work in one case. But there's many other barriers to it, and I think we've all been there. I mean, personally I find some, I hope my skepticism was also conveyed by my opening remarks because I've been in many rooms where some sort of discipline crossing was attempted. And it led to something that you might call, a colleague of mine used to call station identification. Which betrays his age I think, which is that he would just say, everyone just says well, I'm a sociologist and in sociology, we do this and then you just do that forever. And I kind of endless loop where everyone says their positionality or their subjectivity or their discipline, and nothing ever gets done. And so how do we actually make meaningful cross boundaries, contributions to research privacy, ethics, the topics of this conference? Does anyone have an anecdote or example to share, either on the plus side or the minus?  I have one. I remember a couple summers ago, Jack and I were having coffee and talking about the role of computing in creating culture. So over the summer, there was a viral video of an Amazon Web Services Developer Conference that was disrupted by an ICE protester. So she was saying, tech won't make it. Stop making systems to make it so difficult for people who are victimized at the border and they're kicked out of the conference. Person in the audience shouted, we're just software developers, leave us alone. So I thought these guys really don't think they're making culture. They think they're just doing programming but they're not and they were not creating a culture right but they are in fact doing it. So all of these things that we have to consent to were probably last minute decisions at two in the morning by three guys who were high on coffee. And they just never get revived because they're not seen as culture, they're just seen as a tool. And if a tool works, why bother? Why worry about it? But I think as you were saying, the issue of scale. One tiny kind of the stake can become an enormous issue because it starts so early. It doesn't get taken care of later on, it's not seen as a cultural way.  Anyone else want to pick up on my invitation to fiasco? No one's listening.  It was a fiasco [LAUGH]. Having [INAUDIBLE].  No, I wanted other fiascos.  Yeah.  Nothing bad has ever happened to me.  [LAUGH]  Please.  I guess this is less of a fiasco more of a potential for averted fiasco that interdisciplinary can bring to the table. And we see this in ethics and apparently, it's somewhat of a thing that people don't like calling me Office of General Counsel and talking to your lawyers. And this is surprising to me because lawyers are lovely, but I've heard that somewhere nervous calling the Office of General Counsel. You don't have to out yourselves here but are very comfortable calling ethics. And so one thing that has been enlightening to me is that I will get a page as the emphasis on call and I'll show up and I'll say wow, we have a legal problem. Have you talked to the lawyers and people like you call the lawyers for us and I will help call the lawyers. And that is something that, the lawyers at the Office of the General Counsel are here to protect you. We've seen so some evidence to me in your professional capacity, but it's not to everyone because many people grew up that lawyers are there maybe perhaps a doctor to get them. So that is something that I have learned a lot about and I think can be helpful as a method of avoiding disaster.  So I thought of an example, and this is actually where I want you to empathize with the software developers. So, sorry, sorry, didn't mean to give [INAUDIBLE]. Okay, we have, I think, all the Michigan data science team, which is a bunch of very enthusiastic, mostly undergraduate, students who take on various data science projects. And a significant fraction of them really motivated to use their skills for social good. And so the Michigan data science team went did some work in Flint during the water crisis that received a lot of publicity. They've been doing other work less newsworthy perhaps but nevertheless, good work in terms of using their analysis to address problems in Detroit and other underserved areas of our state. The problem is that even if their hearts are in the right place and they have the technical skills. They don't have the disciplinary knowledge of social work or perhaps the people skills necessary that go with this. The consequences of which are that in spite of their being led intention and trying to be helpful. There have been instances of their good efforts being received by the purported beneficiaries very negatively. And this is one of those cases where even if we're talking about something like teaching our students applied data science skills. And doing this in a place where they're performing public service, it is important to have interdisciplinary teams and bring other skills beyond doing the analysis and running the algorithm.  Maybe I just bring this back to you, to the thing that sender's name for example the escape idea, I think it will tie with the why people [INAUDIBLE] lawyers, which is The technologists who were saying we want to do technology for social good, are looking for positive actions that they can take that are gonna have an impact. You framed your center as what should we not do? How do we stop the bad things and I think technologies generally view the lawyers as people whose job it is to say no. And they're looking for counter to yes and I'm happy for you to push me that direction instead of this direction, but if it's no, you're a person that I want to avoid at all costs.  That's a great point and looking at our clock, perhaps one of the last ones. I guess I'll close by, I guess I could respond with the fiasco in two minutes. And I'll say I was actually at an event much like this where the organizers had gone out of their way to invite people from very different disciplines to talk about the future of computing ethics and privacy. One of the things that happened at that event, which I won't name names about is that someone with a qualitative background, presented the results of their research. And the next speaker who had a quantitative background, was very appreciative of it and seemed really delighted by the presentation and said, this is really fantastic. Now, we just need some evidence.  [LAUGH]  The problem for that was that the person who had presented the qualitative work thought that they were presenting evidence. And I think the interesting reason for me that that's a fiasco is, some of the discipline crossing that we're seeing on this panel represents these deep underlying divisions. Like between qualitative and quantitative, or between what we said earlier, what counts as this is a problem, or for example, is this something that can be solved? Do you work in an area where there are solutions that can be found, or do you work in an area where you probably expect human societies will continue to have inequality in the future. It's not gonna go away maybe you can make incremental step. So, I would like to end with that because there is a really interesting paper that someone pointed to a few years ago in education. And it was about how education had become more tolerant of both quantitative and qualitative approaches, the paper is called closing down the conversation. And the argument of the paper was that the education people had all gotten too nice. The way to go forward was actually to find more about the fundamentals because education they claim to, had reached a kind of a deterrence. Where everyone didn't bother to understand what was going on across these divides and simply accepted that whatever you were doing, it was probably fine. So there was a surface politeness, but really no engagement with fundamental issues on which there were still big goals. And so I'm not sure that that's the strategy more fighting. About fundamentals, but it's definitely something that I think about. And so I'd like to end it there. Thank you for your attention.  [APPLAUSE] 