 [Music] right pencil flow federated what's exciting about the FF is that and it enables everyone to experiment with computations on decentralized data and decentralized data is exciting because it's everywhere it's an intelligent home devices in sensor networks in distributed medical databases and of course there's a ton of it on personal devices like cell phones and we love our cell phones we want them to be intelligent this data could help traditionally the way we implement intelligence is on the server so here we have a model on the server the clients all talk to the server to make predictions so all the data accumulates of the server as well so the model the data it's all in one place super easy the downside of this is that all this back and forth communication can hurt user experience due to network latency lack of connectivity shortened battery life and of course there's a ton of data that would be really useful in implementing intelligence but that for various reasons you may choose not to collect so what can we do well one idea is take all the transfer for machinery and put it on device so here we have each clients independently training its own model using on its own local data no communication necessary great well maybe not so great actually we realize that very often there's just not enough data each if you dual device to learn a good model and unlike before even though there might be millions of clients you can't benefit from from the data we can mitigate this by pre-training the model on the server on some proxy data but just think of a smart Keyboard if today everyone starts using a new word then a smart model trained on yesterday's data I won't be able to pick it up so this technique has limitations ok so now what we just give up we have to choose between more intelligence versus more privacy or can we have both until a few years ago we didn't know the answer it turns out the answer is yes we can it's fine in fact it's very simple it goes like this you start with the model on the server you're distributed to some of the clients now each client trains the model locally using its own local data and that doesn't have to mean training to convergence it could be just training a little bit produces a new model locally trained and sends it to the server and in practice we would send updates and not models but that's an implementation detail all right so now server gets locally trained trained models from all the clients and now is the crazy part we just average them out so simple so okay the average model trivially it reflects the training from every client right so that's good but how do we know it's a good model that these procedures is doing something meaningful but you think it's too simple there's just no way no way this can possibly work and you'll be correct it's not enough to do it once you have to earn it so we repeat the process the combined model becomes the initial model for the next round and so he goes in rounds in every round the combined model gets a little bit better thanks to the data from all the clients and now hundreds of thousands many many rounds later your smart Keyboard begins to show signs of intelligence so this is quite amazing it's mind boggling that's something this incredibly simple can actually work in practice and yet it does and then it gets even more crazy you can do things like compress the update from each client - down to one beat or add some random noise to it to implement differential privacy many extensions are possible and still works and you can apply to other things than learning for example you can use it to compute a statistic over sensitive data so I'm experimenting with all the different things you can do with federal learning is actually a lot fun and tff is here basically just said that so that everyone can have fun doing it it is open source it's inspired by all our experiences with federal debt learning at Google but now generalized to non learning use cases as well we're doing it in the open in the public it's on github we just recently started so now it's actually a great time to jump in and contribute because you can have influence on where these goes from the early stages we want to create an ecosystem so tff is all about compatibility if you're building a new extension you should be able to combine it with all of the existing ones if you're interfacing a new platform for deployment you should be able to deploy all of the existing code to it so we've made a number of design decisions to really promote compose composability and speaking of deployment in order to enable flexibility in this regard tff compiles all your code into a an abstract representation which today you can run in a simulator but that in the future could potentially run on real devices no promises here in the first release we only provide a simulation runtime I mentioned that TFM was all about having fun experimenting in our past were Confederate of learning and that's before tff was born we've discovered certain things that consistently get in the way of having fun and the the worst offender was really all the different types of logic getting interleaved so it's model logic communication on checkpointing differential privacy all this stuff gets mixed up and it gets very confusing so in order to avoid this to preserve the the joy of creation for you we've designed programming abstractions that will allow you to write your further reading code that's a similar level as when you write pseudocode or drawing about white board you'll see example of this in later in the talk and I hope that it will work for you ok so what's in a box you get two sets of interfaces the upper layer allows you to create a system that can perform federated trading or evaluation using your existing model and this sits on top of layer of lower level more generic abstractions that allow you to express and simulate custom types of computations and this layered architecture is designed to enable a clean separation of concerns that developers who specialize in different areas whether that be federated learning machine learning a compiler theory or systems integration can all independently contribute without stepping on each other's toes ok protected learning we've talked about this as an idea now let's look at the code we provide interfaces to represent federated datasets for simulations and a couple of datasets for experiments if you have a Karass model you can wrap it like this with one liner for use with tff very easy and now we can use one of the build functions we provide to construct various kinds of federated computations and those are essentially abstract representations of systems that can perform various federated tasks and I'll explain what that means in a minute training for instance is represented as a pair of computations one of them that constructs the initial state of a federated training system and the other one that executes a single round of federated averaging and those are still can abstract but you can also invoke them just like functions in Python oh and when you do they by default execute in a local simulation runtime so this is actually how you can write a little experiment loops you can do things like pick a different set of clients in each round and so on the state of the system includes the model of the training so this is how you can very easily simulate further that evaluation of your model all of this sits on top of HC API which is basically a language for constructing disability systems is embedded in Python so you just write Python code as usual it does introduce a couple abstract abstract new concepts that are worth explaining so maybe let's take a deep dive on and look alright first concept image when you have a group of clients again each of them has a temperature sensor that generates are reading some floating-point number I'm going to refer to the collective of all these sensor readings as a federated value a single value so you can think of a federated value as a multi set now in tff values like this are first-class citizens which means among other things that they have types the types of those kinds of values consist of the identity of the group of devices that are hosting the value we call the de placement and the local type type of the local data items that are hosted by each each member of the group alright now let's throw in the servant to the mix there's a number on the server we can also give it a feather to type in this case I'm dropping the curly braces to indicate that that's actually just one number not many okay now let's introduce a distributed aggregation protocol that that runs among these system participants so let's say it computes the number on the server based on all the numbers on the clients now in tff we can think of that as a function even though the inputs and outputs of that function reside in different places the inputs of the clients in the output on the server indeed we can give it a functional type signature that looks like this so in the FF you can think of distributed systems or components of the stability systems disability' calls as functions simply and we also provide a library of what we call federated operators that represent abstract e very very common types of building blocks like in this case computing an average among client values and putting the result in the server now with all these that I've just described you can actually draw system diagrams in code so to speak goes like this you declare the federated type that represents the inputs to your disability system now you pass it as an argument to a special function decorator that to indicate that in a system you're constructing this is going to be the inputs now in the body of the decorated function you invoke all the different spirit operators to essentially populate your data flow diagram like this at all it works conceptually in very much the same way as when you construct non eager tensor flow graphs okay now let's look at something more complex and more exciting so again we have a group of clients they have temperature sensors suppose you want to compute what fraction of your clients have temperatures exceeding some threshold so in this system in this computation I'm constructing the two inputs one is the temperature readings on the clients the other output is the threshold on the server and again the inputs can be in different places and that's okay right how do i execute this first we probably wanna this broadcast the threshold to all the clients so that our first federated operator in action now that each client has both the threshold and its own local temperature reading you can run a little bit of tensor flow to compute one if it's over the threshold zero otherwise okay you can think of this as basically a map step in Map Reduce and the result of that is a federated float yet another one okay now we have all these ones and zeros actually the only thing that remains to do is to perform a distributed aggregation to compute the average of those ones and zeros in place the resultant server okay that's a third federated operator in our system and that's it that's a complete example now let's look at how this example works in the code again you declared that the types of your inputs you pass them as inputs to the other other arguments to the function decorator now in the body of the decorated function you simply invoke all the Federated operators you need in in the prepared sequence to the broadcast the map the average are over there in that piece of tensorflow that was parameter to the mapping operator is expressed using ordinary tensorflow ops just as normal and this is the complete example so working code that you can copy paste into a code lab and try it out okay so this example obviously has nothing to do it federated learning however in tutorials on our website you can find examples of for implemented predator training and federated evaluation code that look basically just like this they also module some variables renaming so they also fit on one screen so yeah in the in tff you can express your federated lower learning logic very concisely in a way that you can just look at it and understand what it does and it's actually really easy to to modify yeah and I personally I I feel it's it's it's liberating to be able to express my ideas at this level without getting bogged down in all the unnecessary detail and this this empowers me to try and create an experiment with new things and I hope that you will check it out and try it and that you'll feel the same and that's I have everything you've seen is on github as I mentioned there are many ways to contribute depending on what your interests are thank you very much [Music] 