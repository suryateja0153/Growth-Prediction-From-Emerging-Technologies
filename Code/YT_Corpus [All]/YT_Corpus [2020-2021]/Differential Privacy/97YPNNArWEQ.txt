 hi welcome everybody thanks for joining us to discuss automation strategies to protect sensitive data we'll also be going through a case study about these topics that we're going to talk to the beginning of the deck I'm Steve talk a little bit about myself in the next slide joining me is Greg Galloway from artist consulting I'll let him introduce himself when he starts talking but he's kind enough to talk about a case study with a real live customer that where he went through these challenges that we're going to talk about today and how they'd solve them so I'm the CTO Tomita our goal is to enable the legal and ethical use of data we were just recently nominated as one of Fast Company's most innovative companies of the twenty twenty year which is pretty exciting as you can see here we're also a very strong partner with data bricks and AWS the reason I bring this slide up is that you know I've had conversations with hundreds of customers and what we're trying to do in this deck today is boil down the common themes that we see the customers struggle with and then bring that to reality when Greg talks about you know his case study and here's a quick look at the agenda so I'm going to cover all these topics starting with why you should care so if you think about data bricks it is a unified data analytics approach you can really think of it as these three pillars of data AI and people on so your course need the data and then you of course need the tools to do the machine learning and analytics against that data and and it allows you to have one place where all data can occur have your data brought together your people on that data building the algorithms they need to achieve whatever they want to achieve with their data however if you lay governance on top of this a lot of people will kind of throw up their arms and roll their eyes and say oh great this is really going to slow down everything I'm trying to do with data bricks for example you really need consistent controls no matter what language the user wants to use like pythons people are Scala you really want to have a hundred percent of your data live there and a lot of people hold back bringing their data into data bricks because they're concerned about you know the privacy and sensitivity that data which is a challenge and then of course you need to involve leave appliance as part of the people that are involved with this entire framework so they can help make decisions and understand what's happening in terms of policy so I'm actually here to argue that this shouldn't slow you down and in fact it can make you more efficient and get you more access to more data across all your analytics and meet your goals so how is that possible so the first thing we're going to cover from that angle is some different architectures that exist in data bricks to to manage this sensitive data so before I dive into those this is a quick overview of simplified snapshot if you will of how you do access control so there's of course table level controls so this would be think of these entitlements who has access to the entire table and then within that you get more granular such as column controls who can see the columns and I'm going to talk a little bit about masking of columns and what that means then you can do things like make rows disappear or that's termed row-level security where some people can see some rows but other people can't and then you can even go deeper than that and do cell controls which is similar to column controls except not all the cells in the column get massively certain cells based on potentially other values in that same row okay so here are some different options for you in a day to Brick's world and in fact this this stands beyond data bricks and I'll talk about each of these in the subsequent slides but you've got control of compute control at storage and then probably something that people aren't familiar with which is the policy is actually separated from the platform altogether so the first one here is is controlling at the compute layer and I think everyone's very familiar with this this is pretty much how every database works where you build the rules in the platform on who can see what and Brooks has something called table ACLs to manage this for you where you can define what roles or users have access to what tables this is all defined within the data bricks construct and table ACLs is only going to get you the table level access controls this does not enable you to do column row or cell level controls so an example supported policy here at the bottom of this of the slide is only allow members of data engineering group access to the customers table so moving on to the next one we've got control of the storage laters so this is fairly unique to the cloud or I should say very unique to the cloud in the sense that instead of doing the controls natively in data bricks data books can actually do something called credential pass-through and pass that down to the cloud provider at the storage layer and let then make decisions on who should see what buckets or folders or files so you can keep using your regular old controls that you have in a TLS or s3 which define what roles have access to what files and storage and that basically propagates through to the tables that you're using in data bricks so again this is really just table level controls cuz the end of the day it's files that back the tables that you're restricting access to and you can't do things like column row and cell so this example policy is very similar to the prior one I just described except instead of allowing the engineering access the engineering group access to the customer table we're saying the customer s3 directory so what folks end up doing so in those first two scenarios as I mentioned they don't do column row or cell kind of a naive approach to solving this problem is let's just create a bunch of views or copies of the data to manage these these different access levels or the granularity that I need so I'm calling them lenses in the slides the different lenses into your data and so we've got that first lens so you had the original table which everyone could see everything in that original table and then you could potentially build a copy that removes that one call in purple and that blends number one you could have another lens which removes that same column but also removes some rows and you could have a third copy or a third view that removes a different column but that's different rows and then you have to manage access to all these copies and views separately so this quickly as you can see could explode out of control and so what we actually call this is an anti-pattern so an anti-pattern meaning it sounds good on the surface that it actually makes more headaches for you down the road so one of the obvious issues here is that in a lot of systems the user clearing the view needs access to the backing table too so that really defeats the purpose so you really have to end up creating copies which is the case of data bricks and so if you're creating all these copies you obviously create this proliferation of copies all over the place and in order to maintain all those copies you need a series of elt jobs to create all those transformations and create all those data copies and views I call this alt spaghetti but you know even adding more complexity to that is if you have updates happening historical updates you know like if you three months ago someone now updates a value in the table you need to account for all those historical updates and when you create your copy so that everything stays up-to-date this could get really really complex and then of course as I mentioned the prior slide is you're creating all these new copies or views of the data that means that you have to manage all the rule rolls that have access to those copies and views so this relates to role explosion so all of this is going to cost you a lot of time and money which is why it's an anti-pattern it's just you know a very simplistic example down here at the bottom is you have a hundred tables and you need three different lenses and do each of those tables this is probably you know it's not realistic that would always be three but just bear with me for this example that would mean that you would have to have 300 elt jobs which create 300 different copies of views not counting your original table and then you're going to need somewhere between 3 and 300 roles depending on how homogeneous that those three different lenses are across all those tables this can quickly get crazy with very few lenses on I would argue under tables is very few tables so what do you do so this is where really the separate policy from platform comes into play so just like the Big Data era required you to separate compute from storage to scale the personal data era my argument is requires the separation of policy from platform and so you can define your policy external of data bricks and storage in a consistent way to manage these controls and in fact if you do this you get beyond just table level controls and it can be enforced dynamically so you're not having to create these copies or reviews one user queering the same table as me may see a completely different view or lens into that table than I would and this can happen down to the column row and cell level so an example kind of crazy or policy that you could support is you know by default everyone can see rows greater than 90 days old but people that are insiders can see data newer than 90 days but they can only see it for their region and we'll come back to this example in a little bit okay so you know why so I'll elaborate a little bit why would you want to separate your policy from your platform so it's the same reason that I guess that most of the people watching this separate your Identity Management from your tools because you want consistent users across all those apps you don't want Steve to be called Steve in one app and Steven in another you want consistent authentication across those apps and the way people authenticate to be consistent you want complete visibility into who your users are and what their groups and attributes are you want consistent audit when people are logging into where and I think probably the most important thing is you know the reason why a lot of us use octa is it's Octus day job to build identity management they're going to give you all the necessary bells and whistles that you need to do what you need to do similarly you would want to separate your policy management for very similar reasons you want consistency in the policy enforcement across your databases compute and storage you want complete visibility into the policies that are being enforced you need consistent audit across all your data interaction so your queries but also the management of your policies who's changing what and your in your policy rules and similarly it's their day job so asana Muta we are a policy management platform that exists separate from the platform and this is why we're able to do a lot of the very advanced anonymization techniques I'm about to talk about but also the dynamic enforcement down to the cell row and column level so if we lay this back on our slide that we started this with when I said hey don't let govern and slow you down well if you're using the right tool for the right problem it doesn't have to sell this slowed you down so you can have consistent controls across the Pythons sequel are in Scala which immuno supports and in fact you can see I have a bunch of other icons and this triangle now it doesn't have to just be data bricks you can have consistency no matter where your data lids and then down here on the left you were afraid to move your data into the platform because you didn't have the controls needed well now you can move it all there and enable your analysts to have more access through anonymization techniques which I'll talk about where you can actually gain utility from data that typically would have just been completely hidden but now we can kind of fuzz it or half hide it so that you still gain some utility but also our maintaining some level of privacy and then also of course now your legal and compliance teams can see everything that's going on understand what policies are being enforced it's not a black box per compute that you're using so we actually are accelerating your data initiatives through governance okay so now that we understand the architectures and some concepts around how to do the fine-grained controls I'm going to talk a little bit more about anonymization okay so we talked about column controls it's really shouldn't just be about should you see this column or not this can't just be a binary decision and I'm gonna make an argument why here so a slight tangent so I know stuff about Jud and Leslie Apatow so they're having a great time in their taxicab in New York City here and the New York Taxi and Limousine Commission this happened a while ago I think it's this date has been around for like eight years now they release all their information about the taxi pick-up and drop-off times and locations the amount of the ride the the tip amount all this data seems fairly harmless right well Jed and Leslie may not think it's it's harmless so if you look closely at this data you'll see there on the bottom left that we've got the taxi medallion in pickup time and then over on the right we also have the tip amount here and if you look up at that image you can see that we also have the taxi medallion so what some tabloids did think it was Gawker were they were able to map the taxi picture based on the medallion and the timestamp of the picture with the taxi data and amongst millions and millions of records they were able to find Jed and Leslie's ride and because of that they know how much Jed and Leslie tipped the taxi driver there are a bunch of other celebrities they did this to who tipped the taxi driver $0 which I'm not going to disparage here but but at least this gives you a sense of of what happened with this kind of attack so this is an example of what's called a linkage attack so we took some information we know from the outside world in this case the medallion in photo time and we were able able to link that to the medallion and pick up time in the New York taxi data to break privacy so New York actually kind of thought ahead a little bit and and said hey let's let's mask the taxi medallion this might have happened after this this attack I can't but it wouldn't have mattered either way and I'm gonna tell you why even if you mask that direct identifiers at the taxi be died and you could still do this attack so if you look at Jed and Leslie here so remember we had the medallion in pickup time attack so if we go ahead and hide the medallion now that attack doesn't work anymore but we could simply use the pick of time and location if we knew that to break privacy because then that's unique enough to uniquely identify them amongst a millions of pros so let's go ahead and hide the pickup time well what if I need a pickup location a drop-off location again well let's hide the pickup location so you can't do that all right well what if I know the drop-off location and drop-off side well crap okay let's hide the drop-off location and say I think you get the idea here is that as you keep hiding what are called quasi identifiers or indirect identifiers you just make your data useless you're gonna end up hiding everything so how do we solve this problem so and this problem exists well beyond just my silly taxi example so I'm gonna play a lawyer for a moment and this is language from the CCPA and there's also similar language in the GD P R so this first orange blob is basically discussing you know if you anonymize or D identify your data well enough CCPA doesn't apply cool you know this is the get-out-of-jail-free card I can analyze my data on my all I want cuz I've been on in my zip well enough but as we all know nothing in life is free and so PII is defined as information that identifies and this is in the CCPA that identifies relates to describes or it's capable of being associated with or could reasonably be linked so they are talking about a linkage attack like we just talked about in the taxi use case so this if you want to anonymize your data you need to worry about quasi identifiers and indirect identifiers so these little bubbles kind of walk through the kinds of identifiers you would be want to be concerned with and aligned into the taxi data and talk about some techniques you could use to mitigate them so we've got the direct identifiers which are the taxi medallion or the indirect identifiers which is the pickup drop-off times and locations and that was what was used for the linkage attack then we have sensitive data like the tip amount which is what was released about the celebrities that was embarrassing right and so let me go to the next slide which actually discusses some of these privacy enhancing technologies so there's a bunch of different ways you could do this throughout few keishon generalization and randomization and these aren't techniques that Anita made up these are called privacy enhancing technologies that have been around for a while but actually implementing them and enforcing them in something like a data bricks and spark dynamically is highly complex and it's not something you can just kind of on a whim ask your data engineering team to do so these are things like differential privacy randomize response which can replace the the tip amount with random tip amounts k anonymization which can remove highly unique values rounding which could which allows the generalization of making the pickup and drop off locations and times less specific so lots of tricks you can do to balance utility and privacy so to show you what I mean by that balancing let's apply some of these rules to the taxi data so of course we're gonna mask the direct identifier which is the taxi medallion but then we've got all those indirect identifiers the the drop-offs and pickup locations and times so rather than just completely blocking them we can generalize them to make them that less specific so we remove the minutes and seconds from the times we remove some precision from the coordinates and the drop-off and and pickups so an analyst this shouldn't really matter right they could still do all their their aggregate traffic analysis that you do from this taxi data but now they cannot do that linkage attack that that we showed earlier and similarly we can randomly replace the tip amounts if it made sense where sometimes we could replace the tip amount with a legitimate but fake tip amount so the attacker doesn't know for sure what the tip amount is in the data but again so rather basically if if we just naively blocked all these columns I'll be left with was if the total trip length but or the or the trip cost but with these anonymization techniques you get full utility out of this data while also protecting privacy okay the last thing I'm going to talk about for eternity over Greg is scalability so when you start thinking about all these lenses in your data and all these anonymization techniques you need to consider this completely explodes the universe of if we go back to our example of using copies you would have to create and manage and also the roles that you would have to manage so you can see a world where there's five different geographies where all your data lives with different controls and different business units with different rules and different regulatory controls you need to think about this this is beyond human comprehension and beyond a human be able to do this manually so how do you manage it so the first thing you can do is this technique called a back which is how a Muta happens to work where you can define your users and in a back world just attribute based access control you define your users like Steve is six foot two brown hair works for AM you de you know however you want to define your users and then you build rules separately from that definition they make decisions on the fly Oh Steve is querying and he's part of them you know he's not supposed to see this table I'm gonna block him our back on the other hand conflates who the user is with a role and what that role should have access to so now basically you have to create a role for every different access decision you need to make which quickly explodes out of control so if you go back to this example where you know we want everyone to see data older than 90 days but only insiders can see data younger than 90 days but they can only see that younger data for their reason region so I I went ahead and built a policy and a tool called Apache Ranger which is our back based policy manager and in this case we have to build a rule for every possible combination of region in insiders so we could have an insider in the east region so we need a where clause for that we need we have a insider in the West region we need a where clause for that we might have some insiders and I didn't build this in the rule that are both in east and north and in that case we had to build a separate rule for that because remember we're conflating who the user is with the policy that they should get so if we were to account for every single accommodation this would be 19 different roles we need this is only for four regions so the problem is you have to predetermine all these roles up front and build the policy against this and access it is implied so I don't know what giving someone inside or East really means like because because it's all conflated with the policy so it's very hard to comprehend on like where you should add your users role wise and what that actually needs cuz it's all implied whereas with a back you simply define your user so Steve is an insider in the East and this is actually a visualization of building the rule in mu de this is the actual tool but we assign the attributes to the user and then when I query the data in from region East it's going to dynamically see that I'm coming from the east region and and apply that policy on the fly so I just have to build the rule once and it will all wait and it will also be future proof so if I add a new region Central and and I query the data it's actually gonna see central and apply the rule appropriately whereas with our back I would have to remember to create a new rule for that central role so there's a lot of power and a back ok last thing is physical first logical so just like a back ad scalability this gives you scalability as well so if you are building a rule against a physical table like I could build the rule for credit card transactions I want to mask the customer last name and credit card number Kalos so I'm calling out the actual physical table this could be really painful if you have thousands of tables and all those columns are named differently right this would be a nightmare job had to build these policies one at a time so if you build them at the logical layer instead you can actually tag your data with person names and credit card numbers so you're abstracting the actual physical table and databases some of these tables could be data bricks some of these tables could be an Oracle you know wherever your data lives we don't care what the column names are what the table names are that logical layer abstract Sall that you can build policy against that logical layer and you could actually go out and discover all the different places where this sensitive data Lidz through our machine learning driven caste classification algorithms to find where our person needs where our credit-card numbers an auto tag that for you so you can build your policy this way in a scalable manner okay so that's it for me I'm gonna pass it over to Greg to to actually talk about how he ran into these problems in a real world and applied these techniques to solve it thanks Steve well I hope this case study will help bring the message home and make it a little more concrete I'm Greg Galloway with artist consulting and I'm Asha architects and a principal we we were able to do a case study of proof of concept with a particularly large fortune 500 company that I think applies here so this company made a really large bet on Azure and on Delta Lake and particularly on Azure data bricks is really the only way to get data out of the data Lake that makes sense of it and so they came to us with some security requirements they were both internal policies in addition to industry regulations you know such as the CCPA and others and so artists consulting was brought in to perform a proof-of-concept and test out how how to accomplish these requirements both in data bricks and in a Muta so tell me into the level architecture for this percept they had an azure data like store that was central and on the left there was a data engineering focused work space that had readwrite access to the data Lake and middle of the screen is a workspace for data bricks that was really focused more on end users both bi and data science end users and that's workspace and the clusters within it was integrated with data with a muta and so there were a couple of different ways that we tested allowing users to get data out of data bricks in a secure manner and a Muta was able to seamlessly secure any of those those methods the one on the bottom is through data bricks notebooks you a user can access tables in the immunity base within within data bricks they could also use spark drivers in tools like power bi or alternately they could go through the immuno Postgres interface immunes to be a Postgres database to make some of the data available if that's a preferred approach we recommend a generally users using notebooks or using spark drivers to connect so if we summarize the results of the proof-of-concept there were a number of requirements or use cases and those are listed on the Rose seven key ones and we did tests to show essentially what could be accomplished with in vanilla data bricks without immunity and then we we repeated those particular tests with immunity graded to in order to determine what's the best security pattern how do we meet these requirements how do we make this successful in their organization so you can see at a quick glance that we were able to do mmm five or so to some degree with plain vanilla data bricks but with the Muta involved we were able to really well accomplish all of the all of the requirements so I'll go through each of those so first of all the first one was redaction of older data and this was easily accomplished with with data bricks essentially we were able to create a new copy of the table so that we're wiping out the Delta table history and the time travel and we were able to do that with data bricks by creating a second copy of the table and doing some some renames the next requirement was around masking and this was of course a key requirements for example social security numbers and credit card numbers and such should not be visible except under very specific situations maybe the HR department needs to do a study for a couple of days so they need time limited access to data unmasked but everyone else should only be able to see data masked in data breaks you were able to do some approximation of this maybe with the md5 hash function and and views but it wasn't a great solution immutable ushion here that it offered a number of different ways to to format the mask and it also allowed you to do very convenient things with global policies where you didn't end up with a bunch of spaghetti views and spaghetti code like Steve and had talked about the next requirement was around deleting the detailed data but keeping aggregate data and that was easily accomplished with with data bricks the next requirement was around row-level security and this was a key requirement for them they were very insistent that there there was a large complex set of requirements each users should only see their region for example but there were a number of a number of other requirements and like Steve Steve had talked about there is a naive approach where you sort of an anti-pattern like Steve talked about where you end up with a ton of views and data breaks and that really wasn't gonna meet the needs of having this dynamic based upon user attributes and user roles and so with the mutant the the solution was very easy global policies made it easy to apply a solution to all tables that were that had columns tagged in a certain way or that data of a certain type and the policies could be based upon user attributes so that made it very straightforward to set up with the muna the next requirement was around auditability being able to see who has access to what and also see who accessed what and so with data bricks there's some integration with Azure diagnostic logs and with some metadata queries to see who has access to particular tables but I would say definitely that am you know had is built to do this it had a very user friendly UI that would give you deep insight into exactly what people are doing and exactly what people have access to so that was a superior solution the next requirement was around manageability so easily a Muta is built for this it's a single pane of glass that lets you manage policies instead of thousands of views that you have to maintain and test and hope you don't make a mistake and then the last the last performance or the last key requirement was around performance so particularly related to row-level security we did some some pretty intensive testing to make sure that applying policies wouldn't degrade security and the main recommendation we would have is to make sure that you use use policies that are based upon user attributes and particularly for their organization they're using a feature called the external user info endpoint that would let them populate those policies and that seemed to be the the best performing way of the ones that we tried in a muta and overall they were pleased with the performance I think one of the key reasons is that immunity great so deeply with SPARC that it's able to push its policies down into SPARC and have it be calculated in a scaleable compute so overall the customer was happy it was a very successful POC just on a personal note working with a mute it was one of the best product support experiences I've ever had they were willing to jump in and help and offer suggestions and I don't think that the proof-of-concept would have been successful without a muta as it brings a lot to the table thanks Greg that was a great overview and I think you really brought home my points in a concrete way so let's summarize everything that we talked about today so going on to the next one so again just just going through the points we touched on is you know choosing the right policy architecture so in the example Greg walk through of course they need a lot more than table level controls so Wednesday they decided on this architecture where we separate the policy from the platform and in this case using a muta and and and you know successfully avoided that anti-pattern of creating you know hundreds or thousands of views trying to manage that and then of course the second big point is column level controls needs to be a lot more than a binary you have access or not you really if you want to leverage your data effectively especially in this world of regulatory controls around privacy and it's not going to stop you're gonna really need to consider how you manage these privacy enhancing technologies on your indirect quasi identifiers and it's not just about your direct identifiers anymore and then of course the scalability points which I think Greg really brought home which are you know the use of a back to build policies in a scalable way and then of course using our logical layer what he mentioned is those global policies which are able to reference that logical layer so you can build a rule once and have it propagate across all your tables and this is made possible by that sensitive data discovery feature I read earlier which is the Muta can go in there and through our classifiers discover that sensitive data you can of course tag it yourself as well or use other external business glossary and pull that information in which is also a capability that we offer and then a few closing thoughts here if you want to learn more you can go visit the immuno website slash partner slash data bricks to learn more about the integration but I want to really encourage people to do is we have a free trial of a muta so if you go to this website Mediacom tribe you can actually enter your information we will spin up in a mute instance for you you can goof around with it for 14 days for free we have instructions on how to configure it to your data breaks cluster you'll be off and running doing everything that we spoke about today and then please don't forget to leave feedback on this session we really appreciate it and thanks for joining us you 