 you [Music] it's my great pleasure to welcome Daniel wig door here many of us have known Daniel for well it seems like much longer than it's actually been but he's been a great person to collaborate with over the years we've done book chapters together papers many informal fun conversations the occasional beer at conferences and he's done a great series of work and many topics but today particularly gonna highlight some things he's done a cross-device and multi device interaction both designing and building such systems so I'm just looking forward to a little regalis with tales of the wilderness of multiple devices thanks good it's great to be here hey Bill I was I was always hoping you'd be here of course but I'm as as some of you may know I'm from Toronto from the University of Toronto and in particular from the DGP lab that animate graphics project which a couple of years ago had its 50th anniversary and one of the students made this word cloud of our various alumni and so where is Bill there's bill right there so you know I'm very pleased to have come from as Christian says snowy Toronto to come and talk to you about multiple devices today those of you who aren't familiar with DGP or just to give you an update on what's been going on at DGP over the last few years we've sort of exploded in size so just in the last couple of years we've hired four new HDI faculty members and two new graphics faculty members as well so it's become quite a large group at U of T so please come and visit and and find other opportunities for collaboration and if anyone has a good picture of Ravin that I can replace the sketch there are reasons that he chooses not to put his photo online a lot and a few if you know the story if not you can ask him all right so there are bunch of things is what you've played at in Toronto and one of the things that we do as faculty members of course is enable students to go off and do really cool and exciting work and then every once in a while come and give talks and look back at all the work and try to put them into what you can call projects to make it look like you were gifted with foresight for all of this wonderful stuff so this is my sort of post hoc care of the work that we do in my group we play a lot in post wimp user experience the symphony of devices building on the society of devices ideas we do digital fabrication immersive haptics zero latency user interface stuff although that's now mostly spun out as a company and now more recently looking at emerging technologies and developing nations but in particular Ken asked me to focus on the symphony of devices work that we've been doing over the last few years so when this talk and be compressing a whole bunch of Chi and whist papers into one sort of you know bite-sized chunk and of course along the way I'll be presenting the work that was done by a whole lot of really smart and talented people who are represented here on the slide from U of T Microsoft there's this handsome devil here Autodesk research although now Toby Grossman has joined us as a faculty member in Toronto and Anthony Chen who was at CMU when he we collaborated now he's professor at UCLA so these are the various folks who've contribute to this symphony and what's really exciting to me about multi device although we've been we as a community been looking at multi device interactions for decades and and looking at great opportunities for how we can move information between devices what are the experiences to have informations between devices or the opportunities that get created we find ourselves now in the world at an interesting time in terms of the potential for those HCI visions to become reality so here's a great couple of slides that Anthony had in his Chi talk so we have the papal conclave for benedict xvi in 2005 so here's a photo of the people gathered to wait for the white smoke right to find out who the pope is going to be and then just eight years later we have another papal conclave for Pope Francis and here's a photo of that so we see a pretty startling contrast between those two times and of course you think about what's happened since 2005 we get the introduction of true smart phones right that actually are computers rather than things that are pretending to be computers I get tablet PCs we get lithium-ion battery cells produced in mass quantities we get mobile devices we get ARM processors and we have the ability to have these mobile devices and people genuinely now do have multiple devices right so we're not quite at this point yet but we're definitely getting to a point where we have this opportunity people have these things in their pockets now bill is very fond of saying that the way we want to think about devices in the future is like this right it's a stack of paper the way we think about an iPad today is that it's an expensive and valuable piece of technology the way we can think about it in the future is it'll be like a piece of paper it's not a fair characterization of things that you've said so we're driving towards that future and one of the interesting realizations that comes from Parc and Mark Weiser and some of that work is a recognition that just because it's cheap and it's paper doesn't mean that you're not going to have different form factors right and different devices have natural affordances for different uses of the technology so what we're interested in doing in our work is looking at how can all of these things come together and be used together in in interesting and useful ways we build on these notions of the Society of devices and we've dubbed this the symphony of devices and I'll explain a little bit why we've done that as the talk sort of progresses but if you think about what are all of the technologies that people have available to them and I'm realizing I should update this slide because this is the version of the talk I gave a few years ago so you know smartwatches don't look like that anymore and your surface tables don't look like that anymore but you know lots of screens available to us right and what we're interested in looking at is enabling people in answering this question how can we enable users to dynamically form ad hoc arrangements of devices to suit their current needs now when I've given this talk in industry contexts I've heard back from people well doesn't Apple do already doesn't Google do this already because we have things like iCloud what iCloud can enable right now is this vision right and this is a screenshot from a while ago as you can tell by the old iPhone this is largely what iCloud does I can buy my Beatles album on my phone and then I can download it onto my other devices it's about synchronizing States that I can transition between devices so we're starting to get to that idea that it doesn't matter what the device is and what matters is the content but we're not yet getting this idea of the ad-hoc connections same for Microsoft this isn't called SkyDrive anymore now it's called onedrive sorry oh really this was the result of a lawsuit oh okay alright I just thought it was you know another group took over another group and them alright well anyway so the vision for onedrive is sort of similar right it originally back when this slide was created it was about taking content from your individual device you put it in the cloud and then I can access on another device and I did that with a PowerPoint presentation I've got my desktop computer at home and then I you know plugged in my laptop and the PowerPoint was already on it when I was going to the cars which is fabulous and and it also does things like I can synchronize editing of a document with multiple people and it's sort of getting to the Google Doc sort of vision right but most of the functionality most of how it's getting used is about enabling people to transition from one device to another so we not see we do not see yet these platforms enabling people to use the devices together to enable compelling experiences so I'll give you some examples of that in a minute Netflix does something similar right you can watch a movie on your phone on the bus so when you get home you could switch to your your TV your laptop whatever I'm told that someone has developed a Netflix app where you can use the phone as a remote control so that's sort of getting there but it isn't built-in it isn't baked into the Netflix app experience Kindles same thing right I can read my book on my phone and switch to my thing and the state is synched but it's all about moving between devices rather than enabling devices to work together in an interesting way so oops I am multiple devices that's a power point bug for any you know I'm sure the whole PowerPoint team is watching it so we think about this model right we have individual devices we sync content to the cloud that's not what we want what we want is applications that just span devices and yeah maybe it's facilitated by the cloud although you know how good an idea is that how truly reliable is your internet connection for real-time interaction but surance facilitated by the cloud right but what we want is to enable people to sort of span multiple devices so a lot of people have done sort of room where type work so for example here's a project I did during my PhD called we space and in the we space sorry I have audio and it's not coming out I'll just narrate myself and share their screen images on a high resolution data wall each laptop screen on the table is rendered with a control bar with buttons that allow users to switch the priority of the image these images are automatically laid out based on the priority of each display users are also free to manually manage the layout on both the table and wall surfaces gestural input on the table controls the magnification and placement of the laptop images on both the table and the wall so the we space project again was a better room right you could come into this room and when you're in the room you can have your laptop stuff sent into the room and you can use the table to do stuff with it and other people have built these sort of smart rooms also but it's a static environment so this too isn't the sort of ad hoc connection forming that I'm talking about and it certainly isn't enabling people to easily transition between combinations of devices for what they need so let me show you an example of something that we built there'll be a video that I'm about to play from 2014 showing someone in robertlevi who used to work here at Microsoft I think many of you may know him or some of you may know him building what we are showing what we called them the conductor project I'm showing a clip of a video where he describes how PowerPoint might work with a more symphonic vision multi-device interaction so here's Robert for example here we've got this PowerPoint type of applications we've got the slide the slide lists the speaker knows on a timer if I connect in sign into the same application on my mobile phone then what we'll see happen as I sign in is that a slide lists the speaker notes and the timer will jump over to my flow and off of the laptop and this happens not because the application was coded specifically to say that these features for the pure elephone and the current slide should appear on the laptop exactly the application is setup is to say it happens to have a feature that is the current slide the current slide would like to be on the largest screen available and would like to be by itself and based on that our algorithms figured out okay here's the two available license this one top table for the current slide everything else can go here it gets more complicated though when you have a larger number of devices or more variety of devices so for example down here I've got the slide timer on my phone which is neat that's a good thing to have on my phone as acting as a remote control but if I happen to have a smart watch we can run the conductor application on the smart watch and we'll see the timer jump off of my phone and onto the watch so now my watch it's telling me what slide number I'm on how long we've been on the current slide how long the presentation has been going in total and it's not a dumb display just like we control the slides from the phone is remote control we can actually push the buttons on the watch and have it update what slide we're on across all of these devices move forwards and backwards we can actually go farther than that I can add in my my glasses and if I if I connect into the same application I've logged into my glasses what we'll see happen is that the speaker notes will disappear from the phone and the speaker notes will then appear directly on my glasses so I have a nice private teleprompter in my peripheral vision all right so that was the the sort of more refined version the culmination of a bunch of the work we were doing in terms of both user experience and developer experience we put those two my gosh PowerPoint I swear I'm clicking on a different slide than the one that's choosing to show you all right there we go so we put all that together into what we call the symphony of devices and what we're trying to tackle in this project is sort of two pieces one certainly is the user experience right the sort of classical HCI questions of what are the arrangements of devices that provide useful usable desirable experiences for end users yes and I'll talk a fair bit about that also but the other problem we got really interested in was how can we enable content producers to target unknown device combinations and I'll talk a little bit more about what I mean by that but but as you're sitting there and I'm gonna go through a bunch of the HCI now and some of the underlying studies now what I want you to be thinking about as a computer scientist or an engineer how would you build an application that could enable the experiences that I'm going to show you without knowing in advance what are the devices that this is going to be running on what combinations of devices will be there and just think about what tools and toolkits you might need to enable that and then well we'll get to the end we'll talk about developer experience all right so three pieces and want to tell you about I'll show you quickly a piece of ethnographic research that we did that sort of kicked all of this off then we'll talk about interaction design and then we'll talk about tool development so first into the ethnography so this paper was first published at UB comp back in 2013 and what we did or what Stephanie did was went and and how are people today using multiple devices in combination with each other or you know substitute 2013 for today right but 2013 was already late enough that there were good cloud services to synchronize content across devices it wasn't all that hard to send files from one place to another or to send contacts and synchronize calendars and that sort of thing across devices we'd gotten past having to synchronize using like cradles if you think back to the old Palm Pilots the way that you had to sync there was no internet connection right so if it put on the cradle on your desk and hit the sync button this is later than all of that right this is a cloud enabled synchronization of stuff and what we want to understand was how given the tools that were available at that time are people using multiple devices so she went out into the world and recruited 22 participants here is a sort of a snapshot of the sorts of people we have so we have for example a motion designer an interior designer neurophysiologist a hospital pharmacist management consultants and so on so these are professionals who use professional grade tools and she's sad and interviewed and worked with them and performed the sort of basic four steps of ethnography to understand what are the things that you're doing today so in terms of breakdown of Industry we have everything from creatives to engineer's health IT business marketing consulting etc so we have 22 participants and we went and collected all this information and so here are a few of the highlights so for example here's a photograph of a desk and we see no papers that are in this sort of desk space love these photos right and I love this photo alone I think it's pretty illustrative for a couple of reasons one so look at this workplace we have this sort of you know Jonathan Gruden multi-monitor type setup right where people are trying to align the monitor one of things that's really interesting to me about that is that all of it is being driven from a laptop all right so this is we're seeing a transition between you know when I unplug that laptop and go home all the information is sort of automatically synchronized between these setups because there is no synchronization it's just all the state is on the laptop itself and what else is happening here well you see the keyboard and mouse and monitor is fine and all that's plugged into the laptop we see also an iPad that this person is using concurrently and I think just as interestingly we also see a whole bunch of paper that this person is using concur as well and in particular we see this journal taking up the prominent middle spot of the workspace right and the keyboard is sort of shoved aside for for the journal so that's sort of interesting right in terms of multiple devices and in our work we consider these paper artifacts to be peers to the digital devices so we we call them all artifacts together in the study here's another workspace that that we see a couple of interesting things first of all we see two sets of mice and keyboards so they're at least two computers that are being driven by this thing probably oh sorry that are driving all of all of this stuff here again we also see an iPad that's part of the active work right so this is not in keeping with the visions I set before of the mobile devices for mobile time you use it on the bus the desktop is for desktop time you use it in your office here they're using it all concurrently here's another workspace of one of the designers and we see here splayed it on the desk all being used concurrently a relatively concurrently laptop phone tablet okay here's another one this is someone who is in a co-working space and in this instance we see a phone that's sitting on the desk and part of the workspace but we also see this Wacom tablet now computer scientist engineer looks at that and says well that's not a separate device right that's just an input device for the computer but if you the way I want you to think about these things is don't think about what CPU happens to be driving these things right like what we're focused on is what are the artifacts the physical things that this person is interacting with the fact that they all happen to be driven by the same core is sort of irrelevant right so what's interesting here is they've chosen to designate an input space to that input device and still have the other input devices that are built into the laptop right and this isn't a co-working space so this is you know they carry this around with them in their laptop bag and set it up and then the permanent set up so just to give you the quick high-level view of the paper and then I'll point you to the paper for all the sort of detailed stuff Stephani performed a detailed artifact analysis and this is a couple of dozen pages that's an appendix to the paper you can download that shows everything artifact and artifact here includes the particular laptops also includes the physical notebooks and all of those things and then we talk about what are all of the different things that these get used for how do they transition information in and out of them and how does it fit together so that that sort of raw data is there and we've found it extremely useful to go back and mine this over the last six years so then when I have students were interested in creating multi device experiences and wants to say you know what what should we use this stuff for we'll actually go back and look at these tables and say well you know here we see that there is this professional try to create the scenario it's completely unsupported by the tools let's focus on that all right so we pulled out a bunch of projects from this I think sort of interestingly in terms of the analysis I'll pull out and focus on in particular in this longer talk we also Stephanie also produced these landscape diagrams for each user to talk about how are all of the devices or rather the artifacts linked together so what are the cloud services that link them together and what are the task flows that link them together and so we see built into some of these things tools of course like Dropbox and Google Drive Evernote Gmail the presence of an email in the middle of this cloud is sort of interesting right it shows the people are still emailing things to themselves they can open it on a different computer and just as this quick straw poll as you sit here today who in the last week month let's call it two months has emailed a file to themselves so that they can open it on a different device alright and Microsoft has a tool to enable this stuff right okay I actually have one drive installed on all of my devices and I love it I use it a lot but I - still email stuff to myself all the time right it's absolutely fast it's reliable you know it will synchronize right people were doing it then people are still doing it what that gives you a version a you know gives you an ability to search on things the document itself and its position relative to other things in the email log is something that your memory tends to be good at watching on check yeah absolutely all those things it's incredibly valuable yeah and of course that's actually cost for mobile you get the one brighter yes it's great one time so he times action castas it's much much more efficient yeah so it's better tool yeah absolutely yeah you can get the version right but it's it's at the point here being of course it's built into the email right I see it in the date I see it the subject planning this day anyway let's dive into the details of this in a little bit but anyway so we all have these sorts of cross-device behaviors what are the sort of interesting things that I might choose to pull out so let's say here's another landscape right so this is another participant and we see all of the connections between devices do you notice anything sort of interesting about this image I heard someone mention connections we have served two connected graphs right so here we have a completely connected graph whoops there we have the sort of disconnection right and maybe not surprisingly for some this is because they have the sort of professional devices and their personal device is completely segmented and different companies have different policies on this stuff do you guys carry around different phones at Microsoft do you okay Oh two SIM phone I see alright so this paper has a whole bunch of these diagrams you can go back and mine it for a bunch of experiences and I think it's sort of interesting for that reason so they're just a little bit of the analysis that you can see we have we examined all of the patterns of information flow the people practiced in their work and we found that there were both serial and parallel patterns so we call a serial pattern this sort of you know I do one thing and then I do the other like the Netflix the people are doing parallel patterns as well and I think if you sort of reflect on your own experience you'll recognize this so for example who has ever had their phone sitting on their desk running a calculator app while they're doing something else on their desktop computer and so we see although we see this behavior all the time in ourselves we found it in the wild as well where people are trying to do these multi device parallel experiences but the annoying thing at least as of 2013 was you can do your calculation on your calculator but you can't take the result and paste it into your desktop computer there there are actually apps now that will synchronize the clipboards across devices if you don't have one it might make your life better but but we've we have these sort of islands that get created in terms of our digital connectivity for all kinds of reasons we also talk about in the paper how people specialized their devices and tools based on the use cases so certainly things like people use their phones for things the phone is good for like a calculator because it's they can dedicate a screen to it those sorts of realizations then we also talked about all the problems of data fragmentation and suggestions how to fix all this stuff so we're fair to the paper to get to those results but before we get to the sort of fun HCI stuff I always like to sort of highlight those results because I think it's very useful for people who are looking to practice in this area it's a fun paper one of my favorite just as like an example one of my favorite parallel patterns that we saw was people using devices as helpers for each other so there was an architect who would do site visits and when he was on site he wouldn't he couldn't see things properly so we would launch the amaura application that would synchronize to his tablet and then hold his phone up like this and then look at his tablet to see what his phone was displaying and he had sort of figured how to do this with the the calendar app that would synchronize across devices but I like that example for a couple of reasons one it's it's showing the creativity of users to use applications for things that are completely different than what they're intended for he's I'm actually taking pictures he's just using it as a periscope but two we're also seeing how the the fact that this app had that functionality to use one devices uh view finder certainly wasn't built to support this scenario but if applications baked into themselves the ability to break apart and let users choose which devices they want to use for a different functionality in a given moment and then we're able to see these sorts of things emerge and so the application developers don't have to focus necessarily on all of the snares is going to get used for all right so there's a lot more in that paper but I'll jump ahead let's talk about the interaction design stuff so we built on top of the interaction design work and I'll show you three different projects that that applied some of this stuff the first was something called conductor it was published at chi in 2014 and the vision behind conductor was that we wanted to enable people to have multiple devices phones tablets and to use them in any way that they chose to for their application so if you can imagine the the desktop surface going from this some number of decades ago to this some smaller number of decades ago to the laptop to the phone how do we get back to something that enables people to have the original experience that was more in keeping with our behaviors so this is our vision is large number of devices people can form ad hoc connections and use them in combination so here's Peter who's the author of the paper using our system and we built the application that underlay made all of this around a document triage and information foraging tasks so the people would have a use for multiple devices and I'll talk a little bit more about that in a minute when I get to the validation but there's the system let me get to some of the details so the conductor paper talks about multiple applications we built it as separate apps but then enabled frameworks to sort of move between those applications and to enable users to form connections themselves rather than forcing developers to think in advance but what are all of the different connections that a user might want to to make so let me take you through some conductor applications so this first app I'm going to show you is an application that lists tourist destinations in the city so this might be built by business association you might think of it as the results for a google map search yeah so here a bunch of locations in Toronto and in the single application version of this thing if I tap on a particular output I get information about that destination all right this is this is sort of a very very basic show me the cool stuff in a city app that you might run on your phone so how now might this extend to a scenario where I have not just a phone but also a tablet so we've got our original phone on the left and now and Nexus on the right and of course the Nexus Display is much more suited to viewing detailed information and viewing photos but as we've found from our ethnographic work we know that people like to be able to maintain the context of the list at the same time and so we enable the information to flow so what do we see how do we transfer it so there a couple of different ways we can do this one we enable users to do what we call targeted transmission so targeted transmission the idea is I have a device and I've got another device and I on my original device say alright I want to send information to that thing over there right sort of like the sends to another application functionality or the share button on the iPhone or an Android all right so it's sort of user initiate at the moment that you're sending the information out so we can send a command like this so the user can scroll through a list find the thing they want shoes that they want to send it from a PI menu and then see all of the available devices that they can choose to send it to and then they release and it'll send that information to that device so here it is in action so there's the phone they'll find this historical figure they want to send information to and they choose the particular device to send it along now one of the things that is sort of interesting for the designer in this application like this is pretty basic right this is sort of like the share button from the iPhone built into multiple applications one of the things though that's really interesting are the bumpers around these devices they wouldn't notice anything about all the bumpers on those say they're color-coordinated exactly right because we ran into this problem very very quickly that when you have 15 tablets on your table it becomes impossible to identify which tablet you going to send information to now of course ken has published things that you can bump them together or use pen strokes and that sort of thing to use the physical location of the devices relative to each other but we didn't want to wait a more exploring other problem another space so so we bought these colored bumpers and then registered the color and in the system in addition to targeting devices we can also broadcast because although in the in many scenarios you want to be able to say you know send it to device X in a single user scenario if yours if I say I want to enable someone to send to a device of their choice I don't want them to have to sort of choose from a menu based on the color can I instead try to invoke some of the work that that Ken did or I can sort of engage the other physical device in it we came to this realization well yeah we can and we can sort of do it like this and it won't be as annoying as you think so let's take a look just like you do with windows so check this out so I broadcast the thing out all right and when I broadcast it out the queue goes to every device in the symphony if you have a hundred devices then this thing shows up on a hundred devices at the same time that sounds like would be really annoying right except this is a single user scenario so if the user is only on one device at a time the where their eyes happen to be pointing is the place that they want to send the information so they get that cue and on the device and they can just tap it and if they tap it then it just loads the sort of default behavior and it's minimally invasive for the reason that I said because of the eye focus in our implementation it did not immediately disappear from all the others for reasons that I'll show in a minute so we can form multiple connections that's the other reason we want to broadcast hey the other thing that we can do is when you grab the yeah all right there we go there's the feedback we can also have these contextual associations so here instead of just tapping I dragged it over into the browser that was already open in the screen and that instance instead of loading up the page of the application and if I had tapped it with load another instance of this app and load up the the page that came from here instead I was already running a web browser so I dragged the cue over and it recognized the the browser recognized the type of information that would be useful to it and loaded up a web page that described it and that's because there was a URL embedded in the in the tag in the cue so here's the cue running so this is the context list example you see it'll broadcast and come across on all these other devices so there he tap to load up the sort of default page that he's gonna broadcast out the address and he happens to be running Google Maps on one of the tablets so it loads up the person's address and that Google Maps app so from this we get some fairly powerful behavior because we do late coupling of the information to the application the user chooses to send it to sort of like the Android intent framework so that you can say what is the type of information and how am I able to consume it at that moment right so it isn't all decided that priori we also can able using the the phone as a remote control by creating persistent links between the applications so instead of just sending information from one place to the other you can choose to form this bond which we call a duet so you can form a functional bond instead of just sending information so that when the cue comes we broadcast it out you grab the object and drag it into the link receptor and once you form a link every time there's a change in behavior in the original device it broadcasts out the new information and based on which application was receiving it will show the appropriate information okay so in that instance it was the default application so it actually launched an instance of the same app so that when I tapped on the Eaton Center I get the Eaton Center display of this application but instead of replacing this screen it shows it on that screen instead but I can also form other types of links so here for example or to map displays and in the map we have two different types of links so the first link the first target that he dragged to on the top of the list on the right just synchronizes the displays so every time one display updates and updates the other now in a moment what you'll see is he'll bring up the list of links again he'll pull it off and move it down to another receptor to instead just receive the display information but it uses it in a different way it's now showing what is the view area of the other display on this display so it's interesting about this if you think about the sort of developer an application developer experience I as an application developer can say I is an app and capable of being linked to other applications using this using the conductor framework the types of information I can receive are the this this and this alright so in this instance its receiving geographical information or its receiving screen coordinates or whatever and those types can be defined using sort of like a classic mime type definition again this is sending a lot like the Android intent framework for those of you have worked with it but the other thing that I can do is I define multiple behaviors so I can say yes I can receive geographic information but I can choose to do with it actions a B or C I can choose to whenever I receive geographic information I can zoom the map and show it on the screen or instead I can choose to display and Wikipedia page about it or I can choose to do this some other action and at the time that I'm developing the application developer just specifies what are the types of information I can receive what are the behaviors I can link it to and then provide the user with the option to decide which of those sockets we want to permanently connect this into to link to so that the user can make that decision and do that binding at the time that they're running the application but I don't need to know what are all the types of applications that I can connect to right I don't need to know that I might be getting geographic information from this Maps application by beginning geographic information from a website or a web browser or I just define the information type so there are two pieces of information that need to get specified by the user ok so let me clarify this is a great question so the question was why from an HCI standpoint from a UX standpoint is the sending application specifying how it's going to get used and it's a little bit different than that what happens is when I'm broadcasting information I choose what information to send and I choose what aspect of that information to send alright so if I say that I was back in the contacts information I choose a contact and I could say I want you to send the photo or I want you to send the address all right so I'm choosing what type of information I gonna send out once that gets specified in the original device then at receive time I can say okay I know that the information I'm receiving is of type whatever it's an address here are the behaviors that I can perform against that address okay so we're it's sort of a split decision okay so first it's the type then it's what how you act on it and this is sort of like sharing in iOS or Android right where you specify I want to share this out and based on the type of the information and I keep mentioning the Android intent framework is anyone here worked with the Android intent framework me what I'm talking about that's a few okay sure so the this idea of being able to pass information one application to another it its present and small talk i i've never developed again small talk i'm gonna focus on the dev x but but the point is this sort of late binding of information type gets carried forward all the way through mac windows right i can specify what is the type of information its manifest in the mime types and this is how i can choose what application gets opened when i double-click an app alright or in my web browser it specifies which player should i load when i've gotten a piece of information coming in the difference here is i'm giving the power to the user at the time by splitting up this information type and letting them choose am i just acting on it once or am i forming a persistent connection between this source and that destination okay so but now we're actually into some classic problems from operating systems so sacrifice and parallel processes objects and so they've gotten contention on this issue so the investments passing minutes like are using for time monitors P&V how you providing deadlock especially because they've distributed devices you don't you shouldn't assume the same for users so in this projection so that your your question is what is the underlying framework that where you built all this against how are you avoiding so in this instance we're using just a classic broadcast so we're not experiencing deadlock because they're I mean I guess you use could perhaps form a situation where deadlock would occur by creating a cycle between the devices okay so we we have not we don't have a block for that the user can get themselves into trouble this particular framework is single user yes so that's not accessible the concepts are potentially extensible this framework was to allow us to explore HCI issues okay all right so what else do we have so I've talked about being able to form these device cross device duets we also have the sort of classic task manager we replace the Android task manager so that when I call it up on any given device I see not just the application they're running on this device but I see a display of the applications running on all devices and I can grab an application and move it from one device to another for example and this has been implemented in all kinds of multi device yes why not instead of a selection which now is urine let me let me show an example of that in about five minutes all right so we were this gave us a perfectly fine way of characterizing it is the multi device version of information passing in an operating system fine what this enabled us to do was one explore the user experience but to understand are people actually going to use this when it's made available to them so we ran a study I won't take you through all the details but just be aware that it's there if you're curious and this was based on a previous piece of work where to go space to think where Chris North and two of his students wanted to ask the question if you had very large multi device multi display environments with a single device where people actually use all of this real estate and how would they use it so we adapted their experiment to answer a similar question of if you have all these devices are you going to use them and they and we took the vast 2000 2006 data set which essentially is let's see to have the numbers yeah 230 documents presenting to you a fictional thing that has happened in a small town including newspaper reports and that sort of thing and you as the participant have to go through and sort of solve the mystery of what's going on in that town so this is a classic methodology that we're choosing to apply but we were insted to see what people use all of these devices when it was made available to them so did they use it well yes as it turns out so we provided people with multiple tablets we told them their goal was to solve the problem not necessarily to use multiple tablets but we still found that they exhibited all the sort of classic messy desk type behaviors of using the physical arrangement of the devices to store information so for example if there's a particularly important document document I think is important I'll take it and sort of put it in a different spot on my desk as an important thing to remember for later they would save indices they would have like an inbox and a note box we saw all of these sort of behaviors and they actually did choose to use multiple devices right through to the end of the experiment and all the details are there so that was sort of our first multi device project now in parallel to that conducted at Autodesk research I collaborated with George and Toby and Anthony Chen and in the conductor project we called the formation of these persistent connections between devices duets so this idea is the user can choose to form a persistent operational connection between devices we wanted to go deep on a little bit and to explore how could a SmartWatch in a phone in particular be used together to enhance interaction so going deep into this phase space so here are a few examples so here we're using the SmartWatch to detect gestures on the wrist so you can flip your wrist around to bring up a different type of menu we're also using it to detect how you're giving input so if you're using a fingertip then you get a pen if you're using the side of your finger you get scrolling behavior if you tap with your knuckle you get highlighting and that's all based on the IMU the in the watch to detect the gestures we also have things like shared clipboard information so we can use the device the watch is a secondary display and as a tool palette so the people can use the devices in concert and of course this predates the Apple watch so this is sort of a very very early color capacitive watch with an API gain a bit of screen space wouldn't that be faster that was for the bottom would it be faster if the four buttons were in the bottom of the phone yes probably I think and undoubtably I think what one of the things that's really interesting about display bezels and Jonathan talks about this in his paper from well a while ago so Jonathan Gruden you should all know is here he did a project that looked at developers at who have multiple monitors on their desk and these are professional developers I think at Microsoft and looked at how do people actually use multiple monitors and one of my favorite results from that project was he talked about the value of the bezels and the fact that people would choose to use the displays to organize information and would not choose to sort of stretch information across displays now when you first read the paper you think okay well maybe that's just because the window manager does it right like when you click maximize it feels one monitor and not two monitors right although I think back then I'm not sure it was actually smart enough to only fill one monitor but if you if you read more deeply into the results he talks about this idea that people like to categorize information with full screen displays so they could say this is my monitor for task X and this is my monitor for task Y so to answer your question yes having the tool palette on your wrist which does not increase performance it does increase the number of pixels we make available to the user on their display and it's applying some of those Gruden principles to it so we talked about or I talked a moment ago about being able to detect different gestures with the Smart Watch and there's a really cool KY paper if you haven't seen it this past year the Chris Harrison one of his students did that builds on some of this stuff using the off-the-shelf Smart Watch to detect gestures in the wild we also see we do a few different things that are interesting like we showed you the tool palette example we also again are focused on enabling people to choose how they want to use their devices so for example if you have one of the problems that people have with having a lot of devices as all the notifications when I'm when I'm giving my sort of live demos and I have 15 tablets in my bag and it wants to remind me the demo happens you know people think the world is coming to an end right because I don't look as digging on all my devices so so for example if you want to be able to mute or send the audio to one device or another we provide a set of gestures that you can do that so if you want to silence all devices you should have pinched them together to get rid of the sound or pinch a part to distribute the sound to all devices or perform the sort of cross device gesture similar to Ken's work to send the sound from one device to another and that sort of thing so in that paper we go into a lot of detail what I like most about that project is the framework that I think is really useful so I won't go into a lot more detail but all the interactions so there are a lot of really cool things that the team develops there that I'm that if you're interested you should take a look at but I point people to this framework a lot to think about design spaces for applications and for multi device so we looked at the way we conceptualized it was if you have multiple devices you might think of one as being the foreground device one being the background device any given time so for example if the watch is the background device and the phone is the foreground device I just use the wash new things like gesture detection the example I just gave you a moment ago if it's the other way around then we see totally different behavior right so phone background watch foreground both back both background is there actually are some interactions they're both foreground and one foreground so what's interesting here so for example watch foreground and phone foreground we see things like being able to detect users postures of how they're wearing the watch so we had different gestures based on whether you were wearing the watch on the bottom of the wrist or the top of your wrist to do things like the tool palette display while I'm holding my phone and that sort of thing we could detect those things in addition to that we also enabled people to do things like switch layers so we see having that's not playing yeah so here if the watch is sort of around the back and you can't see it there aren't a lot of things you can do with it because you can't see the display to give direct touch input but we can switch layers by performing a gesture or here when it's on the front of your wrist and it differentiates the behavior we can also see things like doing task management to switch applications so a lot of these sort of one-off experiences but what all of this is in aid of I sort of jump forward a little bit what all this is a native is to try to solve the developer experience problem now I'll show one last project maybe I'll just show the video to make sure I've got time to show the dev X stuff this is something we just presented at Kai we wanted to ask the question so we talked you know it's talked a lot about what if devices cheapest paper one of the ways you can make a device that cheap is paper is just to use paper so here we have a hololens providing the digital display it's really nice when you use the sort of third person view instead of the actual hololens because now the view area is the entire display so you're actually user experience it's not quite that you get to see all of this at the same time if you haven't worn a whole ones but in that research project we built an application where I could read physical research papers and then get all kinds of meta information and find connections between them and that gets shown as digital information so for example if there's a reference that I want information about I can tap on the reference on the paper using my pen it recognises the reference looks it up and shows the the reference paper there you can play the video of the video figures and that sort of thing associated with different sections in the paper so that was the last user experience piece I think all of this of course as I've said is to enable us to answer what I think of this maybe the more interesting problem which is how can content producers target unknown device combinations this to me this is what we've spent most of our time trying to solve so I'll show you one oh my power point it really likes my ID photos huh it's showing a different ID photo when I click on different parts of this talk there we go so this is pant panel Rama I was presented a few years ago it's at Kai and the problem we're trying to solve is has previously called write once run everywhere so we didn't coin this term of course and you may recognize our friend from the the Java world over there but many many people tried to solve this problem generally of how do I write an application once and it runs across all computers right and this is the the purpose of virtualization and and just really the purpose of the web and a lot of ways now but our problem is a little bit different from this because the right ones run everywhere problems that were previously trying to be solved was how can I write an application will run on a phone or on a tablet or on a laptop right what we're trying to solve is a little bit different with panorama with panorama what we're trying to solve is how can you write an application I'm sorry you don't know that's the wrong slide Oh PowerPoint how I love you what we're trying to solve in panorama is this problem of how do we enable people to write an application that will run on any of these devices simultaneously and without the developer having to know that those types of devices exist and without the developer having to test the two to the end possible combinations of devices that their application might be running on so I want to be able to write an application once that'll run on any of these devices fine but I wanted also to work if I'm if I have a tablet and a SmartWatch or a tablet and a phone or a tablet and a surface computer and to spread itself out and do the right thing so that's what panorama is intended to do so panorama at a high level does a couple of things one it maintains a list of all the devices that this application is running on at any given time it's based on a web framework and it maintains a list of what we call panels and distributes those panels based on the capabilities of the system combinations at any given time so you take your web application like here's YouTube and you designate panels so this is sort of a user experience problem and I say okay here are the playback controls that's one thing that's one contiguous object that if it's if it's going to get sent to another device they should all be on the same device so I is the application designer know that that's the best behavior we also have the video display we also have the listed videos and I have the comments now for each of those things I specify attributes that will optimize the users experience so for example I specify that playback controls should be on a device that is within the users reach either because it's a touchscreen and the user is holding it like a phone or because it's a device with a mouse the video should be on the largest display the the box that lets you enter comment should be on the device that has the best text entry experience so I specify all of those attributes for my application and I provide them with this waiting all right so physical size keyboard touch quality approximates a user and how it matters then for any given device day to get a score PowerPoint really wants you to know about talks which is a fabulous program that many people here presented at then what I also provide is scores for all of the devices so televisions get big physical size attribute laptop PC gets smaller physical size for good keyboard quality etc and we're able there actually are frameworks that you can subscribe to for a fee that based on characteristics of the device when the application loads gives you information about the device will give you a model number of what device is actually running despite the fact that the web is supposed to abstract a lot of that stuff away so you can get that information and then we do just a linear solver to optimize at any given moment when I add a device which device should that be placed on to and we minimize the burden on our application developers this will be my last point so you can take your existing web application that might be divided up into canvas and all you have to do is recognize that these are the panelled areas and stick panel tags around them and then define provide panel definitions based on the attributes that I just described a moment ago so it's minimally burdensome on the developer and then the idea is every time the user adds a device they go to the website they're logged into the panel Rama framework it looks at all the panels it performs a new linear solver step and distributes the panels across the devices optimally and the developer doesn't need to have designed or checked all of these attributes in the first place the paper includes a developer study or we had people define all of these abstract values and then predict for any given combination advices we asked them okay where do you think your stuff's going to go to see if they could understand what would happen if they really understood this optimization and we found that universally developers were able to do that these were commercial developers not PGS and computer science yep the framework allow for changing of constraints and configurations because they imagine you have device once at the beginning but your workflow is fluid so devices may move and what does that reach an owner of each to make change yeah so any given attribute that's dynamic is updated and then it recalculates the thing the the challenge with that though is that it can create a situation where you're flipping back and forth a lot right so you know if it's like which device is closest and you move it half an inch by mistake and now your stuff will sort of already flows there right so it's not clear what the optimal user experience is but this is our sort of first step at solving the problem occasion going on at a time they can kind of hear each other that's it will tell me more about what you mean each one especially if I was starting and stopping applications and things like that maybe they made big choices too you know suddenly you may find things moving around and where did it where did my search people go them and those kinds of things yeah so a question you have to ask yourself is what do you consider to be the recomputation moments right so you could for example decide well whenever user first starts the application I'll optimize for that whenever user explicitly chooses to add a device I'll recompute then but other than that I won't recompute and I'll give you I'll use like conductor style control to let the user decide to then redistribute all right so even build pods in this in terms of devices with a secondary monitor this plugged in as purple the distinct recognized and such abuse because the challenges that I this this you know I love this this network but the there's these little issues that actually make it complicated because when I'm giving a talk and I got three minutes to get up onstage and sometimes they can't even use my own our fight that has to be on that you know that's the problem with robustness PowerPoint so this solution doesn't appear to extend to that situation and in particular where I'm at a place like you're here and you can't get on the network defense right right and also distinction machines which are public machines which are heard on the corporate network in the conference room which which may or may not be mine yes as long as all the devices are mine I can sort of deal with the security but this comes down to a social other aspect of social is what's the nature the kinship of the of the multiple devices plus the kinship to the networks the multiple networks of different types of security of their and their sudden becomes complicated again yeah so so there's sort of two and I don't say that let me just close then I'll answer your question okay so I know we're at the hour so people have to leave I won't be offended but I'll answer Bill's point but anyway so we've presented some stuff on the ethnography that sort of led to a bunch of these things interaction design and development tools really because those are the most interesting problem so to Bill's point so two things you said one was does it scale nicely like does it answer the point I made earlier in the talk which was don't focus on the CPU don't focus on the CPU think about the users experience in terms of what is a device right in this instance the answer is yes to a limited extent because this is a web framework so if the user we know for example the size of the window the actual physical size of the webview so the tools that we subscribe to tell us what is the physical size of the device that this thing is running if they can interesting I don't remember if in 2014 if this is including the paper or not but the tool that we were using can say this is a 50-inch monitor as distinct from this is a monitor that is this number of pixels and it's able to do that now with the the open ap is so it's solved like this much your second point like you know this is one Kuip a per okay so then the second point was how do you solve the problem of security right and and and also the internet goes down right like how how robust are we tall of those problems we're web framework so we're reliant on people live being able to get to websites by making the choice to be a web framework we are able to sort of get around the corporate security problems as long as you can get to a web browser and log in to your thing this will work but have we solved all of the problems absolutely not and it's a hard problem and I would love for a company like Microsoft to start to solve the much much larger problems but what what we're focused on in this project is how can the developer conceptualize of these things in a way that is useful to them but solves the problem of how do they feel empowered so when I remember when I was on the surface team on the surface table team our designers were were screaming there weren't enough tables to go around for everyone and so the designers were given 30 inch monitors because the diagonal of the surface table was 30 inches so that they could design for something that no the the actual physical size which mattered for direct touch the designers were screaming because the color gamut of the projector and the surface wasn't anything like the LCD that was sitting on their desk and it was making their jobs much harder right and you think about that level of precision the designers want to have and now we're trying to enable this world where you don't even know you know is it on your wrist and it's this big or is it on a tablet and it's this big or is it on a wall that's this big so I'm really interested in why I'm so focused on developer experience I'm trying to find the right balance between giving developers and designers control and avoiding the situation where they have to consider the two to the N combinations of devices in designing their applications that's the plot space we're trying to play but but yeah it's hard you give it very good teases to where the thing is negative future direction which is the moment those are great but the sign tools are also equally importantly and delivering experience that it's useful and enjoyable so you can you can optimize linearly until you know Kingdom Come and things were gonna be laying out sir wait we will lose my crap I don't want to use this because I made our attempt but what what what insert do you have is what are the right things to try first in terms of creating the sign tools for this kind of yeah so Sabo design tools so I think was Michael nibbling at Michigan developed a tool where you can you can enter arbitrary combinations of devices and it'll show you what your application will look like so you can so you as a designer can sort of choose what are your target scenarios I think a tool like that is really powerful because at least it lets designers without force them to go and physically have every device that they might want to play with so they can continue to do a linear optimization across their priorities obviously it's not a solution to the to to the end problem but its empower right in terms of what are the sort of attributes that designers really care about in this space I don't know the answer to that question I don't have intuition and and to me that's what what is so exciting to me about the area is that every time I work with designers to try to explain this problem to them the answer is always no I have to control that no I have to control that no I have to control that right and that the trust over digital tools has been so eroded by device fragmentation in the Android world for example where you know anyone does if you'll remember this term fragmentation right but people complained about Android because sometimes a Indra might be running on a 3.5 inch display or a 3.6 inch display and they called it fragmentation all right and then of course Samsung one and now Android just means Samsung to most people right but the back in the old days to talk about fragmentation designers who have worked with any of these frameworks get told don't worry you'll have the power or you'll have the power but then you know they're not actually given enough time to solve the problems so so anyone who has real experience there they're like battle-scarred and there's there's no trust I've seen this over and over and over and over again I think the maybe the opportunity is to to create design tools where designers feel empowered by creating the rules so that they can themselves use tools like panorama but not just executed at runtime but instead let them define the heuristics that are resulting so that they feel like they have power and they're doing design work by working in the rural space right it does make some sound well like a developer that's the best that I've been able to come up with in terms of making them feel empowered but I don't know the answer I think it's the application of science systems you have things like you know flew in the side or material design but those will now apply to the human intervention yeah I think it's super interesting how they could be a really interesting problem to dive into yep so it's a it's a really interesting challenge in it but one part of it I think there's some simple tools that could be done that would force a lot of the changes again just for the designers perspective is and also make a lot of talks a lot better because it could eliminate a crappy crappy slice is to simply this is simple to do I mean relatively speaking you build a previewer into PowerPoint it assumes your thighs are this far away from your monitor and therefore what does the slide gonna look like in a conference room on a 55 inch display view from this distance and I check the legibility what's gonna look like big monitor instead you if if conferences told me what's the resolution and how about how FAR's for the back of the room what's it gonna look like and I will find it right away there's you cannot design a slide that will work on all those conditions but that's the big mistake because you get a consistency with the graphics which is nonsense we know that just like your your your the interaction the language of your phone changes 100% when you're in a car audio an audio out versus fingers on eyes on and you don't notice it adapts so first of all can you make the thing so sent him the application sent its are like the old stuff in church and then the question comes down to that means it has to know how to adapt and so you build the adaptive behavior into the design process but the proof requisites be able to do that is - it sounds like your twin okay here's how I'd arrange it on this situation here's I do it that way and then you figure it out and and so you have a matter of design it isn't in my example oops I'm example and which goes you know right back to campus myths of Pygmalion fitness so there's but if you can do that quite an adaptation that makes some of these other things simpler so analysis the question tell me what what I need to know how to adapt see one type of negotiation initial negotiations or something else so I love it right but but to me you're asking I know we're over time I really won't be offended people just walk out and we can keep hanging up what you're talking about is is getting designers to behave like computer scientists and and in that you're asking to work at a level of abstraction I understand your point though since I'm person to act work interaction designers decide a graphic design and the problem is they're all graphic designers pretending their actions yes so I have now had the experience of going to three different companies one of them was Microsoft and being on the operating system team or consulting with the operating system team and then looking at how the designers choose to allocate themselves right what do you want to work on say the designer universally Microsoft other two companies I can tell you what they are the the designers shoes to be on like the sample apps right or the shell none of them want to be on the UI controls even though designing the UI controls is actually all the design that really happens in an operating system right and so what's interesting to me is where is that where is that school right so we're where designers learning that talent and I think that's a whole other opportunity for for education and defined the sort of middle ground between developer and designer but but you know anyway I'd love I'd love for it to happen sorry can we probably should run out there said we do have some food outside and I think it's for 20 people but I think if we don't pay all that you're welcome but I just wanted to thank Jane again for coming and kind of - you've heard this big often think busy guys to say seeing this here to wherever so it's fantastic and oh ok so yes so so feel free to join in the discussion [Applause] 