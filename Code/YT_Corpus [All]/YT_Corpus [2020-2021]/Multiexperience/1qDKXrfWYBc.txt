 okay so welcome to you wherever you are in the world so good morning or afternoon whichever whichever applies we're taking the usual webinar format today so I'll talk for about 30 minutes and I'll take questions at the end so if you have any questions you want to ask please submit them in the questions window at the side of the GoToWebinar and well address as many of those as we can there at the end so I mean Firth and I'm the VP of products for speech Mattox I've been working in the voice space for about four years or so now and it's been a really exciting journey with speech Mattox the technology is being used widely and they're being adopted across many industries for a wide number of use cases and technologies and it's become a significant part of many companies as part of their digital business transformation we support those businesses in utilizing speech technology in the language they want through the deployments they want and the way they want making it work for them in the real world so at speech Mattox we perhaps think about things a little bit differently than some other speech providers we like to to be a forward-thinking company not just about the product and how it might be applied to businesses now in the future but also about how we can make it really great without requiring huge investment from companies wanted to gain the value traditionally speech recognition like most machine learning and big data companies can rely on huge amounts of data labeling to achieve those real-world results and this is great in the short maybe the medium term but as we reach the next plateau it gets harder and harder to get improvements from that sort of data alone and the data needs to be more and more representative for the precise use case to get to those really really really high accuracies listening starts to bring the data problem into the customers scope so to get to the next big step of improvement either our customers and partners are going to have to invest in large amounts of time to get data and protecting money and labeling that data all we need to find better or more imaginative it took ways to sort of meet those objectives and speech Mattox is delivering excellent speech recognition now but we're also investing energy into a journey that will enable that next step change and if that's to occur we need to plan that technology now so today we'll go talk through some of the ways that the technology is being adopted now the strategic challenges that are being solved and some early details of where that speech future might take this and not from a totally cancel point of view but from a standpoint that we believe is realistic and coming greater bounces one thing's for sure I guess I don't see this journey getting any less exciting anytime soon so I mentioned earlier there are many use cases across many industries well here's just a snapshot of some of those so within the media and broadcast base this technology is being used for everything from captioning live videos optimizing editing workflows offline captioning searching allocating content as part of both asset management functions and for monitoring functions within the compliance e-discovery space it's being used to assist in locating content from fast archives of audio data identifying toxic content supporting quality of audio communication and training identification of things like pressure selling and other mal practices and really sort ensuring that data is stored in appropriate places to conform to gdpr for example in the communication space it's used to caption meetings transcribing recordings and actions from meetings and interviews and expanding to supporting identifying sentiment and emotion from audio and transcripts and identifying speakers automatically and then in the customer experience analytic space it's supporting agents and supervisors to ensure that they are efficient and using their skills to the right purposes now offloading some of the manual tasks like documenting conversations while supporting QA consistency and things like that for all conversations the overall need is similar in these markets but the speech types accents and domains and accuracy can be different this is why speech might expense a great deal of time making sure that the models we have support what we call any context models making sure that we can support use cases that are noisy conversational or with prepared speech or a single speaker or multiple speakers or accents you supporting terminology usage of these challenges should be overlooked but we certainly see these as our problem not a customer's problem so supporting multi accented models for example provides support for global conversations and do not rely on customers having to choose American English or British English for example even listen that is the first step of providing a service customers can use without requiring them to provide data that had become very experts themselves and need to engage in professional services agents and things like that to meet their needs speech recognition really isn't just about accuracy sure accuracy is important but it's just one of several areas that are important so that core accuracy need does need to be aligned with use-cases industries and domains the things we just talked about but also there's a huge importance and flexibility of deployment being able to use public cloud private cloud integrate into solutions either on-premise or not use single high scalable solutions or distributed multi deployment scenarios like using virtual appliances or on device mechanisms and then language coverage this is not just about whether we support English and French but also the dialects and the accents and the terminology that's used in that language was also a big area to think about so these three areas will need to be considered when we talk about the value of voice and they'll be aspects that we come in throughout this webinar to help understand the challenges and practices now and how in future things might change around them you so let's talk about pure word accuracy itself first the industry uses a mechanism to measure the actually of words called word error rate which is abbreviated to work or wer the higher this number the more the mistakes have been made in relation to some reference text that provided and that's as simple as it is on this slide here just shows some independent testing that shows some of the results we've have recently which happily show speech Mattox doing really well in english and french and dutch we do do lots of other languages as well I think we have more than 30 different languages now that we support this word error rate is the baseline metrics for performance testing these results you're seeing here I think if you're a media and broadcast customer so not necessarily representative across all use cases but certainly representative flat market where a rate is consistently improving and we are adding more and more label data all the time and improving the algorithms that surround that data but these gains are beginning to diminish from just doing this there are still gains across accents and doors and multi speakers to be had but long term that is not going to bring the next step challenge is great and it is good enough for integration in some large enterprise tactics and for many applications that we've talked about earlier although I think actually now should really start feeling placed with the word understanding measuring these word errors is like saying no the best car in the world is the one that's measured by going the fastest but what happens when you get to a corner you need to start thinking about the ability to steer so suspension and traction all become important aspects of maintaining that speed well automatic speech recognition is exactly the same word accuracy is one thing but without delivering understanding that it won't deliver the value when an agility you need when you get around that corner this means things like punctuation sentiment disfluencies audio type language ID speaker ID dilation timings the list goes on these are all important as there are aspects of that understanding and it's much harder to measure those things than it is to just apply some mathematical formula to the words and get a number that we talked about earlier so that number is going to diminish in importance overall as everybody gets that same level but the understanding is where the true value exists you just take punctuation for example and think about that famous book eats shoots and leaves you'll see that the word error it means nothing in terms of understanding without the punctuation this applies to all the other metadata aspects we just mentioned - so to truly understand the boys you need to value more than just the words and as the future progresses more of this understanding will be included in the voice arena not by pulling together many different solutions but in one central real-world model that considers more than just the words may well start to use things like images video and other textual content surrounding the the scenarios to help provide multimodal understanding and get the next level so this is where the sort of solution separates from the technology the speech technology stack will continue to evolve to provide all the metadata needed to build this understanding how this is interpreted is going to be different for every use case that is being applied to the important part here is that the speech platform provides all these attributes in a way that supports the value of these solutions the full value of these solutions cannot be gained without a high-quality understanding being delivered from that speech platform and this is where those other attributes come in importance too so the ability to deliver in a manner appropriate for the use cases so begins simply include that flexibility and the languages that we mentioned earlier so if we think about flexibility today you can get speech recognition from the public cloud and that's great but there is more to a voice strategy than just a single speech service that are a growing number of speech systems of a domain-specific for example focused on meeting transcription some are deployment specific I only work on it on a phone some do speech recognition others do natural language some do a combination and as voice becomes more prominent you don't really want a point solution for speech recognition in IVR and one for agent notes in the call center and one for your meeting technology and one for your chat bot so where do you where you need different tech providers for different stages like ASR versus that natural language intent detection do you want to manage multiple assets for languages and lexicons then there are aspects of privacy and computers private combat ground phone desktop flexibility about performance operation point model size speed that because it's all going to matter so there's a very important area to to think about language kind of goes hand-in-hand with accuracy and it starts to encompass use case to help with things like technical terms places numbers multiple speakers etc in some domains language changes fast resolving social media slang is invented almost daily and in business perhaps slower but terms like bricks it or Kovac still appear regularly and today the technology is what a stage I would call regular update stage where the base language gets updated periodically and there are tools like custom dictionaries to help company companies manage the transitions between these these updates as well as provide general uplift and this is because the trading process for these systems can be complex in gathering preparing and using data and it can cause quite a long latency between wanting an update and actually achieving and delivering it this is going to get better but probably not through just gathering more and more and more label data that won't fix the latest see latency or the ability to improve for many use cases where accessing data is difficult and getting the data label is expensive and tough so as the future progresses there are mechanisms that became impossible that reduce the amount of label data need to train and make much more of a continuous learning and improvement rather than that in periodic improvement that I just talked about this is going to head towards reducing the latency and improving the AXI generically for the use cases and specific needs and I hope this will able customers to move from doing machine learning to what I've called machine training it's sort of comparable to be able moving from being a student to being the university knowing how to train as much more powerful than just being the student itself you so what all this technology is really enabling and where is that where it where the power comes in is supporting the business transformation that has brought my workflow automation the automation is different in different markets but the driving factors for success here are the underpinning accuracy and understanding that is provided by the speech engine we believe that we need to provide the core products that enable solutions from from value-added service providers to enable them to take the voice and apply it as required for their need up level their value there are some amazing solutions to business problems but without the excellence of the core we will not be able to do them full potential so let's just talk to a bit more using a few partner examples on the way to explain a bit better and see where the value is today and where that can go moving forwards so voice technology in a workflow automation is not about all about recognizing humans with computers and uploading questions to voice assistants and chat BOTS it's about making human interactions better so this includes scenarios where the humans are in the loop this covers many use cases such as adding subtitles to videos automatically to enable them to be consumed easier or meeting note-taking or call analytics analysts believe this process automation will be the number one use case for AI for some time to come and this is starting to enable hyper automation which is the core of a digital transformation story how can automation being the process of automating anything that that can be and not leaving anything behind so this is applicable to many use cases and it sports transformations like the ability to automatically segment video and audio feeds identifying speaker segments and capturing identifying valuable content and supporting production workflows in the media sector it enables things like IVR to become voice enabled chat BOTS or digital assistants and they've empowered customer experience management it does things like automate the note-taking for CRM and call history and for meetings act in general to support human to human communication automating meetings and post meeting workflow and it supports things like automatic detection of risk with an audio detecting compliance statements and the like at this point workloads becomes seamless the computer and human elements merge reducing overheads and increasing throughput and value delivery accurate speech recognition forms the bond here technically between the speakers and the technology so although speech-to-text is not the only technology needed here it forms the foundation the interface and the communication paths but simply if you and suddenly customer asked you can't action it through any of these mechanisms you so Daisy our greatest when our partners using this technology in this space they are a leading provider of a speech analytics software and they enable businesses to understand and focus on customer calls that matter through speech and sentiment analysis software that they call Lisa Lisa provides speech and sentiment analytics it provides risk and quality management the software enables businesses to understand the intent context and empathy of a conversation quickly identifying compliance issues and mitigating viscous scale the volume of voice data within content their centers is absolutely massive by transcribing voice into text format is the first step and baseline to understanding that content and the context of those conversations lissa context constantly context editors can transform voice and gain new and valuable insights to reduce the risk of regulation and compliance breaches and listen really helps with that businesses can improve process efficiency streamline workflows reduce customer churn deliver better customers experiences and ultimately reduce costs and protect their brand so using speech the application provides accurate transcription functionality at scale for large volumes of voice data regardless of dialect accent or location so working up as a global market makes accents work very important and they benchmarked across many accents finding great results especially in noisy contact center environments so how but earlier make this even more powerful well often in this space data is private as part of the contract between the customer and the company where the conversation is being held this means that traditionally it is hard to get to that next step of accuracy of transcription understanding as training can only be undertaken using similar data which is also challenging to obtain or simulated data and as we step towards the future I believe we'll be able to incorporate learnings from this data without the need for that that problem to share the data beyond the company boundaries enabling biasing of already high-quality models to the distinct needs of the customer and providing something there heads towards and maybe exceeds the better than human levels that might otherwise not be available so a little bit further out maybe the next two years the expectations that companies are really going to be relying on artificial intelligence to make and support decision making so there is expected to be a really significant trend towards using AI for the likes of Risk Reduction brand protection etc yeah but why wait those two years there's plenty that you can do to start them that path right now a partner we have in this space is Soho squared they are a risk mitigation company and they have a product called speech squared which is a solution designed to take the latest in speech recognition and machine learning and derive the right analysis from the content this provides companies in legal compliance and contact center markets the invaluable ability to analyze voice data from calls and speech Square brings high level of flexibility to data analysis for real-time or pre-recorded voice supporting high-volume mission-critical accurate speech recognition the solution really delivers the ability to derive insight from voice data but also manage risk and can be deployed in customer managed environments to enable control of a personal or sensitive data so as part of this product speech Mattox transcribes the voice data into accurate contextual understanding for analysis and speech squared allows the business to identify and address the risks as well as pinpointing missing sales opportunities along with a being able to identify cases of fraud for example the legal industry can identify risks with the data and it could support event reconstruction workflows so clearly lang and deployment flexibilities support security here is very important for these solutions which is fantastic but the understanding is going to be really important insight from pure words can be very misleading if you think back to that each shoots and leaves comments are made earlier but understanding pauses disfluencies and leaving on to things like speaker identification and language detection disambiguation techniques based on other context like location or images again a lot of this might need learning from local data so supporting mechanisms that are lightweight and enable learnings from local content will be important this relates back to machine training rather than machine learning as I said a bit earlier and the need for this in conjunction with operating in the secure environment is a challenge that must be solved the ability to continually but by us adapt and teach future models from current learnings is going to be important here in that journey to improve the power of these solutions okay so moving on slightly to talk about the beginning of human augmentation we read a lot about virtual reality and augmented reality and maybe that conjures up images of robots and fantasy but it's not really as far away as you might think human augmentation is already being used in the voice world and as far as I'm aware we haven't assimilated anyone into the Borg yet sorry for a Star Trek reference if you're not familiar with Star Trek I'll leave you to work out what that meant so let's look a little bit what you mean about human augmentation by human augmentation it's all about empowerment of everybody and enhancing human abilities through technology so it supports a distributive workforce making sure people have access to the right information at the fingertips wherever they are and right now this is very relevant I mean we've all shifted to working from home and it might stay that way for quite a while supporting people in their workflows without access to the same environment they've been used to is really important keeping people trained and prepared for information that is changing by the minute is also really tough and when that workforce is remote is even harder so making sure that information is relayed to people consistently when people are just bidding is really important customers still want answers they want the first time and then want them to be the right answers we still have to meet that need so all granting humans providing by providing them with proactive insights based on the real-time conversation that's in progress right now can support these workflows and turn great people into effectively superheroes supporting the brand getting great customer support scores and really getting the best insights possible for the business and this can be seen in action particularly right now in call center applications where a wide knowledge is needed to resolve issues presenting the right information to the agents based on the conversation continuously enables the agents to focus on the customer and do the right messages regardless of how recently the advice was updated call centers are being asked agents are being asked to deal with more and more conversations that cover more and more diverse topics and training is becoming very hard for that how can the technology really help here well this is really part of the continuous intelligent space where real time and near real-time systems are understanding conversations being held and offering help this can be in the form of tracking workflow to help make sure the conversations are helpful but efficient and containing all the right information to be compliant with regulation but also with business standards to support the brand for example it can also help in the detailed information by linking to the reams of information that is out there in the form of knowledge bases to automatically provide assistance so supporting these agents in real time as they need to answer questions about pricing competition product specifics but they can't possibly remember all the details of these things can really turn every contacts agent into that Hooper superhero that I mentioned being able to answer quickly accurately efficiently and support the confidence that customers have in the both the company and their brand there is another aspect here too and that is the supervision an agent support this continuous intelligence can also help supervisors operate and support the contact center employees by getting a much better view of what's going on helping understand not just where they need direct help in a call that is perhaps in progress but also understanding the metrics of operation by getting a complete picture and not something based on a few small samples of interactions that occurred something else that is really really hard to apply to a remote workforce so where does that future take us now from this point well this ability to perform recommission and look up actions in real time is quite a new new situation to be in so it's still improving fast in regards its speed performance again the more metadata that can be provided about the context and the way things are being spoken the better so we will get all these features but they need to be included within the write latency model and that's still a challenge getting all the information at the speed is needed so there's an important aspect to tackle here conversational search of indexing will also be very important for more and more use cases such as workflows in meetings interviews and broadcast to get more immediate access more immediate access to targeted information during activities to get results directly in flow rather than having to take issues offline and return them later which is much much more inefficient for everybody perhaps it'll help everyone feel more productive wherever they are within a transaction if it can conclude first time and this will really complete really contribute to that so again those speech engaged in this agent assist is an integral part of the puzzle customer service platform which uses artificial intelligence and machine learning technology to gather relevant information from all the customer queries that happened in the past as well as ongoing customer requests the automatic collection of information and data really gives agent that complete view of the customer journey when the agent is faced with resolving a query these insights are at the agents fingertips and even them to resolve inquiries more efficiently whilst keeping the customer satisfied a great deal of information is locked in the voice data from these recorded calls to create a full customer interaction history it's essential to transcribed voice and enable information to be gathered and insight created agent assist utilizes that speech Mattox technology to power that transcription and deliver actual insights and streamline workflows within those contact centers understanding that history of customer interactions to drive insight drastically improves the customer experience and customer satisfaction utilizing automation technology has an impact on employee engagement and ultimately on the bottom line I'm having transcription at the Corvis platform really enables the creation of a complete view of all those customer interactions with the contact center and enables unlocking of that huge amount of voice data that's there so transparency and traceability is going to become more and more important as system start to support human augmentation and decision making the speech detector text technology it can really help part my customers ensure that this is possible but it is keeping that 60 degree view interactions or detecting issues immediately so it can be actioned and not forgotten the central factors here in the report real time data is delayed there's much easier to see the correlation between an event an action it's left time between them and there's immediate then if it's immediate then it's really really obvious connections and the huge advantage so this real-time or near real-time transcription plays an important part in that continuous intelligence that's needed for that human augmentation covert has also changed the way we operate and probably forever the importance of being transparent and traceable has had a spotlight on it there is a need to change the way we use workflows and use automated technologies to support not just the fast pace of information change but scale with the dynamic of the workforce as restrictions change as people become unwell or unable to operate and the impacts of sudden increased load on places like call centres where help and support is needed to many many people I think we've all seen a slowdown in the response time from these call centers and there have been some real challenges in prioritizing vulnerable people as everyone wants to understand what's happening with deliveries and holiday cancellations and flights and hotel reservations scaling is hard especially when it's as dynamic as this automation and human organization augmentation are both ways of supporting these situations allowing the simple queries to be dealt with automatically and the agents to deal with those complex interactions within the context Center understanding who said what when is very important making sure these facts are accurate can be the difference between making the right choices and losing a customer especially when it comes to district dispute resolution using speech recognition to record conversations to be able to automate the understanding of the flow of a conversation and retrospectively understand what was done for Quality Assurance is one thing but to be able to be proactive and stop an error in the first place is way more powerful having a system that can see what's going on in near real-time look at previous elements and attributes and make sure the right choices of aid immediately can reduce the number and length of calls and ultimately result in a happier customer through better customer experience all of that is a win-win situation that should result in repeat business and reduce churn so the speech recognition paired with other automatic rated technology can really transform this side of the business any technology that's built on AI or machine learning is under scrutiny to make sure it's trustworthy but this doesn't just apply to the pure technology there are solutions built on this technology that are supporting businesses with the same problem and to do this we need to consider ethics open this accountability competence consistency there are roles for technology people and the combination of both needed to get the right balance of help and support whilst maintaining those five pillars of trust it cannot all be about the technology it needs to be thought of as a whole quite a bit of the conversation today has been around these aspects so I'm not going to go back into the detail of all those but there is still a journey to go on as to what we can expect to happen here and how we were supporting the bility to improve and grow in this respect speech Mattox already provide support for operating speech technology and models that enable data to be carefully managed maintaining security boundaries and keeping data on premises and in customers own domains which is a great example of the way of helping this this will extend to the edge enabling voice data to remain in the end-users hands for some cases and there is probably the need to operate in hybrid models to support these different cases if we operate in multiple places we may wanna share models shared models to ensure that the technology works everywhere as expected then there are some issues around how the technology can improve the data remains private without a machine learning companies seeing data how can they deliver continued accuracy needed for all these domains there is a lot of work going on here to enable learning from data in federated ways that support privacy and accuracy and this will bring the next level of controls the businesses wanting to use the technology and conform to growing legislation that we probably will see coming over the years around increased data security and management and ownership of that data somewhere that those pillars of trust that I talked about are really being put the test is in our liquid voice platform so this is another one of our partners that derive insight from voice data that can protect vulnerable customers and minimize GDP our related risk for companies this is particularly relevant in the finance financial service industry where the FCA recently stipulated that firms have a continuous view on how their customers are being treated in all interactions so accurate transcripts can be derived derived from noisy call environments and of course all file formats which enables liquid voice to provide a complete search will archive to used in a range of scenarios which include things like dispute resolution quality management and event reconstruction this consolidation reporting and measuring function provides context centers staff with full visibility and control over every customer interaction and will ultimately reduce fines for businesses and protect for vulnerable customers okay so we've reached the end of the story for today and I hope you can see that we're already at a point where speech Mattox and voice technology can solve and support many solutions all of which help optimize business making workflows faster more efficient and valuable to all parties some through offloading work from humans to automated mechanisms and others through supporting people to help them become more amazing at their jobs scaling businesses is hard especially ones that need people to interact this technology can really help good growth and scale in the best ways delivering better experiences on the way for both the customers and the employees alike this is a trend that is not going away and voice will continue to be an important part of this across an increased marketplace this technology sells a lot to offer and businesses are increasingly looking at ways to use this core technology to add understanding and value to services they provide now and ways that they can have more value in the years to come and I really look forward to supporting those businesses through that journey you cool so we are at the point of questions so if you just give me a second or two I will leave the smooze questions and see if we can get some answered so there's a question here that says accuracy is essential for timing and effective voice transcription how will you reduce the need to edit and correct automatic transcripts in future it's pretty good question so automatic speech recognition is never going to be perfect right people aren't going to be perfect either when you transcribe a conversation yourself you're using a great deal of context that's just not available to the machine or to another person if you are outsourcing it to a company for example some understanding of the subject all the people involved or whether people are located or the terminology or the spelling of their names is essential to get to that absolute perfection speech speech recognition is getting more and more accurate all the time is coating with noise and accents and language and terminology and that will just continuously improve all the time in fact in some places speech recognition does actually better than people it's still not perfect but it's good it's really getting there and we are continuously looking as I've talked about a bit about ways to get more and more data that will help with this gathering data and labeling it can be an incredible time sync not just to the company producing the speech recognition system but the people providing the data the way we get it labeled it can be a tough way to step up accuracy so we look at lots of ways of continuing up this path without needing some of those huge overheads and needing professional service and achieve that so kind of watch the space in the industry there's some really cool things coming on that will really help with that and the you see we'll continue to improve but probably will not ever get the point where you don't need to edit anything if you want it to be absolutely perfect for you there was question here about someone who's interested to understand how we're dealing with speech sex where our where more than one person is speaking at a time this is a real challenge I don't how many conference calls have you been in where there's over talk and you can't understand what people are saying because there's two people's week at same time so you end up asking them to repeat it right that's that's what happens obviously there's our machine reading things it can't do that my far the best way to deal with this is to keep the speakers separate for as long as possible so if you can use a channel per speaker so a microphone pro speaker and transcribe all the channels separately and reassemble it it's by far the best way if you really want to get accurate transcription with that overtalk speech matrix off we can do this for you if you put all of those audio into one single file with a speaker per channel we can transcribe them separately piece it back together and return to you as one big transcript and that's the best way to solve over-talk we can provide solutions that indicate speaker changes on a single channel audio but as soon as there's overtalk that rapidly becomes impossible for anybody to do a good job of that so you have to try and avoid that in me in the first place if you want to so there is a question here which is going to paraphrase it's quite long it's asking about how we tune speech recognition to different scenarios and coping with you know 8 kilohertz telephony in a call center and the specific domain grammar and improving all all of that that system so this is the place where speech magic perhaps take an approach that's different from from others where we train on an incredibly wide set of data sources data types speaker types domains accents and spend a lot of time using some of our very clever core technology as well as some people to balance that data and make sure it has a good balance and supports a really really wide set of use cases and speaker types and frequencies so our model works really well whether it's a context or 16 kilohertz single speaker multi-speaker different accents we don't for example optimize our technology for a specific microphone or telephone equipment provider we make sure that the trajectory train uncovers one of range to make sure that it should be best the way we operate really is we would say we've encouraged people to try our system with their hardware and you know come talk to us about how we might be able to improve it if it's not good enough out of the box after that this will try I guess really the only piece of advice I would say is try and provide the audio to the speech recognition system in as raw as format as possible so exactly as it's come out of the recording equipment the better so don't try and compress the audio into some horrific speaker audio codec try and use lossless compression and don't try and do things like noise from the file to remove it because even if a audio sounds like it's got better for human it doesn't necessarily sound better to the computer the computer is more likely to recognize it in its raw form than if you try and edit it and play around with it so there's a questions around where the speech matters provides sentiment analysis right now we don't some of those partners that we talked about take the speech Mattox output and assess sentiment from the transcript along with some other technology they might use themselves as well to to provide that technology but we don't at this point provide sentiment analysis directly ourselves ok so question here about the ability to scale up to serve large archives are there any plans for large volume cost metric that's that dramatically lower it says right now a vast digitization of videotape libraries is happening without transcription because it's part to expensive just speech medics have any plans for this area of media operations so one of the things that speech thematics enables you to do which may or may not help but I hope it would is take the computer the data so you can use our technology in your own premises on your own compute in your own clouds which removes the need for both egress and ingress costs of moving data around also removes the any costs that a provider of the technology would add for running and operating the system so you can operate the solution on the compute you want at the speed you want where you want yourself and manage that cost and speed at the scale you need yourself which quite often can reduce the overall cost that's there anything I could suggest as an odds of that right now okay so there's a question here about one of the speech Mattox can integrate into all CRN's speech Mattox is a provider of speech recognition technology that is a technology that can be integrated into anything somebody has to do the integration those speech magics do not provide any of the integrations directly some of the partners are talked about here and others take our recognition and provide integration into those solutions so if you want integration into a CRM you would either have to do the integration with our API yourself or use an integrator or buy a solution from one of our partners who integrates into the CRM that you are wanting to use okay looks like that's most of the questions that have through if any more come through after presentations finished we'll do our best to answer them and get answers back to you so thank you very much for attending I hope you've enjoyed the session the recording of this will be available in a couple of days time and like I said I will endeavor to answer any questions that didn't get answered during the session after the event as well so thank you very much enjoy the rest of your day bye you 