 hey everyone i'm Zhuangdi Xu today i'll present our work coral pie a geo distribute edge compute solution for space-time vehicle tracking space-time vehicle tracking is to create a space-time track of suspicious vehicles using camera networks in a city the motivation is manually finding a track of a vehicle across multiple camera feeds can be label intensive and error-proof that's why we want an intelligent system to replace the manual labor and aid human decision making specifically the goal of coral pi is to explore the design space of such a smart camera system the live camera streams are processed locally at ingestion time and cameras collaborate with each other to build up the trajectory of all the vehicles in real time cameras considered in our systems are statically and geographically distributed each camera has its own computational resources and we have a horizontal network resource between the computational resource of each camera the contribution of coral pi includes a scalable by design distributed edge architecture which allows pluggable computer vision components by domain experts we make use the idea of bounded horizontal communication to also bound the communication on each camera we have autonomous camera version monument to handle the camera failures we make careful design considerations for scalable implementation we have a proof of concept implementation of coral pi based on the raspberry pis and coral tpu we did an in situ evaluations of coral pi using video streams from real-world cameras and simulation the following of the talk would cover the functional decomposition of coral pi the placement of functional components and implementation and performance evaluation of coral pi let's first look at functional decomposition of coral pi we decompose coral pi into five function components among them vehicle identification communication and vehicle re-identification are running locally and concurrently for each camera the other two camera topology server and storage are shared between cameras next we use a real world example to show how color pie constructs the trajectory of all vehicles in real time when camera joins the system each camera will send its longitude and latitude information to the camera topology server server will then return the proper information for each camera for example this support information indicates for vehicle moving from the right to left camera b is the downstream camera of camera a and camera c the downstream camera of camera b after camera joins the system every camera will be doing vehicle identification locally and concurrently the goal of vehicle identification is to identify a vehicle entering the camera's field of view and when a vehicle is leaving the camera for example the red vehicle is leaving camera a camera a will generate a unique detection event for the red vehicle the detection event has three purpose first purpose is for communication based on topology information camera a will forward this detection event to camera b's candidate pool to inform camera b the red vehicle is heading towards you similarly later when the grey van is leaving the camera a camera a will also forward its detection event to camera b notice meanwhile there are other vehicles leaving the camera a in the opposite direction their detection events will not be forward to camera b the second purpose of detection event is vehicle re-identification now let's look at the computation happening on camera b when camera b detects the red vehicle is leaving the camera it will first send detection event b7 to camera c then it will match b7 with detection event a0 in its candidate pool using the features similarly when the next grey van is detected by the camera b its detection event b9 will also be matched with detection event a2 in the candidate pool however because the gray van is turning into the parking lot the camera b will not forward b9 to camera c eventually when camera c detects the red vehicle it will match the detection event c12 with the b7 in its candidate pool the last purpose of detection event is storage we store the trajectory of all the vehicles as a graph where the vertex is detection event the edge is a successful matching between two detection events we allow multiple incoming and outgoing edges for false positives from vision techniques and we do frame storage for traceability of our application next we're showing the placement of these function components on our system architecture from our preliminary experiment we found live processing speed should be faster than 10 frames per second to achieve good results which implies every subtask should cost less than 100 milliseconds the second goal of placement is scalability in coral pie we decided to place all the heavy lifting real-time processing on device to achieve the deterministic latency bound and to have no pressure on the backhaul network bandwidth and we heavily use pipeline processing on device to explore the parallelism to achieve the 100 milliseconds latency bound next we're offloading the storage to nearby edge the reason is because limited resource devices like raspberry pis have limited storage capacity which incurs a high on device storage overhead notice we send frame to edge asynchronously which means storage is no longer at the critical path of pipeline processing finally we place the camera topology server on cloud because cloud have global knowledge to maintain the geographic relationship between the cameras topology change is infrequent it happens when new camera is deployed or old camera is removed next we'll show the implementation and performance evaluation of coral pi pipeline processing for each camera spans across two raspberry pis and one edge coral tpu in this work we focus on system issues so we choose to use the off-the-shelf vision techniques coral pi is implemented in python and we use the ZeroMq for communication between devices edge and cloud we use five on campus roadside cameras to build a real world testbed and for controlled experiments we use two thousand frames from each camera with labelled ground truth our work includes five evaluation hypothesis due to time limitation we'll only show the first one in this presentation which is the processing speed should be faster than 10 frames per second this micro benchmark shows the latency for each subtask happening on one camera which span across two raspberry pis here we can show the two costly operation is load operation and inference operation while inference  is expected because it's a deep learning method load is not expected this shows image decoding and encoding is inefficient on pi and becomes the bottleneck of the processing pipeline overall we achieve a processing speed of 10.4 frames per second as conclusion in this work we build the end-to-end distributed system called coral pie to create a space-time track of suspicious vehicles in real time we implement coral pi on raspberry pi's and coral tpus we do in situ evaluation with both real-world cameras and simulation to show the practicality and scalability of coral pi as future work first we want to look for low-cost and better vehicle re-identification algorithms besides color histogram next we want to look for serverless computing to scale the resources for each camera based on the number of activities happening within each camera that's all thank you for coming to my presentation 