 [Music] hello and welcome everyone we are so excited and got to have you join us virtually for this build session on how to explain text models with interpret ml my name is Marissa McKee and I'll be the moderator for this session I'd like to highlight that there is a dedicated area for you to ask your questions we would love you to do that because that makes our session a lot more interactive I'll bring a bunch of questions to the presenter to address in the QA section towards the later part of this presentation without further ado let me introduce minsu Thigpen to drive us through the content min-soo the floor is all yours thanks Manish hi everyone and thank you for attending our session my name is minsu and I'm a p.m. under a dremmel responsibility I team we're super excited to share with you interpret text interpreter Mel's new offering for explaining models with text data in this live session I'll be giving a brief overview of our new explainers along with a quick demo so I'll be giving a short introduction of how we came to create interpret text as well as providing a motivating example from your life then I'll give a brief overview of the general NLP pipeline that we're talking about before diving into our explainer is and how it fits into everything then I'll be walking you through a demo notebook and you'll be seeing these explainers at work and then I'll go straight into opening up the floor for Q&A so please feel free to post your questions on the chat as Manish I said before and we'll try to answer them in the end so if you've seen the responsible ml live session before this one on getting started with understanding your model you'll recognize interpret ml as an independent org that houses multiple interpretability techniques though much of the functionality for interpret ml is for tabular data we've got many requests to extend us support for text data we currently only support topic classification but are looking forward to growing our repo to include more olympos NLP scenarios and we obviously welcome your contributions if there's an NLP scenario that you want to see supported please let us know in the chat interpret text comes with explainers that support Bert and Arnaz as well as theater scikit-learn models with our open-source toolkit users will be able to take advantage of the state-of-the-art interpretability techniques with a common api across integrated libraries as well as its own visualization dashboard for understanding which key phrases and words were important to your text classification so let's consider a real-world example where you might actually need to use this let's say you're a candidate applying for a job and a hiring manager is going to use a machine learning model or an algorithm to categorize the freeform text that you're putting into your application maybe in two distinct buckets or categories that they care about and raise red flags if your submission fall into a certain category so in a high-risk situation like this where obviously your jaw your opportunity for a job is on the line it's very important for hiring managers to be able to understand what the algorithm is doing when raising these red flags as such as needing to know which key phrases or words that it used in order to make that assessment so let's take a step back generally speaking there's two broad classes of NLP approaches though it says classical ml technique and deep learning in classical techniques the first step is pre-processing as you can see in the first box it involves steps such as tokenization normalization noise removal then that process data is fed into the model we support models such as linear regression like GBM or random forests the output depends on the scenario you're modeling for in our case sex classification for some of these approaches the internal functionality is very well understood and as a result the users confident into explanations provided however these methods although widely adopted and easy to use are pretty limited in their accuracies on real data you know real world data sets so then comes deep learning right so recent advancements in NLP owe much to this such as you know the neural networks such as birth these models have high accuracy on NLP tasks however the trade-off is that there are black box models and their decisions are thus difficult to understand and when we say black box model we mean you know you can't really see what the inner workings of your algorithm is the research community has created a suite of state-of-the-art interpretable models ranging from post hoc analysis to plugins during training so promising these methods are kind of hard to implement in practice and not accessible to everyone I like data scientists therefore to make these recent research Troutman's more accessible for data scientists in this toolkit we've released open source implementations of three explainers the first one classical and two state-of-the-art that provide local explanations on text documents so the first that we'll go through is the classical text explainer which is a standard set of glass box models and then the second is the unified information explainer which is a post hoc method and model agnostic and the last one is the introspective rational explainer which is a plugin method and also model agnostic next I'll talk briefly about these models in order to give you a more good sense or understanding of what they actually do so the classical text explainer is implemented as a wrapper around the entire NLP pipeline so you can kind of think of it as including all of the text pre-processing encoding training etc obviously you can support and you can supply your own data set without any external procedures and it's compatible with linear models with support for coefficient and tree based models this API is modular and you can swap out nearly all the components the default configuration we have for users to use is a one grande bag of words first I can't learn the repo as you see on the bottom of each slide has has a sample notebook illustrating how you can use this API for multi-label text classification scenario we'll also be seeing this notebook during our demo the unifor unified information explainer is the our NLP interpretability model and was developed by researchers from MSR Asia the paper was presented at ICML in 2019 so pretty recently last year this explainer is post hoc meaning that it needs a previously trained or fine-tuned TNN model its model agnostic as long as it's at the end so it works for Bert RN n CN n LS TMS however we've only implemented support right at the moment for explaining fir and we want to extend it to other dnns in our future work as well it's a mutual information based approach and it provides the importances of words as it goes through all the layers of the neural network since each layer is explained explanations are unified and coherent thus the name I'd like to provide a quick intuition on how this method works but for more details please see the paper the name of it and a link to it is at the bottom of this slide so the key task here is to associate layton representations of the word with interpretable units in the hidden states of the layers we want to quantify how much information of an input word is included in each layer so very important word for example we'll have a lot of information encoded there are two forces at work here that you can see in the diagram as the word embedding is going through the layers of the DNN it is perturbed and we're trying to add as much noise as possible what we want to see is how much the output changes as the noise is added so if a lot of noise affects the output then it is important word for that layer if it doesn't affect the word then it's not that important on the other hand the loss function tries to minimize the difference between the perturbed hidden States and the original hidden states since obviously we don't want to end up explaining a completely different model by perturbing it too much these two functions work hand-in-hand to help us better understand how the words are being propagated through the layers of the neural network and there's also a sample notebook for this as well in the repo so our last explainer is the introspective rational explainer and this is our other state of the art explainer it was developed by researchers in the MIT computer science and AI lab and was presented at am NLP last November so also pretty recently more details about this method are in the paper but here's a brief intuition this method introduces a plugin into the training process and makes the blackbox model more interpretable this pocket is called the introspective generator and you can see it delineated by a dotted box in the diagram to the left this introspective generator slits the text into rationals and anti rationals which are denoted as r and r superscript C the rationals are a subset of the words that according to the introspective generator are important on the other hand the anti rationals are not important according to the introspective generator the introspective generator simultaneously trains the model on both of these it tries to maximize the accuracy of the model when only using the rationals and conversely minimizes the accuracy when using anti rationals since the model only sees the rationales at training time to make a prediction strong guarantees are provided about which phrases matter to the prediction generally no other method provides such strong guarantees so now that we've gotten through a brief overview of the three explanations I'd like to take a moment to look over this table which is also in our repo readme and kind of explain and walk you through what are the different you know scenarios in which you would be using these explanations or explainers so we recommend using the classical text explainer when using a more classical ML approach to NLP and if you don't want to deal with the NLP pipeline we offer that for you as well however when needing to understand a neural network at a layer wise level we highly recommend the unified information explainer and then if let's say you for instance have an RNN or a combination of Bert and RNN we recommend using the introspective brush like explainer awesome Thank You minsu let's answer some questions so the unified information explainer and introspective rational explainer seem very similar so if introspective rational explainer is also supporting bears and iron and models can you explain a little bit then what is the point of unified information explainer yes so in many instances you actually want to see at which layer your you know the features are kind of having the most important values for and so to understand sort of a layer wise level you're going to really want to use a unified information explainer as opposed to the introspective rational explainer awesome another question is what if someone uses a brand new feature writer I know you mentioned one gram but can can we bring another a different feature Iser absolutely so the classical text explainer is highly modular and we really encourage people to swap out whatever parts they want to bring to the table and you know leverage whatever we built when they don't want to do the work on building it so they can for sure bring their own feature Iser and swap it in awesome and last question if I have an existing PI torch architecture it can explain that - yes so as you can see in this table for the unified information explainer and I introspect the brush la explainer we do provide input model support for PI torch awesome thank you yeah so we'll take a little bit more of the questions later but now let's just go ahead and switch over to the demo notebook to take a look at the classical text explainer as well as an introspective rational explainer all right so as you can see I am you know projecting this from an azure of yan notebook and so you you know you can pip install whatever the package that we have they interpret text you can set it up in your environment you can also just pip install with imports this in your Jupiter notebook but you can definitely run this on our VM so these two notebooks are as I mentioned earlier available on the readme the repo under sample notebooks so you can go to interpret mo backslash interpret text and be able to navigate to the sample notebooks to see these and so here we showcase how to use interpret text for the classical text explainer we sort of you know although we're using logistic regression and one gram bag of words here you can obviously bring in whatever feature Iser as we discuss with varnish we also leverage a lot of functionality from psychic and Spacey so you know we really want our tools to be interoperable and play nice with all the other tools and popular tools that you guys are already using so you can configure your parameters and load up your data and then you can instantiate your explainer by sort of calling this you know API call to create your explainer and so you can see that you will be able to see that this is a similar you know pattern for the introspective rational explainer as well so once you have your explainer then you just go ahead and do the standard split your data into training test and then you want to load the cross bar that you care about and then be able to fit and test your models so we also provide some functionalities for you to just do a quick sanity check with performance model performance metrics that you can just run and so let's you know we see that this is the best classifier here the performance metrics and now we sort of come to local importances what that means is for the text document that we've put in here we want to see okay for this document what are the specific keywords that really contribute towards this prediction and in this sample notebook specifically we're using the moai dataset and we are kind of categorizing different text documents according to genre such as fiction as we see here or government documents or it's a by text classification scenario so as you can see here you just look create your explanation dashboard that we have custom-designed for using our explainers for passing the local explanation which you've already loaded up your document with and in here we you know loaded up the document I traveled to the beach I took the Train I saw various dragons and elves and we see that this was predicted as fiction there's a slider bar here to sort of navigate up and down to see top K most important words that contributed towards this as well as being able to toggle okay I only want to see the features that positively contributed towards this prediction or negatively contributed away from this prediction and also one other thing is that you're able to see what are the feature importance values along with a localized string that will show you where in the text this actually is so we see that we have you know the in front of train and a you know period after a train and so we also see this correlates to the text box at the bottom which has a highlight if it's a positive feature importance and an underlying if it's a negative feature importance so we'll see that this user experience of the visualization dashboard doesn't change according to the explanation that you're used or explainer that you're using and so now I'll hop over to the introspective rational explainer we're using the Sam for sentiment tree Bank data set which we're using let's documents that basically are movie reviews and we're going to do a sentiment prediction of whether or not that movie review is bad or good as 0 and 1 and so you go ahead and import your explainer and in this sample notebook which you can find online we direct you towards you know the three player explain our model that we sort of base this off of as well as the SS to SST two data set and you can also see what data set we use where it's linked here so we go ahead then set up our parameters we want to you know set up the training procedure parameters model configurations determine if your model type is an RN and if it's not an RN n and then we load up the data set and we're able to do the tokenization and embedding so depending on whether or not you have an RN n or a bert we sort of give you the preferred embed encoder for both of those then we can go ahead and you know go through the process of instantiating an explainer which as we saw before in the other notebook is a pretty similar API call where in this case we have to make sure to pass in the model type so whether or not it's an RN a Norbert and CUDA and then we set up the preprocessor and basically give it all the model configurations and parameters that we set and then we fit and train it so we trained it for 100 epochs and we stopped at 20% and we're able to see you know model performance metrics as well such as tests varsity and accuracy and then again we see the local importances for these different text text sentences that you can pass in so we have sentences like beautiful movie really good the pop chrome was bad and the promise is an extremely bad taste etc and so you'll see that these have been sort of predicted as 0 or 1 and we also see the local importance values as they appear as numerical values but obviously these are not so you know helpful to us and our human eye so passing these in as well to the visualization dashboard will give us the same visualization dashboard that will let us interpret what's going on with these local feature importances so we see here that the top three features that are going to you know positively affect the prediction of zero which is a bad movie review is the word bad which is you know that's pretty good sign of whether or not the movie is bad and then the premise and extremely and as you can see here we have tokens UNK four words that are not previously included in the training set and we only have the positive feature importances in this case for the rational explainer - because that is the way that the rational the introspective rational explainer is set up to only have your rationales that you've trained on and gotten the accuracy maximize the accuracy before rate so going back to the presentation yeah so you know thank you so much for joining us and we really really welcome contributions you know we're an open-source toolkit and if you have any questions or thoughts please reach out the repo link is right here along with some other responsible ml resources and more information about interpret ml which interpret text is a repo under and so without further ado I'd like to open up the floor for questions from the audience awesome thank you very much mu sue for a very insightful presentation the very first question is about interpret ml a lot of people actually have seen that in the previous presentation by Sarah Barrett so the question is how interpret ml and interpret text affair yes so as I said in the very beginning the reason we came up with a whole different set of explainer is under interpret text for text data is because the sample space for text data is really really big and not all words appear in all contexts Rea so even the input space behaves very differently and has different requirements than numerical data or categorical data also there's you know a far more complex web of feature interconnectedness ie grammar rules that you can't just like break and like just take words out because that might just ruin like the meaning or the sentiment of the whole entire thing and additionally visualizing text data as opposed to numerical data and their importance values is very different from just tabular data and so it required a different dashboard than the one we designed for tabular data under interpret perfect makes a lot of sense now speaking of different types of data do you have any plans to expand this repo to include computer vision scenarios as well yes so we hope to bring to open source the rich research that is already happening in interpretability for computer vision there's a lot of work happening in interpreting image data and by bringing it under the roof of our interpretability toolkit with interpret ml we want to create an ecosystem around you know interpreting image tabular text data and have it interoperable with other interpretability tools in the open-source community awesome now another audience question can we provide PI torch or tensorflow DNN Barrett's models or or whether it supports a specific library at the moment we support PI torch but yes so yeah so I assume that it's a community driven so you're more than welcome to come and expand it yes absolutely awesome the other question is can you walk us a little bit through the research behind us I know you listed some papers and I guess the audience will be interested to hear how how sort of this collaboration between us and research teams happen yes so for those of you who wouldn't know Microsoft has its own research community and organization called MSR and so that's what I was referring to with a unified information explainer the paper came from MSR in Asia I believe in Beijing and we kind of keep an eye out for all the major and LP conferences and I know eeehm and LP oh man that one's always hard to say is one and also as well as and that's sort of where the paper from Anna sorry she came for and then the introspective rationale came from MIT and we also saw there a paper presented at a major conference and also sorry let me take that back the MSR Asia paper was shown at ICN Oh naughty em NLP awesome and the other question is basically you walk this a little bit through an HR scenario I think it's gonna be very insightful if you share one real-world scenarios where this interpret text was sort of used and and tested and and whether you can share some insights about that yeah so that's a really great question while we were creating this we definitely wanted to take advantage of different customers who had interpretability needs and sort of ran this toolkit with them and kind of got insights from you know what their needs are around this space and a pretty common use case that we found was basically triaging customer feedback people want to automatically create tickets right there receiving like hundreds and hundreds and thousands of customer feedback things via email or whatever tool every day and so they wanted to know if a customer you know if a certain customer feedback warranted a ticket being generated or if it was something that was just like oh I love this feature a great job in which you know you wouldn't have there was no action taken right and so take that one step further and then they might want to classify what kind of issue this ticket might be addressing so that it can be assigned to an appropriate engineering team to kind of get on that and be able to you know solve whatever case that is sounds really great and then there is an audience question Emmanuel is asking I have a use case of basically classifying a comment is truthful or not if it was made by a boat or a real human can this toolkit helped me to basically understand how the model has made its predictions could you repeat that question yes buyer so they do have a text classification scenario of basically whether a comment is truthful or not or whether it was made by the real human so if they can state the toolkit can support that yeah absolutely are actually that's the scenario that we would probably be able to support text classification you would basically have your training and test set and be able to kind of with let's say you have a statement that's made by a bot versus a statement that's made by a human and for you know in the visualization dashboard as well as the explainer you'd be able to generate local explanations for those sentences and compare them and see okay I see a trend of you know if a bot has like a misspelling of this or a chromatic you know like a different grammatical way of using this word then we can see that that's one of the features that go towards predicting this as a bot versus a human so yeah actually I would love to hear more about that use case okay that sounds awesome so let me check if we have any other question okay so is the explanation dashboard also available on UI since we might be able to talk about like basically how can they use it can they host it somewhere how can they access the visualization dashboard as you demonstrated as in like would they be able to build an app and then call the visualization can be hosted somewhere else for instance can they host it and share it with other stakeholders yes yeah so at the moment we only offer the visualization dashboard as kind of it with a widget within the Jupiter notebook but we definitely want to make sure that moving forward we have interpretability which is already incorporated into Azure ml and we want to make sure that in the future the interpret text joins that party as well so yeah you would be able to create a workspace and you know share that and be able to call it wherever you need it awesome thank you so much there are no more questions posted to close the session great so thank you again for attending this deep dive on our new addition to interpretability toolkit and we look forward to growing this community with you and if you haven't attended any of the other responsible ml session I highly and that you go check them out to get a broader overview of the other exciting work that we're doing and you know you can leverage to better understand your models and build responsibly so thank you so much 