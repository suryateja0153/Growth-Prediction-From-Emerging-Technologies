 Many Explainable AI methods have been proposed. But how much Cognitive Load do do they impose? Some are simple, easy to read, but not very accurate. Others are expressive, highly accurate, but tedious to read. By combining them in a hybrid explanation we moderate the trade-off between cognitive load and accuracy. We present Cognitive-GAM or COGAM which has the dual benefits of low cognitive load and high accuracy. Formally we quantified cognitive load with visual chunks and developed specific evaluation measures for human interpretability. Thus, COGAM exemplifies a theory-driven approach to advance human-centric Explainable AI. 