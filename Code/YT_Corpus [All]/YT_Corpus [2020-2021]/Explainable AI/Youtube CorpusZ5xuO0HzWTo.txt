 hi my name is dustin arndt and i'm from pacific northwest national laboratory my colleague maria glenski and i are very happy to be presenting the position paper titled measured utility gain trust practical advice for xai researchers many researchers may have assumed the purpose of a machine learning explanation is to enhance increase or calibrate users trust in the model instead we argue that trust is an insufficient metric for evaluating explanations or models we recommend that researchers objectively measure the utility of the explanation instead of subjectively measuring trust trusts should manifest through experience as users use a system containing a model and an explanation their trust in a system will grow if that system is reliable and provides a benefit so how do we determine if an explanation is good can an explanation be correct i don't think we are in general going to find a way to measure explanation correctness or accuracy except in special cases like image captioning often explanations are visualizations but we don't talk about visualizations being correct and accurate though we do assume they are implemented without bugs or errors instead we talk about visualizations being useful much like a data visualization the purpose of an explanation is to communicate an idea or a pattern to the user ideally we can borrow evaluation approaches from the visualization community allowing us to use a broad range of techniques for evaluating the utility of explanations explanations are intended to be used in the context of a much broader workflow and there are many different use cases that we know of for this talk we'll focus on the following use cases debugging validating and selecting a model understanding a model teaming with a model and challenging a model each of these use cases allow us to imagine downstream tasks where we can quantitatively measure the user's performance with or without the explanation resulting differences in performance provide indirect but compelling evidence of the utility of the explanation i have argued that trust is a flawed metric for measuring the goodness of a machine learning explanation in the rest of this talk maria and i will first provide some historical context for this argument then we will provide practical guidance for how to measure the utility of an explanation across the use cases mentioned before developers of machine learning algorithms both in academia and industry are incentivized to increase the acceptance and quick adoption of their algorithms twenty years ago herlocker and others wrote that current recommender systems are black boxes providing no transparency into the working of the recommendation explanations provide that transparency exposing the reasoning and data behind recommendation so the concept of trust and explanation in ai has been around for a while however the more recent successes in the field of deep learning on the image domain have had a profound effect on ai and explanations in 2012 the now famous model known as alexnet made its debut and many subsequent deep learning models followed however researchers quickly pointed out that there is no clear understanding of why they perform so well or how they might be improved shortly after ribeiro and others developed a foundational explanation technique called lime which was framed as a tool for calibrating trust in these black box models the authors wrote about a case where lyme revealed when the ai was right for the wrong reasons many subsequent explanation techniques followed for example grad cam in 2017 all of these methods were used by the research community to try to better understand the inner workings of deep neural networks and presumably to calibrate trust in their own models in august of 2016 darpa announced its explainable artificial intelligence program which received a large amount of publicity and popularized the concept of xai and the phrase appropriate trust around the same time the european union announced that the gdpr would require companies to explain decisions made by ai that affect consumers these two events influenced researchers to develop explanations for end users who are outside the ai community for these users trust is not the only consideration because end users use these models and their explanations in the context of a much broader workflow from another perspective how much does trust matter for a user when an ai model is used on them rather than by them perhaps giving the user the tools to challenge an ai decision is more appropriate than misleading them to trust that decision the utility of an explanation must be tied to its purpose trust should not be metric that we've maximized through design instead it should be a benefit gained after interacting with a useful system over time we believe that studies evaluating explanations should not measure trust unless they are longitudinal in the wild and consider the broader workflow instead we argue that research studies that seek to measure the benefit of novel explanations should focus on utility over trust to support this goal we have identified five broad use cases where researchers could design experiments to measure the utility of explanations these use cases are as follows number one model debugging and validation where users answer questions like is the model working as designed and why is the model making mistakes number two model selection where users determine which model among several is best number three mental model and model understanding where users learn how the model functions or behaves possibly leading to novel insight about the domain number four human machine teaming where users may do a task with the aid of a model more effectively than either on their own number five model feedback challenging and prescription where users are affected by a model's decision and may want to challenge that decision correct the model when it's wrong or even learn what to change about themselves to get a better outcome the model developer may consider each of the first three use cases model debugging and validation model selection and mental model and model understanding in contrast end users will typically participate in model selection mental model and model understanding or human machine teaming model feedback challenging and prescription is a greater concern to impose users though they may also be interested in mental model and model understanding in addition to considering users and use cases we also need to design controlled experiments with falsifiable hypotheses we believe gold standard xai evaluations should have all participants perform the same task with a randomly assigned group of participants who perform the task without the assistance of the explanation being evaluated by comparing the performance of groups with and without the explanation we can make claims about its benefit or value but hypothesis that the explanation is useful is falsified when there is no significant difference between these groups not all experiments should follow this rigid experimental design but we suggest following some basic guidelines researchers should first consider the three key components of their system the human the machine and the explanation next researchers should determine what can be measured or is meaningful to measure using different combinations of these components for example the human-only and machine-only cases are baselines of the human and machines at the task where possible the human and machine case is a baseline performance of the system when the user can rely on the machine learning output and the human and machine and explanation case is the performance of the system when the explanation and machine learning output are available to the user researchers should compare the performance of the human and machine and explanation group against the human and machine group the human only and machine only groups can provide additional context next we will go over some pseudo experiments that address each of the use cases we identified before model debugging and validation is a developer-focused use case that leverages explanations as a means of improving the model here explanations are used to provide insight into the mechanism of complex black box models for example neural networks to identify flaws or biases in the algorithm or the training data that can be addressed in development an example downstream tasks for this use case would be given an incorrect model decision and corresponding explanation determine the reason the model made a mistake for this downstream task a research team would code a set of model mistakes using integrated reliability of those annotations to establish a ground truth for example for an image classification task these coded mistakes may be an annotation of the image qualities that misled the model for example occlusion of the subject pixelation and artifacts in the image as well as qualities of the model behavior misplaced model attention or a lack of training examples images are presented to users who are asked to identify the reason the model made a mistake the user's ability to correctly describe the reason for the model's mistakes are measured with and without the explanation to determine if explanation methods are useful in helping users determine why a model made an error our second use case model selection is typically the focus of trust and explanation analyses when considered together because it seeks to answer the intuitive question of do i trust this model enough to use it or do i trust this model more than another although we argue this is a narrow application of trusts and explanations together it is a common and important use case to consider an example downstream task for this use case would be to determine which of two models is better suited for a given task in this experiment users consider two models and either the decisions alone or the decisions alongside their accompanying explanations user performance identifying which model performs better on an unseen test set across the two groups can be used to quantify the quality of the accompanying explanations for example by measuring whether using the explanations to identify whether the model decision was right or wrong for the right or wrong reasons enabled users to better distinguish which model is best suited for the task the mental model and model understanding use case relates to whether a user builds an accurate mental model that is a mental model which functionally mirrors the overall behavior and decision making of the machine learning model an example pseudo experiment for this use case can be focused on the downstream task of given information about a model's past performance match the model to a novel output in this task user groups are given a series of input output pairs for a set of models during a training phase which model created each output is specified to help users build mental models after the training phase users are presented with a new previously unseen set of inputs and corresponding model outputs users are then asked to match the models to the new decisions or to identify if none of the previously presented models would have produced the given decision any improvement in the ability to correctly match models to the new decisions with explanations compared to without would signal that explanations help users to better understand the model human machine teaming distinguishes models as teammates beyond simple tools an example pseudo experiment for this use case could be focused on the downstream tasks to determine whether explanations increase the efficiency of a human machine team in healthcare in this task the user works with an automated physician's assistant makes recommendations for data collection testing and diagnosis during a physician patient interaction the model presents choices to the physician with accompanying explanations of these choices in the human plus machine plus explanation case such as visuals of patient data and the risk contribution for certain diagnoses two control groups exist for this task the human plus machine group with no generated explanations of recommendations and the human only group that receives no model of suspense how long it takes user groups to arrive at the correct diagnosis the cost of that treatment and how many incorrect branches were explored are all viable performance metrics to establish whether the explanations of the model benefited the users as this tax requires trade experts the human-only baseline is meaningful and a wide array of extended tasks can be generated from this initial example the model feedback challenging and prescription use case effectively considers trust in the system the model and context of the impact on imposed users of its decisions and subsequent recommendations or actions taken the need for effective explanation of model decisions for recourse is a natural response to the continued widespread application of artificial intelligence or machine learning models to supplement or automate tasks in domains where incorrect or biased recommendations can have significant human impacts an example downstream task for this use case would be given a model's decision identify how to get a better outcome in this task a user may be given an input and a decision from a model and asked to identify what has to be changed or updated and the input to get a better decision user groups are either given an accompanying explanation for the decision of the unaltered input as well as or only the model output this touches on counter factual explanations and the prescriptive use of algorithmic decision making systems given that the model is trustworthy providing the right outcome given the right data but the user desires a different outcome does the model explanation provide enough information for a user to identify the personal changes needed to obtain the desired outcome glassbox is an example of such a downstream task and practice users given a pre-established persona prove a loan application system that provides contrastive counter factual explanations to understand and challenge the model's automated decisions trust can be a benefit of a useful and reliable system employing explanations of machine learning models however we believe trust should not be used as the measure of the goodness of an explanation researchers should design for and measure utility instead utility-oriented evaluation encourages researchers to consider how the explanation is intended to be used it also encourages researchers to employ scientific methodologies to evaluate explanations these methodologies leverage falsifiable hypotheses and objectively measurable quantities as evidence towards this goal we have suggested many pseudo-experimental designs involving downstream tasks that can be used to evaluate explanations in this manner we hope the impact of this work will be to inspire many new experiments that solidify the scientific foundation relating humans machines and explanations finally i would like to thank all of the co-authors who contributed to this paper brittany davis maria glenski and william seely feel free to contact maria or myself if you have any questions and our contact information is on this slide thank you 