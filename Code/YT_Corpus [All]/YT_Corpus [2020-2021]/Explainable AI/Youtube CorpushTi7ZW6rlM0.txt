 hello everyone welcome to interpreting ml models with explainable ai my name is sarah robinson i'm a developer advocate on google cloud this is an overview of what i'm going to be covering in this session i'm going to start by defining some key terms related to model explainability then we'll see some examples of what explainability looks like for different data types then we'll dive into the explainable ai offerings that we have on google cloud today and finally we'll see them in action with some live demos so let's get started i'm going to start by defining explainability now in order to understand explainability i want to set the stage and tell you how it fits into the larger concept of responsible ai responsible ai is essentially the practice of ensuring that ai benefits society and doesn't reinforce unfair bias so you can think of explainable ai as an umbrella concept and explainability is a subfield within responsible ai in addition to other topics like fairness privacy and security so this is a summary of how explainability fits into the larger responsible ai picture i want to talk about fairness and explainability since there are two terms and concepts that work together and can sometimes be confused so when i talk about fairness i'm talking about ensuring that machine learning models don't reinforce biases and subsequently that they treat all different slices of users of a machine learning model fairly and equitably explainability is one way to ensure fairness explainability refers specifically to understanding how and why a machine learning model makes predictions but explainability can also be used outside the context of fairness to do something like maybe explaining why a fraud detection model flagged a particular transaction as fraudulent explainability is a broad concept and can invoke a variety of related concepts depending on who's looking at it so maybe for an executive that's overseeing ai investments they might think of explainability as transparency accountability or governance in this presentation we're going to focus specifically on model explainability which is understanding how machine learning models make predictions and communicating the insights from that to external stakeholders and folks within an organization that are building these models so now let's see some examples of what model explainability looks like this depends largely on the type of data that you're working with the data that you use to train your model so explainability is going to look a bit different for image text and tabular models so let's take a look for image models explainability can highlight the specific pixels or regions in an image that caused a model to predict a certain label for an image so in this example the image on the left of the dog is the original image and the other two images show different methods of getting attributions for the pixels in an image that signaled the model's prediction for text models explainability can tell us the words in a block of text that contributed to a model's prediction so in this example the sentence how could you not love cake which our model gave a sentiment score of 0.9 we can also see the words that contributed most to that model's prediction of a high sentiment and this is an example of how explainability and fairness can go hand in hand so in a text specifically a sentiment analysis model you'd want to make sure that your model wasn't using identity terms to determine the model's sentiment in this example it looks like our model is performing correctly because it's picking up on emotion words and words like not to make its sentiment prediction and then finally for tabular models explainability gives attribution values for each of the features in your model to show how they influence the prediction this particular example shows a model that predicts the duration of a bike trip and we can see here that the distance of a trip of this specific trip caused the models predicted trip duration to decrease whereas the max temperature on that day caused the model's predicted duration to increase we talk about model explanations there are a few different groups of people that need explanations the first group is data scientists people who are building models maybe people who are managing the operations and productionizing those models and for that group of people they want to use explanations to understand maybe strange behavior in their model and help debug it see why the model is not performing as they expect and try to improve the model end users of ml systems could refer to a few different types of groups of people first it could refer to someone that's benefiting from a prediction of an ml model maybe you're using an application that gives you a product recommendation it could also refer to a practitioner maybe a doctor or medical practitioner who is using a machine learning model to determine whether an x-ray contains a particular disease in this case they're using explanations to verify that they can trust the model's output finally we have public stakeholders anyone involved in regulatory or compliance for machine learning models and this could be somebody both within or outside your organization they're using explanations to determine uh whether a model complies with certain regulations and if the model is safe and fit for purpose and each of these types of users will take different action as a result of explanations so just to go over this a little bit so data scientists model builders people who are actually building these models might use explanations to go back and see where they could improve training data maybe refine features or change the model architecture end users are using explainability to have more trust in ml systems and be confident that the model is leading them in the right direction finally public stakeholders might take action from explanations by using higher level summaries of models to develop guidance for different industries on responsible ai now let's look at how we can use explainable ai on google cloud today i'm going to focus primarily on what's called feature attributions but first i want to talk about two categories of explainable machine learning the first is referred to as post-hoc explanation methods this involves looking at relationships between feature values and a model's given prediction independent of model internals things like learned weights the other type of explainability is called methods that are interpretable by design and this refers to simpler models like linear regression models or decision trees and looking specifically at the architecture of those models to understand explainability tabnet presents some new research in interpretable machine learning models for tabular data and we recently released a solution for it on google cloud today i'm going to be focusing specifically on post hoc explanation methods and within that category we'll be looking at feature attributions now let's look at the products on google cloud where you can use explainable ai this pyramid provides an overview of the different machine learning offerings that we have on google cloud organized by levels of abstraction so the products that are on the bottom half of the pyramid are targeted more towards data scientists and machine learning engineers folks that have more of a background in machine learning as we move towards the top these products are targeted towards data analysts application developers folks that want a way to integrate machine learning into their application but may not have as much ml experience right now you can use explainable ai on tensorflow models that you deploy to ai platform prediction along with models that you tabular data models that you deploy to automl tables and with automl tables you get access to explainable ai by default later on in the presentation i'm going to show live demos of both of these tools and how to use explainable ai with them the first demo is going to show how to use automl tables and in this demo i'm going to show a fraud detection model that was trained on a public data set that's available in bigquery and we'll show how we can use explainable ai to get both global and instance level explanations for the second demo i built an image classification model trained on medical images using tensorflow and deployed that to ai platform with explanations for the first model i train this model using a publicly available data set from bigquery and explainable ai provides both global and instance level feature attributions in automl tables so this means i can see overall which features my model relied on most to make predictions and i can also zoom in and see at an instance level for an individual prediction which features were most important for that one prediction for the second demo i trained a tensorflow model on this chest x-ray data set available in kaggle and i'm going to show how we're going to use explainable ai's new local experience sdk to get explanations without deploying our model this sdk is available in cloud ai platform notebooks and i can use it to get explanations on a model that's available within my notebooks instance as i'm developing and prototyping my model without needing to deploy it to ai platform when you're deploying a model to ai platform with explanations you can choose the explanation method that you're going to use so i want to introduce the three explanation methods that we have available all these explanation methods are based on publicly available research that you can check out by going to the links on this slide the first method is called integrated gradients this works with all types of models built with tensorflow tabular image and text models sample chaplet works for tabular models built with tensorflow and the x-ray method is built on top of integrated gradients and does some smoothing to show instead of pixel-based image attributions it shows region based attributions and this is one way that you could deploy explainable models to ai platform here we're showing how to do this using the gcloud google cloud cli to do it so this looks very similar to how you would deploy a model version without explainability on ai platform the only things you need to add are the explanation method you'd like to use and the number of steps you'd like this method to use and i'll get into that a little bit more in the demo so now let's go to the demo i'm going to train my fraud detection model using this public bigquery data set of credit card transactions here's a description of the data and the schema most of the columns in this data set have been obscured and here's a preview of what the data looks like i'm going to run a quick query to show as in the case of many fraud detection data sets the data is heavily imbalanced meaning that only a very small percentage of our examples are fraudulent transactions so to account for this i have done some down sampling which means that i've taken just a small sample of the data from our majority class and to keep things interesting i've also taken the liberty of renaming the column names that were obscured so let's look at automl tables so to import my data i can import it directly from bigquery so i don't need to move my data anywhere i've already done this so let's take a look at what the data looks like i can see a preview of my columns and the different types here as you can see i have renamed many of the columns and i've selected my target column now to train my model in automl tables all i need to do is press this train model button and to account for that class and balance that i mentioned in addition to the down sampling i did i can also use this optimization objective provided by automl tables so in this case i chose to maximize my model's performance on the less common class in this case our fraudulent transactions i've already trained this model so let's take a look here we can see many of the different evaluation metrics for our model the confusion matrix tells us the percentage of examples in our test set that our model classified correctly for each class and here we care most about our fraudulent examples and we can see that our model did a pretty good job with no false positives and it was able to classify 88 of our test examples correctly now let's get into explainable ai so we get this out of the box once we trade an automl tables model we don't have to enable anything what this provides is global model level feature importance so across our test set this shows us which features our model used most in influencing its prediction and looks like the the most important feature here was whether or not there was a smiley face on the credit card signature followed by the credit card color again i just made up these feature names and so once we have trained and evaluated our model the next thing we want to do is use it to get predictions and there's a couple of ways to do this in automl tables we can do it right in the ui which i'll show first automall tables has just populated each feature with a random value and what i want to do here is check this generate feature importance box here we can see the model is 99 confident that this is not a fraudulent transaction and here we can sort our feature attribution values by the ones with the highest absolute value so here we can see that smiley face signature caused the model's prediction confidence to go down whereas the time to expiration caused the prediction confidence to go up and we can scroll through and look at the other feature attribution values here as well so that's how we can try out automl tables and see feature importance at an instance level in the ui but chances are we also want to use the automl tables prediction api to generate predictions and get feature importance so i'm going to show you how to do this in a python notebook using the automl tables api so first we'll create an auto multables client and we'll set up some variables for our project and the specific model version i have provided my inputs in this format uh this is an example of a fraudulent transaction so hopefully our model will predict fraud for this one and notice here that i am setting feature importance to true to get those feature attribution values back in my response here i'm visualizing the feature attribution values we can see that our model predicted this correctly 97 confidence that this was a fraudulent transaction and here i've graphed the feature attribution values just for the top five features and we can see which features influenced our model's prediction of fraud here now that we've seen an example of a tabular model using automl tables i'm going to show a few examples of how to get explanations on an image model using explainable ai i'm going to show you how to do it in cloud ai platform notebooks using the explainable ai sdk which comes pre-installed in notebooks instances so i'll open up my notebook instance here and in this notebook i'm going to show you two examples of image classification models that i've built with tensorflow what i'm going to highlight is how you can do this all locally from within your notebooks instance so i'm going to get explanations on my models without having to deploy them to the cloud both of the models that i'll be using for this demo i've already saved in the tensorflow saved model format in these directories here the first model that i'm going to show you is a model that's been trained on the imagenet data set which can classify images into a thousand different labels ranging from different types of animals food plants etc so i've already saved the model assets in the tensorflow saved model format into this directory and note that i'm importing the explainable ai sdk here when i point the sdk at this directory it's going to generate some metadata that it will need to provide explanations we can see it save this in a json file here and now we're ready to send a test image to the model i'm going to send it this image of a jellyfish i'm going to see both what label the model returns for this image along with the feature attributions to explain that prediction and again to do this i am using this load model from local path method and i didn't need to deploy my model to the cloud to take advantage of this what we see here is the label index for the label that my model was most confident in and below you can see that the index of 108 corresponds to the label of jellyfish which is a good sign because this is indeed a jellyfish and looks like our model was 96 competent in that prediction and just so you understand what these color-coded explanations mean the yellow brighter regions in the image are the regions that contributed most to my model's prediction the darker purple colored regions are the regions that were least important in predicting this label and so what we're trying to see here is that our model is picking up on the right shapes that are associated with this particular label and in this case we can verify that our model is performing correctly it's looking at shapes in this image that are specific to that of a jellyfish which we can see here the next model we'll look at is a model that was trained on a chest x-ray data set in kaggle to predict the presence of one condition that can be found in these images so the first thing we're going to do is save our model metadata and here we're explicitly setting the baseline for this model to be a solid black image and that's what all of our explanations will be relative to we can preview the metadata here so we're going to test out our model using this image and our model output is a sigmoid output which means that it's going to output a value between zero and one with one indicating that the model thinks a specific condition is present zero indicating that it does not so we'll first get an explanation and notice here that we've set our model's step count to 10 this is the number of integral steps that the explanation method is going to use in computing the pixel attribution values and we can see here that the approximation error exceeds five percent now what this means is that we may not be able to trust the explanations in this particular example one way that we can decrease this error is to increase the number of integral steps we're using so let's try increasing the step count to 50 and generate a new explanation now we can see that the approximation error did decrease significantly but looking closer at this image the regions that our model seems to be focusing on are not the correct regions associated with this particular condition for this particular image our model should have predicted closer to one this is likely pointing to a problem with our model or a need to improve the data that was used to train our model since explanations are only a direct reflection of your model they're only going to be as good as your model itself so it looks like here we need to go back and improve our training data perhaps provide a more varied representation of x-rays that have this specific condition present and healthy x-rays once we have a model that we're happy with and the explanations confirm that our model is performing correctly we have everything right here ready to go to deploy our model to ai platform so all we would need to do is copy this directory with this metadata file to google cloud storage and we could do that here and then we would run this gcloud command to deploy our model and now let's go back to the slides and finally here are some resources if you want to get started with any of the products that i covered in this presentation i definitely recommend checking out our sample code we have some example notebooks that show you how to deploy explainable models on ai platform thank you for watching if you want to get in touch you can find me on twitter at srob tweets and i hope you enjoy the rest of next on air 