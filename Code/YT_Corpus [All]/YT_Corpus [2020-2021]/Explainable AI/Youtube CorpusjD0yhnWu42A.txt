 3 [ MUSIC ] SAHA: I see in our sidebar that we still have people coming in, so we'll go ahead and get started -- John, you look fantastic -- and we'll start with intros while other people join us. So, good morning, good afternoon and good evening everyone. Thank you so much for joining us. My name is Monoswita Saha. I lead Global P-TECH Network Engagement for IBM, and I'm very excited to facilitate a discussion on a topic that is all pervasive, artificial intelligence. So, before we get started, a little bit of housekeeping. Please take a moment and use the chat to introduce yourselves and where you're tuning in from. We'd love to get to know our audience. And while you're typing in the chat, please also have the setting set to all participants so that the panelists can also join in and see the dialogue. We're certainly going to have time for Q&A; and that being said, do not hesitate to post comments, questions and reactions into the chat throughout the program. We'll do our best to respond in the moment. All right, enough for me. Now, on to our guests. As you can see, we have a rockstar line up, and I am very thrilled to have these three experts on AI who are deeply passionate about education and about ensuring that our current and future workforces are in a position to adapt and be active players in the technological transformation that we're living in. Charlotte Dungan is the AI Program Architect and Instructor for Computer Science at the Ryden Program for Innovation and Leadership at the North Carolina School of Science and Mathematics. With 10 years of computer science experience, Charlotte is also deeply involved in education and has led multiple initiatives with the goal of bringing STEM exposure to students and teachers in a wide array of education settings from alternative schools to rural areas to camps. John Cohn. John almost does not need an introduction, you can see him on the WebEx. I had to give you, that's John. So, John is an IBM Fellow with the M.I.T. IBM Watson AI Research Group based in Cambridge, Massachusetts. John has been a technologist for 40 years and counting, and it would take a whole hour to list John's accomplishments, inventions and patents. But what I can say, having seen him in action multiple times is that John is incredibly passionate about and effective in getting young people to get excited about technology. Phaedra Boinodiris is a Fellow of the Royal Society of the Encouragement of the Arts, Manufacturers and Commerce, and serves on the leadership team of IBM's Academy of Technology. Phaedra is especially passionate about inclusion in technology. She is currently pursuing her Ph.D. in AI and ethics. Phaedra has also co-founded womengamers.com and is a regular public speaker and contributor to Forbes, Fast Company, the National Academy of Engineering Journal and PR [INAUDIBLE]. So, Charlotte, John and Phaedra, thank you so much for joining us. Before we dive into the questions, I was wondering if each of you would take a little bit of time and introduce yourselves to the audience and share a little bit about how you found yourself in this AI space. DUNGAN: I can go first and then, so I came, I was a computer programmer for 10 years and I got interested in education and I kind of married those two things together after quite some time working with children. I've actually taught pre-K through adult. I've taught every grade level, been fortunate to be teaching high school in the last couple of years. But I am an active Girl Scout leader, I've been working with girls in STEM K to 5. I worked for [scratch] education for a bit and they have really great new AI plug in tools that if you haven't checked them out, you should. They have text to speech and speech to text. So, great, like you can make a wand and point it at the screen, and it will react. So, cool stuff going on in that space. I'm currently working on an initiative for AI for teachers and I'm excited to support K to 12 educators in the AI space. BOINODIRIS: My background is in games, and I had started womengamers.com along with my sister many, many years ago, so we did a lot of consulting to game studios and publishers who wanted to target women as game players. I ended up joining IBM to lead our serious games program. This is when you use video games to do something other than just entertainment. And that's when I really stumbled upon and fell in love with artificial intelligence, because the way that you can use it as a mechanism to enhance play was so interesting. You know, the more that I can learn about a player, the more that I can learn about their environment, about their ecosystem, the more I can trade an experience that is highly customized to them. So, that's how I ended up working in artificial intelligence and ended up eventually pursuing this Ph.D. in AI and Ethics. COHN: That's fantastic. You know, my path, you've already kind of laid the ground, and Monoswita said, you know, we were going to try to get your students to be players here; Phaedra has made a business out of that. The way I got to, well, I mean...I can't talk. The way that I came into AI is the way, you know, when I look back over my 40 years of, I started as a physicist and did chip stuff then did IoT stuff. Each time I went on to something else it was because I had been playing and misbehaving. What, you know, I have...I'm sitting talking to you [at the] shop, you know, [teslic] oils and things like that. And I just love learning by hands on, and I know that many kids do that, too. I got into AI when I was supposed to be doing something else. I just couldn't keep away from it. I heard about it and I started writing it; and so, last night I stayed up way too late writing some AI code for, should have been for work, but it's actually for play. So, I think the secret for me is what brought me here is play. It's the most powerful toy I've ever found. DUNGAN: You're muted. SAHA: I'm sorry. So, thank you all. I'm looking forward to diving deeper into all of your expertise here. So, my first question to the three of you is this. Why is artificial intelligence a technology that we need to pay attention to? And Phaedra, since you're earning a Ph.D. in the subject matter, I thought maybe you could kick this one off. BOINODIRIS: Sure. So, AI today is being used to make so many high stakes decisions from what percentage interest rate you get on your loan to whether you get accepted for the job that you applied for, whether you get admitted to the college that you're applying for. Even for prisoners, whether the judge is going to consider sending you back to prison or not, right? So, there are so many ways in which this is being used. But also my big concern is that there's fundamental misunderstandings about artificial intelligence. Either people don't know that AI is being used to make these fundamentally important decisions, and/or they assume that the decisions that are being made because it's not being made by a person is being made by this magic black box that somehow it is not fundamentally biased against them in any way. It couldn't possibly be racist or sexist or disadvantaged any populations because it's a computer making the decisions. So, again, the Ph.D. that I'm pursing is specifically around responsible AI. So, one of the main reasons why I think it's desperately important for educators to open up the doors of artificial intelligence and the conversations around it to include things like ethics to those students who may not self-categorize as coders who want to be the next governor, who want to be the next person making public policy, who wants to be, you know, the lead fashion designer or lead agriculturist in their region is because, you know, you need them to understand these fundamentals about how artificial intelligence is, how the data is chosen, how it's curated to make sure that AI is not biased against people. Again, we want to make sure that artificial intelligence that is being developed and deployed is moral and ethical in terms of the way it's being deployed. SAHA: Thank you, Phaedra. John and Charlotte, your thoughts on that. COHN: I really like the way you framed that. And Phaedra, I'm so excited that you're going to go do this Ph.D. I would do another Ph.D. if my wife would let me. But it's super fun, it beats the heck out of work, I've got to tell you, of working for a living. But you know, I really echo what you said about, you know, we have to do this right. You know, I come at it...it's a very interesting thing. I just have come off a year's task force for the State of Vermont about AI for the State of Vermont, the second smallest state in the union, and you're like, what does that mean? And I got so excited when I came in. What I was so surprised to find is that a good portion of the conversation with the public was around worry, around worry about privacy, around worry about, you know, is it going to take over the world. I actually come at completely like, this is the most amazing change I've seen in technology in over 40 years of practicing. You know, I was a front row seat for, you know, Moore's Law, the advance of semiconductors. This beats it incredibly. And I guess the key point that I've come to learn is this is something you need to understand whether, you know, whether it's going to be...you need to decide, you need to personally decide, you need to help your students decide how are they going to live in a world that is increasingly aided and also somewhat threatened by AI. I mean, this is something that armed with the right ideas and the right possibilities of what AI can do, what it can't do, what you have to be careful about, what its capabilities, is the best...you know, best thing, track you can put your students on in terms of what, how they ride this crest rather than getting crashed over by it. I mean, there's no doubt it's going to transform. It already is transforming every part of our lives. Every time you pick up your cell phone, there's a ton of AI already running in this, and it's incredibly helpful. You know, transport is being completely changed. You know, look at just, look at the aircraft industry. We worry about autonomous driving, but look at how aircraft safety has, knock on wood, has just increased incredibly over the last 10 years. Well, subtly a lot of that is about AI. It's changing medicine. It's even changing entertainment and play. So, it's going to be a great thing, as long as we take care of it. And I think the main thing we need to figure out is, how do we position our students to benefit by it whether they are going into a field that is AI related that's going to create all sorts of new technical opportunities. But even if they're not, there's a huge change in the future of work, as is always the case with any technological change. You want to position your kids so that they're making the right choices so that they're going into new areas that are actually growing and not being displaced by technology. This is a net win for the world, as long as the world knows what's coming and as long as the world is active and takes activism like Phaedra and Charlotte are going to be talking about. So, this is something that you can't afford to ignore. And I'll tell you, just a little bit of mischief, because I think that's...you know, one of the things that I learned on this task force is that the adult world is having very unnuanced conversation. I know we're not supposed to be political, but I mean listen to the conversations that they're having in government right now. They're not nuanced, they're not informed by science. The best way to change that is to get students informed, curious and making the right decisions, because that's the best factor for changing the world, so. DUNGAN: And actually... COHN: It's fun. I think that's the main thing I should have said, because it's crazy fun. DUNGAN: It is fun, John. And I totally agree with what you said about it being a revolutionary technology. Like when computers came into homes and businesses, it was very obvious: there was this big box in our homes and we began to do things differently from paper to screen. And then this thing came, and we started carrying our computers around like we're in Star Trek, and we had this communicator and we could listen to music. And that was still very obvious. We had conversations about whether or not we should let 10 year olds have this device, how it was shaping society. AI is doing the same thing as those other technologies, and most people don't know it. They don't understand that the media that's being curated for them is being chosen and they're only seeing certain content based on the buckets that they're being placed in. And I think adults often miss that children are having a different experience than adults are. When I talk to 12 year olds, they no longer have any expectation of privacy. They don't expect that when they walk down the street that that will be a private place, they don't expect that the conversation they have with a friend is ever private. So, the way we talk about legislation or we talk about how we regulate AI needs to take into account a generational difference between younger and older people. And maybe have a more nuanced conversation about where this technology is going, because it's certainly just as good or as bad as we choose to make it. COHN: You know what, Charlotte? Hey, Monoswita, are we allowed to ask each other's questions? SAHA: Yes, yes. COHN: Okay. I don't know if you two have seen this, but you know, I think you're right that kids...students, well, most of us call them kids, but you know, proto adults, have a different understanding of it. But sometimes they have a more intuitive understanding than their adult partners. I mean, what I find is just the same thing with screen technology that kids that grew up on it integrate it. Their brains are wired to do it better than...when I watch myself or my mother kind of trying to figure it out versus the four year old across the street, I'm like, you are wired differently. And I believe that they...not only did they understand that they don't have privacy, that they're giving that away, but they actually understand the AI. They understand the training. They work with it in a very different way. It's not a flat object. I don't know. Have you ever seen that? DUNGAN: Oh, no, I agree. And just the idea of like TikTok and how most of this generation interacts through video rather than through text. Rob said in the chat, phones in the hands of students hasn't been a positive thing as far as the traditional classroom is concerned. And while that's true, I think that's because us as educators, we haven't embraced the technology fully to use and communicate with students on their level. So, many classrooms ban phones, but what we should be doing is talking about proper uses of phones and empowering students to use technology. It's a shame that we don't use more phone technology in our math classes, for example, letting students use the technology that they already have. There's a great book by Conrad Wolfram, I'll plug it. It's called The Math(s) Fix, and its all about integrating AI and mathematical education with computing because that's how the real world happens. Someday we'll get there. SAHA: So, I love that you all have very quickly brought this discussion right to the young people, right, to our teenagers, to our students in the classroom. And what I'm hearing from the three of you is that you're very aligned on the perspective that no matter what, this technology is going to deeply impact our lives both in obvious ways and subtle ways. I mean, I've heard everything from the [INAUDIBLE] to prison to where you get on a plane and just your everyday use of your phone, just the technology being part of your lives. And so, I have a question. There was a recent study... COHN: Did we skip Phaedra. I think I might have put us off by asking a question in the middle. SAHA: No, Phaedra is going to take the next one. COHN: Oh, good, okay. SAHA: So, there was a recent study by IBM and the Morning Consult of students across 13 countries, and the study showed that the majority of teenagers agree with you, this technology is everywhere, it's going to be impacting their lives in small and big ways. But at the same time, these teenagers feel very unprepared to work with the technology. So, I have a two-part question and the first part is, one, how do we spark interest in technology such as AI in high school students, say, the age bracket of 13 to 18 years old? And then, where and how do we effectively educate so that they do feel prepared to work with this technology to pursue careers and to act as informed consumers? BOINODIRIS: Great, great question. Can I kick it off? SAHA: Yes, yes. BOINODIRIS: Okay, well, to begin with, and I alluded to this a little bit in my last answer, and that is that I truly and fundamentally believe that we're marketing AI incorrectly in schools. Like somehow it is only for those students that have registered and signed up for computer science class, [INAUDIBLE] identifying as coders or the next generation data scientist, the next generation machine learning scientist. And I think it's a hugely missed opportunity because as I said earlier, if you want to be the global lead in fashion design, you've got to understand the fundamentals of artificial intelligence. You want to be an agriculturist in your region or the lead public policy for your state, you desperately need to understand the fundamentals of artificial intelligence. So, I think that really this needs to be a cross silo practice or learning that we're teaching. And additionally, irrespective of the marketing piece, this is absolutely a STEAM initiative, right? I'll give you an example. When we were teaching middle school kids how to build empathy bots, right, a chat bot, but paired with an artificial intelligence that can recognize sentiment in what people are typing in so that they can curate a conversation that's customized based on what emotions that person is displaying. What the middle school kids found, they were trying to build and empathy bot to help their fellow classmates better manage their stress during exams, right? And what they found was the hard part of the project was not the coding, it was not building the conversation trees; it was understanding behavioral design. Like if I'm talking to a classmate who's completely stressed out of their mind and feeling a great amount of anxiety, do I tell a joke? Do I play music? Like what do I do? What do I do? So, that psychology aspect, the understanding of design, experiential design that will actually motivate changes in people's behavior, I mean that kind of concept, again, is not strictly coding at all. It's understanding people and understanding their behavior. Bringing in concepts like history and music and strategy and art and design all of these are vastly important. Additionally, another way to capture the interest of students is, again, and we've said this several times, incorporate play, incorporate play. We co-developed an AI powered Harry Potter sorting hat with our local high school. And we did this, you know, they had a [maker space] club and they were really interested in tricking out the hat such that when you would say something unique about yourself, it would bring in the actual voice of the sorting hat from the movie. And the hat's sort of mouth would move, right. So, you would say, I happen to know I'm talking to snakes. And the hat would say, slither in, right? And what we did with these students, we started, we used that AI powered Harry Potter sorting hat to also introduce ethics, right, because I happened to decide, all right, I want to make sure I really hit this idea home about the idea around transparent and explainable AI. So, I rigged the hat so that if any of my kids were to put the hat on, it would immediately put them in the slither in. And of course, when they put it on and it categorized them into slither in and they glared at me and said, mom, you rigged the hat. And I said, let this be a lesson to you. Never trust an AI that's not fully explainable and transparent. You should be able to ask the hat, why did you put me into slither in? What was it that I said or did in my past that put me directly in the slither in? And again, this use of pop culture and play to relay some of these vastly critical foundational ideas around artificial intelligence is key. And it doesn't have to stop there. Even things like music and just getting people to think about what is the future of creativity given technologies like artificial intelligence. There's so many playful ways where you can incorporate this into the classroom. DUNGAN: I actually do some work similar to that, Phaedra, where I have kids make music with AI and make art with AI. And then we talk about the ethical implications of that, about what does it mean when you're shaping the world with something external? And so, the question was originally like talking about 14 to 18 year olds and how you get them inspired with AI. But I think it's not like we're trying to funnel more people into the job market. As soon as you stop trying to shoehorn certain kids into like a computer science track and you start talking about building your best life, then there's space for every child to be engaged. And it doesn't mean that you're you feel like you're strong in math. You don't have to be someone who identifies as a strong student to think about who you want to be and what type of world you want to live in. And then if you just use the educational framework of use, modify, create, if you can get students to identify as a user, everyone is using AI and here's where you find it. You find it in your Netflix, you find it on your Amazon, Google, whatever you talk to. You find it on your phone, you find it on your filters on Snapchat. Like it's everywhere. It's all around us. It's deciding what insurance rates you get when you turn 16 based on your ZIP Code. And so as soon as you can say this is your around you, around your whole life, and kids get that, then you can say, you know what? You don't have to just be a user, you can actually modify this technology. You can actually make laws about it. You can actually decide how it's used in society. And then some of you will become creators of AI, and that what you can do with that is super cool, right, but it's not for everyone. It's probably in a lot of future careers, but I find that if you start with everyone's a user, then there's a broader base to get kids interested. COHN: I love what you both said, as it's really getting out of passiveness. This is not something that's going to happen...I mean, if you don't do anything, it will happen "to" us, and kids are the right people to sort of turn on and identify. And I really like what you're both saying about, you don't have to be that...I've got to tell you, don't tell anybody, but I'm not good at math, I hate math. Oh, wait, I used to hate it when a teacher said, I hate math. But I'm not, you know, I wouldn't identify as a mathematician. But I think if you go in, I guess one subtle, maybe it is, I think we'd all agree on this but I'm not sure. I think you've got to lead with, why should I care. And I kind of think it's...my style is to lead with fun and curiosity so that people lean in. And then you can, I like the way you said it, Phaedra, is that your sorting hat is the perfect kind of thing. You kind of make people go, how is that working, and then you can have the ethics conversation. What I worry about is a lot of the content that I have seen like here in the State of Vermont, People's Republic of Vermont, is it always leads with, you know, this is a bad thing this is privacy and you don't know what you're giving up and you've got to take care and we've got to worry about ethics. Not to discount that, but the point is that if you don't see the positive parts of this, then you're not going to be interested. You're going to be protective. It's interesting. I think it's a great parallel, an unfortunate one, but for better or worse, with like nuclear power. I grew up...I marched at nuclear power plants, and now I'm going well, given the alternatives, is that better? I came up with a very unnuanced thing. If we can get people to lean into curiosity and self-identity that this is something that matters to me. I just posted a link to [INAUDIBLE] I made that's a musical instrument. I think, Charlotte, you mentioned being able to make music and art. This was a toy I made that you can run on your laptop or your tablet or your phone and if you put the sound on, you can make somewhat beautiful music. Well, if you get into it, then there's actually the source code is on there and we actually have a tutorial for how, I'd say, a pretty advanced high school kid could go make that. I think this idea of sort of engaging the maker cultures, you get somebody to say, hey, that's kind of fun, that's interesting. Then lean in and say, I'm kind of curious about that. And then identify...I mean, you're not going to get somebody who's not...doesn't, not [INAUDIBLE] to make everyone into computer scientists but we may need to make everyone into masters of their own destiny with AI. And just to think critically. That's the best way to, if we want as a society -- and we'll talk about this, I guess about making ethical AI -- well, we need to build an ethical society and we're not doing...we're undoing that right now. The only way to actually correct that is to have a meaningful dialogue with kids who are still forming their opinions and aren't having it formed. And I think the opinion that someone said is to get them to recognize that actually some of their opinions are being manipulated by technology, including AI and to say, I don't want that. I'm going to my own, we can make more independent thinkers. So, I think use the play and interest as it's bait but not in an inappropriate way and to lead them in and to form their own ethical conclusions and intentions. You know, I'll leave on this point, is I know that it was very fashionable a couple of years ago to say everybody has ADD -- Attention Deficit Disorder -- like I was identified as a kid, you know. I think we have intention deficit disorder here, is that I think every kid needs to say, this is how I am going to move through the world with all of these technologies. And I think if they form that intent, then that'll be their compass as they go forward. So, I think I can clone that, intention deficit disorder. [ LAUGHTER ] SAHA: Thank you. And on that note, I want to dive a little bit deeper into this because this whole thread around ethics has kind of been an undercurrent through our discussion so far. And I've heard you talk about it just now and I've heard you just talk about curiosity. So, let's dive deeper a little bit into this, because it's very much in the dialogue. I mean, everything that you hear now about AI at least in the media and in the things that we consume in terms of books, television shows, movies, we often see the worst case scenario, right, especially from like an entertainment standpoint and especially from like a debate standpoint if it's like a political debate or anything like that. So, you've touched a little bit on this, but from each of you, I'd love to hear your perspective, you know, how does one steer technology such as AI in the sort of like quote-unquote, ethical path so to say. And [INAUDIBLE] world educators [INAUDIBLE] hear on that journey and really guiding young people as they look at this technology and play with this technology and learn more. BOINODIRIS: Who do you want to go first? COHN: You. SAHA: Phaedra, you're talking. So, why don't you? [ LAUGHTER ] And this is your Ph.D. matter, so. BOINODIRIS: Okay, okay. So, AI can absolutely be used to manipulate how one sees the world. So, that's how you start. You've got to start with that premise. And there are absolutely ways in which to mitigate the risk of bias. The reason why I ended up pursuing this Ph.D. in AI and ethics was when the news about what happened with Cambridge Analytica landed [INAUDIBLE]. I was so completely horrified, completely disgusted at the extent of malintent of how people were manipulated through the use of AI that I decided I wanted to learn everything and anything I could about this space. And although there are examples of malintent, I think the vast majority of challenges that we're seeing in artificial intelligence with respect to bias were from those organizations that have the very best of intentions. They just don't have mechanisms in place that mitigate the risk of bias such that decisions being made by an artificial intelligence would cause harm. But educators, this is where you can step in and help. There are three fundamental ways in which I believe that people can mitigate this risk. Okay. One is culture, and this is something that we could absolutely be teaching in schools right now, right. The importance of diversity and inclusivity in teams, right, the idea of having red team versus blue teams to start pushing against or gauging like what are the unintended consequences of the technology. Like start having those kinds of discussions within a classroom so kids can start to think through like, well, here's what we think might happen, or here's the intended audience. What could happen to other groups that, you know may not be on radar should this technology be deployed. Incorporating ethics into Design Thinking, right. If you're teaching Design Thinking already in schools, which is again I think fundamentally important, incorporating ethics where, again, you're training people to think about doing predictive models and about the unintended consequences I think is key. The second pillar is, of course, the use forensic technology to mine data for bias, and you could have conversations about what hidden bias looks like, which I think is really important. And then, of course, bringing about governance standards, like how would you prevent, like what are the standards that your organization would need to stick to in order to ensure that people aren't hurt. I think these are, again, things that you could start talking about in your classroom today. And you wouldn't need to do it only with a class of coders; again, this is for everybody. You're teaching a social studies class, right, this is perfect [fodder, again], to be able to think through the why and how do you protect people to ensure that you're using vast and powerful technologies like artificial intelligence to its best use for everybody. SAHA: Thank you, Phaedra. GRIMM: I think what I'd say is right related to that actually, because I was going to talk about civics and social studies courses. And I went to a presentation at Duke University where it was a presentation on AI, and I found myself in the room with like brilliant people talking about AI and medicine and there were different panels. And I ended up in one about the militarization of AI. And there's a general from the U.S. military, I don't remember which branch of service, but he was definitely a general. And he was talking about robotics, autonomously [INAUDIBLE] robotics and the fact that they are already possible, that you could have a robot go out into the world and act as a soldier. And he was also talking about how we have drones that are flown by pilots in Arizona and they go find targets in the Middle East and a person in Arizona, you know, pulls the trigger and then goes home to dinner. And right now, there's still a person in the link that makes the decision. This type of conversation is really interesting to high schoolers, and you can engage in the conversation about, what are we deciding to do here? And so, at the end of that talk I raised my hand and I said, we see an increased militarization of our police forces here in America. Do you that at some point we would have police officers that are fully robotic and autonomous? And he said, absolutely. And that was the day when I decided to pursue education and making sure that students know about AI. And so I say that because we're not without hope. In the world, in the eighties we created blinding laser weapons. They are exactly what they sound like. And we said, this is a bad idea, and there's a treaty that those weapons will not be used in war throughout the world because of the treaty that was created. We also have the Geneva Convention. There are other examples of times where we said, you know what, this chemical weapon is not okay. It doesn't matter that it can exist, we chose not to let it exist. And so, for me, the power of talking about AI and ethics is to say, look, we can create controls. We can decide whether something that goes into cleaning up a nuclear waste spill or fighting a forest fire is an application where we want a robot instead of a human to go into that space, but we don't want them to go on to the battlefield and make life and death decisions on our behalf. And we can have those nuance conversations with high schoolers. So, that's what I would say is like ethical conversations built on the legislation that we can create, our technology is only as good or as bad as we choose to make it. COHN: Can I...was this a multiple choice question? SAHA: It was a two-part question, but I think you're covering it. COHN: Permission to be a little provocative. SAHA: Yes. COHN: Okay. So, I absolutely agree with the need to really discuss ethics. It's an incredible issue and things like privacy on the small on a personal level and ethics of what does that mean actually in the big world. We can't even agree on our own ethics, as a country, as a world. And I don't say that in a disparaging way; people are different, and it's situational. I have a concern in the overall dialogue in the world about these technologies that kind of resembles a little bit what the beginning of electricity, for example, where fear and concerns that were all legitimate were dominating the conversation. And I don't think that we're anywhere near...we should be so lucky that we were dominating the conversation with ethics, et cetera. But I think we need, from a teaching standpoint, we need to be a little careful because if we only go to the stigma, right of this, then we are putting a very powerful message in that this is an evil technology that needs to be tamed. And while that's not completely untrue, we really need to develop curricula that I think focuses intently -- kind of going back to your sorting hat thing, Phaedra -- then it is like the power of this, the transformational power; whether you're are a practitioner and writing this stuff or you're a user of it, how it will change every line of work. And I really think that we need to as a world figure out how do we frame that conversation so we don't lead with the danger, because the danger is real, and I'm not at all an apologist for this, because I think one thing that I found in this year long task force is that I might have been underestimating the legitimate concerns that people have. But I think the main thing we need to do is talk about technology as being somewhat neutral, you know. The same technology that could be used for battlefield, you know, should never be used for, you should never relegate a life or death decision whether it's in combat or in the operating room to an AI until unless it meets some ethical standards. We, on the other hand, in self-driving cars we relegate...we're starting to relegate those decisions. And anybody's been on an airplane in the last 15 years is already making that task link. So, just getting people to think critically. I think the main thing I would say, so I think we need to balance, it sounds funny because the AI is out there and we really need to talk about these ethical things but we also need to talk about the power of it so that people are making a nuanced and balance decision. I think the main thing in terms of getting ethical...people to think about it ethically is to recognize that certainty in science or politics is a bad thing. And if you teach kids to be responsible questioners about anything, especially not just technology but to not take anything that comes out of a politician's mouth, a religious leader's mouth anything as absolutely fact and to actually have them wonder could this technology be used in a way that most... I think Phaedra, you said it, that...or, maybe it was Charlotte, said nobody sets out to write a piece of biased software or a piece of software that hopefully, not to create a piece of AI software that is intentionally going to have some ill effect, though there are some bad players; what they fail to do is to think about the unintended consequences. And if we can teach kids to be questioning a little bit and to say, how could this be misused? Then I think that is, even though the individual technologies will change vastly over their working life, if you give that applied uncertainty and maybe the wisdom of insecurity a little bit then they will constantly, they will spread that sort of ethical, the decision making, the applied doubt and they will actually serve us very well, to say never trust anything implicitly and don't trust anything you don't understand. And then maybe get them to lean into understand. SAHA: Thank you, John. So, what I'm hearing is there's a lot to be aware of. There's a lot of potential good, potential bad, and there's a deep need for sort of this habit of questioning from all of you. I'm cognizant of the time and I want to make sure that our audience gets an opportunity to ask you all some questions. So, I'll ask one last one. A quick one, and if each of you could just give maybe like a minute or so of your take on this. And I ask because this is the future for the kids in the classroom for these educators. So, what trends do you see in the workforce with AI that are especially important for us to pay attention to? What should we keep in mind as we're dealing with our young people and preparing them for the future? DUNGAN: I'd like to take that to start. SAHA: Sure. DUNGAN: I think if you want to talk about preparing students for the future, you need to make sure the future is representative in that everyone can imagine themselves as being a part of AI. So, if you're talking about being in the classroom, I would just make sure that we create structures that are going to bring women and minorities into the fields of computer science and artificial intelligence. That means that you may need to encourage girls in a different way than you encourage boys to take higher level courses. They may actually need some explicit words to them. You know, you're really good at math or I've notice that you're really engaging with this material, why don't you go ahead and take that advanced course? And girls actually need that more than boys, like if you read the literature on it. So, take the time to do that. And also you can, within your school, structure enrollment for courses that are higher level in math and science, STEM, computer science, and require that the demographics of your enrollment matches the demographic of your school, or, the demographics of your AIG program or whatever it is, how ever you could put a reasonable bucket on it. If you have 50 percent girls in your school, then maybe you should have 50 percent girls enrolling in those types of courses. And if they're not interested in enrolling in courses, maybe find out why. What isn't welcoming to that space? You want to make sure that your workforce matches the world, because then bias automatically disappears, people are looking out for that kind of thing. SAHA: Thanks. BOINODIRIS: Yes, yes, yes to what Charlotte said. It's a definitely big focus on diversity and inclusivity. I could not agree more. I would also add on and say, domain knowledge, meaning, okay we've got this wonderful technology of artificial intelligence, how can it be applied to public health? How can it be applied to agriculture? Like understanding the business use cases, the industry use cases and really getting a feel for that beyond the code, what can it do to really make advances with respect to society and the economy, local economies. I think that is key. An example. You know, precision medicine is a really hot topic, right? The idea that I can use artificial intelligence to learn about a specific person and then recommend a custom drug regimen. Without fully understanding what data is used in order to train such an AI to make sure that it is not biased, like this is things that we've got to be teaching within medical schools today. So, apart from computer sciences, we need to make sure that it truly is cross domain. COHN: Yes. I would say, I just want to, I think two themes that you both said I really like. First of all, on the diversity. As the middle-aged white guy from...I think I'm no longer middle aged, am I? Ugh. [ LAUGHTER ] But I think that diversity is so important because we are leading so much talent. You know, this is going to affect everybody. Our AI will be better, it will be more useful, it will be more playful if we actually have an inclusive there's a lot of, like the...what was the guy's name? Johansson, I can't remember the guy's first name. He wrote something called The Medici Effect. And he did an analytical study and that actually, teams that actually had more diverse ways of thinking, and that included by gender, by political belief, by religious belief, by sexual orientation actually are more resilient as organizations and actually have better innovation outcomes. So, it's just pragmatically we need to get more of our under represented groups into it. But more importantly, as we start to think about ethics, if we wanted to represent our ethics -- and that's a very hard topic, we don't all agree on ethics -- you need the ethics that are being reflected to reflect the community. You don't want people who are just middle-aged white businessmen...I shouldn't say it that way. You don't want one segment of the population making the ethical decisions for everyone; this is something for all of us. But I think that the other theme that you both kind of touched on is that we have to think about what get the students to say, what is AI? What does this technology mean to me and whoever me is, me or us? And to be thinking about that. And the future of work is really, there's some very interesting studies out there of how AI will affect things. And there's a lot of, there's some kind of...there's a kind of traditional it's almost like luddite view that any technological change is going to, you know, robots are going to take over the world. And that's probably not the big worry right now; the big worry is that it is displacing jobs in the middle. If you look at the studies direct hands on labor -- you know, artists, musicians, farm workers laborers -- that's not going to get displaced. So, those are actually, those roles are not going to be massively affected by AI; certainly some robotic kind of stuff. And there are definitely new opportunities that kind of come in that of technological applications of AI. What's a little bit more concern is that the bulk, the middle of all of the in common and skills requirement for service kind of roles like retail or accounting or those kind of things that are now done primarily by humans are going to change fundamentally, there's going to be more and more tasks that are automated. And if your students are prepared for that, they can become specialized in those targeted AI skills that they can excel in those areas, but they may want to make a different choice. They may want to make choices. So, I think thinking about how do they make an intentional decision about how this AI is going to affect my career choices. I think we have to think about how we do that. And I think the last point that I just really want to pick up is, we talk about this technology for those who are maybe going into it as thinking about it from a very narrow lens of computer science, you're going to [write] AI. But I think the main thing is what Charlotte, you just said, is to get people very interested in domains, you know -- maybe it's veterinary science, maybe it's agriculture, maybe it's art -- and to figure out, you know, it's like you can create a cross product of how will AI affect that. And more importantly, can say as a student, how might I pair up with someone who knows AI, or how might that increase my art or improve my efficiency? And bringing that from another domain, because what I see is the most interesting advances are now happening not in the pure technology of AI, which is advancing really quickly, but in the application. So, if we get them to think about AI as a tool like electricity is a tool or steam is a tool or whatever; that not to think of it as an end in itself, because that's a very limited set. Hopefully we can grow that, make it more diverse. But it's going to affect everyone and to be able to ask the question, how can I use this technology on the positive side and how do I make sure that this technology is not upsetting or making bad choices on my behalf. SAHA: Thank you, John. So, I'm looking at the time and I want to make sure that we get at least one question in from the audience, because I know that they want to speak with you. So, here's one that I think is really particular but also it gets to things that affect all of us. So, the question is, how can AI help in the medical [field] in early detection and at any natural disaster, right? So, health, medicine, environment, things that we all grapple with, where does AI play in this? How can it help? All of you really, have at it. We have a couple of minutes left. DUNGAN: Well, I'll start with saying we haven't figured it all out yet. I was on the East Coast about three weeks ago and there was a hurricane coming and we didn't know if it was going to hit us or not and we kept saying yes, and then no, and should we leave, and should we not leave. And we just don't have all of the modeling yet. Sometimes that's, we need more data, and sometimes it's just that there are unpredictable factors in the world that don't allow us to know everything. I think it's okay to say we don't know everything yet. BOINODIRIS: To tag on to what charlotte just said, again, I mentioned precision medicine. You know, in the country of the United States today, African-Americans are routinely prescribed fewer pain meds than white Americans. The child mortality rate for African-American babies is far worse off than the child mortality rates of white. Again, this is due to systemic racism in the medical field. If that same data, that same data is used to train an AI for things like precision medicine, that same systemic bias gets calcified. So, she's right. We are not there yet, because we need to make sure we're educating as many people as possible to mitigate the risks of bias in AI so we do not unintentionally cause harm. It's vastly important we get our arms around it. COHN: Yes. I think I was thinking of a comment that Charlotte made in the text, is that I think that if you look at how AI will impact anything, it's by, at least at its current state, it's by augmenting human intelligence. And I think what's interesting is having been involved in trying to use AI for healthcare the early...I think this is an interesting both promising and cautionary tale that we think about AI and we can see the future about how it might be able to make perfect diagnosis because every doctor would be as smart as all the doctors and those are the kind of answers. But the reality of any of these technologies is you have to adopt it in a slow and careful way. You have to make sure that's it trustworthy, that it's explainable -- I think you had used that word, Phaedra -- that it's no biased. But you also have to be able to, AI and humans it's kind of an interesting dialogue. My experience with doing automation of any sort is that you need to figure out how does AI actually work with human practitioners so you build mutual trust. In some weird way, clearly, we need to trust the AI, because we need to be able to say, why did you make that? If I'm making a diagnosis that might for a medical case ask for a procedure or actually do a procedure or administer some drugs, we have to make sure that we understand the rationale. Now, that's not the say eventually that the AI might encorpus enough knowledge, but if we don't trust it then we really can't make that relegation. So, the art is actually making this kind of, starting with the right dance between AI as kind of a recommender and then to be actually modeling a student to be able to understand if it made a wrong decision because the data was wrong. The data was [INAUDIBLE], data was biased. And to be able to correct and actually make that loop work, then you actually are building mutual trust. You're trusting...you're starting to trust the AI tool, which is not an intelligence in any real sense, but in some very real way the AI is starting to have to learn to trust you with your instincts, because if we want it to replicate our instincts whether they're medical or ethical or anything like that, it needs to learn how to work with us. So, I think that idea of actually starting AI in way that is very much at a mutual advisory level. And I think that it's very interesting. My particular specialty right now is on something called neuro symbolic [INAUDIBLE]. We spend a lot of time looking at pedagogy, looking at Seymour Papert and those kinds of things of how students learn to kind of correct and recorrect the way that we think about not just how AI should technically work but how it should work from a user experience, from a human experience. It's not just a usability thing, it's an appropriate use of technology. So, I think that we're on a really exciting path and I think we have to be, just like I said before, certainty is the enemy of innovation. We have to be cautious, we have to be a little insecure, we have to be a little nervous about this. And a little nervousness is always a good thing. SAHA: Thank you. So, we have about three minutes left, and there's a question in the chat and I'll start it off and I'll turn it over to all of you. The question is, you know, very timely with back to school for the vast majority of our folks here, is where do you find age appropriate curriculum? And I'll start by saying to all of you in case I lose you is that after this webinar you will receive an e-mail from P-TECH network with the webinar recording as well as a link to a site that's going to give you many, many resources, some of it very much curated from our panelists but also Open P-TECH resources on AI that are specifically geared towards students and educators where you can earn digital credentials. So, that is all coming to everyone who registered for this webinar, so you can certainly follow upon curriculum and resources. But while we have our panelists, let me ask the three of you if there are any sort of like go to, like really, like things that you love that you'd like to share with the audience in terms of curriculum resources for those who are looking for for their students or for themselves. Please share. DUNGAN: I want to plug the ISTE, there's a teacher training webinar that they have. It's called something about AI for education and it's a really good resource that fits right with the P-TECH curriculum. I think they dovetail nicely, and they have a lot of great resources you can put directly into the classroom. I've also, within a month, we'll have aiforteachers.org up, and that will have a lot of great selection of curricula. But the P-TECH network has some really great stuff. I just got to plug it, too. SAHA: Thanks, Charlotte, and everyone who's listening. We have the links for those all teed up for you again in this follow up e-mail that's going to come for you. COHN: I think that's, Charlotte, that's going to be such a great resource, because we're trying to build better computer science; and therefore, eventually AI curriculum here in Vermont. I would say there are a lot of things out there. I really like the cognimates link that you shared with us, Charlotte. That's the M.I.T. team that I'm working with. I'm a big proponent of things like First Robotics that has programs for middle schools to senior high school that has an increasing AI component and a lot of resources and a lot of good curated stuff. And I'm a big believer in things like Code for America, Girls Who Code, because those have great curricula. And they aren't necessarily, I don't know how pedagogically they are aligned in terms of standards. But they have a lot of material and they actually have...the most important thing is that they have access to mentoring networks that are very important because that human contact will always be important. BOINODIRIS: In the resources that I sent also you're going to see some links to some more playful activities that I think could be of interest to your students. I'm already linked to the AI Powered Harry Potter Sorting Hat Recipe. We've got a recipe for kids to make medical Minecraft, which is integrated with an AI, smart clothing, the empathy bot. if you want your kids to try that out. Additionally, Epic Games, which makes Fortnite, has also released some really cool curriculum for middle school and high school kids that also have a fantastic introduction to artificial intelligence; and it ties to science, classes...social studies. As an example, there was one high school teacher, he won a grant from Epic Games. They have $47 million set aside for teachers to curate AI curriculum using the Fortnite engine. COHN: Wow. BOINODIRIS: Yes. And he designed a curriculum where kids can create sustainable energy wind farms using the Unreal Engine. Check them out. It's super cool. COHN: That's sounds so cool. SAHA: Thank you. Thank you, thank you so much. So, we are at the top of the hour. I know that we'd love to keep the panelists for longer, but we are ending now. So, Charlotte, John and Phaedra, thank you so much for sharing your perspectives with us today. To all of our participants, thank you for joining. Again, please be on the lookout for an e-mail from P-TECH network with a recording of this webinar as well as all of these free resources, some of which you just heard about right now, some from IBM that are all geared towards teachers and students to further their learning on AI. We have it all nicely curated for you and on one page. So, thank you again, everyone, and I hope that this has been enjoyable for you. It has been for me, so thanks. COHN: Thanks. BOINODIRIS: Thank you. Thanks, everybody. [ MUSIC ] 