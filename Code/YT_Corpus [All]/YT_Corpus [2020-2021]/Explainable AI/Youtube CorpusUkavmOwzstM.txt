 hi everyone thank you so much for joining my session the importance of model furnace and the interpretability in AI system I'm a Francesca at Larry I'm a senior cloud advocate at Microsoft where I lead a team of data scientist AI developers to build an end-to-end solution on answer specifically during this session we are going to talk about how you as a data scientist or a developer can build the end-to-end responsible machine learning solutions we have been writing the numerous articles around these important topic the latest article that you can find is called the machine Ernest I put a link there so you can check it out so there are also many resources that we are going to talk about doing these infection some of these resources that are open sources so you can see that we put several different and github repos such as I interpret analogy pop but also the affair AI further and a I get up before so you can check it out all of these links and resources after this session and again if you want to talk more and feel free to ask me questions so the agenda for today is going to be divided and mean in three main parts that we have the first part where we're going to talk about what said where their responsibilities I what we mean a we they're responsible are teaching intelligence and then we are going to look at the two main packages one is called the day interpret and now package and the second one is called the defer learning toolkit as you know we are in a moment how the history where we are all leveraging the data in order to making a significant decision that really are going to affect individualizing different type of the means such as healthcare justice finance education marketing but also in HR employment so it is very important for and specifically for our customers to ensure the safe ethical and responsible use of the artificial intelligence we know that AI has the potential to drive bigger changes in the way we do business and on the other side like all great technology technological innovation the posture is also important that to keep in mind that this type of the technological innovation is going to have a very broad impact on society as well for all these result we think that is important when you build AI solutions and machine learning algorithms to ask yourself to the following question so how can i as a data scientist and developers design building use AI systems that create a positive impact on individuals and society in an important question for example is how do we best ensure that AI is a safe and also reliable how can we attain the benefit of a high but also respecting privacy so these are all open questions that developers and data scientists have and at Microsoft we have building different tools so that you can use a leverage in your existing solution or to build a new solution in order to assess the furnace level of your machine learning application moreover we also her seeing that many customers are using the same challenge there are many different reports here I'm putting a recent capture near the capture Nene the report that showed that in nine out of ten organization we know that they are facing ethical issues in the implementation of AI systems so this organization cited actually different reasons including the pressure to quickly implement the AI the failure to consider ethics when implementing their system and also the lack of the resources did to ethical AI systems at Microsoft we build a diesel system and it is a sort of a foundation to guide our thinking as a data scientist and we defined six ethical principles that a Isis temple should follow so as you can see from this slide the first war arsonists the reliability and safety privacy and security and finally we have inclusiveness so these are really key properties that each AI system should achieve the second two are transparency and accountability these are somehow underneath all of the other principles and guide how we design implement and operationalize AI systems so let's see what we mean a bi transparency and how we can implement it so let's start with a transparency what do we mean actually with transparency so we mean two main things the first of all is that AI system should be explainable and also that AI system should have algorithms that are accountable meaning that you can actually understand why they are producing specific results there are a few cases of other machine learning repeatability we can define them following two different categories as you can see there is the first one it is a model designers evaluation so this is more at the training time and the second one is the end-user or providers of a solution to end the user so these are at the influencing time this is more the time where they are consumed your AI applications so there are many different use cases that are important to keep in mind for both categories like data scientists needed to explain the output of a model to stakeholders usually these are geeses users and also clients in order to be the trust another very important and popular would say use cases when data scientists needed tools to verify if a model behavior matches the pretty clear objectives finally also data scientists needed also to ensure the fairness of their training model their application and other use cases that are more like from the insurance in the time category when again your predictions so the results that you get from your application need to be explained at the influencing time some of the most popular cases are in the healthcare in finance industry so like why a model you classify the favio like a customer ID at risk for colon cancer another important a question from the finance industry that we receive very often is why a specific client in this case we call her routine was denied a mortgage loan or why his investment portfolio carries a higher risk so these are all questions that somehow you have to know how to answer in this basically you have to know why you got you got the specific reason also so that's why at Microsoft we developed they interpretability toolkit so here is a toolkit that really helps the data scientist to interpret and explain their motto and we put together these are two toolkit in order to explain a machine learning models who globally meaning on all the data or locally on a specific data point to using the really state of our technology in a very easy to use way second we wanted to incorporate the cutting hydrant availability solution that are developed by Microsoft and but also leverage all the open source community so the solutions also dual aspect is that is they're important tech finally we were able to create a common API and also data structure across the integrated library same integrates these will be the azure services interpret an owl he said user a toolkit that you can find that a KDOT Anessa slash interpret ml toolkit really gives you access to the state of art interpretability techniques through an open unified API and they also provides you a lot of visualization that you can use in order to understand better why your model is predicting a specific disaster so in with this toolkit you can understand the model so using the wide range of explainers and techniques so using really type of interactive type of the visuals you can also choose algorithms and experiment with different combinations of algorithms you can also explore the data scientists a different model attributes such as for example if you are more interested in the performance or the global and local features and you can compare again different models multiple models are the same types also this is very nice in order to find more information and you can again look at these pickup repo and also remember that you can run what-if analysis as you manipulate the data in the view the impact also of your model so why either we start to means the project integrated repo you will see that the interpret community was able to extend the interpreter and open source and I don't package from Microsoft research that was used to Train interpretable models and help the ingles so to explain black box system in just a few minutes we are going to see also what we mean with the black box system and so these was air so the interpreter community was able to extend that these interpret capability with additional interpretability techniques and also utility function to endow also I would say the real world data set and the workflow so with this package you can train an interpretable last box model time to explain black box system and also you can use at these packages to understand your model global behavior or to understand the reason behind each individual prediction as you can see our machine learning is a sort of rocker so we call it a dremel in Turpan and is really a wrapper because it helps you save explanation in Ronnie's three remote and parallel computing of or explanation on a German computer so this is an additional capability of that hydra-matic an offer for you and also is able to create the scoring explainer for you and most importantly if you want to push your model into production that can expect rationales these explainers for you in the github repo we will also see that there is a what we call the interpret tax bills on interpreter we have added extension to support the text model so there are two different type of models that are supported as you can see there is what we call the glass box explanation so these are for example explainable boosting linear models a decision tree or dual systems and we have also a black box explanation like lime sharp partial dependent sensitivity analysis the black box models are challenging in order to understand for example deep deep neural networks so but black box explainers can analyze the relationship between input features and output predictions to interpret models so as I mentioned on my previous slide are some of the examples you can clue the line and sharper as well so talking about the shop let's take a closer look at it so shop is a game theory approach to explain the output of any machine learning model so it connects the optimal credit allocation with the local explanations using what we called a classic cash simply value so from the game Atari and they're also they're related extension so let's see together how we can actually apply shuttle to a real sample machine learning use case select consider a black box that predicts a price of a condo or an apartment based on all these features as so as you can see there is a proximity to a green area such as a park and also the father-to-be building itself is a pet friendly or not in this case the feature is a negativity so with this in mind with these features our model predicts that the average cost of the apartment the average price of the apartment is at 300k how much has each of these feature contributed to the prediction compared to the average prediction as you can see we have a different information such as the house price prediction that is about 300,000 euro we have a average house price prediction for all departments and this is about 310 euros so the Delta here is negative is a minus 10,000 game are the absolute values so as you can see we have a different feature as simple I started with the parts so how the parks are contributed to these results so we have a glass of 10 K then we have the fabric and cat suburban also contributed in a negative 50 K so the fact that the building self is not quite friendly also the size of the part of the apartment is a very important feature in this case and we see that he contributed that's actually 210 G and then we also see that there is a final feature that he said the father to Department is at the second floor they had a Farrell unique contribution so this is a final attribute this final feature actually was not really impacting our model results so how did the calculator how did actually sharply calculate all these values so we will take features of interest for example cats van and we will move either from the feature set second we take the remaining features and we generate all possible a possible collision and finally we add and remove your feature all interested to each of there is a collision and we calculated a different the difference that it makes so this is really how shutter works so this is really the logic that is behind sharp of course there are some pros and cons so that it's important to be mind when you decided to use a sharp for example sharp is great because these based on a solid the theory of that and also distributes that they affect in a very clear way however on the other side we produce also contrasting the explanation with a clue like explanation that some sometimes even instead of comparing a prediction to in average a prediction of the entire data set you could compare it to a subset or even to a single data point well in terms of what are the consulship competition time he is a it's possible that for example you can use like a 2k more or less a possible collision of the future values for K features sometimes is difficult to understand it so it can be the mistress interpret and finally the inclusion of the unrealistic data instance and when the features are created is also some it's also very possible so it's a risk that you should keep in mind if when you decided to use a so as I say that there are also a different model so that you can use there are different tools so interpretability approaches based on how you want to use a different models so in terms of glass box models these are models that are interpretable due to their structure example are explainable boosted elbows finger machines and linear models and also decision trees glass box models producer los perros less explanations and are editable by Dominic's expert which is something very very nice to have when you want to get to to use and leverage these glass box type of models so in terms of the GLM so this is a generalized linear model as you can see this is a flexible generalization of the in ordinary linear regression that allows of for Responsive bibles that have arrow dissolution models other than normal distributions so as you can see the main characteristic of the generalized linear models is that are the current standard for interpretable models and also that they learn an anti-diva relationship between the data and response there's another sample is that the explainable boosting machine so here and you can call them also EDM yeah so this is a sort of the interpretable model that has been developed by Microsoft a researcher it is a very interesting model because it uses a modern machine learning techniques like banking gradient boosting and also terminating interaction detection to improve the traditional generalize and DT models these this is why actually the explain about boosting the Machine are very accurate as and they are considered like a very good technique so like for example the random forests and also the gradient boosting the trees so in this second part of the presentation we are going actually to focus on up fairness we are going to see what are the different of fairness a principle which aims at to tackle the question on how we can ensure that AI system treats everyone in a fair ways fairness that has a mean goal to provide more positive outcomes and avoiding the harmful outcomes from the AI systems for different groups of people there are different types of the harm as you can see from these there's life broadly speaking I would say that we developed these are different types of arms based on the taxonomy taxonomy that Microsoft the research I created and there are five different types of harm that you can see in a machine learning system and while I have the definition of the whole of them in these slides for the scope of this project we actually just focus on the first two of them that are allocation as you can see this is the arms that can occur when a system extends or I would say with all the opportunities resources for information to specific groups of people and then we have the other one she's quality of service this is whether a system works as well for one person as he does for another person so the example of the face recognition from many different applications is probably one of the most important example of the quality of that the search is for these the furnace part Microsoft developed a new - Nikita that is called the sure learn this is a new approach to measuring and mitigating unfairness in systems that make predictions serve users or make decisions are about allocating resources opportunities or informations there are many ways that AI systems can behave unfairly for example AI can impact the quality of service which is again whether system works as well for one person as it does for another and also AI can also impact a location which is again the arm harm that occur when Isis to make sense or with all the opportunities resources or information to specific groups of people so as you can see in the in the toolkit that again you can find more at a KDOT MSA / relearn URI in this toolkit that they there are different type of focuses and different type of I would say capabilities so the main goal of these are toolkit is to empower developers of the artificial intelligence systems to assess their system fairness and also mitigate and observer fairness issues most importantly it helps user identify and mitigate unfairness in their machine learning model so with our focus on group fairness so now let's jump actually on a demo I want to show you how you can how you can use that the interpretability to keep them so in this demo we're going to see how you can use that the interpretability toolkit for tabular data in as your data Bri except as you can see we are going to see what said they toolkit that you can use and download for the explanation with us from this clinician experiment and also visualize at the future and the future importance implementing events analytic solution in our organization and for each of our customers is that I would say for a different step process so you first need to adjust the data from in different variety of data sources including batch and the streaming data and as you can see there are different option here has a diva architecture should bilasa and then the most important part is that of course you need to take in and store display to different that's being ingested regardless of the data volumes variety and the velocity we here you can do it of course with different type of products when you get into the prep and train stage you can use again as your addictive extra just to train and deploy to your model so as you can see in beta break so we'll have an option that is called the run time now that includes a variety of popular and libraries the libraries are updated with each reusing to include like new features topics as Disney to the a subset of the supported library has a sort of the top tiger libraries for these libraries ensure data breaks provides a faster update cabins updating to the latest package releases with each runtime release so it is very good for the design this as well in terms of dataset so we are going to use it is a breast breast cancer and Wisconsin dataset which is a public dataset here you can see that there are different attributes that we are going to use it with is a demo and not only in terms of ID number and technologies which are probably the most important attributes but also there are real value features that are computed for each self Mac use that we are going to analyze or use the specific so first of all you needed to first of all you need to install a sure ml interpret and I German contribute interpret packages next you need to train a sample model in a local Jupiter notebook as you can see you can again use the breast cancer data set and then you can split the data into train and test third you can call the explainer locally here you need to initialize an explainer object pass your model and then do some training data to explain our constructor in order to make explanation in the resolution and more informative you can also choose it passing the feature name and output class Sabbatarians in a output the class names for example if you are doing the trans if acacia this code that you seen mrs. lines actually show you how you can in in size sheet and explain their objective with the different types of example here specifically you have a datum blocks planar and P as I explained in a local environmental then if you want to explain the entire model behavior you can call what we call the global explanation so this is going actually to give you a sort of a visualization that you can again leverage to understanding interpret factor your models so some of these are again our producer from your Python code just using these packages and I want just to show you some of the visualization that these a package can create for you as you can see there is here in over an overview of the tree model along with its predictions and explanations so we have the deep exploration these displays an overview of the dataset along with the prediction values then we have the global importance that these aggregates features important values of individual data points to show the models overall top K these are of course configurable type of tree so you can change that number these are important features and also helps understanding of underlying models overall behavior then we have the explanation exploration so these are demonstrates how a feature effects a change in the model prediction values or also the probability of the prediction values and it's a very good visualization if you want to show the impact of a feature interaction finally we have the summary importance so these uses a different individual features important values across all the data points to show the distribution of each features impact on the connection value by using this diagram you can investigate for example in what direction and the future values effects at the prediction you another way to understand better what your model is actually doing is a by using add a local explanation you can see that here and you can get the individual feature importance and values of different data points and by coupling the explanation for individual instance or for a different group of instances here we have a different type of visualization that are created first of all we have the local importance these are shows at the top k important features for an individual prediction and it's very helpful when a data scientist wants to illustrate to the local behavior of the underlying model on a specific date time then we have a date perturbation exploration it's a sort of what-if analysis as you can see these traditional Mouse changes to feature values of the selected data point and observe resulting changes to prediction value finally another important job is resolution that I want to share with you is called the individual country a conditional expectation this visualization allows a feature value changes from a minimum value to a maximum value so it's very helpful when the data scientist needs it to illustrate how the data points prediction changes when a feature changes again this was just an overview of watts at a interpretability toolkit can do for you and how you can leverage that on your solutions I want also to share additional contacts of the product team who work in these on this earth will kit as you can see you can find their names and their emails there in case you want also to follow up offline with a product team again who put together all these tool kits that I presented today again this is one of the article that you can use it to learn more and also to find the some of the resources that have been used and used today during this session and in terms of resources I just want to share those with you one more time and these are all the packages and they get up report that have been used up in his accession and you can also find me on Twitter and github and medium thank you very much you 