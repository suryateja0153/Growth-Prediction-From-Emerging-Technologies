 next up we've got jeremy singer vine you may know him from his data investigation work as data editor at buzzfeed or like me from five years running data is plural a weekly newsletter with interesting data sets running from open government statistics all the way through to the color distribution of skittles and a database of canine cosmonauts jeremy's going to talk to us about data journalism as a specialty and in particular fincen which being used to investigate international money laundering and financial crime welcome jeremy hello i'm jeremy singervan and for this talk the most relevant fact about me is that i'm the data editor at buzzfeed news where i run a small team that collects and analyzes data for the purposes of journalism today i'll be talking about the largest reporting project i've ever worked on the fincen files which we published this september in collaboration with the international consortium of investigative journalists and more than 100 news organizations around the world you can use this qr code or link to read our full coverage today i'll use the fincen files as a lens through which to talk somewhat more broadly about data journalism and how the work we do might differ from the work in your field of expertise work on the fincen files began more than a year ago buzzfeed news received a remarkable collection of secret government documents related to suspicious financial transactions this huge trove had been assembled at the request of law enforcement agencies and congressional committees investigating the 2016 presidential election and other matters the documents were so remarkable and globe spanning that we began to talk with icij the international consortium of investigative journalists the organization that published the panama papers we talked to them about working to quote unquote scale the reporting if you will by the end of the project more than 400 journalists in 88 countries and pitched in we found that in these documents alone big banks had approved trillions of dollars of transactions that they later told the government seemed suspicious these banks failed sometimes to take even the most basic steps to root out financial crime such as determining whose money they were actually moving and we published specific findings about the conduct of hsbc deutsche bank standard chartered and other major financial institutions we also unearthed new details about major scandals such as the so-called mirror trends but now i've jumped directly from the beginning to the end skipping 99 percent of the effort so much work went into this project and i don't have time to talk about it all not even a tiny fraction of it but i do have time to talk a little bit about the data before i do though i think it might be helpful to talk a little bit more about how i think about data in a newsroom like buzzfeed news many of you are likely familiar with the term data journalism it's become quite popular when many people think of data journalism they think about data journalism as a product articles full of numbers and charts and articles that focus on numbers at buzzfeed news we do some of that for sure but much more of our effort goes into something i'd call data for journalism data journalism as a process and that was definitely true of the fincen files although we did publish an article with some numerical findings our main goal was not to produce a data journalism product but instead to use our skills to help our colleagues understand the documents to use data and data analysis to find interesting stories within the documents and to put those stories in context another reason i'm sometimes skeptical of the phrase data journalism is that the data rarely stands alone instead it's supported by and supports a huge amount of other journalistic work conducting interviews for example reading lawsuits filing public regrets public records requests and so on so our goal as journalists with data skills working on the fincen files was really to ask ourselves how can we make this information most useful not just for numbers but for the project as a whole what materials did we have to work with well there was very little machine readable structured data and this is pretty common we rarely start out with some amazing perfect data set instead we often have to work to get the data we need you may find the situation familiar there were some csvs in the documents but they were sort of at the periphery instead at the heart of the corpus were more than 2100 suspicious activity reports which are official notifications that u.s banks must send to the treasury department when they observe transactions that seem suspicious for example the transactions might carry hallmarks of potential money laundering or other financial crime banks filed millions of these per year we only had 2100 that was still magnitudes more than had ever come to public light before and these documents were incredibly detailed here's an example sars have two main parts the first is a set of tables at the top of the document this is an example here of just the beginning of one of those tables and that's just one sar imagine this one going on for several pages sometimes dozens of pages the tables list the people and organizations involved the overall dollar amount of suspicious activity the general time frame the bank that filed the report and more and here's an example of one of the narratives again just the beginning of one some of the narratives are very short but others include a lot of helpful details and unlike the tables narratives often list individual transactions rather than just the total aggregate amount and they often specified the banks and senders and recipients involved in those individual transactions so where did we go from there the most obvious thing we needed to do from the very beginning was to liberate the data trapped in those tables at the top of each pdf this work was in many ways familiar territory data liberation whether it be web scraping pdf parsing or other techniques is sort of our bread and butter we do it very often we have a buzzfeed news parsed pdfs to create new data sets for investigations into foster care providers figure skating judges and other topics because the sars weren't quite standard tables they involved several layers of nesting and they contained some idiosyncrasies we wrote a custom parser using pdf plumber an open source library i've been developing for tasks like these we then converted the raw representation of that data in those pdfs into a series of so-called tidy data tables so we could quickly run new queries in a broad variety of environments these tables were just plain csvs we're talking about megabytes of data and not gigabytes or terabytes the data from the tables helped us answer questions such as which banks appeared most often what people and companies were listed as subjects more than once as well as more intricate analyses tailored to specific stories or specific reporters interests i'd like to give a special shout out to a couple things in our stack now a lot of the tools we use on the fincen files and more generally are pretty well known python jupiter the pandas data analysis library git for example but there are a couple of things perhaps less known that i'd like to highlight and to recommend one is just a concept a few moments ago i mentioned tidy data it's a particular way of structuring datasets some of you may already be familiar with this concept popularized by hadley wickham of taivor's fame but if you're not i highly recommend reading his excellent 2014 paper on the topic here's a link to that paper tidy data both the concept and the practice have deeply changed the way i think about and how i work with data on a daily basis on every project i work on the other thing i'd like to highlight is a piece of software and that's visa data it's a tool for exploring and analyzing data sets on the command line i've been using visit data for years now and have even begun making small contributions to the code base it's gradually becoming more popular but i think there's still a lot of people who still have never heard of it but would really enjoy using it and again here's a link to visit data's homepage i used visitdata practically every day i was working on the fincen files and because it's extensible i could build custom commands specific to the project all of our final analyses ended up scripted in python fully reproducible but visit visitdata was an invaluable tool for quickly searching through sorting and joining the data in the fincen files and for quickly testing out new hypotheses so back to the sars if the tables were in some ways relatively easy to work with what about the narratives those free-form sections of each sar that provide additional details those posed a greater challenge as i mentioned earlier we were most interested in capturing the individual transactions that the narratives discuss and to understand the approach we end up taking i think it helps to discuss some of the most common constraints that we face in data four journalism and how those did and didn't apply to this particular project first are the related concepts of time and money nobody has infinite time but in journalism we tend to have a little less time than in some other fields even so the fincen files was a bit of an outlier from the very beginning editors understood that the material deserved a long and thorough investigation and what about money money as you might have heard is a bit tight in the journalism industry financial constraints don't typically have a huge effect on the tools we use since our stack is almost entirely free and open source but money does have an effect on human resources how many people we can afford to have working on a particular project for instance here too the fincen files was an outline we had hundreds of people working on not all of them full time of course especially during a pandemic but it was still a huge difference from the typical investigation aside from resources there are constraints related to the things we want to publish perhaps the most important are specificity inaccuracy and the sort of tension between those two things investigative journalists want to be very specific rather than to say only in general terms that there have been failures to combat money laundering journalists want to be able to say by whom and when and how much when it comes to data analysis however this love for specificity is often at odds in tension with more cutting edge probabilistic techniques an algorithm that produces correct results for ninety percent of inputs may be revolutionary in some fields but largely useless for many journalistic projects and finally one more constraint i'd like to mention something you might call explainability can we explain our results to our audience and will they understand us now whether something is explainable depends on the audience internally among an audience of colleagues there's a relatively high tolerance for complex data analysis but for our external audience we're very sensitive to the complexity of what we publish our readers are ideally a huge cross-section of society and that includes people who don't know the meaning or even scared by phrases like linear regression or natural language processing of course that doesn't stop us from using linear regressions or natural language processing in our work but we want to make sure that whatever techniques we do use we can fairly and accurately describe them in simple plain language and those results make sense to general readers we spend a lot of time thinking about how exactly to do that so with all of this in mind back to the narrative parts of the suspicious activity reports those narratives spanned 10 000 pages and about 3 million words we wanted to identify the individual transactions described within them at first we thought being computer programmers maybe we can automate this perhaps we could use regular expressions or train an algorithm to identify the transactions and extract the relevant variables the names of people banks payment amounts and the relationship between those variables but we found these narratives were written in such a wide variety of styles each bank employing a different structure and jargon we also noticed that some of the descriptions might be fairly straightforward but others depended on nuanced readings of the full narrative the correct interpretation of a sentence for instance might depend on a set of facts described five paragraphs earlier after testing out a few automated approaches unsuccessfully we started to worry that we might spend more time cleaning up and fixing those results than we would actually save from the automation itself we did of course have another option to read each narrative and to enter all of the data by hand it sounded crazy at first it certainly did to me and it would be a huge undertaking but the more we thought about it the more it started to make sense we believed it would be more accurate which was great we also believe the process would be much more understandable to readers also great and the constraints of time and money or human resources were less problematic than in other projects and here's where the icij collaboration was incredibly helpful thanks to this network of reporters around the globe we were able to distribute the work across more than 80 people each reporter entered the data according to a central set of rules a process led by icij's editor data editor amelia diaz strook and my colleague john templum icij then built a custom django app to coordinate the verification of this manually entered data icij's team of experienced researchers used the django platform to review every entry three times making necessary adjustments along the way it was kind of like an in-house mechanical turk but with secret documents and highly skilled reviewers the data from the narratives became an essential resource for us and for our partners making it possible to search in a structured way for transactions that met only certain criteria this process provided another bonus by reading the documents very closely for the purposes of accurate data entry reporters got a much better sense of the documents themselves the data ended up being its own form of research and reporting so even with the very same set of documents these tables and these narratives and the same actual report we still needed two very different approaches to extract the data we needed computers where computers worked best and humans where humans worked better now i've been told that a theme of these conferences and of this summit is the concept of data teams so i thought it'd be appropriate to conclude on this topic at buzzfeed news data efforts are spread across the newsroom the small team i run is technically part of the investigations team but we also collaborate with reporters in every corner of the newsroom and buzzfeed news more generally is a very collaborative organization there are people who are savvy in working with data who work on other teams and likewise a breaking news team might collaborate with the science team and so on and our best work on the data team has come through collaborations with non-data reporters even so i approached the icij collaboration with a bit of hesitation it's only natural after all it takes a lot of trust to collaborate on a project the secretive and this sensitive over time however not only did we learn to trust icij very much but we came to treasure truly treasure our collaboration with icij's data team amy diaz strook augie armendaries and others it was an amazing and unique experience we collaborated on the data extraction and we were working from the same data sets but we did the analysis separately they had slightly different goals and constraints and they were working on a different set of stories but we'd also come together to compare findings to critique each other's work and to learn from each other what one person had seen in the data might be helpful to another person having that additional perspective that sort of parallel set of eyes looking at the same analyses the same set of documents was invaluable i don't know the next time i'll get to participate in another collaboration of this type of magnitude but the experience was enlightening and truly invigorating and has given me all sorts of thoughts about how to improve our own processes i look forward to watching the other speakers and sessions of this conference and learning more about how data teams and collaborators work in other fields it's something i'd like to learn a lot about i think journalism has a lot to learn from other fields and so thank you for your time and i wish you all a fruitful conference 