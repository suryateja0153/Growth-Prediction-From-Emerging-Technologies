 hello and welcome to the MIT SDM systems-thinking webinar series my name is Naomi Gutierrez Communications administrator for SDM and I am today's host thank you for taking the time to join us today we hope that this webinar can be a chance to recharge and learn something new today's speaker is douglas haig executive director of the school of data science at the University of North Carolina Charlotte he has more than 20 years of experience as an executive advisor to leaders in healthcare manufacturing financial services and the service industry in 2018 he was named by cranium global intelligence as one of the top 50 data and analytics professionals in the United States and Canada Doug is an alumnus of the system design and management program and holds a PhD in material science and engineering and a master's degree in metal science and engineering from Pennsylvania State University his talk today is titled beyond bias what happens after we know and disclose the biases in our AI models if you have questions for the presenter please enter them into the chat window at the side of the video on the YouTube screen they will be addressed during the Q&A portion of this session finally the recording of this presentation will be available online after today's session and a link will be sent to all registrants with that thank you Doug please take it away thank you all I appreciate the time today and just thank you Naomi for setting this up I appreciate it I am disappointed I'm not in Boston you know but the Kovan 19 virus has kept us all at home and thank you to zoom for having virtual backgrounds so you don't see my office so with that I will go ahead and get started and share my screen here screen this one and let's get going so now we said I have a system design in back and management background I was actually the third class when we called ourselves SVM 99 and no I missed again seeing the current students and everything up in Boston but I did work for bride Whitney's print Bank of America and the last last position in corporate corporate with chief analytics officer at Bank of America merchant services and then I came to academia at University of North Carolina Charlotte where we have founded in January at the school of data science so with that is very exciting and I'll just get going so thank you so bias and fairness and AI it's one of the hottest topics that exists today in in the field of data science and artificial intelligence you know this is not a new topic and by us you know as as in the 70s you know the banks were were actually the fair lending acts for credit approvals in the world about bias of who you were providing mortgages to what's called redlining that continues unfortunately some of that continues today if you google fines on mortgages for banks you will still find some of them being being done today and its really transformed the industry on how we look at the fairness and the bias within our modeling and within the artificial intelligence as modeling is involved about obviously the techniques have gotten much more important to us also you know some of the other things is not just financial services that have to worry about heart disease was historically always looked at with male patients there's a American Heart Association has done a great job with their women go red and looking at the research and building the data set to be able to say here's what happens women are very different so still you know as we get into the more recent news and what's really John that started this off and I'd like to have credit to join blow any at the MIT you know her master's thesis in the Media Lab really brought bias and fairness in the AI world to the general public if you don't know joy's work she founded the algorithmic Justice League it's fantastic it's really brought the gun yes okay I can do that sorry to screen share share better sorry about that I'm supposed to be an expert at this now okay we will continue on all right so apologize for that and we will continue going where's my screen you okay after all the practice this doesn't work in any way okay there we go so we'll continue going looks like I'm sharing now but joy screen and joins work really brought this to the general public and hers her recognition and her research was on the facial recognition and the gender and the skin color of people and how it really really made a difference of course there's other other fields criminal justice and recidivism human ROIs of human resources the resume screening you know Amazon and others found that as they were screening resumes they tended to hire them as they had in the past and that wasn't necessarily who they wanted with her first workforce of course we talked about healthcare services and understanding that and one thing I'll say is if you look at the Cova 19 research that's happening now and all the projections and work what you're beginning to see is there's a difference african-americans are having much higher rate a fatalities at least that's some of the initial initial work that's coming out we'll see there wasn't a lot of data collected but the basic bottom line is really understanding the bias in your models and how the how this is fair to the public and the people that are impacted by your models is just being amplified by artificial intelligence so out of this presentation I really want you to get two things first is bias is not the same a fairness sometimes they're conflated with each other but I'm going to talk about why they are not saying the other thing as you know this is a presentation of assistant design management program system thinking is really required when you're thinking about fairness so if I can if I can have you take those two things away then all of succeeded today but I'm going to back up a little bit and talk about some basic concepts and statistics that are important when you're thinking about bias and fairness within within artificial intelligence and data science so the first real concept is the confusion matrix some of you if you're statisticians I apologize if you're not maybe this will be helpful that's part of the conversations it's really understanding what you're predicting which is your predicted case and what whether it's actual the case or not so you have true positives you predicted it true it is true true negatives you predicted it not to be the case and it in fact isn't but then you have the false positives and false negatives and as you're looking about this as you're looking at your particular situation the cost of a true positive or true negative is very different and the mathematicians love this you know this is a very simplified case it can get highly complex but when you're actually measuring bias you know you can measure the accuracy you can measure the sensitivity which is just the true positive range of specificity the precision you know all of these different measures of bias within a confusion matrix or measurement of a model so what I'm going to talk about though is bias is a mathematical concept as soon as you put it in a social system or a system then bias has different viewpoints if I'm one stakeholder I may think that the you know the true negative rate is really the important one you know who did you miss that was really the case if you're in criminal justice right if you're if you're the mother or the the society out there you do not want people that were predicted to commit crimes released on the other case if you're the criminal or the person that was accused you don't want to you want to make sure that you're not kept in jail and when you wouldn't have committed a crime anyway so these true faza t'v and true negative is really a mathematical function but the bias is mathematical but when you go into society what I'm trying to convert convey here is bias is not the same as fairness you really have to take the context and look at all the stakeholders to be able to do this and to analyze this very well so fairness you know bias many of us are mathematicians you can you can actually calculate that you can say I'm as unbiased as I possibly can the other case when you look at the context that your model and your a is actually deployed into then you have to start worrying about fairness and it's really driven by the social construct and by the way social constructs are not the same around the world second concept that I would like to talk about is really what's called a ROC curve now this isn't always appropriate but in many cases this is and this is just true positives versus false positives so as you as you get as you're predicting you have a probability of that's going to be true and as you go down the probability you know you'll start to bring in false positives and you'll be in this area right here as you go down and the probability you're still picking up you know things that are actually true but you also start picking up things that are not so your true positive false positive break for many cases where you have a Barse data set you're going to pick a threshold of choosing what's true versus what's false in your prediction down in this area now different ways though the more you push this way obviously the more true positives to get without having any false positive so you want your ROC curve to be pushed into this corner unfortunately when you're modeling it's never perfect so then as you go you can see one model maybe here you know so this is not it good as a model the other model may be here for what the case I'm going to talk about now is really if this is your optimized model your artificial intelligence on your prediction ratio when you start to look at and I'm gonna bring a different concept in here it's the ability to look at subgroups I'll often called projected variables so whether it was your ethnicity what is your race what is your age different things depending upon your regulatory environment but if you start to split into groups so for example your model is here but if you choose people that are young okay your model is not going to be very good if you choose people that are old maybe your models look better or whatever the subcategory is okay unfortunately it doesn't it isn't always that simple if you look right at different subcategories different shapes of curves will come out no but really the case here and what I'm trying to get and and convey here is once you've built a model when you're looking at the subcategories or the protected variables or whatever you look at when you're looking at bias it is not going to be the same for each group and in fact there's some research that's been than been published that it's in practice it's actually you cannot make it you cannot make your bias the same for all your subgroups it's just mathematically impossible if you have a perfect data set with perfect samples and they're all the same then sure no or if you have to you might be able to pick that point right there and have the same threshold and your true true positive true false positive ratio would be the same but in general cases for all the models that are working in practice you're not going to be able to perfectly match the biases in your subgroups so you might have you know 75% ratio in your in your older people that you're able to predict true positive versus false positive but it might be 60 percent for for younger for younger population so again right bias is mathematically defined and you can balance the bias but actually balancing it perfectly as impossible but again the other case is even if you balance it fairness is not necessarily the same as bias so but now I'm just going to assume something you are all you've done this all you guys are great there is a lot of literature on how to remove bias from the modeling and you've done it congratulations I'm just going to make an assumption that you've done that it's not easy there's still a lot of new techniques being developed to be able to do this but you've done right you've taken your input data and you've bounced it for your various subgroups you've tuned the thresholds to make sure your your ratios are appropriate you've talked to people to make sure that the the bias is appropriate maybe you even build separate models for different for different subgroups there are many different techniques I'm not going to get into those here but I also say one of the oldest methods of doing this is just to remove the protective variables so you erase your ethnicity that's what we did in the financial industry you removed it your lawyers looked at it to make sure you weren't using any too protected variables that you could get in trouble for but what actually we found is your outcomes were so biased so what's happening is you have correlations and other variables in your model that are correlated unfortunately in the United States income is highly it's not is correlated with ethnicity so even if you're using any kind of income which is very important in financial services and many models right you're going to be starting to distribute your race and ethnicity variables within that and your income pick will pick that up so that's very adequate but there are methods now to try and actually trace this and get around it and I'm just going to say you're done you've done it congratulations you've taken your model you make it made it as unbiased as you possibly can and you're ready to pour it okay now the next thing what do you do is your complete you're done let's go however you're actually starting to deploy this into a larger system so let's talk about the system you're deploying it in so you've collecting your data you process it you make your prediction right you've got your model statistics your data is stable the accuracy is what you wanted out all your key performance indicators are there and your subcategory performance is as unbiased as you can possibly make it now you deploy it and you put it into a system or maybe you project it to a human that then actually works on this and you have an outcome so if you're measuring your bias here which is number one model prediction and your stakeholders are measuring here this interaction with the system and the human can actually rebuy us your model or come with a different outcome or a different expected outcome so that it's not fair in this case and point number two but a point number one you've done everything you can and you've explained everything it's as fair as possible so you have to actually look at the larger system now there's some literature being in the academic industry now I'd look at all the literature and really it's all about how you how you define the system and as SDM people you've spent two years a year whatever you've been there and understanding how to define a system so do you include the humans in the loop do you include the different systems that you're going up where are you measuring your accuracy of your system and your process versus the work in the actual model you know what cultural frameworks are you expecting other users and different impact and group and those are actually different sometimes the users of your model aren't the people that are actually impacted by the model decisions you know once you've decided and you've made it as unbiased as possible should you actually be transparent about you know you're deploying and talking it and showing that your model is imperfect how will the public react how will the stakeholders react do you have to manage regulators with this and then of course what happens over time models and as they as they perform right they will typically degrade over time so the framework that I like in this is actually myself at all on the framework for fair AI I'm going to talk about a few of the things and then modified their framework a little bit but we talked a little bit about framing right what are you where are you going to measure your system you measure your mop do you measure the model outcomes do you monitor the system outcomes where do you do that right oftentimes in industry when we did this all the time you're taking a model and you're applying it in a slightly different situation than most developed so as you work through this you know the portability of your model is actually an efficient way to redeploy models if you're in you know modeling medium businesses so medium-sized businesses maybe a hundred million to two hundred million dollar sized businesses and understanding how they react can you take that model and apply it to a 50 million dollar business maybe maybe not right but you have to actually assume and look at that to understand it of course we talked about the formulism which is bias is not equal to fairness and then I'm going to throw in an extra one which is really trust the privacy of the data the transparency of your system are you building trust with the general public and I would say joists work that I talked about before cause people to sit back and and not trust AI as much as they had in the past so as a group of developers of practitioners within artificial intelligence and data science we have to build the trust and show that we are actually doing the best we can with this and then there's some there are some trades that are very difficult and depending upon the point of view of the stakeholder it's it's a tough tough discussion but let's go on and talk a little bit more about framing so I'm going to give you a use case and this is autonomous credit positioning all right so you have your credit bureaus you bring in more data that you have inside your company you're blending it all together and you decide you know I'm gonna offer you credit or not right and so you've measured that I've said okay and all the outcomes people take credit they don't take the credit you know is this fair I've rebalanced my variables to make sure I'm not biased against one ethnic group versus another and I'm saying okay my offer for credit is out so this is purely a fully autonomous system that you've deployed in however there's a human either accepting the offer or not all right so then you have the outcome right I accepted the offer I didn't accept the offer I took the loan I'd defaulted on and I didn't default on it all these decisions feed back in now when you start to think about that right it was very efficient I gave the offer in seconds somehow I just got fine four bodies right somebody went and did an analysis and I just got a fifty million dollar fine because my mortgage applications were biased and I wasn't actually people were not taking it so what happened well humans were involved right so even though it seemed like it was in the autonomous credit decision there were humans right you have to do what's called reject inference which is a statistical a way of measuring those that didn't accept the offer would they have actually did they get alone somewhere else and do big default or not right and you have to realize there's a lot of differential information you have a lot of information as the model developer but you're people that are deciding whether to accept an offer or not or not all on the same level of information and different groups of people have different preferences and different amounts of information and there's also you know I'm assuming right now that you've made your your debt offering into a culturally acceptable way of doing it so liquidity offering and are you doing different things under you well Vanessa but even then different cultures are willing you're unwilling to to take or like to use cash or debt and so they they have this bias one way or another within their own own culture so that will actually impact your outcome and then finally time is unstoppable so if you had a credit offer that was out there today right or out there let's just say it was March first and you had a credit offer out there and you were offering credit and you continued to offer credit in the same model all the way come today I will tell you it is different there are a lot more people accepting your offers of credit than there were in the past there are your model is probably not going to be as sufficient you may have been using data that was six weeks old about their income their income is gone so time is actually a very critical important in the concept of modeling right economic conditions change the Cova 19 is a very important or good example of this I mean talking to other companies about looking at their models what inventory of models do you have hopefully your company has an inventory of models and if they don't you better get one because you need to know which ones are broke and which ones are not right and then the other thing even in normal times individual and group behaviors they migrate they change you know fads common fads go but overall models tend to degrade over time so there's a concept that I like to talk to people about and it's really model risk management it's where you're managing all your models in your company and are they important right which ones are the most risky which ones provide a lot of risk to your company and which ones you know if it doesn't happen and it can continue to run but it really doesn't cost you a lot in the end other than potentially mining your model that's okay financial crisis of 2008 really drove this practice into the financial institutions one of the things I think we need to do and I think kovat will we'll do this is is understanding in other industries how model risk management really works and can bring benefit to the overall corporation about knowing where your models are monitoring to be behavior of them you know I was in a company one time where we had deployed a model from 1997 and this happened to be 2015 so when we looked at the accuracy of the model let's just say it wasn't as accurate as we had thought it was so as we you look at this and understanding the model risk management best practices that are coming out of the financial industry applying those into your companies and your situations that you're in it's an important important aspect of this I'll leave that for another top another day but moderate risk management there are some best practices that can say okay so that's not what what is normally considered autonomous credit decisions so let's change a little bit and let's go into what's now called human in the lips many of you have heard of you know the alphago right and all the in chess playing with with computers are actually better when you have humans making the ultimate decision and so three three-peat three humans and three chests you know this is the story I've heard three humans and three amateur chess players and three compute that are three different models are building it can be can be you know a master grand champion in in chess why is that right so humans can actually improve the model capability by using their judgment they can take into exogenous variables you know they can look at the dynamics of the system that may be changing that you're not picking up for your model and if you go back to the ROC curve concept right in general where the computer is very very positive that they're right you should just let the computer make the decision when you know the computer is pretty sure that you're wrong you let you let again you let the computer do this because you don't want to send this to humans because humans are a more expensive way to do this but there's this group in the middle you know an example of this is in the Frog world in financial institutions right there's some kind of types of fraud that you can catch very easily and you just stop there sometimes where you get a phone call on your credit card that says hey is this you or a text message you know they're trying to check to determine if it's really you or not and so the human or someone in the background is looking at it and being able to do that so what happened so when you bring humans back in well as we talked about before you know mental models and humans often include very powerful in what's called information value in in statistics the information value of some of these variables is very powerful but they're also a biased variable so unguent you know both the unconscious bias can creep back in because the mental models of humans are generally biased so you have to be careful and again you want to measure before and after the human decisions even if you think it's a fully autonomous decision right you need to look and see where this is is the case okay so now let's move on to the next concept about portability and it drives efficiency but I would say a risk fairness right so as you're thinking about portability right you're taking the same model the same model and technique same capability and applying it in a slightly different area you haven't necessarily gone through all the development maybe you didn't have all the data so you're applying into that situation generally the adjacent Jason sees right so their areas just outside the data that you use during development right they're also areas that are outside of your assumptions which sometimes your assumptions are implicit assumptions sometimes they're specific so understanding where you're deploying it and how the model was built so you understand do you think it's appropriate to deploy that in these adjacent areas right sometimes people adopt models from the literature right sometimes you apply it to a geographic area got this model in the United States or I've built it in North Carolina right and I think it's a great model and now I'm going to apply it in Europe right and that may or may not actually work well you can also cross client segments so if I have a one segment in my in my population you know let's just say it's the Millennials and maybe is that millennial segment can I apply it to the the Gen Xers and actually try and understand how you're doing this is actually a fairly common practice within within model teams that are at model development teams that are out in the world but you've got to actually be very careful with this because as you're going to the adjacent teams you've tuned now that you had tuned your model to be as fair as possible with the data you develop it on when you move outside of those there's different economic conditions knowing your behavior of those different to Gen Xers and Millennials actually behave the same sometimes they do sometimes they don't there's also different cultural norms if you deploy in Europe or Asia or Africa it's very different than if you're going in the United States and you have different regulatory environments so in some cases some of the variables that are very powerful but are also protected in some countries you can use those in others do you want to do that you want to risk the fairness and then of course with Kelvin 19 there's a lot of models that are broken so in emergency situations there's a lot of people that are rushed right you're doing this as quick as possible you're deploying your model you're developing it and you're looking at things so let's just talk about the Cova 19 peak time you know peak areas it's continually improving as times to win that's going to peak with a number of infections or the number of deaths but if you listen to the news those models are moving everyday what's happening is there's better data coming in on the backside but then you know within the last week for two weeks there's a lot of people starting to question the disparate impacts due to ethnicity right some of the news I heard last night whether I haven't confirmed this so I don't know if this is true or not yet but the disparity in the african-american community they're dying at much higher rates now the doctors say well that's because they have a lot of conditions that cause it to be worse is that the case is it not the case time will tell but in an emergency situation where we're making these decisions right now right you've got to you've got to make the decision you've got to move on you've got to consider and make this as fair as you possibly can so is there any bias and these models are not that will be determined but we shall see okay moving on to the formalism piece right we talked a lot about this already social systems are people focused right when you deploy into a society there's of course many ways we've talked about measuring bias which you you know different groups will have different rates and if they're mathematically inclined they may they may know they're choosing to different rates but sometimes it's not even a mathematical argument it's a political argument it's a it's an argument of passion or a viewpoint that may not even be mathematical and can't be mapped to a particular mathematical bias you have to take those views into account with the formalism and you're going to have to try and balance them the best you can and of course as I stated before it's matter it's mathematically intractable to actually balance it perfectly so what what do you do how do you do those trade-offs and if you're a corporation right you're trying to optimize your cash flow your profitability you know you may say hey you know that's a very small group and we know it's biased against it but we're just gonna say let's let's go ahead and deploy there's actually a lot of case studies and I'll bring this up there's a list of references in the end that will make this available to you there's some Princeton case studies on bias and fairness in AI situations they're very good and makes you think about the decisions but it's very hard to make these trade-offs when you have very different viewpoints so if you've done this now how do you actually make all these decisions so now what okay well one of the things is we have to generate trust right as a group we've damaged the trust over the last few years for AI and data science and we we've got to work very hard to balance this right part of that is transparency you know I've gotten the bottom that the European Union's I think's guidelines for trustworthy AI you know so actually a very very interesting document to read they talk about transparency they use the right of privacy and it goes through many of these things so it's a it's a guideline for trust for the AI so it's a legalese or a regulatory document but it is actually an interesting document to read so you may want to take a look at that when you get a chance but transparency so you've got to trade-off you know how much are you going to disclose to the public right and ensure some awareness that you're actually being truthful and explainable and and aren't understanding the biases in your system right but you also need confidentiality and you want your competitors to not be able to replicate everything you've done by being too transparent so there's always a trade-off here and then there's of course a lot of work about explain ability so explained ability especially in neural networks you know how do you how do you how do you tell someone why you made the decision there's two concept or a concept in here in modeling of black box models versus white box models the black box models data goes in and answer comes out you don't really know why deep learning and AI a lot of the neural networks have been that way in the past as black boxes but there's a lot of work going on right now it's very interesting about being able to explain why these neural networks are actually making the decision and being able to trace here's where the data came from to be able to make this so explain ability and traceability is something that you know when people ask if you're not necessarily going to disclose everything affront when people ask why did the decision get made you know this is one of those areas that you need to be able to focus on and do of course privacy is a hot topic right right do people know you're actually using an artificial intelligence model there's something in the guidelines or in the regulatory environment that's coming out that's called the right to be forgotten so can you take your data out of the system if you say hey I don't want my data to be used can you as a modeler actually take your data out and retrain your models and when do you need to do that and how do you do that these are not easy issues you know especially after you've anonymized a lot of the data can you how do you take how do you take somebody's request to remove their data out of an anonymized data set those are those solutions or those are the questions many people are asking now of course there is ownership issues who owns the day I do you own your health care data do you own your credit card data do you own any of the data or do the companies that are processing the data own and how what rights they have you know PII personally identifiable information and protected personal information PPI are two of the concepts and of course the anonymization and re-engineering or bet or rien on anonymizing things of things that you know are those ethical or not so a lot of different thinking through this of course there's other things human agency robustness I love the robustness talk but I'm not going to go into that today making your models from us to two different conditions but concepts and trusts you know we really have to to work on this overall as we are moving through to be able to say you know society can trust artificial intelligence data science the modeling that's being done you know because otherwise they'll go reject the ability to use this and regulate you know and put regulations in place and make it difficult to actually use artificial intelligence so we really need to work hard on this and the trust and the transparency and privacy are all part of this okay so let's think about the system again you know as you're optimizing fairness you know we talked about balancing the risks and the rewards and each set of stakeholders is it's going to have a different view so optimizing fairness is that an oxymoron it may not be possible to optimize fairness but to be able to understand the fairness and understand the viewpoints that you're putting out you know you not only have to check the bias which is mathematically you know you can define that you can define it in many different ways there was a time there that all the academics were going through and defining it in a new way and saying hey I've got a new way to define bias in fairness and here's how you should do it here are the groups that would be looking at this way but now we're really talking about fairness and have you framed the system so are you looking at the system and done your system thinking to be able to say I'm not just looking at the modeling but I'm looking at the system and is the system fair not just a model fair but it's the system fair to our stakeholders and users you know and as you deploy as you build in one area you know pilot or proof of concept in one area then deploy and others are you being culturally sensitive in this and then is it brought and and as that is broadly defined you know are the geographies geographies is one of those things that you have to work on is very common to deploy in new geographies and roll out in one state or one city and then keep rolling it out further and fertile there and you're using the same models because you were successful is that okay you should check you know the other thing is as I talked about model risk management are you monitoring this and is it changing over time it will change I can tell you and then the other thing is that are you discovering people that are deploying your models elsewhere that necessary work we're thinking about the use case and as you as you were working in it today to last and this is one of the most difficult ones is how much transparency is appropriate right is it necessary right how are you doing this where do you where do you actually disclose disclose how your that you have a model and how it works and how it may or may not have a little bit of bias left in it is it necessary to do it do you do it in a bunch of legalese like most of us click through or is there other ways where you're talking about it and and working through this because transparency is is key to being able to build a trust have the general public trust are our capabilities in artificial intelligence ok so there are other some interesting topics and fair III that I did not talk about you know there is actually something called AI for social good and a lot of times you're intentionally biasing your models in the use case that I think about most often when I think about this it's really the social determinants of health so in the healthcare industry if you're not familiar there's something called the social determinants of health you know your doctors decisions and recommendations you know are 40 50 60 % of actually getting you healthy again and then the system around you and your your actual situation your social situation right you live near a grocery store where you can get healthy food right what is your income level how how much can you afford to do can you get transportation to see a doctor all these are called social determinants of health they're also very very ethnically very age driven so there is actually there using those variables for a social good now in our society that's actually acceptable so they're intentionally biasing their models and their outcomes to particular groups so that it that it serves a social good we talked about model risk management not going to talk a lot more but it's a very deep subject and I do believe that we should be deploying those in more situations across all industries versus just the financial services industry the methods for removing the bias and your models I talked real quickly about how you do that there are hundreds of papers and very good researchers and data scientists and practitioners that are working on methods to do this right you've got to look at your data you've got to look at your algorithms you've got to look at the situation but technology that you have there's a process flow in building a model they're looking at every little step of where we're fairness and bias creeps in but I'll leave that for another day of course the specific cultural implications and social expectations we didn't talk a lot about those but as you think about the different religions as you think about the different ethnicities as you think about different age groups there's a lot of different cultural implications the Eastern Eastern philosophy and moral codes versus the Western moral decisions and basis you know I talked in the end what I'm talking about by some fairness and different and different presentations I talk a lot about how that links to utilitarianism and you know philosophy discussions that I haven't had since I was an undergraduate but as you think about the fairness a lot of this comes back to the value system of the society that it's in I'm not going to talk a lot about that and all the differences in that around the world but the moral basis is a lot of the conditioning for us okay so I'm going to come back to the beginning right and say bias is not the same as thickness so hopefully I have convinced you of that right bias in my view is a mathematical term that can be defined very rigorously by our mathematicians that our model develop person you can say with no uncertainty this is as fair as I can get it or as unbiased as I can get it and then that works great but it's not the same as fairness again fairness is a societal concept it really depends upon the views of the stakeholders there's a lot of different use cases that I didn't talk about that are out there but I also wanted to say you know as system thinkers right that's really required for us to really operate in this world thinking about how the system interacts and works and then what happens when you put a model into a system how that system reacts where you're measuring where you're measuring your key variables are is one of those critical things okay so I've gone about 40 41 minutes I'm gonna say here's some references and there's a bunch of links in here so we will post this Naomi will post this afterwards so that you can see some of these there's actually hundreds more there's some very good research going on at MIT into the fairness process and the biased process but here's some of the things I actually talked about today and with that I'm gonna say thank you if you want to get a hold of me afterwards here's the email addresses that you can get ahold of me and and Naomi all I'll stop and say thank you very much and take questions absolutely thank you so much for that lecture I think it was really fascinating again to our viewers you are welcome to enter questions into the chat window on YouTube while we wait for some of those to come in I just wanted to say one thing that I found really interesting in this talk was kind of looking at the system that one would have when creating these models and adding in more information one thing that we go over a lot in the SDM core class is figuring out who your stakeholders are and I think it has probably been easy for the data scientists the you know very tech oriented people to create models and think of their stakeholders really as the people inside their organization without thinking about all of the stakeholders as the users for instance in the financial system all of the people who would be applying for credit and the various ways in which they may or may not have access the ways in which they may have faced barriers in the past so I think it's definitely important to frame it in that way yes you know that's one of those things that as I went through the SDM understanding the stakeholders and who it is and taking a holistic viewpoint is one of those things that you have to learn and you're right a lot of the the modelers are so worried about the data you know and what data they have and did I make too great the perfect model that they haven't necessarily stepped back and looked at the holistic view of all the people that will be using it and the people that are impacted by the model and that's a very very good point now I mean the fact that you're looking at the stakeholders and that's really the difference as I think about it as the modelers look at bias and optimizing and working very hard to do that where you're deploying in the system you really need to think about fairness and that's that's one of those key things at the point where a bias is not equal fairness and having that having that thought process and that perspective is critically important definitely so we had one question come in from possibly a classmate of yours Sylvie Bach Sean who asks we always had models what makes them now AI models thanks Sylvie and good to hear your name yes so V and I were SDM 99 together so always great to hear from one of your former cohort so yeah the AI models and this is this is actually very interesting to think about the the term artificial intelligence the term machine learning you know on the the statistician and the financial services people will say I've been doing that forever one of the things that but at least my own personal definition and people can discuss this and we can have a long argument like we used to talk about this definition of the system for me artificial intelligence is starting to tie itself more to the deep learning and the neural network based type of applications machine learning is is more of the clustering and and other things so which which Venn diagram view bill but you're right we've had models all the concepts that I talked about today really apply to all models it's not just artificial intelligence if you have if you think of artificial intelligences as deep learning and different types of neural networks it does include all the machine learning wants and it includes just basically in their progression you need to think through all of that you know that's why I kind of use the words interchangeably today data science artificial intelligence machine learning and modeling we have to have a beverage and a table and and and be at the thirsty or the muddy river to really go in deep on that but you're right modeling has been around for a long time what's happening is modeling is now becoming ubiquitous it's everywhere it's you know every every app on your phone every time you look at a stoplight and it's doing on or turning off and your car everywhere there's so much of modeling now that it's so ubiquitous that it's becoming much more important the very thorough answer I personally appreciate it while we wait for some more questions to come in Silvie says thank you Doug that helps another thing that I found interesting was when you were talking about optimizing fairness and discussing transparency you spoke very briefly about the disclosure of models and how they work and I'm curious about how you how you personally feel about how to discuss those models how to discuss what's going on as far as you know using legalese and now that you're in kind of more of the academic world where people also often have difficulty communicating their thoughts clearly and concisely if you have thoughts on approaches to that yeah how you disclose transparency is not an easy the easy thing to discuss you know from a corporate perspective you know we typically didn't we didn't even talk about our models publicly right we may have acknowledged that there is a model there but that's it we didn't talk about how accurate it was how how you know biased or unbiased it is how you just close its you're biased and you're immediately gonna get sued right so how and when you do that it's a very tricky and very difficult decision to make and how you do that the issue is if you don't disclose anything and then you do have a bias model then it's going to be found out eventually and people will make headline news so you have reputational risks in there if you're not talking about your model so one of the things that I think actually works well is corporate people actually interacting with academia and talking about bias and fairness and in the academic environment which is generally a little bit more safe although some people may disagree with me on that and and working through that you are you going to discuss a particular model probably not but we talk about models in general and how you're adapting to it and how you're convincing the public that it's actually you're you're actually balancing their viewpoints inside and when you get asked this is where it's ability comes in when you could ask why did you make this decision in this way you actually have an answer you know that is one of those things the explainable AI is actually a technique that's coming along now that actually is important so that when when you get asked why did I get declined or why did I you make this decision you can actually say I made it for the following reasons this is the data I had and doing that and disclosing the data the regulator's are starting to get more aggressive or more specific maybe I should stay versus aggressive about how they want you to be able to disclose and what you have to be able to disclose so it's it's not an easy topic I think the academic corporate interactions is one of those areas that would be good to discuss in further detail but it is not easy right and how biased a model is through your lawyers you're gonna say don't don't disclose your biases even if even if it's you've done everything you can to make it as good as possible so if they're tough to cases and actually that's where those Princeton case studies that I referenced helped me think through you know different situations that you know are not easy and and different and actually it's interesting is like talk face studies and classes different cultures have different viewpoints and as you're thinking about you know deploying things and getting out in the business it's important to if you're a start-up in particular I'm going to deploy it and then we'll see and we'll fix what breaks and we'll go and we'll we'll continue to adapt and and that's one method of doing it another is to be very conscious and those large corporations tend to look at things in more details smaller corporations need to get out and get busy and break things and and you know work through but that's really two different cultural and moral basis as for their decisions definitely worthy of its own entire lecture honestly yes we had another question come in from Tom Courtney who asks should epochs in a I become an SDM topic and I'm curious also how you feel or how the school of data science at UNC Charlotte is handling ethics and AI know as for the second one first so the school of data science at UNC Charlotte we are actually integrating the ethical decisions in the development of our courses there's really two choices schools are making do I have an ethics class right so I can take a three-hour credit course and say this is a ethics and we can have a faculty member teach a ifx as you're doing it that's one choice or you can actually take it into integrated into your curriculum you know pedagogically very different decisions we've chosen what I think is actually the harder path is to integrate it into our core course development so we actually have a professor of philosophy and that's specializes in data ethics that's sitting with our computing and our our faculty to make sure that the ethical implications of the class projects and the discussions are actually built in so even in the freshman level courses we're putting we're actually giving data that's actually biased so that the student as they're working through and then you say hey did you check for you know ethnicity and was that did you get different results in your modeling you know like oh they're actually different so they're trying to learn and understand how bias and then the concepts of fairness come in we're also teaching a lot of our data science in the concept of a social science so sociology political science you know criminal justice and using these examples that are in the literature and we're debating as a community in in the classroom so that they can actually bring it closer to to what they are observing everyday now I'll switch to is should it be taught in SDM SDM the curriculum was migrating when we were we they're ever-evolving as it should depends on how much artificial intelligence your your teaching I think it would be a great topic obviously I'm a biased person about artificial intelligence and doing it should at least you know be an option for system thinkers absolutely maybe as an elective or a special topics class would be great but a system system design and management covers a very broad area and it's always difficult to say this specific topic if it's not not actually core that everyone should be taught you know be there but having an elective probably good idea yes it is definitely a very broad curriculum and the core has changed drastically from when you were a student we now have a single integrated core class that runs for the entire school year and then students do actually have the freedom to take electives across the Sloan School of Management and the entire School of Engineering I don't actually know how that's going in to interact with the newly announced School of Computing I know that this is only from a lay person's perspective and not part of the administration for them but I know that ethics has been a very hot topic in the creation of the curriculum there in the staffing of the professor's who are going to work there and the faculty as well so it would not surprise me if that became more of a significant subject with an SDM and certainly the students who come to SDM come with varying interests of their own when we have more people coming in who are interested in computer science and an AI or Illume do you often have the opportunity or the interest in studying topics like ethics and fairness no in the Schwartzman scroll right the Schwartzman School of Computing is going to be a leader in this right it is one of those places will where we will look to and and and see what they're doing and adopt practices and hopefully you know as we're leading through integrating this into our curriculum you know hopefully we can share that with the world and people can pick and choose what they use the Schwartzman school is it's fantastic right I wish I had a billion dollars to use to build a new school but unfortunately here at UNC Charlotte don't quite have that much yes I think it's going to be really interesting how that school involves as part of MIT and as part of the structure of the university and how our students are able to interact with it since currently computer science is part of the School of Engineering so we'll see how it goes I honestly do not have enough information to say at this point that's it's we are all all academic worlds and data science are evolving you know we've chosen to go have a school of data science that actually sits in for colleges you know MIT is doing a slightly different Berkeley's doing a different UVA has a school of data science to a world we're all experimenting and collaborating with each other as to how how to best do this for our students and for society absolutely and I think the only way to deal with it is to continue to be flexible and to adapt absolutely doesn't look like we have more questions coming in right now so I think we can wrap up a couple of minutes early so I will say again thank you so much Doug for joining us today for sharing your time with us I hope we can have you back in the future hopefully on campus and again thank you to all of our viewers for joining us thank you for attending the presentation recording will be available online after this session and we should be able to get the slides and those links to the references up very soon as well we hope you will join us for the next system thinking webinar in May and on behalf of the system design and management program thank you for joining us thank you everyone 