 one we are so excited to have you join us virtually for this build session on responsible ml getting started and analyzing your models my name is Marissa McKee and I'll be your session moderator I like to highlight that there is a dedicated area for you to ask your question and there's a team of my colleagues there answering all of those questions that will help us make this session completely interactive and I'll make sure to bring some of those questions and ask them to the presenter towards the Q&A portion of this talk without further ado let me invite Sarah Byrd founder of responsible AI area in Azure AI to walk us through the content Sarah the floor is all yours great good evening everyone at least it's an evening here in New York thank you for joining us for this special festive build live session we're going to be talking about some of our responsible machine learning capabilities and in particular how you get started and analyze your models so a lot of times when we think about responsible AI people immediately think about some of the the longer-term challenges or some of the things we've seen in you know in fiction however it turns out many organizations are struggling right now with challenges when they try to deploy AI responsibly and so a lot of the tools and technology is that we're going to show you are very early they're still being actively developed in the research community however because people are struggling today we want it to give them the state-of-the-art tools in their hands immediately regardless of where the state of the art is in order to achieve this we have a range of tools and technologies that we've been developing in this session we're talking about our first pillar which is tools we have that help you analyze your models and get a deeper understanding of how the model makes decisions how well it works for one group of people or another group of people and questions like we also have additional capabilities that you can check out on demand or in some of our build recordings that are about protecting people's privacy and data and we have additional ones that are features we're adding to the Azure machine learning platform to enable you to control your end-to-end machine learning process and the resulting systems so as I mentioned a lot of these capabilities are new and are still being actively developed and so we wanted to be able to to develop them in a way where we could respond rapidly to great new ideas and new innovations and so we're developing a lot of the core capabilities in the open source where we can work directly with the researchers that are creating them people who want to expand them to new applications can directly submit changes or expand the toolkit to work for their applications however machine learning especially when you're taking into production is often better supported with a platform that can help you think about that end-to-end lifecycle and so we take our open source capabilities as they mature and we integrate them in Azure machine learning so that people can access them at the right points in the machine learning lifecycle they can easily discover them and leverage the capabilities to maximum effect in their process so let's get on to the topic which is how do we really understand our models and the the effects that they can have so in this session I'm going to be showing you a live demo in fact it's running in the background right now where we're going to focus on a lone scenario we are creating a model that is going to look at loan applications and decide which loans to accept and which loans to reject in order to analyze this model and decide if we want to use it we're going to use a combination of tools we're going to use an agile machine learning is our base platform and then we're going to use it in combination with fair learn to help us answer the question of is my model fair how well does it work for men how well does it work for women then we're going to use interpret ml plus as she learning to dive into that model deeper and understand why is it making the decisions it's making how does it decide which applications to accept and which applications to reject so let's go to the first topic which is fairness fairness is a significant topic in AI development it's one of the the principles we have for responsible AI at Microsoft and there's several different things you need to think consider and but there's many different ways that an AI system can behave unfairly in the case of fair learn the tool we're going to be using there's two types of fairness harms that we consider the first one is what we call quality of service harms where the product just doesn't work as well for one group of people that another for example voice recognition system might not work as well for male voices as it does for female voices or you can have harms of allocation where I'm using the model two to make it a decision a decision that could affect people's lives like whether or not they receive a loan and I don't want that model to reject or accept people based on attributes that shouldn't be relevant in the loan decision and so we need to analyze our models and understand if we're seeing any of these behaviors that could lead to negative outcomes for people so as I mentioned earlier the toolkit we have for this is fair learn it's an open-source toolkit which is designed to help you assess any potential unfairness in your model and then allow you to use state-of-the-art mitigation techniques to address that unfairness if that's the right thing for you or a particular application and I'm very excited to announce that I will be demoing today and soon Ferrothorn will also be integrated in Azure machine learning so you can access the visualizations in the dashboard directly in the Azure machine learning studio so with that I'm going to jump over get started on the demo so my demo is still running here so we're gonna get started at the top of the notebook well we let this run as I mentioned this is a loan scenario I'm using the census data to build a model which I'm going to use to decide whether or not to accept a loan application so in the top of my notebook here I'm just loading my data and pre-processing it getting it ready for model training the second thing I'm going to do then is all of this is still just pre processing the data we're splitting it into training and test sets and then I'm going to use scikit-learn here to train a predictor I'm not doing anything special in this case to consider fairness I'm just using my normal modeling techniques here but now what I can do is now that I have this model I can take fair learn and I can use it to assess how well it's working across different sensitive attributes so I have the the dashboard here in the notebook which we can use to understand the the model fairness so let's let's actually jump back over here and talk more about fairness assessment so now that I have that model I want to assess the the fairness of the model and so I'm going to make a couple decisions first I need to select which sensitive attributes I'm interested in understanding the differences between for example that in women or by age or any other sensitive attributes that are relevant to my particular application the second thing I need to do is I need to select the performance metric that's the the right one for my application so in this case we're looking at accuracy but in another problem you might care about precision or recall after that Farallones going to the predictions and the real ground truth ice and help you understand the difference in the performance of the model is it more accurate for men is it more accurate for women as well as the difference in the outcome is it making different decisions for for men and women so let's go back over here and because my notebooks still running the interactive dashboard is is not going to to work in the notebook and so I'm gonna jump over and I'm going to show you this this feature that's coming soon which is actually the the integrated portions in Azure machine learning and we're actually going to demo it over here so here's the model I've been working on and I've run it several times so let's click in and we can go to our last run here and what I have is you know all of the information about the model and my run and I have a fairness tab where now I can go and actually look at that interactive dashboard and explore the fairness of my model and so I have two different attributes in this particular dataset we have race which has five values and we have sex which has two unique values so I'm going to click on sex here and then for my problem right we want to see you know how we want to measure performance and then as I mentioned earlier in this case we're gonna look at accuracy so I'm gonna click through here and I'm gonna come back to what this this particular tab means but this is my original model so let's click on that and let's understand the results here so here's here's what we see which is the difference in performance of the model and so in this case my model is about 92% accurate for women and about 80% accurate for men which is leading to about an 84 percent accuracy overall there's more under prediction than over prediction and so now I can just kind of get a feel for that that question of how well is the model working for each person however since this is an allocation problem we're actually deciding whether or not someone gets a loan I also really want to look at does the model make different predictions for men and women and in this case what I'm seeing is overall I'm offering loans to about 19% of the people that apply however I'm only offering loans to 7% of the women that apply but 25% of the men that apply now the challenge here is I don't actually know if this is a problem this might not be unfairness it might be that the women that are applying are very different distributions and the men that are applying for example and so all I noticed that there's a difference in my model and this is a sign that I'm gonna want to go and investigate further and understand why do I have this difference and is that reasonable before I decide to move forward with this model so with that let's go back over to the back over here to the presentation and let's talk about how can we better understand why this model is making different decisions for men and women and the best class of techniques we have for that is interpretability so interpretability is a actually a collection of techniques that help you interpret or explaining the behavior of models so I don't get a view an example of why this is important and why you might want to do this so we have a paper on this from Microsoft Research but we were creating a model to help understand which patients that have pneumonia are high risk so that you could for example give them in extra care to help improve their outcomes however we were training this on real data and using interpretability techniques we learned that the model had learned that if a person has asthma their lower risk and if you talk to doctors this doesn't reflect reality at all in fact patients with asthma and pneumonia are are considered high risk however doctors already know that and so in the the real date said they were potentially giving these patient that that extra care and the patients were having good outcomes and so the model had learned something that was actually what was in the data but isn't really what we want the model to learn if we're going to use this to help inform doctors and so explanations can help you uncover model behaviors like this that they would be undesirable if you use the model so we have a collection of all of these together in an open source actually family of libraries called interpret ml where we provide both black box and glass box capabilities and I'm going to talk more about what that means in a second we also have new capabilities for text interpret ability so you can explain your text data which is brand new my field this year as well as a counterfactual explanation so you can answer questions like what would I need to change to be accepted for this loan so there's a lot of great things here and let's talk about glass box and black box models as two of the key types of interpretability so glass box models are models that are designed to be inherently interpretable it means they have a structure that we can actually inspect and use that to understand how they're making decisions so for example think of decision trees or linear models however there are more sophisticated more accurate models available for example explainable boosting machines for Microsoft Research which we have an interpreter mil and the great thing about glass box models is the explanations are lossless you can completely understand how it's making decisions however in some cases it might be that that model format doesn't work well for your problem and/or you already have an existing model for example that you want to generate explanations for and so in that case you can use black box explanations which we can use to explain any particular system and the way that we do this is we perturb the inputs of the model and observe the behavior which allows us to explain the behavior however the explanations are approximation so we don't know perfectly if they are going to be accurate for our particular problem however in many cases they do work very well so with that we're gonna go back over to our demo and let's see where we are in terms of the notebook running so I can so we also could have as I showed you I showed you the dashboard in the studio but you can see the same thing here in your notebook if you prefer to directly work in the notebook so the next step what's been running in the background here is I had taken that model and now I used interpret ml built into Azure machine learning to generate model explanations so I've generated global explanations which are the explanations of the overall model behavior and then I've also generated local explanations so explaining why a prediction was made for any given individual data point so I have generated all of those explanations here and then now I'm going to actually visualize that and I can open my visualization in a new tab here so we can use this now to explore where we are and better understand why am i seeing that difference for men and women and so this dashboard is brand new we're demoing it for the first time here at build and it will be released in the open source and in Azure machine learning soon we designed it by working with Microsoft researchers and practicing data scientists to really try to make explanations as understandable and as usable as possible so in this case let's go here and I want to look at men and women so we have this cohort capability that helps you slice your data into different dimensions so you can do comparisons for example you could do training and testing if you wanted in this case though since I wanted to do this fairness analysis I'm gonna look at women so I'm going to click on sex I'm going to change this to categorical sex is represented by zero and my ribbon is represented by zero in my data set so I'm going to add a filter for that and I'm gonna save it and now I'm going to add a second cohort which is men so I'm gonna add this here and we're gonna go to the sex we will change it to one at our filter in NC and so now what we're seeing is actually very similar to what we saw in the fara learned dashboard but with more information which is that the model is less accurate for men and about 92% accurate for women we can also see the more details like the precision and the recall and the false positive and the false negative rates however what I want to show you here is how we can also use explanations to go a step further and better understand our models so I can click on the aggregate feature importance here so these are the global explanations where we're gonna look across all of the individual decisions how does the model overall make decisions and I have women here in blue and and men here in orange because I set those up as my two cohorts and so marital status is a significant predictor for both however what immediately jumps out here is the second column which is sex which is apparently a significant factor in predictions for women but not predictions for men and I can click on it here and learn more so now I can actually see that the future importance of the individual data points and what I see is that for all women the the value of their sex being zero is in fact contributing to the prediction to reject them so that seems like a significant issue in the model and I can then actually go and if I want I can dive deeper here and I can perturb individual data points if I just want to get a feel for how this is working and how the features effect so I can click on any one data point here and in this case you can see sexy is the the number one predictor for this person and they're currently rejected with a probability of 68 percent if I want that I can do a what-if type analysis here and I can change it to two men and now I have a new data point and as you can see in this case we actually flipped the prediction so by changing the person's sex now they have been approved for the loan so that that seems like a pretty significant issue in this model so I'm next gonna move forward and show you how to mitigate it but you can also for example click on multiple data points and see how it varies across them if you want to explore more than than any one data point so with that let's talk about what do we do now that it looks like there's an issue in our model so in fear learn we have mitigation techniques that allow you to sort of directly improve the the fairness criteria of your model and so the first thing you need to think about is what's the the right fairness criteria for your problem this is a sort of famously published issue where there's many different ones that you can choose the right ones gonna depend on your particular application in your industry a lot of factors and so we make a variety of them available in fair loans so then you can use the one that's right for you and then the second thing you need to decide is what type of mitigation algorithm do you want to use it might be that you're not able to retrain the model and so you want to use a post processing method where for example I could calibrate the model and and change it to a different threshold per group or alternatively if you have the option for example to retrain the model then we have reduction methods available where we can actually change factors and retrain the model and and try to improve that fairness criteria directly in the model training so I'm gonna go and demonstrate that so let's go back to our notebook and see you where we are here so what I've done now is I've used our grid search approach here which is a reduction method been published in this research paper that you can check out if you want to learn a lot more about it and migrants are just going to retrain my model the criteria I picked here seems demographic parity and it's retraining the model by re-weighting the data different ways to try to improve that fairness criteria that I selected so here it's going to train a variety of models we're gonna sort them to look at the best ones and then I have my my dashboard here so if we click now then I have the same feature so I can click through to sexes because we want to look at the same problem and I can click through to accuracy and now what we see is that seems creating that I I skipped earlier which is actually a range of the different models that I've trained that I can compare so here I have that original unmitigated model that we dived into before that has about an eighty four percent accuracy and about that 16 percent disparity however now I can also explore these other models on the trade-off curves so let's get this one in this case this one here it has a little bit lower accuracy eighty two percent however if I jump down here and look at the disparity in predictions which was the metric I was trying to improve in this case now I'm offering loans to just over fifteen percent of men and just over fourteen percent of women so a much closer number so it might be that okay this model looks great to me and now I want to take it and put it in production I could alternately I can use interpret ml again to explore this one more and make sure that it's making decisions for the right reasons before I move forward so the my next step here is I want to save all these different models that I've generated and so I'm going to register them all with Azure machine learning so that they're saved in my workspace then I can explore them later so we're registering all of those models I'm also going to upload the dashboards which you just already saw and upload all of those explanations so now I have them available for for future use or if I want to share them with collaborators or other stakeholders so in addition to the fairness which we already saw here with the different models available you also have the explanation tab built in so you can go back to those explanations anytime if you want to do a different type of analysis or look at a different cohort all that's available to you here in the in the studio so with that um I want to show you a real world example of people using for fair learns so let's go to the video why we fundamentally try to make the world a better working place through trying to quit AI and cloud at the forefront of digital transformations our trust AI platform is something that we've been building to help our clients really understand where the risks of AI are a manifesting realm and now in the last couple years there's been a tremendous desire to use machinery machinery models are created by humans and as I exposed to unfairness as a result they can take the form of unfair treatment of individuals that are part of the productive group such as gender race income age etc we use Azure machine learning and the fair learn open-source toolkit to support our efforts on our trusty AI platform and it is basically designed on different interrogations that we do of our clients AI systems to better understand their risk profile and to manage those risks l-learn is that very very cutting-edge approach towards fairness which actually in fears our approached in using it going forward for all our different projects where we talk about service because it's open source we're seeing the latest in a fairness of algorithms and metrics within the tool when we put fair learn to the test with real mortgage education data it improved the fairness of loan decisions before mitigation the models had a disparity of 7% between men and women after mitigation the disparity was less than 0.5% our data scientists and IT teams use ml ops to help accelerate their work while maintaining fairness levels be monitored every step of the process using the ml ops capabilities and Azure machine learning Microsoft's been a real leader in recognizing the need for ethics and responsibility in regards to how we think about responsibility AI business feeling a real need of our clients they're seeing more successful AI deployments which is translated to greater trust in their systems and ultimately an acceleration in achieving their desired business outcomes you so if you want to learn more about the topic of responsible AI or any of the tools or technologies that we demonstrated today we have a ton of great resources in our Microsoft responsible AI Resource Center we have responsible ml pages for Azure machine learning as well as documentation and how to use these tools with AML and of course all of the information about the core toolkits is available on the github pages there's many more sessions that build that you can watch to learn more and with that let's open it up for Q&A awesome thank you very much Sarah for walking us through such an insightful session the first question that we usually get when we talk about free learn is why not remove the sensitive attribute from the training data yet it's a great question and there's different schools of thought about this and in fact depending on regulations in your problem setting the right thing to do here may be different in terms of whether or not to directly use the attributes and the data now of course the advantage of using the attributes is that you can do things like the post processing techniques we talked about and even if I remove the attribute it might be that that is still represented in the other features and the dataset so I might still see the same effect without directly having the the attribute so it's not so straightforward and one very quick question and feel free to close the session can we support PI torch and tensor flow models basically models train via the deep neural network frameworks yeah so you can check out the readme x' on the github to get the latest information both of what model types we support as well as all of the different algorithms in the toolkit links to the research papers everything that you might need and yes we do support PI torch and tensor flow and psych it and a lot of the common model types that you'd want to use thank you so with that I'm going to say thank you for tuning in we hope that this was useful for you and that you check out the tools and use them as part of your responsible AI development toolkit 