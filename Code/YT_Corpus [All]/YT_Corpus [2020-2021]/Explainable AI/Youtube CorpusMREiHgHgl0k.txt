 Always thought there was a tradeoff between model accuracy and intelligibility, well, you're not going to want to miss this special build edition of the AI Show, where we'll get to hear from Rich Caruana, Senior Principal Researcher on the Microsoft Research Team. He is the creator of something called Explainable Boosting Machine, he's going to go through science and tell us how it works. Make sure you tune in. [MUSIC]  Hi, I'm Rich Caruana, a scientist at Microsoft Research in Redmond, and one of the inventors of EBMs, Explainable Boosting Machines. You may have seen a graph like this, which suggests that there's a tradeoff in machine-learning between accuracy and intelligibility. That is, the learning methods that are more accurate or low intelligibility and the models that are more intelligible have low accuracy. I'm happy to say that this is not true for EBMs, EBMs sit up here in the right-hand corner, where they have all the accuracy of methods like Boosted Trees, Random Forests, and Simple Neural Nets, but because they're a complete glass-box learning method, they're even more intelligible than models like linear and logistic regression. Let me give you a sketch of how we train EBMs, imagine that you've got n features, we go to the first feature and we train a very small tree on that feature, and that tree can only use feature 1. In boosting fashion, we then update the residual and now we go to feature 2, and we train a small tree that can only look at feature 2, we update the residual again, we go to the third feature, train, a small tree, and we do this for each of the features. So this is a round robin pass through all of the features, and each of these trees can only look at one feature at a time. Now the learning rate is so small that it doesn't really matter what order these features are in, we're going to have to do many many passes through the features. So we go back, we go to feature 1 and we grow a second small tree, continue to update the residual, go to feature 2, another small tree that can only look at feature 2, do that for all of the features, that's iteration 2. We do this for more iterations, three iterations, four iterations, we're going to do this as many as 5,000 or even 10,000 times. Now at the end of that, we have 5,000 or 10,000 trees which were only trained on feature 1. So it turns out you can summarize that as a graph, and we do that by asking all of these trees what they would predict for each value on the graph. Once we have the graph, we've captured the prediction of all of these trees and we no longer need the trees, so we can delete them. Now we can go and generate a graph for feature 2 in exactly the same way, and once we've generated graph for feature 2, we can throw away all the trees for feature 2. We can do this for every one of the columns of features, for every feature, we can throw away all of the trees. All of these graphs, all of these models were trained in parallel because we keep cycling through all of the features. In the end though, we throw away all of the trees and what we're left with is just a series of vectors of graphs, and that is the model. Let me show you how this model can be used to create new knowledge. This is a plot of pneumonia risk as a function of blood urea nitrogen. This is a blood test that all of us have received. Doctors were very happy when they saw this plot, because it shows the benefit of the treatments that they give patients. They consider a BUN of 50 to be high enough that they start to treat patients, and notice that the risk starts to flatten right after 50, and that's because their treatment is effective. They consider 100 to be a very high BUN and they start to give patients dialysis at around 100, and notice that the risk actually starts to come down, because most of the patients had 110 or 115 are getting dialysis treatment. But the doctors were also disappointed and that's because they noticed that risk is already quite high, it's a 35 or 40, and they wondered if they shouldn't be waiting until 50 to treat patients, and instead should be treating patients starting at 40. Notice that the treatment thresholds they'd been using, 50 and 100, are very round numbers. Doctors often use simple round numbers for treatment decisions and they just thought maybe they should be using 40 instead of 50. They were even more disappointed with what was happening near a 100, because if you look at it, patients who have a BUN, say of 115, are actually lower risk than patients who have a healthier BUN of 90, and that's because all of the patients at 115 are probably getting dialysis. Whereas maybe most of the patients at 90 are not, and they wondered if waiting to a 100 was waiting to late to be giving this treatment. So now with this model, we can treat it almost like a spreadsheet and do a what if analysis. That is, we can pretend that the treatment threshold, we'll say 85 or 90, and we can draw this red line, which is a conservative estimate of what we would have learned had the model been trained on data where the treatment threshold was 85 or 90. The area between the red line and the green curve, well, this is mortality risk, so this is patients who might be at higher risk of dying. We can estimate from this area that just by changing this one treatment threshold for dialysis from this one lab tests, blood, urea, nitrogen on pneumonia, that we can save as many as 2,500 lives per year in the United States, and that would be a great thing to do of course. Let me show you how you can use this model to debug your data. This is a different medical problem and this is P/F ratio, it's a measure of how well you're converting oxygen in the air you breathe into oxygen in your bloodstream. Patients who have a low P/F ratio are not breathing well, whereas most of us who are healthy have a high P/F ratio at near 1,000. Doctors looked at this and they said qualitatively, the graph looks exactly like we expect, but they wanted to know what is this funny blip that's happening right here in the graph, that didn't make sense to them. So we weren't sure what that was and we wondered, oh, maybe it's a round number, maybe this is some treatment decision that the doctors are making. But it turns out it's at 323.6, and that doesn't look like a round number to me. So we decided it's probably not a round number, but maybe it's a round number in metric units. Maybe it's only when we convert it to English units that it doesn't look like a round number. Doctor said no, this is the natural units for the data, so it's not that kind of conversion problem. So we scratched our head for a second and started investigating and asked, well, what's the mean of this feature? So what's the mean P/F ratio on the training set? It turns out that is right at 323.6. So this dip in the graph is happening right where the mean of this variable is, that's a strong clue. So one thought is, maybe there were missing values in the data and that they got imputed with the mean, and sure enough, someone who knew the dataset better said, yes, there were a lot of missing values in the data, and we think they might have been imputed with the mean before we ever got access to the data. So why does imputation with the mean look like this? Well, notice that the bottom here at this little bump, is about the same height as these healthy region where all the healthy patients are. In medical data, you're often missing a value because you didn't bother to measure the lab test, and that's because the patients just seemed healthy, so you didn't bother to measure it, so it's missing. Now, we've imputed the missing values with the mean value, which unfortunately falls in a region of reasonably unhealthy patients and because of that, the model is learning that it has to drop suddenly down to low risk and then go back up to higher risk as it goes past this imputed value. Because of that, the model is actually reasonably accurate because it's capturing that a large number of patients here are very healthy. The unfortunate thing though is that there actually are some patients who have a real P/F ratio of 323, and they would also be predicted as being lower risk, and that's bad. We don't want the model to be confusing people who have a missing value and are healthy, with people who actually have the value and are less healthy. Intelligibility can also just help you understand things that are in your data that you don't know about yet, everybody is interested in COVID-19, we found an open source dataset for COVID-19 risk. It's not a very high-quality dataset, so don't take any of this too seriously, but we trained in EBM on it. We're just showing you a piece of the model, this is risk as a function of different comorbidities that patients have. The highest risk comorbidity is chronic kidney disease, and if you've been paying attention to the press and early papers that are coming out, many people are talking about how kidney disease seems to be a very high risk factor for COVID-19. Now, chronic bronchitis and Parkinson's disease, just ignore these two bars because we only have three patients and two patients respectively here, that's not enough sample size for us to learn anything meaningful, so just ignore that. But we do have large sample size for diabetes and hypertension, and just as is being reported in the literature, diabetes contributes to your risk and so does hypertension, so these are serious risk factors for COVID-19, and the model has learned that from this data. The model has also learned something very strange, that asthma, COPD, which is chronic obstructive pulmonary disease, cancer, and even heart disease, might not be so bad for you if you've got COVID-19, and in fact, might be slightly protective. We're also seeing the same reports coming out in papers that people are starting to write about COVID-19. So this model is consistent with what is being reported in papers. Now, nobody really understands this, but we don't believe these things are actually good for you, we think the best explanation is that it's some sample bias. In fact, the best explanation is probably that people with asthma or COPD, cancer or heart disease are more likely to get admitted to the hospital even if they're not otherwise that sick from COVID-19, and because of that, we just have a healthier group of individuals in the hospital who have these conditions, so their mortality risk looks like it's lower. Let me wrap up, you've probably heard that there is a tradeoff between accuracy and intelligibility, this is not always true, and glass-box methods such as EBMs really do have a remarkable combination of accuracy and intelligibility that violate this tradeoff. So you can get high accuracy in high intelligibility and with that, you also get editability, because the graphs are vectors, are very simple 2-dimensional structures, you can actually edit these things and the model will follow your edits, it'll do what you tell it to do. Now we've been talking about this interpret ML package that we open sourced about a year ago in this session, and it includes EBMs, and it includes a bunch of other methods as well, such as LIME and SHAP that are used for interpreting black-box models. So we encourage you to try the package, it's very easy to use. If you've been using Boosted Trees, things like Gbt and XGBoost or Random Forests or Linear and Logistic Regression, you really should stop using these things now, you should use EBMs instead. That's because the EBMs will probably give you the same accuracy, but you'll also get intelligibility and editability for free when you use these models. So give EBMs a try, we think you'll like them and find them very useful. Thank you. [MUSIC] 