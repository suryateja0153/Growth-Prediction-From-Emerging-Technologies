 Sanyam Bhutani: Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science. Hello, and welcome to another episode of the '"Chai Time Data Science" show. In this episode, I'm excited to be talking to Patrick Hall, the Senior Director of Product at h2o.ai. Patrick has a background in math and he's done his MS in the field of analytics. In this episode, we talk all about Patrick's journey into machine learning, into machine learning interpretability, broadly speaking, and his journey at h2o.ai, how his work has evolved over the years at h2o. This is an exclusive first, I believe, where we talk a lot about machine learning, interpretability model debugging model fairness. In this interview, the caveats, and the future of this field, including research and applied research, broadly speaking, how are these ideas implemented inside of h2o.ai's products and how can a person working in this field even bring them to their pipelines? Patrick shares many amazing insights along all of these lines. So I'm really excited to be releasing this interview. A quick note to the non native English speaking audience if you're watching it on YouTube, or if you're not watching it on YouTube, please go to YouTube and enable the subtitles. This episode will have proper checked English subtitles that will be manually re uploaded so that you can have a better experience watching it all. Or if you'd like to read the interview in a few days from the video and audio release, this will also be released on h2o.ai's blog, blog page. So stay tuned if you'd like to check that out instead. For now, here's my interview with Patrick Hall, all about machine learning, machine learning interpretability and the future of the field. Please enjoy the show. Hi everyone today I am really excited to be talking to one of the pioneers if I may, of the field of machine learning interpretability, Patrick Hall, Senior Product director at h2o, h2o.ai. Thank you so much, Patrick for joining me on the "Chai Time Data Science" podcast. Patrick Hall: You're, you're welcome. It's my pleasure to be here. Sanyam Bhutani: Thanks. Today you're working as the Senior Product senior sorry Senior Director of Product at h2o and I was researching about your background, I found a few things around software research, applied research and leadership in your background. Can you tell us where did machine learning start to come into this picture and how did these things so to speak, connect with machine learning? Patrick Hall: Okay, okay. So, um, I have always been interested in math even as a little kid. And when I when I and I did lots of time, you know, I did lots of things in my education that had nothing to do with math, but I sort of settled on being a math major and my, in my last years of undergrad, and I was always on the applied math side, I was good at solving equations, I was horrible at proofs, I mean, just not not good. And and so, you know, being on the applied side of math, I ended up in computational chemistry graduate school because I had almost gotten a, I had almost gotten a chemistry degree as an undergraduate. Sanyam Bhutani: Okay. Patrick Hall: And, and so I was pursuing a PhD in physical chemistry. And, you know, in, in modern science, that's probably one of the, the places where big data is the most sort of real, right, these instruments and simulations generate a ton of data. And I got super into analyzing the data and that was kind of my thing and the research group and and then, you know, just by different and sort of varied circumstances, I didn't finish the chemistry PhD and I ended up going into software engineering. And, and I'll tie this up really nicely. It turns out that in chemistry in physical chemistry or computational chemistry, we we move atoms around in a molecule. Typically, there's other things that happen, but until the molecule or molecular system reaches a low energy state, and in machine learning, you move model parameters around or you look for model rules until the model gets into a low error state. And so it turns out the math in in physical chemistry or computational chemistry is really, really similar to the math of machine learning. And just kind of just just ready to go from machine learning, I guess. Sanyam Bhutani: You were sort of secretly still preparing yourself for the math of machine learning if I may. Patrick Hall: Yeah, but I didn't know that's what I certainly didn't know. That's what I was. So it really was a happy accident. In the beginning, I feel really lucky that I ended up here because I really enjoy machine learning. And, you know, I didn't even know it was an option for a career when I was younger. Sanyam Bhutani: But when did you discover your passion for machine learning? When did you decide you want to take up a career in this field, so to speak? Patrick Hall: I'm not sure you know, like I said, I really, you know, I was really much more into the data analysis than the chemistry part and computational chemistry. And maybe that was sort of the first, first sort of insight I got, um, and then I don't know, it just, it just always made, you know, I think I just have a sort of intuitive understanding for machine learning for whatever reason, I think, because of course, like I said, getting the molecule into the low energy state is very similar to getting the model into the low error state and and and so maybe I didn't really feel like I had a passion for it at first, I just kind of had a knack for it. And then now that we've gotten into this idea of interpretable machine learning or ethical machine learning or responsible machine learning, I do feel sort of more personally passionate about that. And, and that's, you know, that's been a really cool thing over over the last couple years. Sanyam Bhutani: Awesome. Coming to your current job you're working at h20.ai, can you tell us what task are you working on? What does a day in your life currently play? Patrick Hall: Um, so I spend a lot of time traveling which is not you know, I don't actually love to travel. You know, visit business travel, it's fun at first and then and then not fun. But I spend a lot of time traveling and sort of talking to customers and other researchers or or sort of commentators in the field are people think tanks and, and I, I do some product management I I'm certainly not a traditional product manager. And and really I think it's really important to understand that the engineers on the on the team, they they do almost all the work i mean I, I I write papers and I sort of interface with the customers and and and occasionally help them translate requests and things like this but but it's really the engineers on on the team that are doing all the work I'm you know, I just kind of help guide it from like 30,000 feet, so. Yeah. Sanyam Bhutani: Okay. You're also an adjunct professor at George Washington University. Can you tell us how do you manage all of these things together? And how do you detect if any one of your student is cheating on there homework with DAI? Patrick Hall: Oh, I don't know. Well, I have lots of ways to check if they're cheating: and it's funny I didn't at first because I didn't I didn't think that people cheated. I didn't look how naive I was. Um, so I don't I don't I'm not sure that I necessarily do a good job balancing all these things. And honestly, I've had to take a break from teaching I probably haven't taught for the last two semesters, but I do really hope I can get back to it. And, and actually, I would be fine with my students using driverless AI. I've been I've been playing this mean joke on them. You know, the last, ever since driverless AI has been existence where I sort of teach the whole introduction to data mining class, you know, 14-16 classes in a semester or whatever it is. And then on the last day, I show them driverless AI and feature engineering and it does and build models and it does cross validation and you know, hyper parameter tuning and all this stuff and so, you know, I think that one thing I try to get the students to understand is the importance and sort of comment and you know, how AutoML it has become so common and and really this idea that, um, I don't know, I have mixed feelings about it, but I think it is important for the students to be aware of it. I'm not I'm not saying that's necessarily 100% best way to do machine learning in all cases. But it's definitely something that people who are entering the field need to be aware of. There's a lot of automation of machine learning processes. Sanyam Bhutani: I believe that's why even if if I may speak H2O: we're an advocate of a human in the loop of AutoML: what are your thoughts on that? Is that better approach? Patrick Hall: Oh I think that's absolutely necessary today. I don't I don't we're not we're not anywhere near generalize strong AI. And; Sanyam Bhutani: You're not on the side of robot overlords whenever they will come. Patrick Hall: I'm I'm no, I don't think we have to worry about that in our lifetime. So I don't I don't think I have to pick a side today. Um, but but yeah, I think, you know, I, I think that for sort of low risk activities, and I'm going to pick on, you know, ad placement or something like this or, you know, maybe maybe it's fine to use auto ML but but for things that are going to really impact people's lives, credit lending employment, things in criminal justice. And I mean, there's just a surprising number of ways that machine learning can be used, it could negatively impact people's lives. And I think, in all of those cases, it's really imperative to have to have a human in the loop. Today, really, I just, you know, I think that I'm becoming a big proponent of this idea of model debugging, which is just sort of glorified model diagnostics and just torturing machine learning models. And every time I really debug a model, I just, it's shocking how wrong and simple you know they are so so I you know my experience has been that the models are are very fallible and and and we need to be careful and responsible with them. Sanyam Bhutani: Definitely, we'll talk a little bit more about this in a bit but lingering on to your career in teaching so to speak you were also involved earlier I think you still are at meetup groups in h2o, or even with teaching, did you always enjoy teaching? I believe you also authored a book with Navdeep at h2o. Do you enjoy this: Patrick Hall: Yeah yeah, I did. And that was like I remember when I was in chemistry graduate school, you know that that there was a push, you know, a certain like an informal push and I think a lot of people in science graduate school have experienced this like, do not you know, just just give the absolute minimum time to your to your teaching, you know, your graduate teaching assistantship and, and, you know, focus on research. And I always enjoy teaching and I never felt right about sort of shortchanging my students and not preparing for class and things like this. Now, I'm not saying I'm always prepared for class. I wish I was. I'm not always but you know, I, I've just always enjoyed teaching and and think it's just fundamentally important, right. I mean, I wouldn't be where I am if it wasn't for good teachers along the way. And and I think most people would, if they look back would would recognize that that, you know, their professional career was sort of profoundly affected by a teacher back in back in the past somewhere. So yeah, I think I'm teaching just really important, and I do happen to enjoy it. So um, so yeah, yeah. Sanyam Bhutani: What are your thoughts on teaching at university versus a person like me who likes to pick up online courses, who maybe doesn't have the privilege of having a faculty like you. Patrick Hall: Um, I guess what do you mean do I think one is better than the other or; Sanyam Bhutani: How should be you; Patrick Hall: I think you said it right you don't mean it's it's if you have time and money to go to school like that's that's really a privilege and I've certainly had to do some some online learning myself and and you know self taught many things along the way too so so I certainly understand both sides of that coin. Sanyam Bhutani: Okay, now coming to your journey at h2o even I believe you joined before Driverless AI was even introduced was even created. How has your overview of the products and work evolved over these years? Patrick Hall: I remember the first, I used to work at SAS, you know, which is sort of the largest analytics company and I remember the first time someone from the Netherlands actually it's asked showed me h2o, and we have been working really, really hard on our neural network at SAS and and then someone showed me you know, the h2o neural network which was just incredibly fast and, and I was just blown away right so that my very first experience with h2o and it was probably h2o-2 I'm not I'm not sure was you know, open source h2o-2 I think and seeing how fast the neural network was and just being blown away. And then then I went to h2o world in 2015 and was really impressed by their sort of focused on Kaggle and their focus on the open source community and the data science community. And then I think it was December, maybe a year later, I actually joined the company. And and the main product was open source h2o three. And which I was which I'm still blown away by. I think that's just one of the most phenomenal; Sanyam Bhutani: Definitely. Patrick Hall: Pieces of software ever released. And, and so the the idea was that the the interpretability or the explanation stuff would just be part of h2o three that that was sort of the original idea just because there was no other product vehicle. And and we started working on it. And I think it just it just made sense to tie it into driverless AI right? Auto ML can it can feel even more blackbox than that one machine learning model right or just training one machine learning model by hand. And so, you know, I think the need for some explanation of how how the model was working was was just great. And we had been working on stuff for this anyway. And so I think it was just kind of natural that that it happened. And, and now the now things are kind of going back the other way where ideas that were sort of pursued in driverless AI are now are now ending up an open source of h2o. Which I love that also so but but yeah, I think it was just natural the original plan was to go in h2o three but that's just because that was all there is. And then you know explanation so visual, right and, and so it just made a lot of sense to get it into the product was a good lead the product that needed more explanation, that kind of thing. Sanyam Bhutani: Okay, now you've been at h2o for much longer than me. Can you give us an inside view of the maker philosophy? The makers gonna make philosophy as we call it? What's it all about? Patrick Hall: Well, I think it's very, that to me, the unique thing about h2o is it's a very human company. It you know, it's a company with a soul, which is, which is good. Um, it's, you know, it's not easy to be at a start up necessarily. But but, you know, I think the people that interact with h2o and the customer side most of them would agree that that, you know, it's a company that brings a lot of sort of human excitement and human investment into the product and the work that we do. And I think certainly know the maker culture is is important and we wouldn't be where we were without it and just the freedom to you know, I've never had more more sort of creative freedom at a at a job to just just pursue the what I thought were the best ideas and and yeah, but but you know, there is a downside to this which is just there's a lot of chaos. There's a lot of, there's a lot of sort of fog of war there's a lot of there's a lot of just miscommunications, because everyone is so creative and doing what they want to do and, and, and so far we've survived and sort of struck a balance between between very being very human oriented and maker oriented and somehow running a business. It's not it's certainly not easier or pretty sometimes but, you know, we've we've been successful thus far. And it's a really a really unique place. That's for sure. That's for sure. That's a really unique company. Sanyam Bhutani: I think that's one thing that people usually miss out is engineering also involves a lot of creativity. It's not a science field that you just sit down code, whatever you supposed to, even driveless AI I think wouldn't have come to the picture if it wasn't for our maker culture. Patrick Hall: Yeah, oh, that's certainly I mean, and it driverless AI was just an idea that came out of a whiteboard, you know, kind of brainstorming session like I and Kaggle. It came out of Kaggle. You know, I have mixed feelings about Kaggle. But like I said, when when, you know, back in 2015 I was just so blown away by h2o's focus on Kaggle. And I thought, you know, I think at that time, it was brilliant. And, and I should I make my students do Kaggle and, you know, I, I don't like this idea that, you know, my model has the AUC of .8899 and your model has a, you know, AUC of .8888, and my model's better. I mean, that's really that's not how things work. But you know, I think there's a lot of good things about Kaggle and, and driverless AI sort of came out of this this focus on Kaggle culture, so it's, yeah, there's lots of interesting stuff going on at h2o. Sanyam Bhutani: Definitely. Now, you mentioned you found your passion for model explainability, machine learning interpretability. Why was this important to you as a field, like you were saying, is it not, is Kaggle not good enough, the Kaggle situation where you're just looking to get the best state of the art model out there, right? Patrick Hall: Yeah. So, you know, I guess because they got tired of me at SAS headquarters in North Carolina, they were sending me all around the world. And what I was seeing is and you know, and SAS deserves a ton of respect rates, SAS did everything before everybody else in this field, you know, they were doing deep learning in 2000s you know, just just, you know, they deserve a ton of respect and, and, you know, I learned so much from the people there. So I make them kind of flippant comments, but they're just, I'm just kidding around. So yeah, they have customers all over the world, and they sent me all around the world. And, and because they've been doing machine learning for so long, right, they had a track record of sort of what works machine learning and what doesn't, and at that time, you know, say 2014 before before, LIME you know, machine learning models are basically seen as black boxes. And, and so it was just like we can't use machine learning model and a scenario where, you know, there needs to be explained ability or interpretability, or transparency into how the model works. And then moreover, the machine learning models are very complex, so it's hard to deploy them. So so back in that, you know, five years ago, 10 years ago, machine learning projects would fail, either because they were too complex to be explained to business partners, or they were too complex to be deployed. And I saw, you know, I SAS actually had a pretty good solution on the deployment side, h2o has a good solution on the deployment side. And so I saw that deployment. problem being getting solved, at least to a large extent. I think Docker has really helped with the deployment problem, and virtual environments, all these things, you know, and that's really an engineering problem and I'm not a good engineer, I'm a bad engineer. I like to code. I'm just not good at it. And so, and so, you know, because of this experience I had I focused on. And my background I focused on well, how do we solve this explainability problem of machine learning, because that's one reason why these projects are failing. And, and, yeah, that's how I got into it. I got into it sort of this business need, right? In the United States, credit lenders have to give for reasons I believe why they're turning you down for credit. And how did they do that with a machine learning model? That was just the original problem that kind of intrigued me. And, you know, obviously, it's it ballooned into something much broader than that now, which is good. I mean, I, I'm glad that that, you know, it's taking this more human turn instead of just focusing on you know, what, what just a small sort of subset of of the market that kind of thing. Sanyam Bhutani: Definitely. What's the, if you could give us an overview of the field right now because many people still think that machine learning is this black box where you know nothing about things going output comes out and you have no control. What's the current state with is MLI ready to use? What are its promising areas right now. Patrick Hall: Yeah, I think that this idea that machine learning as a complete back black boxes is dead. Um, I'd say for for tabular data, we can go even farther than that. For tabular data, there's no accuracy interpretability trade off anymore, or at least that trade off is has been sort of significantly diluted. And I was I was just showing Sri our CEO yesterday some results we had using a fully transparent neural network architecture beat three other models including a standard neural network and a standard gradient boosting machine. So we're talking about on tabular data, I can have a model that's both more accurate than a traditional linear model more accurate than other machine learning models and still completely interpretable. So I think that you know, and I'm not the first person to bring this up like a lot of all the credit should go to Cynthia Rutan and Rich Karwana and and other, you know, real scientists who who have have really devoted significant time and effort to this over over the past 10 or longer years. So, so we can make very strong statements for tabular data, there's just no need to use a blackbox model at all. I mean, you can obviously there's no laws against it yet. You can use a blackbox model, but you don't, you shouldn't. It's just that you know, there there are accurate interpretable models for tabular data, I think for unstructured data, images and text there. There are some really good sort of post hoc explanation techniques. You know, meeting, I train a black box model, and then I'm able to get some insight, some summarization of different mechanisms or predictions of the model. And, and, you know, and I do expect an end, you know, Professor Ruden at Duke is working on this, you know, fully interpretable variants of deep learning or augmenting or changing deep learning architecture so that they're more interpretable even for unstructured data. So I think that's coming. I think it's just maybe not even harder. Just people didn't even think it was possible. So they weren't focusing on it, something like that. Sanyam Bhutani: What industries do you think is this ready to apply? Are we ready to bring this into banking industries? Medical? Patrick Hall: Oh yeah, I mean, banking is is the no brainer. I actually I'd say all industries are the no brainer that that maybe I've just become, you know, to maybe I live inside my own world too much. But I just I've never been in a business scenario where it was like, would you like a black box that you can actually explain or debug or understand? Or would you like a model that's just as accurate that you can't debug and understand and explain to people and I just the risk, the risk associated with black box machine learning are when as compared to more transparent or sort of explainable types of machine learning are just too high. Why why would you do that? Why would any company do that? So, yeah, I certainly think it's, it's ready for prime time and, and, and I mean, I work most closely right now with financial services customers and it's certainly being used in financial services. I'm not I'm not I'm I'm not saying definitively that it's being used for credit scoring even though I suspect that it is but it is being used throughout the throughout different verticals in financial services. Sanyam Bhutani: Okay, talking about industries even h2o, even though we provide Auto ML Products broadly speaking is a huge proponent of MLI, Where does MLI even come into the picture because AutoML is supposed to replace the human if I may, so why is MLI important in this picture? Patrick Hall: Well, I think it's like, it's fine to replace the human for hyper parameter tuning or models selection or something like this, it's, or it can be fine, right? People, people aren't good at certain tasks, and computers aren't good at certain task. And there's no reason why, you know, the certain we can't help each other out with our weaknesses, where, you know, explaining a gradient boosting machine or constraining gradient boosting machine to make it more interpretable you know, that you still have to do hyper parameter searches, you still have to, you know, you know, do model selection, so, so I think that's it. It's really about once you have the model, is that model transparent or can it be explainable and so I think I see, I don't know, it's just just doing that model selection step, right. You know, I can try these different hypothesis algorithms, linear models, neural networks, tree based models. And then once I, you know, I know which hypothesis model or architecture I want to select that now, you know, it has to be tuned and, you know, I think that that part of it, there's, there's no issue with that in my mind for AutoML. And if it's, if it's done correctly, you know, it's very likely better than people people get bored and can't pay attention and forget to do little thing, you know, cross validation, very difficult, right? Validation seems very difficult to get right and, and so I think that's, that's a perfect example of if I have an automatic routine that always does very careful cross validation. You know, that's a great reason to use AutoML. Sanyam Bhutani: Is there a downside to this transparency as well? Do we always need to create simpler; Patrick Hall: Yes. Okay. Um, there's something in cyber security and I'm not a cyber security expert, but there's this idea of the transparency paradox, where it's like, a system can be so simple that that sort of, it can't be its mechanisms can't be questioned. And so you know, if someone's manipulated it, or it can be so complex that you just count on, on no one with understanding how to manipulate it, and I'm probably butchering this AI, this, you know, transparency paradox, but I think it does turn out that that, you know, in some ways, the same idea, at least, this idea of just more data is not always better applies and explainable machine learning. So, if there's all these ways to hack machine learning models, and I'm not I'm not saying this happens very often, yet, I think it will happen in the future. And explainable machine learning makes that easier, explainable machine learning tools are often that are often tools used in the actual hacking, stealing of machine learning model and data training data of machine learning models. And then also when I provide explanations with a prediction or other information about the model it just makes it easier to steal which means which typically makes it the big deal there is one people are getting access to your proprietary business logic and that's bad but two, what's worse is that you know, at least I know how to reconstruct training data from from just a machine learning employee and I'm sure I'm not the only one but I'm you know, that you can really compromise very sensitive training data which which will now get you in lots of trouble. And and it should be the case so, so yeah, there is certainly a downside to explainable AI. There's very interesting paper recently talking about how sort of expert explanation methods that are based on sampling can be hacked and made to say anything that you want right, so, so yeah, there's certainly a downside. And it's basically just that, that all data are, you know, sort of has some risk associated with it. And when I do explanations I'm sort of generating more data that I that I have to be careful about and and if I'm not then I'm just sort of creating privacy prob- you know, potential privacy problems for my myself and my customers. Sanyam Bhutani: Who should be cognizant of all of these downsides, like most most probably ML engineer just looks at creating a model and putting it into production. Should those also be cognizant of these downsides even think of making it more interpretable? Or do you have any guidelines? Patrick Hall: Well, I wish everyone would think about it. Um, you know, the analogy I use is it machine learning is just and it could fail again, right machine learning's failed and and in, sort of just it's failed to scale to meet the needs of the general economy on several occasions, it seems like it's going to work this time, but we don't know for sure yet. And the, you know, so so we have this powerful technology, that sort of being used in different ways across the economy and different, you know, government organizations and other places. And so I wish instead, you know, people think like, I can just do machine learning, right? What would you think like, I can just do aviation or I can just do nuclear. You were it's just a powerful technology, it can be used. And you know, and this this is very cliche, it's the title of a book from the president Microsoft tools and weapons. You know, just any powerful technology can be used as a tool or a weapon and, and machine learning is no different. And yet people should think about that probably everyone involved with with machine learning should think about that. And I do, you can see governments around the world starting to take notice. I've noticed, you know that the UK and Singapore have have put out very detailed at least sort of proposed guidance for the use of artificial intelligence. The Trump administration recently released proposed guidance for US regulatory agencies. I noticed in in in India, there was a there was a state that wanted to do some regulations around facial recognition, facial recognition technology, which I think is really smart. So I do think people are starting to take notice and I think that governments are starting to get involved and and that's probably a good thing. You know, obviously, there's going to be unintended consequences and regulation slows down innovation. But, um, like I said, it's just a powerful technology like airplanes or like nuclear power or or nuclear in general. And it just needs to be sort of handled with care and regulated and and people who are working with it should should bear that in mind, in most cases. Sanyam Bhutani: One of my favorite quotes is when machine learning models fail, they fail silently. And since we already have almost perfect guidelines for debugging software, but we have literally no defined paths for debugging machine learning models. Can you give me; Patrick Hall: I wrote a Medium post about it 28 claps or something don't say there's nothing Sanyam Bhutani: That will definitely be linked, but if you can also speak about it in this interview for anyone who hasn't read it, and will read it after going through the description. Patrick Hall: Well, and I should actually give a plug to my good buddy, Andrew Burt. We did, he helped me kind of downsize that piece and make it more suitable for a broader audience. And there's also something on O'Reilly radar that we just got out in late December about models debugging too. So I yeah, I think model debugging, the looking for bugs part falls into roughly four buckets, in my opinion. So there's residual analysis, sensitivity analysis, being the second bucket, benchmark models, I think it's really important, you know, like I have a working model and this is in Kaggle, you know, they are, used to be when I used to do Kaggle. You know, they always have a benchmark that you can try and work off of, and compare your more complex model to, I think, so I think benchmark models are really important part of finding bugs, right. Why is why is my linear model getting this right, but my machine learning model, getting it wrong. And then the sort of white hat or red team hacking, and I was, you know, the hacking that I was talking about, I think, you know, basically we want to try to find fairness bugs or discrimination, we want to find security vulnerabilities and privacy harms. And we want to try to find inaccuracies. And so I think between residual analysis and I would include a lot of the the standard group fairness test and residual analysis, because you're often times looking at errors across different demographic groups. So, residual analysis, which may or may not include sort of fairness testing, sensitivity analysis, where we kind of perturb the data and see how the model reacts, comparisons to benchmark models and then essentially trying to hack your own machine learning model. I would say those are those are four good ways to to look for to look for bugs. And then there's also ways to fix the bugs if if you find them you know, a lot of the fixes are simple, just get more data or, you know, try try try different hyper parameters or something like this. But, but, you know, I think there are some really innovative ideas and modeling debugging such as model editing, and it, I'm not sure all models are editable but but I think many models are. And, and again that that idea has kind of been brought to the fore by people at Microsoft Research including Rich karwana with this, these ga2m or EBM models, where, where it's an additive model. And so each each input kind of gets a complex spine, and I look at the way that spine is behaving. And if I don't like it, I can just change the form of the spine manually. And so that would be an example of model editing, I think model assertions, which, which I think are just business rules. As far as I can tell. I think people in the that have been in the predictive modeling world forever would just say, oh, those are business rules. But we're talking about this idea that I can just try to correct a machine learning problem that's happening, right? Why the example I always give is, is pre payment on your credit card, let's say you know, for whatever reason, you're going to be out of the country for three months and so you make a large pre payment on your credit card. And in the interim, you're missing payments. And so the machine learning model might say, oh, they missed three payments in a row, you know, they're going to default but you need to have a business rule there to or assertion to check for to check for pre payment and and so I think, you know, the these ideas of there's lots of ways to fix your machine learning models read on O'Reilly read on Medium, the two kind of innovative ones that I've heard of are model assertions and a model editing. Sanyam Bhutani: Okay, lingering on to that, how do we think about model famous and model biases, because again, these might be blind spots to the person who's even created all just checking the model. Patrick Hall: Yeah, so I'm and I get asked to talk about fairness and machine learning. And a lot, and I'm really not an expert in it, but I'll, you know, I'll try. And so yeah, so anytime you're you're, you're working with data about people that even if it doesn't have demographic information in it, that it the demographic information is probably encoded somehow in the data, right? And so even if I'm not using a gender column, or a race column or something like this, that information could still be encoded in my data. The so so I think that if you're working with data and models that involve people then then you really do have sort of a professional obligation to do at least some basic discrimination testing and, and the safest way, you know, I think the safest, most conservative way is is there there are these test for discrimination and been around for decades. Okay. And someone smart just wrote wrote a paper called 50 years of test on fairness lessons for machine learning. So like everything else that you know, there's nothing new under the sun. So so there's these ideas and legal precedents around fairness and sort of testing or employment test or even credit lending that have been around a long time and and they're sort of well established, at least in terms of legal precedent. And I think that the people can use those and so you can google things like adverse impact ratio or standardized mean difference, marginal effects or shortfall on these tests you can do on the back of a napkin essentially. And, and so I think people have a real obligation to do that if they're working with data models about people. And if you find discrimination, that the safest thing to do, and this is actually a place where machine learning is is great as compared to any of your models, probably there's machine learning model out there for your data set that's just as accurate, that doesn't have the same kind of discrimination characteristics. So, you know, machine learning, we have this problem problem, the multiplicity of good models, that kind of becomes a solution to fairness in machine learning when if I do detect discrimination and my predictions, there's probably another model out there that's just as good and hopefully has better discrimination characteristics. Now, you have that all that statement needs a huge amount of caveats, you know, that the standard group fairness testing, I mean, I think people have written tons of papers about why those aren't super desirable and all scenarios there, there's lots of new ways to get rid of biased models or discrimination models. And, and, you know, so I, you know, we can go a lot of different directions in this conversation, but but you know, that the statement That I just made, you know, has some caveats to go with it. But but in general, I think that the testing is, is fairly simple. And then just using that as a model selection criteria is something that all data scientists should probably do if they're working on data about people. Sanyam Bhutani: Zooming back on back on database, someone who's even creating these tools, is it possible that they themselves might introduce some biases? Having; Patrick Hall: Of course, of course, so, um, you know, and I, I don't, it's very difficult, or I don't know, you know, I've had difficulty in my professional life pulling this off. But, um, anytime you're working on a serious project, you know, say like making machine learning tool or making a machine learning product in a large organization. It's good to have lots of different kinds of people involved. And whether we're talking about gender or ethnicity or different intellectual backgrounds. I mean, it just the more kind of people you have involved, the less blind spots you have. And I think that's sort of a piece of common sense advice that and all these write ups about how to avoid discrimination and machine learning is getting other people, you know, different kinds of people involved in the creation of machine learning products. And I certainly think that's true. One thing I'll call out is this idea of sort of a colonization, soft colonization of, of Social Sciences by tech. Right. So I see a lot of and, and commercials and stuff for sort of psychology apps. And, and I always wonder, was there actually a psychologist involved? Right, or did the engineers just read a blog about psychology coding, so so I think, you know, I, of course, I want people of different races and genders involved in in machine learning projects, but I think it's a specific thing that doesn't get caught out there is it's probably really important to have attorneys, ethicist, social scientists, like it's important to have this this diverse sort of intellectual background, especially when you're doing a machine learning project that's going to affect people's lives. And no data scientist that I know is is qualified to act as an ethicist or sociologist, or a lawyer. And, and so, you know, I kind of like to call out that aspect of the of this of the situation. Sanyam Bhutani: I think this that's also a democratizing AI comes into picture, putting AI in the hands of everyone so that they are aware of all of the caveats that you mentioned, and what it's capable of what it's also capable of, in the negative sense. Patrick Hall: Yeah, yeah. Yeah, certainly. And right. I think it just goes back to this almost cliche idea of tools and weapons, right. It's just be used for good and bad and and I think we're, you know, I don't think people are, I think maybe two years people weren't as familiar with this idea. And I certainly wasn't as familiar with this idea. I don't, you know I get, you know I'm lucky to work with probably some of the best practitioners in the world. So most of the people I'm interacting with are pretty aware of this and especially in financial services, it's been a lot of things this has been regulated by the US government for decades and and I think that that just many of the those lessons, model documentation, adverse action notices, bias testing or discrimination testing, that these things have been done in financial services for decades. And and just porting them over to any use of machine learning is probably a good idea. Sanyam Bhutani: Now coming to the future of the field, I know you are also involved in research, not just applied research, why this also important to you working on research papers and putting a push in this direction? Patrick Hall: I don't know I guess this is just because I really just enjoy machine learning and I don't I don't know, I'm; Sanyam Bhutani: It's a side product. Patrick Hall: I don't know, it doesn't actually, last year was my side project. It? I don't, you know, it's a good question. I mean, like I was saying, you know, I was just so pleased to see and hopefully this does get published. But if if ours doesn't get published, someone else's already has been, you know, we did this interesting comparison and I would this is not fundamental research, I would call this very applied research. But, you know, where we were comparing interpretable or constrained machine learning models to to more standard machine learning models and got excellent results from the constrained machine learning models. We were able to do discrimination testing on the constrained models and the unconstrained models and get into a little bit of the differences there. We were able to build these like, you know, sort of complex visual probably too complex. I don't know why we always try to make things complex. It's I do that. I'm guilty of that, you know, visualizations of how the models are working. So, yeah, I'm just I don't know, I think there's a lot of problems left to solve and machine learning. Let's say that let's say that I'm just very interested in in trying to solve the ones that I have expertise about. Sanyam Bhutani: Okay, well, the future of the field also any other research ideas that you're really excited about in this field that have recently come out? Patrick Hall: Well, I just think I'm so I used to work on deep learning and I kind of gave it up like 2012 2014 because I I just felt like it was really computationally expensive easter egg mine, you know, and in the end, and, and and hard to explain right. At that time, there was no grad cam or, you know, saliency maps or you know, gradient based feature app tribution that for whatever reason, you know, that just didn't exist yet or we weren't aware of it. And yeah, so so it's just this kind of bomb you know, I'll never forget you know, the first time I trained an auto encoder on the interstate and saw it make its own clusters and then like sort of running damaged digits bet through this autoencoder and seeing fixed digits come out the top of it, you know, that was just a very like almost visceral experience for me like being like wow, this this thing really learned something. And so I really fell in love with deep learning and then like I said, just just for sort of practical reasons let it go. You know, I I'll, people hate me for saying stuff like this, but we saw it. When we were working on deep learning for text and I think the deep learning model was about point 1% more accurate than the linear model and took 20,000 times longer to try so you know that just that point 1% accuracy just isn't worth it at that point. And; Sanyam Bhutani: Which isn't true in some cases, even in; Patrick Hall: Yeah, and deep learning is still very computationally expensive, easter egg hunt. But I think that, um, you know, like in the paper that I was just referring to, we had these great results from from a nearly completely transparent neural network architecture called the explainable neural network that was kind of released originally, I think, by by Wells Fargo. And then I, so I'm really interested to sort of keep pushing that, keep pushing that technology. So this idea of, of interpretable deep learning is really appealing to me, I think. I don't think people do this anymore. But I spent a lot of times a few years ago just going around telling people not to do combinational neural networks on tabular data because you know, they, there's no local structure in the data. And that's what convolution needs to be successful. And so I don't, I don't think people do that now. And I think there are network architectures that that could exploit structure or patterns or attributes of tabular data. I'm not going to say that what they are on this podcast, but i do i do think that that the future of interpretable deep learning is probably pretty bright. So I'm really into this idea of interpretable deep learning for tabular data that that might actually finally be more accurate than gradient boosting machine. So that's what we showed in this paper, actually, that the actually and then actually in some cases, the all the gradient boosting machines, which that's, that's a big deal because going back to the early 2000s, when Rich Karwana did come an empirical study of a lot of different machine learning algorithms, about gradient boosting, trees were gradient boosted trees are, stumps are always at the top and and so on. Yeah, that's a really interesting idea to me is, it's sort of interpretable deep learning for structured data. Sanyam Bhutani: Awesome. This has been a very insightful interview about this field. My final question to you, actually two questions would be what is it? best advice would you have for someone just starting in the field and someone who's also curious about learning MLI? Patrick Hall: So I think if if you're just starting in the field, you have to, you have to balance this the sort of, let's say, like, the pure sort of curiosity and fun of deep learning on GPUs on on for for computer vision problems, right. You know, you kind of have to build balance this curiosity of the state of the art which is very good, right? Like, h2o isn't, isn't where it is because people weren't interested in state of the art things right. So you have to balance your interest in the state of the art with with making things actually work in the real world. And if you want to be really successful, then you have to know how to make state of the art things actually work in the real world. And I guess that's, that's what h2o is, is really good at. And so, so yeah, sort of balancing those two things. And and again, I people should be curious in the state of the art and people should spend time on on sort of just just sort of the pure joy of GPUs and Python and Kaggle or whatever. But, but then, you know, I think, unless you're going to be a, you know, a pure machine learning researcher, and there's not many of those jobs out there. You got to know how to make this stuff work in in the real world. And then of course, the best researchers make things work in the real world too. So, so I think that's, that's my, you know, some bit of advice I can give and hopefully not, hopefully, that's okay. Um, if you're interested in interpretability then, you should I think you should just go to interpretable models, right? Just go look at the Microsoft interpret project or look at my GitHub look at, you know, monotonic gradient boosting machines or explainable boosting machines. Look at all of Cynthia Ruden's code and paper, code and papers, I just think, you know, that, I think in 2020, which is crazy to say, you know, we can make accurate interpretable models. I think people are pretty swept up in this idea of post hoc explanation. I was originally too, but I think we can just do better than that now. So I think, you know, if you're starting from scratch, why not just start the right way and start teaching yourself these these accurate and interpretable models that are available on open source software? And, and yeah, and and I think you have to think about it holistically, right, like we want interpretable models, we want to be able to summarize what they did for anyone decision, probably in terms of post hoc explanation, we want to touch them for bias, we want to, we want to debug them? So, so I think, you know, also keeping in mind that it's holistic, right? It's not just explanations and interpretability or it's not just fairness, it's it's all of these things and security. Sanyam Bhutani: Awesome. I'd also mentioned your book, again which will be linked in the description. Patrick Hall: Booklet, ebook. It's not a real book. Sanyam Bhutani: On h2o's GitHub, we have MLI, I think it's called MLI resources. Patrick Hall: Yeah, yeah, that's a decent place to look. Um, yeah, there's all kinds of stuff on the internet, you know, you know, how the internet is. There's a lot of stuff out there and, and some of its good and some of its bad. But, but yeah, I think good, good places to look are are hopefully my GitHub. Hopefully, you know, the, it's called Mike-the-Microsoft interprete project. I'm thinking maybe now but interpret ML slash interpret on on GitHub. Professor Ruden at Duke, and there's just a ton about it Sameer Singh has done incredible amounts of work, you know, original author of, of lime and and then, you know, he now works on explanations for for text models. So so I don't even I shouldn't I don't deserve to be spoken about in the same sentence as those people but but you know, I think those would all be good names to Google, if you want to learn this stuff. Sanyam Bhutani: Awesome. Before we conclude the interview, what would be the best platforms to follow? You will have these linked again in the description of this episode. Patrick Hall: I mean, I'm on I'm on Twitter and LinkedIn, I don't I don't do that much Medium. I shouldn't so I put like maybe the first thing I ever put on Medium at the end of last year, um, but yeah, LinkedIn. LinkedIn is super lame. Maybe, maybe I maybe that just means I'm Superlame. That does tend to be the place where I talked to people the most, I'd love to talk to people on Twitter. No one seems to want to talk to me there. But um, you know, I think so LinkedIn seems to be the place where people find me the most, but I'm around I'm around on the internet saying saying things that I'll regret saying in a few years. Sanyam Bhutani: Okay, please audiences help us change the Twitter matter. Thank you so much, Patrick, for joining me on this episode and for all of your amazing contributions and also insights in this interview. Patrick Hall: Now, my pleasure, thank you have a good night. Sanyam Bhutani: Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review, or feel free to shoot me a message you can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science". 