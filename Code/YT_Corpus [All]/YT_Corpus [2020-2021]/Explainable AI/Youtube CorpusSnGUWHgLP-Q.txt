 - Good afternoon everyone. About a year ago, Dan and I hatched a project on AI use by federal agencies that we've since been co-leading along with our Stanford colleague Tino Cuellar, who is a Law School faculty member but also currently has a day job as a California Supreme Court justice, and then also another colleague of ours from the Law School at NYU. Everything we talk about today is joint with both of them. In hatching this project, our motivations were at least two-fold. The first is that much of the debate about government use of AI has focused on its use in the criminal justice context. That include things like use of AI for predictive policing or facial recognition, use of AI in facial recognition by law enforcement. Can also mean things like use of criminal risk assessment tools to make bail, sentencing and parole decisions. Second, when the debate has strayed beyond these criminal justice uses, it is almost entirely abstracted from the technical and the operational details of the tools that government is developing. And so we built a team that you can only build at Stanford, and here some of them are. It included roughly 15 students from the Law School, the Business School, from the social sciences. It also included 10 engineers, including nine computer science PhDs and one undergrad, a major in SymSys. We set to work, and we are soon to submit a report. It's basically a book to a federal agency called the Administrative Conference of the United States that provides guidance to the rest of the Federal Administrative State on how to do the work of government. That report's gonna come out probably this week or next. Today, Dan and I are gonna provide a preview of our report, and in so doing, we hope to convince you of two things. The first is that there are lots of rich and important questions about how to arm the government with what it needs to develop effective and fair AI governance tools. There are also some really rich and important questions about how to build a sensible accountability structure around the use of those tools. That's the first thing. The second thing is that we hope to convince you that we, at the Law School, are continuing to pursue these and other really interesting questions at the intersection of law and technology, and we believe that you, no matter where you're from in the University, should come and join us. From here, we're gonna present some report findings and then we're gonna hear from all of you. Start out at 20,000 feet. One of the things that our research team did was a canvas of the 120 most important departments, agencies and subagencies within the Federal Government to surface as many AI/ML tools as we could. We can start with a number that we, at least, think is pretty arresting which is of those 120 agencies, departments, et cetera, 45% of them have either experimented with or are actually using AI governance tools of one sort or another. Here's another findings. We manged to surface about 160 use cases, and they span policy areas from law enforcement to education and everything in between. They also span governance tasks. Some of these are what you might expect, but quite important. For instance, a lotta agencies are using machine learning tools to engage in regulatory analysis and monitoring. The FDA, for instance, is piloting some use of ML to analyze millions of adverse drug event reports to predict which drugs threaten the public health. Department of Health and Human Services is developing a tool to help with the procurement process. Government has to purchase millions and, in fact, billions of dollars worth of goods and services, and so there's some interesting ML applications there. Other agencies are using ML tools to communicate with the public. For instance, chatbots to field questions from those seeking public housing, questions from tax payers, questions from asylum seekers and questions from business owners. Finally, there are some AI-based tools used to deliver public services. There's a pilot project, or there was, at the United States Postal Service to think about autonomous vehicles for mail delivery. Those are all interesting examples, but I wanna turn our attention to two more examples that we think are even more consequential, and the reason they are consequential is because we can see in them AI moving to the heart of the redistributive and the coercive power of the state. That includes things like agency adjudication of rights and benefits. Dan'll talk a little more about that, but for now, a really good example is the United States Patent and Trademark Office which is using and piloting some ML tools to decide who gets valuable intellectual property rights. Similarly, several of the really important enforcement agencies are using AI to engage in, what we call, predictive enforcement targeting, that is to say to try to identify those who might be violating the law. We can talk about some other high-level findings. This is for the engineers in the room. What types of techniques is government using? The answer is the full set. It, for sure, skews toward supervised learning not unsupervised, although there is some topic modeling as well within the set of use cases that we found, lots of structured data but also some natural language processing, a small amount of computer vision and robotics. Final overview finding that I can place in front of you, we think, is a really interesting one, which is where do these tools come from? We found this surprising, and it's one that we're gonna return to, is that most of the use cases that we surfaced were developed in-house by agency technologists, not by profit-oriented contractors. In sum, what did we learn so far? Agency AI use, it spans policy areas. It spans governance tasks. It spans AI techniques, and it has many sources. But a high-level canvas of this sort can only tell us so much, and so the second thing that we did with our research team was to do some very rich case studies of particularly salient uses. That's what we're gonna turn to now. - Great, I should start off by just thanking Karen and the organizers. I have just tremendous admiration for the work that Karen does and for initiating this really important set of conversations on campus. I apologize in advance as well. I'm down with a little bit of a cold. I don't usually sound this hoarse, so apologies in advance. The particular case study I wanna walk through involves the future of adjudication. That is, a lesser known fact about the US justice system is that administrative judges actually adjudicate far more cases than all of the federal courts combined. These are places like the Executive Office for Immigration Review where we have hundreds of immigration judges making decisions on asylum cases. The Board of Veterans' Appeals where we have nearly 100 judges making decisions about veterans' benefits. And the Social Security Administration where there are nearly 1,600 judges that make determinations about disability benefits. For decades and decades, one of the biggest challenges in this mass adjudicatory system has been how to ensure the accuracy and consistency of these kinds of decisions. For the Social Security Administration, here is some raw data where each data point indicates the award rate by one administrative law judge of disability benefits. What you can see is that despite cases being randomly assigned within a regional office, there are some administrative law judges that grant awards at 8% of the time and others that grant 98% of the time leading many to decry this kind of a system as a form of disability roulette. Here's the actual file room of one of these federal agencies, the Board of Veterans' Appeals, up until it recently adopted an electronic case management system where it has taken an average of some five to seven years from the time that an appeal is filed until it is resolved, and one estimate has it that about 7% of veterans die while waiting for their benefits to be adjudicated. So there are massive challenges in these systems. One of the really interesting stories of innovation that came out of the work that our team did comes directly from one of these agencies, the Social Security Appeals Council, where there was a judge by the name of Gerald Ray, who is one of those government entrepreneurs, who realized early on at his tenure at the Appeals Council that the SSA simply wasn't capturing any of the data and information necessary to actually use that information to improve the accuracy and consistency and efficiency of how these cases were decided. What Gerald Ray was facing was the fact that he was heading up a unit that was only allowed to hire lawyers to adjudicate cases. And so what did he do to overcome various IT hiring prohibitions? He happened to just find lawyers who also happened to have a master's degree or software engineering backgrounds or just interest in data science, had these attorneys actually decide cases for a number of years until he could start to have them reassigned to actually start to develop basic machine learning tools from within this agency. Here's an early prototype of stuff that they did. They built-out a tool to do clustering, really simple clustering based on metadata, to essentially allow attorneys to decide cases in batches. One of the really difficult things you face if you're an adjudicator at an agency like this is you're crisscrossing from one complex area of law to the next. And simply by clustering, what they found through an internal pilot is case processing speeds increased and error rates appear to decrease. Second, they built-out a machine learning model to predict grants. That is, what are the kinds of cases that are so likely to result in a grant that you could actually skip the need for a resource and time-intensive hearing hence expediting the grant of disability benefits? Kinda the most innovative of these systems was built-out by one of these adjudicators that Gerald Ray hired, Kurt Glaze, which is an NLP-based error detection system, that they call the Insight system. Where basically, any adjudicator can take a draft decision and have it checked for roughly 30 or so quality flags that can range from the really simplistic, that is have cited to a legal provision that does not exist, to the more complicated, like internal inconsistencies between the functional impairment that an adjudicator has identified and the job classification identified somewhere else in the decision. What you might get as an adjudicator is something that looks like the following to please reevaluate given the kind of quality flag that has been thrown by the system. This is actually kind of an exceptional story of innovation within government, but I wanna make two points based on this particular case study. The first is that government often sits on some of the largest data sets in existence with huge unsolved questions that machine learning has not even begun to tackle. In the adjudicatory setting, for instance, the length of legal texts are much more substantial than all of the canonical NLP kind of benchmark tasks, so there's some real work that the ML community and NLP community has to do to be able to grapple with the kind of complexity of legal texts. Then, some of the more recent advances within ML towards transfer learning, one-shot learning, that kinda stuff is really quite important because in this instance, trying to label legal texts is really expensive because you have to hire lawyers or have staff attorneys actually give you high-quality labels. And so there's a real importance on being able to do forms of transfer learning. The second point is really the flip which is on the one hand, understanding these kinds of applications within government pose some really novel challenges for NLP and machine learning, but the adoption of these kinds of techniques also actually raises some really interesting challenges for the law itself. For instance, if the Insight system actually manages to reduce errors in decision making substantially, that actually may cause us to revisit the major emphasis of Constitutional Due Process which for decades and decades has been around accuracy. There's a kind of lost strand of Constitutional Due Process that posits that it's not just about accuracy of decisions. It's not just whether the claimant deserves the disability benefits under the law. There's actually a kind of dignitary value to holding hearings. And in that case, the kind of tool that allows you to skip hearings may be exactly the wrong one because there may be instances where some claimants really don't care about the actual outcome but actually care about engaging with the legal system in a kind of meaningful way. Or as one judge communicated to us, she regularly has litigants come up to her and during a hearing say, "Judge, I know I'm gonna get denied, "but I just wanna be heard." We think it has both interesting implications for machine learning and the law. Second example out of adjudication comes from the Patent and Trademark Office. Here, we actually also see some really cutting edge forms of machine learning. The USPTO has over 8,000 patent examiners, 600 trademark attorneys and also faces these significant backlogs in processing patent and trademark applications. What the PTO has started to do is to actually prototype ways of improving the classification and search of trademarks and patent applications. For instance, one system could enable examiners to search piles of potential prior art helping to cut down the significant backlog of the agency and taking that search time down significantly. On the trademark side, one of the more interesting engines that the USPTO piloted was a kind of trademark similarity engine. And the basic idea is that previously, when you applied to register a trademark, there would be a human trademark examiner who would classify this thing and then try to figure out whether there's already a visually similar trademark so that if this registration were to be granted, there would be trademark confusion within that product category. Here's a simple example of the World Wildlife Fund kind of logo of the trademark similarity engine, pretty cutting edge forms of image learning going on here. The interesting thing coming out of this, I would say, or coming out of these examples from the USPTO are there are two kind of interesting lessons here. The first is that as you start to displace responsibilities by a trademark examiner, there is the worry that a lot of our cutting edge CNN-type models still can be quite brittle. Many of you in the room will know that the canonical example of adversarial learning in the image recognition context also happens to involve a panda. And so if you take a picture of a panda, you add some kind of Gaussian noise, it's possible to actually fool a cutting edge image network into thinking that the object that looks indistinguishable to the human is actually a completely different animal. And so that's the worry is that if you start to displace the exercise of human discretion by these trademark examiners, there is the possibility that sophisticated parties may actually start to be able to game that image engine which could erode trust in the system. Second thing I'll just mention is that the PTO also illustrates one of the really tricky dimensions of the reliance on commercial contractors in the government space. Contractors are responsible for roughly 1/3 of use cases explored by agencies, and in the PTO context, one contractor developed a natural language processing based engine to classify patents for different art units, but that exact same contractor also advertises services for private parties to secure advantageous patent classifications. This is the advertisement from that contractor's website. X has been providing classification results to the USPTO for US patent applications for almost 12 years. We use a unique, flexible AI system that is responsive to evolutions in technology, trained with patent applications classified according to the latest CPC scheme and definitions. That is one of the real challenges here. Customs and Border Protection itself was not able to assess errors with some of their biometric scans because they didn't have access to proprietary source code and couldn't figure out the source of particular errors. That is one of the challenges of outsourcing completely to commercial contractors. - In our effort to make all of this very concrete, let's turn to a second case study then. Let's look at the enforcement context, which is something that I mentioned previously. I already mentioned that a number of the big enforcement agencies are using various forms of AI to engage in predictive enforcement targeting. The IRS is doing it for purposes of determining where to direct some of its tax audits efforts. CMS, the Centers for Medicare and Medicaid Services, is using it to try to find healthcare fraudsters. Even EPA has begun to work or pilot some use of AL, sorry, some use of AI/ML for purposes of enforcing the federal environmental laws. The idea with all of these tools is to shrink the haystack, the pool of potential violators of the law, and therefore, be able to better allocate scarce agency resources in performing investigations or even bringing enforcement actions. But there's also a bigger aspiration here which is to make enforcement fairer, to ensure more consistent decisions about who has to face the power of the state. We'll focus in on the SEC for a bit which is already using multiple tools to enforce the federal securities laws and some of the tools they use are relatively straightforward. You might even be able to predict what they are. One of the things the SEC does is it has to enforce insider trading rules. There, you have a bunch of very structured transaction data, and so the SEC is using some machine learning to try to identify the folks who are doing better in the stock market than they should be. But some of the tools are more sophisticated than that. For instance, the SEC is using some NLP to parse unstructured narrative disclosures like these to predict which investment brokers are violating the securities laws. And so you can train a model on past disclosures, past investment brokers who were ultimately sent to the enforcement arm of the agency and then you can try to predict among the current set of registrants, folks who are sending in these disclosures to the agency in order to register as investment brokers, you can actually try to make a prediction as to who might be the bad apples so to speak. For a lawyer like me, these enforcement tools are really interesting on two counts. First off, I think they raise some really interesting technical challenges. We all know that a machine learning tool can't be better than its data inputs, but the challenge in the enforcement context is not just good data. There's also a challenge that relates to the dynamic nature of wrongdoing and the fact that enforcement by a regulatory agency actually resembles, somewhat, that old carnival game of whack-a-mole. Tax offers a really good example of that. We all know about tax shelters. There's a whole industry around tax avoidance. The problem is that once the IRS moves against a particular tax avoidance strategy, the industry shifts away to a different strategy, and so the result is that the agency has to engage in constant iterative updating of its models in order not to focus on last year's artifice. Worse, the agency could create feedback loops, as with the predictive policing models that send cops back to the same neighborhoods over and over and over again and can then lead a police department to arbitrarily concentrate its enforcement on a subset of violations. That's one type of technical challenge. Another really interesting technical challenge is how does an enforcement agency, or any agency really, seize the technical frontier? Here, I can plug one of Dan's projects because he's actually prototyped a computer vision system that can help with enforcement of the Clean Water Act by predicting which facilities are concentrated animal feeding operations and so might be violating the federal environmental laws. All of this raises a really interesting question that we'll talk about a bit more in a moment. It's a challenge that all agencies face which is when they wanna build capacity, do they make that capacity themselves or do they buy it? That's the in-house contractor thing or idea that's come up a couple of times. As we'll discuss in a moment, there's a really significant tension between the advantages of having embedded expertise within an agency and contracting out for needed technology. That's one type of challenge that I think can really be highlighted by a focus on agency enforcement, but here's a second type of challenge which is there are some really interesting legal puzzles here. Major theme of our project over the past year has been this very basic collision which is, on the one hand, the law that actually governs how agencies do their work. It's built on transparency and reason giving. The idea is when government takes actions that affect our rights, it's supposed to explain why. But many of the more sophisticated AI tools that agencies have started to develop are, by their structure, not fully explainable. And the result is we have this basic collision. We have this basic collision between the legal requirement of reason giving on the one hand and the black box nature of some of the tools. But there is a further problem in the enforcement context which is administrative law, this is the body of law that governs how agencies do their work, has long hived off enforcement decision making from judicial review. It's a case called Heckler v. Chaney. There are lots of reasons why the Supreme Court did that in Heckler. One reason is we might worry about generalist judges second-guessing agency enforcement decisions, especially when those enforcement decisions are based on budgetary concerns. The agency's trying to decide where to put its scarce resources. But the court also worried that when an agency is rummaging around in that haystack and trying to figure out who to enforce against, that there really isn't a focal point for judicial review. For me, the really interesting question then is whether the uptake of these new algorithmic enforcement tools make that situation worse or better. The worse is easy to see. It's the opacity of some of the more sophisticated ML tools being used by these agencies. But they might also make it better. And the reason is that algorithms actually encode law and encode agency priorities in ways that actually might make them more tractable, might provide that focal point that that court in that case Heckler wanted in order to second-guess an agency's enforcement decision. Algorithms might also qualify as rules that under administrative law might be subject to something called notice and comment, which is the process of political ventilation that an agency has to go through anytime it promulgates a new regulation. There's some limits here, especially in the enforcement context because full disclosure of the technical guts of a tool kills the tool. Disclosure might make sense when an agency is adjudicating social welfare benefits, but it probably doesn't make sense when the IRS is using it to find tax cheats 'cause, indeed, disclosing the details of the tool simply helps the tax cheats cheat. The challenge and the opportunity, I think, is for lawyers and technologists to work together in this enforcement space in an effort to achieve meaningful accountability. And if they do it well, then it's possible that they could produce an enforcement apparatus that is on-net more accountable in terms of who has to face the course of power of the state than in the current system built around a dispersed set of human prosecutors. - We wanna leave plenty of time for questions from you all, but let me just close with a few implications from the report. One thing that we highlight is that we really do think it's important for there to be substantial in-house expertise within government agencies to get this kind of stuff right. When we talked to a number of these officials, 'cause a huge part of the work here was engaging a wide range of agency officials as to their pain points, what most of them will tell you is their problem is not in finding somebody who can code up a hyperparameter search using Scikit-learn. That's not the problem they have. The problem they have is somebody who both knows what tools are available to solve a range of different problems and who can learn enough about the institution to actually even know what problems are worth solving. That's their real pain point. That was sort of the genius of having Gerald Ray hire Kurt Glaze and have Kurt Glaze decide cases for several years before even starting to develop the kind of NLP toolkit for the Social Security Administration. Or as Kurt put it himself, quote, "I developed the flags "that I wanted to have available as an adjudicator." He knew exactly what his problems were in actually trying to decide these cases, and that's what he ended up building out. Government contracting can be a real challenge in this space. One of our contacts was a deputy council of policy and procedure at one of these agencies and was tasked with one of these major IT modernization projects reported, quote, "If people come in for two weeks, do a bunch of surveys, "go off site, spend two years building something "and then present it as a finished product. "It's going to be a disaster." The quote goes on and says, "This happens all the time in government." Another example comes from the PTO where the PTO actually had a prototype that was actually using pretty good search technology to build out what they called the Sigma system to empower patent search. And when they actually started using it with some potential patent examiners, the only patent examiners that were actually able to take advantage of all of the capacities of the Sigma system were those that had a computer science background. And so it's really important to have that blending of technical substantive and institutional expertise. That said, there are these trade-offs that David alluded to. Over half of the use cases were developed internally, but our team also tried to rate the level of sophistication of each of these use cases, and there are two things to note there. One is that less than 15% of the use cases could be rated as high in sophistication by Stanford engineers, so there's a fair bit of work to do to still close the private/public sector technology gap. Then, based on these public sources, agencies did not provide sufficient detail the vast majority of the time suggesting quite a degree of puffery by these agencies, which is what led us to do these in-depth interviews and these case studies. Third takeaway is what David already alluded to which is that notions of transparency and accountability are really fundamentally gonna have to be context specific. So if we think about the push, as what happened with the New York City Commission, for source code transparency, that might work really well, as David alluded to, in the disability benefits setting where you want to be as transparent about the law as possible, and you want the potential claimants to really know what the state of the law is. But source code transparency might work much less well in the Securities and Exchange Commission context where all that that would be doing is facilitating gaming by sophisticated parties. And so we at least offer a kind of idea really premised on the fact that right now is really the time where most agencies are starting to adopt these tools, and just as in standard machine learning, we would reserve a random test set. We think there might be real value to a form of benchmarking where you basically reserve a random set of cases that are worked up the analog way, think of it as a human alongside the loop, to be able to smoke out when the algorithmic decision tool might be going wrong. The bigger question we're after in a sense is how do we actually make sure that government is prepared for the AI revolution? While we think there is great evidence of innovation within government, our report also comes to the conclusion that government is behind pretty significantly in terms of the adoption of AI. And for far too long, the perception has been something like the following which is that Stanford is over here. There's a huge cliff and then here is government over here. And that's part of what we at the Law School are doing in the Regulation, Evaluation, and Governance Lab, or the RegLab, is really founded to try to build that bridge where we partner explicitly with government agencies to try to foster that kind of exchange both to learn what the most important problems are that we need to solve for government and what the most important ML problems might be that haven't yet been tackled by the community here that are really going to drive the future of AI in government. - One final, call it a shameless plug or call it an effort at the Law School to think how we can build the kinds of capacities that we need to improve government use of AI but here we go. There will be two classes in the spring. One of these, AI and Rule of Law: A Global Perspective, will be taught by me and by someone named Marietje Schaake. She is a new arrival to Stanford. She was, until very recently, a member of the EU Parliament, and she is a leading voice on tech regulation. We will teach this course together, and it will be all about the many really interesting governance questions, including questions around government use of AI, and we hope to draw students from all around the University, not just the Law School, not just the Engineering Quad but everywhere to begin to develop the sorts of fluencies we think students are gonna need. The second class is actually of a piece. It's titled Digital Technology and Law: Foundations, and this is a course that I am, I suppose, convening in my role as an associate dean, and there will be, it'll be a really interesting and unique interdisciplinary course. There will be lots of different lecturers from the Law School and from the Engineering Quad who will come in, and the whole point of the class is to develop these fluencies that I just noted. That is to say, teach the lawyers some engineering, teach the engineers some law, and this goes back to something Dan just noted. It's all about building the capacity within students at Stanford to move forward in terms of thinking about how government can better use some of these digital tools. I think that's it, so we can move to a Q and A I think, and thank you again so much for having us. This has been a real treat for us. (audience applauding) - [Woman] If you wouldn't mind repeating the questions after they're asked. - Sure, so anybody who has any question will please just raise your hand. Yes, in the front. - [Man] I've seen that in tech there's kinda this role of product managers where there's people that have business problems and then there's perhaps product managers or an in between person that can translate that into the language of data scientists who then are, you have a very specialized core team that perhaps doesn't always have as good of a grasp on the problem. So have you seen something like that in government, or what do you think of the applicability of that model to some of these problems? - Just to repeat the question, in private industry, there are product managers who can often serve a little bit of a translational role between the substantive needs and the technical teams, and have we seen something like that in government to solve some of these challenges, David? - Do you want to, I have something to say, but do you wanna take it initially? - Sure, there are (coughs) some units like this that have been set up. I think it's called the Analytics Center for Excellence that was established at the SSA. That is a team where they basically tried to take a bunch of the engineering data scientists and put them under one roof and then have a customer service oriented relationship with the various other business units at SSA. Whether those teams have been successful I think still is something we have yet to see. Part of the challenge that government faces is that it just can't compete with Silicon Valley salaries, for instance, for top data science talent, so there's a huge question of how to actually recruit people, how to retrain people within government. There have been some nice moves made on the kind of hiring and retraining. So for instance, there was a new job classification where it's now possible actually to take job titles and add the parenthetical data scientist to it, and as a result, one expectation is that this is gonna help some agencies actually recruit data science talent to within the agency. But it is a real question because we think it's gonna take a major investment really to build up this kind of capacity. One of the challenges with the executive order on AI is while it said a lot of things about how important AI development is gonna be, it did not say something very explicit about the actual funding that is required to actually build this kind of capacity. David, why don't you take it. - Great, yeah, so it's a terrific question. I think that in an ideal world, you would indeed have precisely those sorts of folks in government who can do that all important translation exercise as between, not really the business people, but the sort of maybe even the political appointees who are thinking about delivering policy benefits of various kinds and the technologists on the other hand. I think it's important to underscore that a lot of the efforts that have produced the tools, at least some of the tools, that we profile in our report are not terribly organized at the agency level. That is to say there isn't as much entity-level organization as you would want. Instead, what you have is you have an agency technologist, maybe even an agency economist, who can pick up enough ML on the weekends to develop a bespoke tool of some sort to solve a governance problem. We've seen that time and again. And so I think this just makes the broader point that this is early in this process. There are these really interesting and important capacity building questions, and it'll be really interesting to see in the coming years how government manages to deal with this. The other thing I'll say is, and this is an example from the SEC context, but I think it's actually really illustrative and maybe even responsive to your question, which is that one of the challenges that the technologists at the SEC told us about was this problem that they can develop a tool, and it can produce some machine outputs that might help the line level prosecutors, the line level enforcers who actually have to decide whether to go after a particular entity or not. But at the SEC for the moment, the use of those outputs is still voluntary at the level of the line level enforcer, and so these technologists actually spend a lotta time thinking in translator terms, which is trying to figure out how to make the UI as user friendly as possible and also in a sense selling those line level enforcers on the use of the tool. What's most interesting about that, of course, is that that NLP tool I showed you that parses those narrative disclosures, those line level enforcers, they don't actually, they're not impressed if the technologists come to them and say hey, this threw a flag, go after them. They actually wanna know which part of the disclosure threw the flag and why. And so there's actually this like built-in demand for explanations in this system, and so it's actually forcing the technologists to think in that translator way about how to design tools that are actually explainable. In our report, we talk a little bit about how this might be an example of internal due process, not external court enforced due process, but a kind of internal due process where the demands of actually getting these tools into the hands of the right people and in getting them to use them produces a sum measure of accountability. - In the back, over there. No, you, yes you with the ponytail, sorry, yes. - Me? - Yes. - [Woman] Oh, okay, hi, I was actually wondering a lot of the discussion around the bridge between Silicon Valley and the government is a lot more going through it in the other direction. How can the government be regulating AI? There's a really interesting question because right now the government isn't able to keep up with the technologies as they're being built. You look at the example of, or and just in general looking at the data bias, so like a facial recognition company that's working off of data, a lot of times has natural bias usin' it, has a lot harder trouble recognizing black or brown people for example, and things like that. But when you're looking at how those technology are gonna be employed in government, there's gonna be sort of a government, generally, there's a lot more regulations on how the government can use data and avoiding those biases. So do you think that there's potential for, as those technologies get incorporated into the government and get regulated in the government, that that'll help us develop eventual regulations on actual Silicon Valley? - Yeah, so a really nice question that kind of puts this project in contrast with the conventional debate around how should we govern AI systems? How should government regulate AI when AI within government may be subject to more constraints, like the Data Privacy Act that applies to federal regulatory agencies, and might that actually become, potentially, a kind of model for how we should actually regulate AI? David, I don't know if you want to take the first go. - Yeah, it's another really terrific question. There is this concern that the government can't keep up. I guess, the one, well, two extra things I'll say beyond Dan's response is that I think the challenge that you're articulating is yet another virtue then of trying to go in-house. It's of actually developing these tools through embedded expertise 'cause, obviously, that expertise has some spillover effects and it also, at the same time, builds the capacity of the agency to regulate AI out in the world. Really good example of that is the FDA. We noted that the FDA has piloted some tools to analyze those adverse drug event reports, but the FDA is also increasingly seeing products that have AI embedded in them that then they have to make premarket determinations as to. And so that's just a really clear example where the development of this capacity kind of raises various boats within the agency and can help the agency to do its job. The last thing I'll say is that in our report, the way we put it is that one of the real challenges here is avoiding a situation in where agencies myopically focus on getting that next tool through the contractor process rather than building that internal expertise, and that's precisely because that internal expertise actually has all of these spillover effects. And yet, in times of fiscal austerity, there's that pressure. There's that pressure, we've got a problem. We need a tool. Let's get it from Silicon Valley rather than actually creating that capacity that can serve, that can produce many different benefits within the government. - Just to kind of follow on that, I do think it's a really nice connection to draw. Both David and I teach the body of law that constrains government agencies. It's (coughs) dryly referred to as administrative law, but it's basically the law of government. You're quite right to intuit that there are more constraints on how governments can use data. A lot of administrative law is essentially about what within like the FADML community would be thought of as a right to contestability that is in procedural due process. You get to challenge whether this procedure is actually promoting accuracy in some way. And so I think there is actually really interesting potential for that cross-fertilization to go from thinking about the safeguards that government thinks of when it is adopting AI tools to how to actually think about the future of AI regulation more generally, so it's a great question. Yes? - [Man] Do you know if there is anything that Stanford could do to encourage more of its technical students to consider careers in public service because a lot of these technological advancements, they do originate at the University, so I feel like Stanford, more so than any other university, has a greater social responsibility. - Yeah, so the question just so everyone can hear is can Stanford do something more to really ensure that this kind of technical, this technical innovation actually gets driven out into the public sector? I mean, I'll take a first stab at it. I think the short answer is yes. I think a lot of the energy coming out of this last part of the long range planning process has actually been about how the University can be a more engaged research university. One of the interesting challenges for Stanford has been that we don't have a public policy school that, for a lot of other schools, is the natural nexus by which to do some of that work that really is much more closely tied in to the world of government. And so part of the way we're doing it at Stanford is through these kinds of impact labs, and these policy practicums that we run at the Law School, for instance, that have often times nonprofits or government agencies as clients. The first one of our policy practicums was run by our beloved colleague Joan Petersilia, and the client was then Attorney General of the State of California Kamala Harris. And Kamala Harris walked in and said, "Hey, I have 20 data sets. "I don't have any capacity to analyze them. "Can you help me answer some basic questions "about California criminal justice policy?" And so I think there is this real appetite by policy makers, and we are very much in the process of trying to figure out how to build the right institutional vehicles to empower Stanford faculty and students to actually do that kind of impact-oriented work. It's not gonna be a policy school, but it's gonna be through something like these policy practicums, these impact-oriented labs. The Haas center is doing some really interesting work and is actually also now just announced to kind of sabbaticals-in-service program for faculty members to actually spend time in government to learn more about those kinds of problems. Because historically, we probably have not been, we've probably made it a little bit easier or at least there's been a greater rate of people spending time at a company than within government, and so the sabbaticals-in-service program is a really nice push toward that. But I think there's definitely more that we can do as a university, David. - When we stood up the policy practicum, we basically had to convince a bunch of engineers to come join, and I wasn't sure how to make that pitch. One way to make the pitch would say hey, come over here and learn some law because you'll figure out where the legal and regulatory speed bumps are for when you go into the private sector. But I quickly, I think, realized that that wasn't what was driving folks over to the Law School and expressing interest in this project, that there was actually real appetite for thinking about how to make the world a better place. That sounds like a terrible cliche, but there was a lot of that. And so I think that there is appetite within the Stanford student body, in direct answer to your question, and I think what we need to do is we need to build infrastructures. We need to build programmatic structures, things like the Policy Lab Program over at the Law School, that provide a convening point for people who are interested in a bunch of really interesting questions. And these questions have both really difficult legal puzzles built into them and really difficult engineering puzzles built into them, and I think I can speak for Dan when I say that this teaching experience that we had as part of this project was just really gratifying because we got to see law students and engineering students get together and really have to talk across that law/tech divide, and it ended up being a really rich and productive set of conversations. - [Woman] This is great and just wanted to thank you all again for coming today, a big round of applause. - Thank you. (audience applauding) 