 [Music] hi i'm rich carwana machine learning researcher at microsoft research here in redmond seattle and we've got an exciting session with three talks for you the session is about saving lives with interpretable machine learning using interpretable methods in machine learning in healthcare and i'm going to give the first talk and i'll basically be talking about how if you're using blackbox methods in machine learning for healthcare that can put people at risk and interpretable methods can help us understand what the model has learned repair the model when it needs to be repaired and even do things like learn from our data how we might be able to do better health care then after my presentation uh anker tara desai who's from the university of washington in tacoma and ankara is also uh the creator of a company that's here in the seattle area that does uh machine learning for healthcare uh anker is going to talk about fairness in machine learning when it's used for for health care so how to prevent bias detect bias and those kinds of issues um and then the last talk is going to be by marzia gassami who's a professor at the university of toronto uh marzia is also one of the cfar uh canada chairs uh in in uh in toronto and marzia is going to talk about you know explanation and ai and machine learning when it's used for health care and that things aren't always as easy as you think and that in some ways uh explanation might actually be a little harmful and in other ways it might be helpful so so she's going to give us a presentation on that so anyway let's get started with the session we're going to have uh three recorded talks each will be about 20 minutes long and then at the end we're going to have a 30 minute session for q a so please stick around at the end come up with questions you can type them in the chat window let us know what you'd like to talk about and we're happy to discuss all of that at the end hey thank you very much i hope you enjoy the presentations see you soon i'm rich carwana and i'm going to talk about using interpretable machine learning to save lives in healthcare before i start i want to thank a number of people who have worked on this project with me over the years in particular four great grad students yin liu sarathan shuzhou zhang and ben langrage a surprising number of people in machine learning believe that if you train a deep enough neural net on a large enough data set and if it looks accurate on the test set it's just safe to deploy the model there are some settings where that is correct but in critical settings like healthcare this can be very very risky the fundamental problem is that machine learning thinks very differently than humans on the left is a panda that a deep neural net has pretty high confidence in we add a little bit of noise to it that the model thinks might be a nematode and then we get this new image on the right and now the deep neural net is very confident that this is a gibbon i can't even see a difference in the image from the left to the right and yet the model has completely changed its mind more importantly i don't think any of the seven billion humans on the planet could see this difference either obviously the deep neural net works very differently than humans do this is an image that the model is pretty confident as a cheetah is a robin is a red shank is a lesser panda and i'm also pretty sure that none of the seven billion humans on the planet would have classified any of those things with those those categories so again the deep neural net obviously is thinking differently than we do finally here's an image that the model is very confident is an electric guitar or as a baseball or a starfish or remote control and the model is doing something really interesting it's capturing the rhythm of electric guitar the stripes of the baseball some of the pattern in a starfish and the buttons on the remote control but it's also missing things that are obviously important such that the remote control is a small device you might hold in your hand or the starfish has legs or the baseball is a round spherical object clearly these machine learning models while they could be very very accurate think very differently than humans do the problem is that artificial intelligence really isn't artificial intelligence from another planet but we want to use it here and it's going to make errors that are very different from the kind of errors that humans would make it's not going to make predictions the same way doctors would so we need to be able to understand the machine learning model to prevent risky mistakes and possibly to discover new things that the model has learned from the data here's how i got involved in this back in the mid 90s i was asked to train a neural net on a pneumonia risk problem the goal was to distinguish low risk patients who could be treated as outpatients from high-risk patients who definitely had to be admitted to the hospital ten percent of the pneumonia patients did die from their pneumonia the neural net eye train was the most accurate model anyone could train on this data set and the question was is it safe to use it on real patients i mean it's a black box this neural net we decided no and one of the reasons was that somebody else doing rule-based learning learned the rule that if you have a history of asthma it lowers your chance of dying from pneumonia that is asthma looks like it's good for you protective if you've got pneumonia now we asked the doctors about that because that didn't make any sense to us and the doctor said wow asthmatics are higher risk from pneumonia not lower risk you have to fix the models but it might be a real pattern in the data and their explanation was that asthmatics notice the symptoms of pneumonia sooner they get to health care faster and then they receive very high quality treatment because they are actually considered to be at higher risk and this rapid high quality treatment actually lowers their chance of death compared to the general population we assume that if rule-based learning learned that asthma is good for you the neural net probably did too but that's not the only reason we were concerned about using the neural net on real patients what else might the neural net have learned that's equally wrong like that asthma is good for you but we didn't have a rule-based system that learned these other patterns and warned us about other things that might be lurking in the neural net that were wrong so we were afraid to use the black box neural net on real patients because we didn't know what other things might be in it this is a fundamental problem in all data sets not just machine learning data sets the problem is confounding asthma is acting as a proxy for two very important features that we're missing time to care and quality of care and this problem is ubiquitous every data set you're using is missing important features that you just don't have access to even important features that won't be discovered until the future for example your genome is not in your data set yet can cause confounding on the features that are currently in your data so you can't escape this problem one way around it would be to run randomized clinical trials on everything but we're not going to do that for problems like asthma where we already know how to provide good care we're not sort of going to kill a few asthmatics or randomize their treatment just to train a better machine learning model that's not ethical so we have to learn to live with the data the way it is unfortunately you might think we could just clean the data or detect these problems in the data but that's actually much harder than detecting them in the machine learning model so we're not going to go that route and although i won't talk about it today this kind of asthma bias that we're seeing in the models it's very similar to race gender socioeconomic bias so the problem is that the bias is in the training signals it's not in the learning algorithm and the learning algorithm is just learning from the data and that's where the problems usually lay one way around this would be to have a machine learning model that's as accurate as a neural net but as interpretable as say rules or logistic regression you may have seen a graph like this which suggests that there's a trade-off between accuracy and intelligibility very accurate models like boosted trees and random forests and neural nets are not very intelligible and more intelligible models like linear regression unfortunately are not as accurate as we would like on complex data so we're looking for a model in the upper right hand corner here that is both very intelligible and very accurate the good news is that for tabular data we actually have an algorithm now sitting here and it's generalized out additive models with pairwise interactions so that's what we're going to talk about today let me just show you briefly what that is here's linear models this is a weight times a feature plus another weight times another feature that's a linear model here's a full complexity model just an arbitrary function of all the available features fortunately there is something in between and that's what's known as additive models gams here's a function of the first feature plus another function of another feature plus another function of another feature we have functions of features that are more complex than just multiplying by a weight they can be arbitrarily complex they can be multimodal and non-linear but they're still only functions of a single feature and that greatly restricts what this model class can learn and also makes it intelligible we can make that a little more complex this is the sum of all functions of individual features or we can add pairwise interactions sum of functions of pairs of features or all the way to three-way interactions and higher functions of three-way combinations of features and more we're going to restrict ourselves though to main effects functions of single features and a small number of carefully selected functions of pair pairs of features and we're going to restrict ourselves to that because this remains intelligible and then our goal is to get maximum accuracy out of it now we didn't invent gams they were invented by stanford statisticians in the late 80s uh but they were very conservative when they fit the models and they didn't have the kind of horsepower and modern machine learning methods that we have now so they didn't get as much accuracy and intelligibility out of these things as we're able to do now our contribution is that we're going to put these things on modern machine learning steroids and in the process we're going to get higher accuracy much higher accuracy we're going to get improved intelligibility and we're also going to get editability and that's a great combination of features this is the pneumonia data set collected in 1989 that motivated all of this work so we have a number of features like age gender if the patient has cancer or asthma patients has a physical exam we know their temperature what their breathing sounds like what their heart rate is there's been a blood test so we have the lab findings from the blood test for the patient and then all these patients have had a radiologist read a chest x-ray so we have notes from the chest x-ray now the model class is a function of a feature plus a function of a feature there's 46 features so we're going to have 46 functions and then we'll throw in 10 pairwise interactions as well although i won't talk about those today so in the end we'll have 56 functions here's what it looks like from a high level we're going to have a function of age represented as a graph a function of asthma a function of your respiration rate a function of your blood urea nitrogen and the way you use these 56 functions is you read the patient's age on the x-axis write down the y-value so it might be minus 0.23 and that's what this particular function predicts then we go on to asthma we write down if the patient does or does not have asthma write down a number we're going to get 56 numbers like this you add them all together the more positive the score the higher the risk the more negative the score the lower the risk these are basically log odds so we use the logistic function that's used in logistic regression all the time to convert this score into a probability in this case a probability of dying from pneumonia it's just 1 over 1 plus e to the minus score so if a patient had a sum of scores that was equal to minus 0.78 then their probability of death would actually be very high it would be about 30 chance of dying so you want to be a minus 2 or minus 3 or minus 4 on this scale this is what the model learns about age so i've just zoomed into one of the 56 graphs so the bottom is a histogram of the distribution of our patients most patients are between 60 and 90 some patients younger than 50 20s 30s and 40s and a few patients at around 100. the top is the risk model that's been learned by the generalized additive model and it's learned this in parallel with the other 55 graphs that is it's learning all of these things at the same time this is not marginalization if you're familiar with that so let's read the graph remember down is low risk highest is a higher risk it's good to be young and the model is saying that young is anything under say 50 years old so when it doesn't distinguish between people in their 20s 30s and 40s it just sees them all as being equally low risk risk goes up slowly as you go through your 50s into your 60s rises pretty rapidly around 66 67 68 69 that's retirement then it continues to rise rapidly as you go through your 70s into your 80s there's a surprising step function rise in risk right at 85 then a surprising flatness in risk in the upper 80s and 90s and most surprising of all is a drop in risk that's happening right around 100 and 101 so let's talk about that we're pretty sure this rise in risk is due to retirement you might have thought retirement is good for you but in many medical data sets we see that when patients retire their risk actually goes up a little so this is consistent with other findings this rise at 85 is surprising 85 is a very round number and we're pretty sure what's happening here is that doctors are giving a little less aggressive care to patients who are over 85 as opposed to patients who are under 85. this data set's from 1989 it's a very old data set and back then patients who were over 85 were definitely considered to be on the elderly side of being elderly and pneumonia was even sometimes called the old man's best friend the idea being that you have the illnesses associated with being this elderly because of those illnesses you've gotten pneumonia and now within a week it's possible that the pneumonia is what takes you out and perhaps that's a friendlier way of passing than it would be to have a sort of longer wasting illness associated with the other comorbidities associated with being so elderly the flatness and risk might be due to what's known as successful agers people have good genes and if they've made it into their 90s perhaps they're really at no higher risk from pneumonia at age 95 than 90 but we're not sure if that's an explanation for this flatness or not the drop at 100 though is really surprising so why would risk go down right at 100 it's likely not a biological effect it's probably a social effect just like this rise in risk at 85. what doctors tell us is when they've got a patient over a hundred who has a treatable illness such as pneumonia they just don't want to give up on this patient like they just keep fighting if the patient needs a second or third round of antibiotics they go for it and they'll do anything they can to keep that patient alive because the patient has made it to being 100 they're a centenarian and that's a really remarkable thing especially back in 1989 so we're pretty sure that this is a social effect here where doctors just don't give up on very elderly patients who have made it to 100. now an interesting thing about this model we've been talking about detail that's on just one of the 56 graphs that form this model i'll show you two other graphs this model despite all this detail we're able to see in the graphs is actually just as accurate as the neural net in fact it's slightly more accurate this is the most accurate model we know how to train on this data so that's kind of remarkable that we can get all of this intelligibility about the model really understand how it's making its predictions and what it's learned from the data and we've been able to do that without any loss in accuracy so that's great now a question how many people actually think that we should predict that a patient who's 105 is lower risk than a patient who's 95 i mean does that make sense biologically probably not we think this is because of the extra care the centenarians are getting so we can fix that if we want by enforcing monotonicity in the model we can the model is a series of graphs so we can edit those graphs and if we edit the graph this way then risk will slowly go up for all patients above 85 and the model will just do what we say now an interesting question would you always want to edit that graph well it turns out if you're an actuary whose goal is to predict just which patients are going to live or die you don't actually plan to intervene in their health care in any way then you're probably happy with the original model because it's telling you something about the survivability of patients that you're ultimately responsible for paying for their health care so you might want to keep the model the way it is and just benefit from the fact that you've learned now that patients over 100 are actually a little more likely to survive than patients in their 90s or maybe you're even happy to use a black box model like a neural net and you just want predictions though there probably is an advantage to learning this from your data that's very important so model correctness and whether it's safe to deploy the model depends completely on how the model is going to be used the original model is good for health insurance providers but not if we're going to use it to decide what treatment to give patients so that's very very very important let me show you a few things this model learned first of all jumps that we see in round numbers or numbers that are very meaningful like age 18 and 21 in the us are almost always due to human social policy effects they're not phase changes that are happening in the biology or physics of the world the model learns that asthma is good for you so the asthmatics are on the right here and notice that the graph goes down remember that's how this whole whole problem started for me so the model learns this and we can fix this now by just editing the graph to make the effect just be completely flat that is we can remove the asthma effect by making the graph go straight across or a doctor might say no i want the asthmatics to be higher risk i want the graph to go to plus 0.5 for the asthmatics and if they tell us to do that edit we can do that and then the model will now predict that as medics are higher risk not lower risk but there are things that the model has learned from the data that are actually more surprising than the asthma is good for you it learned that a history of chest pain and a history of heart disease are both good for you so the heart disease patients are over on the right and that's even better than having asthma and we think the explanation is exactly like the asthma explanation patients who have a history of chest pain and heart disease are afraid when they start getting pneumonia that those symptoms mean they're actually having a heart attack and because of that they get to health care very very fast usually going in through the er whereas the asthmatics don't usually go in through the er so these patients get to health care very very fast and then they get high quality care when it's determined that they're not having a heart attack but that they have pneumonia so that means that their risk is even lower than say the asthmatics but there's something even better than heart disease and asthma and that's a kind of wheezing called strider that doctors are trained to recognize that indicates one of your airways is is actually obstructed and the reason why this is good for you is because patients could ignore that their asthma seems worse or that they might have symptoms that might feel like like heart disease but you can't really ex ignore the the wheezing that comes with strider i mean it is so loud and obvious that if they were in a meeting with other people the other people in the meeting would tell them to leave and right away go to the hospital that's how bad strider is you can't ignore it and the good news is even though your pneumonia is fairly progressed for you to have strider the symptom is so strong this forces you to go to health care and get treatment and if you get treatment early enough that's always good for you now here's an interesting thing the model is additive so it predicts that a patient is 105 who has a history of asthma heart disease recurring chest pain and now has stridor is actually one of the lowest risk patients moreover it's rewarded with extra high accuracy on the test set for predicting those things and that's because the test set looks just like the training set it's just a random draw from the training set so the model is being rewarded for making predictions that are terribly wrong so we have to be able to see these things in the model and fix them you can't trust accuracy to tell you whether the model is good or not because it'll actually be rewarded for doing unusual things like this by the test set so it's good we didn't deploy the neural net back in the mid 90s because it probably had learned these very same things in its 10 million weights but with this kind of model it's like having magic glasses we can see these effects in a really accurate model and then we can repair the effects if an expert like a doctor tells us that we should by the way you might have thought we could fix this problem by removing say the input variable for heart disease from the model how can it predict your lower risk if you have heart disease if it doesn't know you have heart disease and that doesn't work usually what happens is the bias is coming in the training signals the fact that the heart disease patients really do die less often is in the training signals not in the input features input features aren't usually biased so what happens if you remove that input feature is because of correlation with all the other features that you have to keep in the model it'll still do its best to learn to predict that heart disease patients and asthmatics are lower risk so what we do instead is we make sure features like this that we're worried about stay in the model we let the model learn the worst thing it can on those features and then we can detect the problems that are in our data and then we can correct the model by editing the graphs now intelligibility doesn't only help us detect problems with our model it can help us create new science learn from our data this is a graph of risk as a function of blood urea nitrogen a blood test all of us have had and doctors really liked this graph because they could see the impact of their treatments in the graph notice that risk goes up starting at about 35 and levels off right at 50 and then risk continues going up if your bone is higher and then levels often actually drops right at 100 and those are treatment effects so the doctors start to treat high blood with a medication at about 50 and you can see that medication is effective because risk flattens off and then at 100 they start to do dialysis on patients and that's so effective that all the patients say at 110 or 115 who are getting dialysis are actually a little lower risk than the patients who are at 90 who are healthier patients to begin with so the doctors were thrilled that they could see these treatment effects in the graph but they were concerned because they noticed risk is very high by 35 and they wondered if they shouldn't change this treatment threshold from 50 maybe down to 35 or 40 and this made them even more concerned the fact that patients who are less healthy with abundant 115 or lower risk than patients at 90 who are healthier really didn't sound like a good treatment strategy to them and they thought maybe they should move this treatment threshold for dialysis from 100 down to say 90. because it's an additive model we can treat it almost like a spreadsheet and do a what if analysis we can conservatively estimate that if we were to move the treatment threshold say down to 85 that patients would look like this red line or perhaps below the red line that that would be the actual risk profile and the area between this red line and the original green graph those are people dying so that's probability of death that the model is predicting so we can estimate that by making this one change to this one treatment threshold on this one graph for one illness for pneumonia that we can save as many as 2500 lives per year in the united states so that's really remarkable to be able to see that from your graph now treatment effects abound in all data this is systolic blood pressure notice there's an effect at 150 there's an effect at 175 200 225 we see this sort of thing all over the place in data and you really want to be able to see these things to determine if you need to fix them or not now let me show you something more recent so we've been working on covid19 which is an illness that doctors don't even know about so now we can use what we learned from data for covid19 patients to help tell doctors how the disease works notice that the highest risk factor is kidney disease which has been widely reported in the literature we won't talk about chronic bronchitis and parkinson's because we have very few patients with those conditions but we have a lot more diabetics and hypertensive patients in the data and notice that those are also significant risk factors and then there's something very surprising which is that asthma or people with pulmonary disease active cancer or coronary heart disease these all seem like pretty low risk patients and other groups that are studying covid19 are finding this and we don't know whether the explanation is like asthma and pneumonia that these patients are just getting better care more rapidly or whether in fact there might be something protective about asthma and copd for example for covid19 patients so that's something we're actively looking at let me wrap up we definitely want to use machine learning to save lives in health care but high accuracy in a test set does not tell us enough uh for us to safely use a model in something mission critical like health care so these treatment effects are all over the place in health care because the patients are all receiving treatment already for their conditions and that affects the data that you collect and these models are rewarded with extra high accuracy on test data for doing stupid and risky things so you need to be able to see what the models are trying to do and this sort of retrospective test set analysis realize it's always optimistic ultimately you need to be able to do a prospective analysis but before you do that to be safe to patients you want to see the model and edit it if necessary the bottom line is the data is always wrong in surprising ways that you really can't anticipate and intelligibility is your way of getting past that obstacle these intelligible generalized additive models with pairwise accuracy really do give us accuracy intelligibility and editability comparable to any other model on tabular data and we open sourced a package for training these models a year ago so we encourage you if you're already using random forest boosted trees uh simple neural nets or logistic regression we think you would be better off actually using these generalized additive models the bottom line is machine learning really is a foreign intelligence it's an alien it learns and makes mistakes differently than humans and intelligibility is the main trick that we know how to use to help correct this alien intelligence when it does things that would be risky in health care so that we can safely deploy hospitals and save patients lives thank you very much thank you rich for the invitation and microsoft frontiers in ml conference for this opportunity to participate and present my work fairness in healthcare is a complex topic and it's a major topic in making the use of ai more responsible and accountable in healthcare today while the topic may be complex and very much in the limelight we will focus on a few key issues which are in the news due to recent events and i wish to share the components of a working solution framework for unbiased healthcare ai and i'm hoping to both educate and inspire you on the key issues in this field but first let's get a working definition of healthcare in our minds we often think of healthcare as care needed when the human body goes crazy many of us think of it like fixing the the engine or the transmission when a car is broken we often don't think of healthcare being on a spectrum from early detection to proactive interventions the term wellness often reflects the goal of ai driven decision making across the care continuum this is one view of that future of healthcare ai the x-axis is time and y-axis is the degree of automation or as i like to call it assistive intelligence today we live in a world of rule-based systems shown in grave and increasingly we are in the coming decade we are already starting to see use of ai and machine learning for diagnostic decision support tools i bet in another two decades or even sooner we will see increasing automation for ai driven ventilators ai driven insulin pumps and we will be uh you know and each of these will be probably clinically approved the era of proactive intervention will subsume the inefficiencies of a reactive guidance that machine learning is able to provide today over the last decade i have worked closely with numerous health systems across the world and realized that every stage of the continuum of care from birth to death and everything in between has a corresponding objective function in ai in fact every prediction problem in healthcare has multiple objective functions depending on who the user or the end consumer of that model output is a clinician describe prescribing labs a nurse practitioner raising an alarm of uh of course blue or a social worker helping a diabetic patient get screened regularly to patients or or to patients themselves who may be nudged differently by their fitbits and apple watches depending on when they last went for a run ai will integrate and interfere with the practice of medicine so we must now be very careful on how we create solutions for healthcare using machine learning and one of the things that we have to be very careful about is what dr martin luther king jr was highlighting 54 years ago he said to quote of all the forms of inequality injustice in health is the most shocking and most inhuman because it often results in physical harm i want to reiterate this as the inspiration for us to discuss fairness in healthcare ai imagine an emergency room on a monday morning at a large u.s city downtown amongst other patients with severe wounds and trauma waiting to be seen by a physician there are three patients with mild chest pain also waiting in that emergency department now imagine the triage nurse is using your ai model to help her bring to attention the most severe of these three patients it has been shown in numerous studies numerous studies that rule-based systems that don't use any machine learning or ai often have a very low precision that that hampers the decision-making progress of the entire emergency department so now this hospital is using a more precise machine learning driven algorithm to streamline the flow and improve patient outcomes after using the system imagine uh that after using the system for a few months there seems to be a problem the model always fails to correctly restratify the accuracy or the severity of illness of a specific race and gender when trying to select who to prioritize based on chest pain symptoms this is in spite of us having recent data on which the model was trained folks this scenario is not imaginary the city is chicago and the model was something that we trained and was later revised when we realized that the chain the the chest pain factors uh that that we were using were from prior studies that did not include any black males bias discrimination and unfair practices in health care are not hypothetical in fact policy makers across the world are also becoming increasingly aware of this issue of ai adoption roadblocks and see algorithmic accountability as a very important factor while we know that gdpr and the european union regulation has been ahead of the curve even the u.s congress has been trying to pass algorithmic accountability act which would make ftc require accountability from algorithmic systems the relationship of this with hipaa and other laws that address patient privacy remains to be seen there are racial and bias statements against african-americans blacks and and many other races and genders uh pretty much all across ancient history from medical literature be it in greek indian or arab or chinese medicine the roots of modern bias and discrimination in healthcare go all the way back to the birth of modern science representation of blacks in the medical profession was non-existent in the west which made the perpetuation of such bias even more easier or worse healthcare in fact is rife with inequality and racial prejudice three examples of these are presented here the tuskegee study was deplorable sickle cell was under underestimated or understudied for a long time and human being human-led bias led to discriminatory selection of candidates for clinical trials for bypass surgery for a very long time these are but few examples of issues machine learning and ai will have to think about broadly and address them when we talk about fairness in healthcare ml the main point i am driving towards as you can tell by now is that this is not just a data bias issue designing techniques that address issues in data are not going to be enough to address fair and trustworthy ai systems in health care and it's obviously not just a race issue gender discrimination has also played a major role in delivery of healthcare according to a 2009 analysis of u.s healthcare u.s women make up for just 37 percent of the research subjects even though they comprise of more than half of the us population to stress the point home medicine is not alone in this in fact computer science and our own community have also played a key role in algorithmic discrimination all the way back to the 1970s the literature on fairness and bias in machine learning started to appear in the late 1990s and early 2000s it was not until 2010 that it became a prominent subject of the aiml community and recent articles are starting to shine the light on this issue so the role of all of us in this community will go a long way in shaping the solutions but is it just a data problem i don't think that generalizability and representativeness of ai models are a data problem alone ehrs or electronic health records are notorious for observational biases the data doesn't just reflect the health of the patient but rather it reflects the interactions with of the patient with the healthcare system example would be the detection of diabetes in an electronic health record would only happen when the physician made the diagnosis not when the patient actually developed diabetes we also know that electronic medical record systems are primarily used for billing and there are and there are many other such issues with data in the healthcare system but data is just a signal and we have to treat that signal appropriately fairness is machine learning is more than imbalanced data sets in fact a google cloud study in 2018 talked about an entire pipeline of data sets from issues in collecting and gathering and pre-processing the data from imputation to building the models to running the training and evaluation of these models and ensuring that all the models are treating all the protected attributes fairly and then how to set appropriate prediction thresholds when we deploy the model in pipelines and how to understand the model behavior post-deployment in order to make predictions uh and then you know address the issues as we talk about ensuring deployment fairness compared to training fairness this is my own distillation of the dimension of fairness in healthcare ai and i have collaboratively worked with several researchers from both kensai and university of washington as well as a global research team of healthcare partners in various healthcare organizations to think about how we break down fairness in healthcare ai the social cognitive and computational aspects of ai or models have to be taken together in order to determine fairness in machine learning here is a overarching framework of how we need to start thinking about sources of bias in healthcare ai we need we we are very focused on the data bias issues but we also need to look at the non-data bias issues such as model bias the loss function bias the post hoc optimization biases in fact i would cross over to discuss one more issue of bias in delivery and bring in the idea of handling cognitive biases and social biases as machine learning systems help improve patient outcomes under that uh definition of different biases uh the solution framework i'm proposing includes a articulation of the sources of such bias all the way from selection and sampling bias to understanding lack of explainability and assuming that the models are fair and there are various ways to mitigate that bias of course equal representation and ensuring that the data quality is better is just one way but it's certainly not the only way it is not even sufficient to just do that bias mitigation algorithms have to be designed appropriate fairness metrics have to be discussed and explainable ai has to take a huge leap ahead in order for the entire system to become unbiased it is also important to acknowledge the the the quality of social context in the explanation of the explanations produced by the machine learning model when we talk about delivery of the predictive insights in healthcare settings so clinicians practitioners and designers of user experiences as well as the entire delivery team that handles a particular ai rollout in healthcare starts becoming extremely critical when we discuss fairness in machine learning in healthcare so fixing the computational bias that is data and model bias may not be enough there are certain aspects of of thinking around the definitions of fairness that have been proposed in supervised machine learning and they fall into six major sort of categories uh unawareness demographic parody uh equalized odds these three can be grouped into a group fairness aspect and then the predictive rate parity individual fairness and counterfactual fairness all deal with the underlying features and how these features play a role at the individual level as well as the predictive model level an important issue uh and i'm going to highlight this through examples of our own work uh on why artificial intelligence or ai needs fairness accountability and transparency the data in this plot tells us that children and young adults have diff of different ethnicities are bound to have different distributions of hba1c values just within the united states hba1c is a indicator of diabetes and any model that ignores this variation will optimize for the ethnicity that is most prevalent in the underlying data set on which it was trained not just ethnicity but age gender and even social determinants of health can can significantly uh vary the output of the ai model if it is not tuned to estimate and eliminate such bias uh in the in the production system let's look at one problem that we do need to solve for healthcare very actively and every day it's the problem of predicting length of stay of a patient in the hospital this prediction of length of stay helps with proper discharge planning it helps with bad turnover it helps with just managing the patient flow and particularly in these times of kobit 19 predicting length of stay of of patients is quite useful in understanding the load on the system the blue bars here are longer stay patients and the gray bars are the shorter space obviously the number of short stays in the hospital is much higher than the number of longer stage if we study the distribution of longer and shorter stage across different protected attributes we should start seeing if the eventual model that predicts length of stay will be fair or not on this plot we observe that across age groups the short and long stage seem to be distributed appropriately in fact when we look at gender categories the distribution also seems to be equitable are reasonable compared to the long and short stays yet when we look at race as a as a factor we start seeing that there is significant race category disparity between longer and shorter stage leading us to believe that accommodating or creating customizations and adjustments for evaluating fairness in the length of state prediction is extremely important across these categories this type of an uh of an implementation actually led to significant improvement in the accuracy and adoption of the length of stay model at a large midwest u.s health system when we put these mitigations in place here again we see the impact of that fairness across the machine learning pipeline this is joint work with students at the university of washington and collaborators at kensai and there are dozens of such definitions of fairness but they can be mapped into six main categories we will look at a few of these in the next slide the definitions as well as the formulations of each of these categories is given on the screen i won't go into the details of each of these here but suffice it to say that none of this is complete or conclusive there are many flaws with each of these definitions as well and we can discuss this at the panel in fact i was surprised in my research to find the impossibility theorem of fairness it actually says that three different types of fairness formulations demographic parity predictive rate parity and equalized odds can actually be mutually exclusive theoretically such that we cannot optimize for all three or minimize or maximize all three at the same time and thus leaving us in a conundrum on how to fix fairness greater than that the fairness versus accuracy trade-off is well known and we must tread carefully being aware that making the ml model and the pipeline fair will not always result in the best possible accuracy even post deployment but we may have to make some judicious calls and that is what makes this topic so complex so how then do we make ml models more fair first let's ensure that incomplete or underrepresented values in the data are known and communicated to the appropriate end users second let's design algorithms that are bias free by choosing the features carefully that are consistent with the interpretability expectations of the end user and third and equally important is to compare the performance between different ai systems and select the consistent performance not just against accuracy or precision recall as a metric but rather the many measures of fairness that we have designed or we are designing now for choosing the best performance let's revisit this framework once again of understanding the types and complexities of fairness that we may have to deal with along with the mathematical formulations and the implications of each of these and we also need to be aware of the flaws of each of these fairness metrics in order to then make sure that our healthcare ai systems are accurate and fair to the best possible extent i do have an ask for call to action to partner with us it takes a village so i hope all of you inspired by this talk will contribute in some way to deploy enterprise grade grade ai and ml models in healthcare across across the globe and will consider fairness as an important aspect there are many many websites that talk about fairness in ai as an issue in fact there are very nice libraries that are coming out from different contributors that talk about fairness in ai some of these need to be used in health care and we are working tirelessly to release our own and last but not the least in a month from now at kdd uh i and muhammad dr arpit patel from university of washington dr carly eckert and vikas kumar are doing a kdd tutorial on exactly this topic of fairness in machine learning for healthcare i hope all of you will join us there as well for a deeper look at this topic with that i want to thank you for listening and will take your questions on the panel hi everyone i am marcia gasimi and i'm going to talk to you today about explaining yourself or why i think there is false hope in promising explainable machine learning in health care and the reason that i became interested in machine learning and healthcare initially is because algorithms are really a huge part of healthcare already we already use risk scores calibrated models and these have been used for decades to try to understand patient risk or who needs a specific intervention but now these models have become not just certain risk scores for high-intensity situations but we're seeing across a range of human conditions state-of-the-art performance and so for each of these lines you see a model was created that did as well as or better than a human equivalent and so this has changed the conversation recently from one of these are tasks where we need some sort of calculation or risk score to help us with a decision making to perhaps here we actually have models that can perform better than a human can so my research focuses on creating these actionable insights in human health i focus mostly on what models are healthy so trying to optimize models to do well in particular health settings and on health data what kind of health care is healthy using those to investigate how we make models private robust and fair and finally what kind of behaviors are healthy understanding how we can best deploy these kinds of models in the field even if they are robust private and fair and well optimized so one thing that has come up repeatedly across all of these areas is the idea of transparency of explainability and uh the reason that i think that transparency is a a big phrase right now in machine learning for health is that interdisciplinary mismatch creates this difference in expectations because we have difference in expertise there are many ways of asking for transparency for a model but i'll focus mostly on explainability there has been some very good work done in the robotics community on when humans trust over trust and under trust robotic systems that are meant to assist them and specifically humans tend to over trust robotic systems in scenarios where they believe that a robot can perform a function they cannot or when they believe a system can mitigate risks on its own so this is true in healthcare for example when we have an mri machine that can take images that you can't or when you have a bed assignment algorithm that you think will mitigate risks about patient severity of illness there's been some more research specifically on the health side about uh generation of images and so radiology images or x-rays are often used in triage and so some researchers have tried generating these images as part of explanations or recommendation systems but what they found is that humans these are radiologists actually sometimes prefer fake images to the real images and so here you can see that if these images which are very high quality were just as good as the real images and uh people tended to preference them just as often as the real images you would see most people at a rate of 50 percent for users choosing gans as the real images but here you see there's large variation in human preference and this kind of variation really leads to the crux of the issue with health care health care and health are about human decision making a collaboration between a patient and a provider and a community so if ultimately our goal is improved care we need to ask some questions how does advice impact expert or non-expert clinical decisions what role does accuracy of that advice play and is advice rated or thought of differently if you think it's from some sort of algorithm model or ai and so we designed an experiment in the x-ray domain to test this so we have x-ray images and we gave them to clinicians both experts and non-experts so we gave them these x-ray images we gave them some advice it was accurate or inaccurate and we told them it was generated by either an ai or by a human expert and then we asked them can you give us a final diagnosis and then can you rate the quality of advice that we're giving you and what we found is that task experts radiologists rated the ai advice lower than human advice and the error rate is the same we're giving them the same correct or incorrect advice so there's no difference in the quality that we're giving them but when they knew that the advice was from an ai then they rated it significantly lower as shown here but this was not true for the imem doctors and this was true even after correcting for age of the clinicians so it's not just how old you are how senior you are it's actually your level of expertise what you believe your level of expertise to be that seems to be interacting here with the advice and then even though people with more expertise are rating this advice to be lower when they think it's from an ai they still fail to dismiss it just as often and so the accuracy that radiologists have under both advice types is similar it's not statistically significantly different so there's a level of mistrust of ai advice in the quality rating but what the clinicians are saying what these expert clinicians are saying is not reflected in their actions for how they're using or dismissing the advice we further looked at susceptibility if you pick an accuracy level 80 percent you could assume that anybody who uh does this well is doing reasonably well on the cases that we've given them right and if we draw this dotted line you can see more of the expert doctors more radiologists meet this bar of getting most of their patients right and fewer of the less experienced doctors the imem doctors get this right right so you can see that by who is above or below this uh yellow dotted line however if you look at susceptibility which we defined as getting every case where you were given inaccurate advice incorrect we have a similar number of expert and non-expert doctors who are susceptible to incorrect advice and this is not significantly different whether we give them ai advice or human advice you know that it's not really that it's ai that's giving you an advice it's that incorrect advice itself is sometimes challenging to dismiss explainability i believe is not a panacea there's been some recent work on the interplay between machine learning and human decision making the specific situation this was tested in was in the new york housing market where they said look at this unit it has a certain number of bedrooms and bathrooms and tell us whether this model estimation of a rental price is a reasonable estimation and what they did was occasionally they put in an example of a unit that had a very poor configuration it had very many bathrooms and one bedroom models that were more transparent where there seemed to be a formula that was listed for how the model got its recommendation of a rental price these more transparent models actually hampered people's ability to detect when there was a serious mistake this was not true with the black box models people were able to see that the model had made an incorrect or overestimate for the rental price and so models that are more transparent can sometimes make people feel like a choice is good they're making a good choice they're confident in their choice they're not going to do a more aggressive audit because this model is just confirming a bias that they might have had or an anchor that they might have had now with this example uh i want to recognize that there are some very good you know reasons that you might want this in human healthcare decision making even if you don't have it for rental prices because we believe that decisions about routine practice should be justified by evidence or in evidence but the issue here is that many of the practices that are used or have been used in the past 10 years uh are contradicted by studies that get published in top journals so these are things like lancet the new england journal or jama and it's not a small number it's over 10 in the past 10 years and we don't audit doctors for their accuracy or for their performance doctors are a self-governing self-regulating body there's no federal entity that checks to make sure that you have a specific cure rate or that you treat all patients equally you have to have a panel of your colleagues evaluate you and it's a very difficult question that we're asking uh panels of people to do and finally individual doctors have been shown to have strong preferences about the kinds of patients they treat and the kinds of treatments they use and so there's a lot of variation here this is very similar to these other situations in robotics or in rent's estimation that have been studied because there is so much variation so arguments for explainable models do exist and they're good arguments so things like principle there's a principle that i have a regulatory standard where i want a an amount of information to be available to any person so a good example of this is gdpr which states that meaningful information about the logic of an algorithmic decision is a right of every person there's other arguments maybe it's not a regulatory standard but it's necessary in high-risk situations so when experts are using models they need explainable models and finally there's a very good argument that it's important for fairness because these black box models that we don't understand could create increase or propagate biases i think the the first argument is a reasonable one it requires significantly more technical detail about how it could be achievable for whether it's necessary in high-risk situations i'm going to focus specifically on clinicians because this is about healthcare i think it's only really important that we have transparency when clinical opinion differs significantly from a prediction that doesn't mean that all clinicians will care about exactly how a model works or that simple models are necessary for the sake of transparency as we've seen simple models are not a final solution what it could mean is that clinicians need to understand when they should rely on or reject model outputs that they need a notification of what populations a model might work best on or fail on or that there should be limitations of data models that are trained so technical options that exist currently are transparency via these post hoc explanations and these could be useful if they are consistent which means different model behaviors give you different explanations and similar model behaviors give you the same explanation so things like attention-based explanations or saliency maps are very common in images but these are not always consistent there have been some fixes proposed recently for doing competition-based uh explanations post-hoc which means that computing maps for all possible labels and if you're in an image competition over all pixels is used but again this is uh an ongoing field where many many papers are published demonstrating that simple attention-based our saliency-based postdoc explanations are not consistent and they are not robust so some of the work that we have done in explanations post hoc for clinical data is when predicting interventions in intensive care units so if we look at patient data from the icu we could have static data gender age other information we could have time varying vitals and labs oxygen saturation lactate level we could have all the clinical notes for a patient's day if you put that into a matrix there's many ways that you could learn what's going on with the patient and how to predict upcoming interventions if we focus on these more black box style models so uh recurrent neural networks or convolutional neural networks you can ask for post-hoc explanations you can do feature level occlusions to identify which features are important for classes of predictions so here we see that the physiological data is most important in invasive interventions like ventilation and that these topics that we've learned across notes are more important for less invasive ventilation like a fluid bolus you can also use convolutional filters to ask for maximally activating short-term trajectories for different kinds of intervention onsets and so these are some ways that you can ask for post-hoc explanations another option is transparency through natural outputs what this means is that if transparency is just a way of calibrating a clinician's trust in a model you could have clinicians and machine learning experts pre uh training agree on a set of metrics that are consumable and product not just an intermediate risk score and then you could deliver those metrics and that would give enough explanation or transparency so for example if we wanted to create an end product like a report from a clinical image we could automatically generate radiology reports given a chest x-ray so this is challenging because unlike in image sub-captioning where you want to comment on exactly what is in the image like black and white dog jumps over the bar you do not want to just comment on what's present in an image for a radiology report you have to say for example in this image there is no focal consolidation effusion or pneumothorax in the impression you have to say no pneumonia or pulmonary vascular congestion so you have to first understand what topics could be commented on in the image and then explicitly confirm or deny them and so here what we did is decided to first predict the topics that would be relevant in the report and then conditionally generate sentences that corresponded to the topics we used a cnn rnn rnn architecture which gives the model the ability to use these largely templated sentences but still generate diverse text and we also made sure to enforce a clinical coherence score in the generated text because you don't just want human readable text you want text that's actually clinically correct across a range of different labels so we made sure that the generated report disease states were correlated and coherent with the ground truth report disease states and by doing these we had state of the art performance in both readability metrics like cider rouge and blue scores and in the clinical accuracy of the reports that we generated and so if you take an unseen image we evaluate our generated text and also compare it to the actual text and the important thing here i think is that it's not just these readability metrics which often reports are scored on we have these natural language metrics that we are evaluating on but we're also looking at disease accuracy and ours is doing significantly better in this way and so by giving people an end product that they want to consume they would be able to say is this readable and is it correct rather than just saying is this score something that they can then use the final option if you don't want to do a post-hoc explanation or a consumable end product that an expert can look at is transparency in the audit of the embodied data so all data is valuable but particularly health data comes from human bodies so it's embodied data and we may want to focus more on aggressive audits ultimately or even in addition to the other two options and so we have new tools that have been proposed to understand both data and processes so these are things like data sheets for data sets or model cards for model accuracy and they help you understand the process and potential data biases and model biases not just one individual output from a model and so they're really valuable tools in this way the the last thing that i want to talk about briefly is uh the argument that explainability is important for fairness i think that this is a both very important topic but also something that must not be too easily addressed by something like explainability what i mean by that is that bias is already part of the clinical landscape so we know that doctors are humans and humans are biased doctors have biases and so the prejudices that you might imagine exist in uh humans also show up in clinical care this has been very robustly established in the clinical and epidemiological literature for a very long time so we could just audit models for fairness and we could say i want to look at different ethnicities different insurance types different genders and i want to see whether i have better or worse prediction accuracies across these different subsets of individuals but i want to highlight how hard it is to recognize that you might have one of these problems if you're not thinking very carefully so we took a publicly available word embedding model uh cybert which is trained not on the internet uh but on pubmed articles so it's trained on scientific text and we gave it this prompt so blank race patient became belligerent and violent patient sent to blank blank and these transformer models uh these contextual language models are able to fill in the blank very well and it's been demonstrated previously that contextual language models that are not trained on clinical text but that are trained on um large amounts of human generated text online actually have very deep biases and so here we found that if you ask the cyber model what happens to a caucasian or white patient who's become belligerent and violent they get sent to a hospital but if you ask the cyber model what happens to an african african-american or black patient those patients are sent to prison this is really really disturbing because cybert is trained on scientific articles it's publicly available and it is very commonly used as a contextual language model for generation so think about our prior example of language generation and x-rays you would not want this model to generate these kinds of differences and it's difficult to imagine all the ways that biases present in even scientific documents have crept into contextual language models and evaluate and audit all of so them want to close with a note of positivity if we want to improve clinical care and practice maybe there are some things that we can look at so there's this great article a couple of years ago that talked about how gender concordance increases a patient's probability of heart attack survival that effect was driven primarily by increased mortality when male physicians treat female patients you could argue these burnt out doctors who have little time to be empathetic and are used to dismissing female pain or not listening and so that's leading to this increase of mortality specifically in female patients when we have heart attacks but there's this interesting snippet in the same article that said mortality rates decrease when male physicians practice specifically with more female colleagues or have treated more female patients in the past so the funny anecdote for that is that women have a great dose response effect the more you're around them the less you kill them but the reality here is that actually there is some improvement when you have more experience when you have more reference points when you can draw on more information and so i think that there's a lot to be said about new opportunities to improve collaboration between models and clinicians where instead of trying to optimize for a specific goal or metric that was defined maybe with bias in mind we are trying to improve some sort of and outcome we're trying to improve the clinicians practice not some score that they might use in the course of their practice which might be flawed in and of itself and so i think that this could be where machine learning actually improves rather than just creating some metric that we then have to say is transparent or explainable in summary i think that explanations as a form of transparency are actually really important when we debug our models or when we audit our models i think that if you're a modeler or a statistician a developer you use this daily you're trying to understand what your model is doing and why there is an error but it's really unclear how an expert or non-expert end user can or should incorporate explanations or another form of transparency when they use them especially in a healthcare setting we know that explanations can cause over-reliance and reduce our audit propensity and biases that could be created increased or propagated need to be attacked with these aggressive audit standards rather than saying i have an explanation the model is transparent therefore you could see if there was an issue because these are really pernicious biases that could be very deeply embedded and we could forget or not know how to audit one specific bias this work was done with a fantastic team of students collaborations and fantastic funding sources and i believe that machine learning for health has a real path forward in creating actionable insights in human health if we focus on making technology that makes humans better rather than making humans better at understanding prepackaged technology thank you hi so hopefully you enjoyed the three talks uh in this session on saving lives uh with interpretable machine learning methods um definitely different points of view represented in the talk so that's a lot of fun and uh hopefully you've got some interesting questions for us we'll start off with a brief discussion uh and then we're gonna open the floor please uh type in your question and we'll try to answer it um so so let me get started i just wanted to say that in my own work with interpret machine learning care we see at least you know four different uses of the interpretable methods in health so one of them is let's take kobit 19 which is happening right now one of them is the doctors don't know about this disease it's new and they want to learn everything they can as quickly as possible about how the disease operates what the risk factors are and then what they might be able to do to help treat patients and they're they're interested in training interpretable models on new covid19 data and we're interested in doing that not even so much because we plan to use a predictive model to treat patients it's because we want to know what the model is learning from the statistics in the data as quickly as possible one advantage of machine learning is that machine learning can see and digest thousands of patients possibly patients even coming from multiple hospitals whereas no one doctor typically will get to see thousands of patients in a short period of time so there's a hope that the statistical machine learning methods that are interpretable can actually inform us much more quickly about how covid19 works and we're definitely seeing that in our day-to-day work with hospitals on on cobit 19. so they're very interested in the models just because they want to learn what they can another use of interpretable machine learning in healthcare is actually very different it's more like that pneumonia asthma story that i i gave in my talk where we've trained a model we would like to deploy the model and use it but we want to look at the model and see if everything it learned kind of makes sense or if there's anything risky in there and for example we often see that treatment effects things like asthmatics get to care faster heart disease patients get to care even faster they just don't give up on patients over a hundred when it's a treatable illness and if you've got a wheezing in your lung you're more likely to get to care than if you're not wheezing things like that we see that in the data all the time and the model learns to predict counter-intuitive things that uh that having asthma and heart disease are good for you if you've got pneumonia when the doctors are convinced that that is not true it's the exact opposite so there what we want to do is see what the model has learned to but basically vet the model and correct the model before we deploy it and where it might you know harm some 100 year old asthmatics which we wouldn't want it to do then yet a third use of interpretability in healthcare is explanation that is we're giving a prediction to practitioners it could be doctors nursing anyone who's treating patients directly and uh you know most of the time they don't have time to look at an explanation they don't really care what the explanation is if the model is kind of confirming what they already thought they're reasonably happy to just sort of accept it as you know a second opinion but there are times when they definitely want to know why are you thinking that this patient is so high risk or solar low risk and they want to see why the model is doing that and then there's a fourth use which is the model looks like it's making some mistakes and that's definitely going to happen when we first deploy models in some new healthcare setting and you want to debug the model that is you want to figure out why did the model make the specific prediction which we're pretty sure really is a mistake because what's going on so and yet that's a different use of interpretability where the data scientists and the healthcare practitioners might be working together to try to debug the model and figure out why in certain settings it looks like it's not as accurate as it should be so those are very very different uses and the kinds of methods in interpretability that you might use for learning from new medical data covid19 might be very different than what you would use for debugging where a data scientist might be in the loop uh very different from what you would show to an end user a healthcare practitioner who's actually working with patients so i thought it would be sort of fun um if if anker and marzia if they had any comments about their uh presentations uh you know bias uh and whether practitioners trust should trust or don't trust their models enough and these sort of different this different spectrum of you know i only want to train a model to learn from my data to oh no i need to actually understand why the model is saying exactly this about this this patient so so anyway i thought i would start the discussion off there uh and then we'll open the floor and hopefully you guys have questions and we didn't answer them all uh during the chat session so i think um my my biggest concern is with category three right so i think that when users are developing models we need to debug we need to understand what's going on we want to make sure that uh you know we're not in an x-ray image focusing on some feature that might be more indicative of the machine that took the x-ray maybe from a poor hospital that sees more of a particular kind of case than something that might be reasonable or relevant to the physiology and i think also when you're trying to investigate um in a non-inferential setting when you're just trying to understand data distributions whether certain variables are predictive or not when we're not talking about deploying a model at inference time and listening to those recommendations i think it does make sense that you want some sort of way to interrogate the model what i'm really concerned about and i'd love to hear ankur talk about actually with respect to bias is we know that doctors are humans we know that humans are biased we know that there are problems and we don't have good solutions to fix those problems with humans as they stand but one human doctor is limited by the place that they work the hours they work who they can see but one algorithm could be unlimited in the number of people it touches and so my concerns are if we have explanations that we know in non-machine learning settings can anchor your response that's very dangerous now because you could convince a whole slew of new doctors that it's fine to mistreat somebody because we have an algorithm with the sheen of objectivity hey let me let me make a comment before you start yeah i just love marzia's uh uh you know it's the classic all doctors are human all humans are biased therefore all doctors are biased i just think that's a beautiful and a sad update on a popular centuries yeah and then you can extend it extend it to one more level of abstraction and therefore all medicine is biased hence you know right uh yeah no amazing thoughts and i mean from my experience uh over the last five years trying to deploy these systems collaboratively with you know large sort of cloud scale right getting it done uh a cloud scale implementation and we have been doing this with with microsoft azure for some time and large health systems across the world it is fascinating to kind of rattle off the problems that i have seen uh you know come through so i'll give you a quick example a story right uh downtown chicago big hospital emergency room monday morning you know three exactly same patients waiting to be treated and amongst like you know 25 30 patients that are just waiting to be seen and we were asked to sort of build a model to to predict left without being seen so essentially the the emergency department wants to use some form of machine learning uh to to figure out which triage so which patient to triage quickly who to connect with the with the doctor so that they can be you know they can be taken care of faster and won't just you know walk away without being seen because that might be you know harmful for later treatment etc etc and we trained the model everything went great in retrospective data everything you know was fine uh deployed and boom you know two weeks down as we are doing the prospective pilot thankfully right we had sense to not just take the model put it in the pipeline and run it in the wild and uh it's following the protocol in the prospective trial just start seeing that the model is literally you know guiding the triage nurse to go and check in with every african-american person that walks in right and and it's surprising because uh you know uh we just hadn't like we just hadn't done our work right like we we we were and this was like several years ago we didn't know much about uh you know the issues that that can be there in in post-deployment processing and we were learning this thing uh and as the metrics started showing thankfully we have uh like we have we have built what we call as the model health dashboard internally to start seeing what the class distributions are as the model is making its predictions and if it confirms to the class distributions of the original class distribution on which the model was trained and when you start seeing these things side by side just visually suddenly the light bulbs start growing and as we started observing that we had to like literally press a big red button right like like the staples add say you know stop uh this model let's you know go back to the drawing board two weeks of data just two weeks of data and this is happening and then of course we went back to the data observed you know protective classes you know ethnicity was a big one uh somehow mitigated that made the thresholds uh and the cut points at which the model was doing the prediction from a risk or to a binary on who to go and touch and then adjust it for that and came back right that's one example and i have like you know probably 20 such examples across not just like acute care management but also in diabetes care management and recommendation engines for nudging people for a whole you know for a whole country uh and how do you get get that going but simply fascinating and explanations are you know as martha said just not enough it's the actionability so explanations essentially can be broken down into three types stuff that you can't fix age gender that's not an explanation like telling someone that this is happening because of their age is not actionable right so that there is non-actionable very actionable and then the third framework is modifiable right like what can you modify so having metrics uh that look at the quality of explanations across attributes that are non-actionable actionable and modifiable really starts helping physicians get to understand how how to use those explanations and then you surface only the modifiable ones at the individual patient level but you surface the actionable ones at the system or the population level and then you just you know sort of give retrospective idea to the whole sort of leadership team on what's happening on the non-actionable one so that population health can do its job so so starting to think about those frameworks in that way really helps when you're trying to deploy these systems hey there's a question about um what clinicians find interpretable and informative that i think is related to what we're talking about now and most of my work is with uh clinicians who not only practice medicine but there are also people who write papers uh so they're let's say they're also research clinicians so so they're statistically more sophisticated perhaps than the the sort of practicing physician um who you might see uh when you go in for care um and what i find is even with the statistically sophisticated people who write papers uh you know it's critical that a machine learning data scientist person like me is in the room so that we can talk together about how to interpret the uh the explanations they would not find these explanations 100 informative all by themselves and even we find it's critical to have a database person in the room who was involved in collecting the data and processing the data before the machine learning ever touched it and that's because often the bias that we see in the model is actually an artifact of some pre-processing stage or some way the data was collected um you know earlier so we saw a covid19 bias where it looked like it was good to be older so actually the older you were the healthier it was and it turned out that the bias was that people who were older were in the hospital longer we were collecting a data point from every patient every day they were in the hospital we were only predicting a bad outcome which is at the very end so it turns out people who were in the hospital longer had many more days where something bad did not happen so the older patients actually look like on average they must be healthier but it's because of an artifact of the way we were sampling data so i thought you guys might want to talk a little bit too about what you find when you work with your experts what they find to be interpretable and acceptable i i have had such a an incredibly varied response to this to this question which is part of why i don't think explainability is a good idea in deployed clinical models honestly i have had panels of clinicians all from the same background all with the same level of fantastic training completely disagree about what they want and so that's why i put the example at the beginning of my talk of these you know these gan generated radiology images and uh when i saw that that's a sami uh krieger's work from uiuc i remember not being able to say anything after i saw that slide because when he when he you know set it up he said um what happens when you have a good enough example generated by a computer and of course i'm saying well then the expert can't tell it apart from the real image no that's not what happens it's that some people have preferences for the fake thing and think the fake thing is real and some people have preferences for the real thing and think the real thing is real and some people can't tell the difference i think that amount of variation in what and this is one this is a consistent field where radiologists are trained to look at one kind of data and you know make inference about what kind of diseases might actually uh be present in an image if we're talking about anything that's more general right if we're talking about um internets or intensivists right who are in an intensive care unit who have to look at vitals labs images notes you know demographics information i can't imagine a setting in which uh you know there would be a consistent way in which if you asked people how would you like to see this information how would you like an explanation they would agree but even worse you know in the the the uh the really you know traumatizing thing for me in the clinical vis paper was we were visualizing these these things for uh these these pieces of data for real doctors it's real patient data these are real doctors we're asking them real questions about uh what interventions need to be prepped and using a nice visualization that we developed in collaboration with a clinical team everybody became much more confident in their choices and not much more accurate in their choices and so i'm also very concerned about showing people things in a way that makes them feel comfortable and confident and secure and having them then not audit which is what was found by hannah wallick and jen wortman vaughan and crew in this new york housing market example yeah i can i can build a little bit about on that so so in one of the slides i showed the three dimensions of sort of computational bias social bias and cognitive bias what margie you're you're describing here the classic case of cognitive bias so within that there is there is this idea of an automation complacency and the term complacency sort of originated from references in the aviation community where you know accidents or incidents happened when pilots or air traffic controllers or other operators purportedly did not conduct sufficient checks of the system and assumed that all was well when in fact a very dangerous condition was developing that led to some sort of an accident right so technically the performance consequence could also not involve some omission but an extremely delayed action so this type of complacency is is what we call as automation complacency and what it builds is it builds exactly what you described until the whole body of very nice literature that talks about automation bias where what happens is quite often the decision aids in our case in healthcare the the visual aid or the risk prediction model start getting misused for two main reasons the first is that the automatically generated queues are so silent like sorry not silent but salient right they're so salient that that they either that they tend to draw the attention of the user and secondly the users themselves have a tendency to ascribe sort of greater power and greater authority to the automated aid rather than other sources of advice or their own sort of training and background so so you know addressing these two things sort of automation complexity complacency and automation bias and then figuring out which of the two paths this bias is following becomes extremely important in the delivery of the system so when we look at deployments we often try to see if the as we do the user study on the output of the prediction model if that saliency is actually taking away from providing good quality care into the system so i i have a follow-up question that i want to ask both of you and uh this this is uh in a healthcare setting when we talk about deployments often deployments are done under the banner of quality improvement right that worries me because often that that positions our research purposefully as not being researched and so the way we think about doing the uh the deployment the way we think about doing the evaluation our entire incentive structure is really different and i went through this recently during a deployment at st michael's hospital here in toronto where you know i kept having to really pull the clinical team back and say this is still research we still have to keep our mind in the space where this could be worse we could not improve things we need strong baselines we need to compare to what would happen in the absence of these kinds of interventions or with simpler versions of the interventions i bet i i think there's been several really good papers that have come out recently that have said this is not the norm right often you are told explicitly by the hospital system well you can't do this kind of research on patients that's not ethical let's just do some quality improvement so where does that leave people like us who would like to do this kind of research but but would like it to be uh you know very robust uh and reliable uh so so i know in some of the covid19 work that we're doing with hospitals in new york city they are doing a sort of careful randomized clinical trial rollout of the decision support system now there they've gone through irb approval and gotten approval to treat this as a you know an experiment a a sort of a medical device a new new treatment and basically some physicians are getting to see the extra information that comes from the machine learn model and other physicians are not seeing uh that and then they're trying to figure out if in fact seeing the the um the predictions from the model actually does help improve care or not um so so i agree with you i think it's very important to treat this as this could just as easily hurt as help and we need to treat it as as research and i thought what anker said was also very important you know he pointed out that they did a roll out of something experimentally before they actually rolled the thing out in actual deployment and i think that's absolutely critical i've seen cases where something is rolled out experimentally it turns out it was only trained to make predictions for adults but accidentally it was allowed to make predictions for children as well in the hospital when it was rolled out and suddenly the accuracy looked like it was tremendously lower than they expected and when they looked at it more carefully they realized oh we're letting it make predictions for children and fortunately they saw that in a in a safe roll out before any clinicians had actually seen the seen the prediction so it's very important i agree with you to to do these things carefully and treat it as an experiment um yeah absolutely and and the only thing that i would add to that to build upon is we we in computer science so so just taking a little the ball little out of the healthcare field into computer science right we also have to think about healthcare more broadly because when we think of healthcare we think of patients we think of doctors we think of hospitals that's not necessarily true healthcare and wellness is a much broader spectrum so there is you know while we can say quality improvement for something in the hospital setting there's a lot of stuff happening in devices there's a lot of stuff happening on variables there's a lot of stuff happening in personal health that people are in charge of in terms of taking care of predictions and outputs so how do we how do we figure out a way to address fairness as well as address explainability in those settings is also very interesting and i've been looking at some of that but but think a little broader is my advice to the computer science community i i work i i hate to do this but we're well over time so we are going to have to wrap up um thank you very much for watching uh thank you for the great presentations uh and we are happy to chat with anyone who wants to get in touch with us afterwards i mean we really like doing this so so we would be very interested in talking with people who have further questions please reach out and contact us thank you again thanks 