 - Welcome to the 5th session of the AI for Good seminar series. This was developed by ICME, the Institute for Computational and Mathematical Engineering. It's a mouthful so we just say ICME. And we also have many wonderful co-sponsors across campus working with us on this seminar series. Plus the generous support of Google. And you can check out all the details on the website. We've had some great sessions so far. And just in case you haven't been here the last few weeks, we also have all the videos on our website. Like a session on AI in environment, AI for non-profits, AI for government, and so on. So there's been some really great talks. And we'll continue for a few more weeks. We're also live streaming, so welcome to everyone who's listening in on the live stream from around the world. Hope you're enjoying it, and hope to see you in the others as well. Today, we're going to turn our attention to a new topic for us, AI in healthcare. So probably a lot of you are already aware of what's happening, or some of the explorations, some of the challenges with AI and data science in healthcare and medicine. There's been a lot of interesting articles in the press, in the general press, recently, about personalized medicine, about drug discovery, and about disparities in healthcare treatments with algorithms. So it's a really interesting topic, a lot going on there. And we're really thrilled to have today, Professor Marzyeh Ghassemi. She's one of the leading voices in AI in healthcare. I had the pleasure of hearing her speak last year at the Women in Data Science conference, which was fantastic. She is a professor, assistant professor, at University of Toronto in the CS department and in medicine departments. She is also a faculty member at the Vector Institute in Canada. And her work is around healthcare, or what she calls, healthy ML. - Yes. - Which is all about healthcare outcomes. And she has some great articles and publications, some that I sent around already to all of the students. Addressing both, her work around the outcomes, and also, addressing some of the challenges around biases, disparities, reproducibility, privacy, and that sort of thing. So, she has a vast range of experience in the field. Her background is a PhD from MIT, and she was one of MIT Tech Review's, 35 Under 35. (clearing throat) So really, congratulations, that's pretty amazing. She has a BS in bioengineering. Bio-- - I have a masters in bioengineering. - Masters, okay. Masters in bioengineering from Oxford, and a BS in, if I got it right, in CS and double E. - Yeah. - Two BS degrees from New Mexico State University. So, very impressive. And we're just thrilled to have her here today. And if you are excited and wanna hear more, she also has a video on our WiDS podcast, or actually, a audio taping on our WiDS podcast series that you might wanna check out as well. That's the Women in Data Science podcast series. - That was a very nice introduction. I'll try to live up to the hype. So my name is Marzyeh Ghassemi, I'm a professor at University of Toronto, and a faculty member at the Vector Institute. I'm going to talk to you a little bit today about machine learning, healthy models in health. It's acute mnemonic, what do I mean? So machine learning is currently in the wild. Right, it's happening everywhere. It's not like we're talking about in theory, someday, in healthcare we could, you know, use machine learning, we could use models, we could use algorithms, we're actually using models and algorithms. But this isn't a new thing, that's what I want to stress. We've been using acuity scores in the intensive care unit, where you take additions of certain vital signs and then you apply some threshold or transform to it, and then you use that as a tool for assisting decision making for a really long time, for decades. So what's different now? Why is there all of this conversation now about models in healthcare? The difference now is that models are performing at or above humans across a range of tasks in the human lifespan. And so, it's suddenly no longer we have this interesting calculator, the question is, are there some tasks for which algorithms are better suited? So I think that's why the conversation has changed recently and there seems to be more focus on machine learning. But how do these things happen, right? All of those models that we're talking about that are performing at or above, what do they use? Well first, you need data, right. So we get data from clinical practice, right? So you could get the electronic healthcare records, the EHR, from a hospital, but that's watching doctors practice, so you could use clinical practice and maybe train a model using that. Or you could use clinical knowledge. And by that I mean, maybe you wanna use PubMed, all the randomized control trials, medical textbooks, right, all this knowledge that we've generated. So these are two things that you could use in isolation or together. You get this data, maybe you train a statistical model, a neural network, whatever you want. And then you try to predict a clinical event, a treatment, whatever you want. So let's focus for a second on the first thing. Which is what I tell all my students they should do. And the reason why, is when you learn from practice, you're learning from doctors practicing. But there are well-known insufficiencies in the healthcare system, doctors are burnt out. And burnt out doctors don't have time to be empathetic. So, if you're training a model to mimic clinical practice, do you want it to mimic all clinical practice? Maybe not. Right, you probably want it to mimic some kinds of clinical practice, but not all. And doctors who feel overloaded, burnt out, who don't have time to be empathetic, still have ethical training. But even with ethical training, we know that the biases in clinical practice reflect known biases in society. And so, we don't want to take these, all of this is without machine learning, right, these are epidemiological studies that are talking about biases that exist in healthcare as a system without algorithms automating it. And so, when we're talking about using models, using algorithms, we wanna make sure that we're improving care, improving practice, we're not just using discrepancies that exist in the data and making them worse, automating them. So maybe now you say, well, clinical practice, we can't just easily apply a model there. So maybe we should work on knowledge. But clinical knowledge is actually problematic as well, right? So we have these things, this gold standard, where we generate knowledge, where we generate evidence called a randomized controlled trail. But actually, there are very few treatments that we use that are based on a randomized controlled trial. And that's because you can't do a randomized controlled trial for every single treatment in every single population. They're expensive, they're hard to do. And so, they're rare. When you have a randomized control trial, that evidence is often very biased. So the example that I use is for asthmatics. So asthma is a very common condition in adults in North America. But if you took all the people who are treated with asthma today, and looked at what proportion of them, what percentage of them, would have qualified for the randomized control trial used to design their treatment, it's a very small percentage. So, 94% of asthmatics would have been part of the exclusion criteria. They wouldn't've qualified to be part of the study used to design their own treatment. And worse, often we look at these medical journals that have really fantastic papers, so JAMA, Lancet, New England Journal. But a reasonable proportion of those are what we call a medical reversal. Where a paper comes out and says, you know, this thing that is common clinical practice, that's actually really bad for people and maybe we should stop doing that. So we also don't just want to automate knowledge, right. Again, this is all done without machine learning, this is all before we apply an algorithm. And so, it's important that you go into the field with a clear view of what the state is so you can improve it. So let's take a hypothetical. Just to examine why this is so hard, right. Why is this so hard, why aren't we doing better? Let's say, Sumana is having trouble breathing. When she goes to a doctor and says, "can you treat me?" What the doctor is thinking is, what patients are like this? When have I seen a patient like this? Have I seen a randomized controlled trial that dealt with treating a patient who has trouble breathing? Have I treated a patient myself that had trouble breathing? What kind of treatments can I think of? And then you might look through medical records, through knowledge, through practice. There's a really excellent paper that addressed how unique human treatment pathways might be. So they took 250 million records from four different countries, combined them together, and they're just looking at the treatment pathways. So, for people who have common conditions like, diabetes or hypertension. What is the three year trajectory of medications you take? Maybe I only take metformin, but somebody else takes metformin and two other subsequent drugs. And they said, let's just see what percentage of people have a completely unique treatment pathway shared by zero other people. I would've imagined, and I hope you're imagining that that would be very small, 1% of people. But for diabetes, it's 10% of people, in this very large international cohort, that have a completely unique treatment pathway. For depression, it's 11%, and for hypertension, it's 24%. That means, almost a quarter of people, if they came in and said, "can you find a person like me, "and just treat me like them?" The answer would be, no, there is nobody like you in our records. So clearly this is a hard problem, right. It's not as if this is simple and we just know how to solve it. And so, I believe that we need to use complex models for complex data. And we shouldn't just limit ourself to one kind of data modality, we should be thinking about how all sorts of data, about how we're living, about how we are healthy and unhealthy, can inform our ability to make better decisions in human health. So the three broad categories of research that my group does are, what models are healthy, what healthcare is healthy, and what behaviors are healthy? I'm going to show you a couple of examples of work that we've done in the first category, and then I'll give you some questions that I'm thinking about broadly. So first, and I'll replug this in a moment, a majority of the work that I'm going to show you today is done on the MIMIC III ICU dataset. That is what a majority of the work in my community, in the machine learning for health community is done on, because it is deidentified and publicly available for people under vetted access. So an IRB was approved, you can download it, you can use it. Okay? So here's the general focus of the problems that we look at. There is some decision-making or care planning issue that I want to address. So I want to look at real-time, and I use quotes for that, because it's not real-time, I'm playing a patient's record forward as if I'm observing them in real-time, but obviously you're not in a real hospital, this data already occurred. So I play this data forward in real-time, and I try to predict the need for a drug, outcoming mortality, some condition, by some gap time before a doctor would've detected it or would've acted. The choice of the gap time is important because sometimes you can have the student come to you and say, "well, I predicted this patient is going to die in 12 hours "and it's because, the signal is that, "the doctor turned off life support." That's not valuable, we probably need to predict a little bit before that. So, one example is, we want to plan care. So if you're having trouble breathing, maybe we want to figure out if you are going to need invasive ventilation overnight. That's an important problem because you probably don't want a junior resident doing a really touchy trach overnight, so you might need somebody who's more experienced. So, planning ahead of time is important for the patient and for the hospital. So what we did here is used about 34,000 patients from MIMIC ICU data, we used all of the data that we had available. So, static data, the time-varying vitals, the labs, and all the clinical notes for each stay. You turn each of these kinds of data into a matrix. So now you just have a large concatenated matrix of all of the data for a person over the course of their stay. And when you have one matrix per person, you can use a bunch of different patients together as a large tensor. When we have a large tensor, there's lots of ways you can model data. We can use switching state autoregressive models, we can use long short-term memory networks, which are a kind of recurrent neural network, we can use convolutional neural networks, so there's many ways to model this. And what we've found is that using neural networks works really well, which is not surprising. But what we thought was interesting is the representation that acknowledges when data is missing explicitly by using what we call, a physiological word, representation improves the neural networks ability to predict many kinds of upcoming interventions. So here we have invasive and non-invasive ventilation, the need for a vasopressor, which ups your blood pressure, and two different kinds of fluid boluses. So it's good that it helps you predict across a range of tasks and not just one boutique task. And the nice thing about using these neural networks for clinical intervention, is we're able to examine where certain kinds of data are contributing the most to the performance that we see. So for these more invasive methods, like, invasive ventilation or maybe a vasopressor, it turns out that the neural network is using a lot of these labs and vitals to make the prediction, but when we look at things like a fluid bolus, it was the topics, the clinical notes, that were helping with the AUC the most. And so, you need all kinds of data to do a good predictive performance across many kinds of tasks in this setting. And then what you can do, well I'll have a short rant about interpretability and the many meanings of that later. What you can do if you want to understand more about what some of the modalities of this neural network are, is you can ask it, what are the most activating patient trajectories, most and least, for these different kinds of things I want to predict. And those correspond with known physiological states that our clinical collaborators were able to correspond to. So, that's one example. Now, we know that Sumana might need invasive ventilation, right? Maybe her blood pressure is dropping, so she needs one of those vasopressors that I predicted, but which one? There are lots of different vasopressors, you can choose many of them, right. So what is her individual response to treatment going to be? So here, the issue is that, we don't often get fully paired data. By that I mean, we don't often get to see Sumana for 12 hours before she has a vasopressor, and then Sumana for 12 hours after she has a vasopressor. And then get to learn from many different people, Sumana included, what is the function that maps between you before and you after. You come in on a vasopressor, or I have one hour before and 20 hours after. Or you don't get a pressor at all and you're still part of my dataset. And so, what we wanna be able to do is use some of the models that are currently used to, for example, understand what the latent space of faces look like, so generative adversarial networks, to here instead, understand what people generally look like before they get a vasopressor, even if they never get a pressor, we still wanna be able to use that information to understand the latent state, what that space looks like before you get a vasopressor. And then we also want to use data from people who, they come in on a pressor, I only ever see them with this medication, to understand what you generally look like. And we're going to impose something called, a cycle constraint, in our model. To say, even if I don't know where you are on both sides, if I know that you, for example, never got a vasopressor, I'm gonna learn a functional mapping from pre to post, and then an inverse mapping from post to pre, so that when I go all the way around this cycle, you lie in approximately the same space as where you started. And in that way we're able to use all of this additional data that isn't paired in the traditional way, to improve our ability to predict intervention response across only 500 paired patients, but a few thousand unpaired patients. And we improve the mean squared error in several different categories of the vasopressor prediction. Another thing you might want to do, is send Sumana down for an X-ray. So the issue with sending Sumana down with an X-ray could be, radiologists often have many radiology reports to write, there's a well-known phenomenon with clinical notes called, copy-paster, where you just take old things that have been written about a patient, copy and paste it, because this is a rote task and you may be tired, you might have several of these things to do. And so, you perform this task, and it would be nice if you could have assistance with it. So, unlike planning care interventions or forecasting individual response to treatment, this part of what we're looking at is saying, if I need an X-ray and I take that X-ray, I as a radiologist, maybe would like to have an automatically generated note that I could edit, rather than having to do the whole templated note on my own. And so here we tried to automatically generate chest X-ray reports from the X-ray that would have been seen to generate them. So we learned from hundreds of thousands of X-rays and reports, corresponding reports, from several datasets. And here we use the CNN, RNN, RNN model, to go from an image to this templated report. And importantly, we penalized two things. We penalized both the model's ability to correctly predict which conditions this X-ray might have, and we also penalized the model's human readability, the readability of the note that we generate that corresponds to this X-ray. And by doing those, we both get better X-ray reports that we're generating, and we do better across a range of tasks, so these are the 13 different clinical conditions, diagnoses, that you might have from the X-ray. And we don't lose the predictive performance that we want by improving the readability. Which is important because we don't just want to give you a score, we probably also want to give you output that's going to make your job easier. All right, so those are some examples of doing what models are healthy across a range of different tasks that you might wanna do and interesting machine learning. So the goal of my lab is to create actionable insights in human health. I think that there are some longer term things as a community that we should be thinking about, and that you might be interested in thinking about as well. So the first one, is what I sent a bunch of readings out to Karen on, for fairness and ethics. So I've thought a lot about this. And the reason I've thought a lot about this is because in healthcare there are questions beyond the obvious. It doesn't maybe concern us so much if we can't tell every Labradoodle from the Golden Retriever pool, but if we suddenly can't recognize a clinical condition in a specific sub-population, that feels wrong in a very visceral way. And so, we don't want to exacerbate the known inequalities in healthcare, we want to help eradicate them. There are a range of different inequities that happen in healthcare. It could be that Mary and Ian, two patients, don't have access to the same doctors. And so, we won't even see the same patterns of care, not because they don't have the same conditions, they just don't have access to the same doctors. It could also be that they have the same insurance or were in a single-payer system, but they have inequity of treatment. So Ian and Mary can go to the same doctor and say exactly the same things about the condition that they have, but one will get a treatment and the other won't. Or it could be that they will see the same doctor and they'll be given the same treatment, but one of them will actually respond to the treatment and the other won't because of social determinants of health. Maybe Mary lives in a food desert, or she has to work back-to-back shifts, and so she doesn't have the ability to take her medication with reasonable regularity. These are all things you have to think about because if you are making a model that somebody might deploy, you might impact these things or automate them. So we want to make sure that we're improving healthcare for all, right? We don't want to, for example, say that if there is a medication, like medication B, that's really given to wealthier people, we say, well, you know, it seems like medication B is really what's working best, but it's just that way because those people are in better health generally. So, we have a couple of examples of work that you've read that's looked at this. The first is trying to understand when models have these quantified disparities, using a zero-one loss. And this was a very simple investigation of a model that I published at the beginning of my PhD, so way back in 2014, where we demonstrated that, just, even if you have a model, for example, that improves the overall accuracy. Let's say that doctors can predict a condition, like psychiatric readmission, with 60% accuracy, okay? If I told you that I had a model that could predict on average with 70% accuracy, but for women, it was 60%, and for men it was 80%, would you take that model? It does better than the average doctor. Are we comfortable increasing the accuracy, in unequal ways for different groups, even if it brings the total bar up. And that's not a question that technology or optimization can answer, we have to decide that as a society, right, we have to make that choice. The next thing that, you guys didn't get this reading because it's under review. Okay, does everybody know what a transformer is? Approximately, I see some nods. This is the thing you might've heard about in natural language processing that can generate humanish text, right? So it's very good at copying the kinds of text that maybe a person would generate. So this is a specific model, it's cyBERT, it's a public model, you can download it now, you could use it in your healthcare system to automatically generate notes, for example. People are doing this right now. I know several startups who are doing this right now, okay, they use the public cyBERT model, and they generate a note. Here's a prompt, this is a real prompt from the MIMIC dataset, okay? The prompt is, race, token, patient became belligerent and violent and was sent to, token, token. If I fill in the first token with Caucasian, a Caucasian or white patient became belligerent and violent, they were sent to the hospital. If I fill it in with African, African American, or black, I get prison. This is a publicly released model, and this is the actual blank that gets filled in. I thought maybe I would do like a live demonstration of it, but Karen said, you know, maybe we don't have enough time. So, there's code available later if you wanna talk to me about it. So this is really, really bad. One of the reviews that we got for this submission, the reviewer said, you're not tackling a real problem, there's no way people would ever use a transformer to fill in clinical text. And so, we had to reply and say, like, citation, citation, citation, the reviewer was still, just did not want to accept that this was actually happening, but it is. And this is something you really see, if you deploy this kind of model, you're going to get this kind of automatically generated report. And when we tried to debias these models, it's not successful. So these models have 12 out of 57 tasks with a statistically significant recall gap, and we tried to adversarially debias the model, which works in some other domains, it doesn't change the proportion significantly. So we went from the baseline model, having 12 recall gaps across the task, to the debiased one having 13 models with a significant recall gap. So just using out of the box adversarially debiasing doesn't work and we need to think more deeply about how we want to deploy these models and possibly make them fair. Another thing that I feel very strongly about is explainability and transparency. This is a hot topic right now in machine learning for health because we're tired of this, we're tired of people telling us let me throw a model over the fence, just trust it, right. Because if there are problems that a field has been struggling with for decades, you know, an algorithm probably isn't just going to fix it. But I think that the issue with talking about model transparency, model explainability, model simplicity, is that really what we're asking for is trust. But there's a problem with trust. The problem is, trust has a real impact. So these are my favorite set of examples of human trust. So we know when there is physician-patient race concordance, or gender concordance, patients do better. My favorite example of this is the last study. Gender concordance increases a patient's probability of heart attack survival, the effect is driven by increased mortality when male physicians treat female patients. But the best quote from this paper is this, mortality rates decrease when male physicians practice with more female colleagues or have treated more female patients in the past. Which means, women have a dose-response effect on men wanting to do better. That's great, right, that's fantastic. But while we can laugh about that here, in a real setting, we don't want, for example, a clinician to trust a model more just because they've seen it more, just because it's simpler, just because it's more explainable, right? You could imagine that that could go bad really quickly because if we have simple, explainable, interpretable models, there's some very good work in the robotics community that has said people tend to over trust those robots. And so there's a couple of scenarios where humans will place too much trust in a robot, they are, when a person believes the robot can perform a function it cannot. I can't take an MRI, but you can, machine. Or a person expects that the system will mitigate the risk, it'll tell you when you're overdosing the patient. So we don't want to turn off people's own alarm bells, we want people to think about audits here. There's some really good work, that I think you all should read, I sent you too many readings and so I didn't send this one as well. There's some very good work that's shown that simpler models don't help you make better decisions, they hamper your ability to detect when the model makes a simple mistake. And so, you know, just saying that a model is simple, or explainable, or transparent, isn't a fix, we still need to think really carefully. And the final thing I would say, when people say, yeah well, clinicians won't trust a black box, medicine is full of black boxes. We do not know how acetaminophen works, we don't know how lithium works, and we also don't know how Oscar the cat works. So this is a New England Journal paper, I'm siting from a reputable source, this is about Oscar the cat. My clinical collaborator told me about this, I didn't believe him, so I had to look up the book and the New England Journal article. This cat lived in a geriatric care facility and would go to a patient bed a few hours before they would die and curl up by its feet. And so, it was this, you know, fascinating predictor of mortality. If we trust the cat, we might trust a model, a robot, the question is, should we? And do we want people to be in a regime where they overtrust? And if we say that a model has to be simple, and explainable, and interpretable, I'm concerned that we might. So, I think that there's some transparency that we probably need. It's probably when clinical opinion differs significantly from a prediction, it doesn't necessarily mean a clinician cares about exactly how the model works. It could mean that they want to know when they can rely upon or reject a model output, when the model might be in a failure mode, when there is a limitation of what data the model is trained on, I've only been trained on one specific suburb in the United States, so I don't generalize to Toronto, for example. And in these kinds of cases, we can do transparency with post-hoc explanations, right, so that's like the inclusion map that I showed. People have used things like, attention-based or saliency maps. Those are not always consistent, there's a new proposal for computation-based maps, where you say that all labels have to be in competition for explanation with a pixel. I think that there are, so we have one example where we use a Hilbert-Schmidt independence criteria of regularization to say that we want to know which latent variable is tied most closely to the dosage of a specific drug to cells. So that as we dial along different dosages of this drug to the cells, we can hallucinate what the cell would look like. And so, I think there are options for explainability, or interpretability, but that shouldn't just default to things that we might overtrust, it should help us think through a problem. The other thing that we could do is transparency through natural outputs. Much like generating this radiology report, I'm not giving you a number, or a score, or a directive, I'm just giving you a report that you can edit, right. And here, another thing that I think that we should do more in machine learning is engage with the HCI community. So here I collaborated with some people on the Google brain team to create a visualization for patient data. So you can plug MIMIC data into it and it'll give you a pretty map so that you can try to interact with it, right. So if we could give people information in a way that they are able to, maybe play with it, modify it, maybe they can be more introspective about how they're making their own choices. Finally, reproducibility and robustness. Models are already being considered regulated advice givers by the FDA, right. So again, the cat's outta the bag, it's not like we're talking about if, or could, we're talking about when and how, right. So things are being cleared. I think that the FDA has done a very good job of engaging the scientific community, the public, many different people for comment and feedback on how regulation should happen. I think that we need to have a really honest conversation about what kind of robustness we want from our models, and correspondingly, how that translates into doctors who currently practice, right. So we're talking about regulating these models that can do these things, right. We don't regulate doctors like this. Doctors are a self-regulating body, right. Like, there's a bar, like, they pass it, and your colleagues decide when you're not doing your practice up to a certain level. There are standards that are imposed by your community, not by, for example, the FDA. And so, I think we have to make decisions about, when we're regulating algorithms that can do tasks at a certain level, does that impact how doctors do the same task or not, and how does that work in tandem. So, I think that there's also a lot of conversation within the community about robustness. Because if you've developed machine learning models, you know that sometimes they can be very fragile. So these are my two favorite examples. So, the first one is deep neural networks are easily fooled. So it's showing you the canonical king penguin, starfish, freight car, and remote control from a deep neural network. And the second one is this adversarial pixel idea, so it's a one pixel attack on a neural network. So that these state-of-the-art models think that a ship is a car, a horse is a frog, and a deer is an airplane. And these are funny because they're exploiting these inherent fragilities and the data that you've shown the model and how it may be overtrained for a specific dataset. And, you know, we've talked about explainability, right, you know, so you can ask, for example, why did you think this husky was a wolf, and the model says, hey, it was the snow, the snow fooled me, I've only ever seen wolves in the snow, huskies in playgrounds, and so, I didn't know, sorry. But these explanations are only really comforting in a domain where we're all natural born experts, right. We're all experts in natural language processing, and speech recognition, and visual evaluation, we're not all experts in chest X-rays or dermatology. And you have to go to school for decades to be experts in these things. And even when you are an expert in these things, I can't tell, and no radiologist that I know can tell the difference between that clean and that adversarially perturbed pneumothorax and normal chest X-ray. So that's why it's more worrying, that's why we talk a lot about robustness. Because attacks in this setting would be very hard to detect, even if they weren't very clever like this. And so, we don't want to over-rely on models, because sometimes they have inherent fragility that somebody could exploit. So, I think that there are many different kinds of reproducibility and robustness that we should think about. There's technical reproducibility, if you gave me the code and the data and I ran it, would I get exactly the graph that you gave me. There's statistical reproducibility, are you giving me variance, are you giving me arrow bars, right, with different random seeds on different cuts of this data, what is this variance that I'm reporting in my performance. And there's also conceptual reproducibility, which is called replicability by a large proportion of the clinical community. If I took this thing that you discovered in this dataset and I moved you to another place, another time, another loca-- whatever, would you find the same thing? And that's a much deeper question, right. Which is obviously the goal, but sometimes in machine learning, we don't even meet the first two, which I think is an important standard that we should all try to comply with. We also need to think very carefully about what happens with distribution shift and stationarity or lack thereof. So this is demonstrating that, right there, where those models crash, in a very sad way, there was a transition in the MIMIC dataset from one EHR system, CareVue to another EHR system, MetaVision. And so, these really wonderful, fancy neural networks are just not performing well because they don't know the mapping and it takes them a while to recover, right. And so, we need to make models that are going to be robust to, not just simple EHR shifts like this, but also to the onset of a new virus, the development of a new disease, the application to a new population, these are all very real, very plausible situations in which you might deploy a model. And then, I think that there's a large conversation around privacy and what the difference or definition of privacy in healthcare data should or could be. So, I talk a lot about the MIMIC dataset because I think it's a wonderful resource for my community. I'm often told, well why don't you just train differentially private models? So you never have to, you know, see the data, you can adjust noise and then clip the gradients, that's fine. So this is a work in submission demonstrating that if you want differentially private models, you pay a large price in performance. So, here we're showing the AUC and the AUPRC for these models. So, the shift down in shown in red. And so, you can see in the highest differential privacy setting, there is a significant gap in your ability to predict mortality in the MIMIC dataset, I've shown you many of these graphs. It shifts from the solid blue line, AUC, to the solid red line, AUC. From the dotted blue line, AUPRC, down to the dotted red line. And so, there's going to be a loss that we incur if we demand something like differential privacy, we have to decide if we're okay with that trade-off in this setting. And I think that we should reemphasize that there are tools to understand data and processes, right. So we have things like datasheets for datasets, model cards for model reporting. This is pointing towards, we need robust processes around data, it's not just a model, right. And there's really value in access to this data, this embodied data, it's data about human bodies, data, you know, for humans about humans. And we probably don't want to leave this as something that private industry can just do for us, right. They have resources, you know, they can acquire this data, they can learn interesting things. Because a resource like MIMIC has been around for over a decade, the IRB was created in 2013, or 2003, and it's still this fantastic resource that keeps going. And if we didn't have something like MIMIC, we wouldn't be able to create, develop, audit these kinds of models. And as a community, I think it's really important that we choose an ethos that we want to be part of, right. So for example, in the academic community, people can do state-of-the-art vision, you know, there's still vision hires in machine learning departments, we've put out these really great papers at CVPR, at NeurIPS, and ICML that are doing state-of-the-art machine learning in these vision contexts, right. And it's because we have open open datasets for imaging, large open datasets for imaging. And the community has made that choice, it's an ethos, right. In speech we don't. So you can't audit speech models that come out from Microsoft, or Google, or Facebook, or any of the providers. Amazon, we can't audit Alexa in the way that we can audit a visual recognition system because we just don't have that data, it doesn't exist, you can't train the models to the same capacity. And so, as a community if we say that access has to be gated and private behind these walls that really only people with very, very large amounts of funding are able to gain access to, we're handicapping our ability to do state-of-the-art science in this very important public space. So, my lab is composed of many fantastic individuals and a large part of the work that I've demonstrated today is due to their hard efforts. Thank you for listening. (audience clapping) - [Karen] We have about 10 minutes for Q&A, 10, 12. - Hold up your-- Yes? - [Audience Member] I wonder how different working with this problem has been in Canada versus the US given the differences in the healthcare systems? - Right. So the question is, how different is it working with American or Canadian healthcare data. There are pros and cons. In Canada, we don't have the first inequity, right. It's a single-payer system so everybody has exactly the same health insurance, right. There's bias everywhere, and so you still have some of the other problems. I think that there's also, so in the US we have HIPAA, right, so if something's deidentified, it's deidentified, it's legal under data Safe Harbor, you can distribute it, sell it, whatever. Canadians are much more conservative because they're part of the single-payer system with the ability for you to deidentify and distribute data. And so, I think while it's really fantastic that there's this pride about the single-payer system, they're also slightly more worried about letting people do things with it, because they're worried, well, what exactly are you going to do with it. So I think the ethos is nicer, but I think also, it comes with a little bit more reticence to use it in maybe the same ways. Yes? - [Audience Member] Didn't you say that this model might, it's got infinite qualities, but might be better than what is standard practice in clinical practice. If we say we accept it, but still people have a kind of diligence to prove this model, do you think that's a better option than-- (speech overlaps dialogue) - I do. So, like, just to be clear, I think that there are, I don't know if anybody has seen like, "Hidden Figures," right, so it's this really nice movie. The thing that I thought was really funny is they call the women who do the computations, calculators, or computers, right. They used to train children in middle school, like, middle school was mostly just adding stacks of numbers and doing, that's what you did in math. Because we needed humans to do that, we needed a tool, and humans were our tool. And now we recognize that, it's maybe nicer if we have a calculator or a computer that can do those things for us, that can be that tool, and then we can work on top of that for decision making. I think we should treat machine learning in the same way, right. If a model or simulation gives us something really crazy, we don't just believe it, hopefully, right, in most fields. But we use it, most of the time, within the conditions that we believe are reasonable. And so, I'm a very strong proponent of using machine learning. I think that the current state of where we are, with practice and with knowledge, should be improved. And I think that there are many tools in the machine learning library that can be used to improve it, many novel technical challenges, many interesting clinical challenges. But I don't want us to start seeing machine learning as some sort of magic wand that is different than these other tools that we've gotten very good at integrating. Yes? - [Audience Member] Can you talk about the difference between personalization and bias. Because, for example, the algorithm that you talked about, which is bad for women and good for men, it may be the case that the disease is inherently hard to predict in women and that's why the algorithm is bias. How you can you say that it is biased and not personalized? - So the question is, how do I say that something is biased or personalization. It's an interesting theoretical question, but practically, there is no question. A majority of these cases are biased, they are not personalization. So it's not that women, there are so many unfortunate examples of this. But the big one is, heart disease, or anything to do with cardiovascular conditions. Women will go into the room and say exactly the same thing as men and they just won't be referred for care. And so, you know, there's also several claims that it's harder to detect some things in women, that's because doctors aren't trained to. So it's not that there is something inherently, biologically different about a woman, for example, that makes it very hard to detect this condition, it's that the body of knowledge that we've generated disproportionately reflects a certain kind of person's experience. And so, we as a clinical community, as a machine learning community even, are trained to recognize that. It's the same thing as when you go into Google Images and when you look for a wedding, you get a white, poofy dress wedding. And if you look, for example, for an Indian wedding, it's tagged as performance art. It's because that is disproportionally the data that's been used to train this model. That doesn't reflect the reality of one being more of a wedding or less of a wedding. In the same way that, a heart condition is not, you know, different in these two settings. I think we need to unlearn some of these biases. There's a really great JAMA, I think opinion piece, last year, about the color of kidneys. Where if you've ever gotten a lab test, have you seen like, the eGFR, it's this lab value that you might get. There's this really weird sentence under it in most lab reports, it happened for me when I got a random blood test. It said, well, if you're African American, multiply this number by this constant. And so, I asked my friends who are in medical school in Boston, like, what is this? And they said, like, don't look at that. You know, I asked my attending what do I do if the patient is Hispanic, and they said, well how dark is the Hispanic person? And so, like, this is not, this is not something that we really believe is scientifically true to an extent anymore, right. I think a lot of what we've learned is inherent differences, that's biased perception, biased investigation, biased data collection. If there really is some difference, I'd like to see that established more robustly than the small datasets that we have and the small samples that we've focused on. Yes? Yeah! - [Audience Member] So, EPIC system EHR is widely used by institution, including Stanford, and they are developing models. Seems to have a widespread clientele. What do you think of the quality of the model that they're developing? And also for non-EPIC system is federated, the machine learning where data are kept within the institute and the machine learning model is distributed, how do you compare those two? - So, the first question is about EPIC, and I think EPIC specifically in the Stanford system and the reach of that population. So potential robustness of a deployment from data from the EPIC system. And the second question is about federated learning and pros and cons of that. So, I don't know how well the Stanford hospital dataset demographically compares to other locations yet. I would imagine that, depending on its size and specific demographic distribution, it probably is reasonably generalizable for some other parts of California, for example, but maybe not for, you know, Iowa or Idaho, right. Like, you could imagine that there are conditions that might be more representative here that are not in more rural locations, or places that get colder, or places that have a stronger industrialized industry part of their economy, right. So, I think it is caveated, we can't just say, because we think we have, like, a reasonably diverse population it might apply everywhere, you have to think about, like, the entire system that feeds people into healthcare. With respect to federated learning, I think it's like differential privacy, it's a good idea. I am very cautious, right. Models are fragile, we work very hard to make them as robust as possible and to address those fragilities. It's always nice to triple, double, a thousand percent check, and if you tell me I can't see the data, that I can't ever poke it and look at an example and see exactly what went wrong here, it makes me nervous. Specifically because, for example, let's say that there's a patient that I'm always getting wrong, I always predict too early that they should go off of a drug. What if that patient was left on the drug for too long? If I can't look at, for example, their corresponding clinical notes, I can't verify that, right. So we know that healthcare practice is practice, it's gonna vary by provider. What if there's a provider who's practice is significantly different from other people's practice? So I think being able to investigate and dig down into what's happening is a valuable resource occasionally. - [Audience Member] If there's tools for facilitating explainability per institution, would that help? - I don't think so. So the question was, if there's tools for explainability per institution would that help? I don't think so. I'd be happy to convince of that later. - [Audience Member] Okay. Yes? - [Audience Member] So you mentioned that one of the handicaps are the sales of private data, and it's been expensive to access. Do you see any progress with like, Blockchain or any other type of solutions? - So the question is, it's sometimes really expensive and difficult to access data, and are there other things, like, Blockchain or, you know, privacy, differential privacy, federated learning. Those are all examples of things that I think have been proposed. I don't see any strong progress in those directions yet. I am the biggest proponent of things like, MIMIC, or the UK Biobank, or the All of Us Project, because, for example, what if the metrics are wrong? Right, so there's a really great example of this that, it's not out yet, but I keep talking about it, the paper will be out soon. That somebody else did, I'm a big fan of hers, she's, Emma Pierson, here at Stanford. She did this really great paper where she looked at knee X-rays, it's NIH data, you can go download it, okay. With knee X-rays, you have these two scores, you have what the radiologist looks at the knees and says, ah, here's your KLG score, how bad are your knees, and then you have the patient's self-reported pain score, the KOOS score. And for a long time there's been this discrepancy between those two scores in poor patients, in black patients. So, you see, like, there's this difference, and you ask, why is there this difference? And the explanation to date has been, well, maybe, it's just in their heads, you know, if you're poor, you just have a harder life. And so when I ask you, how much pain are you in? You're just saying, well, I'm in this much pain, even though my score is here. And so, Emma has this really fantastic paper where she shows that you can use deep neural networks on just the X-ray image of the knees to explain the discrepancy in these scores. The pain is not in their heads, it is in their knees, it's just you haven't trained clinicians to make this score so that it reflects the diversity of what people's knees look like. When you trained them to understand what a KLG score is, it was probably on a well-off, college educated, white population, which is the volunteer population for most of our studies. We don't wanna generalize things that we learn on Yale and MIT undergraduates, right, like, that's probably not representative. So I think that there's a lot of value in data that everybody can access and not things that are behind a wall. - [Karen] Out of time, maybe one more question. - Last question. - [Audience Member] Okay, so for AI becoming more integrated into the healthcare system, do you think the biggest challenges are more technical or social? And by social I mean, say, privacy, or litigation, or mistrust. - Right, yeah, yeah. So the question is, do I think that the integration of healthcare, of machine learning into the healthcare system, has more technical or social barriers? Social, 100%, there's no question. There are all of the wrinkles of, do I work well with this EHR system. But, for example, let's say that I could make a model that would predict accurately, 99% of time, some condition. Should I show that to you in red text or black text? At the beginning of your shift, should I send you a text message, should I beep your pager, should I put it in the EHR, should I circle the information that I used to make it? We have no idea how to help humans make the decision in a task-based setting. If it works well for a radiologist, will it work well for a pathologist? Will it work well for a psychiatrist? Is it gonna work well for a family doc, pediatrician? We have no idea. There's this entire middle space of, I have a great model, you might want to use it, how do I actually make that work in a task-specific setting where you're practicing, that has not been investigated. And I think some of the avenues that the FDA is exploring, for saying, this algorithm is certified, you can go ahead and try it, are reasonable. But I think we also need this safe space. Because right now the analogy that a professor here at Stanford, Neekam Shaw made, very eloquently was, we can test the windshield wiper, we can test the rear-view mirror, we can test the wheels, nobody's letting us drive the car. I can show you that my model works on MIMIC, and then I can show you that it works in this federated space, and then I can show you that it's differentially private. Nobody has a test space where I can, in silent mode, see how well I do on a novel task, whether doctors like the way I'm giving them this input. It's often very intense negotiation under non-REB settings, under this quality improvement lens that you have to do these kinds of things, right. And so, I think it's mostly social. All right, thank you. - [Audience Member] Thank you. (audience clapping) 