  Hello, everyone. My name is Witalij Rudnicki. I'm one of SAP Developer Advocates, and this is Do-It-Yourself Machine Learning session. So what is machine learning? If you look at Google trends, this is certainly something that is on the rise. But some people are saying this is just the latest iteration of something that happened long before this-- predictive analytics, data mining, or even earlier statistics. And some others are saying there is no machine learning anymore because now it is all about artificial intelligence. Whatever name we are using, we are talking mostly about the same thing. We can see applications of everyday machine learning in our consumer life, for example, with the face detection in our mobile applications. On the other hand, once we are talking about the enterprise machine learning-- and this is what we are focusing at SAP-- we are dealing with business data, and we are trying to find some hidden insights and discover some new patterns in the data that our customers already have. And this is where the current decade that we call Intelligent Enterprise decade is leveraging machine learning and artificial intelligence for your business benefit. But under this, no matter which name we are going to use, there are bunch of statistical formulas. The good thing that you don't need to know this formulas by heart because they are already implemented in the form of machine learning algorithms. This is why SAP HANA comes with two embedded machine learning libraries. The first one is Predictive Analytics Library. It has almost 100 classic and trending machine learning algorithms already implemented. And by executing these algorithms in database, we can leverage the fact that we don't need to move massive amounts of data from the database to application layer to process, and we can leverage the high end hardware of SAP HANA server to do massive parallel processing. Predictive Analytics Library delivers these algorithms in eight different categories, and let us have a look at one of these in our first demo. Let me move to my demo environment because Jupyter Labs is the most popular environment recently among all the data scientists and data engineers. This is what I'm going to use for my demo as well. Because I want to query SAP HANA via SQL, I need to first load the SQL extension, and this is something that will allow me to include so-called SQL magic at the beginning of the cell that would connect to HANA system. And for that I have installed already SQLAlchemy dialect for SAP HANA as well. Let me switch to presentation mode to make the font somewhat bigger. So I now can connect to HANA using SQL. I'm connecting using the demo user key from hdbuserstore You can verify this as well here from the hdbsql command line. I am connecting with the user key DEMO USER to execute the SQL statement, and this database user is called HANAML. And this is what we should get here as well. The example that I'm going to use is coming from the HANA ML samples repository available publicly and for free at GitHub. There are different examples for PAL SQL. The one that I am going to use is Random Decision Trees, and I picked this example somewhat randomly as well. Random decision trees algorithm requires several tables to train the machine learning model-- obviously, the input data then some parameters that we can tune to make this model better. The output is the table with the model itself plus three tables describing the model. I loaded already the training data set and test data sets into the schema DM_PAL. So let me set the schema as active, and now let us move to the training phase of the machine learning model. With the training data, we have a table that defines if we should play or not play outdoor based on the current weather condition. And this further is described using four columns-- outlook, temperature, humidity, windy. Temperature is obviously in Fahrenheit. Then the second table, as we saw in the documentation, is the parameter table. So let me create this table as well. But for the moment, we are not going to populate this with any parameters. We're just going to leave default settings to make it simple, and then we need to create a table to store machine learning model. Now that we have all the required tables created, let us train the model. This is the process that sometimes is called as well fitting the model. So as you can see, I'm going to call PAL procedure, called random decision trees, and the input is the table with data, table that describes the parameter. Once again, we left this table empty, and therefore we are just using all the default values. And the output will be written to the model table that we have created. Now that the training step is executed, you can see we have 100 records in the model table which describe the model created during the previous step. Once we are done with training the model, we can move to the prediction step. And for this, I do have test data loaded as well, seven records describing different weather conditions. And for the combination of this outlook-- temperature, humidity, and windy-- we want to calculate should we play or not outdoor. So once again I'm calling the PAL algorithm for random decision trees, but this time to predict. And we have the input table with the data, we have table with the model. And here is the result, which says that, in first case, even though it is raining with the confidence 0.76 based on the previous data used to train the model, these are the good conditions to play. As I mentioned, we used the default values to train this model. So now let us change these parameters. Back to the place where this parameters table was defined. Let us set the parameters which are described in the documentation. We need to drop the existing model first and to recreate the table, and now we can train the model once again. This time we have 300 records to describe the model, which would fit one of the parameters I set. And let me call this model once again. This time, we received exactly the same recommendations as before. But as you can see, now model is more confident that still, even though it is raining in the first case, we should go outside and play. Now let us run the same algorithm on the same data, but using Python. SAP provided very popular Python Client API for Machine Learning Algorithms. You can find this in the documentation as well, which allows you to run algorithms from Python using HANA data frames where data still resides in the database itself. First of all, I am importing the HANA ML library, which is already installed in my environment, and I'm connecting once again to the database using the same demo user. The training data is exactly the same. We just point training data HANA data frame to the table in the database. And to be able to display this data, we need to use collect method. Training model is very simple. We are just importing random forest classifier, the same that we used in the previous example. If you want to look at the signature of this method, what parameters can be included and what their default values are, but once again, we are going to leave all parameters default. So let us create the object for this classifier. And training the model is as simple as this one line calling the method fit on training data. Now that it is executed, we can see the model itself. Once again, it has 100 rows describing rules, and each rule is described using PMML, which is an open standard markup language for predictive models. We can have a look at the importance of each variable. The weather outlook gets the highest importance, according to the model that has been created. And once again, we can now run the prediction using the model created in the previous step. Let's load the test data. No data is being brought from database to the client side to execute any algorithm. It is only once we want to display the data and we call the method collect this data is transferred to the client and displayed. I want it to show as well here: I did not use HANA table, but I used this SELECT, which included ORDER BY statement. And for the test data object defined this way, we can check what is the select statement on the HANA side. Let's now use this model to predict. The prediction method is executed, and here is the result. The example that we've seen with the clustering is one that is following the supervised learning paradigm. This is where we have the training example, or labeled training set, and we teach the model how to label the future test sets. The other example is unsupervised learning. This is something that is used in clustering algorithms where we have just an number of records and algorithm by itself trying to build the model, cluster these records, according to some patterns. The third machine learning paradigm is the reinforcement learning, and this is the one that is used, for example, in machine translation or chatbots. Let us have another look at the clustering algorithms. Here in the example, we have exactly the same data, but you can see how different clustering algorithms can split this same data into different clusters. So now you would need to spend your time trying and sampling to find which one to choose. This is where data scientists can spend lots of their time, including the step of hyperparameter tuning. And this is the area that can be easily automated, and this is why SAP HANA comes as well with Automated Predictive Library. This library automates most of the steps which are related to creating and training predictive models. Let's have a look at the example of APL function in our next demo. For this demo, we are back to Jupyter, although this time we are going to run demo using HANA Automated Predictive Library, or APL. Same as with previous example, I am using data and the code that you can use too by going to HANA ML Samples Repository on GitHub. And for this demo, I'm going to use this fraud example. You can find APL documentation as well on help.sap.com. For now, let's move back to code. Once again, because we're using SQL, I need to load the extension into the Jupyter. And I am connecting to SAP HANA system using the user HANA ML. One thing I want to check first is that everything is set properly for this demo. There are multiple tables with data used in APL examples. In my case, I have all of these tables already loaded. And because I'm going to use this fraud scenario, I need these two: table-- existing claims where fraud was already identified, this table with new claims. Second thing, there are some procedures that are called technical. One of them is ping, that allows you to check that APL is available and properly configured. Everything looks good. So now let's move to the training phase. And similarly to PAL, we need to check that our training data is available. I selected five records that were identified as a fraud and five that were not. This is the column, and then there are many different variables that can be used to build a machine learning model. For the training setup, I need to create some tables required by the APL procedure definition. And now we can move to training the model. Once again, what is different between APL and PAL is that APL helps you to automatically pick the best algorithm and as well the best type of parameter, something that, if you remember, in PAL, you as a now data scientist would need to run through multiple iterations trying which hyperparameters would produce the model with the best quality. Now that the model is trained, we can go to the evaluation step. There are several queries which would help us to check this. For the model overview, we can see that out of the 15 columns in the input table, 12 where used as the variables, and we have a total of 2,000 records. Next query show that for this target variable, 88% of the records have value "no" and 11 percent have value "yes". Let's move on to partitions. The answer tells us that 72% of input records were used to build the model, and then 27% of the remaining records were used for validation that the built model is proper. Descriptive statistics help us to understand numeric variables. The values for the column H were between 18 and 97, or days to report between 0 and 30, and 14 as the mean value. Features importance shows which variables, according to the model, contribute the highest weight to calculate the target variable. So in this case, bodily injury amount is the one that has the highest weight. Let's move on to the accuracy. Based on the input data, SAP HANA APL was able to produce the model with quite a high accuracy of 96%. Now that the model is built, we can move to predict step. There are 147 records in the test data table. We can have a look at some records. They have the same columns that were used to train the model as well. For prediction configuration, we need to create some tables. I'm not going too deep into the details here. It's all well documented. And now, once all these tables are created, we can finally move to the step where we apply the model using our test data. The result is available in the table APPLY LOG And to make it more readable, I'm using the same SQL statement as in sample here. We can see for claims that were marked as potential fraud, we've got the probability , the score, and as well top three variables influencing this decision. In this demo, I focused only on a SQL side of using Automated Predictive Library, but in samples you can find for the Python API code for APL. And indeed, if we go through the documentation of the Python client API, then you can see that APL is available here, similarly to the previously used PAL. APL is widely used in SAP own software. Should you use SAP Analytics Cloud and you want to create a new story, one of the options to create a new story is to run a smart discovery on the model that you have already. On Best Run Juice sample model where we can say quantity sold is the measure we are mostly interested in. I'm not going to run the smart discovery right now because it would run for about two minutes. And save our time, I have this already created. And here's the result of the smart discovery execution in SAP Analytics Cloud. Once again, let me remind you it was Automated Predictive Library to calculate, for example, key influencers of the quantity sold, or as well to calculate some outliers that now you as a business user might want to look at. Let's have a look at the complete process of working on machine learning. It all starts with identifying the data source that we need to use. And then, actually, about 80% of the time is spent on data preprocessing. This is where we need to understand the data. This is where we need to profile the data. This is where we need to do the data clean up, maybe some data transformation, so that we are bringing this data set to the format that can be used in the creation of our model. Then there are multiple cycles to do the model training and model validation so that we are getting to the model with the best possible score. And then this model can be deployed and now can be integrated into your applications and used by the developers by just calling the API of this model to do the data prediction. We need to understand as well that, as time passes, these models can become outdated, and therefore it is quite a constant loop where we are going through this process of checking the quality of the model that has been productized, and then maybe re-training and re-validating this model. As I mentioned, this data preprocessing and model creation can take significant amount of the time. But the good news is that you can use models that have been already built by someone else, and then you just give your developers access to the APIs and these APIs can be embedded in the applications. SAP comes with a group of the already trained models in SAP AI Business Services, and we are going to look at one of the services where the model is already created and we are just calling this model to do the document identification. Let's have a look and Document Information Extraction, which is one of SAP AI Business Services. This service uses machine learning to automate Document Information Extraction process. Imagine you have an invoice, and you want to avoid the manual process of checking and then typing in your system when this invoice was issued, what is the gross amount of the invoice, in the what currency, and so on. This service is available as well in SAP Cloud Platform Trial Account, and you can quickly set this service up by using the booster provided for that. I executed all these steps, so the service and the service keys are already created in my account. I have a service instance of Document Information Extraction. I have a service key already, and I used all this information to log on to Document Information Extraction Swagger UI, which now allows me to test these APIs. I will focus on the very core of this service, and therefore I have some of the required steps already executed. What I really wanted to show you is how to do the information extraction from the invoice. I am using this document jobs API. I authorized already myself in this UI using the service key. I can execute. So here I have some sample invoices. I'm not going to change anything. And let me execute this step. The invoice had been uploaded to my instance of the service. Inference job has been triggered. It is in status "pending". Right now let me check its status and get it from this API. Set the ID and the status of job. Sorry. And the status of job is "done". So my simple invoice processed, and we can verify extracted information by comparing this to the invoice visually. Here is this simple invoice. It says that its total amount is $93 and 1/2. So indeed, the amount is in US dollars with a confidence 76%. And the gross amount is 93.5, and the document number is INV 3337. INV 3337 is what we can see on the invoice. Its invoice date is January 25, 2016. And this is what we can see has been extracted by the model as well. So you could just simply use the trained model by calling its API from your own application, and there is no need for you to spend a lot of time selecting, training, and tuning this model by yourself. If you would like to try these services in your own SAP Cloud Platform Trial Account, go to developers.sap.com, Tutorials, look for AI Tutorials, and then choose the services interesting for you to try them out. One more important aspect to mention when we are talking about the machine learning is that, in the past, these machine learning models were considered as black boxes. So basically, we know what we have on the input. We know what we are getting from the output. But we do not always know what is happening inside of the model, especially if we are talking about the models that are based on neural networks. That's why the area of Explainable AI is something where lots of data scientists are spending time right now. Let us have a look at this example provided by one of the universities where we have a model that was trained to differentiate if picture that is supplied to the model represents a wolf or a husky. And looking at the results, seems like the model behaves quite OK. There is only one instance, one image, that was wrongly classified, where the model predicted that it was the wolf, while it was, in fact, a husky. And we would say that the model works OK. But it is only once the explanation was provided, which pixels have been used to identify if this is a wolf or a dog on the picture, scientists found that the model was primarily trained to recognize that there is snow on the picture. Because in the trained set, all the pictures with wolves had snow in the background, the model learned to differentiate if there is snow or if there is no snow, and then it would classify the picture as a husky. This kind of explainable AI is extremely important for us at SAP as well because the model that we are producing, or the models that you want to use in your business, for example, to make a decision if particular product should be purchased or not, cannot rely only on some black box. But you need to understand as well why this machine learning model suggested you to make this or another decision. Our machine learning engineers who are working on SAP Cash Application came up with a project that they made open source and that is available for everyone to come to get to that is called SAP Contextual AI. Let us have a look at the example what Contextual AI can do. Contextual AI is an open source project from SAP that you can find at GitHub. It spans three pillars of explainability, each addressing a different stage of machine learning solution lifecycle. This library provides several features for data exploration, such as distributional analysis or data validation for training stage, including feature importance or simple error analysis, and inference stage, like explanations for individual predictions. Finally, this project provides a compiler, a component that aggregates the output of the above functions into a single PDF called Explainability Report. Let's have a look at one of these. I'm using a Python notebook provided as one of the tutorials. Here we are looking at the automobile data set. Again, we need just to execute several steps, loading data, maybe doing some quick check, followed by feature quantity engineering, and then maybe visually understanding what we have in this data. Here data is sorted alphabetically by the make of the car, but we could sort it as well by price. Then we do usual split of data into the training set and validation, executing the model training and then saving this model as a Python pickle object. And now we are involving Contextual AI compiler to produce the final Explainability Report. These warnings are not relevant for us right now, and the Render function is the one that is going to produce the final PDF report. We can ignore these warnings for the moment as well. The PDF report has been produced. I can as well output its path and the name. Let's have a look at this PDF. So as promised, it just provides data statistics analysis for the attributes and as well analysis of the future importance for the model generated. It's interesting to see that curb weight is the one that influences the price the most, according to the model. Maybe this is expected-- the bigger the car, the more expensive it is. Because this project has been released as an open source, so the contribution is welcome. I hope you enjoyed this quick introduction into machine learning as well the demos that I showed you that would give you some understanding of the technologies we produced and we are using at SAP in the area of machine learning. I would suggest that you join the following SAP TechEd sessions, and as well I would like to see you joining our SAP Community as well as checking our data management website for SAP Developers at developers.sap.com, where you can find access to more free trials and tutorials that you can use to learn. There is even more learning experience from SAP, so please do not pass on these opportunities neither. And with that, I would like to thank you for your attention. Once again, my name is Witalij, one of SAP Developers Advocates, helping you to dig into the world of machine learning. [MUSIC PLAYING] 