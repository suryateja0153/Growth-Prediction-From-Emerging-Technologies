 in this video we're going to show a demo of a neural network model performing real-time image recognition on a cortex m7 processor using arms CM CNN library this highlights the fact that you don't need a high spec machine or cloud compute to do real-time machine learning tasks we've made it so that you can do them fast and efficiently on embedded devices will briefly touch on all the steps that we took to get this up and running and we've made all of the files available on github here for you to do this yourself in a Linux environment so here we're using an stm32 FCM board and we've got an ST camera connected as with any peripheral we'll need the basic program for the camera to interact with the board but we'll come back to this later on as we're only interested in optimizing our model with CMC CNN the steps we're going to show here apply to any board with a cortex-m processor so the first step in our demo is selecting and training a model and for a model to fit and run on a constrained device it needs to be small so here we're using the C file 10 data set which has been trained with Caffe available here the C file 10 data set consists of 60,000 32 by 32 color images in these 10 classes and we're using a three layer convolutional neural network which we can see illustrated here so we've already trained our model so we're just going to show you how to get it to run on a cortex-m device it's important to note that CMC's n ends optimizations make use of sim D instructions because only cortex m4 & m7 course support sim D you'll only see the performance benefits with these two cores the next step in the process is to quantize the model now this is a key step for being able to deploy a model on a resource-constrained device like a micro controller as it greatly reduces the size of the model by converting the 32-bit floating-point model to an 8-bit fixed point model as well as improving the overall compute performance this only has a very small impact on the accuracy of the model it goes from 80 point three percent accuracy when is unwanted just to seventy nine point nine percent accuracy after quantization so here we navigate to the directory that we downloaded from github and then in the cmcm folder we can see all the scripts and files that you'll need to generate a quantized model that you can then deploy so using this command here we're running this Python script to quantize our C file 10 model for cortex-m 7 here we've specified the weight and also the location of where we'll save the quantize model so now we run that and this is a good time to take a break and do something else as it can take a couple of hours to complete on just a CPU so now the script passes the network graph connectivity and then it finds the right quantization parameters to quantize so when the quantization is completed we need to transform the model operations and network graph connectivity and generate the code we need consisting of neural network function calls essentially we're transforming the model from a cafe format to a C former so we run the transform Python script on our quantized model and here we specify an output directory and this generates these files so let's take a quick look in our main file and we can see that the transformation has defined all of the layers needed for this particular network and it's generated all of these function calls that call the CMC's and n library functions these functions are for the different layers extracted from our trained cafe model with the different weights and then here we have a mock main function and this shows the run underscore an end call that runs all of the different layers and saves the alper on the buffer but because it's a mock function we need to incorporate this function into code that actually captures images from the camera and displays them on the screen so for simplicity we've combined part of the C program the run underscore and N function with the basic program that comes with the camera and if you want to run this for a different application you'll need to do this with the program you use after we've combined the code we then run a make file as a way of compiling the combined code so from this we've generated two files a hex file and a bin file and we can see both of these in our build folder and now the final step is to upload one of these files to the board and then we can see the program in action so here we've got the final program running on the board and we can see the image classification in real time and it's performing very consistently with a high degree of accuracy and we can see that the time taken to compute the result of an image through the network is very fast so again we've made all of the scripts available that you need to quantize your model and to convert it to C code here on github and for detailed step-by-step instructions on how to run this demo yourself you can read the guide we've put together on our developer website by following the link in the description 