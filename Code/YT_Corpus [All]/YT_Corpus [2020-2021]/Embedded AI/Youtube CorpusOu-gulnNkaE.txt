 hi everyone I'm Billy Rutledge from the aiy team at Google and we're here today at CES and the XP pavilion to introduce our new product the edge CPU dev board that features our edge TPU chip combined with the NX bi mx8 SOC as a kid for developers to experiment with artificial intelligence for the first time so the board itself is actually two pieces it's the base board here which has all the connectors that most developers would use to prototype a new product idea and then the just saw module som includes the CPU GPU and CPU chip as well as the memory and Wi-Fi and Bluetooth and it actually snaps into the base board using high density connectors and so that allows you to experiment with the actual hardware in a development setting but be able to buy the somme part 4 production line when you're ready to take your smart speaker smart dishwasher smart TV to a scalable to production plan so today we're showcasing a few different demos of how you might experiment with this type of technology and hoping to inspire people to explore using AI on the edge hi my name is Peter Mulkey and I work for Google I'm a software chickweed for my projects and today we're showing you a demo of facial detection that runs on HTTP you the key point about HTTP U is the privacy and security from now on your pixels do not need to travel to a data center you do not need to contribute your data to any company you can run all your machine learning inference locally on the chipset in this case in particular we've trade a network that can recognize human face and it's running locally on device on a small embedded system that runs Linux hi my name is Jun taegons I'm actually one of the software engineers working on Aoi projects one of our demos here at CES is actually a feasible machine where we actually use local inference to train a model directly on the device with no network connectivity we call this our teachable machine demo and it's right here and essentially it has a camera pointing up at the sky now the first thing I have to do is train it to teach it about what the background is so it can differentiate between the objects I'm about to show it and what the background is the first thing I do is I press one of these buttons to actually tell it what it's looking at so now it knows what the background is I can now train it on an object in this particular case I'm going to use this ice cream so hold the ice cream over press the button and now can differentiate between background and ice cream and you know it's machine learning and doing inference because I can show it a different color and get the same result and it can be extended to other objects as well so such as this hot dog so hot dog nice cream hot dog again and the same thing with the donut so don't I hotdogs and ice cream hello my name is Manny and I work for Google for a white project and I will talk about our well type demo here so if you're not you can notice that under each of they are bigger demos we have a small display underneath with the left camera and it shows the time and that's important characteristic because it tracks how much time people spend looking at the other bigger demo and you can notice that we display the bounding boxes around the people faces so here is my face and there is a green board showing that I'm looking toward the demo stand and if I turn away like right now you can probably see the red box I'm not sure myself but it should have been red and this has been run totally on the development board you can notice that behind this light we have the same words like alveolar damage here so there is no internet or cloud connection required and that's a good application because we are ourselves ourselves interested how how many people are looking at other bigger demos and you can notice like in the middle we have something like four and a half hours right now and in the corner stand there's like slightly more than three hours it's quite explainable but still interesting statistics together so we're just beginning to scratch the surface of what's possible with artificial intelligence today and we're excited to offer the Google edge TPU dev kit for the world to experiment with on device a I specifically to explore the capabilities with high performance on device security with having all the data on the board itself and performance by being able to process everything locally on the machine we think it will open up a world of opportunities for new product development we're excited to see what you might build with it next [Music] 