 hi folks welcome to the near fridge a twitch channel my name is will and we will be working with graph QL and grand stack today every Thursday at 2 p.m. Pacific so last time we were working on building a real estate search app with grand stack kind of like a Zillow clone and we're gonna pick that up today last time we sort of got everything started from the grand stack starter project and then we deployed to nullify and we started looking at some data modelling using the arrows diagramming tool so I pushed up everything to github I'll drop this link in the chat here there we go so if you want to follow along this is the github project and last time we just used the grand stack starter project to create our sort of skeleton starter with the graphical API react application on top of near 4j last time we used a neo4j sandbox instance to do this here's sort of just a kind of a rough sketch of what we're building so the front end this is a react application we use Apollo client to make data fetching queries to a graphical API our graphical API is no js' graph QL server that's built with the FDA graph kihl-jae s and uses the new javascript driver to send queries to near 4j previously we had this in a sandbox and then queries come back data comes back through the graphical API and and to our front-end so that's the the basic architecture this is the data model that we had sort of sketched out last time just kind of the initial nodes and relationships there and I talked a little bit about how data modeling is often driven by the requirements of our application so I think of this as this iterative process that we go through where we start defining the requirements of our application and then we identify what are the entities those are the nodes how are they connected those are relationships and as we start to draw out our data model we start to look at the questions that we want to ask of the data so as a user I want to search for properties for sale in a specific city so that I can view property details and I want to at least think of some traversal through the graph if if I can write the cipher query for it that's that's great so that I know that yes this data model addresses the requirements of my application right so let's take a look at an example of what we want to build here so this is Zillow it's a real estate search application we want to look for properties for sale in this case in in San Mateo right and so I can search by city and and view property listings I can view detailed information about a property such as its square footage the the number of bedrooms and so on all right so that's the basic idea for this for this requirement so what I want to do in today's session is pick up where we left off with our graph data modeling I want to see sort of that we've addressed some of our use cases with this data model and maybe we need to tweak this a little bit and then I'd like to get started with importing some data into engineer 4j and then see if we can start querying it with graphic QL so first a little bit about data modeling so we were using this tool called eros which is this graph data modeling graph diagramming tool it's hosted on the web it's it's very simple it uses local storage to store our model which we can then exports in various ways I mean drop a link to that in the chat cool so one thing that I really like about arrows is that we can check in this markup that defines our data model into version control so here I've I've pushed the mark up for my data model up to github so I can just copy this out of the readme and then if I go in arrows to export markup and paste in what I copied out from the readme that will load my my data model and then I can go in and and start tweaking things cool so in this case let's look at what requirements we've identified so far let's clean this up a little bit so we said we have listings listings were created at a certain time so any okay we have date/time object that we can use then we have this boolean flag as this is this listing active and we'll probably have other information on our listing maybe such as like asking price which is going to be an integer so this is a listing of a property and maybe this property is in a specific city it's reversed the direction of that okay and then our property has a address it has a location and then it has bounds so location is a single point so in this case latitude and longitude and then also it has this bounds which is property which is an array of points and this is what's going to be defining the geometry essentially a polygon of latitude and longitude points that are going to define the boundaries of the property parcel and why do we have both of these well we have one latitude and longitude to represent if we just want to annotate a map and show show the points on a map so let's take a look here so Matteo let's choose this one right so we can see that okay it's annotated on the map as a single point but if I if I movie view the details of the property and if I view the map for the details of the property now I have the ability to view the lot lines and the lot lines are going to show me how big in the dimensions of the actual lot for the property so we keep track of not only a single latitude and longitude point to represent the property so that I can annotate on the map and so that I can use that in maybe search on the map something like that then also the bounds because I actually want to do work with that polygon geometry okay so that's what I have so far let's look at some of the requirements that we have here so second one we identified is as a user I want to limit my search to properties with certain attributes or range of attributes so that I can narrow the results to those relevant for me okay so what does that mean well if I look at this example in Zillow let's close out of the property details I have let's say like a bed and bath filter so maybe I want at least two bedrooms let's say at least four bedrooms and at least two bathrooms all right and now this is changing my search results and there's a lot more I can filter by things like home type by things like price range and so on so let's take a look at our graph model now last time when when we were talking about this I made the distinction between a listing and a property and the reason for that is because listing is sort of at a point in time I may have a listing of a property maybe it sells and then two years later the people who bought that property then they listed and sell it and so it's really a listing of the same property so I can have multiple listings per property and the question is okay well I have lots of detailed information about the property and the listing things like number of bedrooms square footage and so on and the question is okay does that belong on the listing or does that belong on the property here we've started to add it to the listing so here we have number of bedrooms as a property on the listing and this is this is a good question this is sort of getting at this idea of entity versus state right so the property is is kind of the entity and maybe when it's listed once it has two bedrooms but then someone bought it and did a remodel and this time it has three bedrooms or perhaps I know there's there can be some discrepancies and how things like square footage are calculated so maybe the next time that it's listed it's listed as maybe slightly different square footage and so on so the way I like to think of this is the property that represents the entity and the things that are never going to change should be properties of the entity in this case the property node so things like address things like the location latitude and longitude those are not going to change but the detailed information of the listing that may change based on sort of changes through time so that is sort of the state of the entity so anyway that's how I like to think of that but our question was can we sort of envision a graph traversal where we are searching now for properties in San Mateo that have at least two bedrooms and four bathrooms or four bedrooms two bathrooms whatever so yeah if I have bedrooms I can also add things like bathrooms and so on I can add things like square footage and so on so now when I'm searching I just have a predicate that is going to specify the number of bedrooms it's at least four bathrooms at least two and so on okay so this this requirement I think is is addressed by our data model and then the next requirement that we identified last time is a user searching for properties I want to view property details so I can learn more about the listing yeah and again I think that's that's addressed in our data model since we have the property details information in the listing we can think of things we might want to add here and when we get to adding the functionality of creating a listing for a property we'll see maybe how to address some of the specifics but sure we can see that we just sort of need to return the properties of this node to be able to show the the user some detailed information about the property cool there we go okay so the that's sort of addressing the requirements with our data model let me check the chat here just make sure we don't not missing any questions you can feel free to reach out and in the chat if you have any questions or or thoughts okay so that's that I think is a good initial discussion of graph data modeling again this is this is an iterative process as we discover more requirements we may need to update our data model and that may then inform some of the different queries that we run so the next thing I want to explore today is starting to import some data into neo4j and start to sort of see how we can use graphic QL to start to query here for J and I thought it would be interesting to start to look at how we can start to import this property and in the city node as well so if we again if we jump back to our example Zillow here where we're searching in San Mateo and remember if we zoom in really far we can see that well we have lots of information about all of these properties within with in San Mateo both you know we know even if they're not for sale the bounds of the property we know the the square footage we can see kind of a Google Streetview we have an estimate of how much the house is worth we have things like property taxes and and so on and and this data in the u.s. anyway is for the most part public data information on parcels and property taxes and size of the house and so on this is typically public information that is available at the the state or county level in the US I'm not sure how this works in Europe but anyway I thought it would be interesting to start to import some of this data engineer 4j and see how we can build a graphical API on top of that so I found for the state of Montana some of this data so I was working on a unrelated project and came across that so we'll start here importing some data some parcel data from the state of Montana this is their their FTP server which I'll I'll post a link to in the readme and also add all the all the steps we go through to import this so what we want to do let's pick start with Gallatin County and we can see we have sort of XML file that tells us what the data looks like and then we have a few different versions of this data we can download one is a gdb file which i think is some sort of geospatial database format that one I'm not too familiar with and but the other two formats are shape files so I'm going to copy the link address for this one and let's jump into a terminal so this is this a bit bigger this is beautiful so here I am in the willow grande stack directory let's create a data directory and let's hold down this zip file at the BART some reason that didn't work you not W get a zip file that's fine let's do a save as okay let's say this to you want have it in I think I have it in my home folder just sitting there willow grand stack and data okay cool let's unzip that okay and so we have you have some shake files now that has parcel information for Gallatin County Montana what we want to do now is load this data into neo4j how do we how do we do that well if we take a look at at the shape file we can see that this is some this is a binary format so there is a plug-in that allows us to load shape files engineer forge a there's a spatial extension but what I'm gonna do today is convert the shapefile into geo Jason so geo JSON is just JSON for a specific schema so it has specific structure for storing things like geometries and metadata about those geometries and Zhu Jason is nice because once we have that its first of all it's basically human readable we can look at a JSON file and sort of understand what data is there but then also there's a whole suite of tooling that we can use for importing JSON engineer for J for transforming it and so on so I'm going to use a tool that's part of G da let's take a look at that close that notification I'll drop this link in the chat to Sochi Dollaz is part of this suite of command line geospatial tools this specifically is useful for transforming geospatial data files into different formats I installed this this is a Python command line tool I think I installed this through through anaconda there's different ways to install these tools specifically the tool we want to use is ogr to ogr so this is a command line tool that is going to allow us to convert from the binary sheet files that we downloaded into geo json so let's go ahead and do that so what I want to do is a gr - o gr I want to generate this file called Gallatin geo JSON this - F this is just saying what format do I want to output right so I could go from geo JSON to a shake file or lots of other different formats but in this case I want to output geo JSON this lco so this is an option that's gonna allow me to specify what is the ID field in the Geo JSON and in this case I just happen to know that we have a filled parcel ID for each parcel and we want to treat that as the ID field in our geo JSON okay this next option let's look at the docs for this there's T underscore SRS so this is saying okay which which transformation do you want to use which coordinate reference system do you want to use and I forget which the shape file is using but we want to specifically use to be just a t4 projection also known as the EPS g4 two three six projection this is I think commonly called the geographic reference that's what we want to use there and then the file we want to pull in is the shape file that we just downloaded so let's run this and this will take a minute or so if you hear a bit of background noise my neighbors are getting some trees cut down so apologize for that let's just chainsaw noise in the background okay so now we should have a Gallatin geo JSON shape file or sorry geo JSON file so here we're just looking at the first few lines of it so here's just sort of some metadata now we have an array with the key features and then for each one of these features we can see that we have first of all telling us the type feature okay great we have an ID this is where we specified parcel ID from the shape file up here and then we have an object properties for each one of these features and within properties we have things like the GIS acres so I guess that's the acreage for this parcel that we're looking at we have some information about property taxes in this case so for the 2020 tax year that we have the assessment code we have things like the address how big is it and so on I think we for some of these we even have information about number of bedrooms and so on things like that cool and then we have a geometry property let's take a look at that so here's our geometry this is of type polygon and here are the coordinates so this is a list of latitude and longitudes pears so this is what forms the the polygon of of the parcel cool so we're getting on time here okay yeah plenty of time great okay so we have this we have this this geo JSON file now we want to start importing this into your Virginia so let's see how we do that last time we were using near forge a sandbox that's just a really easy way to get started and have a hosted instance of neo4j running in the cloud this time I'm going to use neeraj a desktop new desktop allows us to run near 4j instances locally think of it as sort of a delivery mechanism for neo4j database if you haven't used here for Jade desktop before you can get it at neucom slash download drop that week in the chat and just close some of these windows here okay so let's create a new project in desktop yeah it's call it willow gran stack and we'll create a new database I want to create a local database I'm gonna choose version 3 518 the reason I'm choosing this version instead of a one of the newer for deado instances is because there's a specific plugin that I want to use later on that I know works with this version not sure about the Florida version we give it a name and then a password can be anything we just have to remember what that password is later on okay so we have some JSON data how do we import that into near 4j well there is a there's a library for near forge a called a POC which stands for awesome procedures on cipher and a POC you could think of as kind of like the standard library for for neo4j it has a ton of tools for things like data import and export things for formatting data things for virtual nodes things like that it exposes more complex paths expansions and so on so specifically we're interested in the data import functionality in a POC and we can see here that we have a load JSON procedure so it's life for itself you may have used load CSV which is built into cipher so we can stream CSV files directly an engineer for J and what cipher specify how we want to create that data in the graph and a POC load JSON gives us similar functionality but with JSON files cool so I'm going to go in here to manage plugins and install a POC so that will install the a POC library in to this willow grande stack database that I just created in desktop and then because I want to load a file locally I want to be able I want a POC to be able to access files in my file system I need to add a POC dot import that file enabled equals true I think it is a POC import that file enabled sure yep so in order for a bhakti able to access files and the file system imports I need to add that in my new frontier settings cool got that so let's hit apply it to save that and then we'll go ahead and start you know for J okay there's one more thing while this is starting there's one more thing we need to do if we look at the geometries we can see that we have polygon geometries but let's do a grep for multi polygons and we can see that we also have lots of multi polygons Multi polygons so the difference between polygons and multi polygon is polygons is sort of one single set of coordinates that define a polygon but a multi polygon can have polygons maybe within the polygon so maybe this is the boundary of a parcel but there's a piece that's cut out or there are two disjoint polygons and what want to do is filter out any of the parcels that have multi polygons because the way I want to deal with polygons as a list of points is a bit too simplistic for handling Multi polygons initially but actually I think I think maybe we may not even get to working with polygon geometries today so we'll save that for next time so in that case I'm ready to grab this geo JSON file so let's go here into desktop let's open a terminal so one thing that's nice about desktop is I can open a terminal and because desktop manages a bunch of different installations of near 4j database it gives me the ability to open the terminal with all of the all the binaries loaded into the path so I can do things like cipher shell or neo4j admin and they're on the path specific to this installation of this near-50 DBMS that I want to work with which is nice I mean you can see if we look at where this is this is buried somewhere within within the New York a desktop installations but I don't need to worry about that so if we look we can see we have some few directories set up for us there's a data directory that's where the actual data store for efg data is written there's a comp directory that's where my my settings are written there's logs and so on what I want is to go into the import directory by default this is where files will be read from for things like a POC load JSON load CSV and so on and what I want to do is copy not from that I want it from we're low grand static data Gallatin Geo JSON so I want to copy the Gallatin Geo JSON file that we created now into this import directory so that I can reference it with a pilot Jason okay cool so let's fire up our new 50 instances running let's go ahead and open it the FG browser cool so this is a fresh installation of neo4j there's nothing here verify that you match in return captain I'll be here but let's import some of our property data a POC load JSON and it is Gallatin geo JSON and so by default this route path is going to refer to within that import directory for this installation which is why we move that file over and then we want to yield value but remember this the structure of this geo JSON file let's go back and and look at that was such that all of the data that we care about is jammed under this features key so I don't want to just return sort of this whole JSON file that's gonna be I don't have room any hundreds of thousands of rows instead just to take a look at this let's do an unwind overvalue dot features as feet and return the first 10 so how does how does this a POC load json work well it parses this json file and then yields an object this is a like a map like a dictionary which I can then work with in cipher to specify how I want to create that data in near for J so here we're saying for the features array so for this giant array of features I want to iterate through that now when an alias now each one of these objects which one of these dictionaries are maps Li I'll use those interchangeably for our purposes and then let's just return the first 10 so let's see let's see if this works so this is parsing our JSON file and then it's going to iterate over that features array and hopefully should return just the first 10 for us cool and so here's here's the first one so it returns back this this object that has a geometry key with polygons as the type and a bunch of coordinates and then we have a parcel ID and then we have a bunch of property information cool so this is the information we have for one parcel one property one thing that's nice about load JSON with epoch versus load CSV with cipher if you've used load CSV before you may have noticed that by default there's no sort of inter halation of the type the type of our data that we're reading in so everything is by default treated as a string and we kind of need to to cast that specifically but if we notice here we've interpreted the type of the objects here so property ID this is this is an integer this is a string total value this is as an integer shaped length as a float and so on so that's one nice thing we get with with a puck low jason okay well that's fine we need to do more than just sort of look at these in year four J in the browser let's actually create some data so this is just parsing that JSON file and then just returning it to the browser we haven't actually created any data in the database yet so let's now do a for each so again we want to iterate over we want to iterate over all of our features so we'll say for each feet and valued features and by the way I like to use the cipher ref card if you haven't seen this to that in the chat cipher ref card is just sort of quick reference documentation for lots of the common cipher functionality so let's search for for each so for each allows us to as it says execute a mutating operation that's for each relationship in a path for each element in a list is what we want to do so this is the syntax for each the alias you want to use in whatever list and then a pipe and then whatever you're mutating operation is going to be so in our case we want to execute a create operation creating a property node for each element in that features array so what is that going to look like well something like create property so this will create the node and then we want to set some values well if we look back I close the tab let's let's run this cipher query again to show us the first ten well let's just look at the violator so for each feature we have an ID and then we have a properties object which goes through here and then our geometry object is separate from that so let's go ahead and set unwind must've cleared out what we had so far that's fine so we said it was 4-h feet in value dot features and then a pipe and then create P : property and then in curly braces let's set ID to feet dot ID and then because we have this this map of key value pair objects anyway we can just say set P plus equals feet properties and what this will do is take this properties object and just add these as key value pair properties on this node that we just created cool so let's run this and see what happens so this should be iterating over all of these features in our JJ's on file creating a node for each one setting the ID property and then setting all of the key value pairs and the properties objectives properties on the note let's take the check the chat so if we have any questions doesn't look like it okay so once once this is done then the next step is to see how we can expose some of this data through our graph QL API and then sort of see how we can maybe do some some sort of filtering and so on okay so this created let's say 50,000 nodes okay so let's match the let's look at the first ten of these okay so here's one we can see some information about this such as the type of property the total acres what city it's in and so on cool okay so there is obviously not all of the data that we need and we need to flesh out our data model a bit more if we look at our data model in arrows there are some things in here that we might be able to extract we might be able to maybe pull out the city pull out some of bedroom/bathroom square footage and so on but what I want to do in the next few minutes is see okay now that we actually have some data in neo4j how do we then start to change our graph QL schema and expose the graphical API and start to query this data in the air for J so let's see if we can do that and again let's run a called DB that's schema a dot visualization all right so far all we have is property nodes so not very graph e at this point but that's okay let's just make sure we can query some of this data using graph QL okay so this is it's now back in our Willow grand stack project this is exactly the code that's that's up on github and if I do an NPM run start this will start both the graph QL server and remember we also had this skeleton react application so the the starter project that we started from had data for a sort of business reviews application and what we wanted to do now is sort of start to change that to reflect our real estate search application so let's jump over it says our graphical API is running at localhost 4001 / graphic ul so this will give us graphical playground which we can use to query our graphical API and if we take a look at the docs yep you can see our schema has things like users and businesses and so on so let's let's change that and instead let's set this up so that we're able to query data based on our property node that we have in near 4j so we want to change our graph go schema to reflect that the first thing that we want to do you will notice we have some errors here saying that we're unable to connect to neo4j and yep that is reasonable last time we were using in the air for J sandbox instance so if we look in the slash API env file we have the connection credentials to our new FJ sandbox instance which I think I have since terminated and and moved on to other things in sandbox so let's change this now let's change our neo4j URI connection string to point to localhost : 7 6 8 7 that is the default for local databases that we created and started using near for desktop so this is going to refer to that database that I have running in desktop the username we left that is near for J and then the password I set as let me in okay and then if we look in API source schema graph QL so these are graphical type definitions this is what's driving our graph QL API and instead of users and businesses and so on we want to we want this to reflect the data that we have in neo4j so I'm going to stop our graphical API since we're gonna make some changes here one thing that's nice about the near 48 graphical integration is that we have the ability to infer these type definitions from an existing near 4j database let's check out the docs on this so grants takeo slash Doc's and I want how to using first schema it sounds like what we want so if we already have an existing near 4j database this infer schema function can be used to generate the equivalent graphical type definitions that describe the property graph model that exists in that database so for example if we have data about movies and actors and users that are created those movies this this may look familiar this is the the data model for the recommendations neo4j sandbox but if we run in first schema on that database it automatically generates for us the graphical type definitions that we see here which is really nice because then you don't sort of have to write these by hand we can keep these in sync cool so a nice thing about the grand stack starter project is that it comes with an NPM script so if we look in the root package JSON there's an NPM script for infer schema : right and that just runs the scripts and for schema let's take a look at what that is so this infer schema script is just going to run the grand stack CLI tool and pass some flags in there so we're saying in for schema then it's reading from my dot e NV file the environment variables that are being set for the connection tanea for J and then it's saying right to this schema file API source schema graph QL ok so what's going on there so if you remember in the last video when we were talking about the grand stack CLI tool we said that it was something that got installed as part of the grand stack starter and we said it it had some sort of more advanced functionality that we might use later and inferring a schema is is one of those things and this NPM script just sort of gives us a nice wrapper on top of that so we should be able to do NPM run in first schema : right and because we've updated our env file it should now be able to connect to our local neo4j database running in in the average a desktop generate the inferred graph Gil type definitions based on the data that we have and then update our schema graph QL file which looks like it did so this is the data that we have so we have a single node that has the label property every node has an underscore ID the underscore ID and the ID field are a bit different so the near 4j graph QL library adds this underscore ID field to every type in graphic QL and this maps to the internal node ID in here for J which you typically don't want to use in most cases that's sort of an internal implementation detail however it can be useful in some cases but don't confuse that underscore ID with properties that we've set explicitly such as the ID field cool okay so we can see also what's interesting is that we have some fields that exist on every instance on every one of those property nodes and those are identified with a bang so this is the continuous field which I'm assuming this is maybe like continuous square footage or something we'll have to to dig in to see what this data actually means and it's of type float bang which means this is a non nullable float a required field so every every one of those nodes contained a value for this property but then some things like certificate city state Z these are optional and because our in first schema process added these as optional well that means that there are some nodes in the database that don't have values for that so we'll have to think about in our application how to deal with with cases where we where we may not know a value for that property value for that property in the data so I have to think for how to account for that in our application but okay so let's let's fire up p.m. run start fire up our graphical API this also start the react web server I'm going to close that though just because we aren't quite ready to work with our react app yet since it's still set up for working with a somewhat different data model but okay now we can see you graphical playgrounds what sort of graphical API was generated based just on our type definitions and so we can see that we have one query entry point for property and then we have mutations if we want to create update merge or or delete some of these so let's starting off here let's make this a bit bigger there we go cool so property let's look at the first 100 or so and let's close the docs here let's return ID we know everyone has an ID and then address we have a few different address properties so this should return yep cool so you can see some properties don't have an address actually quite a few don't but every one has has an ID let's take a look at what else do we have we have things like assessment we also have things like total value okay so we can see that requiring some data from our nifty database if we jump back we can take a look at the generated Seifer queries so what's going on here is when we run this graph QL query so this says find the first 100 nodes with the label property and then return some of these values these fields is called the selection set so ID address assessment and total value that graph QL query is then translated into this cipher query that actually runs against neo4j and and returns our data okay so that's pretty cool um let's maybe do something a bit more interesting let's order this by maybe total value descending so what is this this is gonna tell us the most expensive property in that county so that is 915 Highland Boulevard and the assessed value is 112 million dollars I guess is what that means okay cool so now we can start to see how we can use some of our features that we get in graph QL to map to some of the business requirements of our of our application I should say specifically some of the features we get with the New York 8 graph QL integration right this this sort of query generation is automatic ordering we can also do filtering which is which is neat so do we have anything on square footage let's see we have total acres 34 so maybe maybe we want to let's say filter now we're total acres has to be though maybe at least ten so one thing that's nice about the new trade graphical integration is that from just our type definitions that we defined here is the generated API includes these ordering and filtering arguments so now we're filtering for parcels that have at least ten acres ordering by their total assessed value cool so we can start to see how we get a lot for free with the new york geographical integration that's super helpful that's going to be really nice for us when we start to when we start to look at how we can search for properties in the front end I have a few more minutes before I I have to leave and jump on another call but let's let's take a look at one more cool thing we can do here so if we open somewhere here we go so one thing that we wanted to add to our application is sort of an estimated home value feature Zillow calls it the Zestimate so for any house it's sort of an estimate of how much this house is worth and there's the the Zestimate one is quite complex I'm sure there's a lot that goes into it we will we will have to look at sort of a more complex way to go about this so for example Zillow has this idea of comparables similar homes and what they have sold for but one thing we can do that is maybe a initial way to look at this this estimated sales value is well we have this total value this is sort of the the tax assessed value and if we had information about the sort of percentage difference that houses in a certain market we're selling over the total value I know in in the u.s. anyway and in most cities in most counties there's sort of an assessed value which may be something like a hundred thousand dollars but on average homes in that county sell for maybe like twenty percent above the assessed value so if we have some simple model like that we can then add a computed field to our graph QL schema that's going to be defined with a cipher query in this case to determine what is the estimated sales price based on based on just multiplying that total value and that assessed value by some amount so let's add that really quick we'll just call this estimated sales price maybe and it's going to be an integer and we're going to use the cypher schema directive so what is what is the cypher schema directive if we jump back to our documentation look at schema directives so schema directive is a way to annotate a field or a type definition in your graphical type definitions that indicates that there should be some custom logic going on and there's a few that we use in the New York Geographic ul integration cipher specifically allows us to define custom logic using cipher allows us to define computed fields in this case we're defining a computed scalar field so we're automatically injected a this object so this is going to be a node so if we just made this node total value times 1.2 so maybe our model says that houses in our market are selling for on average 20% above the assessed tax value multiply our assessed value by 1.2 and we have the estimated sales price cool so let's see if we can pick that up now we have the estimated sales price let's add that it cannot represent non integer value oh right because we're multiplying by a fraction here that is a float let's cast this to an integer so we'll say return to integer this dot total value times 1 point to cool let's try that again okay server was starting there we go cool so now we have a total value so again we're ordering by total value descending so these are the most expensive properties so this is total value of 112 million estimated sales price is 20% higher 134 million and so on cool so I think that is pretty good progress for today so just to recap things that we touched on we did a little bit of talking through our data model and verifying that our requirements can at least the ones we've identified so far can be met using the data model that that we defined last time we imported some data into year for J based on the data that we downloaded for for a specific county we converted that from a shape file that we downloaded into geo JSON and then we used a POC load JSON to import that data into neo4j then in our graph QL API we used the infer schema functionality to generate graph QL type definitions from that data in year four J which gave us the ability to query that using graph QL including doing some of these filtering and ordering that will be very useful for us for our property search functionality and then we used the cypher schema directive functionality to define a computed field in our graphical schema using Seifer specifically to calculate estimated sales price for our properties cool so that I think is pretty good we'll go ahead and stop there I'll be sure to push all of this up too to github the link is is in the chat but it's it's this willow grand stack repo which again I had just created last time just from the grand stack starter cool so so next time I think we'll continue on our data import journey next time working with some of the geospatial data that we had we had latitude and longitude for the properties we had the bounds right the the polygons to work with so we'll see how we can include that in here for J and then how we can work with that geospatial data in our graphical API so we'll we'll pick up with that next week so just a reminder every Thursday at 2 p.m. Pacific will be working on building out this application using grand stack cool so I hope you enjoyed that in the meantime feel free to to reach out and ping me on Twitter or on the new FJ users slack which you can join it you have to calm slash slack cool thanks a lot and we'll see you next time 