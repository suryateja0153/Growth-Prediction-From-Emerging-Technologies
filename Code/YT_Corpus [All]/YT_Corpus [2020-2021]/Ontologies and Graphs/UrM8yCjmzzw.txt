 hello and welcome everyone to data mesh in practice here at data ai summit 2020 i'm max schulze and i'm joined by arif veda today to talk to you about how europe's leading online platform for fashion goes beyond the data lake but before we dive straight into the topic let me give us a short introduction about ourselves i for my site a lead data engineer at zalando europe's biggest online fashion platform and i've been with the company for about five years during that time i was mostly working on data infrastructure topics specifically focusing on distributed storage distributed compute and building a lot of data tooling and data services on top of that beyond that i'm a huge gamer so i'm a big fan of league of legends i've been playing a lot of animal crossing recently but most importantly i probably spent my last decade on traveling to magic the gathering tournaments on several different levels of competition all over the world so if there are any topics around gaming feel free to chat me up after the talk arif do you want to introduce yourself as well yes hi everyone so yeah my name is arif i'm a software engineer as well but also since very recently i'm a software engineering professor at htw which is a berlin-based university but before taking this position as a professor i was leading the data and ai business for thoughtworks germany and therefore i was a full-time consultant which brought me among other clients to zalando and therefore to meet max which is ultimately the reason why we're giving this talk here together today i actually continue to work with thoughtworks in what we call a fellow role but that is only on a part-time basis now next to work or actually also quite a lot during work i am quite serious about coffee so um yeah if you want to have a lengthy discussion about a topic with me that is not about software then coffee is certainly something that i'm always happy to talk about all right and then um i give it back to max and let's get going thanks for the quick intro reef so what can you expect from us today well at first i will give you an introduction about the lando's data platform or what was the starting point of our journey that we actually went on here then arief will give you a little bit of an overview of what this data mesh is actually about what this concept what is behind this concept and what are the main pillars of it and then i will take you back on our journey to actually show you a little bit of how we were able to apply some of these principles in practice so starting off with the lando's data platform when i am talking about zalando's data platform usually i'm talking about three big areas the first one is ingestion how does the data actually get in the second one is storage how the data is actually persisted and then of course eventually also access through the third part which is the serving layer and what are the possibilities to actually use this data and to actually derive information from it when it comes to the first part we're talking here about three main data sources and three main central data pipelines that we were maintaining at that time we are having a pipeline that gets data from our central event bus something that was originally purposed for micro service to service communication but soon proved to contain a lot of very valuable information also for analytics we have a data pipeline that is collecting data from a legacy where a data warehouse that we still have yes of course there is still always some legacy that you have somewhere and even though there are big migration projects to actually get away from that there's still a lot of very very valuable information in there that is totally worth also getting into the central data platform and last but not least we are also collecting uh behavioral data about the customers how they are actually moving on the web page of solando and putting this data also available centrally to combine it with uh data like for instance sales information and take very very valuable uh decisions based on top of that when it comes to the storage site for us this decision usually at the beginning was very simple salando is mostly aws based which means that we are when looking for a very simple storage solution we went with exactly that s3 as our main backup to store all our data but when talking about storing data it's not just about storing the data itself but also storing data about the data like meta information like for instance usage analytics of the data to later on pass this along to the producers of the data as well lastly on the serving side we have a bit of a split model between analytics and transformational processing on the analytical side we are offering presto as a distributed sql engine uh where mostly analysts or also a lot of non-technical users can use this as an option to generate insights from the data that we store on the processing side we offer something that we call a central processing platform which is essentially spark infrastructure as a service to the rest of the company so we are providing spark clusters we are here a big customer of data bricks um to work this out together and to provide infrastructure so that not every team has to do these things by themselves and lastly we are using calibre as a data catalog which is the main entry point for discovery of data and for also learning additional things about the data before actually starting the usage on that side looking at this model that we have set up here there's one big challenge that it brings with us and this is centralization all of these components are maintained centrally and this brings a couple challenges with it the first challenge that i want to introduce is that data sets provided by central data infrastructure teams lead to a lack of ownership there's a lot of data that is flowing through these central data pipelines that is default archived into the data lake that we have but whenever somebody wants to use that there's actually a missing connection to the people that actually provide that data that actually own that data secondly we have the issue that data pipelines operated by central data infrastructure teams lead to a lack of quality from an infrastructure perspective i usually care about things like uh latency of my data like throughput like correctness on let's say a data set level but i will never look into the content of the data how can i we are talking about thousands of data sets here we are talking about petabytes of data that we are actually processing and it's impossible for a central infrastructure team of a couple of engineers to really get ahead of all of that and understand all the content of all of it which lastly brings me also to the third big challenge which is the moment your organization starts scaling the central team immediately becomes the bottleneck they will always be the first contact point for any user that actually wants to do something with the data but also for any producer that actually wants to let's say introduce a breaking change because they don't even know who they actually users are and everything is always funneled through the central data infrastructure team which immediately becomes the bottleneck these were some of the challenges that we observed in the central setup that we were operating arif can you maybe take uh it from here and tell us if these were challenges that just we were facing or if this is something that you already observed also in other contexts yes of course so um having been a consultant for the last couple of years i was also part of several of such data platform and data engineering teams and i can indeed confirm that what max just described is really a recurring pattern and i've seen those issues with data ownership and data quality again and again so if you zoom out a little bit you can describe the situation often in the following way there are three general parties the data producers the data consumers and the people who are building the central data platform so if we start on the left with the data producers those are the people and the teams building the production services and they're often fairly okay with the situation they do they do not feel the pain of lack of data quality that much unless they are data consumers themselves because they are basically happily generating their data and they have their own incentives about their production services then on the right side we have the data consumers and they definitely already feel pain points of lack of data ownership and lack of data quality because those are data scientists and decision makers that really have to then try to understand what a certain field means and deal with those quality issues right but then when we look at the party in the middle that is the people who built a central data platform they are really the ones who are often in a very tight spot because they are the ones who have all the responsibility to provide high quality data in a reliable fashion to all those data consumers but at the same time they have not much control over the quality of that data because they are not the ones who are ultimately generating this data right so what i've seen very often is that the people in those teams they are fire fighting all day trying to solve issues that have been introduced somewhere upstream so overall this is not a very good situation not for the central data engineering team but also not overall now interestingly this situation appears no matter whether you're building a classical data warehouse or whether you're building a more modern data lake because the issue here is not technology but it is the centralization itself or to be more specific the central ownership so if we look at this picture here for instance and let's say we have on the left side an ever-growing number of data producers and we have on the right side an ever-growing number of data consumers then this central data platform will always become a bottleneck but why is this actually the case that this is becoming a bottleneck one important reason is that such an such a centralized data platform with centralized data ownership is kind of a data monolith i would say that cuts through domains so what do i mean with this let's imagine we have a checkout service and that is maintained by a team and that is one of the teams of the producer data producers here on the left right and they are building this checkout service and this checkout service is generating checkout events which in the end end up in the central data platform now when you look at this you see that the knowledge about checkouts is scattered across different teams and that also means that the responsibility is scattered and that leads to friction to misunderstandings and ultimately prevents this whole model to scale now coming to the data mesh or coming from those observations what really is the data mesh the data base is mostly a paradigm that is about applying concepts and approaches that have been applied to the general software engineering domain for years very successfully but now applying them to the specific challenges in the software engineering in the data engineering domain and specifically i want to go into three of those concepts and those are first product thinking second domain driven distributed architecture and third infrastructure as a platform and now before i go into those one by one let me quickly refer to the original data mesh article by jamaica ghani that you still find on martinfaller.com because jermark really was the one who coined the term data mesh initially and this article which is really an excellent article explains all the details of the data mesh paradigm in much more depth than what i can do in those few minutes here so now let's go through those concepts one by one and let's start with product thinking and i want to start with product thinking because i feel this is actually the concept that has um the biggest potential to help with those ownership issues that max talked about earlier and this is mainly about thinking of data as a product so what does this mean it means that you really think of a data set like product on a market and that means that you have to answer questions such as what is my market who are my customers and how do i even make sure that my customers know about my product right and in order to answer or tackle those questions thoroughly you probably want to have a dedicated role for this which can often be something like a data product manager and such a data product manager should be part of a cross-functional team that is taking full ownership of such a data product that means they are maintaining all the data pipelines have all the knowledge etc to make all the value of this data set available to customers within the organization now the second concept here is domain driven distributed architecture and applying this here means that one of such data products should really always capture one clearly defined domain and that means that the team building that data product or owning that data product they can really become domain experts about that domain and that is the key idea here and then when you have such a team of domain experts here then such a data product can serve as the fundamental building block of building a mesh of such data products and there can be different kinds of data products like for instance you can have more source oriented domain data sets that are closer to where the data is generated but you can also have so-called aggregated domains that consume other data products to create some higher value by building things on top of the existing data products and now looking at this thing of a data mesh setup this has certain similarities to a microservice architecture and similarly to microservices we as an industry we developed a certain notion what a microservice really is for instance that it needs to provide monitoring needs to be independently deployable those kind of things otherwise you wouldn't call just any service a microservice and similarly here not every data set is a data product in order to be a data product it needs to be discoverable that means you need to be able to find it it needs to be addressable self-describing secure and trustworthy and more important than all these other things a data product needs to be interoperable and this needs to be provided by an open standard because only then you can build an ecosystem of those data products from it now finally coming to the third concept here which is data infrastructure as a platform so first of all the idea of providing data infrastructure as a platform here is simply to avoid unnecessary duplicate effort right and this idea is not very new this is what the big cloud providers are doing for years a key thing to make sure here is that this data infrastructure platform really is a data infrastructure platform and doesn't become a data platform that means it needs to be domain agnostic what does that mean it means that if you are for instance a developer an engineer of that team that is building the data infrastructure platform and suddenly you need to understand some of the details of the data products um that you are supporting with the data infrastructure platform then you already need to have domain knowledge and that means you're already on the way again to become a centralized data platform instead of being a domain agnostic data infrastructure platform so wrapping this up before i hand it back to max again who can give you exactly that perspective of a data infrastructure platform let me sum up that the data mesh is really much more of a mindset shift than a technological shift it's about going from centralized ownership to decentralized ownership it's about looking at data domains as the first class concern and not technical things such as pipelines it's about treating data as the product and not just as a byproduct and it's about building cross-functional domain data teams and an ecosystem of data products from this and with this i hand it back to max who will tell you more about the data infrastructure thanks the lottery for this great introduction into the data mesh concept so what i now want to do is to talk to you a little bit about how we started applying some of these concepts in practice concretely what are the things that we were able to achieve on the technical side but also what were the changes that we made on the organizational side and of course i also want to then give you some numbers afterwards for backing this up with the adoption that we already see and the value we are generating here in our company so first let me shortly recap of where we were actually coming from well we were in this situation where the central team was always the bottleneck but we as arif rightfully explained we much rather want to get to the setup where we have a data infrastructure as a platform where people can use self-service tooling um to to actually do the things without the involvement of central engineers at the same time we want to get away from the central data monitors from the central data platform and we want to get towards interoperable services well how did we actually start setting this up well the important part is of course that there was a lot of things that we already had in place we had central services that were already up and running we already had central data sets thousands of data sets petabytes of data and we already had these stored and well described as well as already having a governance layer on top of that that was managing things like data access that was managing things like automated metadata collection and these were all things that we could already leverage to expand them in scope and to bring them better into a global interoperable setup so the first thing that we then started adding on top of that was a concept that we called bring your own bucket or byob how we like to shorten it which essentially explains a setup where teams now have the possibilities to store data by themselves they work in their own aws accounts they have their own buckets they now are completely free to store data in their own buckets but share shared with the same day central data infrastructure that already exists so to plug it into this governance layer that was already available and to make data available and with that lower the the barrier to actually bring your data in at the same time this was immediately leading to people picking up ownership of the data sets they were actually providing because now they store it and they much easier immediately feel a responsibility for the data that they actually provide to others the next thing that we went into was well we doubled down on this processing platform approach that we already started before the processing platform for that matter is central provision of infrastructure without actually knowing what the people use it for and just to give you an example for the spark cluster setup that we had there was a lot of teams who were operating spark clusters by themselves and every new team that had a new use case every single day was reinventing the wheel just to figure out how to run and operate a spark cluster not even speaking about cost efficiency or anything like this and this was a big point of centralizing this part and bringing it together into a team that then takes care just of the infrastructure part by now if i want to have a spark use case and i want to run it in production well i actually go to the central team via a template describe to them what i need and then i get my cluster and i can do with it whatever i want and then we went one step further and did not just stop at s3 buckets for actually contributing data but we started adding more and more things like redshifts like rds postgres and not just even for getting the data from there but also for writing back and really expanding this infrastructure setup that we add there are common use cases like people who are picking up data that is archived from the event bus loading it into their redshift and again using it for some some of their use cases and setting up all of this without the involvement of a single central engineer these were like some of the bigger changes that we did on the technical side but now i also want to move over to the organizational side a little bit so where would we actually come from here right decentralized ownership does not necessarily imply decentralized infrastructure this was one of our biggest learnings because now we had decentral storage but we still had central infrastructure that was provided by central teams we now have decentral ownership because through this true decentralization of the data the people now have much higher responsibility for what they're actually providing yet still we have the central governance layer which is taking care of the processes around data access around automated metadata classification for instance because true interoperability is only created through convenient solutions of a self-service platform now going to this organizational site just to recap again we weren't a situation where a central team was providing data and that led to the problems of lack of ownership of lack of quality and we started addressing this by immediately working with the teams who are actually providing the data to ensure the quality from that perspective instead of default storing all the data sets we went to a model where we asked the people to actively opt in to decide actively and make conscious decisions about what they should store and to move to this behavioral change to treat data as a product to have dedicated people that take care of this data at the same time we were looking about into the usage of the data because well how to can you give the best incentives for caring about the quality well you have to care about your users and looking about at the data sets that we had they were around 70 of data sets that were not used at all there were some data sets that were used every once in a while they could probably stay as they are but there were data sets which were like the golden data sets that were available that everybody was using that the most value was actually generated from and making people aware of this allowed them to actually dedicate resources to a understand the usage of the data sets that were that they were providing but then on the other side also take this additional resources to ensure quality and to even expand the features potentially even add new data sets afterwards backing this up with some numbers to show you the adoption and the usage that we already had um just as a rough context we are currently working with around 200 teams and tech at solando and out of these 40 teams already started adopting just bring your own bucket of bucket approach to share data and we got a lot of very positive feedback that this simplifying this data sharing allowed for you to increase the productivity on their site over 100 teams are already using the processing platform to use central data infrastructure for actually taking care of that of their parts and we even have already first curated data teams that build data products on top of other data products to really get to this aggregated level of data products that our reef was already mentioning most importantly we reached a point where we actually have zero operational effort for the central team well not exactly not yet it's a journey we are still on it we are actually striving to get there but there's of course still a long way that we have to go uh the amount of operational effort was dramatically reduced uh that the central effort the team had before but there are still a couple things that we need to do here and because it's a journey i also want to just give a very glimpse outlook of what are the things that we are currently tinkering with and this is something that i like to describe as off-the-shelf data tooling this is really something where you provide blueprints of standardized solutions that people can just take and apply for their use cases this contains some things like decentralized archiving like giving people the option to make sure their data actually ends up in this and in the storage that they maintain this contains decentralized gdpr deletion tooling once you store data you are responsible for it you need to make sure it's compliant and we offer you the tooling to actually take care of this and lastly this contains template-driven data preparation where really we want to give some blueprints to um take care of some standardized use cases as i said this is a journey that we are currently on but interestingly this is a journey that you also can join all of the teams that are actually currently involved in all of these topics that we've been describing today are currently hiring so if you have an interest into taking part of this journey and making this journey your own please talk to us after the talk with that being said i want to close it out this was data mesh in practice today and i'm mark schulze joined today by arif and together we want to thank you for your attendance 