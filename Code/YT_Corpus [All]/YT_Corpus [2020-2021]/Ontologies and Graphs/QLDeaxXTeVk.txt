 with today's natural language digital assistants, natural language processing or NLP has become a hot topic within AI there are several design paradigms for NLP one being statistical NLP. One of the foundational ideas in statistical NLP is the concept of an n-gram this represents the frequency that some n number of text tokens appears or is expected to appear in a body of text n-grams are an aspect of statistical NLP that utilizes an area of AI called machine learning unlike other areas of programming, machine learning does not use explicitly programmed rules. Instead, these systems are frameworks that are capable of learning information from training data. a 1-gram or unigram is simply the frequency that a single particular token appears for example if we have a corpus ie a body of text that has 10,000 words and the word "the" appears 320 times, then the probability of any randomly chosen word in the corpus will be "the" is 320 over 10,000 or 3.2 percent. This probability is also called the weight of the n-gram and is a straightforward calculation using basic fractional arithmetic. each Each element of an n-gram is called a lexeme the most common lexemes in a body of text are words but punctuation, numbers, and abbreviations are also lexemes before the weights of n-grams in a text is calculated it is usual to first perform pre-processing of the text to normalize certain aspects. The most common is to remove all case sensitivity thus a word that is capitalized at the beginning of a sentence is counted equally along with uncapitalized instances of the word another common pre-processing task is tokenization this separates text elements that may be connected like genitive possessive cases in two separate tokens here possessive clauses like girl's book are separated into three separate tokens this allows the root token girl to contribute to the weight of other instances of girl. Yet another pre-processing task involves identifying the various morphological forms of a word. For example: bring, brings, bringing, and brought, may all be counted towards the weight of the unigram: bring. depending on the NLP use case we may want to preserve the metadata of how many past tense verses present tense instances of a verb are in a corpus. This information can also help to identify which nouns are associated with a given pronoun. This is called coreference resolution. Hopefully you now have a better understanding of how n-grams can be used by data scientists. Thanks for watching from Lymba. Please reach out to us with any questions or for help on your next project 