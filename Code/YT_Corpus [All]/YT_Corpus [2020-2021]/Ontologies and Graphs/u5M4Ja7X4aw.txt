 Hey everyone, I'd like to introduce our next speaker. His name is Mark Musen. He's Dr. Musen, a professor of Biomedical Informatics and a Biomedical Data Science at Stanford University, where he's a director of the Stanford Center for Biomedical Informatics Research. Dr. Musen conducts research related to open science, intelligent systems, computation, ontologies and biomedical decision support. His group developed protege, the world's most wired, these technologies for building and managing terminologies and technologies. He has served as principal investigator of the national center for biomedical ontology, which is one of the original national centers for biomedical computing created by the us national institutes of health. He also directs the Centre for Expanded Data Annotation and fievel CEDAR, which is the talk is going to give them founded under the FAQ Big Data knowledge initiative. CEDAR developed semantic technology to ease the authoring and management of diamond experimental metadata. So Mark without further ado, please take it away.  Thanks for welcome, I'm really excited to be here, and I think it's really a fantastic opportunity. I'm really glad that CMU has recognized the importance of bringing together people interested in both AI and data management and data reuse. I'm gonna talk to you today about CEDAR which is a project we've been working on at Stanford for about the past five years. I'm gonna be talking to you about the problems of accessing data and why getting data at least experimental data online is so hard and why I think data we use is such as challenging and important problem. Earlier today when Melissa Sandel gave her talk, she made a statement that we basically need to have better data in order to have good AI. I'm gonna argue in this talk that we actually need to have good AI in order to generate even better data. And the emphasis here is on what AI is gonna do to make the online data available for all of us more useful. We've watched most of her a lot of talk today already about the fair principles. And the idea that data reuse requires data that are findable, accessible, interoperable, and reusable. And that mantra has been just sort of with us for the past five years and everybody agrees that if we wanna be able to reuse data, we need to have fair data in the first place. Great idea, the problem is, when you look online and see what is actually available for reuse, almost all data are not there. They're not there because the fear principles are, in their own way rather obtuse. They're principles they're not easily operationalized in a way, which makes it easy for us to take these principles and ensure that the datasets that we create are really fair. And what that means is that when you look at the data repositories, they exist in science and I'm gonna say up front that I'm biased because I work in biomedicine, and so most of the data that I see are biomedical. But fundamentally we have a problem because investigators view their work as publishing papers, nobody thinks that their work is to leave a legacy of reusable data. All the sponsors right now are getting very excited about fair and saying that data sharing is important. They're not really paying investigators to share their data. And most important, where my emphasis is gonna be today, is that data are not gonna be fair unless the metadata that describe data sets are themselves fair. And getting metadata that are there is really hard. So here's some sample metadata. This is a random record give it to me by one of my coworkers from NCBI, the National Center for Biotechnology Information at the National Library of Medicine. This is for a data set that was put online budget net tech. And this is what metadata look like, usually there are sets of attribute value pairs. And they describe the data that are online, they describe in particular, what is the experimental situation that led to the creation of those data. And this all looks pretty good when you look at it from a high level, what you sort of look at exactly what the investigator here is free putting into the back put into the metadata. Well, carcinoma hepatocellular it tells you what the diseases but it's not a standardized term. Saying that the ethnicity is Japanese is a bit odd, saying that the age is 57 year, not 57 and not yours plural is also on, if you were to search for Asians with a pad of cellular carcinoma, you wouldn't get this record if you're doing a search online. Because the metadata don't give you the information that you need in a form that makes the metadata useful. And indeed you realize that we entered that situation, because most of the time when people create metadata, they create metadata by filling out spreadsheets, and they're filling in spreadsheets and give them very little guidance, don't really tell them what are the control terms they should be using. Don't really tell them what are the explicit expected information, and one of these metadata assessments. And what do you get is this a look at the gene expression Omnibus one of the online databases at NIH and provides information about a kind of high throughput biological experiment. And if you were to look at records and wanted to find the age of the subject, you would see a gazillion ways to represent age, age, age, age, pop age, age after birth, age and years and so on. Again, if you wanna search for records and find patients of a certain age, not only you have to anticipate how the investigator may have encoded the metadata to describe the age of the patient. But you have to think about what are all the variations about that? What are all the typos that could have been introduced into the metadata, and it becomes a real mess. In fact, we looked at the biosample repository at NCBI. This is the repository of metadata regarding the samples that are referred to and other kinds of data at NCBI, which is supposed to be their best data set. And we'll say, despite the best efforts of NCBI and the NCBI tries really hard and despite the best efforts, efforts of the investigators, they try really hard. Well 73% of the Boolean meta data values are not actually Boolean, they're values like nonsmoker and former smoker, 26% of the integer meta data values can't be parsed as numbers. There's things like JM 52 and pig, 68% of the meta data entries that is supposed to represent biomedical ontologies don't. And so what we have is a situation where if you're an investigator trying to do data reuse, and you want to search online datasets at least in biomedicine, Elise using NCBI resources, you're innocent situation where, despite all the best efforts of everybody involved. The metadata don't make it easy for you to do that, I have colleagues at Stanford who are really good data parasites so to speak. They spend weeks and weeks going through metadata by hand, because the online search capability is so impoverished. And so that gets us back to the question of how do we make our data fail? And honestly, we're at a situation where we recognize that at minimum, we need access to experimental data sets. We need mechanisms to search for metadata to find the relevant experimental results. We need annotation online datasets with adequate metadata so we actually know what investigators did. We need control terms so that we're not looking for various spellings and typos. And we need a culture that actually cares about all of this. Where the investigators who create metadata recognize that really, they are creating data sets, putting the model on, because there are gonna be new discoveries that will be made from their data, and that they have a vested interest in making sure that their data are as reusable as Is possible. Well, that's all good and how the real question is, how do we get there? We get there number one, by making sure that we're using standard ontologies to describe what exists in a data set completely and consistently. And fortunately, at least in biomedicine where I work, we've had good ontologies around for about 300 years. And so this is Linnaeus who created the system for speciation in biology which is probably what all of us learned in high school. And this particular kind of ontology for identifying species of living organisms is just one example of the many hundreds of ontologies that are available in biology and medicine. In some sense having hundreds of ontologies in biology and medicine is a problem. But the good news is we at Sanford are working for many years on a system called Bio portal. Bio portal is an open online system that gives you access to basically all the publicly available biomedical ontologies exist. If you go to buy a portal you'll see that we have hundreds of ontologies many of them are standard ontologies used frequently in clinical medicine. Others are more unique and created by investigators in various specialized areas of biology. But we can go to buy a portal. We can take a look at what kinds of ontologies are available. We can search them. We can identify, for example, all of the myriad terms and snowmen CT that might be relevant for annotating it by biomedical specimen. And we can, identify the terms that are necessary in order to make the metadata for our data sets more standardized. And therefore more fair. That's good. But what about the actual structure the metadata themselves, we need to be able to describe experiments completely and consistently. So that the people who wanna reuse those data can search them through the metadata in a way which is guaranteed to have maximum recall. And the real question is how do we do that? Well for 20 years again in biomedicine, the people who do high throughput experiments called microarray studies have recognized that because their high throughput experiments generate thousands and thousands of data points. They're not gonna publish the data in a journal and put them online and they have to make their data searchable. And to do that they recognized that it wouldn't be just adequate to put the data online. They needed metadata that would clarify what was the substrate of the experiment. What was the platform that was used? What were the experimental conditions we're gonna try to prove. And that led about 20 years, just something called minimum information about a microarray experiment or miame. Miame is more than just a clever acronym. It really is something that has revolutionized the way people biology. Think about describing metadata. Because Miami said. Look, it's not good enough to just say who you are and what your study was in very broad terms. These are the kinds of things you have to say about your study. If someone else is gonna reuse your data and make sense of those data, and Miami caught on like wildfire, and then not only do we get Miami, but the biological community recognize that for all the standardized kinds of high throughput experiments that they do routinely. There was a need to describe his minimum information modules that would clarify what they meditator needs to describe in order for some third party to actually make sense about what experiment was done. And so we have the other and we have MIRIAM and MINIMESS and MISFISHIE, and really, dozens and dozens of these standards that describe what are the minimal things you need to say about an experiment in order for someone to actually make sense of what you've done. And that is a really important, and it's not unique to biomedicine, but it's a really important attribute about the way people in biology think about creating metadata which we could take advantage of in Situ. But you need more than just the ontologies, you need more than just the kinds of structures for creating metadata, you need the ability to do this in a way, which is convenient to the people who are running the experiments. You basically need to make it palatable to describe experiments completely consistently. People who are doing experimental work love spreadsheets, because they're so easy to use, because they're so familiar. But because they don't provide the information that's so necessary. Often to just do things in a structured way. What we really need are things like cedar. Which provide a web based platform that allow investigators to describe their experiments ways, which really clarify for themselves what they've done. Clarify for the computer, what they've done and obviously clarify for third parties who wanna reuse their data, what they can with what they can learn the data to the benefit put online. So if you look at CEDAR, we can see think of CEDAR as an approach that has three steps. In the leftmost panel, we have an ability to create templates that describe metadata and these templates are based often on the kinds of community based standards like Miami, Mistubishi, mini mass and lots of others. That are used to be able to describe the information necessary to communicate what was done in scientific experiments. In the center panel, we have technology that uses the template and fills in that template in order to make it possible, to describe in detail what was done in a particular experiment. And so we have the metadata template created on the left. We have the actual metadata for a given experiment. They uses that template created in the middle. And at the very right, we have the ability to export that information to some online repository, like the NCBI repositories or import or CGA or a lot of other repositories. In the middle panel, if you will, this panel over here, where we say we explore and reuse datasets or metadata. We have the ability, obviously, to search metadata that an already created CEDAR. But another element that we have, which I'll show you in a minute, is that we can use the metadata that have already been entered through CEDAR to learn patterns in those metadata. So we can use AI to understand structures in the metadata that can actually help us inform the acquisition of new metadata. But knowing our old metadata and understanding the patterns there we have the ability to acquire information which allows us to ensure the filling in of these metadata templates is as easy as possible for the investigators who are trying to use them. So here's what Cedar looks like when you log in. You get a library of metadata templates. You can see templates that are created for a variety of purposes. Suppose we're interested in bio sample human, that's the template that allows us to enter metadata for describing samples that are going into the bio sample database at NCBI that refer to human subjects. So we click on it, we say wanna populate it and here's what metadata template looks like in cedars a little bit cleaner than those spreadsheets that you saw earlier. And it's a set of attribute value pairs. There's a sample name, there's an organism, there's a tissue, there's a sex, there's an isolate, perhaps there's an age and so on. And we can see that in this particular specimen, there was a person who was 74 years old who had dermatitis. And there was a cell line that was created and more and more of these metadata will describe exactly what was done to create the sample that was used for a variety of experiments. Okay, so why don't we create the template that we fill in, that allows us to generate these metadata? Well remember the creating of the templates is the first step in our three step process. And we have a whole easy to use web based authoring system that allows us to describe a template. In this case, we're entering information that describes the bio sample human template that we've been talking about. We can say that there's a sample name, which is alphanumeric. There's an organism which is alphanumeric. There's a tissue, which is also alphanumeric. But here we could say, we just don't want this to be a type in. We want this to be something where the value is selected from an ontology. That already exists in that bio portal resource that I showed you earlier. And so the triangle symbol you see at the left hand side of the information of information presented here suggests that we're gonna be using, an ontology to create the values. We can click on search if we like. And what we can find out is that if we go to Bible Portal. We would see that there's a lot of ontologies that talk about tissues. The best one is at the top that's overrun. And suggest that might be the ontology and you might want to use it. We're gonna ask a user down the road. To enter information about tissues, and then we can go look and see what looks like. We can see that Uber on seems to have what looks like good selections for tissues. And so it would say, okay, let's go add Uber on. And so we've added Uber onto our template, and now if we want to actually annotate it. Did the template I've added are gay with metadata by filling in the template. We can see how to do that we can see here's the bio sample human template I showed you earlier. We can see that Uber on provides information on the tissue through a drop down menu. And so instead of having to type in some random value for tissue. Cedar makes it really easy to say okay. The tissue is going to be taken from and not only going to show you the tissues that are. I'm not gonna show you a few dozen tissues and make it really hard. We're gonna use our knowledge of the previously entered metadata. To put to the at the top of the drop down list. Those values that were selected most frequently by people who previously entered bio sample human metadata and talked about issues. So by looking at our previous metadata. We can see that for example, blood was the most frequently used tissue in the samples that had been annotated previously. And therefore there's a good bet that blood ought to be at the top of our list now. And we can see, well, we can just go select it. And if you look at other entries into this template for example. Suppose you said the tissue is long, and we want to choose what disease might the specimen or the subject have had from which we took the specimen. The list for the possible diseases is not an unordered list of all the diseases in the disease ontology. But they're ordered in accordance to what diseases have we previously seen. When we've had metadata with a tissue as long as the species was homoserines and we see we get selections like lung cancer, chronic obstructive pulmonary disease and squamous cell carcinoma. The kinds of things that we'd expect. So we can make it easy for the user to fill in better data values by taking advantage of all of our previous metadata entries and learning from them and producing a predictive data entry and reformatting our menus. To make it really easy just to fill in these blanks. Now, is this gonna be as easy for investigators and spreadsheets? People still love those spreadsheets a lot. But this approach has turned out to be really easy for us and easy for our collaborators. When we said the tissue was brain that they get moved. And they dropped out and they had Parkinson disease and CNS lymphoma. And. Disorder other kinds of diseases that would be associated with the brain. And for every selection made through a template in Cedar. We make it really really easy by trying to learn as much as we can from previous metadata entries. Making it simple to click off menus or enter. Other streams with predictive data entry. Making it really fast, and not only fast but very specific and very accurate to create comprehensive, detailed metadata. That becomes searchable, and become usable by other investigators. So things that are important to remember about cedar from high perspective. Is that all the semantic components that you see in Cedar. The template elements that build up templates, the templates themselves, or the. The ontologies the value sets that are used to fill in the blanks. All of these are manages first class entities. Most things are stored in the bio portal resource and make it possible for us to upload new versions of them and to edit them that. Through that mechanism, and the user interface is taking advantage all these semantic components by generating on the fly, drop down menus, whole forms. Basically everything that you see in the SRUI for entering metadata. Is created on the fly through the semantic elements. What that means is, if you wanna change the metadata. you don't have to do any new programming, you don't have to do any new UI development. You just change the template, you change the model. And from that everything else follows you get a new user interface, you fill in the blanks. All the software components in SR have well defined API's which make it really easy to have SR used by different clients. They wanna get access to different parts of the system. And everything that you see your all the metadata and SR. Is translated into JSON LD, which is really convenient because we can translate that to RDF if you prefer RDF. And lots of other formats that make it really convenient to use a standardized representation. To get access to all information necessary to really understand the metadata that investigators have associated with their data sets. So, we don't have spreadsheets anymore. We don't have to worry about making the mistake of putting things in the wrong row or column or not having access to the right ontologies. We don't have to worry about a confusion. Kind of information, because people are not constrained to enter ontology information. But still can only put in information related to the ontologies that on top of that, the template designers have suggested are appropriate. And we're in a situation that we have lots and lots of folks. Using cedar giving us feedback telling us that by creating information through these kinds of templates using ontology terms. Doing it in a web based manner. They're able to create metadata that they believe are going to be more useful. Than the kinds of poor quality metadata that we that we know are extent in the biomedical resources that frequently are associated with scientific datasets. So, I would argue that our online data are never gonna be fair. They're not fair now than ever will be fair. Until we can identify the kinds of reusable templates that will give us in a standardized form. What we wanna say about biological experiments in particular and scientific experiments in general. In order to be certain that we're seeing everything that we need for someone who's reusing our data. To get information about what experiment was performed. We're not gonna have fair data until we can use controlled terms to fill in those templates. So that our ontologies are giving us the guidance that we need to ensure that things are represented in a consistent way across experiments. And we need technology that's gonna make it easier for investigators to annotate their datasets in these standardized searchable fashions. And frankly, we need more than technology. We need a culture. We need the scientific enterprise to recognize that data we use is important that fair data are important. And that none of this is gonna happen until we develop an infrastructure that will make it easy for investigators to be able to create the metadata. That make that make their datasets useful, discoverable, and fair. So like the research parasites to the world. People like Purvesh Khatri my colleague at Stanford are gonna have the opportunity to be able to learn from data assessor cleaner and better organized more searchable. If we can make the metadata better by using technology that gives us standardized templates and standardized ontologies in the way that SR does. Scientists are going to be able to recognize that they can actually use intelligence. Pushing agents to search find to find new experimental results and other investigators performing in a way which is much more specific than searching the literature. Which right now which only gives us access to abstracts are perhaps full text. But always with the limitations of natural language processing, with the opportunity of looking at the actual metadata that investigators enter in ways which could give them more information about the experiments that are being performed more details about experimental structure that. And it's really important when they want to know how to expand their research programs. And clinicians who want to get access rather to clinical data will be able to understand better how theirs Subjects might be able to have situations relating to the subjects of experiment of those who are in clinical trials in a way that helps us to match better. The clinical trials that are available online with the subjects who may have different kinds of conditions so that we know what is the best scientific evidence that is available, in order to know how to treat patients with unusual, unusual conditions. Overall, we see this technology and what it will spawn as a mechanism whereby we can have investigators create very detailed, very structured very machine interpretable descriptions of their experiments. And add those to the kinds of metadata that are used routinely to describe experimental data sets online. And when that happens, technologies such as cedar, will allow the automated publication, if you will, of scientific results that go beyond the kinds of information that we have in our current online journal articles. And instead give us machine readable information at a level of detail. That will allow our intelligent agents if you'll to search and read the literature represented the form of online metadata, integrate this information with existing datasets in ways which is not possible now. Track scientific advances again with a level of specificity that goes beyond what is possible when just looking at the literature. We explore existing data sets. And because all this is kind of the machine processable those agents can suggest, what are the next experiments to perform. And you know what's happening in robotics these agents may actually be able to do those experiments on their own. Have to see what happens there. Many that we see CEDAR as a mechanism which allows us to move beyond spreadsheets, move beyond poor quality metadata. To move beyond datasets that are not fair and to create datasets that are basically not only fair. But getting the kinds of structured comprehensive information that makes it possible for investigators to reuse data in ways that were never possible, and to create new datasets online easily. So that the scientific community can benefit from the work that's going on basically throughout. Let me stop there and see if any questions.  Thank you, Mark. Yeah, please anyone who has any question that just speak up or feel free to put them in the chat and I can read them out.  Hey Mark, that was a great talk. I wonder, is CEDAR available across fields or is it especially optimized for health data sets?  That's a good point. CEDAR is not biomedically specific. So, obviously being in the School of Medicine and being surrounded by biologists, we use CEDAR all the time in this area and because most of my funding is from the NIH, our collaborators tend to be biomedical. But there is nothing specific to biomedicine here. Everything is sort of generic technology using standards Semantic Web approaches. What we can do and what we have done on a small scale is allow ontologies and other areas be shortened by a portal. And people are creating metadata templates in areas outside of biomedicine. So for example, my colleague john Graybill is working with a bunch of engineers in Denmark who are interested in physical science and climatology. And we We have a whole series of metadata templates and ontologies and cedar now that deal with collecting data from marine based windmills, that doesn't really sound biological to me and it shows you that this is, this kind of system is really quite open. And one thing I may not have emphasized is that my team is really eager to collaborate with anybody who may view this kind of work is valuable. And we love to see cedar being used in other areas because we think the ability to have arbitrary ontology stored in our ontology repository as well as Templates for a variety of scientific disciplines. And CEDAR really would give us new new ways of studying this kind of work.  As you see, in a moment, we have data infrastructures for learning data and metadata challenges one I totally appreciate and I just sent. My software team, I'll link to your website. So thank you.  Yeah, I know and if you want to collaborators drop me a line. It's great.  Thank you. Mark, I have a question for you. So I see it seems awesome. Is it an open source project if we wanted to contribute to it It's Yes, it is an open source project.  You can see everything in GitHub. We're trying to make it increasingly modular. It's not engineered in a way, which makes it really easy to plug in new components. But again, we're willing to, we're very eager to collaborate and so it is certainly something where we work on the software engineering as well as applying it in new application areas.  Awesome, thank you.  Any other questions? Anyone? Great. Well, we're running we're actually running ahead of schedule Ken, you're up in five minutes. So if everyone's take like a five minute bio break, we can Loop back around in five minutes and Ken yeah, you were obviously for the share sides. Ken I think you're on mute?  Yeah, same Zoom error. Yeah, I'll test it just to make sure.  Yep, we see it. And there we go. Great. Perfect. All right. So let's start at 925.  Or sorry, they guess that's 1225.  1225 great. Hey Ken. So you're on mute, I'm gonna start with your introduction. So Ken coating or hopefully pronounce our hate is a professor of human computer interaction and psychology at Carnegie Mellon University. Dr. Kissinger has an MS. master's degree in Computer Science and a PhD in Cognitive Psychology. Experienced teaching in an urban High School is multi disciplinary background supports his research goals of understanding human learning and creating educational technologies that increase student achievement. His research has contributed new principles and techniques for the design of educational software, and has produced basic cognitive science research results on the nature of student thinking and learning. Connor directs learn lab at Learn on.org which started with 10 years of National Science Foundation funding and is now the scientific arm of CMU Simon initiative. Learn lab builds on the passive cognitive tutors and approach to online personalized tutoring that is in use in thousands of schools and has been repeatedly demonstrated increased underachievement. For example, doubling what algebra students learn in the school year. He was a co founder of Carnegie learning incorporated that has brought cognitive theater based courses to millions of students. It's just formed in 1998 and leads learn on not assigned to nursing assignment initiative. Dr. Cohen has authored over 250 peer reviewed publications and has been a product that approached investigator on over 45 grants In 2017 he received the Hillman Professorship of Computer Science. And in 2018 he was recognized as a Fellow of Guardian of Science. So Ken, off to you.  Well thanks for that wonderful and thorough introduction, appreciate it. So, I'm going to talk about use of data to understand learning, and to Try to improve it particularly through implementations of educational technology. And in the process. I'll be illustrating a couple of data infrastructures that we've created with generous support from NSF, including data shop and learn sphere and I invite you to go to those Websites and try them out yourself. So the the key messages I wanna make today are first, that in education, there really are a bright, vibrant set of activities around data discovery and reuse and it started in older fields of AI and education. Learning Sciences and in AI education there's been a lot of effort to create so called intelligent tutoring systems that mimic human tutors. And in the last ten years or so, new related fields and associated conferences have Emerge, including Educational Data Mining, Learning Analytics, and Learning at Scale. So there's lots of activity. A lot of that activity is around doing analytics that creates better predictive models, and a lot of it stops there. But I really wanna emphasize the importance of going the next step, what we call closing the loop, and using discoveries to Actually, redesign systems and make predictions and test those predictions in randomized controlled experiments. A couple discoveries are illustrated in this one. Looking at how students engaging in learning by doing activities, appear to learn much more than They do by watching a lecture video or by reading text. But more specifically, I want to probe our efforts that efforts to optimize that learning by doing process by Using learning curves to discover hidden skills that lead to educate better educational technology and then to Better Learning and I'll summarize some of these efforts popping up to our web-based infrastructure that's designed to make sophisticated Analytics. Easier for social scientists and educators who don't want to necessarily write python or our code. That's a bit of background. We've had a long history of creating educational Technologies including the math tutors that you heard about in the great introduction. These have been both widely used but also widely evaluated and in perhaps the one of the biggest educational technology randomized field trials. 140 schools were. Either 70 were randomly assigned to use this cognitive tutor algebra course that we had developed, where a good chunk about 40% of that course involves students interacting with our intelligent tutoring system, our cognitive tutors, but we also use cognitive science to develop the text materials and teacher professional development. So it's got a big. Package kind of investigation. The other 70 schools used their traditional Algebra course materials and this is meant to illustrate, summarize a key result that the learning gain over a school year was essentially doubled. For students using the kind of tutor algebra, and we've done similar development efforts and evaluations with online college courses the Open Learning Initiative here at Carnegie Mellon University has produced lots of online interactive learning materials learning by doing opportunities and one particularly impressive. Result was from a statistics online course that was adapted through data to optimize student learning such that it was taught in a half of the semester and led to greater learning gains both on final exams as well as these percentages of learning gains are Standardized assessment of concepts in statistics. So if you know the physics education research is a little bit like the force concept inventory in physics. This is similar general test in statistics. The Learning gain on the standard exams I'm sure was much bigger. But this shows that not only you get better learning gains in shorter time, but better transfer as well. So these educational technologies provide lots of opportunity to explore learning by looking at the what's sometimes called the Data exhaust of student interactions with these systems and the Oli psychology course, was used as part of a MOOC that was developed at Georgia Tech where the lecture materials were delivered through Coursera you know with later videos This Georgia Tech psychology lecture, but the online reading materials, and importantly, the interactive learning by doing experiences. These are essentially formative assessment questions. Those were provided by Open Learning Initiative. And you can see an example here of an activity. Related to dimensions of, of personality, where students, get feedback as they drag and drop these different dimensions into this table. If they need, they can get hints as well. So the instruction, is embedded, in the context of doing. The students, get a sense for, what they know and don't know, and then can adapt. So we looked at the thousand students that completed the final exam in this course and their variation in watching, reading, and doing and use some causal modeling techniques developed here at Carnegie Mellon to build a causal model of the relationships here. So controlling for pre-test. Students who engage in more of the bottom these activities do better across the 11 unit quizzes in the final exam, as compared to students who watch more videos or do more reading And these are standardized coefficients from making each variable essentially a z score. So this indicates the effect size of an extra standard deviation in doing, on the total quiz score and then a big effect size of the quiz score on the exam. And essentially the summary result here is that learning by doing, I'll produce positive so more of all is good. But learning by doing in particular, produces six times better learning than by watching or eating. So, there's a lot of other learning science research suggesting that various forms of learning by doing one of them called deliberate practice or highly effective. But it really depends on how well tailored the activities are to students needs and they should be designed to address the edge of students competence. And we all have our sense of our own learning, but it turns out much of what's going on in the learning process and even the thinking process is below the surface of our conscious awareness. It's been estimated that as much as 70% of expert knowledge is outside of their conscious awareness. So that's a huge opportunity for data to help us gain insight into what's really going on beneath the surface. When we were building the algebra cognitive tutor, I was interested in exploring why story problems are alleged to be so hard and built a set of assessment items that are matched as these are. We also surveyed math educators and math teachers who suggested, as I thought at the time that the story problems and the word problems would be harder. In the sense that we gave students these kinds of problems, their performance, the percent correct would likely be lower on these two than on the equation. But it turns out, that's not what the data said. What we found, and we replicated this numerous times, is that in fact, the equation was the hardest for beginning algebra students. There are some nuances on the form of these problems that will change these results in sensible ways. But, this result was striking an important for our design of the algebra tutor. But more generally, illustrated this idea that experts have a big blind spot with respect to what they know and don't know. Algebra teachers do not necessarily see into the hidden skills that students need to acquire to be good at equation solving. And a lot of it goes to, essentially, algebra is a language, so skills for seeing the grammatical structure that the multiplication needs to happen before the addition. So you can't add the 1666 or the Asterix means times, or in the 6X format that the juxtaposition means times. The semantics, the syntax, the grammar, the lexicon of algebra, are something our brains are very good at learning by doing, but we don't necessarily realize how much work happened to get us there. And so as experts, we think this is clear and obvious, it just pops into our brain. But it turns out that isn't the way it works for novices and there's a lot of work that we could help the brain be doing, by better optimizing the instruction along the way. So the approach we've taken is represented in this loop where we start. In this particular approach with data from an existing educational technology system, use it to discover these hidden skills designed better instruction to address those hidden skills, and then deploy a new version of the system in comparison with the old. To confirm in these closed loop random assignment experiments that we get better outcomes. So I wanna walk you through one of those and we've done a number of these now. But the sources and intelligent tutoring system, here's a screenshot it's pretty old screenshot, but, from a unit on geometric areas. And one of the challenges in this context, are getting beyond helping students work on problems that aren't sort of point solutions. A simple formula answers the problem where they have to combine multiple formulas to come up with a solution. And here, they're asked to figure out the area that's left over when this end of the can is cut out of this metal square in here. This table starts off empty, the students working through the intelligent tutor is tracking their performance. And they make an error at this point, they asked the system for help, the system says at this column wasn't there. The tutoring system suggests they add a column to first find the square area. And then add a column to find the circled area, and then come back to this. So all that data is being logged, and each step here can be coded with respect to progress and learning. So, here's the general frame of an error rate learning curve where we have on the x-axis, which opportunity is this first student to display their competence in one of these Formative assessment activities. If they do struggle, they're gonna get hints or feedback. So they'll get either opportunities to assess but also opportunities to learn. And what we'd like to see is that the average error rate across students and across components of competence, across skills and concepts that average error rate goes down. And if we look at a learning curve straight out of a course where we don't code it, like any particular topic or concept or skill just by the order of each activity experienced, we get basically a mess. We don't see a learning curve, the error rate, man is gonna be going down for a while, and then it blips back up, and it goes down, and blips back up. That generic level of coding the data as though there's just one component of knowledge, geometry, does not lead to a smooth learning curve. And I'm now showing you a screenshot of data shop, I mentioned one of the infrastructures we built with NSF to help. If you code this data with one component, you don't get a smooth learning curve. But if instead, the data's, same data is to be coded to say these 12 components, not where this blip up here is maybe when trap is already the area's first introduced ,when we recode it in this way we now can re-average. So this was the 30th opportunity of geometry might be the first opportunity now of trapezoid. And the red data summary here shows the decline in error rate associated with each opportunity to practice. And the blue line is a logistic regression model, that growth model that models both the contributions of the difficulty of each of these knowledge components, the rate at which each is acquired and the student's overall competence to create a reasonable fit to the data. But the key point is this very general contrast here that I made, can be used more specifically to probe each one of these 12 components to see if it's showing a smooth learning curve. And when we do that we see that some of the individual curves, even at a reasonably small dataset here. I think this was 50 some students. We get good learning curves for some components. Some components actually start off with a low error rate, so we could improve the system right away by eliminating that busy work. But what I particularly wanna focus on is those that look like that overall curve. It's got a lot of these upward blips in error, what's going on? Well this particular compose skilled labeled the step I illustrated earlier of, for example, finding The leftover area when you cut a circle out of a square, and there were tons of problems like this, sometimes it's adding a triangle on top of a square there various versions of it. But what we saw is that error rate vary quite a bit. All of these steps here are essentially the same idea, the same procedure as taking two areas and subtracting or sometimes adding them. But the error rate was much higher in this one. Very low for these and sort of medium in here. And the key insight that we came to is that, when you provide scaffolding for students which is meant to help aid their learning, they actually it's sometimes over scaffolding. They don't have to do as much of the planning work with the scaffolding provided. Here they have to demonstrate that they can do the planning on their own. This scaffolding is often used as an instructional manipulation but may not be very effective. And it turns out, we can model this more formally. These discoveries and hopefully tracking the loop that I'm going around with deployed we have data, we've made a discovery. But now we're gonna redesign. So, a particular strategy is when you've got a hidden skill that students need more practice on design tasks. So that's the thing they practice and this is the key deliberate practice idea, certainly popular in athletics where we'll practice kicking the soccer ball into the upper right corner of the goal. But this works for cognition too like in reading phonics is a version of this kind of focus. But here the particular focus is we're just gonna ask them to plan a solution. We don't actually have to execute it. And when we do that, we see in the treatment now this is the closed loop random assignment experiment. That we can reduce a lot of that time they're spending on individual formulas most of which they pretty well mastered, might have overdone it a little bit. But we dramatically increase the time they spend on these planning steps. Overall, they're spending 25% last time. Some people say who cares about time, but if you think about that at scale 25% less time for learning means three years for college rather than four. And importantly, we get a positive effect on better learning. So we've been through this loop a number of times now and we've built some automated AI search methods to facilitate it in those papers about that. They also lead to better close the loop outcomes. I didn't start a timer here but I hope I'm doing okay with time. I just want to say-  We have about a minute left,  A minute left, okay. To say a little bit about DataShop. We built first and you can share data in any format, but if you share it in the standard format, you get a whole lot of these analytic tools for free. LearnSphere is an effort to allow folks to share in analytic components. And you can go to learnsphere.org and see that in particular, we have this web based workflow authoring tool where there's a menu of analytic components that can be dragged out here and configured in various ways. And that the connections between these components are data table data flow and the user can adjust the inputs. And this is comparing actually different statistical models for these learning curves and then see the output. So importantly, we're helping educator, education researchers, psychologists, folks, again, who want to do these kinds of and analytics. As well as helping folks who are developing new analytics share it by creating those analytic components and uploading them into LearnSphere. So I will stop there and see if there's any questions.  Thank you so much, Ken. Any other questions? No questions. Ken are you gonna be available on the slack or on the zoom chat after this?  I guess there's a session immediately following this  Mm-hm  Yeah, I can go to the open area. I think I'm forgetting what it's called, but I've been in the environment before.  Great, awesome. So if anyone has any questions can please, do that. And if you wanna get done with the platform, we can also go to gather town later.  Yeah, I was trying to think about that, yeah.  Great.  [CROSSTALK]  Sorry.  Yeah.  So I'd like to introduce our next speaker. It's Dr. Adriana Kokoschka. So, Adriana is an assistant professor in computer science and leadership Pittsburgh, excuse my professor when I took her computer vision class, her research interests in computer vision and machine learning. She has authored 18 publications on top of computer vision, artificial intelligence conference in journals like CDPR, ICCB, All those names. And ten second tier conference publications, like BNBC, ACV, she served as an area to care procedure in 2018 to 2021 neuroscience ICLR triple AI, which is going to serve as a co-production of ICTV 2025. Which is planning way out there. She has been on committees for over 40 hundreds of journals and has co organized 70 workshops. And our research is funded by the NSF, Google, Amazon and Adobe. Adriana, welcome and thanks and happy here. [BLANK_AUDIO  All right, thanks for the introduction, very much. I got a weird zoom morning. Hopefully you can see my slides, yes?  Good to go, yeah.  Okay, cool. Yeah, thanks again for the introduction and for inviting me. So, yeah, my research is about doing two things that they're kind of hard to make compatible, but one is understanding media and intent and persuasion in the media and the other is actually using weeks of revision to accomplish this. But, along the way, I've also collected plenty of non so weakly supervised datasets. So probably don't need to convince you especially nowadays as we have things coming up that the media affects public opinion and the societal outcomes like elections and whatever follows after those. So we want to understand implications in the media. In other words, we wanna understand the agenda the different media content has. So, earlier on, we looked at visual advertisements. And then we looked at images and text and political articles. And the challenge here is that data is limited. So we can take all kinds of pictures of dogs and cats and mountains and we have lots and lots of those. But images with intent and with some kind of agenda are not as abundant. And of course also annotating them is expensive because they appeal to a human audience with all of the human audiences, knowledge acquired along the way. So it's expensive to annotate them with all the knowledge that these require to actually properly analyze. So the goal is to learn some useful models from whatever data is already available, even if that data is noisy in at least kind of where I'm trying to take this reach to the research nowadays. So just the you know a motivating slide, with some images that are very powerful and impactful in their time. The first one is from the 60s, second one is from the 90s. Basically, these are Set to exchange society in some way, is an overall summary. I got interested in looking at advertisements and I have some examples of the bottom because even though you might dislike ads. Because they're trying to manipulate you some of them are very interesting and creative and require, basically AI to be solved to actually understand. So we're not gonna really understand them completely, but we're trying to get some of the way there. So, we argue that state-of-the-art vision systems are inadequate to describe the messages hidden behind purposely created ads such as this one that you have here. So hopefully it's somewhat clear what this ad is saying. I'll have our ground truth answer and vision systems we know nowadays are, are much better than, than it used to be, five or ten years ago. And we can fairly reasonably annotate them with recognized concepts or objects or even generate full sentence descriptions. However, these descriptions miss kind of the point of view. They missed what the ad means, which may be food at Burger King was taste really good and even the competitors and employee secretly buys it. So you have to recognize Ronald McDonald from his shoes, maybe in his hair and you also have to recognize and this is requires common sense, reasoning. You have to recognize that he's secretly buying it because he's wearing this trench coat. He's trying to be in disguise. And, vision systems can do this just because they were never trying to to understand meaning. So this is our goal to understand meaning and intent of these ads. Awesome challenges to understand what these all of these various and public service announcements mean. Purely visual challenge is that a non trivial fraction of them show objects in very creative ways, very typical way. So a standard vision system trained on imagenet or whatever photorealistic data set you want is not going to do well on these. So part of our research is focus on developing robusts representations that actually generalized across these modalities that is photo realistic or art painting or whatever. So this is kind of an offshoot into a more mainstream direction of domain adaptation and generalization. There's also a visual challenge is there's implies physical processes in these ads like melting and crushing. More on the reasoning end, there's associations that humans have acquired over the years, like guns are dangerous, that maybe we get from media or from experience, I'm not sure. Some of them are from experience, but not all of them. So, guns are dangerous, china is fragile, hot sauce is hot, and oven mitts are hot, but in a different way. So these are all the challenges. To get started, here's ago we collected this advertisement dataset, which for something it's not as large as the the numbers you've seen for other image datasets. But for something that requires actual human authorship and significant thought it's pretty reasonable. So we have about 65,000 images with various annotations that we crowd source with various mechanisms to ensure quality. So this is for images and we have another one for, videos, but much smaller. And you can think of these videos as being stories that we can analyze. Is an example of a task we'd like to solve on this data set, we want for the vision algorithm to multimodal with our algorithm to match an image like this where you see a crash motorcycle. So this is more of a PSA than a product ad. So we want the algorithm to match this image with text somewhat like this. So I should be careful on the road so I don't crash and die. As opposed to something like this, which is in terms of objects mentioned, it's close, but it's actually the opposite. So clearly this very somber ad is not trying to say buy a motorcycle is go very fast because that would be a happier image. So we have some way of representing regions and this images, the slogans so the text that's embedded in these images. Here we have something cool that we focused on which is symbolism in ads so you have trees and grass and fruit symbolizing nature. And then you have guns and bullets and knife symbolizing danger and ads kind of appeal to these symbolism a lot, this is part of what people studied in media studies is how symbolism in visual rhetoric. And then, as a way of transferring knowledge and transferring data essentially from other datasets. On this dataset we don't have objects annotated, but we can generate predictions from another model. So we get these dense captions that give us a proxy for what the objects in the image are. And basically what we do here is we put all these things together to get an image representation. And then we do metric learning where we try to sorry, where we tried to bring this image representation close to the representation for the correct piece of text. Is a high level idea of what the results show. Basically, if you look at ads, so ads have the image part but they also have a slogan embedded. These results show that the text is more useful for decoding the message of the ad and the message here. I should have made this more clear, the message is basically this. It's what you're supposed to do, what the ad calls for you to do, which is actually be careful on the road, and then it also provides a reason and the reason is so you don't crash and die. So we want the system to be able to retrieve this message. And if it uses the slogan alone, it's much better than using the visual alone just because the visual is very ambiguous. I can skip that on video you similarly two images have visual and textual channel the textual theory speech. And they generally find that if you just use a speech and videos to try to predict the message, it's actually a lot less useful. So speeches is less cleanly mapped to the message of the adder or vice versa. So speech is still more ambiguous than slogans instead of cans. He gives an example of how our method can correctly retrieve the right statement for this. So, I know these boxes all related here but the boxes are kind of importance regions, and the ad is actually this lady's putting on lipstick, but it's actually a cigarette. And so, this ad is meant, on first sight to look like a beauty ad or makeup ad. But, actually, if you look closer that's kind of where the power of ads comes in is that there's some kind of twist. So the twist is that it's a stop smoking kind of ad and our method can correctly figure that out. But in terms of using this knowledge which is my motivation here, humans have a lot of contextual knowledge, a lot of world knowledge. Usually these ads are not targeting newborns, they're targeting adults that have a lot of experience. And so this experience, our current method doesn't really have that. We don't want to just learn it in with supervised learning. We wanna be able to retrieve it from, just like humans rely on a knowledge base in their head we want to kind of utilize that as an actual knowledge base here we look at dbpedia. The problem with that is that a lot of information you can retrieve about any entity is not gonna be relevant. So if you try to retrieving information about Nike. The sports company here you get Nike, that's good. But he also have Nike, the asteroid, Nike, the Greek goddess, and so on and so forth. So we have it as a way of dealing with that our algorithm, First of all can learn which pieces of information are relevant and can also to make its training more robust kind of drop information in a training time. And by drop, I mean entire words to accomplish more robust training. But the benefit of that is here we have a graph that shows your regions in the image and kind of parts of the slogans. So this is our method. This is like a more basic version of our method. So the full version of our method can more appropriately utilize external information like Chanel is a French privately held company. Or here, this is the deer made out of trash and the slogan says, rubbish can be recycled, nature cannot. So it correctly retrieves information about what nature means as opposed to getting information about say nature of the magazine. And so basically its use of external knowledge is significantly more accurate than other methods. The problem of understanding ads is challenging also because of the relationship between the images and the texts in these images, so, here's a more typical ad for clothing and it says Winter Collection. And this is nowhere out of the ordinary. We've seen lots of images like that. But then this image also says Winter Collection and it's just the person dressed in a cardboard box so I can, for the sake of time, I'm not gonna pause and ask if you know what this is about. But basically, this is a public service announcement against human trafficking. This one is a typical clothing ad. And so, there's kind of an interplay of vision and language here that's interesting that prior methods haven't looked at. Because prior methods in terms kind of visual language method has looked at captioning and captioning is just both image and text are kind of redundant, but here image and text are actually complimentary and work together to convey a message. Just because image and text co occur desn't mean they're actually redundant or identical. So we've looked at whether image and text are parallel or not. And our goal here is to do this in somewhat weakly supervised or unsupervised way. We've also looked at generating ad-appropriate faces. Again, here we're trying to reuse data from prior models. So we see in this example that faces are fairly distinct for different ad categories. And we wanna generate them, but our data is diverse yet very limited in count. So standard generation approaches don't work, so instead what we do learn in our dataset is a very sparse attribute signature for each category. Such as domestic violence ad faces are gonna contain something like black eye, which we think of as an attribute. So we learn a signature from our data set, but then we learn how to go from each attribute to actual pixel space on a entirely separate data set. And here's an example result that we get. So is an extension of from this fairly kind of niche and focus space of ads, we've gone back to something more mainstream where we try to actually discover regions and learn about objects from weak supervision. So, here's an example that kinda motivated this, but basically, we were able to discover in our ask data set these Oreos and ketchup and so on. So it's like we learned an object detection model without actually having data specifically for this, without having any boxes as annotations. So, we have this approach where we try and learn from unstructured text like captions in this case. So from unstructured text that by no means mentions all the objects in the image and definitely doesn't mention their locations. We learn to actually localize objects such that a test time, we actually can provide a box with rather non trivial and fairly competitive accuracy by learning to model the ambiguity in text. So here we have three captions for this image. Actually, he has a tie, none of the captions mention a tie. We have a variety of methods for actually getting the pseudo label tie even though it's not in our caption. And lastly, we've also looked more recently at, so previous stuff was about ads. Then more recently we've looked at rhetoric in political articles which are multimodal. And what we wanna do is we want to give it an image predict whether it comes from a left leaning or right leaning media source. We have images and paired lengthy articles, but we just want to predict bias or leaning from the images. For training we're going to use weak labels such as the bias of the media source and we know media source biases from this website. So we only have labels at the source level. And they may or may not be correct or relevant for any particular instance. We have a data set of about 1 million unique images with fair text. So we are gonna use a text but we're only gonna use it at training time so we're not gonna use it to classify a test on, we're just gonna use it as an auxiliary modality, auxiliary feature. So our approach basically uses a text. So our observation is that bias in text is more obvious. So we're going to have this two stage approach where at, in our first stage of training, we do use text as an input. And we basically learned an entire CNN, including the feature extraction from images in text. But then in a stage two, we just retain the feature extraction part but get rid of the text input. And then we just learn a single shallow classifier on top of the pre extracted image features, because here we're not trying to classify objects, we're trying to classify left to right bias. So actually it turns out image features learn on image net or whatever are not actually very useful. And just as an example of our prior right before generation and to show that there is actually significant visual bias in how the same person is shown on the left and the right, we learn to generate faces of well known politicians without any extra data. So here are some examples of the photos that we're going to actually modify to be more left to right leaning, and we're gonna make them very,extremely left or extremely right, just to show the effect more cleanly. So here's our not so great reconstruction. But what's interesting is kind of what happens if you take all these pictures. So here, this is a smile. Here, we kind of retained the pose, his mouth is far down. So it's exactly the same pose of the face but now it's our model learned that what's gonna happen is that the same face is now gonna look angry, On the left. On the right where there wasn't a smile, there's gonna appear a smile, or where there was even anger, there's going to appear, A smile. So, I do have only one minute so maybe I just want. And the reverse happens on the right. We have a method that goes back to something more common which is retrieval, which is given in a political image or text and we have this new metric learning approach. But I'm going to stop there and see if I have 30 seconds for questions.  Any questions anyone, you can put them in the chat or just speak out loud.  I have a quick  Go ahead, please.  I'm wondering, Whether feature engineering plays a role and I guess at some point you were talking about expert import into this and can you say a little bit about?  We could, so a more strongly supervised setting here would be to take an image and say which parts of this image make it left leaning or right leaning. So if you have say this is our kind of silly made up example of, I don't know, there's values associated with the left and the right. And so maybe if you see a table of people sitting and having dinner, that's kind of a conservative value or whatever. So if someone could take images and tell us what parts of them are associated with a certain leaning. But we don't wanna do that, we still wanna at this time just classify things that actually have a bias rather than say classifying images of cats as being left to right leaning. So at this time, we do have, they're not actually expert labels, they're proud labels, but we ensure consensus in those. There's a lot of media literature that we could use here to kind of learn better features, but our motivation was not to do that. We do have a baseline that's a little bit more strongly supervised, with actual concepts that are associated with left and the right. But we do comparable to it with this weak supervision. Thank you for the question.  Yeah, thanks thanks that was a fascinating talk. Amazing work as always, so. We're gonna break now for lunch, and post the session on gather town. So, about until 2:10 PM Eastern, we're gonna be on break. So, feel free to hang out here, feel free to hang out and gather.town. But let's meet back at 2:10 for a fireside chat with the one and only Marshall Abear. 