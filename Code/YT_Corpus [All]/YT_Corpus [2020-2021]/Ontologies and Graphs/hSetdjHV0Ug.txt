 - It's my pleasure to welcome you today to Machine Vision For Cultural Heritage and Natural Science Collections, a program of the Yale-Smithsonian partnership. This is the first in what we hope to be a number of study days that are held either in New Haven, or in D.C. that sort of bring together folks not just who are on the board or who've been active in the Yale-Smithsonian partnership, but who are active in all levels of the university and in the institution respectively. The theme we've for this first study day, held here in Sterling Library and the Franke Family Digital Humanities Laboratory, is machine vision. And you can hear this described as computer vision, you hear this described very broadly as artificial intelligence, neural networks, but we really wanted to focus on the computational analysis of Raster pixels. Especially coming from an institution like Yale, which has such a strength of excellence in linked open data, RDF tuples, open access images, we really wanted to push the boundary and explore what would happen if we actually started analyzing the pixels of some of our natural heritage collections, in our cultural heritage collections as well. And so, that was the reason we decided to call this Machine Vision for Cultural Heritage in Natural Science Collections. We knew that both institutions had world-famous natural science museums and institutions of research and teaching, and we also knew about the world-class collections of art and other objects, sculpture, history that were present on both campuses and both institutions. So, Machine Vision for Cultural Heritage and Natural Science Collections unifies those resources, which are hundreds of years old, and it puts that together with some techniques which are very new, which have their origins perhaps in the mid-century, the 20th century, but which in the four or five or six years, have really taken of as ways of analyzing the visual data at scale. There's really no better way to start us off than with our first keynote speaker, who's Rebecca Dikow from the data science lab and the Office of the Chief Information Officer at the Smithsonian Institution. She'll be speaking today on data-intensive approaches to digitize museum collections. Rebecca? - Thank you, Peter, and thank you very much for the invitation to be here. I'm happy to talk a little bit about some of the work we're doing in the data science lab. I will preface it by saying, I'm a trained biologist and came into the high-performance computing world when I was generating genomics data as part of my dissertation. And since I arrived at the Smithsonian, after my PhD in September of 2012 as a postdoc, I start thinking about Smithsonian collections more broadly and not just genomic data. And so, today, I'll talk to you about how we're expanding our big data research outside of biology, but I'll start with biology. So, the Smithsonian has a number of challenges to conducting large-scale informatics research. First is there's a large diversity of locations, heterogenous kinds of digital data, incomplete meta data, and a lack of purpose-built software tools. This is particularly true for our biology groups where most genomics tools have been developed around the human genome, and the genomes that Smithsonian scientists are interest in are everything but the human genome. So everything from clams to bacteria, bacteria are a little bit easier than everything else, to plants, to pretty much anything you can imagine. And so we're constantly on the lookout for new tools, and seeing how our data fit into existing models. This isn't just a terribly resolved photo, it's a photo mosaic of an aerial shot of the mall from our archives that all of the pixels are filled with other aerial shots and other building photos of the Smithsonian. for a machine vision workshop that might be somewhat interesting. So the Smithsonian has 19 museums, 9 research centers, and a zoo most of which are on the Smithsonian Mall, but some of which are very far-flung. We have the Smithsonian Tropical Research Institute in Panama, which has a large group of very accomplished evolutionary biologists, field biologists. The Smithsonian Environmental Research Center out in Annapolis, Maryland, as well, who do ecological research. So, we're not just collections based, but research data based as well. When we talk about digitized collections, I will keep a natural history focus for now just to to talk about the different kinds of digitized data that we're talking about. Photos, so for example, this is a photo of a pinned fly. The taxonomic names, specimen records, genomic sequences, so we have data in GenBank, geo-referenced localities, and label data, transcribed label data as well. I would say that these are all part of the collections object, but then there are other kinds of auxiliary data that might be part of our understanding of this one specimen. Which could come from field books where scientists who have collected these specimens wrote down some observations about it, illustrations, again observations, you know, what was this fly eating at the time it was collected, scientific publications, and taxonomic descriptions. These all could be linked, these all are part of what we know about this fly, and being able to computer cross all of these data is something that we strive for. This is just an example of what a specimen collecting event looks like in terms of where the data go. In the center this is Vicki Funk, she's the curator of botany and a great collaborator of mine. So, for example, she went out to collect this purple very pretty flower, the species is called centrapalus pauciflorus. We decided we wanted to sequence this genome because it is a part in the sunflower family tree that has no genome around it, so we thought it would be interesting. She collected the seeds, actually, and we grew it in a greenhouse because we needed a lot of DNA for the kind of DNA sequencing we were doing. From the actual specimen, most of the plant gets pressed, and so you can see an image of a pressed plant. Some of it goes to a bio repository, so we have its DNA forever. You might sequence a small piece of DNA, a barcode for plants, those are chloroplast genes. Those data go to NCBI, which is part of NIH, it's generally called GenBank. The specimen information, there's the specimen record, which is in the Natural History Museum it goes to a database called Emu, and those data are pushed to GBIF, the Global Biodiversity Information Facility, so that you can go to GBIF and find records of specimens from our museums and also lots of other institutions. The photo might go to iDigBio. So, this is just one specimen, and this all the places that those data go to. It gets way more complicated. So, that's before any research question, right? So that's just like we're going to document that we collected this, and have put it in the places it needs to go, but in order to do the research questions Vicki's interested in, we didn't wanna just sequence the chloroplast gene. We wanted to sequence the entire genome. These are the different kind of parts of research that we engage in. The little museum icons at the top show in order to generate the deep learning models that Alex will talk about later, we need data from lots of different museums. We need specimen records, locality information, and images from all over the place. In order to build a phylogeny, we need to sequence, in this case, this is a paper we're about to publish with 256 species in the sunflower family, we sequenced 1,000 genes for all those 256 species. So those data go to NCBI, but we may, in some cases, pull data from NCBI from other institutions. And then, for the genomics, there's a huge amount of software tools that we need just to assemble and annotate the genome. I'll show closer views of these. We'll make a genome browser that we can share with our collaborators where they can continue to keep editing our annotation. Well, that's one genome. What about all the genomes that are being generated at the Smithsonian? I'll show something that's under construction for us, which is the Smithsonian Genome Hub where Smithsonian researchers can put all their genomes in place, and start thinking about how to use them together. So, the research is what adds a huge amount of complexity. Again, this is for one group of plants. There are hundreds of Smithsonian researchers that are kind of doing things along these lines. I'll take a short little look at genomics. Genomics at the Smithsonian has been, we're generally really good at natural history. The scientists at the Smithsonian know their organisms, they know where to go in the field to collect a species, they know everything about it, but genomics has been a big challenge to get all the software that researchers need working well for their data. This is just an example of some of the tools that you need to assemble a genome and annotate it. And a lot of times you have to try multiple kinds of software to see what works best for the data. This is just zooming into the genome browser that we're creating where you have these prediction tracks. So, this is just zoomed into a very small piece of one scaffold, and all of the gene predictions on there, which scientists can go in and say, "Okay, I know about genes for drought tolerance." I will go and look at those, and see in detail does that look right, and how can I extract that, and compare it to other genomes that I know about. We're doing this in the cloud right now, so we can easily share it with other researchers outside the Smithsonian. So, we have a high-performance computing cluster, and part of my work is helping to administer this. There are about 4,000 CPUs and 26 total terabytes of RAM. That graph, the plot just shows in the last 30 days the usage. So, it is kind of up and down, and it has a weekly cycle, so people don't do as much work on the weekends, which is good. But it's been really, really busy. We have 16 very high memory nodes. So, a genome assembly often you need a terabyte or two terabytes of RAM to assemble a genome. These are pretty beefy nodes. We have many high CPU nodes as well because most of bioinformatic software doesn't work well in NPI frameworks, it's multi-threaded. For GPUs, we have a node with NVIDIA K80s, but more recently we've, my team, has gotten two workstations with NVIDIA GP100s, and we'll probably upgrade the GPUs in Hydra very soon. We have more than 300 genomic software packages installed on Hydra. This is just an ongoing thing where people are requesting things. I don't know, maybe five times a week we get requests for more software. And around 550 users. In terms of natural history, that's about, I think, 182 natural history museum users of Hydra. That's a lot. Hydra started at the Smithsonian Astrophysical Observatory, astrophysicists are way ahead in terms of of high-performance computing, but we're catching up. And so, we have as many biology users as astrophysics user, which is really a big change. It's really exciting. Training is a big part, as well, of what we do because we have hundreds of researchers, curators, postdoctoral fellows, graduate fellows, interns that all wanna do this kind of research. So Smithsonian has joined The Carpentries organization. I'm sure many of you have heard of data software and library carpentry. We joined, I think, two years ago, and now we have 13 trained instructors with, I think, 10 more that are just finalizing their instructor training. In the past three years, we've trained more than 300 researches in Python, R, genome analysis. We started doing more ad hoc workshops, so people want to learn genome assembly, so we would a six-week workshop with an hour a week talking about genome assembly, but then we realized... We did a survey of Hydra users, and said, "What kind of workshops are you interested in?" and it turned out that Python, R very ground level workshops and techniques that they can then apply to all sort of research were what people were really interested in. We keep all our workshop material on a GitHub page, which you can go to. As part of our website, we have a schedule and place where you can request workshops as well. So, our instructors would be happy to go anywhere and give these workshops. Okay, and I'll talk a little about the Biodiversity Genome Hub. It's under construction, but we're really excited about it. So, the plan is to have a place that's accessible not just within the Smithsonian, but to our collaborators where we can have all of our genome data together. So, it's even hard for us at the Smithsonian to list all the genome projects that are going on. I've tried to get this information. It's something like 150, but having one place where we can bring all the data together, and have standards about how people are storing that data, and making sure it's backed up, and making sure it's described in a way that makes it reusable is really important. So, we're constructing this website, which will be run through Galaxy, which is a bioinformatics software. An opensource way to make reproducible pipelines and workflows of different tools, as it wraps around any kind of bioinformatics tool you'd want. And we hope to provide researchers the ability to do basic analytics on their genomes once their sequenced and share them with other people, and hopefully allow students and undergraduates and graduate students to really get into genomics in a not so intimidating way. So, now I'll talk more about the computer visions stuff. Sorry for the detour, but I think the genomics work is still pretty exciting. So, I'll give you four examples of some of the work we're starting on. The botany project is the most mature, and Alex will be talking about that this afternoon, and the other ones are just still experiments at this point, but pretty exciting. The Smithsonian Digitization Program Office has started a program in the Natural History Museum to digitize, or generate images and transcriptions of the labels for all the botanical specimens in the herbarium of which there are five million. So, there's this conveyor belt that runs eight hours a day, and generates thousands of images per day. I don't know exactly. I think there are three million at this point that are completed. And as a first step, two and half years ago or so, we thought, "Okay, let's see if we can "train deep learning models "to classify, and to detect mercury staining?" Why mercury staining? Well, botanists used preserve their specimens with mercury to keep pests away, and we know now that's not a great thing to have alongside people. And you can see the specimen on the left has been stained with mercury, and over the years, the mercury crystallizes out, and so it's very visible to the human eye. But it's not generally part of the metadata for the specimen. Sometimes there's a little poison stamp on some of them. We're not exactly sure if that directly correlates to whether it has mercury or not or it could mean something else. But we can see it, it's not part of the metadata, so we thought an interesting question was, well, can the computers see it, and can we then point out hotspots in the collection that might be, where some mediation might be required. They put the mercury stained specimens that they know about into special folders, but no one person can open five million folders to check the rest of them. And then family ID. These are two different families of fern allies. They're quite similar to non-fern biologists, such as myself (laughs), and we thought we would do a very simple binary classification problem. Can we tell these apart with good accuracy? And NVIDIA helped us with this first pilot as well. So, these are just confusion matrices showing for each model that we built the number, this is just the test set, so we... I should have mentioned earlier that we had to manually find a training set of mercury stained specimens. So, it was a lot of hours of going through images, and we pulled, we had a set of 10,000 that were mercury stained and 10,000 that were clean that we labeled by hand. And the confusion matrix shows, just for a test set, which of the stained and un-stained actual images were predicted to be stained and un-stained. So, you would hope that the diagonal going this way would be 100%, it's not a 100%. It did very well, so the percentage is in the 90s for both the family identification and the mercury staining. So, we're really excited about the fact that this worked, and you will hear what's next for botany in a very, very, we've expanded this greatly this afternoon, so I won't talk anymore about that. We'll talk a little bit about bumblebees as well. You may say like, "That's pretty random. "Plants and bumblebees," but (laughs) because the Digitization Program Office has targeted specific collections, we've kind of gone with where the data are right now, and so the bumblebee project was to photograph all the specimens of bumblebees, around 45,000. Bumblebees are all part of the same genus, bombus. And so, all of these were photographed. This is just examples of how the photos look. They're generally just a lateral view. Some of them are a dorsal view. The labels are, as well, in the image, and when I looked at the images I thought, (laughs) "I can't tell these apart at all. "I don't know if this is going to work." Entomologists might say, "Well, you know, the way we tell these apart "is very specific banding patterns and genitalia." Well, we can't really see that in these images very well so I don't know, but we'll give it a try. The other interesting part of this project is the transcription was crowdsourced through the Smithsonian Transcription Center, which is where volunteers go online, and can transcribe label data which is a really great way to get people involved in real research. All of our labels, we have good transcribed data. So, we didn't really have to go through, and make the training set ourselves. The interesting thing is that about more than 10,000 were unidentified, and I talked to the bee curator at Natural History, and he said, "No, that can't be." And then, we opened the cabinets and said, "Yeah, 10,000 of these are unidentified." So, the idea being that if our model can classify to species pretty well, we could at least have estimates of what the other unidentified bees are. We may not be able to put that directly back into the collections system, but we can at least tell researchers coming in, "You might also be interested in these specimens "because we think they're this species." So, this didn't show up super well. We trained two models so far. A subgenus model, there are 15 subgenre of bombus that we have in our data set, and the overall accuracy is above 93%. And for the species model, this was really interesting, we're still just starting out working with this, but there's 178 species, and some of them have 10 images or so. And so, I've thrown them all in even though those are very small amounts of data. It's still overall very accurate. The next steps are to talk to the bee curator in more detail about whether any of the things that are more often confused are actual taxonomic issues that are either mislabeled in the data or they are very, very closely related. We're also just starting to experiment with the activation heat maps that we can generate from the models that allow us to look at which pixels are activated in the model, and start understanding what the model is looking at in the bee. Success in this area will be really important to convincing the biologists that it's not just a black box, that we're actually... the model is seeing the same kinds of characters that you're looking at on your microscope, and it's not so scary. So, now I'll talk a little bit about the project we're really just starting, but the Smithsonian Conservation Biology Institute Center for Conservation and Sustainability, they work with oil companies mostly to look at impacts of installing pipelines, mostly in the Amazon. And, with this project, there is a company that's looking to install this flexible pipeline, and they are looking to look at baseline biodiversity before, during, and after the installation. They've traditionally used camera traps, and people actually catching animals, taking DNA. But we're trying a new approach where fisherman have been given cameras and given training into how to photograph fish that they catch. We're getting the images, and now we will start training a model to see if we can identify fish to species. And as the pipeline gets installed, we can have a time series, hopefully, of what the communities look like before, during, and after at different time points, and then figure out what impacts the pipeline has on fish communities. That's really exciting because it has real world impact. And then, I'll talk briefly about two other collaborations we're working on. The United States Holocaust Memorial Museum approached us because they have lots of digital data, mostly scans. Although, the project we're talking about is scans of documents without any metadata about them, just what archive they come from. So, we are current looking for a postdoc to lead this project to start understanding more about the kinds of documents that they have. So, they have a lot of diaries, letters, lists, birth certificates, marriage certificates. Just all sorts of archival data around the time that the Holocaust was happening, and making it more discoverable for researchers that come to their museum to do research and in terms of the digital data don't know where to start. So, we're hoping to make a dent in that. And the Smithsonian American Women's History Initiative is another pan-Smithsonian initiative that started last year with a large digital component, and we're hoping to contribute by training machine learning models to again, have a better baseline data about how women have been involved in the Smithsonian over time both in collecting objects as well as producing artworks. You know, where are the women in the Smithsonian? What stories have we missed in the last 150 years? So, to that end, we have some preliminary data from Smithsonian archives where we're looking at official Smithsonian photographer images where generally they go through and identify people that, let's say for example that this is an example of a photo. They're really just interesting on a human level to look at the photos of what was going on in the 1970s. In this case, the women are all identified, but there are many, many thousands of images where people aren't identified. And in group portraits, more often, men are identified and women are left out for whatever reason. So, we're hoping to first get a survey of how many times women that are labeled in these images show up in other images that aren't labeled, and to start getting some baseline data about that. Here are just some examples. There are lots of group portraits like these where it's from a meeting or from something else, and people aren't labeled. But maybe we can start making connections between people where we have images where we know where they are. And also finding completely outside of knowing who they are clustering faces as well in the official photographs to try to find people who maybe haven't been described. We don't have their story represented in any Smithsonian collection or metadata, but maybe we can start elevating their stories. So, the takeaways are we've really only just started this work. There's so much to do. And right now every, oh sorry, there's a typo, every application of machine learning is a research project given our diverse and incomplete, unique data. So, I made another photo mosaic of the castle with all the portraits data that I've got from archives, so portraits of Smithsonian people where we know the people. I keep thinking who are we missing when we're talking about who has kind of built the Smithsonian, and so, hoping that we can use these tools to elevate new stories that better represent the people who actually visit the Smithsonian. And so here's my team, and this work would not happen without them. We have from left my postdoc Mirian Tsuchiya, Mike Trizna, and Alex White. Alex will be talking later. Our partners across the Smithsonian, again this work wouldn't be possible without all the other Smithsonian units. And funding, the Smithsonian Women's Committee, the Office of the Provost, and the Office of the CIO. Thanks. (audience clapping) (light music) 