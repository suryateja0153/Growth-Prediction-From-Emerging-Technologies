 and start us webinar start webinar oh are you sure you want to allow the attendees yeah I'm gonna start sharing my you could also do Google Hangouts in the future you know but then I have to download it and maybe hey hippie butter hey everyone let's see chatroom since you guys for attendees that are here just so you know you can actually chat I love Thomas raise his hand so the so the raising hand thing basically means um I have a question we don't have to do we don't have to be that super formal here because we do we do have a small intimate group so if you guys want to turn on your microphone you could turn your video on I think there's only a maximum of four videos at a time so but we can make this into like an intimate session you know instead of having it be like a super II normally webinar anything thanks everyone for joining hi again in training and um so everyone this is Alicia but you may have probably know her do you want to introduce yourself out you in traditional sure so I'm Alicia frame I'm the lead data scientist at neo4j and I work on the product management team responsible for the graph data Science Library which I'm super excited to tell you about today working with engineering to build the product set the roadmap and working with customers and the community to get people using graph algorithms so just a brief heads up for all you guys that are in here I'll also post it in the chat just to make sure that people join late because it's literally one minute into the webinar some of this stuff is not necessarily publicly on a secret preview for you guys yeah kind of secret preview ish so we ask that you don't go screen-sharing and you know posting it places in a couple weeks when did you say these are like a couple weeks or like a month so they'll have a press release probably the first week of April and then we'll be doing a digital event I think at the end of April to do an official big press marketing launch and but we'll have the press release out I guess April 2nd is about two weeks from now mm-hmm and then it'll be generally known so I'll be talking about the graph data science the library which is the successor to the graph algorithms library I GA on March 5th so we're like into week 2 of general availability which was very exciting my talk today about how to use it how to use it specifically at scale we've done a lot of engineering work around supporting you know multi billion node graphs and running graph algorithms quickly reliably in production and kind of what went into that how did what are the tips and tricks for getting it to work all that good stuff awesome so just just a heads up for everybody here if you this is your first time using big marker if it's not you already know just ignore this stuff but there is a public chat room which you guys see Alicia if you decide you're like putting your whole thing in your screen if you can't can you see the chat room when you I can't I bet two months and then there's also a QA tab there if you have a question that you want to be saved for later or something like that you can punch it into the Q&A section the way you do that is when you type out your message it'll give you the option to instead of pressing send to add the Q&A and yeah I think that you can also private message each other which I don't know if you guys need to do that but I'm gonna tell you a little secret oh yeah I think that's it so uh alright guys enjoy and um I will talk to you guys after the presentation and you know since Alicia is watching the chat she can I'll let you know if there's any like questions that are gonna be raised hands in the middle of the I can shout it be if they're something really important when I'm in person I always tell people they can throw something at me I don't know what the equivalent is online alright here's what we'll do if anyone want to ask a question like using your voice cuz that you can literally you can control your own microphone you can turn it on on your video or whatever if any of you guys want to do that during the presentation if Alicia doesn't mind that you're gonna talk during the presentation I'm like it if it's relevant for that uh you can put your hand up and then just turn your microphone on and just ask you can literally just you could just turn your microphone on ask you don't even have to okay very good okay all right let's get started so neo4j graph data science what I wanted to talk about today is what the heck is the graph data science library you might have seen on the community forums or on either slack we said hey the graph data science library is in preview or it's been at least what is it and talk a little bit about the changes between the gravanos library and graph data science that changes to syntax specifically around the algorithms and the graph loaders and then spend the rest of the presentation talking about how to build scalable workflows for kind of large production graph algorithm use cases so I like to kind of start off with just like what is graph data science and how are we defining it and like the official definition this is probably in some of our blog posts is that it's it's basically data science but with graphs and what we mean there is you want to use the relationships and the structure and topology of your data to make better predictions and decisions so it includes everything from querying a knowledge graph calculating statistics about your graphs running graph algorithms or building machine learning pipelines and kind of the two places where we've seen the most traction and interest in our users is either in graph analytics where you have a specific question that you want to answer like I have a graph tell me which nodes are the most important or what are the communities in this graph where you can say this is the right algorithm to answer this question and I want to execute this on my graph and directly use those results so I want to say how many communities are there I run labeled propagation and then I use the results of label propagation and I say well this community number I'm going to go do something with this or I hand it off to an analyst the other kind of more advanced side of that is when we talk about graph we enhance machine learning Nai and that's when your training models using graph eat data so you have kind of your existing model building pipeline let's say you build your random tourist classifier you're just adding new features that you're extracting from the graph either with queries or Biograph algorithms to summarize the topology and what we've really seen is kind of everyone that we have up and running in production using graph algorithms or graph data science is really starting in the analytics space so they're using the graph algorithms to answer questions they couldn't answer before like who is committing fraud or I have a entity graph I want to do entity resolution and figure out which are unique users or I have a marketing problem and I want to figure out who do I send my marketing materials to to get the most impact and they're running one to five algorithms to answer those questions and then the results are being handed off to an analyst who does some manual work with those where everyone wants to be is graph enhance machine learning and AI where instead of saying here's my question I run this algorithm you run a bunch of different algorithms to generate features and then you use variable selection to figure out which features are most predictive go build your machine learning model and it's more accurate because you have relationships what we do to support this at neo4j is the new graph data science library so probably I think most of you are somewhat familiar with the graph algorithms library that was originally developed by neo4j labs it's been out in one form or another for about plus two years we have an awesome book about it a year ago I joined neo4j to basically take that labs effort and turn it into an officially supported product and I guess week and a half or though we GA the graph data Science Library which is the successor to the graph algorithms library so what we've done is there's kind of three pieces in the GDS library we talked about how its optimized for analytics so our engineering team has spent a lot of time developing custom data structures that are optimized for global traversals and aggregation if you ever used the old Elgar's library and you remember like graph the graph equals huge or graph is what was the other heavy those were the different kind of data structures we've spent a lot of time optimizing the data structure so there's only one to choose from it's the right one it skills it flexibly subsets and reshapes your graph the next piece is graph algorithms so the graph data science library includes the maturity of the algorithms from the old graph algos library there are a few that were retired they were broken there are also some new ones in the GDS library the product supported algorithms in this library are all highly parallel lines and they're guaranteed to scale into the billions of nodes so we're really focusing on providing robust solutions so that you can run your graph algorithm your graph analytics on a production graph for an enterprise organization and we're also offering at the same time early access to dozens of experimental implementations so I'll talk about this a little bit more later on but we have kind of the product supported namespace but we also have the alpha and beta namespace which give you kind of previews in to experimental implementations of new algorithms and that's where you'll see basically things start off in alpha and then as they mature in the product they move up into beta and then into the product supported namespace and then one of the things that I'm the most excited about is we've spent a lot of time on developer experience for the graph data science library I think there was a I used to graph all those library enough to know that there were some frustrations with the syntax of inconsistency with it wasn't always intuitive you could specify the same thing in many different places and get interestingly incorrect results so we have spent a lot of time on simplifying and standardizing the API so you can subset reshape your graph run your algorithms it's intuitive it's as customizable as you need it to be but it's simple and straightforward to run and then we spent a lot of time on documentation training and examples so getting started is easy I'll have this on the reference slides at the end but we have a browser guide to get folks up and running we've got a sandbox for graph data science we've got a lot of materials you'll see coming out for training in the next couple of months so we want to make it easy for everyone to use graph all those in the graph data science library and then the big piece here is it's all products supported in being actively developed so we have a team of eight engineers who work full-time on this we're putting a lot of engineering resources into providing folks with the data science library that they need so what the heck is in this ga released that I keep talking about and this is kind of the summary slide when we talk about algorithms we have 48 different algorithms across three tiers of support we have five production quality algorithms so these are the ones that we guarantee will scale their parallelized optimized implementations we have two beta algorithms and we have 41 alpha algorithms and this is the majority of the old lab cell rhythms and they all support four modes of execution so before you had streaming and if you didn't specify anything it would write to your graph we now also include stats and estimate I'll talk a little bit more about that later on for infrastructure we now have a single scalable flexible in-memory graph structure that you use to execute your algorithms so if you've ever seen the graph that load command what that does is that takes your underlying transactional neo4j database reshapes it into the in-memory graph structure that we use to execute the elements and we spend a lot of time decreasing the memory footprint of that fixing it making sure it's as big as it needs to be the only limit right now is really the amount of memory you can get your hands on an API I talked about this already standard simple API supporting production scale where quotes and then a lot of work has done it documentation so those are kind of the four pieces of what's in the GA release and I think one question that I've heard from our field team is is this actually any different from the graph all goes library and the answer is yes so at one point in time there was a two billion node in-memory graph limit that's gone we now have a single native craft loader that's optimized for memory footprint and speed so it's much heap as you can give it you can store as many notes as you'd like support for multiple relationship types so we have you can now see and store and recall multiple relationship types in your in-memory graph we also support relationship aggravation so you can flexibly deduplicate or summarize relationships in your transactional graph when you load it to the in-memory graph and we also allow you to combine node labels without using the cipher projections you will hear me say this several times in this presentation cycler projections are super flexible but they're also quite slow we want to make it fast and flexible to use the native graph loaders so we've spent a lot of time making sure that that's expressive and able to support kind of what common work patterns we see um we've really shifted the workflow into named graphs and graph catalog operations so probably if you've used graph others before you used to a workflow where you type something like you know call L goes up page rank person knows and then you hit enter and it calculates and it writes it back to your graph that's really good for a one-off but if you think about running something into production when you call algorithms that way you have to load the data into the in-memory graph execute the algorithm and delete it if you're running multiple algorithms and an analytics workflow you probably don't want to keep reloading and deleting your graph so what we've done is we've pivoted towards you load your graph once using the flexible graph loaders and you look kind of all the data you need you execute your algorithms against that in-memory graph and then you delete it and often the slowest part of algorithm execution is that graph loader step so we've pivoted so you're only doing that once or as few times as you need to instead of kind of constantly reloading so we spent a lot of time kind of pivoting the library towards that type of workflow all of the product algorithms have been reimplemented and for improved performance and scales so rebuilt from scratch bug fixes across the board and there's a complete API overhaul with new consistent syntax in error messaging so I'm gonna give you a lightning fast tour of how to use the du/dx I want to talk more about the workflows but I realized it doesn't make a lot of sense if I don't explain it like what the syntax looks like first so bear with me um super-exciting um all the algorithms in GDS now follow the same syntax with the same consistent configuration parameters across the board it pretty much always looks like call GDS top tier if you have a tier for your algorithm dot name of algorithm dot execution mode you can use that estimate and then you provide it with the name of your graph or the configuration and you just have to remember this in order to run out goes walking through what these different pieces are we have tiers of support so I already talked to this a little bit product supported means they're supported by product engineering if something is broken we fix it right right away they're covered by like our enterprise support agreements they're supposed to be stable scaleable fully optimized these are all called by just called GDS algorithm name so this is PageRank levain label propagation let's see if I can remember the other is node similarity and weakly connected components beta all nodes are candidates for the product support it's here so what this means is we've spent time optimizing them and building them up but what we haven't done yet is maybe they're missing some documentation maybe they're missing one of the different execution though it's probably stats but we're actively developing it and it will become product supported probably in the next release and so right now I think k1 and modularity optimization are in the beta namespace and then the Alpha algorithms are the experimental implementations these are currently least stable and they can be changed or removed at any time this is basically where things get put to test to see what the traction is and then as we see adoption there's mature up into the frontage but we wanted to keep everything available to all of our users I'm sort of saying well these five are the five we finished so we have three tiers of support different execution modes so this is pretty exciting to me um anyone probably most people on this call of use the algos library had forgotten to specify dot stream and accidentally written to their graph so right now we have kind of the three core execution modes so dot stream is still there um streams here is a Bacchus cipher result rows now we have dot right so instead of writing to the graph being the default behavior that's quite slow so we want you to explicitly say dot write and they specify a write property and this writes back to your graph and then we have dot stats so this rule returns statistics about the algorithm output things like percentiles and comments so this is a good way to say like I want to run this algorithm before I read it back to my database I want to see if the results make sense I'm gonna run blue vein I'm going to pull back the stats and I'm gonna look at the percentiles and see if they're reasonable before I save all that data so this is kind of a nice trick to check your work before you commit to using it in the next release so the 1.1 release which should be coming out there should be a preview by the end of March and we will also support that mutates which will let you update the in-memory graph and that's pretty exciting and might talk more about that later then we also have dot estimate so estimate is really important if you've used GBS or graphics on a large graph you've probably out a memory that your database and knocked it over that estimate tells you if you have enough memory to run the algorithm before you try to run it so it's just like that estimate with graph catalog operations it's called GDS dot PageRank I will tell you how much memory do you need to actually execute that algorithm and this is only supported for the production quality others so the main main space and but as things graduate up with support I've got estimate an important thing to be aware of anonymous versus named graphs so when I talked about before probably a lot of folks are used to doing you know call L know dot PageRank person notes and that's an anonymous graph so it's loaded at the same time as the execution of the algorithm named graphs are loaded separately with a name so they can be referenced so that's when you call call an algorithm and instead of referencing the nodes and relationships you pass it a graph name and what we're recommending is for scalable workflows moving towards using a single named graph floating it once and then flexibly running a bunch of other great lines across it cool so that was super lightning fast what is going on with the other syntax now to quickly cover the graph loaders um why do we have this graph that loads stuff anyways why is it there the kind of technical explanation is that executing graph algorithms is hard we need the right data structure to do it so if you think about transactional neo4j like the core database it is a doubly linked list and this is optimized for fast local reads it's mutable and it's queryable but when we think about like running and algos workflow you need to kind of iterate over the entire graph many many times so we actually store our data or we reshape the transactional data into an adjacency list it's currently immutable it's optimized for these global traversal operations and it reads writes from the neo4j database but what this means because we're not executing on the core database is we can flexibly reshape your underlying data and create and manage multiple analytics graphs to support different workloads so I only need to look at this part of my graph for this algorithm I can load this up via the graph loader I can reshape it I can change it and then can save my results after and this is kind of the schema of how this works so you have neo4j you read your projected graph via the graph loader into an in-memory graph which is the adjacency list structure the algorithms are all procedures that execute on top of that and then you can either stream the results out or write them back to neo4j coming soon you'll be able to also update the in memory graph so it'll Dex craft management now that we're telling you that you should create graphs super important to know what to do with them you can create graphs you can track graphs and you can remove graphs important not to forget about removing graphs they will stick around in memory until you remove them and if you create dozens of in-memory drops you will eventually not go over your database and I am saying this from experience so when you create graphs I already talked a little bit about names versus anonymous graphs so the named graph is loaded once with a name and it's used by multiple algorithms so I load my graph into memory and then I run PageRank and I say call a GDS PageRank my graph and it knows that that's already been loaded and it executes against that and the anonymous case is where you create the graph execute the algorithm and delete the graph in the same step the other dichotomy on the graphs to think about is some data versus cipher projections so these are the native projections or when you use something like graph that load person knows and you're referencing the node labels and relationship types and we can load directly from the data store so that's very fast we can do it in parallel if you are trying to run something scalable and repeatable native is kind of where people start the siphon projections use a cipher query to populate the nodes and relationships then get put into the same data structure for the analytics graph this is still bottlenecked by cipher so so for as great cipher is expressive cipher can sometimes be slow so basically there's a trade-off where native is less flexible but lightning-fast safer is more flexible but it not as performant so kind of be aware of those two trade-offs the syntax for this is pretty straightforward called GDS graph that create and you specify a graph named anode projection so the node labels and nerd properties you want to load a relationship projection which are their relationships and relationship properties you want to load and a configuration parameter generally speaking you're just gonna be setting read concurrency you don't have to set this but for some use cases if you have multiple people you may want to set your concurrency what this actually looks like you can it's easiest just to walk through starting with node labels so for nodes you specify there's a shorthand in a longhand form shorthand it's easy to understand it's just GDS graph type create and your graph name your node label your relationship type the long form is you can specify a node label that you want to call it in your analytics graph the underlying node label in neo4j and the properties that you want to map for neo4j so you can load into memory a node label and property is associated with those nodes and right now you can combine multiple nodes to be represented in the same node label in the in-memory graph but you can't load two node labels and distinguish them will just be combined coming in 1.1 though there will be support from multiple node labels in the end memory graph relationships are a little bit more complicated there's a little bit more going on here we talk about support from multiple relationships so now in the GDS you can load multiple relationship types so you can load as many relationship types as you want the reason you might do that is let's say you have your gameofthrones graph you have person interacts with person battles person likes and you could load those three separate relationships once and then when you run your algorithm you could say I want to use my Game of Thrones graph that I've loaded in a memory and I want to use the lokes relationship for page rank and now I want to use the nose relationship for PageRank and see how it differs so it lets you kind of quickly run a bunch of different others on different flexible configurations for this so you load the relationship type the orientation so this is basically direction before aggregation so how you want to combine multiple relationships or if you want to combine them and the properties associated with that so any data like weights that you want to load into memory I'm not gonna go into this any further I will have some links to the docs at the end or if people are interested I have a much longer deck that goes into this so safer graphs native graphs are fast Seifer graphs are flexible they're okay to use if you're kind of in experimentation phase and you're trying to pick a data model and if you don't really care that much about performance and repeatability or if you have a small branch and the syntax for this is GBS graph create cipher and otherwise it's basically the same you have the graph name the node query the relationship query and the configuration and so just in a simple of what that looks like you have I'm creating my graph I'm not channel the people nodes I have a relationship that I want to project and I might have the first person as a source and the second person is the target if I want to change directionality I just do this in the second query so if I want to reverse the direction I might say ID p2 as source and p1 as target or if I want to have it undirected I could just have the query the undirected cool so that was lightning fast drinking from the fire hose overview of the syntax let me check there's a little red light on the QA AHA I see question so I will answer these before I go on so is there a rule of thumb for graph size unavailable memory uh it's all done on heap there's not really an easy rule of thumb there is a long formula that I could share on graph size and available memory on easiest thing to do is to use the dot estimate function so GDS that graph dot create that estimate um the other thing you can do is it's in the docs is you can do estimations for fictitious graphs so if you don't know how big your graph is going you don't have your app loaded yet but you say I want to know how much memory it's gonna take for a billion nodes and five billion relationships you can just pass node count and relationship counts to the graph estimate function to come up with an estimate of the memory for that fictitious graph for dot o-- compatibility not yet right now we run on three five eight two two whatever the most recent thirty-five releases we will have four old compatibility targeting the end of april so we're looking at having a preview release of a for that foe compatible jar probably in the last week of April um GA the first week of May can you work with a subset of the graph you've loaded into memory yes so if you've loaded a multiple relationship types into your memory you can specify when you execute the algorithm which of those relationship types you want to run over as soon as we roll out multiple node support you can do the same thing for the multiple node labels so you can load more into memory than you plan to execute on and then how do we know which ones are most helpful and we would like to see implemented tell us I do read and respond to things on the community forum we also kind of have an ongoing early adopter program with customers who use the graph algorithms library we're keen to get started scale up and we work really closely with them to say ok what is it you need what are you working on what meets your requirements and let's scale this I'll go up for you so I can say right now triangle counting between this and degree centrality are ones that are in the candidate list to be worked out we also have a new Algar in the pipeline for community detection with multiple community labels so a node can belong to more than one community these are based on feedback we've gotten from customers in the community so tell us we're listening we don't have any way of spying on which other ones you call so you have to tell us which ones are you know I will go back to prison so how do I put this all together and actually use this um the main the topic of this webinar was graph L goes at scale I'm going to spend a little bit of time talking about the other ways you can use them and talking about deciding what's the right workflow for what you're trying to do so I talked about already in the presentation I touched on a cipher graph versus a native graph and a named versus an anonymous graph and really for all of these there's a trade-off between flexibility speed and ease of use so I kind of like to talk about this like you have your super flexible sacred rock you have your really performant native graph you have your reusable named graphs and you're really easy to use anonymous graphs so if you're starting off with like I want to teach someone how to use this library in five minutes teaching them to use an anonymous graph using a native projection is this easiest thing to do right that is called GDS PageRank dot right and passing it you know person knows you don't have to think about all of the infrastructure it's easy to get started and get results but be not very expressive when you talk about like starting with proof of concept so I have a have a graph data science problem I want to figure out what's the right data like what's the right data model what am I trying to do I'm just doing quick proof of concepts often we see people moving to the anonymous algorithm calls using the cypher loader so that's where you can flexibly kind of test out different graph shapes and it's not performant but at this step you're just kind of saying well what do I want to do once you get to workflows where you're kind of trying to do something repeatable the when you move over to the names graphs if you don't have type time requirements or if it's something collectible where you're kind of doing one offs we do see people using the cipher projections and the names graph so that's loading a graph into memory using cipher but its name so that you can reuse it and then when you're at kind of your most performant I have a what's our biggest customer I think we have someone with a twenty billion node graph in production right now that they need to finish executing connected components out in under 24 hours with a bunch of other stuff going on they use a native of named graph that's the most performant it's the most reusable that's where you end up when you need performance and that's what I'm going to spend time talking about in the next couple of slides so education and quick demos anonymous graphs native projections it's how you get started without worrying about the syntax when you're talking about a production workflow you probably don't want to use this it's it's quick but it's not robust it's not repeatable and also you lose the benefit of separating out graph loading from the algorithm execution so it can make it a little bit harder to debug what's going on when I've done hands-on trainings if you do the graph load stuff separately from the algorithm execution stuff you can see what you've loaded into memory and we find that about 2/3 of the problems people run into is a mistake with like the relationships in the nodes they're loading and when they load that in first they can see the statistics and it's like oh I loaded no relationships and two billion nodes I know why this didn't work and separating those out makes it easier to see this so only use the anonymous native combination when you're doing kind of quick teaching we talked about using cipher projections for prototyping um with an anonymous graph it's good for just testing out different data models checking the syntax or if you only need to run one algorithm and it's a good with a nîmes graph if you need to run Altaf allow rhythms but you're not quite as sensitive to speed and you can't modify your on your line data model for whatever reason but again this is slow so often when I have someone come to me and say I hate I hate graph algorithms I hate graph data science it is slow I say show me what you're doing and they're using a very complicated cipher projection so when you want speed don't use cipher projections so this is my favorite form like infographic I spent entirely too much time making this how do you use gds in production and so we talked about going from question formulation checking that you have enough memory loading your graph executing your algorithms cleaning everything up and analyzing it and I'm gonna walk through each of these steps with kind of a motivating example and hopefully this will make some of what I've talked about a little bit more hands-on and easier to wrap your head around so let's start off with question formulation so with question formulation we're talking about defining the problem you're trying to answer I've had many people come to me and say I would like to do graph data science and you say well what are you what do you want to do and they don't know you need to have a problem up front because that's going to drive what your data model is what algorithms you're choosing and kind of how you proceed so start with kind of the problem space are you making recommendations finding anomalies once you know what you're trying to answer you can match that with the right set of algorithms so like really commonly we see people want to make recommendations oh I'm gonna use similarity or community detection and I'm looking for anomalies I'm gonna use centrality so start with your problem match it's you an algorithm and then once you've chosen your algorithm um then you need to think about modifying your data model for the algorithms you want to use so most of the graph algorithms expect in mono or bipartite graph so similarity will expect a bipartite or multi-part a graph all the others expect a mono portrait graph so that means one node type and one relationship type on poor performance you may modify your data structure to reflect this to make this easier so you don't have to use a cipher projection every time you may also think about things like okay I need to use different labels or relationships to use an a of graph so I need to do a data split so I'm going to call one set of relationships test in another train you also want to think about using weights or seating so if you're going to be running the same algorithm every day after it let's say you load 10% new data every day and then you rerun your community detection algorithm well then you want to use seating so you can save your results from the first time and only run on the new data or if you're running a lot of the centrality algorithms weights are really important in calculating these things so think about the data you need to have in your data model to run your algorithm so I like to kind of use this example of drug discovery my background is kind of in the life sciences this was several years of my life in a previous job how do you find new target genes for diseases and this is just a toy problem but let's say you have a graph this is from HEC dad island you have genes diseases and chemicals you've got information about the pathways and the cellular function of those genes you have information about side effects and clinical pharmacology for different drugs and for diseases you have information about anatomy and symptoms and this is kind of the graph that you start with and so you start with what is your motivating question for this kind of toy example and they say well I want to find new genes it could cause a disease so these genes should be associated with the disease but there's no edge in my graph right now so starting with that question you can quickly choose some algorithms so I said well nodes similarity could tell me something important so if I find a gene that's really similar to another gene that I know is associated with the same disease maybe that's a new target or I could use something like community detection so find communities of genes interact frequently with each other and if I find a gene that's in the same community as a gene associated with my disease but not yet associated maybe that gene is relevant to my disease and then maybe I want to do data QC so I say it I want to use the centrality algorithms to make sure I'm not choosing really highly connected or really important regulatory genes so if you think about how the body works if you knock out something like ubiquitin ice or a metabolic inside you will monitor the disease you will just kill the patient so you want to say I'm gonna use centrality to filter my results and so now I've said okay I know my question I know my algorithms and now I have to think about reshaping my graph so what kind of graphs do these algorithms expect so for node similarity you're really looking for a bipartite graph so I have my genes and I have the things that they're similar to the other two community detection and centrality expect Mon apartheid graphs now I'm going to go back to the graph that I had and think about how I want to modify the data model and so the simplest thing is I can say well I want to look at my genes and what they're similar to so I want to keep pathway function drug and disease so those are things that could be similar to and here are the relationships that were in the underlying data that I want to consider for mode similarity so genes can be similar to each other based on common diseases common drugs common pathways common functions pretty straightforward and then I want to say well I really want this to be performing I don't want to use a cipher projection um I want to modify it so that I can run this over and over again I'm going to add some mono partite relationships around my genes so that I can run community detection and centrality so I'm going to go in and I'm gonna add you know the same same pathway same function relationship for genes to other genes I can add these genes treat the same disease these genes are same disease based on a common drug where these genes are targeted by the same drug so I want to go in and I want to take that pot of raw data model I started out with where I had everything in the kitchen sink and kind of fine tune it just to what I need for the algorithms that I want to run so step one has been done we figured out our question we figured out our data model we figured out our algorithms now that we know that we need to think about how is my database configured and do I have enough memory to run the algorithms I've chosen get an idea of how long this is gonna take and think about if there's anything I need to change so from memory that's pretty easy you can just run GDS craft create estimate and the star star syntax is if I were to load the entire graph into memory all relationships and all nodes it's not a best practice but that gives you kind of your worst case the MOOCs you could load into memory generally speaking and doesn't fit as well on the slide you would type out specifically the graph that you want to load in the memory stick a dot estimate after that and that will give you lower and upper bounds for heap consumption you can also call estimate on your algorithms so you can say I want to run PageRank that's one of my centrality all those I'm going to say I want stream mood let's estimate how much memory that'll take after you've checked about compare that to what you have if you don't have enough memory you probably need more in one got one we will be introducing some guardrails so if you absolutely don't have enough memory even in the best case um we will throw an error instead of letting you knock over your database but right now you have to check yourself there's no really easy way to check how long something is gonna take but we do know relatively speaking which others are faster in which are slower so nodes similarity is the slowest algorithm in the library because it is a pairwise comparison across the entire graph you can use a named cipher graph to load a subset of the data in bunch mark how long it takes given the concurrency settings that you have or you can use the graph that generate function to generate a random in memory off of the specific size you're gonna run over and run against that to get kind of a ballpark of how long is this going to take if you want to make it faster if you need to change anything um so you have to have enough memory to execute the elbow and this is kind of a hard limitation the upper bound is of guess so if you are in the range and you can't get more memory try to execute it if you are below the lower bound it's probably not going to run and more threads will give you faster algorithm execution both for the graph loading stuff which is roughly linear as well as for the algorithms some skill better than others so node similarity the more threads you throw at it the better where's page rank kind of pops out depending on your graph structure at like 16 to 32 cores is kind of the optimal and then consider pre-processing to break up or paralyze your work so when we said okay nodes similarity is really slow I have a billion node graph and I would like it to finish someday I might run weekly connected components first figure out how many sub graphs I have and then run that analysis on each of those so kind of use this stuff to iterate on what you're doing and see if you can get acceptable performance once you've done that load your graph into memory so this is the graph that create function you're going to load your analytics crush and you can't skip this we just want to minimize the number of graphs we load into memory every graph you load into memory will take more memory so an important question is what's the minimum number of graphs I need to load to answer my question so the way I like to do this is I start with what are the algorithms that I want to run what are the nodes those algorithms are going to execute over and what are the relationships so here I have node similarity I have weakly connected components levain and PageRank and so node similarity runs on multi part right graphs of genes with pathways functions diseases like drugs whereas the amount of apartheid algorithms are just going to run on genes but each one is using a different subset of relationships and as I said before we can tell relationships apart in the in-memory graph so I can load same pathway same function same disease same drug and then I can execute on night and memory graph and say only use the same drug relationship the old nodes will be combined so if I think about this I can kind of sketch out what does this look like so with one graph I would just put all of my nodes together into the node projection and they would all be combined in memory and then in the relationship projection I would have all of the relationships that I had listed out on this previous slide so throw everything in the kitchen sink into the end memory graph uh and then if I have two graphs I'd load one graph just for similarity because I have different set of nodes that I'm looking up so I have gene pathway function disease and drug and then I have the relationship projection so it participates associated downregulated up-regulated to measure just similarity and then I would have my community detection and centrality mono pie pie graphs where I'm loading gene and then these no no per tight relationships so what are the pros and cons of this so if I load everything into one graph I can track different relationship types and selectively use them so I could tell the difference between the same function relationship on the associated relationship but I can't tell the difference between the node types so I'll be an end up running centrality and community detection on a bunch of disease drug pathway function nodes as well and since these aren't connected or relevant they're different node type from my mono car tight relationship that will throw off my results and similar it is slow so it's often best to try to break up your workload which website if I use two decks I'm loading the right data for each class of algorithms so it's less memory and I'm not running over nodes I don't need to go look at and I can write the results back from community detection or centrality and then use that influence running similarity so the idea here is if I use two graphs I'm using a little less memory I'm still able to reuse that mono car type graph for all of my centrality and community detection algorithms but I'm only running on the right number of nodes so for this use case you'd say well really two graphs are the best choice for this so what this looks like is you'd say looking at the morning part I graphed for sensuality and community detection you'd say call GPS great and I call it mono part I promise I'm being super creative and I'll load in my jean and then the relationship types I'm interested in and this is actually the syntax for the library I'm using a shorthand somewhat less expressive version so that it fits on the slide then for my similarly graph I'd be loading in my Jean and all the things that I'm measuring the similarity to and then I'm gonna what I'm able to do is I can kind of flexibly reshape my relationships a little bit so I mentioned before we have aggregation and orientation so let's say I want to use participates as a relationship type I want to use a single aggregation so I only use the first relationship what I see and but I can take disease and I can combine two relationships into one to relate or three relationships in my transactional graph or in my memory graph into a single one that I'm seeing for the others and I can specify the orientation so I'm saying reverse etc so you can do a lot of flexible reshaping even with the native back order and so that's it for this one so we've got our graph in memory time to start executing algorithms so there's three things you can do you can do stats stream and right stats returns summary statistics about the results without writing the data to the database so this is usually the one that I recommend starting off with so it helps you see if your calculations make sense so let's say you're running PageRank if you run dot stops and you see that your all your percentiles are zero something is wrong if you're running community detection and you find that every node is in its own community you want to change how you're calling the algorithm but stream will return all the results as a stream this is usually useful if you're using the results elsewhere so like in Python or for some kind of machine learning pipeline and then got right will write your results to the neo4j database this is the slowest of the three modes so try to only run run right once you know that your algorithm makes sense so this is the thing I want to store in my database then you honor it I'll see a question I will talk about this later and so back to running your algorithms centrality and community detection so this is what I talked about running on the named graph so I'd say called GDS PageRank right and my named graph is the mono partite graph and someone asked before can i specify which relationships and that in-memory graph do I want to use I can call it I can specify this right in the algorithm call so I'm saying run it on my name's not apartheid graph but only run it over the same pathway relationship site and I want to write it to the pathway PageRank and then I could run this again and run it over in the same disease relationship and write that as a different property um I can do the same for looping I can run mono apartheid crawl on the same pathway relationship and write that to a community so then when I am loading so yeah I run it over different relationships to see how it affects the results and you can just name different properties if you want to compare these later on if we remember back to the original big picture we had right and reload so there's a double arrow so it's not just graph create right sometimes it's you right back to the database you load a new graph in the memory so community detection is fast similarity is slow we don't wanna run similarity on the whole graph we want to save calculations so maybe I'm actually looking for genes associated with multiple sclerosis my question isn't that open-ended so I can say I want to find all the genes and associated with multiple sclerosis I want to see what community they're in and I'm going to label that community multiple sclerosis genes and then I'm only going to run on that one labeled community so what I'll do then is I'll say create my graph using my now I've written back to the database and I've labeled them ms genes and I pack wave function disease and drug I've got all my syntax to loaded in memory once I've got that I can write my I can run nude similarity and write my mom apartheid graph result back to those genes of interest and then I've run this much more efficiently on Geo sub sub that's relevant to me and once I finished up with that the final step is cleaning up don't forget this I forgotten this and wondered what was wrong with neo4j GDS that graph that removed I need to update the slide and drop the graphs that you've already loaded in the memory you need to double check if you're like me and sometimes you call graphs test1 and test2 and test3 you can use the graph that list feature to see all of them once you've cleaned up after yourself when you can data science so this is the final step we talked about where maybe you're doing manual review and that analytics use case we talked about handing off to an analyst who's gonna say look at everything in a community by hand maybe it's a scientific expert and they'll manually choose what's interesting you may be transitioning this into some kind of visualization so we have a new sandbox up for bloom and graph data science where we give examples how do you visualize the results of the algorithms and what do you do with them or are you using those for some kind of model building exercise where I'm now gonna pull these properties out and we put them into some kind of machine learning pipeline as new features so kind of once you've finished this workflow of I figured out my data I figured out my question I've reshaped my graph uploaded in a memory bring my algorithms cleaned up after myself then you get to do your fun data science work and so that is that there was a lot to cram into fifty-five minutes with time for questions but if you have any questions or you need resources I've got links probably most relevant here if you want the GDS it is available on the download center right now one dot o dot you can also in neo4j browser type play graph data science to get a nice walk through that has examples of the syntax and of all of kind of different ways kind of new Syst algorithms we have to sandboxes up one looks basically an extension of the browser guide using graph data science library and then one using graph data science with bloom the documentation is up it's live and our code is open source um you can go to github slash neo4j slash to graph data science if you run into problems you have bug reports you have questions our github is actually the best place to put those issues we check that and automatically those to our team's Trello wall we prioritize stuff from there so check it out with that I will turn to less we do have some some questions but I want to give you give people the option to be able to just turn on their microphones and just have a conversation if either you guys Nathan or Lavanya if you guys want I know you guys have questions and if anybody else here you want to turn your mics on or I guess we could just walk you the questions but if anybody does you could just literally turn on your microphone and we can have a conversation you could ask questions that way so do you want to answer Nathan's question first it's shorter yeah so separate instance for GDS separate neo4j for GDS versus production do you mean like you have a transactional workload happening on a causal cluster and then you want to run on data science instance dedicated for analytics or are you thinking like I have a prototyping machine and I have my production by well he's a bong he's writing it out why don't we um go to the next question I guess I think I mention it's a question Lavanya raised using a different term instead of manipur type graphs it is a monoprotic graph you need to create the edges to create it so you use a mono apartheid projection I don't think we want to join new terminal but it's a it's an interesting way to think about it instead of calling it mono partite calling it an induced graph but all of our documentation is no no party graph data science comes out of graph theory so we try to stick to the academic terms maybe Nathan fell asleep sorry I heard in the chat window okay I need to bounce back again yeah we can start asking the questions in the chat window just to make it easy okay so if the genius loads are expensive how do you schedule them so the genius loads are expensive the most expensive stuff is actually usually right back and what we've seen in production workloads is basically they add new data load the graph into memory execute right and then remove the graph repeat and the way that they're handling kind of new data coming in is they're using seating for all of the functions that they call so you run it once let's say I run weekly connected components and my data I save those components and then the next time I run my workload I use the seeds I load those into memory and then I'm only identifying communities on the new data or if they've changed on the old data and then I only have to Ray pop on those new notes and and it doesn't save you on the load but it saves on the calculation on the right box stuff and because right back is actually the slowest that's a big performance improvement so that's how you can do you know the tons of billions of nodes workloads in kind of a reasonable time frame but in general when we talk about GDS most of this is intended for a single user and so don't try to load many large graphs into memory from multiple users at the same time we kind of think about it like a sequential operation and the important thing to remember is when I talk to we're reshaping the data into the in-memory graph they're not synced so if you change your underlying transactional data um then it's not synced in the end memory graph so we have the snapshot that we load with graph that load we execute on that slap shot and then the right back just goes back to those note IDs in the underlying graph so you have to reload when data changes and so you kind of have to do it sequentially cool cool does anybody have any other questions if you need time to write it out you can put the little hand raised icon next to the emoji and then I'll give you like a moment you will know that you have a question coming Alicia that was very thorough very good present it's a lot of information I'm sorry no almost like oh never seen this library before so not to explain yeah well these are definitely the people that we want to know and understand it so we want everyone to know and understand it you know but our ninja that they're active and always helping people on staff now to its that's important for the ninjas community keep your eyes out for the preview releases there's happened two weeks before we GA so we should have a preview with multiple nodes support mutable in-memory graph and for memory requirements built in coming out March 26th is like next week so test it out give us feedback we time to do some updates between preview and Gi so look at those things so people said they really enjoyed it Thomas said I want to find some more memory and storage or just a small data set all right guys thank you very much for coming thank you Alicia so much oh wait you know when there's a question in QA that I think is yeah another one Lavanya just posted a question oh cool I'm get back up that later um okay I think it's not a question more of commentary yeah that's fine all right well thank you very much thank you Alicia I really really appreciate it and mr. lovely presentation thank you guys eat later bye everyone let me stop recording 