 you [Music] okay well we'll get started it's an honor to have Tom Hope here from Hebrew University of Jerusalem tom is working with multi time multi turn intern for Microsoft Research in close colleague Daphna half and so we view them as a kind of a grandchild of sorts Nick's got her scholarship and he's been working on a topic that I said talked to extra brainstorm with Daphna about on analogy and creativity it's great to see her falling to this work and you of course taking the leadership role on it on boosting innovation and creativity with methods that come at the intersection of natural language processing supervised learning and crowdsourcing and Tom is early in his career yet he's already received a number of best paper Awards including kdd best research paper and a number of best student paper awards and he's his work has already been appearing in multiple venues including the Proceedings of the National Academy of Sciences wisdom h KY c SE w and so on he's beyond doing his PhD work he's been leading an applied research team at Intel they'll talk later about that and hey you balance those two things and he before doing his PhD work he did master's in statistics he's been involved in in the heidelburgh laureate forum and even as his time to write a book for O'Reilly on tensorflow for all you folks to want to do tensor floor for dummies I guess or a nice introduction attention flow okay Tom okay so thank you again for inviting me and today i'll speak about automating innovation and discovery with machine learning which is the story of my PhD so far with dr. Sadoff now how many of you know this from Matz might great so let's start by imagining a circle that contains all of human knowledge and by the time you finish elementary school your learn a little in high school you'll learn some more and in your bachelor's degree you then gain a specialization and then your master's degree you deepen the specialization and then you read some research papers you read and you read and you get to that boundary of humans knowledge that's what you start focusing and you focus for a few years and you push it this boundary and you push and you push until finally one day this boundary gives way and that dint you create is called the PhD and that's me hopefully celebrating now getting a PhD is great but we have a couple of problems first as our knowledge keeps growing and growing and growing it's getting exceedingly hard for any one individual person to reach that edge and secondly a lot of important inventions across history involve connecting the dots between two different fields and not just reaching the edge of one of them for example the ancient Greeks more than 2,000 years ago studied sound waves by analogy to water waves and there are many examples of course the Wright brothers for example taking ideas from bicycles and transferring them to flight we have NASA recently being inspired by origami to design parts of their spacecraft and the problem here is that in order to connect between these fields you first have to know of their existence in the first place right and then you have to have quite deep understanding in each of them in order to make proper connections and this again is of course pretty hard to do and not really scalable for any one person across many ideas and many fields now on the other hand that's never been actually easier for us to get our hands on ideas so we have millions of patents and scientific publications crowd-sourced innovation sites and many many more sources all of which giving us an opportunity to accelerate I'm solving innovation and discovery by giving people inspirations and identify promising directions but there's also a big challenge these datasets are hard for people to work with and the question is can we help automate discovery and innovation on top of these idea repositories to help boost innovation so in this talk I'll speak about two directions we are taking the first is boosting creativity with analogies and the second is identifying novel ideas with the weak supervision framework that we developed so let's start with analogies a lot of important inventions across history involved analogies we spoke about the Wright brothers and the ancient Greeks and NASA here are a few more important examples for example Thomas Edison said of an that an important quality of inventors is a logical mind that sees analogies and use analogies explicitly in his own innovation process there's an analogy between slot machines two bacteria mutation that won a Nobel Prize and the closer perhaps to our own field we have genetic algorithms and simulated annealing all kinds of optimization approaches inspired by nature and of course our many many more examples a different kind of example a recent one is of a car mechanic who came up with a device to extract babies stuck inside the birth canal to ease difficult childbirth and the World Health Organization actually recently said that it's one of the most important inventions in this area in many many years but the really cool thing about this invention for us is that it came by an analogy analogy actually to some random party trick the mechanic saw online on YouTube showing how to extract a cork stuck inside a bottle and by analogy came up with a way to use this kind of mechanism to ease childbirth so we'd like to take this kind of process of discovery by analogy and help automate it to these large idea repositories so that we can boost creativity the problem with automation is that analogies are hard for machines so NLP information retrieval methods as I'm sure we know in this room do especially well at surface similarity especially when we have similar source and target domains for example you could have this scraper that scrapes ice of your car and then you could have burnt oil stuck onto a kitchen pan and in both cases we see this abstract case of removing a material stuck onto a surface but described in completely different distributions of language and we'd like to be able to find these connections now what was done in the past and the early days of computational analogy was creating these kind of pretty crazy predicate logic kind of structures for example this is what the structure may look like for that --cork extraction device and as we can immediately see this is pretty much infeasible wouldn't scale it could take a huge amount of hours to extract for real products so our first kind of quick attempt at this was let's try some supervised learning to directly learn analogies and we went to a crowd-sourced innovation cycle quirky which has many different categories which which is good because we want to learn analogies across domains and they are written by ordinary people and the kind of messy variant of natural language as we you can example see in this execs tear and we want to collect some labeled examples so we built a search interface on top of quirky allowing crowd workers to get a seed product and then search for analogies and when they find one they give us a match this way we collect our positive examples we also collect negative examples as in standard information retrieval or viewing the top results a user viewed but did not select as a match and we also collect the query so in this first worker Val's we learnt a similarity metric reflecting analogy I won't go into too many details about this one but we trained a CNN based network with the contrast of loss so that we can rank analogous products more closely than non analogous ones and importantly we incorporated in the years of query because the user query is what their user inputted to search for analogies and we use it use this as a kind of an anchor to help us focus on non surface features the more structurally related ones at least according to the user to cancel out the surface effects and find more abstract relations so the good news about this work was that it beats standard retrieval baselines and the query indeed helped us improve results but the not-so-good news was that labels were costly and noisy and slow to obtain it's a hard task for people people tend to be good at finding superficial matches like phones and different kinds of phones cognitive psychology study this effect for many years and because it's a slow and hard process our data size is limited and importantly our model is left blind to the kind of rich structures that go into analogies and these abstract structures and it's not really realistic to expect this model to learn it from raw text with the kind of data scarcity and noise that we have so now second work in kdd we aim to go for our structural representation so we want to find something that's expressive enough analogy mining but also simple enough to be learned so somewhere in between that raw text and that very elaborate crazy predicate logic kind of structure and we end up focusing on the purpose and mechanism concepts so these are deeply rooted in cognitive psychology that tells us that analogies are inherently related to the purposes and mechanisms of ideas and in terms of purpose we can simply think about what a product does so for example the scraper removes ice from your car and the mechanism how it works in this case is with the brush a blade and a grip now it's a suit for a moment we have representations capturing purposes and mechanisms we can do some core tasks like finding two products that share similar purposes but have diff mechanisms or do what's known in science and engineering as repurposing finding two products that share similar mechanisms but have very different purposes for example that car mechanic coming up with a completely new way new purpose for that extraction device so our goal here is going to learn bro you be to learn victory presentations capturing the purpose and mechanism of inventions so that we can use them to find analogies and boost innovation now we go back to Mechanical Turk and we collect data this time collecting annotations of product texts with respect to purposes and mechanisms and our goal here is in this work is to learn soft vector representations capturing the overall aggregate kind of notion of the purpose and mechanism of an invention and we do this simply by aggregating across our workers so we learned tf-idf weights for purposes and mechanisms across our corpus having two sets of kind of importance relevance weights for each concepts then we take our pre trained word vectors for each given products we weight them according to purpose and mechanism across workers and then we get to output target vectors a purpose target vector and the mechanism target vector and we want to train a regression multi-task model to extract these two target vectors for new products I'll quickly know the technical note that because we have a normalization layer somewhere you're normalizing our purpose and mechanism vectors so by minimizing the MSE we are actually minimizing the cosine distance so we're finding things in the general overall direction of the purpose and mechanism thereby hopefully finding some more abstraction and we see that our approach is able to overall overall outperform standard retrieval by quite a large margin in most cases and this is despite having noisy annotations and importantly because on our test set people tend to focus on the more surface based analogies these surface analogies obviously favor the surface and B approaches and we are able to both recover the superficial matches but also the more distant ones with our approach now as a quick reality check we also want to see what we were able to interpret here so we take a kind of sparse coding approaches so you often see for understanding word vectors and we model our target vectors as a kind of sparse linear combination of words in our vocabulary and the bottom line is that we over will see that the mechanism words are indeed of a more mechanical nature and then the purpose words so our main evaluation and application goes back to our motivation of boosting creativity and we do this in an ideation setting which is a common creative task you often see in psychological studies and also in design firms we give people a problem for example a cell phone charger case which is a case that charges your phone and we ask people to find new ways to solve the same problem and we give people inspirations so this is heavily based on ideation protocol now and similar experiments so we give people random inspirations we give people surface-based which is what you get from standard search for example and with our approach we aim to find near purpose far mechanism analogies because we're looking to find new ways to solve the same problem and our assumption of hope is that our approach will help explore the design space in a more diverse fashion so sharing the diversity of the random approach but also the relevance of standard search so what kind inspirations do we give people well for example random is of course all over the place for example one of the results we show people was a dog meetup app which I'm sure you all have installed on your phones as we speak and we have standard search surface based very relevant more cases more batteries and phones not very diverse and in our approach we are able to first see that we give things ready to charging but not necessarily for phones and indeed one of the top results we were able to retrieve is the human fully powered generator suits now this is kind of a silly invention but it generates power using your movement and the important thing here is that it doesn't mention phones at all and in an abstract way is related to the problem of charging and creating power just with the completely different mechanism which is what we were aiming for so when people for example viewed this inspiration a bunch of them came up with ideas for generating power for your phone using your kinetic energy generated while you're walking so to make this more concrete we give all these hundreds of idea generated by workers to a panel of external judges and we have a rate according to standard ideation protocols such as novelty and quality and feasibility for example you could come up with this time-travel machine idea and it could be great with respect to novelty and quality but feasibility not so much so we first observed substantial judge agreement with respect to these criteria across our judges and looking at our results both relative and absolute terms we're able to generate a considerable large positive effect generating better ideas with our analogy based inspirations so the takeaway here is that our approach yields a substantial increase in people's creative ability using analogy based inspirations a quick note about another application we carried out is boosting scientific ideation so here we want to find analogies between scientific papers and we slightly refine our structure so that now we also include the background and finding annotations which you often can of course see in scientific papers and we work at the biochemical biomechanical engineering research group helping them find inspirations and novel applications now this group had quite bad experience with standard search like standard academic search finding near direct replication of the labs work whereas with the our analogy based approach we were able to find results judged as both relevant and new out of the main alternative approaches that were never Lee never actually seen before by the lab and judged as potentially useful so we saw models taking in the raw text and then we added structure the purpose and mechanism structure and we slightly refined that for scientific papers and now you want to take another step and we're going to move from soft vectors to fine-grained functional models so we're still going to Mechanical Turk and we're collecting annotations of purposes and mechanisms but we refine our structure in all kinds of ways discuss in our recent paper which is under review but now importantly our goal is to extract spans of text corresponding to purposes of mechanisms so we want to find multiple fine grained purposes and mechanisms and that's importantly for us enables all kinds of new applications which we'll discuss in the next slides now another step you want to take is to go to pattern data which is a really kind of important large-scale source of engineering innovation it's kind of an index of Technology and importantly it's much larger than quirky so quirky is challenging in terms of language and noise but we want to be able to discover many many more purposes and mechanisms so that we can find rich analogies and the the problem with patterns is that as I'm sure a lot of you know it's a really messy kind of creature so a common guideline for patent examiner's and writers says that the subject matter of the invention should be described and one or more clear concise sentences of paragraphs well not quite the case so here for examples one patent sentence forget about being forget about being concise and about clarity so patents often use this kind of obfuscated language actually often done on purpose by lawyers who write these things and take a look at this pattern dating back to 2001 speaks about a circular transportation facilitation device and goes on and on about this transportation facilitation device until you reach the pattern figures and then you understand that what actually is going on is well reinventing the wheel yeah this is a true story it was an accepted patent it later won the ignoble prize and only then was silently removed by the Patent Office so this probably goes to show you quite a lot about patents so annotating purposes and mechanisms on quirky was hard so patents is gonna be harder really our task for the kinds of reasons I mentioned and also patents offering tend to kind of skirt around the purpose of the inventor and often focusing on technicalities so we try to ease this for ordinary people with sampling from domains that are easier for crowds to understand there's a few examples of them of course is on relative terms we still get noisy annotations and often partial tagging so workers often skip sentences presumably because they're hard for them to read so we realized that looking both at our quirky properties and our patent properties we could make use with a graph representation this graph will do two things it'll capture that more local syntactic relations well known to exist for purposes and mechanisms and we do this with a dependency parse tree or presentation but we also want to capture semantic information propagating semantic information across long spans of the text which we hope will be particularly useful for patents with this kind of repetitious language across long texts and we do do do this with an embedding based similarity and to make this more concrete our graph is a contextualized graph convolutional network so we take as input multi-channel input embedding such as pre-trained row vectors but also other pre trains part of speech information named entity recognition and dependency parse information we feed all these embeddings and Inter by list en Network to capture sequential context and then the outputs emitted by our Barrett by Alice theorem are fed into that graph structure with a relational graph convolutional network so we have multiple types of relations as in our graph and we want to prop Gate information across this graph and we do this with multiple layers of GCN and finally each node in this graph corresponds to tokens in our original text and these token representations are fed into our CRF to capture tag dependencies now as a baseline we first take a buy less DMC RF we enrich it with all kinds of syntactic and semantic information the same kind that we use and in an ablation study we also study the contribution of the different kinds of edges we use in our graph and the bottom line in terms of accuracy is that our graph model was able to significantly prove results and indeed for patents using the long grained semantic edges was able to boost our results further now overall accuracy is quite low we have our noisy training annotations and in noisy languages it's a really hard task and our model predictions are often better than what we get from workers so here's one example of a product annotated by us and by the workers that zoom in quickly so the crowd told us for example that the mechanism is a travel mug but we are able to first extend this mechanism to a dual heat to travel mug and also add a purpose completely missed by the crowd your coffee being warm on the go so importantly we're doing annotation in a noisy setting with untrained non expert workers on noisy complex texts with noisy annotations so this is pretty far from what you see in standard NLP we're taking a scalable low-cost approach and to evaluate though we want data which we can trust much more so we go to a gold standard test set that we curate and importantly one final note is that we mentioned the workers have a hard time reading sentences and they skip quite a lot of them so we add self-training a semi-supervised approach to propagate our labeled information to training sentences that were not labeled erroneously and we see this significantly proves results especially for patents we were able to boo to recall which is what you'd expect now we evaluated in terms of accuracy importantly for us you want to evaluate in terms of usefulness of the model and unlike those soft aggregate vectors which we saw before now we have finer grained vectors which support our applications which is what I'll discuss next so our first application is building a common sense functional ontology so we're interested in mapping the landscape of ideas with a hierarchy of purposes and mechanisms and these often used in engineering functional ontology but they are handcrafted and these functional apologies are useful first in terms of abstraction so this can allow problem solvers to break out of fixation to one of those fields right at the edge there and connect the dots to more fields so that they can better innovate and also reasoning so understanding the interplay between purposes and mechanisms will potentially enable us to ask and answer complex questions on top of this graph and your for example you can see a snapshot of some graph of what we were able to learn so you have here this purpose node which is your generating power and energy efficiency red corresponds to the purpose green to mechanism we have a sub purpose of generating power which is charging your battery and or devices in general we have a sub mechanism of generating power which is solar power and mechanical stored energy and sustainable sources yet another sub purpose of solar panels for solar power and storage batteries for stored energy etc etc now to learn this ontology or any ontology we first need a discrete representation of concepts this we do by clustering our purposes and mechanisms for example that's how we got our solar power cluster and we also need hierarchical relations and this we do with rule mining we observe the core occurrences in our data between those purposes and mechanisms for example we can find out that of protecting your head entail safety but not necessarily the other way around and we compare as a baseline to an approach based on part of speech tags for example telling us that verbs are typically more related to purposes and noun phrase is typically more related to mechanisms and we see first a large improvement over this baseline but we also see good performance now closer and absolute terms so we can't see those figures right there but for example for the purpose purpose relations in our ontology we're able to cross eighty percent accuracy and importantly we give this to judges who are research scientists and engineers and we see substantial judge agreement with respect to the relations we are able to extract second application is building expressive search engines to help boost innovation so say for example you are a light bulb manufacturer you want to expand to new markets so you know other obvious stuff you know about lights for the office or flashlights you want to find new places where lights can be used but light is not the main purpose so it's pretty hard to do this with standard search you need a more expressive language so we enable to ask for example finally product ideas whose mechanism includes light but purpose must not include light and then for example you get warning signs on foods warning about food content using lights or sanitization with light so killing bacteria with UV lights these are two products we were able to retrieve technically we work on top of a set of purposes and mechanisms and query terms and we define distance metrics on top of them to solve retrieval problems and to evaluate we first construct for search scenarios common in ideation these kind of templates for example using light for the purpose of cleaning and we compare against standard search and also our own previous soft aggregates purpose and mechanism vectors which we saw before to see how much finer granularity is able to help us and we have our distance metrics which I won't go into right now and importantly we're able to that our finer grain purposes and mechanisms are able to substantially boost results with respect to other approaches so to summarize this part of the talk we spoke about raw texts fed into a machine learning model to find analogies we then added structure the purpose and mechanism structure we then slightly refined it for scientific papers and then we went on to finer grained representation it's importantly has applications in terms of ideation boosting creativity also in science purpose and mechanism common sense functional ontologies and building expressive search for inspiration queries and importantly this is work done with the help of my colleagues at the Hebrew University and also at Carnegie Mellon University ok so we spoke about analogies let's take a deep breath now we're going to move on to the second part of the talk which is identifying novel ideas with a weak Supervision framework we developed so let me start with our high-level motivation we want to discover novel patents those patterns at the edge and we're working on this right now the problem with identifying novel patents is that getting labels for patent novelty something really hard so that a novelty is kind of a hard concept to understand in the first place and you could take a patent and all its prior art and show it to a domain expert and it would still take a huge amount of time and effort to be able to rate this patent according to how novel it is now you could ask why not just use citations well citations are well-known to be a very weak proxy for innovation this is drastically true in patents but also in science and another problem with citations is well the community often doesn't realize that through innovative potential of an idea or it could take a long time for it to do so thereby greatly biasing and delaying this metric but the good thing that we do like about weak proxy is that they are easy for us to obtain so in this case we focus on aggregate patent portfolio information so we could go to economic research telling us that the average proportion of truly novel patents is really low and we simply know for example that some academic Institute's for example are more innovative than others and we'd like to go from this rough aggregate information on groups to inferring on individual patterns for example telling us that a patent about autonomous vehicles is well more innovative than reinventing the wheel obtaining labels of course is generally difficult it's not only in this task so you often have simply no access to ground truth information so you could take user behavior on some app and ask an expert to rate this user according to who the user voted for or what are the users underlying health conditions that won't happen because that information is just not there and not it's not accessible you often have privacy issues with showing data to experts or the crowd and in general crowdsourcing is simply not a cure-all so workers are not domain experts and this process can be difficult and noisy and a costly process as I'm sure we're all aware of but getting rough aggregate information is often much easier for us to obtain and we want to work with this rough aggregate information and we developed for this a new framework we call ball ball park learning and the name ball park comes from an educated guess or estimation falling within acceptable balance comes from an analogy to baseball and we love analogies as we said so that's the name we chose and the general framework for ball park learning is taking training instances X with unknown labels Y and our instances are divided into bags for example patent portfolios comprised of individual patterns as instances and we have constraints on the unknown label averages in these bags for example telling us about the average proportion of novelty up lower bounded by certain constraints for a certain institution we can also have difference balance bounding the differences of label averages and bags according to various encoding x' and our goal here is to learn to predict instance level labels from rough constraints on label averages given four groups okay so let's start with our formulation in terms of classification we have a discrete binary label space this can also be extended to the multi class setting now we solve the following by convex optimization problem I'll just going to a few details here our objective term contains a hinge loss objective function contains ins loss term and also slack variables so we could have labeled examples in some cases a few labeled examples and we have proportion constraints upper and lower bounds on proportion constraints and bags and the intuition here is that the objective helps find an assignment to the latent label of vectors Y such that it accurately matches model predictions but we also have linear constraints to ensure correct assignment to that hidden vector of labels Y to ensure that they fall within our ballpark feasible set and we solve this by convex problem with a chord in the descent alternating approach which converges quite quickly in practice now we also have a formulation for regression with continuous targets now this problem is convex and Factory in fact quadratic with respect to the label Y which has all kinds of advantages and we also have a formulation in terms of a feasibility problem so now we're going to constrain our model predictions themselves to fall within our ballpark constraints this has the advantage of having much less parameters we don't need to estimate that hidden vector of labels we also have a probably approximately correct formulation with a sample complexity bound that goes with that and more details and discussions appear in our wisdom paper now a final note is about hyper parameter optimization so how do we find hyper parameter we don't have any labels so we develop a scheme we call constraint violation cross validation so what this means is we split our bags our training bags into training and held out subsets and then because we sample these bags uniformly we're going to assume the proportions remain approximately unchanged in the training and held out bags and then we'll measure constraint violation on these held out bags construing how far off our models predictions were in terms of the aggregates proportional predictions from the ballpark constraints and then we simply pick the hyper parameter with a minimal average violation now our end game is patents but first we want to evaluate and lots of benchmarks data set so we have labeled examples so we predict across lots of classification and regression tasks here is a small sample for example we predict recidivism return to jail according to groups of inmates such as based on a prior record we predict prices of apartments on Airbnb according to neighborhoods and other simple attributes and we predict in a socio-economic setting and to get a bit of feeling for this let me just get a bit deeper into one of these examples so say you want to predict income as being either high or low in the well-known census data set but now we're going to emulate the example we don't have any labels at all but we do have bags based on education and gender so we can build these bags and constraints based on expert knowledge and prior knowledge for example coming from surveys or for previous census data or from sampling or even intuition telling us for example that the proportion of high and income PhDs is higher than in the overall population also my adviser Daphna tells me I hope that's true and we can build all kinds of other constraints and importantly here the bags we build are not necessarily used as features okay so the model at test time can predict the label in this case income without knowing the education level of a new incoming in the test instance case it's kind of information used only during training now to evaluate we first see that we need quite a lot of labels in standard supervised learning such as SVM to match what we can get with no labels at all and we also compared to a setting in machine learning called learning from label proportions published for example in JM LR where exactly the proportions are assumed to be known and this is a much stronger assumption than we take we actually extend and generalize this work in all kinds of ways and we see that were able to rival all kinds of learning for label proportion approaches by making use of much less and much weaker information now that was a case so we had expert constraints but where else could we get constraints well one case would be crowdsourcing so we pull noisy crowd guesses on intuitive groups so we ask people to predict return to jail that likely to return to jail on average according to simple groups or guess ranges average ranges for prices of apartments according to simple attributes such as apartments having TV available on Airbnb and the bottom line is that our results rival supervised models that use many true labels and when we feed our individual labels collected from the crowd by showing them as in standard practice here's an instance here's an apartment yeah all its attributes much more information than what we have we actually get much poorer results the crowd generated labels when fed into our supervised models generate really noisy results and in addition we do this we do our weak supervision approach at a cost that is much less than that standard labeling approach because our questions are able to cover many groups at once with the kind of weak signal so when would you use this approach well when you have no labels only rough information on groups or when crowdsourcing for individual ables is difficult so we need much less domain expertise because we rely on this human intuition on groups and comparisons which is well studied in psychology we ask much fewer questions thereby cutting the costs and we have virtually no privacy concerns because we really never show individual instances to workers it's can also help with all kinds of other difficulties and characteristics you'll see in these kind of tasks such as continues targets and crowd biases and outliers which we discuss in our wisdom paper and importantly this approach can help complement standard crowdsourcing when these scenarios and difficulties come up okay so towards the end of this part our kind of end goal is using our ballpark approach for patterns and as I said this is something we're working on right now so where do we get our aggregate constraints well first we go to economic research telling us something about the proportion of novelty according to various scenarios and categorizations of patents we also have all kinds of institution rankings companies and academic institutions according to our innovative layer that gives us constraints and bags we also need features so we connect back to our previous work on ballpark layer on analogies so we can connect to work on recombination telling us that innovation often involves connecting different mechanisms or using mechanisms for far untypical purposes purposes which you wouldn't expect to see usually now one final challenge and of course we enrich our features of all kinds or other patterns citation networks and textual representations but a challenge here is that we have a small number of truly innovative patterns and we find that we need to gain a finer level of control on the distribution so when we want to compare two companies for example we don't want to compare their averages we only compare only the top of their distribution because that's where the novelty is and we'll do this with the some of the top k ranked results which is a convex function but unfortunately this puts us in a violation of convex programming but luckily we have dedicated specialized optimization approaches for this scenario and as I said work in progress but our initial eyeballing of results is able to reveal higher recombination all kinds of other novelty indicators for the patterns we are able to retrieve better than anomaly detection for example and we've started working with patent examiner's to get better more concrete evaluation okay so in the next few minutes I have left I'd like to quickly discuss some future work and also my own broader interest in taking this forward after completing this so in terms of analogies we spoke about richer NLP models and representations I'm interested in extracting graph graphs of purposes and mechanisms that describe the interplay of purposes and mechanisms in individual products and patterns and scientific papers and injecting external common-sense knowledge graphs to understand these documents better and perhaps help us with better abstraction we've started working on a live search engine for researchers and engineers and the general public to be able to find new ideas and then ballpark learning we spoke about a non-convex extension for patents which we're working on and we've also started working on learning deep neural networks from rough aggregate constraints on groups rather than collecting many many individual instances as we would usually do now in terms of my broader interest in taking this forward so I'm interested in developing new models for complex texts and behavioral data in the domains of scientific knowledge discovery health social science and psychology now obviously that's quite a lot I'll just give a few examples in terms of modeling complex texts well we spoke about scientific papers and patterns obviously there are plenty more examples like conversations evolving over time and medical documents and transcripts many more examples and graph neural networks which say which is a promising die and NLP right now could allow us to propagate fine-grained information across long spans of the text and also potentially give us different levels of resolution about what the document is discussing now one kind of area I'm interested in is can we do a multimodal fusion of search behavior with scientific paper content and an example question I'm interested in asking is how do we search for new ideas so sometimes I kind of call this research or gradient descent so you know when you have a new idea and you go to an academic search engine like the Microsoft academic search engine and you want to find if your idea is out there now we can take all these searches from researchers and see if we can model this process this kind of search for ideas looking for interesting gradients in the space of ideas can we find directions that we should look at in this complex space of ideas can we model this process of how we as researchers look for new ideas discover these directions predict how they evolve and then use that to boost our own discovery process and also in general search beyond the scientific domain so people always have ideas you have ideas for products for startups general ideas and you would often go to standard search engines to look for ideas so can we identify those sessions those search sessions in which people are looking for new ideas and then do that same kind of process of helping them discover in your ideas and modeling how people search for new ideas and graphs could be a useful representation here by matching search behavior with fine grained information and content and propagated in a learnable fashion now another useful component where graphs could help us is by incorporating external world knowledge graphs and information into NLP models for example medical knowledge graphs or EHR electronic health record data fusing that into our NLP models to gain better understanding of the documents now we spoke about boosting innovation and discovery and one more way to do that what by observing science in action so taking a look at the trajectories of science this dynamic a true genius continuously evolving graph of scientific knowledge for example the Microsoft academic graph and asking for example can we predict the path of experts so what will your next papers be about can I predict that in some cases it could be quite hard in some cases may be less so and this has applications both for understanding science and how it evolved but also potentially help boosting it so think for example that you could emulate great researchers and how their methodology of searching in that space of ideas this gradient descent in the space of ideas and then helping boost our discovery process in general can we identify structural holes in this rich graph of scientific knowledge these areas in space where it's interesting to look at maybe connecting the dots between fields and not just any random connection so you could come up with any crazy idea like I don't know a bicycle for Elephants so yeah so it turns out some sort of that one already and secondly who cares right we want the high value research potential areas and finally this is kind of even more ambitious so can we learn generative models of ideas by observing this graph sometimes I call this the algebra of ideas so learning how ideas compose together or decomposing ideas and into finer grained ideas and can we amplify certain aspects of ideas so imagine you have this knob which you could turn up a certain aspect of an idea where does that throw you're in the there in the space of ideas and then can we sample in a controlled fashion from this generative model to point us in to interesting interesting directions in space finally last part is we like analogies and we like abstractions so let's take this to an abstraction of innovation and think about novel social phenomena a novel health-related phenomena and new concepts and new misconceptions so for example can we take the anti-vaccination movement and under standards with NLP from online discussions discovered and predicted and model and maybe even discover analogies to other movements can we predict how people make decisions and in particular collective decisions can we predict collective behavior and collective decision-making from group conversations identifying group dynamics and latent individual roles that are studied in psychology and group psychology and a bit more generally discovering new health and psychological behaviors and symptoms and causes where a useful component here will be to learn a broad knowledge graph of health-related activities and psychological states mentioned in context and context of what people do online and how they speak about things discovering fine grained pieces of information how they all interrelate regard regarding health and psychological information finally I'll quickly know as Eric also said that uh speaking about health and social knowledge discovery oh I've also been leading an applied research team in industry working on NLP computer vision and graph machine learning for healthcare knowledge discovery from social media and the web for example we recently published a paper with a mixture of experts approach for an adaptive fusion of a lot of pre trained vision models and NOP pre-trained language models to improve transfer learning across many different data sets but I won't go into that one today so to conclude we spoke about boosting innovation with analogies extracting purposes and mechanisms from noisy real-world texts from products and patents and scientific publications for enhancing creativity and expressive search engines and building functional apologies we spoke about weak supervision a new weak supervision framework we developed working with course label average constraints we evaluated across many domains and regression and classification and in working progress we developed a non-convex extension for patterns to identify novel ideas and in terms of broader interests we I'm interested in modeling complex texts and behavioral data for knowledge discovery and science and health and the discovering novel social phenomena that's it thank you any questions yep I was hooked on the last part there sure your vision for you know finding relationships between different domains that you're predicting some useful accounting of dots yeah I mean you can look it let's predict let's find areas of common structure you know let's find that there's a functor from you know this this area to that area and you know you find some mapping that these objects map to those you have these relations map to those and maybe from this this algebraic structure you find you find some proposed thing other people have this kind of situation they do this and they've done it over here try it over there yeah but how do you know that that's gonna be valuable do you have any sense of how to predict some cases this relationship might hold and where well that's a great question so as you'll see I don't come up with bicycles for Elephants so you could think about a way to maybe there's a problem of transporting elephants so you could look at a previous work on transportation and it used bicycles okay so let's just put that together but it's not gonna have high value right so that's an interesting question of how do you model high value usefulness and impact and how do you define it and how do you predict it so one way to go about that would be by you know unlike our work on an analogies in the sense that we just came to discover general analogies we also want to observe science happening in action and if we can identify historically the scientific inventions and recombinations and analogies that had the higher impact can we have a model that not only is able to retrieve those analogies will also predict the impact and predict usefulness in a combined fashion now that's not gonna be easy of course it's good to start with crazy ambitions and then tone them down a bit when you're writing the paper but in a sense there is lots of historical information out there the history of science what and the history of Technol in the history of inventions Ashley may ended up making impacts in the world can we observe those and identify them and distinguish between them and those that didn't really make an impact and then training our model to take that into account too so that's a difficult challenge but I think that's one direction to go about what you're saying as you say it's uh it's really a hard problem how do you find a high-value areas in space not only the crazy creative ones I'm very excited about what you're doing but for another reason and that is that our planet is melting we have ten or fifteen years before all of our children are facing a Mad Max future if we don't do something and I think you actually are potentially contributing massively to that the reason is that if we start with impact and work backwards you start filling holes that with what is necessary to achieve that or what is likely or what is in the way and the kind of approach you're using could be very powerful there's a coalition that Microsoft is a part of called BT energy coalition which is trying to define roughly five major thematic areas and another 10 or 15 within each area of innovations that is actually product lines rather than cute inventions product deliveries and practice changes that would massively reverse or substantially reverse some of the climate screw-ups we've made what's interesting is what would happen if you started with some or many of those outcomes and started working back and seeing if you can start predicting white space needed space risk profiles using your approach I think would be enormously powerful so first of all I'd be happy if I can help help improve the climate and I do think that you're right that analogies and between ideas and learning what was done in the past to project to the future and discovering perhaps at a conversation before about discovering materials and maybe we can remove certain aspects of those materials that are more harmful in terms of their impact on climate but keep their more useful properties so we can think of this concept of discovering these abstract connection in a more general sense than just purposes and mechanisms cancelling out certain properties but keeping the good ones and then retrieving all kinds of inspirations of similar materials for example or similar approaches and their own kind of profiles as you say that could suit our own approach so I definitely think this has implications indeed in general science but also you would have to come up with adaptations for as you usually do for any approach for general domains and general verticals and I think this one sounds like a promising direction I'll be happy to speak more about it and see what kind of specifics could take place there with regard to paths specifically yeah work we run the organization called the lens Lance org which is a unique amalgam of global patents and scholarship and we do this in collaboration with Cantus on Steam Microsoft one of our observations is that the information content of patents has dramatically increased by removing boilerplate that is is theme specific so we use classifications which are very powerful tool to actually look at whether we can dramatically increase information content by removing the layer so each field of activity has a different syntactical gone so in Michael mo biotechnologists there's ways of framing biotech patents in terms of their embodiments of this and that if it is basically formalistic to get the grant of claims you need okay by actually structurally thinking about removing those you could possibly increase the information content of each textual sample dramatically and not have to worry about as much of the noise and have you started to toy with classifications Pacific Ocean removers yeah we have the girlfriend is the detection the course to tour today you dove over there so I guess to start sending straight of passable figures yeah but what was your task with the patents what is our task yeah I mean what why do you want to remove the boilerplate so human beings can read them that are not patent it was a simplification without simplification is the wrong word I think increasing the information content yeah patents were originally conceived as a tool for business not as a tool for patent professionals yes and we'd like to restore it to its master as a teacher so we actually did work with a classification so in terms of identifying novel patents we definitely use categories we also use them for a crowdsourcing to sample from more simple categories but we also had not such a good experience with categories and we discover that often tend to be I have super broad in many many cases or super fine-grained information like basically repeating what the patent says and has long CPC category which seemed kind of strange as if there's only like a few pairs that you take a patent family small patent family and basically created a category just for that and we found that being able to at least for our goal of purposes and mechanisms we gave these to the crowd instead of the text itself we didn't get such good results because the super broad ones tended to focus much too much on abstract concepts and not so much on the making mechanistic so we do want and the super fine-grain ones didn't to say anything about the purpose at all and in between we did we had a hard time finding the the right combination it's sometimes it did exist but for a lot it didn't but they definitely are useful for us in terms of learning features and also in traversing this kind of tree and graph and helping with abstraction I do think there's more room for us to use these especially for abstraction of a care of categories I will say that too another thing we did to help crowds read patents is also by this kind of magical sentence and patents it often appears in the background which tends to be much clearer it's like the first sentence of the background it often says the purpose of this invention is which is you know it's got an obvious thing but you have to look hard for it or this invention is built of something and something and it's meant for this domain suddenly the sort of magically in human friendly language that's only it again vanishes back into legalese so we take that sentence toon we show it to the crowd in addition to the abstract and the title what you sense on on the prospect of cutting to the to the mic all the borders of understanding and novelty and characterizing combinations of ideas whether it be purpose mechanism or other with other characterizations of patents in terms of what's actually the novel the novelty itself you have terms of getting out what the contribution is its specifically and how it fills a hole that might might have been a hole in obvious Ness you know it's not obvious to us right it's not obvious and x y&z these these characteristics be both novel and non-obvious whatever it means the novel and non-obvious so getting the idea okay that suggests there's something new year yeah which is not necessarily which might be new with an old analogy might be new in terms of mechanism of you in terms of human purpose and so the newness is interesting to me yeah but I look at a new idea or learn something new I have it I think yes it seems that some ways information-theoretic in terms of us surprise punishing yeah and where there's a pop of what that's actually interesting and new gives me a new set of ideas that I can build on I'm not sure what you've done and it gets tricky of that concept yep so go back to this slide yeah so let me quickly tell you about some of the directions we've taken and this is indeed very important because we have thought about how do we tease apart what is the new part of the pattern so first I'll tell you about what we have been doing a initially it's not new legally what are you in it yeah patent law yeah it gets me excited like I haven't thought of that before that's a whole new oil other other audiences that might see the this this contribution and light up and say this it really is therefore should the novelty function lights up yeah it's all I'll say quickly what we've done is are some thoughts about how we could do this even better so first we connect to recombination sure that again tells us that if you find two products that share similar mechanism it has shared distant mechanisms or use mechanisms for new purposes that's a high indicator of novelty so we're able in a sense without formation information extraction approach to slightly break down the patent into a subset of mechanisms and a subset of purposes and then we can find distances only with respect to these so this can slightly enable approving approach the second thing we've been thinking about in terms of the surprise this is related to the surprisal your major because it's surprising to see these two mechanisms together it's surprising to see this mechanism in the context of this purpose kind of surprising in a language model kind of sense but learned on top of a corpus of patterns another thing we've been doing is looking at citation based approaches so there's a work on that a novelty based on citation like trying to quantify what recombination means it's quite a highly cited paper there where you for example look for patents citing across very different parts of space I was all patterns that have been cited after they came out a quote in from different areas so this means it's broad interest coming from many different directions but that stuff we've been looking at and what interesting part that's directly related to what you're getting at I think is quantifying that Delta so what in this patent is the new elements with respect to prior art and I think that ties to what I spoke about that I'd be into it I'd be interested in doing which is decomposing ideas so if we had a good ability to like any idea most of these can be described kind of a tree or kind of a flowchart we did this we added this we went to that if we can extract these for any ideas then maybe we can decompose ideas and then identify what's the Delta what's the novel element being added that's like this algebra so take this idea - that idea you get that you see how novel pulleys you know I mean give me a break you know for the media lab maybe 1989 contains speakers and motion and that's a really old idea and simply attaching to a USB charger is not really novel or interesting to me if someone said hey check it out I have a new material you can make sure it's out of it has Paizo effects so in the wind when a blow is it generates 14 kilowatts now that's like holy cow that go separate light and I think of that and then I get to the Snapple that way they'll be really mind blowing is it same lights ambient energy but you know the shirt as a sale crinkling in the wind generating kilowatts so quickly spot like you know something jump out is like hopefully the kind of pattern that versus that's really interesting and if you have way to do that you should patent all you want to share that and license it share everything with the academic community so signs of the community and you can buy something cuz you deserve the license versus you know it taken this is yawning old idea you know sort of slightly modifying there anything about jumped and what would be interesting you ain't getting in that yes aspect of what we call interestingness or surprise yeah I found missing from a bunch of your work and I'd love to see that direction yeah so in a sense you've got another slide no no I mean in the sense that this is what we're trying to do we're trying to quantify it in a sense but there's much more than this is indeed work in progress and one of the features we're trying to get in there is how did this pattern change the language distribution of what followed for example and quantify this how surprising is it and in multiple ways and how many other the change will came after that how different is it from what came before but I agree that is much more to be done there in terms of actually breaking it down to what is really the new aspect so you may get to that or maybe maybe later later stages gets to the same idea but I've been asking some people here what do they think is good research one answer I got was is when you found a conservation the steal believes the snot field believes that and they're believing the things often a different terminology which makes it hard to uncover but so these surprises are when you point out the contradiction and you take a side and I'm wondering with us if you could actually do that because if you build these knowledge grabs rather than trying to you know come with some new connection and if you can take this knowledge graph and find contradictions in them which required unifying orthogonal terminology for the same thing yes actually in in the in the other theory and literature about recombination they come from a biological often Oh aspect also physicists and biologists write these kind of papers at least the canonical ones and they talk about like modeling recombination which comes as an analogy from biological proceeds and you can think about how hard it is to combine two things so the easy things that are the things that are easy to recombine are going to be the less surprising innovative ones the things that are hard to be combined but you can see the kind of contradiction because it's not a quite a concert if it was a true contradiction so well you wouldn't be able to do that because you get something that's false but it's it's convicted in the sense that it doesn't appear intuitively that you will be able to combine these two things or naturally fuse them together is that related to what you're asking because standing right now but that's the true novelty and changing our view yep yep yep thoughts on how this these inspiration techniques and the ontology could be applied outside of these sort of hard values it's like science more creativity or education yeah actually a backup slide which probably doesn't appear you anymore but so I'll just tell you my thoughts they're thinking about analogies between historical events think about analogies between pieces of literature take canals between Wikipedia pages one describing a battle happened five thousand years ago and one that happened fifty years ago think about now seas between battles in a football match okay so are these two fervent teams chanting and there's even physical action going on there so there's abstract structural relations can go across not only technology in a more general sense analogies between people so you can cancel out some elements of people and find analogies with respect to certain properties yes one of the things I'm interested in is also analogies between events and stories for example on you know a good data source will be Wikipedia or news events history repeat itself that's like my catch phrase for that imagine if you have used up this course and so on you get how the Patent Office use them themselves they give me you'll give me a break notion or quantify the novelty in some political agenda or novel it is with respect to you know what everyone else is being saying so one of the interesting ways we're just just whispering here with once on this is we've extracted many tens of millions of non patent literature citations from the global patent corpus and with constants ontology so we could actually get distance matrix matrices between the different scholarly citations within a single patent and look for patents that have the most distant types of scholarly references and that might be really a fun empirical study to do just to see if this is something we're directly looking at we have these features they can be the two main metrics one is called originality so and the other one is more ready to recombination and so basically measuring how diverse the incoming citations are and now how diverse the outgoing citations are these are two well-known metrics and economic literature it's interesting to bring them into AI and take our own kind of approaches that we know of and make this a bit better yeah okay I think we've reached quiescence [Applause] you 