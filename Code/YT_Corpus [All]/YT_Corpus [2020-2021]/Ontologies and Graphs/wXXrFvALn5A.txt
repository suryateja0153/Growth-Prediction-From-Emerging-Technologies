 you [Music] today we are very very lucky to have Jimmy young interviewing with us and Jimmy is from Georgia Tech this advisor is that we work with and configure a same language including of the detection of generation relation detection is calcium focus on this topic Jim we do a lot of interesting work including in the very basic stuff neck to understand the image to generate to connect to build a single for image and also applied to apply them to a video language navigation applied to kind of how to activity in their object detection activity and their kind of question answering this kind of problem also Jim with works on image generation based on his quad images Tsinghua idea so I will give more time to Jimmy to introduce his work okay don't wait please okay thanks a lot to punch one so yeah I'm very glad to be here again to introduce my work and so so this topic is about a structure of visual understanding and interaction with human and environment so as we will know the world around us is highly structured so for example take this room as an example so we have we can see a lot of chairs and the cables and we have bunch of are sitting on top of the charts and the lean on table so actually when I talk about this actually in my mind actually I already construct the structure about the whole environment and I know our kids are a bunch of object and there's some interactions between the object and and the war so even for a single char actually we can notice okay there is a charm there is chair legs so this kind of structure actually is exist as surrounding us everywhere so see really actually images are 2d projection of the 3d world so that's why actually for each of the image we can observe some structure for example in the image we can't see a bunch of object and what so the the the relationship between the object and the even besides the foreground object we can see ok the background actually is what so contain some semantic information for example the grass for example the sky if we're going to see et cetera so my whole research during my PhD study actually is motivated by this kind of motivation so to model the cross image structure so we proposed a motor quadrant and Schwester learning model to learn to link to cluster images and the war so the representations from these images strongly so finally we can see given a set of image without any labels we can construct some cross image structures for the whole data set so on the other hand gave a single image actually we can also model the pay image structure so based on this motivation we proposed ago of icing to extract their single off from a single image so which consists of their foreign object the relationship between the object or even the attribute of each object so furthermore so what if we have a single object then we proposed to use a neural communication network to modern the relationship between different parts of the model specifically we regard each of the channel in a single convolution layer as a pot or Python detection and the thing we enable the communication across different channels so that it can live more compact and complimentary neurons for detect will classify a single object so based on the pay image structure motivation we further introduced a model called alga to learn to generate an image layer by layer we first generator for gone or with service to change the background and then we generate the for gone one by one and finally we model the relationship with between foreground objects so that the whole image layer out will be reasonable enough so on the other hand based on still based on the parameter structure we want to modern the interaction between the vision system and the human through the language for example given a single representation we can describe the image more visual grande that's why we propose a neuro Big Talk model to describe the image based on the detected object on the other hand so to enhance the visual understanding of the image we further propose ignore the code of visual quality to lend to ask a questions about the image to gather the information about an image to help to refund the visual and standing model finally we want to model the interaction between the visual system and the environment in this model we want to enable the agent or the visual system to move inside a 3d environment because as window object including image is usually occluded by some other object or some others stuffs in this way what if we can enable the agent to move around the occluders anything we can get a better visual understanding of the object or the whole structure of the environment so in this talk I will mainly talk about this rework so the first one is how we extract the structural representation from the image and the thing how we build the interaction between the visual system and the humans through the language and finally I will talk about how we build the connection between the visual system and environment so I refers to talk about the single generation model so this is a paper from easy CV last year so what is it's in graph so let's say we have an image like this so this is the image from image Annette so we can categorize this image as a single level for example king crab but what if we can also actually detect the one of the object in this image and finally we can even model the relationship or enter the relationship between the object so for example the man-whore the king crab the human look at box acceptor so why we need us in broth so let's say we have two images in these slides there left one and the right one both contain two object the main and a horse but the difference is that the left image the men and the host has a relationship like Oakland and the right image actually has a relationship with like feeding so in this case actually modeling the structure of the image actually helped us to distinguish the image is more accurately and again we can also describe the image more granularly for example if we know the relationship between the object actually we can describe the image and more accurately then if we do not know the relationship between the object and what so we can answer questions more precisely so if we know the relationship of course we can answer the question more currently and more precisely and finally even we can generate the questions more gradually so if we know there is objects main and address anymore but we do not know the category of the animal but we know the relationship between them is working then what animal is a man working with maybe let's say there are another horse on the left side buzzards there is no relationship between them like okay so this kind of question generation will be more beneficial from this structural understanding of the image or they can only the question answering they the previous example yeah you can see that in fact in this case it cannot answer the questions does not meet you understand the relation because there's only one posterior what it's me to do is just okay detect one more classics there there was experiment just predicts a horse or just say yes yeah yeah yeah it's better than both of us yeah that's that's why I just over yeah they are Marvel horse maybe a horse on the right side and the horse on their left side and the drusen relationship between the man and the horse at the right side buzzer no relationship between them maybe they're just far away anything yeah so this kind of question actually can help us to understand or keep out to refer the horse at the rasa instead of their horse indirect to improve BQE tasks but it will be without strictly to any defined the portion of the big view question can be queued up there can't shoot any of it I say I think I ever talk about later actually we also have a work ongoing work that trying to propose a new symbolic go out for reasoning model on top of the sing graph and we found some interesting is that if we improve the single generation model the performance after victory performance actually improve as well so so from our observation of for example if we use a very poor single generation model as a as a input and thing the chicory performance or the variable yeah we welcome the chicory and that you can performance actually is not good and if we improve the synchronization model in repeat and then we can find at the performance of the chicory is what improved so the question are question for just like this question you do not need to understand that everything so yeah yes yes that's right yeah and but I still think if we can't do it in the Sonora symbolical level so actually we can make the model more interpretable and we can easily diagnose which part of the model is wrong as as this part I stupidly improvers right right yeah yeah but I still believe because we we know that the upper bound is very high yeah and yeah okay so you are using a crack here single generator for the cheek area yeah improve it to get the improvement on the chicken so do you think the GK is a dataset that is designed to show how good you are at the reasoning well how good you are at just vision back yet yeah better objectivity relation view the results I think that's depends right so because reason most of the chicken actually do not explicitly use a reasoning model so the chesil use a previous model like the model work well on the weekly that's it and on that day I said actually the just is simple okay extracted the features and from the imager from the question and then the judge can burn with some attention model with some bilinear kind of operation and I think the fan allocated the answer by classification but I still believe that to finish this kind of task for him that you create task actually the goal is that okay so we have a better understanding of the image I think this is a basis for us to reason otherwise actually we do not understand at the image and the thing the reasoning on table it is not grounded so that's why I think both of them are very important so we needed to understand the image fully and completely completely and it was all we needed to understand the reasonings in the question completely and then finally we can perform the reason on top of these two input and yeah yeah that's that that's what I think about the chickens applications something that people actually do in some of these data sets because you can have an improvement or data set yes specific good build so you can show an improvement for your the clean but for what purpose so so you will be interesting to hear your thoughts where would people actually be using I see actually I really have a experience so for example it's simple experiences that okay so one I well I go to a supermarket I want to find something right anything so I need to ask some some some person in the supermarket you hear me to fun ok so I want you to let's say we I want to find a bottle of coke coke and then I do not understand okay so where is place I need to go and that this way actually what if we can build a structure or a structure map for the whole environment anything I can ask the system okay if I want to fund a coke and then we have a good guidance from the system so that it can help me to find a coke through it can turn me the path to what sir the players so that I can find their fund the fund the coke I still think that this kind of application highly rely on the understanding of the question so for example different different so what if we want to find and others some kind of other things so which means that actually we needed the motor to understand the question and what I understand that the environment of course this case the environment is just an image about a lot of different applications that ok the environment will be more complex and then how to get is a structure of the whole environment actually helped us to build and motor that account reasoning on top of their questions yeah so basically the sink off actually yeah yeah it's a kind of relational graph but it's not a single off right yeah yeah can we serve the of work it will hokum boss right interactive interaction with 3d environment so I think you know we were just bored okay yeah and actually for the single generation that general plan behind this actually can be summarized by this figure so give an input image we first extract to the regions based on for example the region purpose a network anything we have two branches the first branch is optic feature the single branch is a relationship feature we do the ry pooling on top of the bounding box or the Union by the bounding box and then we get at the obvious call we get a relationship score and finally we can generate a single offer for this image so there are some initial previous models like where you can't update in the feature of the object and the what is their relationship by from some kind of message passing for example user get your key to the recovery current unit or some other kind of music and furthermore we can add extra supervision to the model by adding the region captions for each of the bone box and by this way it can further improve the training of the object features and the wall so we can add some kind of pry frequency or prior knowledge on top of the model so we can summarize relationship Paris a relationship based on the of your powers and then we can plug this kind of immersion to the model for help the training of the whole system so in our model actually we do where we have we have two new part the first part is that we do the message person on top of the feature level and what is the school ever and also we add a relation purpose on their work here to extract more reasonable relationship powers from the from the whole candidate pool so the motivation behind this model is like first object in the saying usually have relationship with others for example a simple case is that the car has two wheels and the car is two cars is next to each other and the water not what object Paris have relationship the sink Rafi is usually spot for example the person at his left side and the car and the red cell actually can be hardly more you described by some relationship and the existence of the relationships highly depends on the object category and the type of the relationship highly depends on the context so based on this we propose model and the first let's assume that we have a region producing occur already and we extract a war of the boundbox anything we can post a dense graph because we do not know which house has a relationship or not and then based on this we use the relation purpose in the work to prove that things go off to a space graph and built on top of this space graph we propose attentional tisanes to help to propagate the information across a space graph finally can't get it there Simba off so in this model actually we can regard the relation purpose under work as a hot attention because it directly pruned the dense graph to a sparse graph and that the attention of collusion Network actually is kind of soft attention modal it's attend the sparse graph and it adjusted the connection connection strengths for the sparse graph and finally do the publication and we can factorize the whole object function into a three part for the region proposal Network actually it's given an image and then it's extracted of ethics for the sin graph and for the relation proposal Network actually it given their image and the Washington vertex its extracted ages for the graph and finally given the image given the ethics and ages we finally perform the graph labeling to get the final result and that's the sin graph so let's briefly introduce each of the module actually to save the computational complexity and we propose a very simple way to do the relation person walk so first step is we compute the relationship needs between the subject and the object and we use a kind of functioning to first the project if the relationship or feature queue to different space and then we computed the similarity just using the product and finally waste sorted and the with chance to choose the top key to to use and then compose it space graph and the signal step is we just use a ms2 for the object house to do the filtering on the other hand the attentional chasing actually is trying to perform the propagation of across scruff and the original kissing layer is kind of like this so we can see that the important part of in this formula is that the Alpha is not label or it's pretty fun affinities because it's usually used in the for example social network we already defined as a connection however in our case we do not have this kind of prior knowledge so based on this motivation we want to learn the affinities by this way and based on this land Ivan it is actually we perform the object representation updating based on this kind formula and then we also perform the political representation updating based on this formula so we plug in this kind of attention she saying on top of the future lever and also the school error or semantical ever choose the previous image passing measure or or the sing-off generation can you finally do Albert with can you can debrief me come next door well dismiss her to the previous image and message passing measures yeah I sent this part so as you think compared with previous work actually the potential G saying it set the goal is the same so one of the model including the previous model actually the goal is the same so once you problem get the information across a single and a heel the difference is that we want to reduce computational complexity and we found this teaching actually can help to propagated the information causes sparse graph and the water can significantly see of the computation or complexity and this is the first thing and the second thing is that so actually as you can see from our my previous slides we apply this intentionally saying on top of the semantic level representation which means that we directly take the proper distribution over different object category protocol category as input and finally in in my later slides I wish you that actually we can't directly learn some kind of common sense or co-occurrence between the object were object and predict in in this semantic layer attention and design I think this part is kind of interesting to show I can show you two later yes yes yes oh yes so the actually in our model we have two meaning so the first one because we added this attention season on top of the feature level so which means that in this case that the is a feature for each after Union button box which is yeah the Union button box which represent the future of the relationship and I know as the future of race as a feature anything at the semantic level or school ever this C is directly the probability distribution on top of the object category and the predicate category and that's why we introduced to lever and so we yeah we wish you later actually the school ever agency actually can lend some kind of common sense from their data if you use the image of feature button under the coroner or concur occurrence every nation but in fact if you use say same idea level on labels label distribution in fact you can learn more useful information yeah I I can so so you know you experiment actually we found directly apply to athe saying on top of the school ever actually it can already achieve almost the same performance if we convened this to so I think this kind of model indicated tight we can't directly perform the probe information publication on top of the semantic lever instead of the feature level so for the training of the model actually because we already factorized whole object function into three different components and then for the first part we just use a regular binary cross-entropy laws and similarly we also use a banner across in job allows to change the relation person work and finally we can use to cross into laws one for note and one for age to perform the graph library and matric so the metric for the single generation is described as follow so the aim object extract from an image thing n times n minus 1 edges will be composed and step 1 we take maximum for object scores and previous course and of course excluding the background class and then we compute the relationships course by multiply these three different scores and finally we sort this scores and we computed the record at 50 and the 100 based on the ground truth and the predictions of course if we want to solely measure the performance of the political classification then we use two extra matrix the first one is fries classification if we assume the location of the object is given then we just need to classify the button box and classified the predict between the bounding box and a federal we further assume that the category of the object is was a provided then we directly with solely focus on the relationship prediction of the model and so I'm not sure exactly what is an object in this sense so really you have this example of the bird and it looked like the scene graph had parts of a bird or wind and foot and tail that weren't necessarily in the photo but are you counting those as objects or is that is bird Roger because tail given given that you have the head of a bird you know where the tail that is likely to be right so or is it bird that's the object oh yeah so actually in this single generation model actually we assumed that so each object for Thunderbird and well may be a kite is a single object and this sin graph actually described in this single of the node actually each of the node in the Sun graph represent an object a single object and I think the relationship would be the age describe the relationship between two objects for example a kite is sitting is sitting on top of the table or a kite is watan on the road or something like that so and which means that actually we assume that in the image in a single image the multiple object and we want to model the relationship between multiple object inventory of objects one that you're taking from some set of object classification algorithms are you know what these are officers are predefined what was the number of the years oh yeah so number the number of object in this benchmark we have 150 object categories yes yes yeah yeah I think for example if you're I think the objects are detected by all proposals by origin proposal but if a written proposal propose both the bird and his head has two boxes yes in fact the head will also be able object and note right right right yeah what is the secret for I think care of it yeah to me that tail and it seems like the inventory of some object labels is kind of arbitrary you could just as well have long yeah yeah yeah that's true yeah that so it's kind of given a liberal person guava so we can't describe the whole image with which contain multiple object into a single graph and we can also describe a single object with different component into a graph that's different kind of lever war granite after yeah was there was the basic unit it's a word or phrase very named entity in fact in this benchmark there's a relation called hot off ready describing kind of hell is part of the bird where you could get more more granular okay that work pre-trained or is it trained together with everything you know more actually China together so the Oliv wall as we just recently images D but and the output is the scene graph yeah yeah yeah so we turned the whole model jauntily things but then you could actually run your whole operation is that yeah yeah yeah we can you can actually discover parts but probably here is that I don't think they're much annotations on research in normal describe the objects component for combat air for example well it is very good to say it's worth now yeah one of the worth using your model instead of commercial network so the character learn parts yeah yeah I think so yeah I think we can learn to tick tick detect multiple parts anything to modern the relationship between different parts yeah I think that's yeah that's doable and we'll work on the vichy Namdeo set and as we just mentioned and there are 150 object category and we have 50 predicate category and we use different metrics to measure so comparing with previous work we found that our model has over four point improvement on single generation matrix and the two point on sega genesis which we proposed in our paper and the due to the interest of time I will not introduce the t-test for up new pro poster metric but generally is similar to the single generation matrix and the similar trend actually can be observed from the recall at 100 and there are some yeah like we just mentioned so give an input image we want to detect the wall of the object that we want was an auditor relationship between the object main red wave making water and man on surfboard and yeah as I just mentioned so that some come can come since images on top of the school every attention teasing so I still think most of the feature level and the school ever actually handling some kind of co-occurrence but the difference that we cannot directly interpret the feature lever AGC but we can directly interpret the school ever HSN for example if we sort the weight lint in the 1820 saying and rank it and we can find that if we know there is a box in the image thing is very highly likely that there is a water or a beach and I think if there is a plan in the in the image thing it's where we'll be very likely that there swing and a tell in the image detective as well so what so similarly we can learn some kind of object predict the co-occurrence and that this is a predict and object so which means that okay we can hold head we had an import sit both carry amber hold and Brer this kind of co-occurrence actor can be was a link between the object and the predict and we was a give some epilation study so it's kind of where is straightforward to note to see that our model by adding the relation purpose and work actually each improved the last of three metrics most early and by adding the attention into improved their first two metrics most early and finally we want to dive into why it also improve the performance of the object detection so we draw the average precision for each of the object category and if we do mean the top ranked a category we found that okay ragged short when shell and bottle this kind of object category actually can be improved most early so we suspect died the reason is that because if we directly detect this kind of object actually it's very hard because these kind of object is very small size and it's usually be clued in about some other object and the but if we can model the relationship between the object thing actually we can use the context container in the image and further improve the performance of object detection your single generation although even greater than the previous models and object detector but you can say similar relation yeah yeah exactly yeah some takeaway message so a festival introduced a general base model for is kind of a skeleton model for the single generation and we found a pruning the fully carrion graph is important for single generation I think the reason is twofold the first for is that we can see of the computation of complexity by pruning the dense graph to a sparse graph and a signal is shown in the experiment by pruning the food Aquino of actually the performance will be further improved and also we needed to explore the context across the object and the predict this is very important for us to improve their single generation and wasn't there Oppie detection performance yep to assess because so I miss this or so you can meant that then you have derivation fiction yep so it's really improved object detection as well yeah you to work usually improve their object detection performance yeah but not very much I think insecure for these smaller objects yeah for the small object is more significant yeah and for some other parameter the large object almost a frequent object may be the improvement it's just a little bit or or even decorated they do repeat objects they have not much relation annotation yeah this one is more yeah I think yeah that's could it be a reason yeah yeah I grew it yeah yeah that's probably to some other size right right there's balance within the nationalities of italics so if you repeat all these results at the resolution that's two times smaller in dimensions you would have different results more things with require relations however if you didn't run it on piece of level but you have the object proposals for then you calibrate your proposal based on observed error rate then again you will see a bigger bigger advantage I agree I think that's the core suggestion so yeah if we can change the size and thing is probably get a better result and but generally I think the quad at behind this is that the relationship or we build a connection between different objects can be continually help the performance of the objects detection I think I think a previous work that do not use a relationship about it to directly moderate interaction between two different object even without any annotation of the relationship thing you can still improve their performance or if you actually went into some part smaller things taking yeah it's my find little pieces and hearing health animations yeah rather than then getting an airplane and saying there's a man in front of the airplane I mean you're not gonna get much relations there yeah yeah that's yeah yeah I think so yeah I agree yeah so Jimmy here means that when you are between the election predictions hearing training you are also fine-tuning your occupation vacuum wheezing we we directly because we don't only train the whole model so basically we just have trend the whole model from scratch and we are preserved this so we can buy this model with a motor died with an object detective that directly trend without any relationship annotation so the post of the model our trainer from scratch jauntily so and we do not do that kind of stabilized training so you mean you don't preach oh yeah yeah yeah yeah there is a no-fun - you do too - animation division same yeah yeah yeah we do not fix the object actor and then we fine-tune it based on how single Sandra instead of we directly drawn to retrain the hormone if you're training everything all together so in our case the region proposal for each image is 256 yeah yeah yeah yeah okay so let's go to network so and let's see what if we do not have a good structure which understanding solely based on the annotations thing what if we can enable them visual system interact with human so that it can ask questions about the entities in the image so which acrostic we want to lend to ask questions for the agent to learn visual recognition so let's say we have an open word recognition problem like this if we bring some kind of visual system which is pre trained on image net or MS Coco and then maybe it cannot recognize some object like this and but this way what should I do so actually we can learn some kind of model enable the agent to ask questions about this new concept in the new environment so let's say okay so what is the orange object to the left of television if because for example the MS could actually already have their television category but it does not understand what this and the thing that it will ask okay what's the orange object to the left of the television or something like this and then okay that's a candle and based on this information it will go back to tune its visual system to learn about this new concept and the for example hero and hero actually in this kind of question actually it's near the agent cube get a reference in their television and it's new to understand the relationship between the this object this new object and the television yeah I think that's that says I've got a question so basically we assumed that it's kind of a visual system and what even a robot and move move in the 3d environment and a dancer move hero move hero but if we have a screen or even if we hi a arm for the agent thing we need for example we want to we want to directly ask the person okay what's category or outside the attribute of the bunny box actually this neither person need to stay with robot overs anything is kind of of course it's a kind of a way to teach the robot but another way that what if the person and the robot actually the person cannot see directly the screen or the finger or the arm of the of the robot then in this case it's more like a two-person communication which are because the robot will ask a question right and then less it first a robot here and there is a person here anything okay so the television okay they're human already know yeah yeah exactly yeah and so then so then the human can see because what is this yeah yeah I said that's a cool that's that's another kind of a way but we assume that in this case actually we do not need their robot to move specifically to the target object and then it can't stay anywhere it's like and then just mention the object by ask a question and I think the human constraint yeah it's come on yeah yeah yeah actually we purposely story about this so actually I really think a very natural way is that we can ban the counting and the words of the reference because in some cases the punting is not that exact war because for example because it's a 3d environment or anything if we were pointing something okay so there is a distance so the robot can order the human cannot directly understand okay the part that the object the human is parking is in front of or behind so but this way we can can ban the action and the war so the expression of language to exactly look like localize what refer to the object so that's our motivation why we need you back this kind of and so the whole the whole agents architecture is like this so let's see yeah as we just discussed let's say the robot and the Oracle or human I'll see the same content or in the same room anything the agent have a visual system and it has a go of memory which maintains the information about the whole environment and then given the current ago of memory it will use the question generator to generate a question and ask this question to the human or Oracle and Oracle will provide an answer to the agent and based on this answer the agent will update it graph memory and the thing furthermore up to the vision system so that at the next one the vision system or the agent actually understand the new concept and then into we're asking more about the new concept it to not understand and so we have the visual system so basically is it's not that complex so we just extracted the burn box and we just model here which were only motor the spatial relationship and the thing we extracted the arch boot and object category and also the special relationship from the image and the finally we can build a visual graph based on the visual graph we can update the graph memory and maintain the by the agent so we just do some do a simple neuro symbolic symbolic a copy copy this kind of concept to the graph memory and on the other hand this graph memory where B was updated by top-down behavior so let's say the Oracle give an answer to the question and then based on the answer the agent will updated go after memory what are different colors I know it's stupid oh yeah so we this is I this is object category and this is a HP so different yeah please because for each object we moto we imitate the object category and also to attribute different HP for example the color or the material with a ship yeah so I know case then post the object distribution the actual distribution will be another car distribution over respectfully disagree which means that's okay I knew that all the uncertainties and yeah I didn't know everything yeah yeah exactly yeah at the beginning it's just high and very high identity so about finally we want to get some very peaky disputing and the suit and the water we want to ensure this pika dispersion is correct on top of there it's what to Oscar got a new concept or new object in the image so this seems like a situation where you could potentially leverage the scene graph maybe go into a corpus of just natural language to try to figure out what that might be if you don't know what it is it it's not in the inventory of objects that you know to make some sort of informed guess rather than just ask what is the what is the color of this thing or what is this thing with with the spatial and color signature it were it seems like you could start to make it from this but it maybe not is that a candle but is that a lamp or something like that because I know that is typically where something yeah looks like that might be a lamp might be different yeah yeah you're not doing that here you just you know city to these higher level questions yeah that's a worker part so in this work with chillin out to that but indeed we can't do like that way so because we previously we want to introduce yes and no question for which means we want to use these kind of questions clarify some entity in the image that the agent is not very sure about sto can't get some prediction and but finally we do not add this yes and no question because actually the whole training of the whole model actually already is already very a challenge for us because we use the reinforcement learning to train their whole model and we found introduced this we're introduced more difficulties and that's why we we did not add it but yeah ideally we should add some question like this so for the clarification question four years and no question yeah so that's a word good part yeah and question generator so we basically we want to Lin based on the graph memory as import based on previous cursing and answering and we wanted to lend to get the target object and a target attribute and what sort of the reference object this is three different elements and based on this we can compose some questions like this so for example if we the agent want to ask about this object and then there is no reference and then what is the shape of the front most large red object if it's already know it's a large object it's a red object but the shape it does not understand then it will ask about this and thing it will also ask like this but in this case we have a reference to just this object and for example then it will ask about the color of the metal cube on the left side of a small object and finally we can further ask about object five in a similar way and the Oracle side the question to answer the question actually it has different type of answers first if the question is answerable so which means that okay the Oracle can understand this question and then it will answer great but in some cases the question generator is not answerable so what is the shape of the object and left left of the green object actually there is no current object yeah and the thing it's kind of invalid question think the article will kill the agent okay this question is invalid because it does not mean evil and in other cases that what is the color is a small cube actually there are multiple small cube actually here and here so in this case actually the answer will be ambiguous the Oracle will tell ok your answer your question is ambiguous because there are multiple cubes in the image something like this and finally which end the model as the return the visual system and the policy together and so the for training of the visual system we use crouched cross-entropy laws between the graph memory and the visual prediction or with war images object and educate and it's kind of basic conventional sugarville Schubert's supervisor supervisor learning and we update the visual system after each dialogue with std and on the other hand we introduce a question of policy and we turn this policy within reinforcement learning algorithm and we want to maximize we want to maximize the reward over word images and the darker dialogue runs so that the agent can ask as few as question to the article to get as much as possible informations from on top of the image because this university nurtures a kind of assurance the design you want to design goodness this kind of day and this possible number of experiments to get and all other and we build up yeah so so I'm interesting what's exactly the reward function yeah so basically this is a this is a reward at a time time step T because we want to increase the Quaffle recovery the information recovered by the data grant so basically this kind of this s measure the similarity between the current graph memory and the grant use government very anything we manage the current one by the last time step and then we want to get the change of the sin graph of the graph memory recovery anything based on this reward actually we want to ensure that at each run actually the agent can ask us some information questions to get more okay get get a positive reward go back to your previous event for its its idea to have a sort of aura saying you know which object to ask person selects you can ask the next object using this one person so how do you you know mix why super the mountain reward we found this reward is very helpful and we actually we only use this rewarding are trained also use the number of questions oh yeah there is a panic like nervous yeah yeah yes we incur you the a you into a spa yes smart questions eventually you don't need to ask a lot of questions yes yeah so we have a penalty on top up there a number of step or what a LaGrant and and companion wins this reward it is true together yeah Jimmy this report is easy to you today yeah to to game it so fine because I think I think this kind of I think this is a problem in green for instant learning so actually this kind of reward is kind of rewarded reshaping because in this case actually each time stable have a zero reward war positive reward and the thing most of the time we so compared with conventional reinforcement learning actually the reward is is obtained at the yeah at the fan of state about Hill at each time step we can probably get a positive reward and I think this is a most important thing why we can trend the policy well yes a variety yeah yeah yeah you don't have to specific yeah there is no specific about this so that's why yeah we can turn the reinforcement bingo yeah I think that's very important yeah and so this kind of reward actually as we just discussed so asking and vigorous or informative questions it's kind of a top-down and what so we needed to link a better visual system quickly based on the graph memory collected by their interactions with a human so it's kind of bottom-up so these two things actually are both very important for us to get a better performance version punishment to you in two thousand vision recognition arrows so in the spinning arrows the video system is this probably this obvious screen yeah yeah could be a mistake yeah yeah yeah we concern about this yeah we take this haircut actually the prediction from the visual system as far as I remember it's kind of 80 to 90 percentage accuracy and it's not a perfect yes similar to the normal system yeah the NOAA needs to understand if the input but there's a special conditioner or natural changes yeah yes yes yeah yeah so there is some errors for the visual system actually so but the whole mode actually we found is still hunt can handle this without any diverge or something like that experiments especially is no it's yeah yeah hasn't actually do they're two kinds of questions that are validated questions all happen and bigger questions which ambiguity maybe that kind of thing is that kind of question do you have is they an error in the European vision system is actually this too is indeed a very important so forth from the ark Assad so if in this case actually the Oracle will tell the agent okay this question is invalid or this question is ambiguous actually this will push the agent to derive more informative and and Vickers questions and the thing based on this actually the annotation obtained or the graph memory obtained as through the Targaryens will be more accurate and and that's why the visual system will be trained with more how accuracy so yeah I think that that says yeah that's a good point so we have 238 and the first one is since its synthesized it does it so we train our model on top of this in societies yet and then we directly test our model on top of this really scared as I said we want to check with it whether this kind of a model can be directly generalized from a census that they are said to a really skaters here and a finally we found it works and we we come back with some baseline so I think you you might you must consider some baselines like if we randomly select a toggle option if we random selector attribute and we if we randomly select a reference object and then we also use a highest entropy selection strategy to select the object or attribute with the highest entropy in the coop memory and what so we command the highest entropy and what is the special together for the comparison and we collect a standard cassette is based on the claret acid and we have different option cards colors material and cest and normally the number of objects is from five to ten the task is to of what what object will object all other model or other object so for each image the agent will ask at most 50 runs of question to the Oracle and after this 50 round of dialogue groans anything ideally we want to measure how much as the agent can recover on top of the image yeah and we have yeah like with just measure so there are 50 run at most anything we measure okay so if we if the agent already asked us things wrong and there how much information can recover from the dialogue anything finally we can't get to the recovery at the luster on and will measure the effect so we compare the random and in from entropy based the entropy class context and ours and finally we add a appellation static without updating the visual system so as you can see here so there is a very clear trend that our model consistently improved the the graph recovery compared with the baseline model and finally we remove we do not if we do not update the visual system to read during the dialogue around so what happened so we confirmed that this performance is reduced a much I think this is a very reasonable result because if we do not link the visual scenes before the ancient during the dialogue system which means that what other information are stored in the growth memory the agent yourself actually do not link anything it's just a memory memorization on top up there on top of the graph anything of course the performance will degrade your feet and we further collect some debt is kind of a novel concept and the mixed and war so we'll discuss it and we'll only train our questioners on that standard set and one of the other testing is directly performed without any phantoon or any other training so as you can see here there is training on standard and the pasted on standard test set and the training on standard test on the novel set and the mix the one and the release go on so basically we we over this three synthetic attea set even it's novel or mixed actually there is almost a no decoration on for in regarding the performance of the graph recovery but on the real estate data set actually there are some decoration and we suspect it's not because of the motor itself is because on the realtor said actually we have more object category more attribute category on top of the release cadet and generally the performance will degrade a little bit and for example we just directly transfer our model on top of this really quick yet it will ask what materials they left the most thing and then it's food and doing the lips and water left most object what it's and potato and the left most of your watercolor and exception anything to ask about the other object that need some reference paper and finally career so there's some takeaway message so we put in this model we propose in your symbolic pipeline tooling to ask a questions for agent to lay in the visual recognition model so through the interaction with human or the Oracle the agent actually improves it will understand the capacity of Godot on the fly so finally the lender question actually generalized generalized we're very well from the synthetic idea that you really skate this are saying this is a advantage of the model itself because we use a neuro symbolic pipeline instead of a purely neural pipeline and that's why we can simply generalize from synthetic tears to the radiative okay so finally I would like to introduce how we can model the interaction between the visual system and the environment to help the visual system to understand the environment well or the visual signal which are data more yeah better and this is a paper from ssv this year and the motivation is like this so clearly let's say we have a visual system here and at the beginning it's just a proposal propose a region like this anything it's one to understand what's the category and what's a ship of this object of course we can do it with a static visual system model and we can predict it as well and but the performance may be like this the the result mapping is not good and the thing what if we can enable the agent to move around the object and they okay it's learned you okay so more left oh sorry mu right and they move right and add a move right and then move forward something like this and finally it can get a better visual recognition result oh it's a sofa and the whole shape of the sofa is like this and this is our motivation behind the paper and embodied a model recognition we introduced a new pipeline it's quite involved in motor recognition model so generally the price of one is something like this given image and then we predict the category and segmentation result and usually it may be it may fail to recognize the object and obtain we proposed embodied in model recognition pipeline is kind of combining the motor recognition system which is a visual system and what so indicate action policy to hit the agent tooling okay how to move to get a better observation and we introduce our task so it's consists of three subclasses so object recognition 2d and moderate localization and a 2d and motors communications yeah so that's a good question so a model means that the the segmentation or the localization sure that was it considered the parts of the object which is a crew data and which means that we do not only just a predict the reasonable part we'll also need to predict the invisible part for the object as a total and this kind of a mode of perception actually is kind of psychological observation and on how about ourselves so in general human has this kind of a mode of perception ability but there the resources were actually usually do not have this kind of a moral position yeah and this is a model a model recognition parts it's kind of temporal temporal image processing framework so the object function is like this we have a reasonable one box as input and then we observe multiple images by moving and then we want to predict the result based on this one of this information when we get during the move and we propose to use a GRU to do their temporal aggregation to get better and better results and we use different laws actually this realist is for classification for button box localization and the force mask generation yes you only get the bounding box for the last entrance in the whole thing for the first thing which were where you saw in article oh we actually we well we are predicting on top of the first image so you information 0i1 a night which one is the first division III Thoreau is the first image and that this is not means YT it does not mean okay so these predictions for the t's image so YT is the prediction at the time step T and is still for the first image we still always yeah it's your first Google yes yeah yes that has the field prediction yeah exactly oh yeah yeah kind of late and the zero yeah I think this will be tricky here more even and you'll be weird but one week given this task actually our motivations want to compare all embodied or active region system to the passive one and for the passive one actually it's only have the input the first input and then for the fair comparison with a passive visual system when what that's why we also constrain our model was a predict for the first frame but but of course we can extend to extend our model to predict wall frames were the last friends were even the whole even the 3d shape of the object yeah and lane to move similarly it was to take the sequential image as input and it want to learn some kind of policy to gather the agent you move and we use reinforce man reinforcement learning model to change the policy to do this task we collect a new data set based on the sensor G so we collect eight object category and as you can see here there are different kind of occlusions in the environment for example here this is not kind of much but this is severely occluded and the in this were actually the object is out of the viewpoint of the visual system and we was a collected shortest paths there was a target object when given the location of their agent at the beginning shot so far you are going to move around and then figure out what it was in the first scene and put the body parts but you also learn how to compute your predict where the object is and things that you don't see even without me around so what I mean is I see a chair and I can imagine the rest of the chair here yep yep yep yep I can walk around and see ya and verify it but I don't have to I can do it now because I've walked around before so I know what the chair looks like because I walk around before so in your system is that your goal or is it just to go to backtrack what's the goal of that was yeah so the goal of this talk Tusker is - so let's see so to go up the task is trying to link a action policy so which can help any kind of system a visual system - yeah yeah to see the full object to understand learn how to imagine what object is despite the rules so for under set of the visual system actually we do not so actually we focus on the under action policy part and regarding least regardless of the visual system actually ideally we want to see that okay given which cut whatever visual system you provide me and I think I can plug our action policy to help the original be system to improve the visual understanding of the object so this is our idea or goal and yeah and now actually the vision system actually we just changed some visual system and then we wanted to learn a better action policy to help the model the visual system mode or - and standard the object better so which means that our go is focus on the policy part yeah and the result let's see some result here so we we find we we have thing max 10 times tip movement for movement maximally and then we computed the accuracy and that the motor burn box are you and what are the masks are you and we confirmed that the environment helps to improve the emotive visual recognition performance so at the time step 0 and you can find the performance is not that good and then if we keep moving and then the performance will be improved and what so I'll learn the moving part of strategy for the agent actually outperformed the other moving strategy for example random one was the shortest past one and finally we confounded that actually the emotive reckoning performance chance to saturate at the end of remove it I think the reason for this is twofold the first fold is that because we use a very simple temper of fusion or temporal aggression may say to aggregate the information from mark for viewpoint or mark for frames and if the current saying is that because the time step wizard with gruel of their time step during the the training or the fusion of the the information we're becoming much more challenged and that's why we can see some kind of saturation finally but I think it can be improved by for example having some tracking system to help to localize the object better yeah we was given some ablation study here so as I just mentioned so the aggregation actually plays a very important role we compared with some average fusion or maximum fusion and also we use the optical flow to help to do the feature working between multiple frames and finally we can ban the GRU and up your flow based the feature working and we found that this kind of combination works best was all is not that significant and finally we want to shoot the length actions as you can see here this is a distribution of shortest paths across different time step and the arrow pointing up is a moon forward and then move left move right and the turn left turn right and move backward so as you can see here the lender Python is very different from the shorts pass and we can also observe this difference here you can see here the shortest path of course is approaching to the target object but our Lena pass actually is not necessary to approach the target object it's maybe just okay moved shift left shift right and move forward even more pi chord actually and to help the understanding object and let's see some some results sorry so this is a passive one they're performing some something like this and think it what it will learn to move maybe in this case actually it's limp backward in European because the object actually is out of the scope of of the agent hallucinate is can is kind of led directly to do the model perception based on a single image examples not not very example so it's just a given single static image but with a yeah so so forth hallucinate actually describe the traditional statical and motor perception so which merely you have that image they also have a label of where that is yeah yeah any sense right so yeah that yeah during a training that's your yeah during the training we have their annotations right yeah and so the other one is actually good at all without without having examples or do still we still have the example yeah yeah so but the the supervision is the same the difference that we plug a action policy for the visual system yeah and yeah due to the time limitation so we were shootin takeaway messages so we we confounded that actually environment helps to get better visual recognition uncover the object in 3d environment and we can also observe that our model actually link a bit movie strategy and even though is sometimes we're challenging for us but Estero can improve the performance of the visual system and finally I think a future work on top of this is that what if we won what if we can enable the object enable the agent you move inside the 3d environment and then get a full understanding of the environment maybe even in 3d structure so finally to summarize so in this talk I just talk about our three works the first one how we extracted the same graph from a single image to perform the structure of each one standing and then how we leverage the interaction with human through language to help to understanding the visual data and then how we leverage the interaction with environment to help understand the visual object in this reading environment so basically to summarize is kind of learning from interactions with human and environment and furthermore actually we come also instead of learning from asking questions actually we can also learn from the image corpus and then for example giving a image and a lot of descriptions and how we are leveraged this kind of description to help understand the visual data on the other hand actually we can even motor the straitly structure in the 3d environment and to help to understand the whole environment on the other hand actually what if we already have a good structure of visual standing on top of the image or on top of the 3d environment actually we can learning we can use this for interactions with human and environment so for example this is one of our previous work in your baby talk we extract the object and the thing we can post a description of the image and also another work and doing now is called cooperation machine so we extracted the image single off from the image and the thing we extracted the reasoning reasoning programs from the question side and then we use an executor to get the answer and finally this is a reason a very popular visual language navigation task and and we note that actually in this model the usually do not have a structure withstand which one stand after image and what if we can extract the object and what if we can't get the ID filter and even the relationship of the object it's mad probably Hiep the model to get a better we shouldn't like language navigation performance to summarize so I so I think I think the vision in as a whole actually is very important and especially I think the structure with your understanding is very important and on the other hand so because the visual system is not solely isolated from the framing or environment we definitely needed to model the interaction between the human and the visual system and what is the invite environment and the system to help to get a better structure or understanding on the hut other hand is kind of do a problem so what if we already get a structure which I understand actually we definitely want to use it to interact with human and environment for example the task I mentioned in the last slide so finally we can get a visual system as especially a structural resource system that can get a better interaction performance reassuming and environment and yeah so basically this is one of my yes this is maybe one of my future goal to work in in my research career yeah and finally I would like to thank one of them and without them actually maybe a bunch of work cannot be finished and yeah thank you I your relationships were all pairwise relationships do you think it's important to model relationships between more than two objects like for all the people playing together or things like that yeah did you see that in the data sense at all where are all the pairwise or as far as I know I actually idea now notice any kind of this group relationship in the research in Nantes yet and but a steer or Sankhya is very important to model in the relationship between two groups for example so maybe there are two groups people to play in the basketball actually it's not between two single person it's between two groups and what if we can motor this kind of even high level relationship within we can capture more information from the image or from the video sequence I think that's that yeah that's a very good apart and the challenges would be with extending your approach I think the first challenge is that we can hardly get at the annotations so the first is about data itself so recently I think even for the vision genome actually is very nice and it's not a complete annotation for the relationship between two single object and yeah that's why I think the faster challenging is the data so how to collect the data so how we can annotate the relationship between two groups more completely and the more comprehensively and I think this is a refresher challenge and the piece a disease so how to model the relationship between two groups and I think in still very challenge if we directly use any of this the single generation mode or admission in my slides because the problem is that so it's kind of hierarchical modeling so we needed to press a model the relationship between two single object anything we further needed to aggregate the information for each of the group and then we know that the relationship returned to aggregated of groups and thing I said the model the reason mode actually cannot be to actually applied to this but yeah sto very interesting direction to explore isin yeah [Music] you can follow me yeah I think we still have 20 minutes oh also very technical question kind of fun find your country works especially then ask to come on it seems to me that the projects is so much content so many components and I can hardly imagine that I can finish this kind of even one project within one year because there's so many components oh what's your kind of your experiments to finish this kind of Nasaf you know many people component you may be given skills to finish this for days yeah that's a very good question actually this this room and means a tough year I work on this kind of project actually so so actually this to project post involved a reinforcement learning a gruesome I think maybe you're also experienced the difficulty to tune reinforcement any model and yeah yeah I think the whole wall looks very challenged and actually I'm not sure whether you'll notice actually we simplify a bit so for example for the six second one we assume that one of the burn box is already provided so there is no miss detection one miss localization of the burn box and the for the third one actually was a simplified a little bit we assume the reasonable one box is accurately provided by the purse but by the user and actually this to simplification hipping ask to get a get a model work otherwise if one of the one of the one of the can mr. okay Lin or just provided from scratch and being the whole model I can hardly imagine I can work it out I think so I guess in this to Morris yeah we do some simplification I think this is very important and what yes yeah are you collaborating with other members so far but some of this look for some system developments yes yeah so you it's hard to imagine develop this resistance part right it must be a team yeah yeah that's what is true your team as well yeah yeah we were walking about the fire Superboy it was of a party system system development yeah so yeah I think yeah impose this actually we have a lot of collaborators right and and I think during during the project actually we usually discuss a lot so especially we just yeah share coats and we have a shared github repository and the different people can get access to the repository very very flexible and this is the first one and second why is that actually different people have different kind of skill set for example justing is more focused on regional language communication and and more focused on the region set and Steven for example a postdoc and actually focus on the high-level discussion it's kind of he's very smart and he can get us at a very high level and usually I found ok what he talking about is is very reasonable and that is correct and actually he he helped us a lot as well yeah so and what so for the last work and the Cholula and means were actually we discussed a lot I think we discuss every two days and how about the project actually behind this model actually we tried a lot of different motor designs actually so but finally we can make a work I think is of group effort is not just my service effort onions occur when I interned at Facebook and he at that time she's a researcher at fair and then of course she already left to create to co-found a stop and we collaborate on the Aragon project for the image iteration yeah you can who is young we [Applause] 