 all right well I'm really glad to be back in San Francisco I moved away about a dozen years ago I used to live in Noe Valley and it's just great to be back so today we're going to talk about how you actually get stuff done and you know we've had a lot of great presentations on capabilities but the real question that a lot of folks ask is well how do I get started what do I need to do how do I talk to my leadership about graphs how do I explain what the opportunity is and I'll tell you one thing you know I've been working in data for probably 25 years now nobody gives a crap about data they don't they really don't what they give it what they care about is they care about experiences they care about analytics they care about insights and so neo4j as much as we love it and we think it's terrific in it and it's so powerful you go up a couple levels in an organization and it all just becomes a blur they want to know what can you do that's new with this approach you're telling me about and so what I'm going to share with you today is a way it's sort of a structured way that you can go from zero to an enterprise deployment and it's essentially the same pattern that we follow at UI and as Lance mentioned we are very pleased to be able to sponsor an u4j it's a huge opportunity there's a ton of work to be done my basic prediction is that I think that within about 10 years something like 50% of the sequel workloads will move over to graphs in one way shape or form all right so next all right so I think people have seen various versions of this kind of slide traditionally you had single vendor monolithic databases sequel based there's been a whole explosion of no sequel solutions the basic philosophy here is you know rather than torturing your data to fit a single data model figure out solutions that can retain your data fidelity and use your data in the shape that it's already in we're going to be talking about graph databases here in the lower right but you'll see throughout this talk I'm going to reference some of these other systems why are graphs popular so the underlying internals of graphs graph mathematics graph data design all that stuff it's been around for a long time you just couldn't deploy it because you didn't have the capability to and I believe that if Larry Ellison had what we have now Seagal would look totally different remember once you get past the concept diagram in your sequel design exercise everything else is a workaround to conserve CPU and memory in sequel graphs are essentially exploiting the fact that we have really cheap memory and that cost of that memory is continuing to fall and so you can go look around on the cloud platforms and you know several years ago AWS offered their x1 servers with 4 terabytes of memory last year Microsoft has got machines now that have 12 terabytes of memory so it's it's quite likely that within you know Dex 2 3 years we might see 100 terabytes VMs and we may even see by the end of the decade you know petabyte scale VMs so it's really quite interesting what's going on and the implication here is that you now have the opportunity to put a really vast quantity of data on a single server you don't have to pay the penalty of network connectivity that you would with a distributed commodity server system so the other underlying part of this paradigm is that because of these really capable VMs that are now coming online it actually has challenges some of the older thinking around distributed systems and obviously the fastest i/o you can get his motherboard i/o rather than network i/o I think this graph has been shown I just grabbed this this morning graphs are still taken off in terms of popularity among developers and who here is brand new to graphs raise your hand if you are ok good alright this is great so super quick what is a graph a graph is a visual representation of how data is connected and I'm going to be talking a lot about the labeled property graph model which is neo4j z-- model I think it's very easy for people who have thought about sequel for a long time to get their heads around and so basically you know a graph shows how data is related this is an example of an e-commerce you know data model so I send a bunch of email to a bunch of customers and I'm trying to get them to visit my website and I'm trying to get them to buy some product it's sold on that website and if you go talk to a business owner and you say show me how your business works they'll go grab a marker they'll stand up by the whiteboard and they'll start sketching out stuff like this and one of the really interesting things is that these diagrams you know they convert over to nodes and relationships and very quickly you can actually start roughing in a graph schema that has very high fidelity to the way the business thinks about their business and that's an important point because that actually lends itself to do to the entire agility and velocity of the project and so in this diagram you know we have the semantic representation you know here's what the description of the business is we send email to people so they'll get to the website by our product we know what that looks like from a process perspective we can represent that in a graph and then when we start loading data into a system like neo4j we can actually have a schema that does exactly this once that data is in then I can write query traversals and so neo4j is query language is a declarative language like sequel but the difference is is that rather than declaring sort of in the mode of a bricklayer I need this table I need to figure out my joins I need the next table I need to figure out my joins I need the next table I need to figure out my joints and in cipher you think much more like a snake okay where you're saying I need to traverse you know through this geography through this landscape and I need to hit different things along the way and so if we take a look at this very short query what we're doing here is we're writing a path through the graph and we're declaring that we want to see data that satisfies this path and the way that I see that a graph database works is that you'll get a row of data back every time that traversal is true and so here we're saying let's send it you know show me all the email that was sent to a person who has a who has a name value of Steve Newman and I'm going to require that this person will have visited the website and I'm going to require that Steve you can see the runtime variable the tuple P well it has also purchased a product that was sold on the website and anybody that's done sequel will immediately recognize this as a correlated sub-query and you'll also notice how compact it is this is very common with graph expressions when you convert sequel over to graph and so of course this is going to pull all the emails to Steve as long as Steve went to the website and purchased a product if Steve never visited the website or never purchased a product we get no results back okay what about graphs so graphs are everywhere this actually was my favorite tagline when I start first started interacting with neo4j so any kind of densely connected system data process as almost always inherently a graph you can have graphs that are complex hierarchies so for example I've got ships I've got containers on those ships right those represent bills you know those are described by bills of lading inside those containers or packages that have destinations that are going to and those containers are going to get offloaded onto trucks they're going to go to warehouses very very complex graph typically associated things like supply chains any kind of componentry is also a graph and so if you think about the concept of a digital twin you've got graphs that are you know built of you know assembly sub assemblies components sub components and so forth and you think about the cost of you know of those items you think about their availability you think about long lead times graphs could become very important for analyzing things like this and then I made this point earlier of course every single c-cold database is in fact a graph and they lend themselves to being very quickly ported over integrals there's lots of use cases and graphs I think you're probably familiar with most of them what I will say is that nearly all of our clients begin with some kind of a 360 view might be a customer 360 view it might be an asset 360 view or project 360 view and this is pretty typical because in large enterprises you have many different databases and applications that are all driving towards some common business goal and it's not uncommon for business stakeholders to say I need to see everything that's going on with this customer with this asset with this project etc and the in-state can very realistically look basically like this where I have some kind of a data Lake shown there in the blue layer it's consuming a variety of different data types ranging from unstructured to streams and I've got a graph that's that's basically serving the role of a unification layer where I've got a consistent set attack semantics a consistent taxonomy and I've elevated from my data Lake into the graph the most business valuable data and I've done basically a data value engineering exercise a conversation with the business that says you know what what is the important stuff you want to connect having done that now I have an opportunity through all of the terrific driver and API support that's associated with neo4j to connect my applications my bi suite and then open it up even for my data or my data scientists so well typically when we do projects we'll start you know with one of these interest groups it might be way over on the marketing side or it might be in the sales side or it might be something around operations and then typically we'll begin with one data domain and it's not not unusual for it to spread very quickly to other data domains okay so what are we talking about today today we're talking about a roadmap for how to do these graph projects and this is a general process and you can leverage this these slides will be distributed but generally speaking you'll need a small team graph architect some kind of a data engineer who knows Python and and tools like kettle or ETL etc a full stack developer possibly a data scientist if you're interested in deep analytics or maybe some kind of a or bi or front-end developer if you're thinking more experientially or for operations so generally speaking the key thing is to figure out is the problem that that is under consideration is a truly graph e which means if we put it into a graph can we expect a big lift around this problem and we solve it and then generally what what I recommend is that you get get your hands on neo4j and start building something on your laptop make a very simple POC that just shows that in fact you can create kinds of queries that your business has never been able to pull off before and then we go into a pilot mode which is typically based off of a snapshot of data and that'll teach you a whole lot about your environment it'll teach you about some of your assumptions you have an opportunity to refactor and then you go under your production build and so typically there's a sort of a set of activities that tend to overlap across these steps but they're you know there's a lot of stakeholder input in the beginning you do some graph design work there's a ton of data work usually you start thinking about your API your data services how you're gonna mobilize your data and then you know some more work around integration and a refinement applying lessons learned as you move from your pilot into your production and then it's really about scaling hardening and it becomes very typical of any large enterprise software our deployment and of course there's a set of key conversations that you can have as you develop your graph project and one of the really great things about graphs particularly neo4j is they're very easy to refactor I can roll in new data load it into the graph and then decide how I want to connect it to my existing graph later I can change the relationships without affecting the underlying data I can add new relationships I can deprecated relationships I can refactor properties into labels for example lots of different things that you can do as you as you get your arms around data you don't have to basically do the same kind of really gruesome refactoring that you would in a typical data warehouse so what's a graph II problem so these are some of the key characteristics of graph you problems anything that requires many entities anything that involves recursion anything that it recall that involves complex hierarchies there's a whole class of use use cases algorithmically driven that depend critically on the relationships and the data that those relationships carry and so you think about wayfinding you think about shortest path analytics all these kinds of things you're not really talking about the data you're talking about the relationships between the data and then any anything that requires you know complex identity mapping where you might have to reconcile identities or you might have to support multiple identities simultaneously and so we see a lot of this with data like unification and then of course since neo4j is an in-memory graph database it's really fast and so if you've got you know some system and you've got a you know a pretty good data design but the thing just isn't running very fast just simple raw performance can be gained by moving that data environment over to a graph and so most importantly though talk to the business most business stakeholders will have a large backlog problems that have been failed to address be addressed historically and you know asking questions what do you want to see what could you what can't you do what answer what questions can't you ask an answer and then and then it's also important to see if you can pin down your business stakeholders on what would be a compelling demo that you the stakeholder would like to turn around show your leadership when we go and actually ask you know the CIO for budget to do this thing so how do you get started yourself so the first thing you do and if you haven't done it yet go do it right now which is go download do for J so it's neo4j dot-com slash download and you'll get the Enterprise Edition nice desktop wrapper around it and you can fire up a graph and some of the things that aren't necessarily completely made made clear is that there's a really great set of utilities called a POC and you want to make a couple of key configurations to move data in and out of your graph using a puck and so they're listed here we'll distribute this and and you can take a look at these and then of and then probably the very first thing you do is you go through some of the basic lessons there's great self learning tutorials that ship with with the distributed version of neo4j you can get into them from the browser and then you want to begin developing your facility to move data in and out of neo4j so learn how to do load CSV functions and you can use neo4j loader or you can use the a POC loader and the point that I'll make here is that any reasonably sized laptop should be able to develop a graph that will have millions of nodes and relationships in it it's no big deal and that's because graphs are so compact and then you know for extra credit go out to neo4j - examples and begin to clone the github projects and you can basically find starter projects that will show you end to end examples of full-stack applications that use your favorite language you know whether it's Python or Java or JavaScript or go or whatever and then the next step is once you've you know once you've kind of figured out how neo work stands then go grab some data my recommendation is you know start with three or four data sources begin with some simple sequel snapshot queries pull down the CSV files and load them into your graph you use common sense you know labels and nodes for your initial graph design and then don't be afraid of recursion so you know in sequel recursion is sort of this weird thing that you know people try to avoid or you know sort of sits outside the the actual design because nobody really wants to do recursive queries in sequel in graphs you can you can do recursion all day long and so I can do something like this you know this is that little line of code there that's basically an entire HR graph here's an employee there reports to an employee right very simple set of relationships but it's very powerful and you can tell by the directionality of the relationship who the who the boss is right so the boss is the person on the right and then you know there's a bunch of neo4j modeling that's out there don't get too hung up on this at this stage you know just follow your common sense use use simple semantics is simple naming so that if you go show your graph design to your business stakeholder they'll recognize the entities and the relationships and it might even be the same stuff that your business stakeholder white boarded for you there's a great website called a PC Jones comm slash arrows that will allow you to draw to do diagrammatic schema design we use this a lot I'm going to show you a bunch of these schemas and then test your design write a bunch of queries against it see if you can break it see if it tells you new things and if it doesn't tell you what you think it should be telling you go back and look at your design and so here are some example designs to get you thinking so in the world of say supply chain and contracting you might have a graph design that looks like this and so as you're doing your mental map between graph and sequel recognize that each one of these circles here could be a table in a sequel environment and there would be dozens or millions of records in each one of those tables and in a graph those are all loaded as nodes and they're given a common set membership by the label and so here we have a product supply chain a set of nodes we've got some tracking and traceability and the key the key thing that's going on in this graph here is we're looking at how invoicing is occurring we're looking at contract entities we're looking at who is the actual service provider and you can see that there's a very complex hierarchy and it might may tie out to things like master agreements and so forth so this might be you know a solution for say a a chief procurement officer who wants much better reporting customer 360 graph say in the consumer space so here I have a customer who is really the center of this graph and what I'm doing is I'm surrounding this customer with a bunch of other data elements around the customer segment various digital interactions products that they may have purchased the actual transactions the tender types we've built many graphs like this and then it goes all the way out - you know Account concepts like the email address the billing address credit card and so forth right again dozens of tables all being rendered simultaneously in an in-memory graph all of this data is available and you can write really rich queries against it with just a couple of hops here's another one master data management is a big deal in graphs and you can use graphs to basically algorithm algorithmically determine what is your golden record and so an approach for that is say I have a b2b scenario I've got customers who are really businesses and those customers have contacts and the quality of those contact records depends on a set of core data elements that might be being supplied by a variety of different legacy applications that have different levels of authority I can actually put that level of authority as a value on the relationship and then I can do a runtime query that says give me the best email address the best the best physical address the best phone number and all of those details may have come from a webform an invoice or something else that all have different levels of authority data discovery across different types of data and so here this is this is a something that we do a lot of so we might have a lot of structured data that's coming out of databases but most companies will also have a lot of unstructured data so you think XML and JSON that might be coming off of you know register systems or legacy applications things like that and then they might also just have a bunch of documents that sit as PDFs proprietary data formats etc and you can mobilize all of this data and make it discoverable in the context of a graph once your PLC is is done then you want to think about your applications that you're putting on top of it so you want to think about you know what are my real breakthrough queries I want to really start thinking about my use case maybe you go out and look at some of the you know some of the literature on a particular use case and then you want to think about how you pull that data up into an application and one I'll give you a quick hint if you haven't checked out the grand stack it's quite powerful and will help accelerate you in connecting your graph to very powerful applications that you can build and react um and again what you're really doing is you're focusing on the business value you should have a stakeholder meeting and show show your work at this point well dip into graph QL for just a second so what graph QL allows you to do is essentially sidestep some of the middleware that's associated with a typical rest api and the way graph QL works is that the database itself publishes the full schema of data points that are that are available to applications and so then an application it can actually query that schema directly and then a graph QL query is essentially a post with a JSON snippet and so you can see here number one I've got a power bi and I've got a curl wrapper around around a little query and the query is shown there that post goes to the graph QL endpoint or all of the available data is typed and then is converted into cipher and then hand it back and so this makes for very lightweight transactions and it also takes out this entire loved layer of abstraction between the application and the database so people that run the database publish the data that's available people that are writing the applications you can see exactly what data is available and only pull the data that they want so this is very powerful and here's an example of a report that does this so this is done in react and I built this with the react starter kit and you know what's going on here is we're pulling some data graph we're pulling some data out of Couchbase and over on the right there we actually have the blob pointers to the original documents in this case they're all PDFs that are sitting in Azure all of this has made it mobilised by the Graff all right so now we're in pilot phase and in pilot phase what I recommend is that you know pick a cloud that your business uses don't do something don't pick a different cloud for example use the marketplace images if you can so for example on Azure you there's a basically a point-and-shoot image for either a single instance neo4j a VM or a cluster start with a single instance generally speaking you're going to need about as much RAM as half of the size of your sequel database on disk I typically will attach external drives and keep the data there so that I can change the server out if I want a bigger or a smaller server you figure out your architecture understand your processing I love Python I do a lot of work on Python it's good for getting thin getting things done quickly and there's a great neo4j Python driver near 4j has a high speed loader and then of course you can't escape data data cleansing 80% of all the other projects are you know it's all the data cleansing and then if you need any help you can reach out to your friendly system integrator or neo4j services and you'll find you know a deep body of expertise who can get you over rough spots my philosophy for around for doing graph data projects is what I call an iframe model and basically you know you figure out what are the minimum viable data domains you need to work with you push the graph on top of that you make choices in the graph design that you know are gonna help that the eventual target application figure out what is the least amount of middleware you can put in place to surface the graph data this is why I'm introducing graph QL here and then get that edge experience built as quickly as possible if you can do all of this and you know just a couple of months you can maintain velocity around your enterprise discussion with this technology and then later on you can take that same data resource and maybe there's additional opportunities for new experiences and then finally you can go out and expand horizontally on the domains themselves as you get into bigger and bigger scale so that's a model that we use a lot so here's an example architecture for say a sales and marketing data warehouse that uses polyglot persistence and is unified by a neo4j graph and you know what we like to do is we like to leverage you know some of the core modules that are offered in the cloud each cloud provider has a set of enabling you know tools and applications and then I typically like to have put into those cloud environments different types of data management systems that are specific to the data that I'm working with and so I might use Couchbase I might use snowflake as a warehouse there's great tools out there for moving data around at enterprise scale so for example kettle has a neo4j plugin where you could load data through you know structured yupi ETL scripts very visual and then you'll have a data service on top of this and then you know you can surface data into whatever your executional platform is or your data science platform or your reporting platform just as an example you know when you're traversing a lot of different kinds of environments you know you can basically take a very large amount of data and reduce it so for example I just did a demo where I took five terabytes of data and Azure I turned it into about five gigabytes of documents and Couchbase and pulled that over into about two gigabytes of neo4j graph with pointers that went back to all of the original Azure documents and so this is a typical pattern okay and just to show you how quickly you can do this so once your data is cleaned up yep you can you leverage neo4j is fast loader tool and so here you can see we're loading about a half a billion nodes and two billion relationships in just over an hour and a half or at an hour and a half and then we've talked a little bit about analytics so I'll just mind you that graphs are great for searching you can do reasoning with them and you can also do lots of interesting things around graph topology and then there's a whole body of work this is what Amy mentioned where you're using graphs to actually engineer features for additional deep learning exercises and a sandbox for doing something like that might look like this so this is an AWS example or there's bit where it's basically neo4j on top of hive where we're also running whatever your machine learning application is that you like and that could be h2o it could be tensorflow etc okay and then final step going into production and when you go into production this is really when the neo4j project begins to look like pretty much any other large-scale data project that's being executed at your company you're going to follow your bet your IT best practices think about security what actual data do you need in that production system it might be different than in your pilot system you know you might want to be masking you know PII you know all of those kinds of considerations you want a full stack of environments going from dev all the way up to prod you know think about DevOps automation you want test automation you want to do load testing against your API is you want to monitor all your logs think about look at the queries that are coming in and maybe refactor or redesign or re-index to make those queries run faster and then think about leveraging that iframe model so you can take this same the same installation and deliver more value with it and so I'll close out here and the key thing about the key thing about neo4j is that because it's so flexible you can really drive a lot of agility into what are considered pretty clunky kinds of data projects at the enterprise level and you can iterate so quickly on neo4j that you can have a really rapid cadence of conversations with your business stakeholders and so I'm calling out all of these conversations down here on the bottom these are important to do and that'll help and the project with the enterprise it'll help you secure budget and support and resources and ultimately will help the graph become a sticky solution inside your company we've done lots of projects this is just a handful of them we've done you know massive global 360 account views for b2b companies we've reconciled ecommerce data and retail data on a national scale for a footwear company we've got recommendation engines they're currently in production for on mobile apps in production for a cruise line we've done money-laundering event recommendations HR applications all kinds of things ok so I'll stop there with just a couple reminders so how do I get a better understanding of my customers to create more relevant experiences that's really where graphs come into play how can I mobilize and syndicate data how can I get more business value and better insights and really what's the next best action I can take in whatever my business context is and graphs will speak very well to all of these questions so with that I'll stop we have time for probably just a few questions and then lunch is upstairs so if you have any questions fire away it is yeah and there's and there's a set of very good connectors between neo4j and Couchbase yeah so what I'll do is is I'll generate I'll generate those uu IDs on couch based load and then that becomes the document ID and then that document ID then I'll push farther up and denuo for J that's exactly right yep so the question is is neo4j x' since there's a VM limit on the amount of memory does that limit neo4j scalability so that's a complex question and the reason it's a complex question is because it depends on the graph design and the data that you're going to put into your graph in my practical experience I've yet to run into an actual VM imposed limit on the work that we're doing with graphs I've built graphs that have billions and billions of nodes and and have only exceeded the terabyte size graph a couple of times and so one of the things to keep in mind is that graphs are very compact they don't have any nulls you only write data where you have data and and then if you have a thoughtful graph design you're gonna pick data that you want that you want to have in the graph so you wouldn't so for example digital data I'm probably not going to build a graph that has every single impression in it but I might a granade my impressions and spark and push those counts up in the graph but I may keep the raw clicks for example and if you're really concerned about scale and there are some government agencies out there you can actually go get yourself a a 500 terabyte physical machine from IBM that'll run neo4j beautifully and I know a couple of instances like that so but what I so I don't I in practical terms I don't see any scale issues what is coming around the corner though is that there is discussion around how neo4j can shard and then there's also a lot of discussion around how you integrate with spark so you can do OLAP queries on say subgraphs we use relational for certain kinds of data sources or if I'm building a data lake for example it depends a little bit on the client right but like snowflake for example is a is a server less relational database right just like redshift right so it just it depends but what we typically do is that we will always be consuming data out of relational databases and there's great connectors for doing that it tends like sometimes you know we still have to support legacy applications yeah yeah so you absolutely yeah but in Pratt and practical terms you usually end up having some sequel running around yeah other questions you can tell me your question I'll repeat it so the question is if I have internet scale data how does it do the loading so there are there's a bunch of technical documentation on the neo4j site that will tell you exactly how that works neo4j clustering uses what's known as a raft raft algorithm that requires an odd number of consensus servers to execute the right and then in their clustered architecture you can have as many reed replicas as you want hundreds and so and yep so so neo4j is an in-memory graph now there are strategies if the data is is physically larger than your VMs and there's and then you haven't been able to design your way around that what you can actually do is you're gonna warm up parts of your graph so you can have a much larger graph sitting on disk so an example might be I have a global I you know I have a global footprint and I have data for a pack I have data for Europe and I have data for the US I could have all of that data sitting in a single graph on disk but then in turn as far as my applications are concerned I might have only though I might only have warmed up the US part for my US load and so forth Europe for Europe a pack for a pack and the bolt drivers will actually allow you to route to specific servers to get to that in memory cache so that's that's a typical strategy alright so why we stopped there if you have any other questions feel free to approach me lunch is upstairs thank you [Applause] 