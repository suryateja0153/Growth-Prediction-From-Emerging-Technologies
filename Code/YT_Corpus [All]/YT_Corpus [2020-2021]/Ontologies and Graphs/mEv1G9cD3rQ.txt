 CBIIT. Want to remind everybody that today's presentation is being recorded, and it's going to be made available on the CBIIT Speaker Series wiki page. And you can find information there about future speakers and also by following us on Twitter at our new Twitter handle @NCIDataSci. Today I'm very happy to welcome Dr. Joyce Niland, who is a chair and professor in Department of Diabetes and Cancer Discovery Science within City of Hope's Diabetes and Metabolism Research Institute. And the title of her presentation is different than what I have on my page, but that's fine. It's "Iterative Interaction Enrichment of NLP Queries Shared Across Institutions." With that, I'll turn it over to Dr. Niland.  Great. Thank you very much, Tony, and thanks everybody for dialing in today and being in the room. Really appreciate being able to present to you this topic about -- iterative interactive enrichment is what we've coined it, IIE, for natural language processing and how we can share it across institutions. So with that, we'll get started, hopefully, if we can move the screen. This is the issue we were having before. There's a little delay. There. Okay. So as an overview of today's talk, first I'd like to go over a little bit about the critical need for codified clinical research data that we all are facing. And then we'll talk about the IIE, iterative interactive enrichment process, our study design in evaluating the IIE, the results that we've obtained to date, and some future plans. So let's talk about this critical need for codified clinical research data. There's a couple of articles that I found that I think really speak to this from the patient's view and from the scientific view. From the patient's view -- this was an LA Times op piece done by Laurie Becklund, who was a Times staff writer at the time, February 2015. Also a Fulbright scholar. Very bright woman. And she wrote a piece called "As I Lay Dying," and I'm just going to excerpt a few sentences from that. She said, at the time, "I am dying, literally, at my home in Hollywood, of metastatic breast cancer. The medical establishment tells me I have 'failed' a number of therapies. That's not right: the establishment and its therapies have failed me. We now know that breast cancer is not one disease. What works for one person might not work for another. We are each, in effect, one-person clinical trials. Yet the knowledge generated from those trials will die with us because there is no comprehensive database of metastatic breast cancer patients, their characteristics, and what treatments did and didn't help them." She went on to say, "In the Big Data-era, this void is criminal. Even the tiniest companies can see how much stock they sell, compare themselves to cohorts, review history, predict trends. Why can't we create such a database for cancer patients so we can all learn from patient experiences and make more educated decisions on what treatments will extend and improve lives? We must create a new system of data collection and an open, online, broad-range database about histories that will provide information invaluable to those who've been given a death sentence. Patients as well as doctors must contribute. It would come too late for me," she said, "but it is possible." Laurie died on February 8, even before this piece was published in the LA Times. The other topic from a scientist's point of view is from a colleague of mine John Quackenbush. He wrote that "Genomics can provide powerful tools against cancer -- but only once clinical information can be made broadly available." And I strongly share his viewpoint. We have the genome now, but what good is it without the phenomic data, the clinical data, the treatments, the outcomes? He went on to say that the "draft sequence of the human genome in 2000 led to enthusiastic predictions of dramatically changing cancer treatment. There's been a huge drop in sequencing cost, a few hundred thousand dollars, and time, to about a day, since then. Yet expanded data generation has not transformed medicine to the degree expected." And the major contributor, he contends -- and I agree -- is that we have a "failure to deal effectively with capturing and sharing clinical data on large samples." So we have this critical need for codified clinical research data, and yet we're really struggling with this. We have the cancer registries in place around the country, and they conduct high-quality data collection on all patients treated at cancer centers nationwide, which is great. But there's still a lack of the additional disease-specific data points and codified phenotypic data that we need for comprehensive research. Some examples. I used to oversee the cancer registry at City of Hope. They have a great standardized system, and they have coded data on all the types that you see here. They stage very well. Surgery and radiation types, chemo types and very global terms -- single-agent, multi-agent -- hormone therapy and endocrine therapy, transplant. But, on the other side of the coin, everything you see on the right side here is only in text: the demographics, the imaging, the labs, the specific agents, and the treatment details. And recurrence is only followed, the initial recurrence, and not required to look at all recurrences, so very difficult to do research on that. I'm here now at the AMIA meeting in San Francisco, the annual summit, and I gave a talk earlier in the meeting about another NLP project we're doing to -- excuse me -- to try to find recurrence in the notes, which is also very difficult. So we need methodology to support cancer research to try to capture full clinical data, to support outcomes, patterns of care, tissue correlative, genotype-phenotype analyses. Typically, clinical trials have great case report forms and standardized data dictionaries, but all these other types of research which are so important suffer from a lack of this type of data. So we could expand and enhance the cancer registry by adding more detailed coded data elements, although that's a lot of time and effort. At the beginning of AMIA at the opening talk -- it was given by Dr. Greg Simon. He was the president of the Biden Cancer Initiative, and he spoke about they're having an initiative now for cancer data elements and are trying to really encourage cancer centers around the country to adopt these common data elements, which is great. But we all know it's very difficult for physicians and caregivers to stop and code data, and it still is often expressed in text even today. And so my topic and kind of passion right now is to try to deploy natural language processing to help codify these largely text-based EMRs. It's been estimated there's still 80 to 90% of the information is in text and dictations, reports, outside reports. So sort of the holy grail of EMRs would be to have coded data at the point of care, but we just can't get there yet. But the idea -- what we have currently is unstructured data, scanned documents, use billing codes, and so it's kind of a mess. And what we're doing is trying to take free text dictations and turn it into coded data. If only we could reach the desired state where the data are coded from the beginning, then you can turn that into structured text that would be human readable and consumable. But since we're not there, we need to use things like NLP. So that leads us to the Iterative Interactive Enrichment Project that we've worked on. This is a joint project with the Huntsman Cancer Institute and City of Hope. And the overall objectives -- we also are working with Linguamatics. That's the software tool for NLP that we adopted at both of our centers. And we are trying to develop and then export NLP queries across institutions for their joint use and evaluate the extensibility and portability of these shared queries. And then we wanted to assess and document the impact of this new process we call IIE on the completeness and accuracy of information extraction and try to record what are the most critical NLP factors that impact those results and then measure with the relative value added by the IIE process. So iterative interactive enrichment can be demonstrated like this graphic. Initially, the queries were developed by Huntsman and Linguamatics working together. In the use case, I'll talk more about immunohistochemistry that typically is reported in the pathology reports. We were looking at hematologic malignancies. So I heard about Samir Courdy and his group developing these queries with Linguamatics, and we were interested in also doing such work and queries. So we said, "Well, why don't we share? What if we first adopt your queries and see how they work at City of Hope?" And then we compared our results to a gold standard. I oversaw the National Comprehensive Cancer Network outcomes database for about 15 years, and so we have a highly codified, very well quality controlled data set that we can use as a gold standard and measure the precision and recall, calculate differentials as we move through the iterative phases, and identify any incorrect results and what enhancements can be made. Then it went back to Huntsman, and they compared the query to their previous results and saw what they could change to calculate a differential and continue to improve and look at a random sample to determine accuracy in their subgroup. And then we all came together to share the results, and we each time would decide if an additional iteration was warranted or we had pretty much optimized our queries. So, again, the NPL queries were built with Linguamatics. Platform is called the I2E. The use case was immunohistochemistry marker results, positive/negative, from pathology dictations for heme malignancies. We limited our evaluation to 15 specific markers on malignant NHL non-Hodgkin lymphoma patients, and part of the reason was we had this gold standard NCCN-NHL database. And then we applied this interactive iterative enhancement process to continue to improve the query performance for I2E. So here's the study timeline. We have the queries all defined now. We've incorporated ontologies and completed five iterations of the project, starting from 0 up to 4. Again, starting with Huntsman, then getting it to City of Hope to deploy, and we give feedback on the results. And then then we use some interactive WebEx sessions, like today where we could actually exchange some data and information and false positives, false negatives, try to improve. And then we did further iterations on the data at Huntsman, and finally, we added the UMLS and ontologies in the last iteration. So for the evaluation and the study design, we trained the data on 123 NHL patients with curated marker results that had been recorded previously in the NCCN database. And Huntsman trained their data on 53 randomly sampled NHL patients, and their data were evaluated, then, by an expert abstractor, a subject matter expert, or SME, from City of Hope. So the same SME was used at City of Hope and Huntsman, so we had that consistency of the accuracy in assessment. And I kind of went through with that graphic that we already did these phase zero through four iterations. And, at first, we used WebEx sessions, and that was really useful because we could actually visualize the data and the findings together. The Linguamatics query itself -- I'm just going to say a little bit about it. It's complex, but that's the point of this research, is it's very complex to develop these. It's almost an art to get a query right and to optimize it, and you need clinical expertise. You need the programming expertise. You need the informatics expertise, so it takes a team, and it takes a lot of difficulty. So we're hoping that sharing those queries will be possible so that not everyone has to develop their own queries. The ones that we developed contains 14 single queries linked together before we could find all of the different stains and immunohistochemistry and different results. So it was a multi-query approach. This is just a screenshot of some of the query statements. Don't expect you to read all that but just the impression that it is rather complex and convoluted to get these right. This is a screenshot of the actual I2E queries as you build them and how they look as the expert is trying to link all this together and find the right information. So what did we find? So our query results are -- here are some examples. Here's one. I'm going to give you some of the difficult examples where things were not quite right. So this was a no marker hit. It had a marker present, but it was unable to pick it up. And it's probably this statement here. It turns out that even capitalization, dashes, various forms of grammar in this very expressive free-form text can be difficult for NLP to pick up and to find the right answer. You can train it. You can make it better. You can improve on it, but you have to find all these nuances and incorporate them. In a second issue, so here there were clear I2E results in the beginning of the record, but it just was not picked up. In the second example, the text in the interpretation section had unfamiliar keywords for the results, like "show numerous large stain cells staining for CD20 and cells stain for CD3. So those things had to be incorporated as well and make sure that language was familiar to the queries. Here's another example. Another thing they found in this use case is that sometimes the query would come up with both a positive and negative result for the same marker, which is clearly erroneous. And so we had to deal with that, and we treated it as an error when we calculated our F score. But this example here -- it has the term "scattered small lymphocytes positive CD3 colon negative," so that's the confusing phrase where -- on this particular case. And the tags of the text are showing up as both positive and negative at the same time, which, of course, cannot happen. So the -- what are the results? How did we do? Well, the great news is that both centers did really well. Here's Huntsman on their 53 patients, and the F score started out pretty low, 0.17. And 1 is, you know, optimal, ideal, so you want to get as close to one as possible. Precision was good, so picking up the accuracy was high from the beginning, 0.93. Recall was rather poor, so being able to identify all the cases was low and find all the information. By the second iteration, they did quite a bit better, 0.8 and 0.42. Their F score climbed to 0.55, and the -- that's the second and the third iteration. Phase 2 it's called. Is, again, 0.8, a little higher recall, little better F score. Then it climbed again a bit at 3. And, ultimately, in the fourth iteration, which is when we decided that we had optimized as much as possible, we were at 0.92. So that was very positive results for Huntsman. Now, what happened at City of Hope when they shared the results with us? Well, here's the really good news, is we were able to use it quite well also. We just picked up their query initially without any changes, and we had a 91% precision, even higher recall than Huntsman -- and I'll tell you why I think that is in a moment -- 0.3, and an F score that was higher in our first iteration, 0.45. So taking their query that they developed on their patients and their pathology reports and just exporting it to us, we had a great result relatively speaking. It really -- it performed well. Then when we started the IIE process, we climbed pretty quickly, dropped a little bit in precision. So sometimes the changes that you make can actually be detrimental, and you have to fine-tune and figure out what have you now knocked out that you really wanted to be in. And our F score was about 0.7. Then we got to 0.8 across the board pretty much in recall and F score 0.9, 0.89, climbing up, and then ended up pretty similar to Huntsman 0.9 precision. 0.8 recall, a little lower F score, but a good result overall adopting their technology. So the other way we look at this is some graphical presentations. The precision is shown here plotted against the recall, and you can see the red dots are City of Hope, and the blue dots are Huntsman. And you can see that we're moving in the right direction. As we move up this graph, higher recall, higher precision as we go along. And then the other way that we looked at it -- and I thought this was clever. One of our staff scientists, Rebecca Ottesen, came up with this graphical display that just kind of gives you the impression of what's going on. So it's called -- I'm calling it a heat map. We're adopting the heat map type of graphical approach, and this is for City of Hope. On the left side are the 15 different markers that we looked at. The top half of the graph is iteration 0. The bottom half is iteration 4, and the true positives are in blue, the false positives in red, false negatives in this paler pink. Any conflicts -- that's the positive and negative together -- are kind of in the purple, and then N/A means there were no marker hits. It was not collected on that patient. They did not have that testing. So you can see you go from a lot of pink up in the top, some red, some false positives, some negative results. Not as many blue as you'd like to see. And then it moved down, and by iteration 4, you've got a lot of blue scanning across the graph so that you can see visually that the results are really improving. And then if we look at the similar heat map for Huntsman, they had a smaller sample size, of course, but similar pattern. A lot of pink and some red at the beginning, some negative results, and then moving into much more true positives as you go through to iteration 4. So the overall results, then, is that we saw an F score result improve after each iteration. And I really facilitated this, we felt, and working with Linguamatics was very helpful. They had their experts available to us to consult with what we were finding with false positives and true negatives and help us fine-tune the queries. We did experience that drop in precision at both institutions at iteration 1, and that was due to some initial query modifications that we did. Sometimes if you have the wrong distance between sentences or that may have the information of interest, you can lose some precision at that point. Iteration 4 Increased recall at Huntsman considerably while causing a small positive bounce in the City of Hope results. And now the primary difference -- this is an important finding that we believe in the early results between Huntsman and City of Hope. We believe you can attribute to the additional structure that is found in our lymphoma dictations at City of Hope and less structure at Huntsman. And this is one of the take-home messages that could really help NLP queries to be more effectively deployed, is the amount of structure in the dictations that you're trying to evaluate. So here's a typical Huntsman pathology dictation. As you can see, there's a lot of text. The green indicates desired results for B-cell NHL. We were only focusing on B-cell, and so that one sentence there are the results that you really want to focus on. The red font here indicates extraneous results for other histologies that we were not focusing on, T-cells in particular. So in the Huntsman dictation, the NHL -- the NPL -- NLP, rather, query needs to ignore all that red text and find the green text before you've even honed in on the pertinent subject matter for our use case. And it was quite variable from one dictation to the other. They would write it a totally different way pathologists by pathologist, and I think that's why we actually had a higher F score at the beginning when we adopted their queries. Now, here's a typical City of Hope pathology dictation, and you can see that desired results are right up front. They use kind of a reporting template, and they always give the results in the same order, in the same type of language, in the same terms. And there's more of the green text below that also is pertinent, but very relatively little extraneous results for other histologies. So this structure, this result paragraph at the top of the pathology report at City of Hope, really assisted NLP in getting to the right answer. And that would be something to really encourage. So this is just kind of the graphic that shows, again, what we were doing here. We had City of Hope, Huntsman, and Linguamatics, and we developed the queries, evaluated the results, assess the impact, and then we improve and iterate. And that's what we're calling IIE. Then sharing and reuse, and reuse is kind of the most important principle here that would really facilitate what all we need to do in trying to get the clinical codified data. So what are some of our take-home messages, then? That -- I think we've shown NLP queries can be effectively shared across institutions and that the queries initially developed at one site, Huntsman, could -- performed even better initially at the second site, City of Hope. And, importantly, this avoids reinventing the wheel for everyone across the nation if we can do this kind of sharing. Secondly, I think we've shown that the iterative interactive enrichment process where you go back and forth, you look at your data, their data, your type of dictations, the other institution's, and fine-tune and pass the baton back and forth really did help enhance the I2E queries. And so an interactive team-based approach for query development across institutions should be encouraged. It really takes shared communication. At first, we tried just doing it statically, sending back emails back and forth without really interacting together, and we found that didn't work that well until we'd had these interactive WebEx sessions where we're really looking at data together. That really sped up the process for query development, data review, quality assessments, analytic reporting. The gold standard is also very important. We were quite fortunate to have the NCCN, NHL database as our gold standard and could utilize that. We -- it's been audited before. We know that it has extremely high quality, so we had the right answers to compare to what NLP was coming up with. Huntsman didn't have that capability or that accessibility to such a database, so we actually had to have a data use agreement between our two centers. And, by the way, that's how we conducted this. We had an IRB protocol and data use agreement. And then our subject matter expert actually served as their gold standard determinant for their data. They reviewed their results and checked for the positives and false positives and negatives so that we can build their gold standard. Took a lot more effort, though. If you look at time and effort, we had quite a savings because our database was in place already. Took much more time to develop that for Huntsman. And when we added the ontology, we added the NCI, the source. It really did benefit the queries as well. That was in the last iteration. So that was another take-home message that these formal ontologies can really improve the process. And so what will we be doing in the future? So we have several ideas we'd like to pursue, and I'd love to have a discussion with all of you about these ideas. One thing would be, can we increase the structure within text? We saw that this improves the performance of NLP queries, so we hope to work with our EMR team. We just moved from Allscripts to Epic, and so this is a wonderful juncture and opportunity to work with them to build dictations that are more structured than before. And then when you have to apply NLP, because we still have so much text involved in the EMR, at least it should perform better. So that would be one suggestion, is to try to work with your EMR teams to heighten the structure of reports. The other thing that we have thought about doing -- we've done this in our pathology reports to some extent -- is create a quality control feedback loop using the NLP. So as the pathology reports are finalized and looked at, use the NLP to extract the results at that point, after the dictation is created, right in real time within several days of the dictation while it's still fresh on the pathologist's mind, and feed back to them the codified information. So, you know, Dr. Smith, you had this pathology report. Here are -- there's a coded report of your data that you put into your dictation. And then they could evaluate well right away whether the NLP was correct or not. And so you could build your codified information as you go and also improve the quality of the results. Sometimes the dictations are wrong. Sometimes they make a mistake. Sometimes it's picked up incorrectly, so this would have a sort of a real-time quality control feedback loop of the abstracted results. And then a very important concept that I'd like to kind of throw out to the crowd and discuss with you is, should we be creating libraries of shareable queries across institutions? You know, all of our cancer centers across the nation are trying to do the same thing. So if we could develop these queries for -- this is just one use case, as I mentioned. I just presented at AMIA on the current use case, and everybody in the audience said, you know, "We're struggling with that too." So why don't we try to develop reusable, shareable queries and then store them in a library, maybe facilitated and monitored by the NCI? And need to check out a query and reuse it to try to find information within your EMR, your dictations. Granted, it would each time require some fine-tuning, as we have done with the IIE. But it's just a thought to avoid reinventing the wheel, really speed the research, you know, help the Lauries of the world not feel that they've died in vain. And, you know, as John Quackenbush is saying, help to have data available that we can merge with the genotype for genotype-phenotype correlations. So those are just some of my thoughts and ideas about how we might utilize this going forward. I want to thank the whole team. We have a great team in place across the institutions. We have Janet Nikowitz. Served as our clinical subject matter expert. Julie Hom is our senior systems analyst. Rebecca Ottesen is our staff scientist. Isaac Kunz was the NLP programmer at Huntsman, and Samir Courdy the informatics director at Huntsman. And to show how collaborative we were and our, Samir, after he finished this project, decided to move to City of Hope, so he's now the vice president for research informatics at City of Hope. Also, I'd like to thank Linguamatics, David Milward, Simon Beaulah, Himanshu Agarwal, and James Cormack, who were their experts in NLP query development who really helped us with this whole project. So with that, I think for the first time in my life I'm finishing a talk a little bit early. But I'd love to have your comments, questions, discussions. Thank you for tuning in. Any questions?  Great. Let's thank Dr. Niland for a terrific presentation. [ Applause ] So at this time, we'll open the floor for questions. If you're in the room, please just raise your hand. And if you're online on the WebEx, use the raise hand feature in the dashboard, and we will call on you [inaudible].  Great. Thank you, Tony. Thank you, Dr. Niland, for a great talk. This is Shannon Silkensen. I work with Tony at CBIIT. And I really enjoyed your presentation. Was just curious. How easy is it to adapt sort of the query that you did with NHL to either other lymphomas or other tumor types?  You know, that's a good question, and I think it's fairly facile. What I forgot to mention --  Oh, great.  -- is, yeah, that Huntsman -- actually, they didn't develop this just for NHL or even just for immunohistochemistry markers. They developed it for generalized data on heme malignancies, so it went beyond NHL. With the recurrent query that I talked about at AMIA, we started out with a query that we had developed for breast cancer recurrence and then modified it for ovarian cancer. And, you know, granted, there will be modifications and subtleties, but it gives you a good starting point. I think these queries are fairly readily adaptable to different use cases, slightly different populations, et cetera. So that's the good news.  Thank you.  Yeah. Just a bit of a related question to that but on a different tangent. I was fascinated by your suggestion about creating libraries and sharing these libraries, which I think is a terrific idea. But I'm curious about how easy is it to share queries that are developed specifically from a Linguamatics tool with other NLP tools. And I don't know whether you had the opportunity to actually try that or have gotten any input from folks on how facile or difficult that might be.  That's a very good idea. We have not. I know in the literature that some people have evaluated extensibility and across different NLP tools. You're right. We developed one -- used one single product, Linguamatics. But they use similar -- they use similar processes: chunking, tokenization, parts of speech tagging, parsing, normalization. I think most tools do use similar processes. So one way we could think about this is -- and we started talking about this a bit with other cancer centers -- is to develop it kind of in a nontool-specific open language, is to record the terms, the queries, the linkage, the ands, the ors, you know, what you did, and then be able to deploy those within any different tool. But we haven't tried that ourselves. That would be a great experiment.  Great. Did you still have a question [inaudible]?  Yes. [Inaudible]. That's why I have two questions. One was already stolen by Tony about queries. [Inaudible] kind of feature which stores the queries. I'm told that could be [inaudible] between different tools [inaudible] when you can connect types of queries through the ontological [inaudible]. And the question, actually, is what parts of NCI key did you use most?  Mm-hmm. For this project, we didn't have a lot of ontology, but you're right. That's a great point, that one way to connect across the different tools is through the ontology. For this one, we just used the immunohistochemistry ontology, and that was really all that we had. For the recurrence NLP project that I was mentioning, we used the NCI organ thesaurus in ontology for organs. Was quite helpful. As I gave in my talk two days ago, the one thing that it missed, for example, though -- in ovarian cancer, there was one dictation that said there was a recurrence in the cul-de-sac, and that was one term the NCI thesaurus didn't pick up as an organ term. Cul-de-sac -- you think of, you know, having kids playing on their bikes at the end of the street but not necessarily an organ. So, yeah. But that's a good point. The ontologies are very important, and I think that would be one way to link across tools by using a common ontology.  Thank you very much. And I have one more suggestion which could potentially develop in collaboration. You have moved your pathologist in the, you know, project. If you move the ontology, also, beyond the Linguamatics, it can help to enrich the whole, you know, process.  Right. Right. That's a very good suggestion.  [Unison] Thank you.  Jordan.  Thank you. So I have a -- we have a collaboration with the Frederick National Lab at NCI and Department of Energy on developing deep learning model to extract information for pathology reports. And I wondered if you have tried any of these techniques to extract some of it. [Inaudible] believe that we're looking for markers [inaudible] I think they're looking for. And I just wonder [inaudible].  Yeah. No, we haven't delved into the deep learning approach. That's really interesting and kind of a, you know, offshoot of this. So I applaud you for that project. That sounds great, but it's not something that we've really tried. We had more of a just [inaudible] more of a qualitative iterative process, so rather than any kind of artificial intelligence or that sort of thing. But I think it would be a great enhancement. How far along are you in that project?  I've been, like, working for three years, and I've published, I believe, like, five or six different architectures for the model they have been using that show - or the five biomarkers they're looking for that can get them all right correctly for 50% of the reports [inaudible] thousands of reports that will -- that used for further testing, and they will show me that the models can be used across [inaudible].  And we are planning on -- we are planning on implementing that in the registries very shortly. The Department of Energy has implemented an API that we're looking at integrating in the cancer registries this year. Cancer registries. Very exciting.  That's terrific. Yeah. That's great.  One online first. One online first.  We have a question from Melissa Cooke.  Hi. Am I unmuted?  Yes, you are.  Yes.  Okay. Hi. Thank you. Thank you for a great presentation, Joyce. I want to [inaudible] related to your thought about using -- creating a library of shareable queries because you said it really made a difference to have a consistent template that you use. I wonder what your thoughts are about possibly creating shared templates for capturing the data so it's more convenient.  I think that's a --  And do you think the community would be open to that?  Mm-hmm. That -- I think the latter part is kind of the problem. The keynote speaker at AMIA talked about this and had -- Biden met, actually, with Epic and Allscripts and all the different vendors, and, you know, we always hear that, "Well, it needs to be so customized hospital by hospital. And if we change it, it could cause errors." And, you know, you have to wonder about that. I think that would be fantastic. I don't think we should all have to be restructuring how we report information. Templates would be a wonderful approach, but I think it is a cultural issue. It's a vendor issue. It's difficult to get people to share. And, you know, when one hospital gets used to what they're doing, then it is more facile for them, and they don't want to change. Do you have any ideas about how we might speed that adoption or spur that on?  Well, I know it was a huge challenge, and it still is a challenge in biopharma industry to get people to use, like, standardized data collection instruments that are based on a global standard. CBIIT, obviously. Every company thinks that their forms are better than somebody else's. I think, you know, partly it's education. Partly it's, you know, showing the kinds of results that you're showing, that it makes a difference to standardize from the point of collecting the information. And that's where the -- we can shift the equation. We're always going to be in this situation of trying to interpret results and compare things that may or may not be comparable.  Exactly. The other thought I had -- the other thing that physicians frequently say or pathologists say is they need that expression to really capture that individual patient. They need individual expressivity. But what if we had at least a structured, you know, dictation as part of it and then free-form? You know, if you really feel that you need to go on and express some nuances, you can have at least a combination of the two. So I think it would go a long way to capturing some of the codified information that we all need. And you're right, the comparability. I mean, how are we ever going to do big data analyses and combine across institutions and patients if it's all so uniquely expressed, if we can't get to common terms, common ontologies? It really is critical. Thank you for that question. Is there another?  Oh, hi. [Inaudible] Bill Park from NCI. Just want to make more of a comment. I kind of have to giggle when we all try not to reinvent the wheel, but we end up initially doing that. [Inaudible] build a database, search clinical trials, and the biggest problem they had was the unstructured data and how data between each of the medical institutes for the clinical trials was in a different structure format. And you have the same problem with pathology reports and also trying to find a unified ontology to be shared across institutions. So it's like we still keep trying to reinvent the wheel. And not even to mention how we can't share software or across platforms, which is the big area, right?  Right.  You know? I guess it keeps us employed, but it's funny how we all keep running up against the same problem. But thanks. Great work.  Thank you. No. That's an excellent comment, and, you know, it's great that you admit it. We all do it, you know? It's that old adage, you know, "There were so many standards I didn't know which standard to use." So it happens. It sure would be great if we could get some commonality going.  Sort of a two-part question about -- going back to Melissa's point about additional structure. And you mentioned that you have -- that City of Hope has additional structure in the lymphoma dictations. So the first part of the question is, can you talk about how City of Hope developed that additional structure? Was this -- what was the process that got everyone to adopt a more structured approach to actually doing the dictation? Was that an easy process, a hard process? I'll let you tackle that one first. Then I'll ask the second part.  Okay. Good. Yeah. I've been at City of Hope for 30 years, and I've worked with probably five, six different heads of pathology. And I remember at the beginning just beating my head against the wall. The first pathologist, brilliant guy, you know, very bright doctor, but, you know, he said, "I will never make my pathologists have standards. They can't do it. You know, it ruins the accuracy and the impression, and we will not use standards." Come to today. We had our sixth pathologist along the way, and he insisted on structure and templates. And what they based it on -- the other thing that helped quite a bit was the CAP protocols coming out from the American College of Pathology, and so they basically were adopting that. You know, you have to comply with the CAP protocols, so having the structure in our pathology dictations actually assisted them a bit too. So that's where they could see the carrot that this structure may, you know, help your life as well. And mandates. I mean, they just -- he finally just had to say, "We will have templates, and we will all do it the same way." And it's not just for lymphoma. They have it pretty much for every cancer diagnosis that we deal with, which is great for us.  Great. So the second part of my question is, when you compare the results between Huntsman and City of Hope, you said part of the difference may be attributable to the fact the City of Hope does have this additional structure in the dictations. Yet the scores were lower. So how do you reconcile that? What is it about -- is there something about that structure that really the tools, the queries need to be, you know, structured even more differently even though you've gone through the iterations? You couldn't get the score higher with the structured dictation. What are your thoughts about that?  Right. The thing that City of Hope had -- remember I talked about conflicts where the way we express it was confusing the NLP, so it would pick up the same patient, same marker as both positive and negative.  Mm-hmm.  City of Hope had a much higher preponderance of that in our dictations. I think that's what attributed the somewhat lower F score for City of Hope overall. I don't have the proportions in front of me, but we included that in the paper we're submitting. And it's like, you know, maybe 9% at Huntsman and maybe 20% at City of Hope, something like that. So I think that was it. We'll have to look harder at our templates and our structure and how it runs together some of the reporting such that NLP cannot disambiguate between positive and negative on the same marker. So that's one answer. I think it just could -- we could use, you know, a couple more iterations on our side to get it even more fine-tuned. You know, also, it's the sampling. You sample different patients, you might get a different F score. So I think those all are factors. But it's a good point, and there's still a bit of work that could be done.  Do you think that one possibility might be to change behavior in the dictation itself to avoid the sort of conflicting results? And is that even possible?  Right. That would be a good approach. If we can identify the structure that causes these conflicts to be picked up and get compliance and cooperation from the pathologists that they would then modify their template slightly to avoid that, I think that would be the way to go. Exactly right.  On top of Tony's question, it's Bill Park again. Did you use any normalization software? Like, if you have a continual way that you're -- at Huntsman, pathology reports are dictated. Perhaps you could use a normalization software that would recognize the negative in front of, you know, the next set of terms in the histology report.  Right. We didn't use normalization software per se, I2E, the Linguamatics tool, has various components within it so that once you identify a pattern like that, you can modify your query. You can build your queries to make sure it says if this occurs before that, occurs after that, or within this distance or within this chunk of text, then, you know, interpret it this way. So I think I2E has enough tools within its tool kit that would allow you to handle those situations once identified.  Thanks.  Thank you.  You mentioned a bit at beginning the -- you know, the Biden Initiative has been pushing this -- advocating for this M-CODE approach. What are your thoughts about that [inaudible]? You think that that would be something that would be useful if there was a small number of these agreed upon data elements that were captured in all systems? How -- what impact do you believe that would have on this?  I think that would be great. I don't know enough about it. I must admit I just heard about it at the keynote address, and I haven't had time to investigate it. Maybe you or somebody in the room has a little more knowledge about what they're trying to do. My only concern when I heard it was, again, we have so many standards. We don't know which standard to choose. I just hope that they are looking at what's already out there, you know, AACR and cancer registries. And, you know, there are a number of standards for cancer to adopt those, make them very utilitarian, and very ubiquitous across cancer centers would be great. I have a memo that I keep in my desk that I'm never going to throw away. It's -- when I started working at City of Hope 30 years ago, and I asked the head of our cancer registry and our data managers, you know, "What would make it easier for you to code data on patients, whether they're on a clinical trial or we're trying to do some outcomes research?" And she wrote me a memo, very thoughtful, with about 20 data elements in it that -- she said, "If only we had these coded at the point of care -- performance status, stage, final diagnosis, you know, about 20 things -- our lives would be so much easier. We could go after the more difficult, subtle data. We'd have the core. And you know what? We still don't have those 20 data elements coded in the EMR today. I think -- and I think she was right. If we can get this corpus -- a bolus of data elements coded that we all do in the same way, it will greatly facilitate research. Then you can tackle the more difficult things that are always going to require some human adjudication.  Right. [ Inaudible Speaker ] Yeah. I think that that's my -- that's my impression about this M-CODE initiative, is exactly what they're trying to do, so --  Good, good. That's -- I think that sounds like the right way to go. Yeah.  I just have one final question. This is maybe more of a future thinking thing. But just from some of the dictations that you actually showed, it seems to me that given the right set of artificial intelligence tools, one could extract from that and actually structure the data upfront, rather than going in afterwards. At the time of dictation, capture, I mean, all of the different CD -- CDs that you mentioned in -- that were mentioned in some of these. It seems you could actually structure that right at the time of dictation. Have you heard anyone talking about that? Has Linguamatics mentioned that? Have you investigated any other companies or researchers who are looking into doing that right at the time of dictation?  No, I haven't. Let me see if I understand you correctly. So you're saying allow the free-form dictation but then use AI tools to immediately structure it and to --  Exactly. Put it into some very structured form right from the very beginning.  Right. I haven't, but that's kind of similar to that other idea that I mentioned of the quality control loop.  Mm-hmm.  And it's a pretty similar concept, I think, where you would feed it back to the dictator, to the physician, to the pathologist right away in some kind of structured format and have them make sure that they QA it and that it is what they did intend. So I think it dovetails with that idea that we would, you know, turn it into structure as soon as you can while -- and while it's, you know, freshly dictated, and make sure it's correct. We've used those feedback loops with the cancer industry too for some of the reporting that they have to do now for quality assurance. We have given them feedback loops right away, and they do their cancer registry work so that they can see if it's correct, and they don't have to go back later. And it doesn't decrease the score of how it looks, where the patients are being treated and their morbidity. So we want to get that right from the beginning. So it's a good idea. I think you should pursue that.  Right. Yeah. [Inaudible] That makes a lot of sense. [Inaudible] coupling it with that feedback loop, I -- make a lot of sense. But, again, that's going to require some more social engineering, I suppose, to get them --  Right. Right.  -- get them to adopt a new protocol. But, you know, if we can demonstrate the, you know, the benefit on the back end and really the quality of the dictation itself, the quality of the results, the ability to be able to now look across patients more easily -- you know, I think we all know what the projected outcomes are. It's a matter of demonstrating that value. Should be the carrot that would get people to adopt -- to adopt something like that.  You remind me of an acronym that I've used in many of my informatics talk, which is HIT, and it stands for the human element, the information element, and the technology, the H-I-T. And then I always ask the audience, you know, which do you think is the most difficult aspect of doing informatics? Technology, easy to conquer. Information, very challenging. Human, that's the most difficult of all. [Laughs]  Great. Okay. Well, it looks like there aren't any further questions. So I want to just let you know that our next presentation will be on Wednesday, April 24, and our presenter will be Dr. Gordon Harris from Harvard Medical School who will discuss NCI-funded clinical trials, imaging informatics, machine learning, and open-source web viewer technology. So I hope you can all join us for that. Thanks to everybody who joined here today here in the room and online. And let's give Dr. Niland thanks again for a terrific presentation and discussion. Thank you. [ Applause ]  Thank you.  Thank you so much, Joyce.  Thank you for inviting me.  Enjoy the rest of the conference.  Thank you. Bye-bye.  Bye. [ Background Conversations ] 