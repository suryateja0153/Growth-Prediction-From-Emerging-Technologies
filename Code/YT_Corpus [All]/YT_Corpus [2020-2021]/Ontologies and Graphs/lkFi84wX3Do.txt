 hello Rob hi I am the you'll have to forgive us we didn't know we were giving this talk until yesterday so it's a little bit last minute a little bit thrown together so please be patient so I'm just gonna set the stage a little bit and then I'm gonna let the real engineers over there tell you about how it all comes together this is kind of a mission statement if you will oh gosh turn off notifications okay so the world is full of knowledgeable insightful incredibly wise people who spent their entire lives in some cases learning their skills applying their skills refining their skills over time and many of those people are in this room and the rest of the world includes those people as well and between us all we effectively possess the entire body of human knowledge insight and wisdom if humanity were to go to stink tomorrow all of it would be gone unfortunately we don't have any way we don't possess the technology to share our knowledge our wisdom with each other directly I can't copy the things that are in my brain to your brain so in order for us to share our knowledge we have to communicate and when we to communicate we've invented countless ways right we went from speech to written language to encyclopedias books radio TV all of this right all just to share information share knowledge share wisdom about how the world works humans are a creative species but we didn't do this just for fun right sorry we did it because communication is hard and communicating at scale is especially hard speak up it's hard because we don't share the same backgrounds we don't share the same education we don't share the same experiences the same perspectives the same biases etc so we all have to start from scratch each time and when we share we reduce our knowledge it gets degraded because we can't share with high-fidelity right our knowledge becomes a text document that gets sent off to a publisher and is put into a book and all of the nuance and detail and interconnectedness that is in our minds does not come through in those communication mediums so what do we do we spend countless hours reading text right trying to master a topic you have your 10,000 hours right to really understand something because all of this knowledge is hard to share effectively and so we kind of think maybe it shouldn't be so hard right maybe one day we will possess the technology that allows me to copy the ideas in my mind and share them with someone else maybe one day we'll get there it's going to be awesome but until we do isn't there something that we can do to take all this information and make it more accessible more available for people to use every day that is the driving motivation behind Hume what we are here to talk about today it is the culmination of two years of research and development and we think it is going to dramatically impact how we are able to interact access and act upon information knowledge and wisdom so I know this is a technical talk so I'm not gonna give you too much philosophy I'm gonna hand it over to my colleague here and he's gonna tell you how this stuff actually works thanks Rob so I would like to start with this image that I'm quite sure that we have seen before because it is quite recurring and a lot of blog posts and other presentations and I really like it because it shows the evolutionary path that transforms data in something different in something more interesting that could be insight wisdom that could be actions so because we took it a lot in they say last five years about data-driven application but they say that data by itself in its original nature is just a useless because it doesn't provide any real value for a company because it is sparse it is distributed across a lot of data series or data sources and it is unstructured where unstructured doesn't mean exactly that is referring to the text but even just having a lot of the zeros each of which has a it's a its own data structure means that at the end you have to deal where with a lot of structures in your data so this means that at the end you have a lot of issues in managing these a variety of your of your data and this is at the end a chaotic situation in which you cannot get anything from this are really just converting these data collecting them and organizing them you can get information but if a measure by itself is not so useless compared with with knowledge and migrating from information to knowledge is a hard task it requires a lot of effort because it is a quality change because of knowledge it is connected information so you need to connect dots to obtain knowledge from from information but once you get knowledge of course you can assign meaning semantics to your to your data to your information and it is at this point that you finally can get insight you can get wisdom from your data so starting from something let's say completely useless you can achieve a situation a status in which you get real knowledge and at some point you can move further and extract insight and wisdom and from wisdom you can that either I mean related information that connector let's say data and extract other knowledge you can drive your company because for instance you can let's say deliver a better product analyzing your production chain or you can deliver better services to your end user you can save costs for your for your company so we got this idea in mind we designed the Yuma let's say processing pipeline because here you can find out what is that the process of converting data wherever you have them in insight in wisdom and then threw them into concrete action for your company the first step is of course ingesting data from your data sources so extracting them pre-processing in some way from wherever source you would like to have them and start organizing it so starting having that is the kind of information instead of pure data of course we will focus on non the textual data that are generally considered as an unstructured data but this is not true because text or languages you prefer has a lot of structure related to grammar to lexicon to the relationship between words and to the other kind of constructs that are beyond the text so what you need to do and this is the second phase has to extract the hidden structure of your textual data and organize them in a way in which a machine can process it and but this is not enough because sometimes the data that you have is not enough for making any kind of process or it's not let's say the right set of data that you need so the enrichment process allows you to extend the knowledge that you have about your own business sir introducing external sources that will allow you to get more insight at the end of the process about your specific knowledge and you can do this using as a set external knowledge but at the end you can even just process the data that you already have the information that you gather in order to extract other knowledge I will give you an example supposing that you are analyzing different documents you can compute similarities between them and this is not outside of your knowledge is already inside you are just processing it to extract a new information that are and in this case similarities between documents but that will be useful for again navigating your data for instance your knowledge once you have this knowledge and we will see what is the problem about knowledge because you have to store and manage this knowledge you can start applying some machine learning tools that will give you real insight abroad about your your data at the end and of course you need you have to index in them because you have to provide some kind of access to this data and at that point you are ready to deliver insight that could be as I said new a kind of report for the managers or could be a recommendation engine or could be whatever you think would be useful for your specific business to do with your initial data so but in this evolutionary path there is a big problem because we talking about knowledge but there are two main problems related to knowledge the first one is of course how we are going to learn a new knowledge we are going to construct the knowledge is starting from the information that we can gather from our data but the second problem and we will see it is quite a big problem is all we will represent these this knowledge let's start from this point because you know who of you working with machine learning knows exactly that a representation is one of the big issue in in machine learning is one of the most complex and the compelling tasks in machine learning because the representation or a specifically knowledge representation means how you are going to organize your information so that's an autonomous system that could be a machine learning agent or in general a machine can perform its complex tasks on your own on your knowledge so the way in which you will actually model your knowledge will affect the quality of the end results that you will get from the from the knowledge that you get so in human specifically we decided to use a knowledge graph as a way for representing knowledge but we use the let's say the broader concept of knowledge graph because we don't like to consider knowledge graph just as a way for representing RDF in a let's say in a property graph here we consider let's say knowledge graph as let's say a set of interconnected entities with their related properties this is a very broad concept on knowledge graph because this is what we would like to get at the end just a way in which we can represent a different type of relationships between items in your in your information and so if you are in mind what we saw at the beginning and the image in the central image this is no more than this the knowledge just connected the dots that will give you sense semantic meaning of your information and in this evolutionary path knowledge graph plays a very important role because it lets say represented the core of the a enabled evolutionary path because all the things that we will build on top of our knowledge will happen on top of our knowledge graph then of course as I said before we have to create these this knowledge graph and it is where they say the fun part will come in and as you can see in this in this diagram the idea is that we would like to have these knowledge graph that starts just from the information that you can get from your textual data or even structure structured data and grow this knowledge graph at each process that can be I mean not only one of these but we can repeat again and again because this is this is an iterative process and what you get back can be used in different ways you can export for instance a vector in different ways for processing these vectors using other machine learning tools or you can export a different type of documents that are views of the same set of knowledge that you have and store them in elasticsearch for providing some kind of I don't know enterprise search engine or semantic search or you can even visualize them and so helping people to have a better clue about the data that you have just because you connected already the dots for them and they can go over your data set and see with their brain something that any other machine learning tool can provide them so at the end let's say that what we get back at the end of this evolutionary path is a graph like this of course this is a very simplified version but what is important to notice here is that what you have is that your knowledge now has a lot of navigational patterns that you can use because once you start storing your knowledge as a knowledge graph every single point and every single the relationship could be for you an access point for your analysis for your query for your investigation because for instance so you can start from a topic and say okay what is the generic the general sentiment about this topic in my newsfeed for instance or whatever or I can start the navigation from equipment and say okay what is the generic sentiment or tartar the documents related to the equipment so this is what we would like to achieve and this is what Christopher will show you in details how our platform provided the right set of tools that allows you to easily start from your data and achieve a knowledge graph like this that will be the entry point for any kind of further analysis for extracting inside and taking at the end the right set of actions for your own company Chris so I will not speak about ingestion and for example indexing into elastic search I will really focus on natural language capabilities and which meant and the type of machine learning algorithms you can apply on the resulting graph once you start ingesting data the way you might represent it after a first pass of natural language processing is a simplified version like this so you have your documents an annotated text that represents actually the entry point of the natural language processed view of the same document to not interfere with your original domain and then sentences of the document and entities and words dug are tokens for example and entities there is one very important aspect in this is the Calla T of your entities because entities are representing real-world concepts like persons organization but of course you can rely on probabilistic models so there is no 100% accuracy on entities so you will always have wrong results this is something that will always happen the problem is that the quality of your entities will determine the quality of all the further steps you will apply to your graph so if something is recognized as a organization instead of a location in the wrong context an example is Amazon in one sense it can be a company in another it can be the forest right so if you have the wrong entity the graph analysis you will do after or the usage of this graph for providing to your user applications for example will provide problem so this is why the Calla T is important so by default natural language processing tools provide general entity recognition models like person locations organization money number also dates time etc those are very generic because they apply in every domain in you we have the capability to actually train your own models based on your domain this is very important because if you are working in insurance industry you want to recognize a cause of an accident for example or if you are working in finance you want to recognize startup names or company names more efficiently than with general model Healthcare is also a very specific domain because drug in one part of healthcare is good while in other parts of health care it is bad right so this is very important to have great entity recognition models so we use actually the capabilities of Stanford NLP based on general entity recognition model but also the ability to train your own model however you always have noise so we actually found a technique by combining deep learning based model like motivic to remove the noise based on the context of the text in which the entity is appear so to give you an example how what avec word is just returns you a vector representation of a word or an entity because we have this vector representation we can apply mathematical operations on them so we can apply cosine similarity for example which is a well-known similarity argument for computing distance between vectors so we can remove actually words or entities that are not relevant to our context so the advantage with this is that it accomplishes state-of-the-art precision like on like 99% on very complex domain models musical instrument it doesn't sound like but it is a very complex domain to recognized entities actually so I mean explaining the full details on this would take me two days maybe so if you go to you know GA slash NER we have written an article that explained in details the whole process how we achieve this accuracy for named entity recognition to give you an example of applications why those entities are important you always need an information need at the end right so what is the problem you want to solve here an example might be I want my user to access document very easily so here we extract entities that represent locations in the documents or geographical low latitude longitude points we when it is a location we can use geolocation API is like Google API is for example as Saudis in the graph that means that if you use a click of the map it is very easy now thankfully with no participation to find the document that have mentions of entities located around I don't know hundred miles around the click for example this is very easy and it offers actually a very easy way for businesspeople to tree tree document or your knowledge workers etc entity recognition is not the only problem or not the only way to enrich your graph there are other ways you can use of external knowledge bases like wiki data like concept net 5 but not only we will see there is not only text so we can it offers the ability to and which entities find that Amazon is a company that it is a retail company etcetera that and concept net 5 offers us the ability to understand that secret is a is a device or that San Francisco is located in California for example so you can actually create new connections into the graph between your documents because if you take for example document with San Francisco a document with Los Angeles there is nothing even with entity recognitions that relate them however by using external knowledge you can create a new connection with California United States etc so you will automatically and with your graph and have more connected data and more knowledge about your documents by using this so we arrive to a graph approximately like this so we see that tag tokens words can be recognized as a device or city that is located in a region equipment that is part of another equipment equipment etc but if you here it is a representation of one document but if you bring that to 100,000 a million documents you have a lot of connected data in the ability to provide lot of services to your user only based on the graph analysis of your documents in terms of enrichment with wiki data for example we can so we have created procedure a top neo4j we can just call a procedure you get back that the enrichment so for example an on mask works is one of Tesla Tesla is in the car industry or pentagons is actually the real name is the Pentagon and it's located in Washington for example but as I say this is not only text if you take articles that you are reading every day they contains also image so you can add additional metadata to your graph like if you use image recognition platforms like IBM Watson recognition or Google Microsoft computer vision or something like this you can actually get back results of the recognition like electrical device black or mekinese machine etc and add this information into your graph to again provide better access patterns to your user but also more connected data more ability to provide relationships related documents similar document etc to your user however again the carroty of what you will bring in your graph will determine the quality of the applications you will build so if you go one step down you can go by a Google home and take a picture pass the image to I don't I be my visual recognition because this is the one I use here I'll get back that this Google home actually is recognized as a soap dispenser which is very funny so I wouldn't be proud to be Google home product designer right so so I mean this cannot go into your graph right again the Calla T of entities or the quality of additional metadata will drive the cavity and success of your application so this is why we have used again deep learning based model like what effect but not only there is also growth models fast X model to actually filter out the Labour's written by IBM Watson or other services is an example with the whole document that you are of your entity or concepts etc so it will remove out of context words or results from external basis and this can be applied also with wiki data because there is a great example sometimes we stand forth it can happen that north will be I mean will be recognized as a location in the text then you will try to pass north to wiki data that will return ten results ten possible results finally the first Norv the first search results from wiki data that will be a location will be Republic of Macedonia well from North so again you can use other techniques like string similarity matching for validating the result or what to make for example so there are a lot of more of techniques that you can use to enrich do more into your graph so there is dr. vac press emeriti so what is dr. vac instead of having a word vector you will have a document vector so you will be able to apply mathematical operations like cosines ability to compute distance between documents distance of meaning actually and if you store that into the graph like the top500 for documents or the top hundreds related document this is very easy because you improve then your search or recommendation engine because of the graph structure you don't have to fill to compute the similarity or to compute the traversals on the flight you already know who are the top hundred related document so you can see here you can just store a relationship between two documents or two concepts and know what is the similarity between them topic modeling very important it's offers you the ability to actually cluster your documents but in an ER supervised manner so you don't have actually to have a label data they can analyze your text and and cluster them between oh if you look at the map they will make groups of them based on meanings so you can see in the top left topic nodes in the top bottom left another topic nodes so if you only look at this graph by adding additional information you have again new ways of providing relevant results to your user again this is not only search this is not only finding related document this is really important in those type of applications to have your business or subject matter experts persons to be involved in the whole process because they will need to mind actually which what is useful for them to use in the graph and much much more so instead of documents we can use also paragraph to vac which is on the paragraph level we can we use also doctor vac techniques for building classification models like sentiment polarity analysis which is sentiment analysis which actually solve a big problem because once you start using sentiment analysis we stand for the performance drops completely so you can use actually like built we stand for a subset of data to actually have build a model with doctormick and then we use dr. vague to have a faster sentiment analysis model what I'm speaking about faster the times are crazy because once you enable sentiment under resistance therefore or modest-sized documents you can pass from two seconds to 22 seconds for document processing because this is a complex task okay you can start a new project cluster going to three days Avila is a lot of data build the model with dr. vac you use this model so you will have approximately the same accuracy than Stepford but you will get result in seven milliseconds so once you want to actually go to scale or a doctype lines this is really important to know the tools you are using and important to have actually composable components to be able to solve units because for example customer support for Airlines they cannot afford 22 seconds for document right it should be fast paced so again imagine one document what we can extract from only one document air which compare but take that to two documents this is already a huge knowledge graph take that 2001 million it will be an awesome knowledge graph in terms of use case I have to be quick financial market analysis so you can actually combine structured data from finance like shareholders etc and apply sentiment analysis on finance news and then you can propagate what is the sentiments across the network so how the sentiment of a person impacts other person or other companies where they have shares etc significance engine this determines this is a real application this determines if a document in the government should be archived or not based on its significance in in the graph geosearch we talked about it in terms of performance this is the performance for one new forge a note because everything is built atop neo4j so if you have a tree notes cluster it will be approximately three times faster so let's take the longer text document research papers it takes approximately one day to process 1 million documents so but all plugins are not only fun effigy you can take the same code scale on Apache spark or build spring boot application just by requiring the code in your pond and you can scale as much as you want so what the end what is it it is really a bigger system of components that you can compose for solving an information need very important you need an information need of course this is really the difference between data driven and knowledge driven and the great advantage with this ecosystem is that everything is on-premise so we are we don't care about data or we don't want your data in the cloud so we are really focused on on-premise deployment I will let the last word to rob and then it will be Kristin okay so every day now I'm asked what is Hugh it's a great question humans that approach to turning machines from passive things into active things right it's about building machines that collaborate with us to understand huge amounts of information quickly efficiently precisely and reliably that's it that's the whole idea turn information into knowledge at scale for people [Applause] the length of the context will not be really important what's your complex domain what will be important is to the ability to provide enough examples in enough different situation to actually build an accurate model so you cannot build a model with 100 documents this is impossible but you need examples in the with different situations so you can build a model with sentences of six seven words this is not an issue but you need examples but anyway let's say that we tested it with with Wikipedia and we noticed that training it with 400 different musical instruments allowed us to recognize over more than 1,000 different instruments so this means that this specific argument used by stanford allows you to infer musical instruments even though during the training phase is not I mean they are not available so this means that it understands the meaning and create the right it's a model for recognizing this type of musical instruments even though they are not seen but as Krishna said of course you need any way to train this model with a lot of data but doesn't mean that you have to train with all the use cases that you will have but that at least the system should learn how to recognize in a proper way a musical instrument in a context not the specific musical instrument that a music instrument is a generic concept in the text yes no you I mean the knowledge graph is the result of the of the process so the ingestion allow you to take your documents as they are in their normal format that could be PDF PPT Excel whatever and convert them in your own knowledge graph so the idea is that since as Christopher saying it is installed on premise the knowledge graph the resulting knowledge graph will become part of your assets part of the company's asset so because it will be created for you ingesting data that you have that is already part of your knowledge we just convert that knowledge distributed useless where they where it is in something that you can really use so it becomes a value for the company well generally we combine structured documents with a circular so for example a use case I worked on is combining research papers with funding from the government to understand actually by topic or by keyword or actually where is the money going or finding that if I am working on a subject and I've already my abstract I can actually take my abstract compared to all the research papers that they know about to know if one somebody is already having money for this or if somebody worked on something if research have already been completed on this so all that kind but the financial information jury comes with instructive form so yeah so when you just combine them so you you take you try to validate the entities against a structured entity so and anyway you can work on different connectors because our platform is flexible enough to build any kind of connectors to your existing data source wherever your darrell wiki data is an example for example for us we worked a lot with Thomson Reuters data for example or with customers data so it is very common to use another year plot for example to combine it with wait for patron okay thank you thank you [Applause] 