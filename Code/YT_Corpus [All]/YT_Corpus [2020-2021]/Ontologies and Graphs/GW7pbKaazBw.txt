 hello everyone uh thank you for joining my session i'm maureen tessier i'm chief data scientist at riotomy which is a commercial real estate company creating actionable property intelligence today i'll speak to you about knowledge graphs about why we use one and how to use machine learning algorithms to construct all of the components for knowledge graph a little bit about me um i was an academic for well over a decade i i used cosmological simulations to study the evolution of galaxies on the right side you can see a stimulation of the filamentary structure of the universe and on the left side you can see a visualization of that same stimulation where i've re-resolved one of the nodes in that filamentary structure to show your spiral galaxy uh i have been in the startup space for a little over five years and i have worked on projects that have used knowledge graphs for about four years i have always straddled the line between data science and engineering i love it and the work that i'm currently doing in the commercial real estate space that i'll speak about today gives me the opportunity to continue to do just that this is what people think about when they think of commercial real estate but there are 52 million parcels of land across the united states that cover much more than office space and retail space they cover agricultural land they cover transit hubs they cover vacant land we track over 100 attributes on each of these properties as well as their financial histories and then we track even more data if there are multiple structures on the land the information that we have from these structures has historically been stored in in paper in forms in filing cabinets in books in over 3 000 local tax assessors offices across the united states in this modern age uh we have an electronic version of what is essentially just the same forms um for the use case of collecting taxes but if you think about how most of the human beings in this country interact with all of these physical structures they are not collecting taxes they are building these structures or expanding on them they are working within them they are doing maintenance on the buildings they are improving their infrastructure or maybe they're using the value of the properties as one component of a much larger market and tracking that over time and trying to make predictions of the future or possibly they own a portfolio of these structures that they use to generate income the information from these structures this you know the very detailed tax information that we have really only covers one aspect of what is a picture that connects people companies the environment that the structures are within the demographic information for the population that lives there the financial history of the structures as well as just the structural information so we collect information from the tax assessors offices but we also have data sources that cover uh companies across the united states and all of their subsidiaries their dbas llcs that make up a company structure that uh is both all the owners and attendants for these properties um we also collect demographic information um we collect uh location about the financial history information about the financial histories of these properties and individually each of these pieces of information are only a one aspect of this complete picture and none of them even the ones that are focused on people and the contact information from the people and the demographic information from the people they don't cover all of the aspects of a person because of their complex relationships with companies and the relationship that they have through the companies or directly with properties it's only when you combine all of these data sets that you're able to create actionable data for example you can reach out to a property owner a person that owns a portfolio these commercial properties about additional investments or you can reach out to the maintenance person for a company collect their contact information and submit a bid to do some construction work some maintenance work on their on their building so we capture this actionable data through a knowledge graph which is an intellectual ontological structure filled with data and ontology as defined by the oxford dictionary is a set of concepts and categories in a subject area or domain that shows their properties and the relations between them and what that means is that we are actually capturing the entire world of commercial real estate in a way that the human beings that interact with it understand it where they're organizing the data to reflect the real world so we don't have context specific uh buyers owners sellers we have a company that is a tenant we have a person that is a buyer and also a seller we have right so it is uh recognizing the um real world concepts and by approaching it that way you can build much more flexible products and your data is holistically aggregated onto the things that um are the fundamental components of your data structures so here is a cartoon structure of our knowledge graph it's a very simplified version and you can see the connections between the different entity types and the different edge types within our graph you can see that they there's often a hierarchical structure for companies and in fact we capture the hidden ownership ownership which is often hidden entirely legally hidden for the purposes of strategic acquisition and sales looking a little bit more closely at the structural components of the knowledge graph you have your entities so all of the nodes are different entities with different types looking at the entity type person each of these nodes these entities are going to have unique ids and they'll have attributes that live on the entity and the attributes for a person would be first name last name middle name gender age some income bracket etc you also have provenance that lives on the entity so that you know where the attribute information came from and your confidence level uh for the attribute information for that for each entity we also have edges i'm showing edge type that is a manager edge type edges are structurally very simple they only require a type a from id and a 2 id other components that could make up the structure of the edges are are optional based on your product requirements so your edges could have strength uh they could have a directionality um but it's it's it's not a requirement the power of using a knowledge graph of using these structural components is that you're capturing the entity in this very general way that i described a little bit earlier so instead of maintaining code that is specific for owners and code that is specific for secretaries and code that's specific for signatories you're maintaining pipelines and code that is specific for one structure there's a person structure and the contextual information is contained on the edges right and the edge structure in and of itself is also very simple you try to maintain one edge structure for the entire graph so you're actually decreasing the amount of code that you see generally in these types of pipelines very dramatically by using this graph structure it's it's enormously powerful and allows you to build much more quickly than you otherwise would so building these components uh it is all worth taking a step back and looking at the current status of this field um there has been publications on the benefit that you get when you combine information data sources outside of the context specific view that they've been delivered in since the mid 1800s and there was a bit of a resurgence in the idea of knowledge graphs in the 60s as led by newcomb and hsn but you don't really see growth of the field strong growth until the mid 90s i pulled data from the archive api to generate this figure where you see the number of publications per year and you see how quickly the the publication rate is growing so this is excellent news for academics there are a lot of things to discover it's it's a phenomenal time to be doing research into knowledge graphs for industry it's also a very exciting time it means that the tools and the methodologies are growing very quickly they're becoming more and more scalable and able to handle larger and larger volumes of data but what it also means for industry is that this is a very nascent field there isn't a textbook that you can go to that will tell you how to use modern engineering tools in order to build a knowledge graph essentially we we go to these papers we read white papers we read academic papers we read anything that we can get our hands on and we do tests and we build prototypes and then we do full implementations and that's how we've collected the knowledge that i'm speaking to today so let's talk about some of the modern ways that you can build these components i one of the wonderful and terrifying things about knowledge graphs is that you will have many models living within your engineering pipeline there's no separation from your pipes you don't have a situation where you're delivering data into a data lake and then your data scientists operate on the data lake um your machine learning algorithms your ai your statistical methodologies they live within your engineering pipelines the defined shape of the lake so i've separated the construction methodologies into two different um into two different sections in order to emphasize the ramifications on the engineering because although when i speak about these methodologies they might look like different sides of the same coin uh what you'll see is that they have very different implications for what the pipelines look like the architecture looks like so let's speak about uh the maximalistic destructive methodologies that you can use so if you choose to do go this direction you'll be creating your edges first and what i mean by that is that you will be creating as many edges as you can using explicitly defined information in the source data so if you have primary and foreign keys go ahead and use them you will need them and and then some you will also have information that is implicitly defined in the source data so just the fact that information is present together in the same row um will mean that there should be an edge between the entities that live together in that row so uh you have these edges you throw it up into a uh into a graph structure and then you start examining the the edges and the entities that you have that are that are unresolved entities um and you go through the operation that's called edge contraction in order to create entities so i have two graph fragments here so jenny smith is represented twice both jenny's have a phone number 867-5309 and luckily a phone number is a very unique identifier and so you can use uh the fact that it is a unique identifier in order to contract the phone number and then traverse to the jenny smith entity and contract that entity and then you end up with um a structure that has some aggregated attribute information right we have her age and her location and we have the fact that we have three properties that are connected to her whereas previously we only had part of the complete picture so this is the absolute simplest case um and one of the benefits of exploding your data into the graph into these graph structures right off the bat um well after you've done some standardization and some cleaning um is that you can try to capture as many of these simple cases as you can but the reality is that you will go very quickly from simple cases to something that requires a lot of machine learning potentially you're using a deep learning model that's taking the structure of the graph and it's as well as the attribute information and the entity types and making a decision about whether to do a contraction or not do a contraction [Music] this traversal heavy uh methodology has implications for the runtime and uh a lot more i'm gonna mention that that'll come up again later but let's go to um the other avenue the minimalistic constructive way that you can approach building a knowledge graph so um with this approach you're taking an entity's first approach probably um so to illustrate a simple example let's say that you have um person information in one data source or orange data source and you want to compare all of the records in that orange data source against the person information that lives within all the records in the yellow data source so you would be doing a huge volume of comparisons and uh you have to mitigate that you that that's that's problematic and the way that that's done is using an adaptive blocking methodology so you take this huge volume of comparisons and you use a fast high recall method for eliminating a lot of the comparisons that you are would be completely unreasonable to do so there are a lot of methodologies that you can use for adaptive blocking the simplest one the most common one is to use lsh and you can do some joins with a threshold and then um whittle down the number of comparisons that you actually have to explode into a much more thorough feature set and then feed into your high accuracy model in the second stage right and this high accuracy model is making the decision do these pieces of information belong on the same entity or not so you're using a clustering algorithm you're using classification an algorithm or you're using a probabilistic methodology now although these probabilistic methodologies have been shown to be very successful in academic papers they usually don't scale well because they involve the calculation of beijing priors and um it's only recently that work has been done that actually iterates through makes it an approximate calculation for the prior and then iterates through this two-step uh methodology multiple times um uh you can uh take a look at some open source code uh this has been made available by this academic group it's called deep link um i highly recommend it um it's written in spark scala and so if you're using scholar pipeline then you should be able to play with it right away so after we've uh created our entities we've resolved our entities now you have something you have unique ids that you can attach your edges to and to create these edges you grab everything that you can that is available from the source data of course but you will end up doing the same adaptive blocking and then a high accuracy model as a second step process in order to capture your edges the same way that you did in the same way that you did to create resolved entities right so instead of answering the question does all of this information belong on uh the same entity or not you're just asking a slightly different question that says does this information is this entity related to this other entity or not and there's no way to build those edges using this minimalistic constructive methodology if you are trying to make connections between completely different data sets okay um one thing that is a byproduct of the more minimalistic constructive way of building a knowledge graph is that you will be uh generating a lot of skew if you're doing adaptive blocking well right so if you're blocking methodologies working really well then you will be creating skew that mimics the underlying distribution of data um and to illustrate that a little bit uh consider the prevalence of john smith's in the united states right there are about 50 000 john smiths in the united states there's one jamie samoa and naively you could say okay i'm going to have blocks that are 50 000 times larger than than other blocks the reality is a little bit worse it's not likely that your data sources have resolved the john smiths as well as they should just because there are so many of them so you're dealing with a ratio that's a little bit over fifty thousand to one um but uh in your distributed pipeline you can mitigate uh the skew that's created by adaptive blocking methodologies um by changing your partitioning strategy um increasing the size of the partitions increasing the instance sizes for the machines that uh are making up your cluster in order to prevent overflow from the super large blocks and make sure that your job is still chugging along in a good clip so to kind of sum up the two approaches to constructing your knowledge graph our maximalistic destructive um approach uh generates your edges first um it throws all of the data into a graph structure early and to do that you're exploding your data um so you're generating edges and nodes that won't end up being there when you have your completed knowledge graph and this in and of itself could be a big problem for you if you have very large volumes of data and if you're intent on kind of throwing these graph structures into a graph database early you'll you're going to have to load all of these additional edges and nodes into neo4j or aws neptune and and that's going to increase your load time it's also a traversal heavy methodology um and traversing a graph um anybody who has worked with the graph knows is that traversing graphs is is slow um and the larger the greater the traversal the slower your query and the slower that you'll be resolving your entities however this this approach does create maximal edge creation you are going to um have the benefit of the existence of all of these edges regardless of the source and uh potentially you will um create edges that you didn't anticipate using for the product but will be beneficial for doing resolution and so there could be unexpected advantages to taking this approach for the minimalistic constructive approach um you're creating your entities first it's much more compact so if you have larger volumes of data you're not loading things that you're going to destroy into any kind of graph structure or saving those things you're eliminating them from your pipeline as early as possible you will be creating a lot of skew that you will have to um you compensate for um you will be creating only what you [Music] only what you think you need right which um would be great or it could cause you to um miss some of the relationships that are that are present in the data um and then this minimalistic constructive approach also tends to have a little more flexibility and attribute choice when you're take the other approach you want to explode the data as much as possible so that you are able to take a simple approach to doing the entity contractions you can you don't need to worry about that so much if you're taking this minimalistic constructive approach you can really collapse the data as much as you need to in order to get the data through the pipelines so there are um product ramifications for any of the choices that you make around the methodology that you choose around the models that you choose um really an entity resolution itself there are only two things that you can do you can under aggregate your information for the entity or you can over aggregate um if you are under aggregating that means that you'll be missing some of the attribute information but you'll also be missing some of the edges um and this means that whatever products that you're building off of the graph will be missing that information or you'll have duplicates that show up in your ui or your api where your customers are accessing the information if you're over aggregating um you run the risk of uh connecting a having kind of a cascade effect where you're connecting large graph fragments that really should not be connected and so this also causes similar product problems but in the other direction so it's important to kind of think about what those ramifications are when you're tuning your model and when you're approaching how to build the pipelines another thing to consider is um if you are constructing the edges you're using a model it's non-deterministic the model is going to have a confidence level or there's going to be a probability that comes from your statistical methodology that you can translate into the strength of an edge so you can choose to record that information on the edges and surface multiple edges or only surface the best edge that you have for each edge type uh or each um not each edge type but between uh two entities um and and these choices also will affect the product affect your ability to reverse the graph and fact how the information is displayed in your product another tip is that it's always beneficial to include your domain experts when you're building your ontology you even if they're completely non-technical it will actually be easy for them to engage the way that you're organizing the information because um ideally you're representing the world and is true to life way as possible um your another point is that your graph is not going to directly support your product so you're not going to be querying the graph uh from the ui um you would be delivering the data through aws lambda or through elasticsearch index and um to kind of mitigate the issue that people have when um there's an unexpected amount of traversal when you're collecting information from your graph and then a last point here is that your collaboration between data science and data engineering has to be on point it will absolutely affect the rate that you're able to deliver the data and build the graph to kind of elaborate that on that a little bit more and provide an example um i'd like to talk about the spark evaluator and the way that we think about data data scientists handle data and think about data in a column-wise way we create distributions of data we do feature comparisons a lot of the handling we do is column wise um well the spark evaluator is not column wise it's it's it's row wise and um i'm so happy that we no longer have to think about load balancing and message passing and writing our own tools we have spark that does that for us but we still have to think about how data is being moved around all distributed systems and we can't be fighting the spark evaluator and when you have a habit of thinking about things in a column-wise way you write code that is reflective of that thinking and that's something that i see everywhere online and all of the resources that we all use um to help us learn about the way that our our peers are writing code i see udfs everywhere i see uh sql uh queries everywhere and this that kind of reflects this uh column wise thinking this column wise habit but when you have your your data scientists and your data engineers uh working together on a platform like databricks where they're not only just doing co-coding but they can also show each other a big chunks of data that means that you will be able to overcome the habits that both sides have um that that prevent you know really good robust healthy pipelines from being built so uh the code examples that you have been looking at on this slide um show a record-wise way to write features for a model using the scholars apply method completely circumventing the need to use [Music] any of these udfs this code is it's easy to unit test it's easy to read uh it's easy to like it it errors on compile which sql queries don't um and it's two four six uh depending on what your situation is it's it's many many times faster it's actually much much much faster because um record wise code that looks like this makes the spark evaluator really happy and it still creates objects uh data sets or it could create a data frame if you want in in order to do a uh column wise analysis uh after you've generated your features um but i fully believe on bringing your data engineers and your data scientists closely together as possible especially when you have these complex pipelines where you have many many models living within the guts of the pipes and they have to be performant they have to be good and it it absolutely is possible to have your data scientist writing production quality code i know that because we're doing it so i'd like to show you uh this is just a snapshot from our ui it's a property centric snapshot and it's not as snazzy as some of our predictive models like our likely to sell models built off of the graph or our beta portfolios project but what it does show is that is a property in san jose and with this property we're able to bring together building and lot information so this is environmental information as well as structural information bring it together to bear on individual properties um in a easy to absorb way we're able to create ownership information we have an ownership drop down and a tenant drop down it's the same company information with different edge types and then we're also because we have our property resolved it has a unique id it's very easy to attach the financial histories of uh for sales debt and taxes to this property so even though we have a knowledge graph that's supporting the guts you don't see the graph you just see clean easy to use products okay um so today i've spoken about why we would use a knowledge graph what situations would uh would call for it and what the components of the knowledge graph are and how they are structurally uh there's a there's a you're decreasing the number of structural components you need to maintain which is of great benefit um we talked about some of the modern ways to go about constructing large-scale knowledge graphs and what the engineering and product considerations would need to be during that construction i also spoke a little bit about the impact of cross-functional teamwork and tools and i hope that when i covered these topic areas today i created a strong enough framework that you will be able to contextualize the information and decisions that you may come across when you have the opportunity to build a knowledge graph of your own thank you for listening you 